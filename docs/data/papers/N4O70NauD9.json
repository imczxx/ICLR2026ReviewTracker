{"id": "N4O70NauD9", "number": 20271, "cdate": 1758304312307, "mdate": 1759896987087, "content": {"title": "This Is Your Doge, If It Please You: Exploring Deception and Robustness in Mixture of LLMs", "abstract": "Multi-agent systems of large language models (LLMs) operate with the assumption that all the agents in the system are trustworthy.\nIn this paper, we investigate the robustness of multi-agent LLM systems against intrusions by malicious agents, using the Mixture of Agents (MoA; Wang et al., 2024) as a representative multi-agent architecture.\nWe evaluate its robustness by red-teaming it with carefully crafted instructions designed to deceive the other agents. When tested on standard benchmarks, including AlpacaEval, our investigation reveals that the performance of MoA can be severely compromised by the presence of even a single malicious agent, which can nullify the benefits of having multiple agents. \nThe performance degradation becomes more severe as the capability of the malicious agent increases. On the other hand, naive measures, such as increasing the number of agents or replacing faithful agents with stronger models, are insufficient to defend against such intrusions. As a preliminary step toward addressing this risk, we explore a range of unsupervised defense mechanisms that recover most of the lost performance with affordable computational overhead. Our work highlights the security risks associated with multi-agent LLM systems and underscores the need for robust and efficient defense mechanisms.", "tldr": "", "keywords": ["Mixture of Agents", "Deception", "Robustness", "Large Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8aac7ada2e6b39c79ea5686a214ef31a17fe8cfe.pdf", "supplementary_material": "/attachment/0cafd99a92f26a07d77a515ac279b32a1752b2e2.zip"}, "replies": [{"content": {"summary": {"value": "The paper explores the robustness of multi-agent LLM systems against intrusions by malicious agents and finds that performance degradation becomes more severe as the capability of the malicious agent increases. It then investigates a range of unsupervised defense mechanisms that can recover most of the lost performance with minimal computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The topic is interesting and the writing is easy to follow."}, "weaknesses": {"value": "My key concern lies in the definition and realism of the threat model. The paper assumes the presence of malicious agents within a multi-agent LLM system, but it is unclear how such intrusions occur in realistic deployment scenarios or what practical motivations these agents have. Clarifying whether the attacks are external injections, compromised components, or emergent misbehaviors would make the work more convincing. Otherwise, the defense would be simply to remove such agents.\n\nAdditionally, the intended application scenario of the Mixture of Agents (MoA) framework is not clearly articulated. This paper only conducts experiments on easier datasets. Without a concrete context—such as collaborative reasoning, planning, or autonomous decision-making—it is difficult to evaluate the practical relevance and impact of the findings.\n\nWhile the experimental setup and results are clearly presented, the conclusions themselves are somewhat expected: it is intuitive that stronger malicious agents cause more severe degradation, and that simple scaling (e.g., increasing the number of agents) cannot fully mitigate the issue. \n\nOverall, the paper raises an important problem—security and robustness in multi-agent LLM systems—but it would benefit from a clearer threat model, a more realistic application setting, and a deeper analysis of why the proposed defenses are effective and generalizable."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lgL8P6AvWY", "forum": "N4O70NauD9", "replyto": "N4O70NauD9", "signatures": ["ICLR.cc/2026/Conference/Submission20271/Reviewer_Ce3T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20271/Reviewer_Ce3T"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761649598504, "cdate": 1761649598504, "tmdate": 1762933750694, "mdate": 1762933750694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies robustness when one or more agents are malicious/deceptive in a multi-agent system (specifically Mixture of Agents, MoA). The paper evaluates a  3-3-1 MoA on two tasks: (1) QuALITY multiple-choice long-context comprehension with distributed evidence across agents; (2) AlpacaEval 2.0 instruction following. Experiments show that a single deceptive agent can nullify MoA gains and sometimes drop performance below single-model baselines. The paper also proposes unsupervised defenses inspired by the Venetian Doge election: Cluster & Filter (embedding-based clustering of references), Dropout & Cluster (random subsetting then clustering), and LLM-as-a-judge. Experiments show that Clustering defenses recover most performance with low overhead, while Cluster & Filter is both effective and cheap."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  The threat model is clear and important. It demonstrates a concrete and underexplored failure mode of multi-agent LLM systems with compelling empirical evidence.\n\n2. The experiment is comprehensive. In addition to demonstrating the risk, the paper also proposes practical and unsupervised defenses with favorable cost-performance.\n\n3. The presentation of the paper is good. It is clearly written and easy to follow."}, "weaknesses": {"value": "1. Cluster & Filter uses k=2 clustering in embedding space and assumes deceptive outputs cluster apart from truthful ones. This may break when (i) deceptive agents imitate truthful style closely, (ii) multiple deceptive subgroups exist, or (iii) truthful agents disagree (e.g., due to ambiguity). \n\n2. The paper does not report performance with >1 deceptive agent in the defense experiments. The defense results focus on a single deceptive agent.\n\n3. Deception is injected by explicit prompt instructions (opposer/promoter), which is a strong, overt adversary; how well does this capture more realistic/inadvertent failures (e.g., subtle inconsistency, partial hallucination, or distributional shift)? \n\n4. Tasks are two popular benchmarks (QuALITY, AlpacaEval). Additional domains, such as tool-use agents and code generation, would strengthen external validity."}, "questions": {"value": "1. How sensitive are results to the embedding model, k, the filtering threshold, and the number of dropout subsets?\n\n2. In the QuALITY setting, what happens if the aggregator also sees the full passage? Does that blunt deception?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The paper studies robustness when one or more agents are malicious/deceptive in a multi-agent system"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m8ILQgj5qM", "forum": "N4O70NauD9", "replyto": "N4O70NauD9", "signatures": ["ICLR.cc/2026/Conference/Submission20271/Reviewer_RuQ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20271/Reviewer_RuQ6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888113373, "cdate": 1761888113373, "tmdate": 1762933749524, "mdate": 1762933749524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work conduct a comprehensive study on the robustness of MoA architectures against deceptive agents. The authors first evaluate MoA's robustness by red-teaming it with crafted instructions designed to deceive the other agents. And they find that performance degradation will be observed on two mainstream benchmarks. Then, inspired from Venice's legacy, the authors explore a range of unsupervised defense methods to recover the lost performance in the compromised MoA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### **Strengths**\n\n1. In this paper, there are sufficient latest LLMs in the experimental part to investigate the phenomenon of performance degradation of MoA, including llama, qwen, gpt-oss, and mixtral.\n\n2. The proposed method is effective. According to table 5 and 6 in the manuscript, the proposed defense method can obviously promote the robustness of MoA architecture."}, "weaknesses": {"value": "### **Weaknesses**\n\n1. The evaluation scenarios are extremely limited. There are only two benchmarks to investigate the robustness of MoA architecture, which is hard to prove the conclusion of this paper.\n\n2. The authors only investigate the phenomenon of the MoA architecture, it is uncertain that whether the experience can transfer to the other multi-agent architectures.\n\n3. This paper is not the first work to identify this phenomenon, and the proposed defense strategy appears to have little connection with Venice Legacy."}, "questions": {"value": "### **Questions**\n\n1. Could the authors provide additional experiments on more diverse benchmarks or multi-agent architectures?\n\n2. Could the authors further explain the motivation of the defense method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sh0gI05Lkt", "forum": "N4O70NauD9", "replyto": "N4O70NauD9", "signatures": ["ICLR.cc/2026/Conference/Submission20271/Reviewer_cDXM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20271/Reviewer_cDXM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919950401, "cdate": 1761919950401, "tmdate": 1762933749027, "mdate": 1762933749027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the robustness of multi-agent LLM systems, focusing on the Mixture of Agents (MoA) architecture under deceptive agent intrusions. It analyzes key vulnerability factors, such as deceptive agent capability and aggregator model scale. Inspired by the Venetian Doge election process, the work presents several practical and efficient defense strategies which address critical security risks in multi-agent LLM deployments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper tackles a timely and critical problem concerning the security of multi-agent systems, an area of growing importance as such systems are increasingly deployed in high-stakes and safety-critical domains. \n\n2.\tThe experimental evaluation is comprehensive and well-structured. The authors systematically explore the impact of various factors across two distinct tasks, making the findings robust and generalizable.\n\n3.\tThe proposed defense mechanisms, whith is inspired from the Venetian election process, are creative, well-motivated and empirically demonstrated to be highly effective."}, "weaknesses": {"value": "1.\tThe study only concerns isolated and explicitly prompted malicious behaviors, without considering cooperative or adaptive attacks, which limits its generalizability. It is suggested to include multi-agent and dynamic attack scenarios and show robustness curves under different attack intensities or adversary ratios.\n\n2.\tExperiments are limited to reading comprehension and open QA, lacking evaluation in high-risk domains, like healthcare or finance. It is suggested to extend experiments to such domains and include risk-sensitive metrics to assess real-world robustness.\n\n3.\tThe defense assumes malicious agents are a minority (Sec. 6.1). When attackers are the majority or mimic honest behavior, clustering and filtering may fail. Robustness under different attacker ratios and mimicry-based attacks should be tested.\n\n4.\tComparisons rely on a single baseline, omitting other multi-agent defenses and anomaly detection techniques. Broader baseline comparisons are supposed to validate the proposed method’s advantage."}, "questions": {"value": "Refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5y7YH1El9V", "forum": "N4O70NauD9", "replyto": "N4O70NauD9", "signatures": ["ICLR.cc/2026/Conference/Submission20271/Reviewer_m9S5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20271/Reviewer_m9S5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20271/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009974250, "cdate": 1762009974250, "tmdate": 1762933748791, "mdate": 1762933748791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}