{"id": "V0Fb4pwhS4", "number": 20102, "cdate": 1758302468717, "mdate": 1759897001909, "content": {"title": "HexMachina: Self-Evolving Multi-Agent System for Continual Learning of Catan", "abstract": "We aim to improve on the long-horizon gaps in large language model (LLM) agents by enabling them to sustain coherent strategies in adversarial, stochastic environments. Settlers of Catan provides a challenging benchmark: strategic success depends on balancing short- and long-term goals in the face of dice randomness, trading, expansion, and blocking. This is difficult because prompt-centric LLM agents (e.g., ReAct, Reflexion) must re-interpret large, evolving game states every turn, quickly saturating context windows and failing to maintain consistent strategy across episodes. We propose HexMachina, a continual learning multi-agent system that separates environment discovery (inducing an adapter layer without documentation) from strategy improvement (evolving a compiled player). This architecture preserves executable artifacts, letting the LLM focus on high-level strategy design rather than per-turn decision-making. In controlled Catanatron experiments, HexMachina learns from scratch, evolving players that outperform the strongest human-crafted baseline (AlphaBeta). Our best runs achieve a 54% win rate against AlphaBeta, outperforming prompt-driven LLM agents and shallow no-discovery baselines. Ablations further confirm that greater focus on pure strategy improves performance. Theoretically, this shows that artifact-centric continual learning can transform LLMs from brittle per-turn deciders into stable strategy designers, providing a reusable path toward long-horizon autonomy.", "tldr": "HexMachina is a self-evolving LLM system that learns Catan from scratch, preserves code artifacts, and refines strategies. It outperforms prompt-based agent baselines and rivals the strongest human-crafted bot, demonstrating continual learning.", "keywords": ["Multi-Agent System", "LLM", "Strategic Planning", "Lifelong Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d570c095dcc7ed4f2c145121a9815796d113e7f8.pdf", "supplementary_material": "/attachment/f7471f46f64c7668ded995c39bff561ab64883d6.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduced HexMachina, an agentic game-playing framework that receives feedbacks from gameplay and then evolve a python code as policy. The paper tests its method in the game called Settlers of Catan, and the results show that the method leads to better policy after rounds of evolution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is sound and the performance beats the baselines compared in the paper."}, "weaknesses": {"value": "1. Lack of novelty. Receiving feedbacks from game environments and leveraging them to improve python-code-as-policy is not a new idea. And the way to implement it using role-playing is also a common practice.\n2. Lack of in-depth analysis. For example, as mentioned in the last paragraph, current method is limited by context length. However, it is mentioned or analyzed in the paper.\n3. Lack of baselines. The baselines seem to be simple. Is it possible to include more baselines? For example, is it possible to train an agent using RL as a learning-based baseline?"}, "questions": {"value": "1. The alphabeta baseline seems to perform quite well already (according to Table 2). Given a larger budget, would it surpass the FooPlayer?\n2. Why are the experiments conducted solely in a two-player setting, given that the paper introduced it as a 3–4 player game? It would be interesting to see how the agent evolves in a multi-player game environment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GPc3hXypCp", "forum": "V0Fb4pwhS4", "replyto": "V0Fb4pwhS4", "signatures": ["ICLR.cc/2026/Conference/Submission20102/Reviewer_pNbf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20102/Reviewer_pNbf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718794393, "cdate": 1761718794393, "tmdate": 1762932998372, "mdate": 1762932998372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HexMachina, a self-evolving multi-agent system on Catan games, which separates environment discovery and strategy improvement, so it can focus on high-level strategy design rather than per-turn decision-making. In order to verify the system, the paper conducts experiments on Catan games by comparing several baselines including Random, LLM Player, basic continual player and AlphaBeta."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a self-evolving multi-agent system for Catan games, which separates environment discovery and strategy improvement. This design can evolve players that consistently execute intelligent, long-horizon strategies, outperforming traditional LLM agents.\n\nThe paper conducts experiments on Catan games, and outperforms several baselines, such as prompt-driven baselines and alphabeta."}, "weaknesses": {"value": "The paper is hard to follow, and many parts lack clarity. For example: How does the multi-agent system actually operate and evolve? how to learn an unknown environment without any human documentation, especially inducing execute code into Adapter files? how to preserve key code and knowledge as artifacts, and improve its strategy via a closed-loop process? Why is the system able to consistently carry out a long-term plan across an entire game? How exactly does each agent (Orchestrator, Analyst, Coder, Researcher, and Strategist) perform its tasks? How does the Memory mechanism function in practice?\n\nAdditionally, a significant limitation is that all experiments are conducted in a single environment (Catan). The generalizability of the approach to other long-horizon, adversarial domains remains unverified. The paper also does not compare against other continual LLM agents (e.g., Voyager, Eureka) in the same environment.\n\nWhile the architectural components of HexMachina, such as multi-agent LLM systems and code generation for policy implementation, are well-established in prior works like GameGPT, MetaGPT, CAMEL, and Voyager, the paper should better articulate what is fundamentally new in HexMachina beyond the application to Catan."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QPJyVhpfnW", "forum": "V0Fb4pwhS4", "replyto": "V0Fb4pwhS4", "signatures": ["ICLR.cc/2026/Conference/Submission20102/Reviewer_XLCZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20102/Reviewer_XLCZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814590720, "cdate": 1761814590720, "tmdate": 1762932997625, "mdate": 1762932997625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HexMachina, a self-evolving multi-agent system that enables large language models to learn long-horizon strategies in Settlers of Catan. By separating environment discovery from strategy improvement, the system allows the LLM to generate and refine executable code autonomously. Experiments show a 54% win rate against the AlphaBeta baseline, surpassing prompt-based agents. While the idea of artifact-centric continual learning is promising, the results are limited to one domain and modest in scope."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important challenge—long-horizon strategy learning for LLM agents—by introducing a novel self-evolving multi-agent system.\n- The separation of environment discovery and strategy improvement is an original and well-motivated design choice that enhances coherence and performance.\n- The work is clearly presented and empirically validated on a meaningful benchmark (Settlers of Catan), demonstrating measurable gains over strong baselines."}, "weaknesses": {"value": "1. Limited empirical scope: Experiments are confined to a single domain (Settlers of Catan), which weakens claims about general continual learning or long-horizon reasoning. Broader validation across different environments would strengthen the argument.\n2. Marginal improvement over baselines: While the win rate exceeds AlphaBeta slightly, it is unclear whether the gain reflects genuine strategic learning or task-specific heuristics. More analysis of learned behaviors is needed.\n3. High system complexity and cost: The multi-agent setup requires multiple LLMs and heavy computation, raising concerns about scalability and practicality for broader deployment.\n4. Insufficient system visualization and excessive appendix code: The paper lacks a clear architecture diagram and illustrative case study that explain how the components interact throughout the self-evolution process, while the appendix includes excessive raw code that detracts from clarity and readability."}, "questions": {"value": "1. How generalizable is HexMachina to other environments beyond Catan? Could the same discovery–improvement architecture transfer to non-game domains (e.g., robotics or programming tasks)?\n2. What concrete mechanisms ensure stability during self-evolution (e.g., avoiding catastrophic forgetting or overfitting to AlphaBeta’s style)?\n3. How are adapter induction and code validation automated? Are there failure cases where the induced API abstraction was incorrect or inconsistent?\n4. What is the computational cost (in tokens or hours) per evolution step, and how does it scale with larger LLMs?\n5. Were any qualitative or behavioral analyses performed on the evolved agents to verify that HexMachina truly develops long-horizon planning behaviors—such as consistent expansion strategies, adaptive trading, or opponent modeling—rather than merely optimizing short-term heuristics for higher win rates?\n6. Is there a risk that HexMachina simply overfits to the fixed AlphaBeta opponent rather than learning generally robust strategies?\n7. How dependent is the framework on high-end proprietary models (e.g., GPT-5-mini, Claude 3.7)? Could comparable performance be achieved with open-source LLMs such as Llama 3 or Qwen2.5, or would performance degrade substantially?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CgMARFOsNr", "forum": "V0Fb4pwhS4", "replyto": "V0Fb4pwhS4", "signatures": ["ICLR.cc/2026/Conference/Submission20102/Reviewer_8Ma2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20102/Reviewer_8Ma2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939099828, "cdate": 1761939099828, "tmdate": 1762932996844, "mdate": 1762932996844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "HexMachina is a multi-agent system that learns while it plays by having several different agents with different roles interact to refine their game strategy. An orchestrator agent during the discovery phase tasks other agents to write code in a adapters file that can be executed in the catan game. Then during the improvement phase the agents are orchestrated to improve the code that will be run during the game and refine it as it plays."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The work shows some really interesting results when the system learns continually as it plays Catan, evetually beating a strong human policy baseline.\n- Dual memory system is interesting, comprising of the game memory and semantic memory to enable continual learning."}, "weaknesses": {"value": "- Only a single game is tested, Settlers of Catan. It would improve the work much more if more games were considered, otherwise I find that it is easy to make erroneous conclusions from just results on a single noisy game.\n- The ablations in 6.3 seem too superficial. I was interested in seeing whether the multi-agent setup really mattered or if a single agent would also work fine. While 6.3 ablations remove different agents from the setup, its unclear how they are removed and how the fewer agent setup is designed. What are the prompts, what tools do they have or are aware of etc.\n- Limitation section mentions that code hallucination was a common failure case and had to be addressed. how was this addressed, did the system simply iterate on the code until the code ran without errors, or was there human intervention? This is quite unclear.\n- Appendix lacks significant information, e.g. what are the prompts each agent is given. With the lack of information, it becomes more unclear how such a multi-agent system can be reproduced or whether it even matters."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AP7Qu0nlsG", "forum": "V0Fb4pwhS4", "replyto": "V0Fb4pwhS4", "signatures": ["ICLR.cc/2026/Conference/Submission20102/Reviewer_dBFM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20102/Reviewer_dBFM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942964810, "cdate": 1761942964810, "tmdate": 1762932996334, "mdate": 1762932996334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}