{"id": "HIi4lNsvXW", "number": 11187, "cdate": 1758192589245, "mdate": 1763381701101, "content": {"title": "Efficient Submodular Maximization for Sums of Concave over Modular Functions", "abstract": "Submodular maximization has broad applications in machine learning, network design, and data mining. However, classical algorithms often suffer from prohibitively high computational costs, which severely limit their scalability in practice. In this work, we focus on maximizing Sums of Concave over Modular functions (SCMs), an important subclass of submodular functions, under three fundamental constraints: cardinality, knapsack, and partition matroids. \n\tOur method integrates three components: continuous relaxation, Accelerated Approximate Projected Gradient Ascent (AAPGA), and randomized rounding, to efficiently compute near-optimal solutions. We establish a $(1 - \\varepsilon - \\eta - e^{-\\Omega(\\eta^2)})$ approximation guarantee for both cardinality and partition matroid constraints, with query complexity $O\\left(n^{1/2}\\varepsilon^{-1/2} (T_1 + T_2)\\right)$. For the knapsack constraint, the approximation ratio degrades by a factor of $1/2$, with query complexity $O\\left(n T_1 + n^{1/2}\\varepsilon^{-1/2} T_2\\right)$, where $T_1$ denotes the computational cost of evaluating the concave extension, and $T_2$ denotes the computational cost of backpropagation. By leveraging efficient convex optimization techniques, our approach substantially accelerates convergence toward high-quality solutions. \n\tIn empirical evaluations, we demonstrate that AAPGA consistently outperforms standard PGA. On small-scale experiments, AAPGA achieves superior results in significantly less time, being up to $32.3\\times$ faster than traditional methods. On large-scale experiments, our parallel multi-GPU implementation further enhances performance, demonstrating the scalability of our approach.", "tldr": "", "keywords": ["Submodular maximization", "Sums of Concave over Modular functions", "Accelerated Approximate Projected Gradient Ascent", "Randomized rounding", "GPU-parallel optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f481dd7de0b4cf21b2560dfd719d39b919a84b72.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors study the problem of submodular maximization where the objective function can be expressed as the sum of concave composed with modular functions (SCMs), subject to various types of constraints. The proposed algorithms utilize the continuous relaxation, Accelerated Approximate Projected Gradient Ascent (AAPGA), and randomized rounding.  The method achieves an approximation ratio of $1-\\epsilon-\\eta-e^{-\\Omega(\\eta^2)}$ for both cardinality and partition matroid constraints and $1/2(1-\\epsilon-\\eta-e^{-\\Omega(\\eta^2)})$ for general matroid constraints."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an interesting problem of submodular maximization where the objective function is formulated as a composition of concave and linear functions. The motivation is clearly presented, and the inclusion of representative examples such as the coverage and facility location functions helps illustrate the relevance of the studied setting.\n2. The authors tackle an important challenge in continuous optimization methods for submodular maximization. While such methods often yield stronger approximation guarantees compared to discrete approaches, their high query complexity and computational overhead have limited their practical use. Hence, the effort to design continuous algorithms that are both query-efficient and computationally efficient is timely and significant.\n2. The integration of deep neural networks into the study of submodular optimization is an innovative and inspiring direction. This approach has the potential to be generalized to a broader class of submodular functions and could open new avenues for combining learning-based techniques with classical optimization theory."}, "weaknesses": {"value": "The key concern is the poor clarity and organization of the paper. Several important details are missing from the main text, and some parameters are introduced without proper definition or explanation (see the Questions section for specifics). The core ideas underlying the proposed algorithms are not clearly presented, making it difficult to follow the methodological contributions. Furthermore, the paper does not clearly articulate how its technical contributions differ from or improve upon existing work."}, "questions": {"value": "1. The parameter $\\eta$ in Algorithm 1 is introduced without explanation, which makes it difficult to interpret the theoretical guarantees.  \n   - Is $\\eta$ assumed to lie within the range \\([0, 1]\\)? If so, it seems that the approximation guarantee is valid only for certain values of \\(\\eta\\), since when $\\eta=0$ or $\\eta=1$, the bound becomes trivial.  \n   - In addition, the scaling factor $\\beta$ does not appear in the theoretical guarantee—are $\\eta$ and $\\beta$ intended to represent the same parameter?  \n   Clarification on this point is needed.  \n2. The definition of the set $P'_L(\\mathbf{x})$ (Line 206) takes a point $\\mathbf{x}$ as input; however, in the actual expression, only $y^{(t)}$ appears. \n    It is unclear what $\\mathbf{x}$ refers to in this context. Please clarify the definition and role of $\\mathbf{x}$.\n3. Can you clarify in detail why the proposed Accelerated Approximate Projected Gradient Ascent method is better than the existing PGA method?\n4. The computational cost of \\textbf{Algorithm 1} is not clearly analyzed. In particular, Line 5 in the pseudocode requires determining $i_t$, which involves evaluating Equation (4) for each possible $i_t$. How is the number of $i_t$ evaluations bounded? \n5. The paper mentions the use of \\textbf{deep neural networks} to accelerate the computation of the supergradient. However, if the objective function is simply a composition of a concave and a linear function, it is unclear why a neural network is necessary for this task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FrLa5eKQCZ", "forum": "HIi4lNsvXW", "replyto": "HIi4lNsvXW", "signatures": ["ICLR.cc/2026/Conference/Submission11187/Reviewer_suJC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11187/Reviewer_suJC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853001417, "cdate": 1761853001417, "tmdate": 1762922341225, "mdate": 1762922341225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the maximization problem of the sum of concave composed with modular functions (SCMs), an important subclass of submodular functions, under cardinality, knapsack, and partition matroid constraints respectively. Paper proposes an optimization framework that integrates three key components: continuous relaxation, accelerated approximate projected gradient ascent, and randomized rounding. The experimental evaluation assesses the effectiveness and acceleration advantages of our method from three perspectives: convergence speed, small-scale submodular maximization, and large-scale submodular maximization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a novel optimization framework that employs the concave extension during the optimization phase to enhance computational efficiency, while leveraging the multilinear extension in the rounding phase to ensure theoretical approximation guarantees. The relationship between these two extensions is bridged by a key lemma, thereby balancing computational performance with theoretical assurance. Furthermore, to address the non-differentiability of certain activation functions in SCMs, the paper presents a supergradient calculation formula based on the right-hand derivative and integrates it with the backpropagation mechanism of neural networks, effectively resolving gradient computation in non-differentiable cases."}, "weaknesses": {"value": "1. It is unclear whether the results for the randomized algorithms in Section 4.2 represent averages from multiple runs or the outcome of a single run.\n\n2. In the experimental section, several key parameters that affect the performance of the AAPGA, such as the approximation projection error $\\delta$ and the step size $L_0$, are not provided.\n\n3. Some related works on submodular maximization under knapsack constraints may require further discussion to contextualize the current paper, such as:\n\n   * *Fast adaptive non-monotone submodular maximization subject to a knapsack constraint*\n   * *Submodular maximization subject to a knapsack constraint: Combinatorial algorithms with near-optimal adaptive complexity*\n   * *Streaming algorithms for constrained submodular maximization*\n   * *Linear-Time Algorithms for Representative Subset Selection From Data Streams*"}, "questions": {"value": "The paper discusses how to handle the non-differentiability of the activation function in SCM, but Theorem 1 assumes that $F$ is continuously differentiable. There seems to be a discrepancy between these two statements. Could you please clarify the rationale behind this assumption?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Ls8KKAYASO", "forum": "HIi4lNsvXW", "replyto": "HIi4lNsvXW", "signatures": ["ICLR.cc/2026/Conference/Submission11187/Reviewer_nxbx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11187/Reviewer_nxbx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914171539, "cdate": 1761914171539, "tmdate": 1762939994428, "mdate": 1762939994428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an efficient algorithm, Accelerated Approximate Projected Gradient Ascent (AAPGA), for maximizing Sums of Concave composed with Modular functions (SCMs). The method applies a Nesterov-accelerated projected gradient ascent optimization directly on the function's concave extension. Theoretically, the algorithm achieves a $(1-\\epsilon-\\eta-e^{-\\Omega(\\eta^{2})})$ approximation guarantee with a \"square-root level\" query complexity of $O(n^{1/2}\\epsilon^{-1/2}(T_{1}+T_{2}))$, where $T_1$ is the cost of backpropagation and $T_2$ is the cost of function evaluation. Empirical evaluations confirm that AAPGA converges faster and achieves superior results compared to the standard Projected Gradient Ascent (PGA) method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper considers a subclass of submodular functions and provides the algorithm with sublinear query complexity.\n2.\tThe paper considers several important constraints in the submodular maximization area. \n3.\tExperimental results are provided to show the effectiveness of their algorithms."}, "weaknesses": {"value": "1.\tThe meaning of the parameter $ \\eta $ is barely explained in the abstract or the main text, which can be confusing to readers.\n2.\tThe meanings of the parameter $T_1$ and $T_2$ are not consistent in the paper. It is confusing. \n3.\tThe knapsack results seem weak: the rounding procedure sacrifices a $1/2$ approximation ratio by enumerating the largest item in an outer loop."}, "questions": {"value": "1.\tCan the costs of $T_1$ and $T_2$ be made explicit as closed-form expressions? If we consider the traditional value oracle model, what is $T_1$ and $T_2$？\n2.\tThe meanings of the parameter $T_1$ and $T_2$ are not consistent in the paper. For example, let us consider the time of evaluation of the concave extension. In the abstract, it is $T_2$, but in the corollary 1, it is $T_1$. The parameters in table 1 and in the description of table 1 (in page 2) are also not consistent. \n3.\tWhat is the parameter $ \\eta $? It seems this parameter appears in Lemma 1 which shows that $\\eta$ is related to the SCM function itself. Thus if we cannot choose arbitrary small $\\eta$, the approximation ratio of the proposed algorithm may be very bad."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hSS6S3ox53", "forum": "HIi4lNsvXW", "replyto": "HIi4lNsvXW", "signatures": ["ICLR.cc/2026/Conference/Submission11187/Reviewer_ehgF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11187/Reviewer_ehgF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919375074, "cdate": 1761919375074, "tmdate": 1762922340204, "mdate": 1762922340204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies submodular function optimization where the submodular function is a concave function over the sum of modular functions (SCM).  The authors establish a continuous algorithm for cardinality constraint,  whose approximation ratio is (1-\\epsilon) and query complexity is $\\sqrt(n) (T_1+T_2)$, where $T_1$ is the time needed for evaluating the concave function and $T_2$ is time needed for back propagation.  Authors present algorithms for knapsack case with an approximation ratio 1/2-\\epsilon. \n\nI would like to disclose that I have only superficial familiarity with continuous optimization techniques."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "This work substantially improves the query complexity compared to previous works, especially for the cardinality constraint.  For example, for cardinality constraints, the best known query complexity is  $O(n/epsilon)$. Similarly, for knapsack, the best known quesry comlexity isO(nklog^2 n)$. \n\nThe main technical contribution is the development of the Accelerated Approximate Projected Gradient Ascent algorithm, that converges faster than the known algorithms."}, "weaknesses": {"value": "While the query complexity is smaller, when measured in-terms of $n$, it is not clear what is effect of T_1 and T_2. If they are large, then it is not clear if the proposed algorithm offers any advantage.\n\nTypically, continuous optimization algorithms run slowly compared to the discrete versions. Given that it is not clear how much computational advantage the proposed algorithm offers compared to the LS+PGB algorithm that uses $O(n/epsilon)$ queries"}, "questions": {"value": "Please see the weakness\nWhat is the effect of T_1 and T_2? How large could they be in the worst-case?\nIt is not clear to me that the rounding techniques proposed are any different from the known pipage rounding. Can you explain the differences?\nCan you compare the performance with the algorithms listed in lines 59, 60 and 61?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BIuR2kczfS", "forum": "HIi4lNsvXW", "replyto": "HIi4lNsvXW", "signatures": ["ICLR.cc/2026/Conference/Submission11187/Reviewer_4AFT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11187/Reviewer_4AFT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762024883619, "cdate": 1762024883619, "tmdate": 1762922339721, "mdate": 1762922339721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}