{"id": "xtatNQbMdm", "number": 18305, "cdate": 1758286235464, "mdate": 1759897112497, "content": {"title": "Rethinking the Flow-based Gradual Domain Adaption: A Semi-Dual Transport Perspective", "abstract": "Gradual domain adaptation (GDA) aims to mitigate domain shift by progressively adapting models from the source domain to the target domain via intermediate domains. However, real intermediate domains are often unavailable or ineffective, necessitating the synthesis of intermediate samples. Flow-based models are recently used for this purpose by interpolating between source and target distributions, but their training typically resorts to sample-based log-likelihood estimation, which can discard useful information and thus degrade GDA performance. The key to addressing this limitation is constructing the intermediate domains via samples directly. To this end, we propose an $\\underline{\\text{E}}$ntropy-regularized $\\underline{\\text{S}}$emi-dual $\\underline{\\text{U}}$nbalanced $\\underline{\\text{O}}$ptimal $\\underline{\\text{T}}$ransport (E-SUOT) framework to construct intermediate domains. Specifically, we reformulate flow-based GDA as a Lagrangian dual problem and derive an equivalent objective that circumvents the needs for likelihood estimation. However, the dual problem results in the unstable min–max training procedure. To alleviate this issue, we further introduce entropy regularization to convert it into a more stable alternative optimization procedure. Based on this, we propose a novel GDA training framework and provide theoretical analysis in terms of stability and generalization. Finally, extensive experiments are conducted to demonstrate the efficacy of the E-SUOT framework.", "tldr": "We propose a novel semi-dual optimal transport perspective to improve the performance of flow-based gradual domain adaption.", "keywords": ["Gradual Domain Adaption", "Optimal Transport", "Gradient Flow", "Semi-Dual Transport", "Convex Optimization"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba12143c0255ea6df7dcb01e693242d0273d53cf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposed a semi-dual formulation for gradual domain adaptation. Such a  formulation avoids the use of probability density which is hard to estimate. An additional entropy regularizer is added to ensure the uniqueness of the problem. A concrete algorithm is developed based on this formulation, and a theoretical bound is proposed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The formulation itself is interesting as it avoids the use of pdf.\n2. The use of entropy regularizer ensures the uniqueness of the solution."}, "weaknesses": {"value": "1. The author mentioned that gradual domain adaptation is useful \"when the source–target shift is substantial or class overlap is weak\" in line 41, but I do not find any theoretical and experimental evidence to support this. In the experiments, all baseline methods are gradual adaptation methods. The comparison with one-shot methods with different \"class overlap\" and \"source–target shift\" is lacking.\n\n1. The lack of experiments on standard datasets like Office-Home and VisDa.\n\n2. The author mentioned that \"flow-based methods still require explicit estimation of the target domain’s PDF to guide the evolution,\" (line 448) However, in previous works like Zhuang et al. (2024), the pdf of the target domain is never explicitly estimated, and only the samples are needed. Also, the statement in contribution 1 should be modified.\n\n3. The algorithm seems extremely inefficient. For each time steps, $w$ is trained for several epochs, then $T$ is trained for several epochs, finally $h$ is finetune on all time steps.\n   \n4. The author claims that the proposed method is stable in line 76. However, no evidence is provided. I suggest adding an ablation study on this."}, "questions": {"value": "1. In figure 1, why the generated samples of E-SUOT are different from the ground truth? The variance of the generated samples seems much smaller than the ground truth.\n2. What is the self-training method?\n3. In Thm 6. The hypotheses and the loss function need to be Lipschitz. Does this result apply to the classification problem considered in this work? How to ensure the Lipschitzness of hypotheses?\n4. Line 303. \"From Theorem 5, we observe that as t increases, the transported PDF ρ(x) progressively becomes similar to pT (x).\" Why?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "A part of Fig.2 is a reproduction of Fig.1 in [1], but this is not stated anywhere in the paper.\n\n[1] Zhuang, Zhan, Yu Zhang, and Ying Wei. \"Gradual domain adaptation via gradient flow.\" ICLR. 2024."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CCoNUhLVnS", "forum": "xtatNQbMdm", "replyto": "xtatNQbMdm", "signatures": ["ICLR.cc/2026/Conference/Submission18305/Reviewer_e9LB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18305/Reviewer_e9LB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761688074769, "cdate": 1761688074769, "tmdate": 1762928026713, "mdate": 1762928026713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work considers the fundamental limitations of flow-based method for gradual domain adaptation, i.e., the quality of estimated PDF of target and generated intermediate domains. The key idea is to avoid the explicitly estimation of PDF by introducing the semi-dual formulation of gradient flows with entropy regularization, which ensure the stability and convergence of training. Theoretical results on optimality and generalization error are provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The idea of improving flow-based method with PDF-free metric (i.e., Wasserstein) seems to be interesting and sounded.\n\n+ The theoretical results are solid to ensure the numerical property of the defined flow model and the generalization error of the adaptation process.\n\n+ The empirical results are convincing, which show the proposed method indeed shows consistent behavior with the theoretical analysis."}, "weaknesses": {"value": "+ The justifications w.r.t. the theoretical assumption and results could be improved.  \n\n+ The empirical validations seem to be limited, e.g., the compared baselines and evaluation datasets."}, "questions": {"value": "I have no major criticisms on this submission. Here are several minor points. \n\n**Questions**\n\nQ1. The smooth label space assumption in assumption (A.3) could be justified more deeply. Though such an assumption could be satisfied in many scenarios, there are still some cases which the label space could change rapidly with a slight change of feature x. For example, the fine-grained recognition tasks, where the subcategories could be close in feature space while totally distinct in label space. Thus, it would also be important to clarify the feasible and infeasible scenarios of the derived results, e.g., the condition of the assumption.\n\nQ2. There seems to be intractable terms in the generalization bound Eq. (12). Specifically, the constants in the third and fourth terms. For example, the selection of the loss function could significantly affect the upper bound, while other terms like label smoothness are even unknown. It would be highly appreciated to justify the intractable constants in detail.\n\nQ3. It seems that some related advanced framework is not compared in empirical evaluation, e.g., diffusion-based methods that share similar innovations. Specifically, compared with the standard diffusion process, does the proposed framework admit better properties? Besides, could the proposed method achieve better empirical performance?\n\nQ4. Will the developed flow method be sensitive to the data scale? Specifically, would it be feasible or efficient for large-scale data? Moreover, the existing empirical evaluation only considers simple and small datasets. Are there any other potential application scenarios of GDA that have larger data scales?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MuP0735bmu", "forum": "xtatNQbMdm", "replyto": "xtatNQbMdm", "signatures": ["ICLR.cc/2026/Conference/Submission18305/Reviewer_bhcj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18305/Reviewer_bhcj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759187420, "cdate": 1761759187420, "tmdate": 1762928026292, "mdate": 1762928026292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a semi-dual optimal transport formulation stabilized by entropy regularization for flow-based gradual domain adaptation. It aims to overcome the weakness in existing flow-based GDA methods which rely on accurate, explicit target domain probability density function estimation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tNovelty: The idea of reformulating flow-based GDA within a semi-dual OT framework to eliminate the need for explicit target PDF estimation seems novel.\n-\tSolid Theoretical Analysis: The theoretical analysis of the semi-dual formulation, proofs of uniqueness/convergence, and generalization bounds sound solid.\n-\tImproved Stability: The entropy regularization effectively addresses the inherent instability of the adversarial semi-dual formulation.\n-\tReproducibility: The source code is available, facilitating its reproducibility."}, "weaknesses": {"value": "-\tLimited Empirical Evaluation: Experiments are conducted only on low-dimensional (8D) UMAP embeddings of relatively simple datasets (Portraits, rotated MNIST). The performance and scalability on high-dimensional, complex real-world datasets (e.g., Office-Home, DomainNet) remain unproven.\n-\tHyperparameter Sensitivity: The model performance is highly sensitive to multiple key hyperparameters (batch size B, discretization step size η, simulation steps T, entropy regularization strength ε,), as shown in Figure 3. This necessitates careful tuning and could limit practical applications.\n-\tComputational Burden: While scaling better than some GP-based methods, the approach still requires training a sequence of neural networks (w_φ and T_θ for each intermediate step), making it computationally intensive compared to simpler baselines.\n-\tLack Intuitive Illustration: The dense notations and propositions without the intuitive illustrations influences its accessibility."}, "questions": {"value": "-\tWhy the features are mapped to low-dimensional (8D) UMAP embeddings? \n-\tHow does the performance and stability of E-SUOT change with the dimensionality of the input data? \n-\tAre there limitations to the current neural parameterization of w_φ and T_θ in very high-dimensional spaces?\n-\tThe motivation of using unbalanced optimal transport instead of the standard OT in this application had not been made clear. Also, how did you tune the unbalanced factors lambda_1 and lambda_2 defined in Equation A.6 for the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4rEy4Ok6mv", "forum": "xtatNQbMdm", "replyto": "xtatNQbMdm", "signatures": ["ICLR.cc/2026/Conference/Submission18305/Reviewer_eTpZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18305/Reviewer_eTpZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883519917, "cdate": 1761883519917, "tmdate": 1762928025736, "mdate": 1762928025736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The method reformulates the flow-based adaptation as a Wasserstein-distance-regularized optimization problem. By deriving the semi-dual of this problem, they create an objective function that cleverly avoids PDF estimation and relies only on sample-based expectations. In general, the paper makes contribution by unifying flow‑based GDA with semi‑dual OT and by offering an entropy‑regularized, sample‑only training objective with uniqueness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and easy to follow. The proposed solution is also theoretically sound. It involves reformulating the gradient flow as a Wasserstein-regularised problem and then moving to its semi-dual form in order to bypass density estimation. However, the novelty of this reformulation is questionable because the same steps of reformulations has previously been applied to different OT methods. However, this type of optimization and practical problem had not previously been considered,  so I am more in favour of the novelty of the method."}, "weaknesses": {"value": "The proposed solution is theoretically sound well. The reformulation of the gradient flow as a Wasserstein-regularized problem and the subsequent move to its semi-dual form to bypass density estimation. But novelty of this reformulation is questioning because previously the same tricks was applied to the different OT methods. Specifically for this type of the optimization problem it was not previously considered, so I am toward more that the method is novel. \n\nOne of the main weaknesses of the paper is that all datasets are relatively simple (Portraits; MNIST rotated 45°/60°) and the pipeline uses semi‑supervised UMAP embeddings to “preserve class discriminability” before adaptation (Sec. 4.1, p. 6). It is unclear whether all baselines receive the same UMAP features, and whether the gains persist without this strong, label‑aware pre‑processing. The absence of standard multi‑domain vision benchmarks (e.g., Office‑Home/DomainNet/VisDA) makes it hard to assess real‑world impact. Also table 1 marks methods that E‑SUOT “significantly” outperforms but does not report standard deviations or the number of seeds/runs in the main text; the testing protocol (data splits, early‑stopping criteria, target‑side tuning) is summarized at a high level only. This makes it hard to assess robustness.\n\nThe paper focuses on adapting the feature distribution ($p(x)$). The workflow described in Algorithm 2 appears to assume the labels are invariant along the transport path ($y_{t+1} \\leftarrow y_t$). It is not clear how the framework would perform in scenarios with significant label shift (i.e., where the relationship $p(y|x)$ also changes between domains), which is a common for unbalanced settings."}, "questions": {"value": "*Question 1*: Can you add experiments on widely used multi‑domain image benchmarks without UMAP pre‑processing, and complementary tests that introduce mass/prior mismatch (label‑shift, class imbalance, missing classes) to demonstrate the benefit of the unbalanced formulation? Please report per‑method standard deviations across multiple seeds.\n\n*Question 2*: Your ablation study shows that the KL divergence (via its conjugate $f^*$) performs significantly better than other f-divergences like $\\chi^2$ or an identity function. What is the intuition for this? \n\n*Question 3*: Does KL divergence have a specific property (perhaps related to its gradient flow or the stability of its conjugate function $f^*$) that makes it uniquely suited for this semi-dual transport framework?\n\n*Question 4*: The full algorithm chains multiple transport steps ($T_{\\theta,0}, T_{\\theta,1}, ...$). How do errors from a non-optimal transport map $T_{\\theta,t}$ at an early step $t$ propagate through the rest of the chain? It would be interesting if authors can draw a parallel to the diffusion process and importance of the earlier steps."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rZoQ0fWlTG", "forum": "xtatNQbMdm", "replyto": "xtatNQbMdm", "signatures": ["ICLR.cc/2026/Conference/Submission18305/Reviewer_Pos6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18305/Reviewer_Pos6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18305/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762373457777, "cdate": 1762373457777, "tmdate": 1762928024999, "mdate": 1762928024999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}