{"id": "Vg9Mx9vCE4", "number": 4525, "cdate": 1757698029197, "mdate": 1759898028317, "content": {"title": "MixReasoning: Switching Modes to Think", "abstract": "Reasoning models enhance performance by tackling problems in a step-by-step manner, decomposing them into sub-problems and exploring long chains of thought before producing an answer. However, applying extended reasoning to every step introduces substantial redundancy, as sub-problems vary widely in difficulty and complexity: a small number of pivotal steps are genuinely challenging and decisive for the final answer, while many others only involve straightforward revisions or simple computations. Therefore, a natural idea is to endow reasoning models with the ability to adaptively respond to this variation, rather than treating all steps with the same level of elaboration. To this end, we propose MixReasoning, a framework that dynamically adjusts the depth of reasoning within a single response. The resulting chain of thought then becomes a mixture of detailed reasoning on difficult steps and concise inference on simpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning shortens reasoning length and substantially improves efficiency without compromising accuracy.", "tldr": "", "keywords": ["Large Reasoning Model; Efficient Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/347f10ae24972426d655e48763ecda11e1cacdc9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tackles redundancy in chain-of-thought (CoT) reasoning, where all sub-steps are treated with equal detail despite varying difficulty. It introduces MixReasoning, a framework that dynamically adjusts reasoning depth within a single CoT—using concise reasoning for trivial steps and detailed reasoning for critical ones. The approach employs a LoRA adapter enabling a “concise mode,” and monitors token-level uncertainty (via next-token entropy) to switch between modes during inference, with minimal overhead through KV-cache reuse. Experiments on reasoning benchmarks (e.g., GSM8K, MATH, AIME) show significant token reduction (≈47%) while maintaining or improving accuracy. Overall, the paper offers a novel, practical method for adaptive reasoning that better balances efficiency and accuracy by mirroring human-like focus on challenging reasoning steps."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is an original and well-executed contribution addressing redundancy in chain-of-thought reasoning. It identifies the overlooked issue of intra-chain heterogeneity—that not all reasoning steps require equal depth—and introduces MixReasoning, which dynamically switches between concise and detailed reasoning modes based on token-level uncertainty. The approach is novel in both concept and implementation, using a LoRA adapter for lightweight mode switching and entropy-based triggers for adaptive control.\n\nThe work is technically solid and clearly presented: experiments on standard benchmarks show that it reduces reasoning length while maintaining or improving accuracy, and the method’s efficiency (via KV-cache reuse) makes it practical. The exposition is clear and logically structured, supported by quantitative and qualitative analysis.\n\nOverall, the paper’s strengths lie in its originality, methodological soundness, and practical significance. It offers a compelling step toward adaptive, efficient reasoning in LLMs—an idea likely to influence future research on dynamic CoT and budgeted inference."}, "weaknesses": {"value": "1. Limited Baseline Context & Novelty Clarification: The concept of adaptive reasoning depth within a chain is innovative, but the paper does not adequately compare it with existing global mode switching methods (e.g., AdaptThink, Emergent Mind). Without clearer positioning, the claim of originality appears overstated. The paper would benefit from a more detailed comparison to highlight the unique contributions of this approach.\n\n2. Insufficient Heuristic Sensitivity Analysis: The entropy-based mode-switching heuristic lacks a thorough sensitivity analysis regarding key hyperparameters (e.g., τ↑/τ↓, rollback window B/F). These parameters likely have a significant impact on both performance and computational cost. Ablation studies should be conducted to analyze the effects of these hyperparameters and their influence on false positive/negative triggers in reasoning.\n\n3. Narrow Evaluation Scope: Experiments are limited to mathematical reasoning tasks (GSM8K, MATH, AIME), and the broader claim of \"reasoning efficiency\" is not supported across different task domains. To validate the generality of the approach, experiments on non-mathematical reasoning tasks should be added, or the paper should acknowledge the method's domain limitations.\n\n4. Reproducibility & Deployment Concerns: The paper lacks detailed implementation information and underreports practical performance, especially latency due to rollbacks. While token savings are mentioned, real-world efficiency may still degrade because of rollback delays. The authors should provide more implementation details, pseudocode, and wall-clock latency results to evaluate the feasibility of real-world deployment.\n\n5. Interpretability & Readability of Reasoning Chains: The paper claims improved readability of reasoning chains, but evidence for this is minimal. The concise mode, while reducing reasoning length, may compromise the transparency or completeness of reasoning. Human evaluation of reasoning quality and explanation completeness across different modes is needed to validate these claims."}, "questions": {"value": "1. Could you clarify how your method differs in terms of granularity (token/step vs. question-level) and switching criteria (entropy vs. RL)? A direct comparison or ablation against a question-level switching baseline would be helpful.\n\n2. How sensitive are the results to changes in the hyperparameters (τ↑, τ↓, B, F, α_low/α_high)? Did you perform grid or annealing experiments on hold-out sets to select them?\n\n3. What is the end-to-end inference latency (including rollback overhead) compared to a baseline without switching? How often did rollbacks occur in your experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "c5Um8TmfXO", "forum": "Vg9Mx9vCE4", "replyto": "Vg9Mx9vCE4", "signatures": ["ICLR.cc/2026/Conference/Submission4525/Reviewer_9avu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4525/Reviewer_9avu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744639446, "cdate": 1761744639446, "tmdate": 1762917423897, "mdate": 1762917423897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the computational inefficiency inherent in Large Language Models that apply a uniform, step-by-step reasoning process to all problems, regardless of their intrinsic difficulty. The authors posit that not all steps in a reasoning chain are equally complex; a few pivotal steps are decisive, while many others are routine. To address this, they introduce **MixReasoning**, a framework that dynamically switches between a detailed \"thinking\" mode and a concise \"non-thinking\" mode within a single response generation. This mode-switching is triggered by monitoring token-level uncertainty during inference. The framework is evaluated on the GSM8K, MATH-500, and AIME benchmarks, where it is shown to shorten reasoning lengths and improve efficiency, often without a loss in accuracy."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "**Targets a Critical Problem:** The research focuses on the important and practical challenge of adaptive reasoning. Reducing the substantial inference cost and latency of long chain-of-thought (CoT) processes is crucial for making Large Language Models viable in interactive applications. The paper correctly identifies that a finer-grained, intra-response approach is a logical next step beyond problem-level routing, which treats entire problems as either simple or complex."}, "weaknesses": {"value": "* **Evaluation on Saturated Benchmarks:** The primary benchmark used, GSM8K, is largely considered saturated, with top models achieving over 95% accuracy. On such benchmarks, it is difficult to demonstrate meaningful accuracy improvements or robustly differentiate the performance benefits of a new method from random variance.\n\n* **Limited Model Diversity:** The experiments rely heavily on the Qwen model family (Qwen3-8B and Qwen3-14B), which constitutes two of the three models tested. This lack of diversity raises questions about the generalizability of the findings and the \"model-agnostic\" claim.\n\n* **Insufficient Ablation Studies:** The paper lacks critical ablation studies to validate its core claims. While it includes an analysis of LoRA targets (MLP vs. attention layers), it fails to investigate:\n    * **The efficacy of the uncertainty trigger:** There is no comparison against simpler or random switching heuristics to prove that the uncertainty metric is the key factor driving performance.\n    * **The standalone performance of the concise mode and verbose mode:** The paper does not report the baseline performance of the model when forced to use only the concise (LoRA-adapted) mode. This makes it hard to discern what generates performance in MixReasoning .\n\n* **Marginal Performance Improvements:** The reported results do not demonstrate strong performance gains across the board in terms of performance compared to number of tokens.\n\n* **No comparison to Continuous Chain of Thoughts method [1]:** Continuous Chain of Thought has showed the ability to reason with less tokens which would be an important baseline to compare to \n\n[1] Hao, S. (2024). Training large language models to reason in a continuous latent space"}, "questions": {"value": "1.  The uncertainty-based trigger is the core of MixReasoning's dynamic behavior. How does this trigger compare to simpler switching heuristics, such as switching at fixed intervals, randomly, or based on the presence of certain keywords (e.g., \"Therefore,\" \"Let's calculate\")?\n\n2.  What is the standalone accuracy of the concise model and non-concise model (i.e., the base model with the LoRA adapter always active with strength α_low) on the evaluation benchmarks? This baseline is essential for understanding the trade-offs and determining whether the concise mode is a capable-but-brief reasoner or a degraded one that requires the \"thinking\" mode to salvage accuracy.\n\n3.  The framework is tested on two Qwen models and one other model. To better substantiate the \"model-agnostic\" claim, have you considered applying MixReasoning to models with different architectures and training schemes, such as models from the Llama or Mistral families, which are widely-used open-source baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HhCyY3rwcN", "forum": "Vg9Mx9vCE4", "replyto": "Vg9Mx9vCE4", "signatures": ["ICLR.cc/2026/Conference/Submission4525/Reviewer_87iN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4525/Reviewer_87iN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937362326, "cdate": 1761937362326, "tmdate": 1762917423648, "mdate": 1762917423648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MixReasoning is an inference-time framework that dynamically adjusts reasoning depth within a single CoT response. Rather than applying uniform elaboration across all steps or making binary problem-level decisions, the method adaptively switches between detailed \"thinking\" and concise \"non-thinking\" modes based on local token-level uncertainty. This achieves substantial token compression (25-47% reduction on benchmarks) while maintaining or improving accuracy, offering practitioners explicit controllability over the accuracy-efficiency trade-off."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Different from previous problem-wise reasoning mode selection, the proposed method adjusts the reasoning in a more flexible way: it determines whether a detailed explanation is needed during the generation of the single response. \n2. The plug-in LoRA-based adaptor for uncertainty estimation without sacrificing the capability ofthe  base model. \n3. MixReasoning is runtime-efficient by allowing reuse of KV cache: the KV cache tokens generated in the previous thinking mode can be prefilled and reused in the new thinking mode.\n4. Figure 3 shows that with the same generation tokens, the accuracy of MixReasoning is higher than baseline models, demonstrating the better accuracy–efficiency Pareto frontier the proposed method achieves.\n5. They also investigate the control of reasoning length and find that MLP layers contribute more than the attention. Besides, without modifying the attention layers, the KV cache can also be reused to speed up inference."}, "weaknesses": {"value": "1. Experiment results are not strong enough to show the effectiveness of the proposed method. In Table 1, the improvement of pass@1 is not significant. For example, on GSM8K, the average improvement (compared with the best baseline models) is around 0.2%, indicating that only 2 more problems can be solved. It is unclear whether this improvement is caused by the randomness in sampling. Similarly, the improvement on Math-500 and AIME 2024 is also marginal. The significance test and standard deviation should be added to verify the effectiveness.\n2. Some evaluation details are missing. For example, what is the temperature used during evaluation? Besides, it is unclear to me how the evaluation results are calculated. In the appendix, it says that all results are averaged over 5 runs. Considering the 500 cases in Math-500, the pass@1's granularity should be 1/500 = 0.002. After averaging on 5 runs, the last digit should still be an even number. The pass@1, such as 0.8937, looks weird to me because it indicates that 2,234.25 problems are correct in 500 * 5 instances. \n3. Figure and Table typos\n\t1. In Table 1, under the setting of Qwen3-8B on Math-500, the best performance that should be bolded is 0.9320 instead of 0.9313. \n\t2. Figure 5: The meaning of the y-axis is unclear. Is a higher y value better, or reverse? According to Lines 459-464, it seems that Figure 5(a) aims to compare the reduced token count. If that is the case, there should be one baseline without finetuning to show the initial token count."}, "questions": {"value": "1. Can the LoRA for uncertainty estimation trained on one dataset be transferred to another dataset or domain? \n2. While the window size and uncertainty threshold can control the mix of modes, is there an empirical way to select suitable hyperparameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "khIvTgVReT", "forum": "Vg9Mx9vCE4", "replyto": "Vg9Mx9vCE4", "signatures": ["ICLR.cc/2026/Conference/Submission4525/Reviewer_SvJm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4525/Reviewer_SvJm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952537423, "cdate": 1761952537423, "tmdate": 1762917423445, "mdate": 1762917423445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}