{"id": "ioeOm5tFDM", "number": 7341, "cdate": 1758016966521, "mdate": 1763338307016, "content": {"title": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention and Contextualized Learnable Token Eviction", "abstract": "Linear-attention models that compress the entire input sequence into a fixed-size recurrent state offer an efficient alternative to Transformers, but their finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct access to past tokens. We interleave token mixers of intermediate time and space complexity between linear attention and full attention, including the query-aware native sparse attention, and sparse attention with token eviction. We further propose a novel learnable token eviction module. Combined with sliding-window attention, an end-to-end trainable lightweight CNN-based eviction module aggregates information from both past and future adjacent tokens to adaptively retain a limited set of critical KV-pairs per head, maintaining linear attention's constant time and space complexity. Empirical evaluations on retrieval-intensive benchmarks support the effectiveness of our approaches.", "tldr": "Enhanced retrieval performance of subquadratic time/constant space models by the hybrid of linear attention and query-aware sparse attention or our novel learnable token eviction network that evicts KV cache based on past and future local contexts.", "keywords": ["linear attention", "language modeling", "sparse attention", "token eviction"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a91300c1be51445a85c6a54999f9483eb1674855.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a form of learned token eviction which uses a CNN to learn the token eviction scores within a sliding window. The proposed method requires training from scratch."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Sparse Attention is an important topic for long context models as the attention operation is quadratic\n- The authors utilize many recent works as a motivation and backbone for their method."}, "weaknesses": {"value": "- L165: The probing step remains quadratic due to the constant setting of the block size $M$. As this is part of the attention computation as a precursor operation, I don't think you claim that the attention computation is a constant $MK$. \n\n- How can the method presented in figure 2 ever learn to retain a token in a simple task such as needle-in-a-haystack? For example, datasets like RULER have some tasks where the retrieval target can be any number of key value pairs which is not known until the query is given at the end of the prompt? How could this scheme know what to retain in this situation?\n\n- Figure 3 says the out of window KV's have a capped capacity, but it looks like the second row extends to all previous KV's. Shouldn't this be capped instead of storing all of them?\n\n- I don't quite understand equation 6. If the mask was applied, meaning that there was no computation in the forward pass for $v^\\prime_{j,h}$, as this computation was skipped. Is the last term on the RHS supposed to be $\\langle v_{j,h}, \\frac{\\partial \\mathcal{L}}{\\partial v^\\prime_{j,h} \\rangle$?\n\n- L301 states that unimportant tokens will receive negative gradients as a matter of fact. How can you be so sure of this?\n\n- Table 2 is very hard to make sense of with all numbers above 70% being bolded.  \n\n- This is supposed to be for linear (and therefore efficient) attention, but there is no latency comparison between models. \n\n---\n\nOverall, I find the method and the results to be underwhelming at best. The results seem mixed and inconclusive between methods which makes it hard to pin down the exact strengths of the method. As there is no latency comparison, readers have no idea of the overall efficiency compared to the other baseline methods. It is also hard to see the exact benefit of the learned token eviction when it is not compared to a simple baseline such as aggregated attention scores within the window. \n\n---\n \n## Minor\n\nL107: stemm --> stemming?\n\nL130: \"tokens can be self-evidently crucial\" --> I find \"self-evidently\" to be problematic here. If they were self evidently crucial, then they would be easy to identify and retain. But in the case of a passkey, it is not self evident that they are crucial for the task without perfect knowledge of the query which will be asked. I think these words should be rephrased."}, "questions": {"value": "- RULER was only considered for NIAH single tasks up to 4K. Can you extend this to the more complicated tasks with longer contexts? The model that uses NSA should be capable of handling this at least, right?\n\n- Could it be possible that getting ride of the LTE module and only tracking token scores could lead to a similar or better eviction policy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y6RIA9c8Wd", "forum": "ioeOm5tFDM", "replyto": "ioeOm5tFDM", "signatures": ["ICLR.cc/2026/Conference/Submission7341/Reviewer_Hr9r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7341/Reviewer_Hr9r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761197727561, "cdate": 1761197727561, "tmdate": 1762919466164, "mdate": 1762919466164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Learned Token Eviction (LTE) algorithm. The authors combine LTE with linear attention, specifically sliding window attention, and refer to it as laLTE."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The method is simple to implement."}, "weaknesses": {"value": "## Lacks of novelties\nThe method is somewhat akin to combining existing elements.\n\n## Lacks of efficiency analysis\n\nEven if the method is claimed as linear complexity, the linear complexity does not always mean faster and efficient than flash attention. The author lacks a critical analysis of efficiency in real-world hardware. Any latency seconds were not reported.\n\nEspecially about CNN, the latency analysis is really crucial, since the small size of Conv operation is known to be slower than normal vector-matrix and matrix-matrix operations. I want to hear the answer and a detailed analysis of the following things:\n - Really need CNN?\n - How much does CNN slow down in wall clock latency? Please compare it to HW efficient alternatives (Mamba, LightningAttention2)\n\n## Eviction is still a critical problem in a multi-turn request scenario, especially for tool calling\n\nSince the KV cache is more and more critical in an agentic AI scenario, we cannot drop the KV cache without a precise study.\n\n - Is there any analysis about tool callings?\n - What is the agentic LLM performance?\n\n## Where is the training curve?\n - I cannot be sure this model is sufficiently trained or scalable\n\n## Lacks of some strong performance baselines:\n- Mamba2\n- Lightening attention https://github.com/MiniMax-AI/MiniMax-M1\n\nI think we need to include such alternatives to build a competitive method for deployable methods."}, "questions": {"value": "## Typo \n- line 259: se -> sequence (?)\n\n## Questions\n- How does the inverse rotation affect the precision errors? on fp8? fp16? fp4?\n- Table 2, you must put the latency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F3ibv8Lqyr", "forum": "ioeOm5tFDM", "replyto": "ioeOm5tFDM", "signatures": ["ICLR.cc/2026/Conference/Submission7341/Reviewer_LAgD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7341/Reviewer_LAgD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918217354, "cdate": 1761918217354, "tmdate": 1762919464754, "mdate": 1762919464754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the recall gap of linear-attention and recurrent LMs by re-introducing targeted access to past tokens without abandoning efficiency. It studies two hybrid designs: (1) laNSA: interleaves linear attention with Native Sparse Attention (NSA) that combines query-aware block probing, a compressed branch, and SWA-style gating. This improves retrieval but keeps an O(N) KV cache. (2) laLTE: interleaves linear attention with Learnable Token Eviction (LTE). A tiny per-token, per-head 1D-CNN predicts whether to retain a KV as it leaves a large SWA window; combined with an attention sink and SWA, this aims for O(1) time and space per step while preserving long-range evidence. Experiments at roughly 0.4B and 1.4B parameters trained on 10B/30B tokens (FineWeb-Edu) evaluate short-context language tasks and long-context retrieval (RULER S-NIAH, EVAPORATE). Both laLTE and laNSA outperform strong linear baselines; laNSA is strongest among linear-time models on EVAPORATE, while laLTE is the best constant-space option and approaches hybrid full-attention variants in some settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Novelty: \nThis work proposes two complementary mixers interleaved with linear attention—NSA for query-aware sparse access over the full past and LTE for learned keep/evict under a strict cache budget; introduces per-token, per-head retention via a tiny 1D-CNN with SWA-enabled look-ahead and an attention sink to maintain near-constant KV memory; provides deployment-minded decoding/KV design (two-segment cache, lazy batched scoring) and frames a clear accuracy–efficiency Pareto frontier (laLTE for constant-space, laNSA for higher accuracy under linear time).\n\n### Differentiation from prior works: \nThis work moves beyond fixed windows and global/uniform/time-decay heuristics by making head-aware, context-conditioned retention decisions; regains direct token-level access to salient long-range evidence without reverting to O(N²) attention, contrasting with state-space/recurrence approaches that compress history into fixed states; offers a stronger NSA baseline (query-aware probing + compressive branch + SWA gating) that sharpens comparisons."}, "weaknesses": {"value": "### Scale and generality: \nResults are limited to 0.4B/1.4B. It is unclear whether the trends hold for larger, modern LLM families (e.g., Qwen2.5/3, DeepSeek) or for multilingual/code models.\n### Benchmark breadth: \nThe evaluation focuses on long-context retrieval. Broader benchmarks commonly used today (e.g., instruction following, math, and code such as AlpacaEval, GSM8K, HumanEval) are absent, making it hard to gauge side effects beyond retrieval.\n\n### Efficiency reporting: \nThe paper argues constant time/space for laLTE by design, but it does not provide systematic, measured wall-clock throughput and GPU memory usage across mixers (GDN, +SWA, laLTE, laNSA, +full-attention).\n\n### NSA dependency: \nlaNSA still requires O(N) KV. The trade-off versus laLTE is discussed conceptually; clearer guidance on when laNSA is preferable in practice would help."}, "questions": {"value": "1. Can you report measured GPU memory (GB), effective KV size, tokens/sec, and latency for GDN, GDN+SWA, laLTE, laNSA, and a GDN+full-attention hybrid at 4K and at least 8K contexts on the same hardware?\n\n2. Since laLTE/laNSA are trained modules, could you also include an inference-only comparison on the same GPU (with matched context length and batch size) against training-free efficient attention approaches such as HiP [1]? Reporting retrieval accuracy (e.g., RULER/EVAPORATE), throughput (tokens/s), and peak memory would help clarify whether the additional training cost yields meaningful gains over training-free sparse attention methods.\n\n--- \n[1] Lee et al. A Training-free Sub-quadratic Cost Transformer Model Serving Framework With Hierarchically Pruned Attention. ICLR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yXirROwlqd", "forum": "ioeOm5tFDM", "replyto": "ioeOm5tFDM", "signatures": ["ICLR.cc/2026/Conference/Submission7341/Reviewer_uoix"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7341/Reviewer_uoix"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924011765, "cdate": 1761924011765, "tmdate": 1762919463798, "mdate": 1762919463798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets the “forgetfulness” of linear-attention models by interleaving Gated DeltaNet layers with stronger token mixers. Two variants are proposed: (1) laLTE, which introduces a learnable token eviction (LTE) module that scores each KV pair per head using a tiny 3-layer 1D CNN with a short receptive field and then retains only a capped number of out-of-window tokens. (2) laNSA, which swaps in Native Sparse Attention (NSA) layers that perform query-aware block, offering more direct access to the past but requiring O(N) KV memory.\nThe authors position these mixers on a complexity–access hierarchy. Empirically, on EVAPORATE and RULER, the hybrids often outperform pure GDN/GDN+SWA, while full-attention interleaves remain the strongest but are also the most costly."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Per-head, per-token scoring from short local context with grouped 1D convs is simple, parallel, and adds ~1% params. The design is a constant budget and predictable latency.\n2. Results are reported on both synthetic (S-NIAH) and realistic (EVAPORATE) retrieval benchmarks, showing consistent gains over pure GDN/GDN+SWA in many settings."}, "weaknesses": {"value": "1. The novelty is limited. The laNSA component is adopted from prior NSA work, and the overall recipe seems an alternation of existing hybrid attention rather than a fundamentally new mechanism. The idea of LTE also sits close to the broader family of token-eviction methods, making the novelty feel incremental.\n2. Evidence does not decisively beat the common practice. The improvements on EVAPORATE are modest averages (e.g., laLTE/laNSA only several points over GDN/GDN+SWA), while interleaving full attention (GDN+Attn.) remains stronger in many settings.\n3. No end-to-end latency measurements (prefill & decode) under the claimed constant budgets, nor e2e latency comparisons against strong kernels (e.g., Flash-/Flex-Attention baselines) make it hard to assess the practical gains of LTE beyond proxy complexity.  \n4. The evaluated models are relatively small (0.4B/1.4B on 10B/30B tokens FineWeb-Edu), which limits the strength of conclusions about scalability to modern LLMs. \n5. Ablation study on LTE is insufficient. The paper motivates head-wise independence and a short receptive field, but there is limited analysis on (i) sensitivity to the cap b, window w, and receptive field R; (ii) alternatives to CNN scoring (MLP/other efficient attention predictors)."}, "questions": {"value": "1. Could authors report e2e prefill and decode latency vs. GDN, GDN+SWA, and GDN+Attn., all using the same optimized kernels?\n2. How does laLTE compare to other recent learnable-eviction or head-aware KV-budgeting methods under a matched training budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bP4UKuOXAs", "forum": "ioeOm5tFDM", "replyto": "ioeOm5tFDM", "signatures": ["ICLR.cc/2026/Conference/Submission7341/Reviewer_37ke"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7341/Reviewer_37ke"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7341/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969837177, "cdate": 1761969837177, "tmdate": 1762919462990, "mdate": 1762919462990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}