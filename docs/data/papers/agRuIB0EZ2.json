{"id": "agRuIB0EZ2", "number": 15518, "cdate": 1758252258621, "mdate": 1759897301858, "content": {"title": "Do MLLMs Really Understand the Charts?", "abstract": "Although Multimodal Large Language Models (MLLMs) have demonstrated increasingly impressive performance in chart understanding, most of them exhibit alarming hallucinations and significant performance degradation when handling non-annotated charts. We argue that current MLLMs rely largely on visual $\\textit{recognition}$ rather than visual $\\textit{reasoning}$ to interpret the charts, and visual estimation of numerical values is one of the most fundamental capabilities in chart understanding that require complex visual reasoning.\nTo prove this, we introduce ChartVRBench, a benchmark meticulously designed to isolate and evaluate visual reasoning ability in chart understanding. Furthermore, we propose ChartVR-3B/7B trained with a novel Visual Reasoning Reinforcement Finetuning (VR-RFT) strategy to strengthen genuine chart visual reasoning abilities. Extensive experiments show that ChartVR achieves superior performance on ChartVRBench, outperforming even powerful proprietary models. Moreover, the visual reasoning skills cultivated by the proposed VR-RFT demonstrate strong generalization, leading to significant performance gains across a diverse suite of public chart understanding benchmarks. The code and dataset will be publicly available upon publication.", "tldr": "", "keywords": ["Visual Reasoning", "Reinforcement Tuning", "Chart Understanding", "Multimodal Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b69cab09605ad7da0c5ec5ae442ab2677243e7a9.pdf", "supplementary_material": "/attachment/1f82151f10b370caf8b6f29addd0c17c29bbe884.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates a critical failure mode in Multimodal Large Language Models (MLLMs): their inability to perform visual reasoning on charts that lack explicit numerical annotations. The authors argue that MLLMs excel at text recognition but fail at genuine visual reasoning, which they define as the ability to estimate numerical values from a chart's visual geometry (e.g., axes, scales, and positions).\n\nTo address this, the paper makes two primary contributions. First, it introduces ChartVRBench, a new benchmark specifically designed to isolate and evaluate this numerical estimation skill. The benchmark contains both synthetic and real-world charts, all deliberately stripped of explicit data labels. Second, it proposes ChartVR, a series of MLLMs trained with a novel two-stage Visual Reasoning Reinforcement Finetuning (VR-RFT) strategy. Stage 1 (\"Activation\") uses Supervised Fine-Tuning on distilled Chain-of-Thought (CoT) data to teach the model a structured reasoning process. Stage 2 (\"Generalization\") uses Group Relative Policy Optimization (GRPO), a reinforcement learning algorithm, to refine the model's precision, guided by a continuous quadratic accuracy reward function."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Excellent Problem Formulation: The paper's core premise is strong, clear, and important. It correctly identifies a key weakness in current MLLMs and frames a well-defined research question: are models reasoning or just recognizing? The focus on non-annotated charts is a methodologically sound way to isolate this specific capability.\n\n2. Purpose-Built Benchmark: The creation of ChartVRBench, a benchmark designed specifically to test a single, well-defined skill (visual numerical estimation), is a good scientific practice. The inclusion of both synthetic and real-world data is a positive design choice."}, "weaknesses": {"value": "1. High Risk of Data Contamination and Invalid Evaluation: The paper states that the training data for both the SFT and RL stages is derived directly from the ChartVRBench dataset itself. The SFT data consists of reasoning traces distilled from ChartVRBench's question-answer pairs, and the RL data consists of samples from ChartVRBench where the SFT model was inconsistent. The paper never explicitly states that it maintained a strict separation between the subset of ChartVRBench used for training data generation and the subset used for final evaluation. This strongly suggests that the model was trained on the same data distribution it was tested on, making the impressive results on ChartVRBench a measure of memorization or overfitting, not generalization. This potential train-test overlap invalidates the paper's primary empirical results.\n2. Incremental Methodological Contribution: All the techniques are from prior works.\n3. Narrow Evaluation Scope and Unconvincing Generalization Claims: The paper's claim that it imparts a \"foundational and highly generalizable\" reasoning ability is not sufficiently supported. The evaluation of generalization is restricted only to other chart-specific benchmarks (CharXiv, ChartBench, ChartQAPro). This demonstrates in-domain transfer, but it does not prove that the model has learned a general reasoning skill. To validate a claim about foundational reasoning, the evaluation should have included out-of-domain benchmarks that require similar geometric or logical skills, such as mathematical reasoning.\n4. Concerns with Distilled Data Quality and Utility: The SFT stage relies on reasoning data distilled from a powerful teacher model (Qwen2.5-VL-32B). This introduces two issues: i) The student model's reasoning quality is capped by the teacher's ability. It is learning to mimic one specific, potentially flawed or suboptimal, reasoning style. ii) The very fact that a 32B model can generate these reasoning traces (even post-hoc) raises questions about the dataset's utility for training even more powerful models in the future. It creates a potential \"ceiling effect\" for the task itself."}, "questions": {"value": "1. Can you please provide an unambiguous confirmation that a strict and held-out test set from ChartVRBench was used for evaluation, and that no part of this test set (images, questions, or answers) was used in any capacity during the SFT or RL data construction phases?\n\n2. Given that VR-RFT is a pipeline of existing techniques (CoT-SFT, GRPO), could you clarify what you see as the core novel algorithmic contribution beyond the application to this specific task and the design of the reward function?\n\n3. How can you substantiate the claim that the model has learned a \"foundational\" reasoning ability, rather than just a set of chart-specific interpretation skills? Why were no out-of-domain reasoning benchmarks (e.g., math, general VQA) included to test this claim of generalizability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GeUQmKkhgP", "forum": "agRuIB0EZ2", "replyto": "agRuIB0EZ2", "signatures": ["ICLR.cc/2026/Conference/Submission15518/Reviewer_P44o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15518/Reviewer_P44o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760694392155, "cdate": 1760694392155, "tmdate": 1762925803876, "mdate": 1762925803876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines whether Multimodal Large Language Models (MLLMs) truly comprehend charts, especially non-annotated ones requiring visual estimation of numerical values. It introduces ChartVRBench, a benchmark with 2,453 Q&A pairs across seven chart types, using synthetic and real-world data to test visual reasoning. The authors propose ChartVR-3B/7B, trained with a two-stage Visual Reasoning Reinforcement Finetuning (VR-RFT) method—Chain-of-Thought Supervised Finetuning (CoT-SFT) followed by Group Relative Policy Optimization (GRPO) with a continuous accuracy reward. Experiments show ChartVR outperforming models like Gemini-2.5-Flash on ChartVRBench and generalizing well to other benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Important Topic**: Chart understanding is a crucial area for advancing MLLM applications in real-world data analysis, such as scientific papers and financial reports.\n\n**Comprehensive Training Approach**: The VR-RFT strategy effectively combines SFT and RL, with the continuous quadratic accuracy reward delivering a dense, fine-grained signal to refine visual estimation precision.\n\n**Strong Empirical Results and Generalization**: ChartVR demonstrates superior performance on the new benchmark and transfers well to diverse public datasets, suggesting the cultivated visual reasoning is foundational and not task-specific. The paper includes qualitative case studies highlighting improved reasoning chains."}, "weaknesses": {"value": "**Limited Novelty**:  I think this is the main weakness of this paper. The core idea of visual reasoning on non-annotated charts is not entirely new; benchmarks like PlotQA (cited in the paper, but several years before) already include such data without explicit annotations. Also, the synthetic data generation relies on established methods like Code-as-Intermediary Translation and Self-Instruct, reducing the perceived innovation in dataset curation.\n\n**Insufficient Comparison to Recent SOTA Models**: The evaluation lacks direct comparisons with stronger open-source chart-specific models like Chart-R1, which employs similar CoT supervision and RL strategies to achieve state-of-the-art results on chart understanding benchmarks—potentially undermining claims of superiority."}, "questions": {"value": "See Cons."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6V2X4ReV4a", "forum": "agRuIB0EZ2", "replyto": "agRuIB0EZ2", "signatures": ["ICLR.cc/2026/Conference/Submission15518/Reviewer_M2Y6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15518/Reviewer_M2Y6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572259800, "cdate": 1761572259800, "tmdate": 1762925803256, "mdate": 1762925803256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem that MLLMs fails to undertand charts without annotations. A specific benchmark ChartVRBench is developed to comprehensively evaluate the ablites of MLLMs in such a scenario. Besides, to boost the performance of MLLMs in undertanding annotation-free charts, ChartVR is proposed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-organized, clearly introducing the background and motivation.\n2. The motivation and problem studied in this paper are significant, highlighting current MLLMs could not understand charts without annoations accurately.\n3. The benchmark is necessary for the field of chart understanding, which measures the ability of MLLMs in chart understanding in a specific scenario.\n4. The performance of ChartVR on both ChartVRBench and public general chart understanding benchmark is great."}, "weaknesses": {"value": "1. In Table 2, since MLLMs with size of 3B are compared, the performance of ChartVR-3B on public chart understanding benchmarks is supposed to be compared.\n2. The performance of MLLMs on Radar charts is poor and far from human baseline, does it mean the reasoning pattern could not generalize to this type?"}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t2CjcZbzzg", "forum": "agRuIB0EZ2", "replyto": "agRuIB0EZ2", "signatures": ["ICLR.cc/2026/Conference/Submission15518/Reviewer_kvg2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15518/Reviewer_kvg2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806921884, "cdate": 1761806921884, "tmdate": 1762925802834, "mdate": 1762925802834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ChartVRBench, a benchmark designed to isolate and evaluate visual reasoning capability in chart understanding. The authors proposed ChartVR, a series of LVLM with significantly enhanced visual reasoning capabilities for chart understanding. The paper tackles an important problem at the intersection of LVLM and data visualization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow, with a clear structure.\n- The motivation behind the work is interesting and well-presented.\n- The proposed method (ChartVR) is technically sound and clearly described."}, "weaknesses": {"value": "1. The paper is missing a discussion and analysis of the proposed benchmark. This paper claims the benchmark, ChartVRBench, is part of its contribution. However, there is limited analysis of the benchmark's quality or comparison with existing benchmarks.\n2. The chart data in the benchmark are mostly synthesized by an LVLM. Without assessing its quality, the performance on the benchmark cannot be supported. Evaluation, such as using human evaluators to check the correctness of the generated answers and the diversity of the generated questions, is highly recommended. Also, using an LVLM to generate data can introduce its own biases; for example, generated charts tend to have monotonic trends (only going upward or downward). One can observe in Figure 5 that the charts seem to have only simple trends, which is largely different from the real-world distribution. How is this addressed in the data generation pipeline? Is there any post-filtering or evaluation to ensure correctness and diversity?\n3. The benchmark features charts without annotations. However, existing benchmarks (such as ChartXiv, ChartBench, and EvoChart) also contain charts without annotations. What is the difference between the proposed benchmark and these existing works? What are the novel benefits that this benchmark brings to the community? Is the data higher quality? Does the data cover more chart types? Or is the data aligned more closely with the real-world distribution?\n4. A head-to-head comparison is needed to show the effectiveness of the proposed solution. The training of ChartVR involves an RL algorithm (GRPO), which is different from existing supervised finetuning techniques. A head-to-head comparison with previous methods on published benchmarks is necessary to support the effectiveness of the proposed method. For example, the comparison in the table is unfair: ChartVR-7B is built on Qwen 2.5VL, which is a much stronger backbone model than those used in existing approaches. A fairer comparison would involve keeping the backbone models the same as in previous work to showcase the effectiveness of both the proposed dataset and the training algorithm. Additionally, you should fix the model and data for both previous works and your model, then train using the previous SFT algorithm and your RL training to showcase the effectiveness of the proposed training strategies. Without these head-to-head comparisons, I cannot be convinced of the approach's effectiveness."}, "questions": {"value": "- How are the intermediate steps of the reasoning evaluated on ChartVRBench? I cannot find this in Section 3.2 of the evaluation protocol."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mB9XX3Qx1a", "forum": "agRuIB0EZ2", "replyto": "agRuIB0EZ2", "signatures": ["ICLR.cc/2026/Conference/Submission15518/Reviewer_zvwv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15518/Reviewer_zvwv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15518/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945976880, "cdate": 1761945976880, "tmdate": 1762925802363, "mdate": 1762925802363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}