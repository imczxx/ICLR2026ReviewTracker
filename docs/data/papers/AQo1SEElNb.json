{"id": "AQo1SEElNb", "number": 21436, "cdate": 1758317562764, "mdate": 1763733202383, "content": {"title": "Selective Rotary Position Embedding", "abstract": "Position information is essential for language modeling. In softmax transformers, Rotary Position Embeddings (\\textit{RoPE}) encode positions through \\textit{fixed-angle} rotations, while in linear transformers, order is handled via input-dependent (selective) gating that decays past key-value associations. Selectivity has generally been shown to improve language-related tasks. Inspired by this, we introduce \\textit{Selective RoPE}, an \\textit{input-dependent} rotary embedding mechanism, that generalizes \\textit{RoPE}, and enables rotation in \\textit{arbitrary angles} for both linear and softmax transformers. We show that softmax attention already performs a hidden form of these rotations on query-key pairs, uncovering an implicit positional structure. We further show that in state-space models and gated linear transformers, the real part manages forgetting while the imaginary part encodes positions through rotations. We validate our method by equipping gated transformers with \\textit{Selective RoPE}, demonstrating that its input-dependent rotations improve performance in language modeling and on difficult sequence tasks like copying, state tracking, and retrieval.", "tldr": "We introduce Selective RoPE, an input-dependent rotary embedding that enhances gated linear transformers.", "keywords": ["RoPE", "Linear Transformer", "Attention", "State Space Models", "Forget Gate"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7bad7b2fdcb5b8af1d5147d48293daf5e3737fe8.pdf", "supplementary_material": "/attachment/d5138e376c0a05c9bf5c8959fd9d6919c66d9754.zip"}, "replies": [{"content": {"summary": {"value": "The authors generalise RoPE to a mechanism allowing it to choose angles in a way that is input dependent. The authors perform analysis mainly on linear-attention models and show that selective rope seems to improve performance over baselines such as RoPE or NoPE."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The connection between SSMs, linear attention, and RoPE is interesting. I particularly liked the presentation in Table 1."}, "weaknesses": {"value": "My main area of research is in Transformers and not linear attention although I have some experience with linear attention and RFF.s \n\nI do not think I quite understand the point of \"Softmax attention implicitly applies a selective rotation, to encode relative positional information between tokens.\" Are you arguing that the rotations come from the relationship between the softmax kernel and RFF? So you can view the softmax kernel as applying RoPE but where the angles are sampled IID from a Gaussian. This however would really only be true if your angle samples tend to infinity of course. Is this how you are connecting RoPE with a \"NoPE\" softmax?\n\nI found the notation slightly hard to follow especially as someone not coming from SMMs. I mainly found confusing that RoPE is really a method used in Transformers, but the paper seems to only implemented the selective mechanism for linear attention and was not implemented for normal quadratic Transformers? Is there something stopping you from implementing for a normal Transformer? \n\nMinor\nTypo in abstract: rotation in all angels -> rotation in all angles"}, "questions": {"value": "Please see questions in the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZDa85mPL54", "forum": "AQo1SEElNb", "replyto": "AQo1SEElNb", "signatures": ["ICLR.cc/2026/Conference/Submission21436/Reviewer_Rgdu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21436/Reviewer_Rgdu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741659188, "cdate": 1761741659188, "tmdate": 1762941772689, "mdate": 1762941772689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a version of RoPE with learned, input-dependent arbitrary rotations. Theoretical analysis provided indicates that softmax attention implicitly performs selective rotations, motiving the proposed architecture. Selective RoPE uses a learned linear projection and cumulative sum to produce input-dependent rotations. An analysis of diagonal SSMs is provided which shows distinct roles for the real and imaginary parts of the state matrix, motiving the incorporation of Selective RoPE with GLA to provide better memory. Experiments on language modeling and show improvements with Selective RoPE compared to RoPE and softmax attention in GLA models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper gives a detailed theoretical justification for the architecture design. The analyses of softmax attention as implicit rotation and spectral leakage in SSMs may be useful to future work. Experimental evidence is provided to support claims."}, "weaknesses": {"value": "The conclusion that softmax attention applies implicit selective rotation is based on the RFF approximation and additional normalization assumptions. The paper does not prove that the resultant normalized approximation converges in the limit to softmax attention, and so this analysis may be overstating the connection. \n\n\nThe real-data language modeling results in table 3 omit the RoPE condition."}, "questions": {"value": "Did you compute RoPE setting for Table 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6H5GN7zbKm", "forum": "AQo1SEElNb", "replyto": "AQo1SEElNb", "signatures": ["ICLR.cc/2026/Conference/Submission21436/Reviewer_uVEP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21436/Reviewer_uVEP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942578342, "cdate": 1761942578342, "tmdate": 1762941771779, "mdate": 1762941771779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a selective Rotary Position Embedding (RoPE) that uses an input-dependent rotation to enhance the performance of models with Gated Linear Attention. The paper provides a theoretical analysis of how softmax attention performs a hidden form of rotation and further proposes to determine the rotation angle via a linear projection of the query. Experiments are conducted on GLA showing that selective RoPE achieves better performance than NoPE and RoPE."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is clear with summarized insights and clear figures.\n\n* The analyses on the implicit rotation of softmax attention are interesting.\n\n* The paper provides an in-depth analysis from the RFF perspective."}, "weaknesses": {"value": "* While the paper takes a lot of effort in the derivation of implicit selective rotation in softmax attention, the proposed method is applied to gated linear attention. Given that the derivation heavily relies on the Random Fourier Features (RFF) approximation, the practical impact of the proposed method has not been validated.\n\n* Limited experiments. The experiments are conducted on small-scale models, which raises my concern about the stability and scalability of the proposed method. The leaned rotation angle may lead to unstable training."}, "questions": {"value": "* It would be appreciated if the authors could provide additional experimental results of applying selective RoPE to softmax attention with more details on the experiments.\n\n* Since rotations are composable. Can the proposed method be equivalently viewed as applying a rotation to the queries and keys before the RoPE operation? What is the significance of doing so?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QLlAawBCMK", "forum": "AQo1SEElNb", "replyto": "AQo1SEElNb", "signatures": ["ICLR.cc/2026/Conference/Submission21436/Reviewer_zi2k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21436/Reviewer_zi2k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975995872, "cdate": 1761975995872, "tmdate": 1762941771231, "mdate": 1762941771231, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Selective Rotary Position Embedding (Selective RoPE), an input-dependent mechanism designed to generalize standard Rotary Position Embeddings (RoPE) by performing rotations at arbitrary, selective frequencies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Interesting theoretical insights, such as:\n- Softmax attention implicitly applies a selective rotation, to encode relative positional information between tokens\n- Linear Transformers can be enhanced by using both forgetting via real decay and rotation via imaginary gate."}, "weaknesses": {"value": "Very limited evaluation for language modeling.  Would be good to have at least RoPE as baseline (Table 3) and evaluate Selective RoPE in different settings (i.e context length)"}, "questions": {"value": "Do you expect GLA  and Softmax Transformers to benefit equally from Selective RoPE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "h6PRb8jVaZ", "forum": "AQo1SEElNb", "replyto": "AQo1SEElNb", "signatures": ["ICLR.cc/2026/Conference/Submission21436/Reviewer_NYgW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21436/Reviewer_NYgW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993354647, "cdate": 1761993354647, "tmdate": 1762941770709, "mdate": 1762941770709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to the Reviewers"}, "comment": {"value": "## General Response\n\nWe thank all the reviewers for their constructive feedback on our manuscript. As requested by several reviewers, we have extensively expanded our language-modeling experiments by including Gated DeltaNet (GDN) and the Softmax Transformer (w/ Decay). We compare Selective RoPE against RoPE and No Position Embedding across all models—GLA, GDN, and the Transformer w/ Decay (FoX)—and show that Selective RoPE consistently and significantly boosts performance for all models. \n\n|Model|LMB.(ppl↓)|LMB.(acc↑)|PIQA(acc↑)|Hella.(acc_n↑)|Wino.(acc↑)|ARC-e(acc↑)|ARC-c(acc_n↑)|Avg.|\n|-|-|-|-|-|-|-|-|-|\n|**GLA**|||||||||\n|NoPE|*23.15*|**39.4**|*69.7*|**48.0**|53.1|*50.9*|24.6|*47.6*|\n|RoPE|23.96|36.1|*69.7*|47.7|**54.0**|*50.9*|*25.1*|47.2|\n|Selective RoPE|**21.16**|*37.4*|**70.6**|*47.9*|*53.9*|**52.0**|**26.2**|**48.0**|\n|**Gated DeltaNet**|||||||||\n|NoPE|22.50|37.2|**70.9**|*47.6*|53.2|*52.0*|**25.9**|47.8|\n|RoPE|*20.84*|*38.9*|*70.7*|**48.2**|*53.4*|51.3|25.1|*48.0*|\n|Selective RoPE|**19.28**|**39.4**|70.1|*47.6*|**54.9**|**52.4**|*25.4*|**48.3**|\n|**Transformer** (w/ Decay)||||||||\n|NoPE|26.04|37.4|*69.6*|47.0|**55.2**|50.7|*25.8*|47.6|\n|RoPE|*23.16*|*37.7*|69.5|*47.6*|*55.0*|**52.7**|25.3|*48.0*|\n|Selective RoPE|**21.89**|**38.2**|**70.2**|**47.8**|54.1|*52.4*|**26.1**|**48.1**|\n\nMoreover, we have included several new experimental results: \n\n- An **ablation of the additional architectural components introduced in Selective RoPE** (phase gate and bias) on the MAD benchmark, showing that the core Selective RoPE mechanism already achieves  gains over NoPE/RoPE and that the variant with both phase gate and bias attains the best overall MAD average while preserving improvements across all MAD tasks.\n- A corresponding ablation of these components in the language-modeling setup, where we systematically vary the presence of the phase gate and bias for GLA, GDN, and the Transformer w/ Decay (FoX in the manuscript); this confirms that Selective RoPE is robust across architectures, that the phase gate mainly helps optimization stability and downstream accuracy, and that adding only a bias does not yield consistent additional gains.\n- An **efficient Triton implementation of Selective RoPE**, demonstrating that our fused kernel is almost as fast as RoPE/NoPE in prefill throughput and achieves up to a 3.4× speedup over the PyTorch-compile implementation at long sequence lengths, thereby addressing concerns about the runtime overhead of Selective RoPE. We will publish our implementation after the closure of the review phase.\n\n\nWe would also like to note that we have considerably improved the writing and general presentation of the manuscript. The content of the paper has remained unchanged but the readability has improved significantly. The primary changes are: \n\n1. Consolidating the motivation for Selective RoPE and the description of the method in Section 3 (before: Sections 3 through 5). \n2. Moving implementation details into their own subsection in the prelude to the experiments in Section 4.\n3. Addition of a related work section (Section 5)"}}, "id": "e6zc4CnQsz", "forum": "AQo1SEElNb", "replyto": "AQo1SEElNb", "signatures": ["ICLR.cc/2026/Conference/Submission21436/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21436/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21436/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763732691200, "cdate": 1763732691200, "tmdate": 1763733905968, "mdate": 1763733905968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}