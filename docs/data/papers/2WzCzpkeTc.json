{"id": "2WzCzpkeTc", "number": 11414, "cdate": 1758198498568, "mdate": 1759897577096, "content": {"title": "CaliDrop: KV Cache Compression with Query-based Calibration", "abstract": "Large Language Models (LLMs) require substantial computational resources during generation. While the Key-Value (KV) cache significantly accelerates this process by storing attention intermediates, its memory footprint grows linearly with sequence length, batch size, and model size, creating a bottleneck in long-context scenarios. Various KV cache compression techniques, including token eviction, quantization, and low-rank projection, have been proposed to mitigate this bottleneck, often complementing each other.\nThis paper focuses on enhancing token eviction strategies.\nToken eviction leverages the observation that the attention patterns are often sparse, allowing for the removal of less critical KV entries to save memory. However, this reduction usually comes at the cost of notable accuracy degradation, particularly under high compression ratios. \nTo address this issue, we propose CaliDrop, a novel strategy that enhances token eviction through calibration. Our preliminary experiments show that queries at nearby positions exhibit high similarity. Building on this observation, CaliDrop performs speculative calibration on the discarded tokens to mitigate the accuracy loss caused by token eviction.\nExtensive experiments demonstrate that CaliDrop significantly improves the accuracy of existing token eviction methods.", "tldr": "", "keywords": ["LLM", "KV cache compression", "low-resource", "token eviction"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d2ca0c88ea8ae9820d1913c0766766ed3a42c18.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces an incremental calibration strategy for token-wise KV cache eviction. It leverages the attention outputs of previous, similar queries to calibrate the attention results derived from the evicted KV cache, while selectively recomputing when necessary. This approach effectively reduces the error introduced by token eviction and improves the accuracy of existing token-wise KV cache compression methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* **Novel Insight and Theoretical Foundation.** This work identifies that queries at nearby positions produce similar attention outputs, enabling the use of historical queries and attention results to calibrate future outputs under a pruned KV Cache. It further provides a formalized attention decomposition theorem and proof, offering a solid theoretical grounding for the proposed calibration mechanism.\n\n* **Generalizable and Plug-and-Play.** The proposed idea is simple yet versatile, and can be seamlessly integrated into many existing token-wise KV Cache eviction methods—including StreamingLLM, H2O, and SnapKV, serving as a training-free add-on that consistently enhances accuracy."}, "weaknesses": {"value": "* **Lack of Ablation on Calibration vs. Recomputation.** Besides the proposed calibration mechanism, the implementation also performs recomputation approximately every eight decoding steps using the evicted and offloaded KV caches, which contributes notably to the accuracy improvement. However, the paper does not separately analyze the impact of calibration and recomputation on attention error and end-to-end accuracy. An ablation study is needed to isolate the effect of the calibration mechanism itself and validate its claimed contribution.\n\n*  **Limited Performance and Efficiency Analysis.** CaliDrop’s behavior depends on semantic query similarity, dynamically deciding whether to recompute, calibrate, or skip each step. Yet, the performance evaluation is conducted only on fixed 1024-token inputs and 128-token outputs, without reporting the ratio or cost of these different branches. The study should include more diverse, real-world prompts (e.g., ShareGPT [1]) and provide a breakdown of latency metrics such as TTFT and TPOT, to better quantify the runtime overhead of recomputation and calibration during both prefill and decoding phases.\n\n[1] shareAI. (2023). shareGPT-Chinese-English-90k Bilingual Human-Machine QA Dataset. Hugging Face Repository. Retrieved from https://huggingface.co/datasets/shareAI/ShareGPT-Chinese-English-90k"}, "questions": {"value": "* Regarding the proposed CaliDrop as a token-wise eviction method, recent studies such as RazorAttention [2], DuoAttention [3], and HeadKV [4] have explored head-wise KV cache compression. It would be valuable to discuss whether the proposed calibration mechanism can adapt to head-wise eviction, and how the implementation overhead might increase as the eviction granularity becomes finer.\n\n\n\n[2] Tang, Hanlin, et al. \"Razorattention: Efficient kv cache compression through retrieval heads.\" *arXiv preprint arXiv:2407.15891* (2024).\n\n[3] Xiao, Guangxuan, et al. \"Duoattention: Efficient long-context llm inference with retrieval and streaming heads.\" *arXiv preprint arXiv:2410.10819* (2024).\n\n[4] Fu, Yu, et al. \"Not all heads matter: A head-level kv cache compression method with integrated retrieval and reasoning.\" *arXiv preprint arXiv:2410.19258* (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2F27Qinfq0", "forum": "2WzCzpkeTc", "replyto": "2WzCzpkeTc", "signatures": ["ICLR.cc/2026/Conference/Submission11414/Reviewer_dEEK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11414/Reviewer_dEEK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761206191816, "cdate": 1761206191816, "tmdate": 1762922532044, "mdate": 1762922532044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CaliDrop, a query-based calibration strategy for KV cache compression in large language models (LLMs). Existing token eviction methods reduce memory but harm accuracy under high compression. CaliDrop leverages the high similarity between nearby queries to estimate the contribution of evicted tokens through calibrated attention recomputation, thereby recovering lost accuracy. Experiments on LongBench, RULER, and Needle-in-a-Haystack benchmarks show consistent accuracy gains across Mistral-7B and LLaMA-3 models, with minimal throughput cost. Overall, the work presents a simple yet effective improvement for token eviction–based cache compression."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "•\tCaliDrop introduces a query-level calibration that compensates for evicted tokens using nearby historical queries, supported by the “attention decomposition theorem” (Eq. 1–2) and L1-loss reduction evidence in Fig. 1 (Sec. 3.1.3).\n•\tExperiments cover multiple models (Mistral-7B, LLaMA-3-8B/70B) and benchmarks (Tables 1–2), showing consistent performance gains across KV sizes 64–512 (Secs. 4.2–5.1). This breadth supports the robustness and generality of the method.\n•\tThe calibration mechanism requires only lightweight recomputation (every ≈8 steps; Fig. 3b) and maintains comparable throughput to SnapKV (Fig. 3a), highlighting its applicability in long-context inference."}, "weaknesses": {"value": "•\tCaliDrop is applied only in the prefilling phase (Sec. 4.1.2); no evidence is provided for dynamic or streaming decoding. Memory overhead from offloaded KV caches and detailed latency breakdowns are also missing (Secs. 5.2–5.3).\n•\tBeyond the exploration of $\\theta_{1}$ and $\\theta_{2}$ (Table 2), the paper provides limited investigation into other critical factors such as calibration size, per-layer contribution, or offload-cache management. Moreover, the absence of statistical validation (e.g., variance or significance testing) makes it difficult to assess the robustness of the reported improvements.\n•\tThe approach extends existing token-eviction techniques through a supplementary calibration step but does not establish a new compression framework. The theoretical analysis mainly reiterates standard properties of attention mechanisms (Secs. 3.1–3.2) without offering new learning formulations."}, "questions": {"value": "1.\tCould the authors provide per-layer or per-head ablations to analyze where calibration contributes most across transformer depth?\n2.\tHow does CaliDrop perform in real-time or streaming decoding settings, where query similarity varies more rapidly?\n3.\tWhat is the quantitative GPU-memory overhead of storing offloaded KV caches for calibration at different sequence lengths?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Nhdpw0XMrP", "forum": "2WzCzpkeTc", "replyto": "2WzCzpkeTc", "signatures": ["ICLR.cc/2026/Conference/Submission11414/Reviewer_yoKp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11414/Reviewer_yoKp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912238085, "cdate": 1761912238085, "tmdate": 1762922531617, "mdate": 1762922531617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies the two fundamental limitations in the existing KV cache compression methods: 1) discarding tokens that can become crucial later, and 2) the accumulated effect of discarding tokens is overlooked. To this end, the paper proposes CaliDrop, which compensates for evicted tokens by recomputing attention outputs for queries at nearby positions, alleviating memory pressure while maintaining model accuracy. The experimental results show that CaliDrop can be applied to different KV cache compression methods and improve their performance while introducing little computation overhead."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The method is well motivated, and the paper is well written.\nThe two observations are interesting: queries at nearby positions are similar, and the historical attention outputs can be used to predict future attention outputs.\nThe experiments on different KV cache compression methods and models across various tasks demonstrate the effectiveness of the proposed method.\nThe analysis of throughput and recomputation frequency shows the efficiency of the proposed method."}, "weaknesses": {"value": "The hyperparameters $\\theta_1$ and $\\theta_2$ require manual tuning and may have different optimal values in different tasks.\nThe recomputation introduces a memory peak. What are the possible impacts of it, e.g., what is the maximum length of context/evict-KV CaliDrop can handle?\nThe recomputation cost and frequency may increase in larger models. It would be better to include the throughput and recomputation frequency in larger models."}, "questions": {"value": "see in weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SIXOrujOqD", "forum": "2WzCzpkeTc", "replyto": "2WzCzpkeTc", "signatures": ["ICLR.cc/2026/Conference/Submission11414/Reviewer_ygNM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11414/Reviewer_ygNM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978677523, "cdate": 1761978677523, "tmdate": 1762922529549, "mdate": 1762922529549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a technique that complements KV cache eviction techniques and improves their ability to retain information from evicted KV entries. The method consists in offloading the evicted KV cache, and using a historical query that helps trigger a calibration step where the attention output is adjusted based on the evicted cache.\nThe authors conduct extensive experiments on the LongBench and RULER datasets for several models of 7-8B and 70B parameters. They also explore threshold parameter choices and discuss the impact of their method on latency and accuracy.\nOverall, this paper copes with the important question of LLM efficiency and tackles the problem of lost information in token eviction methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is straightforward and sensible. It is well-presented and the long-context experiments are well-designed. It is also sound from a theoretical point of view.\n- The method can complement any KV cache eviction method and shows noticeable benefits for every tested method (Streaming-LLM, H2O, SnapKV).\n- The method is not very sensitive to threshold choices according to Table 2, which could have been a concern in such threshold-based methods."}, "weaknesses": {"value": "I have concerns about the practical efficiency of the method that are not addressed in the current state of the paper. I also think that the experiments do not cover more extreme use cases where KV cache compression is crucial.\n- **Doubts about practical efficiency**: The CaliDrop method relies on offloading and reloading past KV entries. It is not clear if the KV entries are offloaded to disk (in which case the computational overhead may be heavy for long sequences) or to CPU (in which case the CPU RAM can be saturated for long sequences).  These questions and the overhead they imply are not handled properly in Section 5, which only reports the throughput in relatively short-sequence setups (1024 with 128 KV cache budget), and shows how latency scales with batch size. It would have been more relevant to see the effect of sequence lengths and compression ratios on both memory (offloaded and on GPU) usage and latency. My main concerns are 1- that the latency gains may decrease with longer sequences as more KV items need to be offloaded, reloaded and the corresponding attention map needs to be computed for each recomputation step; 2 - that the VRAM can be saturated earlier than with the raw compression methods because the recomputation steps are dependent in the total sequence length. It would be insightful to at least report latency and memory usage statistics in the benchmark evaluations to show the overhead that is traded for better performance with CaliDrop.\n- **Lack of long-context experiments**: In its current state, the paper lacks a discussion of the evolution performance gains when increasing sequence length. The NIAH results in Figure 2 are only conducted with an 8K context length when similar experiments are usually conducted with 32k to 128k context lengths. A study of perplexity evolution for long sequences similar to what is done in Devoto et. al could also be relevant."}, "questions": {"value": "- What is the impact of offloading on RAM usage and how does it scale as sequence length increases?\n- Did you try your method with metrics other than cosine similarity for query comparison? Is the query taken before or after positional encoding?\n- The direct role of \\theta_1 on latency is not exposed in experiments. What is the empirical impact of \\theta_1 on memory usage and latency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kSZdAWdKjg", "forum": "2WzCzpkeTc", "replyto": "2WzCzpkeTc", "signatures": ["ICLR.cc/2026/Conference/Submission11414/Reviewer_nBZu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11414/Reviewer_nBZu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015527248, "cdate": 1762015527248, "tmdate": 1762922528785, "mdate": 1762922528785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}