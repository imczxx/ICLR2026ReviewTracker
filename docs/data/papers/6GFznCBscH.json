{"id": "6GFznCBscH", "number": 4063, "cdate": 1757594283411, "mdate": 1759898054972, "content": {"title": "Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language", "abstract": "Automated interpretability aims to translate large language model (LLM) features into human understandable descriptions. However, these natural language feature descriptions are often vague, inconsistent, and require manual relabeling. In response, we introduce *semantic regular expressions*, a structured language for describing LLM features. By combining primitives that capture linguistic and semantic feature patterns with modifiers for contextualization, composition, and quantification, semantic regular expressions produce precise and expressive feature descriptions. Across quantitative benchmarks and qualitative analyses, we find that semantic regexes match the accuracy of natural language while yielding more concise and consistent feature descriptions. Moreover, their inherent structure affords new types of analyses, including quantifying feature complexity across layers, scaling automated interpretability from insights into individual features to model-wide patterns. Finally, user studies show that semantic regexes improve interpreters’ ability to understand and reason about LLM features.", "tldr": "We develop a structured language to describe LLM features, resulting in accurate, concise, and consistent descriptions that inherently describe each feature's level of abstraction and help humans reason about LLMs.", "keywords": ["automated interpretability", "LLM features", "structured languages"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67c8282470356f6b9e67608ed35b83217e5de045.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose semantic-regex, a structured language framework, aimed to improve expressiveness and clarity of feature description protocols. This aims to address the typically verbose natural language output of feature description methods. The authors use the Neuropedia suite for extracting feature descriptions and the FADE framework for evaluation and find that semantic-regex perform broadly as well as natural language feature description methods, while sometimes being of higher clarity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Using structured language for re-framing the problem of feature description is an interesting idea, especially for improving clarity (as also supported by some of the evaluations).\n- The methods are clearly defined and appear to be reproducible with reasonable efforts. \n- The human user study provides a complementary contribution, highlighting differences in scoring between the max-acts baseline and semantic-regex."}, "weaknesses": {"value": "- The framework appears to work with binary assessments of feature presence, i.e. the semantic regex indicated presence/absence of a feature description. But, simple pattern matching is insufficient to capture some of the diverse patterns learned by today’s foundation models, e.g. nonregular language.\n- A clear presentation and discussion of limitations is missing. Specifically, the quantitative experiments (Fig. 2 and Fig. 5 of the human user study) remained vague regarding the differences and benefits/disadvantages that semantic-regex would bring compared to previous natural language-based approaches. What are the limits of semantic regex, i.e., in the context of polysemantic feature descriptions, or features encoding pragmatics, see [Kop25].\n- The paper does not effectively address how the necessary semantic regex language should be developed, how to agree on primitives and grammatical structure? \n- Limited model ablations are included, e.g. what would be the effect of another explainer model (so far only gpt-4o-mini is included) or another evaluator model (also gpt-4o-mini is used). This may be a poor experimental choice as the explainer and evaluation are the same (closed source) model. \n- Overall, the contribution is limited in scope to reformulating existing methods to using a specific language format (semantic regex) for defining and communicating feature descriptions. The paper does not provide a novel method for detecting feature descriptions and reuses the methods defined and implemented via Neuronpedia [Lin, 2023; Bills et al. (2023)]. The proposed method should thus be seen as an extension to these methods.\n\n**Refs:**\n\n[Kop25] Kopf, L., Feldhus, N., Bykov, K., Bommer, P. L., Hedström, A., Höhne, M. M. C., & Eberle, O. (2025). Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework. arXiv preprint arXiv:2506.15538."}, "questions": {"value": "**Questions**\n- How much of a challenge is the explainer’s ability to parse regex correctly? \n- The used evaluation framework FADE (Puri et al., 2025), relies on generating synthetic control samples, e.g. for the faithfulness metric. \n- How well can models generate synthetic data from semantic regex feature description compared to natural language ones? Here a comparative investigation would allow revealing the advantages and limits of the proposed framework.\n\n\n**Typos**\n- “regexes as as” (l. 61) -> “are as”\n- “has found structure in the concepts features represent.” (l.90)\n- “that semantic regexes make activation patterns explicit and express them via example.” (l. 424)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ltuOsT9ucp", "forum": "6GFznCBscH", "replyto": "6GFznCBscH", "signatures": ["ICLR.cc/2026/Conference/Submission4063/Reviewer_riWE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4063/Reviewer_riWE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579963132, "cdate": 1761579963132, "tmdate": 1762917160337, "mdate": 1762917160337, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to explain features with semantic regular expresssions, which can provide more consistant,ni concise, and precise explanations compared to ambigious natural language explanations. I think this is a solid work and provides a practical tool for managing and standardizing feature descriptions. While I find the core contribution strong, there are some limitations in validation and minor issues to address."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-structured and clearly written, which is very easy to follow. The methods used in this paper are straightforward as well.\n\n- The categorization of regex components is clean and comprehensive. The progression from [:symbol:] (exact matches) to [:lexeme:] (syntactic variants) to [:field:] (semantic variants), combined with context modifiers, provides appropriate levels of abstraction for describing features.\n\n- The experimental validation is thorough, incorporating multiple automated metrics (detection, fuzzing, clarity, responsiveness, purity, faithfulness) across different models (GPT-2, Gemma-2) and feature sets. The results effectively demonstrate improved precision and consistency at both syntactic and semantic levels."}, "weaknesses": {"value": "- Human sanity check only involves 12 features. This raises questions about the method's reliability across all features.\n- Lacks discussion of failur models where semantic regexes might fail."}, "questions": {"value": "- L61, typo \"as as accurate as\"\n- Is the example for quantifiers in Figure 1 correct? Should \"it is a da isy\" be \"it is a [color] daisy\"\n- L242, missing \"measures\" in the sentence?\n- How were the 12 \"accurate\" features selected for the user study? What percentage of automatically generated descriptions were deemed accurate? This information is crucial for assessing the method's practical reliability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ehmwJGk1IX", "forum": "6GFznCBscH", "replyto": "6GFznCBscH", "signatures": ["ICLR.cc/2026/Conference/Submission4063/Reviewer_CRyN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4063/Reviewer_CRyN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866347949, "cdate": 1761866347949, "tmdate": 1762917160134, "mdate": 1762917160134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces semantic regexes, a structured language for describing LLM features. The authors demonstrate through experiments that semantic regexes perform on par with natural language feature descriptions while being more concise, consistent, and able to reflect feature complexity. A user study shows that semantic regexes benefit interpreters’ ability to understand and reason about LLM features."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper studies an interesting task of generating language descriptions for LLM features. The paper provides concrete examples to illustrate the methodology and experimental results. The implementation and experimental settings have been presented in detail. The insights presented in Section 5.3 sound intriguing."}, "weaknesses": {"value": "There seems to be a lack of interesting insights apart from Section 5.3. The facts that regexes are more concise and consistent than natural languages seem quite obvious given that regexes are a more restricted form of languages.\n\nThe experiments can be enhanced by evaluating more datasets and LLMs. It is unclear why the evaluator model, gpt-4o-mini, is the same as the explainer model since this may introduce bias. Would it be better to switch to a larger model like gpt-4o?\n\nThe population size of the user study, 24, seems too small for drawing conclusions."}, "questions": {"value": "What is the rationale for using GPT-4o-mini as the evaluator model?\n\nHow many repeated runs have been conducted in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "esW2axyjl9", "forum": "6GFznCBscH", "replyto": "6GFznCBscH", "signatures": ["ICLR.cc/2026/Conference/Submission4063/Reviewer_YyVu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4063/Reviewer_YyVu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4063/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975242710, "cdate": 1761975242710, "tmdate": 1762917159910, "mdate": 1762917159910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}