{"id": "HtMt9XNZv6", "number": 11617, "cdate": 1758202519992, "mdate": 1759897564521, "content": {"title": "Transfer Bound of Graph Convolutional Networks across Arbitrary Sparsity", "abstract": "Size transfer methods in Graph Convolutional Networks (GCNs) is a common treatment to mitigate the high cost of training on large graphs, by transferring the model trained on randomly sampled smaller graphs. However, the theoretical guarantee of such transfer has only been proved in previous studies for random graphs with fixed sparsity. In practice, downsampled real-world graphs may exhibit multiple sparsity regimes. To fully understand the theoretical performance across arbitrary sparsity, we establish the GCN transferability bound by introducing Stretched Graphon Convolutional Networks (SWNNs) based on the recent generalized graphon model. The bound decomposes into error components arising from expected edge density and graph size, which jointly determine the sparsity. Experiments on real-world networks validate our theoretical findings.", "tldr": "We establish a transfer bound for GCNs across arbitrary sparsity.", "keywords": ["Graph convolutional networks", "sparse graphs", "convergence", "transfer learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/01184634517c0b158cb71ce495e64466e72e5c93.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper develops a unified theoretical framework for sparse Graph Convolutional Networks by introducing the \"stretched graphon\" model. Addressing the limitation that classical graphons vanish in sparse limits, this framework rescales sparse graphs to ensure convergence to a stable, non-zero representation. The key contribution is a Transferability Theorem proving that GCNs can transfer across arbitrary sparsity levels, extending prior work limited to fixed sparsity. The authors formally establish a double-convergence framework for asymptotic consistency and empirically validate their findings on the Cora dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "## Stretched Graphon Framework\n\nThe paper generalizes the classical dense graphon limit to a stretched graphon framework that remains non-degenerate for sparse graphs. This is a novel theoretical contribution that conceptually bridges two previously disconnected regimes, dense and sparse graphs, under a single mathematical limit. Although there are many mathematical definitions, the writing is dense but logically coherent, with intuitive illustrations.\n\n## Formal Convergence Analysis\n\nThe paper provides formal convergence proofs for both sparse graphon sequences and random graph sequences, then integrates them to derive the transferability theorem. This framework can serve as a theoretical foundation for studying generalization, transfer, and stability of GNNs on sparse and heterogeneous networks."}, "weaknesses": {"value": "## Experimental Scope Limitations\n\nThe experiments only use one dataset (Cora), which is a small citation network with 2,708 nodes and a fixed label set. Given the paper's theoretical ambition to establish transferability across arbitrary sparsity, validation on a single graph is insufficient. The authors should include larger and structurally diverse graphs such as PubMed, OGB datasets, or synthetic graphon-generated graphs to demonstrate that the observed convergence trends generalize beyond one topology. In particular, testing on graphs with heterogeneous degree distributions (power-law networks) or directed and weighted edges would better demonstrate the robustness of the stretching mechanism and support the universality claims.\n\n## Limited Model Diversity and Ablation Studies\n\nCurrently, only standard GCNs with 2–3 layers and 32/64 hidden units are tested. Since SWNN is presented as a general limit model, it would be valuable to test whether GCN variants such as GraphSAGE and GAT also exhibit similar stretched transfer behavior. This would provide empirical confirmation of the framework's universality and show that the theoretical insights apply beyond the specific GCN architecture tested.\n\n## Constructive Suggestions for Improvement\n\n- Extend evaluation to multiple datasets and include synthetic sparse graphons for controlled experimentation\n- Test robustness across different GNN architectures and normalization variants\n- Include larger-scale datasets and graphs with diverse structural properties\n\n## Overall Assessment\n\nThe paper's theoretical contribution is strong and original, but the experimental validation is narrow and only partially demonstrates the claimed universality. Expanding the empirical scope and quantitatively aligning results with theory would substantially strengthen the work."}, "questions": {"value": "1. Could the same convergence and transferability arguments apply to transformer-style graph networks?\n2. Could you elaborate more intuitively on why stretching by the L¹-norm specifically restores a non-zero limit for sparse graphons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YWtbrRDYQG", "forum": "HtMt9XNZv6", "replyto": "HtMt9XNZv6", "signatures": ["ICLR.cc/2026/Conference/Submission11617/Reviewer_vGFE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11617/Reviewer_vGFE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619270994, "cdate": 1761619270994, "tmdate": 1762922692136, "mdate": 1762922692136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a well known phenomena en GNNs which is the transferability of of GNNs between graphs of different sizes. In particular, this work focuses on the Stretch Graphon to be able to deal with graphs of different sparcity. All things considered, the paper is a minor extension of existing works, and the contribution (if any) is incremental."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper studies a relevant work which is the transferability of GNNs. I believe the paper is well written and easy to follow. \n\nI believe the authors do make a comprehensive work citing all relevant papers."}, "weaknesses": {"value": "I fail to understand the motivation and utility of the work. Most of the resutls are well-known and fully understood. And it seems to me that the only contribution of the work is the introduction of the Stretched graphon, which is a minor contribution to the existing graphon works. To make matters worse, the controlled sparsity is a global phenomena, and is not well suited to study graphs such as citation networks with varying degrees. The theoretical results are very difficult to interpret. And the numerical results are very noisy."}, "questions": {"value": "- How is this work fundamentally different and not incremental from all the existing works on the topic?\n\n- Can the authors properly explain or at least attempt to explain Theorem 1 in a way that is interpretable? \n\n- The authors say in line 352: \n\nHere, we guarantee transferability across graph subsequences with arbitrary sparsity, while prior\nworks (Keriven et al., 2020; 2021; Wang et al., 2023; 2024) analyze GCN only under fixed sparsity\nsettings. . In our framework, transfer errors vanish as both tm, tM and n, N increase to infinity, while\nthe convergence errors in Ruiz et al. (2024) converge to constants.\"\n\nBut this seems to be a very incremental work compared to Ruiz's previous works. How is this result fundamentally different? It seems to me that the work is incremental."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i8jF8wX9Wv", "forum": "HtMt9XNZv6", "replyto": "HtMt9XNZv6", "signatures": ["ICLR.cc/2026/Conference/Submission11617/Reviewer_zEqV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11617/Reviewer_zEqV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946851771, "cdate": 1761946851771, "tmdate": 1762922691714, "mdate": 1762922691714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel treatment of GNN transferability across sparsity regimes. It introduces a method for generating sparse graphon sequences through truncation and rescaling and analyzes transferability bounds in this setting. The approach is conceptually clear and connects naturally with the interpretation of sparse graphs as zero-graphon limits. However, there are several weaknesses that need to be addressed prior to publication."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Presents a novel treatment of GNN transferability **across sparsity regimes**.\n- The generation of the sparse graphon sequence via truncation and rescaling is clear and intuitive, with helpful illustrations.\n- The interpretation of the $L_1$-norm of the classical graphon sequence tending to zero is coherent with sparse graphs having the zero graphon as their limit."}, "weaknesses": {"value": "- Under sparsity regimes different from those imposed in Lemmas 2–4, asymptotic convergence does not necessarily hold. As $n$ and $N$ tend to infinity, graphs do not converge unless additional assumptions are made on the corresponding graphon sequences. This is expected and not a limitation per se, but the paper slightly overstates its contribution by suggesting that transfer is possible across any two graphs associated with the same graphon regardless of such assumptions. This limitation should be highlighted, and the claims softened accordingly.\n\n- In eqn. (7), the distance under which convergence holds should be specified. It is unclear in what sense the limits are taken, which relates to and helps explain the point above.\n\n- I did not check the proofs in full, but I believe the constant terms related to $\\mathcal{H}$ in the transferability theorem depend on the filter order $r$ and the GNN depth. This can lead to loose bounds for deeper architectures, especially given that, under weight regularization, learned weights are typically small. This is why Ruiz et al. advocate analyses leveraging the Lipschitz continuity of convolutions in the spectral domain. Those are more involved when considering polynomials over the positive reals but are well-motivated under truncation. Did you consider this perspective?\n\n- The above also explains why the bounds in Ruiz et al. do not vanish: they can be made to vanish only if the filters’ Lipschitz constant tends to zero near small eigenvalues. Therefore, contrasting your result with Ruiz’s under this lens does not reflect an “advantage.”\n\n- Another limitation not emphasized enough is that the tails of $X$ and $W$ must vanish; otherwise, the bound does not vanish.\n\n- It is somewhat counterintuitive that, for fixed $n$ and $N$, the bound decreases with truncation length. Increasing truncation effectively reduces the density associated with the desirable convergence properties of graphons. Please expand on this. In particular, is this behavior an artifact of approaching the zero (trivial) graphon as truncation increases?\n\n- The purpose of Sections 5.2 and 5.3 is unclear. They appear to introduce supporting results needed for the main theorem. If so, indicate this explicitly—specifically, that the conditions they impose are requirements for transfer, ensuring compatibility between sparsity and node growth.\n\n- The experiments are weak. Why is performance evaluated only at node 1358 instead of over the full graph? Please extend results to the full graph. Regarding the sparsest case, where high transferability is attributed to sampling skewed toward high-degree nodes—did you check for class imbalance? A possible improvement would be to reproduce these experiments in a synthetic setting (e.g., node classification on stochastic block models) to avoid such issues and to better illustrate the theoretical findings.\n\n- Overall, the paper should better discuss the limitations of its framework and more clearly position its contribution relative to existing work. Specifically, in the related work on transfer in sparse graphs, the authors should include\nRoddenberry et al. (TSP 2023), Alimohammadi et al. (ISIT 2025), and Le and Jegelka (NeurIPS 2024)."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JXQfZkbSF9", "forum": "HtMt9XNZv6", "replyto": "HtMt9XNZv6", "signatures": ["ICLR.cc/2026/Conference/Submission11617/Reviewer_cDY7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11617/Reviewer_cDY7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045726464, "cdate": 1762045726464, "tmdate": 1762922690856, "mdate": 1762922690856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper looks at transferability of graph convolutional networks, focusing on graphs with varying degree of sparsity. They introduced stretched Graphon Convolutional Networks (sWNN) based on extended graphons."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's math is correct as far as the reviewer can check, with good illustrations and is relatively easy to follow. Overall, the technical contribution of the paper is substantial. The approach used in the paper can deal with very sparse graphs, which is both practical and theoretically challenging."}, "weaknesses": {"value": "The paper largest weaknesses are its motivation highlighted by certain key misconceptions:\n1. Sparsity and big-O notation: when discussing sparsity of graphs in the literature, most authors (that are cited in the paper) refer to sparsity of a **graph sequence** and therefore wrote the level in asymptotic notations (e.g. $\\Theta(\\log n)$ average degree). This means that all of their results are valid for graph sequences whose degrees are constant factors away from one another. This include Keriven, Bietti and Vaiter's \"Convergence and stability of graph convolutional networks on large random graphs\" and \"On the universality of graph neural networks on large random graphs\"; as well as Wang, Ruiz and Ribeiro's \"Convergence of graph neural networks on relatively sparse graphs\" and \"Geometric graph filters and neural networks: Limit properties and discriminability trade-offs\". These works were cited repeatedly to motivate the current paper's application to various sparse graph regimes (e.g. line 353  \"while prior works (Keriven et al., 2020; 2021; Wang et al., 2023; 2024) analyze GCN only under fixed sparsity settings.\"), which is incorrect. By definition, a function g being in $\\Theta(f(n))$ simply means that there are constants c and C such that there exists a N, where $cf(n) \\leq g(n) \\leq Cf(n)$ for all n > N.\n2. Generalized graphon (Definition 1) are known in the literature as (a part of) graphex (e.g. Definition 4 in Borgs, Chayes, Dhara and Sen's \"Limit of sparse configuration models and beyond: graphexes and multigraphexes). This is minor, but is a crucial literature to study (and cite).\n3. Missing key comparisons to unbounded graphon/operator. The two papers that closely implement the authors idea of 'stretching' a graphon is Maskey, Levie and Kutyniok's \"Transferability of graph neural networks: an extended graphon approach\" and Levie et al. (2021),  \"Transferability of spectral graph convolutional neural networks\" where they use unbounded operators to model very sparse graphs (even O(1) average degree). Compared to the approach of the paper, which changes the domain of a graphon to being unbounded, unbounded operator changes the range to being unbounded. The two are equivalent if there is a change of measure from one to another (which is probably the vast majority of cases). Seeing that the unbounded operator approach can do any and all things that the paper is claiming to do, a comprehensive comparison is warranted. \n\nI am willing to raise my score if my concerns are fully addressed in the rebuttal."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "03UigV2tv8", "forum": "HtMt9XNZv6", "replyto": "HtMt9XNZv6", "signatures": ["ICLR.cc/2026/Conference/Submission11617/Reviewer_VfQf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11617/Reviewer_VfQf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762263266013, "cdate": 1762263266013, "tmdate": 1762922690093, "mdate": 1762922690093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}