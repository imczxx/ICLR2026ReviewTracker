{"id": "BgX4s1d2d0", "number": 15977, "cdate": 1758257973249, "mdate": 1759897269538, "content": {"title": "Combinatorial Bandit Bayesian Optimization for Tensor Outputs", "abstract": "Bayesian optimization (BO) has been widely used to optimize expensive and black-box functions across various domains. Existing BO methods have not addressed tensor-output functions. To fill this gap, we propose a novel tensor-output BO method. Specifically, we first introduce a tensor-output Gaussian process (TOGP) with two classes of tensor-output kernels as a surrogate model of the tensor-output function, which can effectively capture the structural dependencies within the tensor. Based on it, we develop an upper confidence bound (UCB) acquisition function to select the queried points. Furthermore, we introduce a more complex and practical problem setting, named combinatorial bandit Bayesian optimization (CBBO), where only a subset of the outputs can be selected to contribute to the objective function. To tackle this, we propose a tensor-output CBBO method, which extends TOGP to handle partially observed outputs, and accordingly design a novel CMAB-UCB2 criterion to sequentially select both the queried points and the optimal output subset. Theoretical regret bounds for the two methods are established, ensuring their sublinear performance. Extensive synthetic and real-world experiments demonstrate their superiority.", "tldr": "We develop BO and combinatorial bandit BO frameworks for tensor-output systems, built upon the proposed tensor-output GP with a non-separable kernel as the surrogate model.", "keywords": ["Tensor data", "Non-separable kernels", "Gaussian process", "Bayesian optimization", "Combinatorial multi-arm bandit", "Upper confidence bound"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3801ac3aa056401bb53cb2ee440aa5e338b16803.pdf", "supplementary_material": "/attachment/165107d04a6c99bc15e8139945a01004ad5bdbc8.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Tensor-Output Bayesian Optimization, a framework for optimizing black-box functions with tensor-valued outputs using a tensor-output Gaussian process. It proposes both separable and non-separable tensor-output kernels that capture input-dependent cross-mode correlations, and develops a UCB acquisition tailored to scalarized tensor objectives. The work further extends to a combinatorial setting where only a subset of tensor entries contributes to the objective, using a partially observed TOGP and a CMAB-UCB2 criterion for joint input and subset selection. Theoretical sublinear regret bounds are provided, and experiments on synthetic and real tasks show consistent improvements over vectorized multi-output GP baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Modeling tensor outputs directly with non-separable, input-dependent kernels is a clear step beyond vectorized MOBO, enabling richer cross-mode structure to be exploited. The CBBO formulation and the PTOGP-based acquisition rule are principled. The regret analyses leverage information gain and cover both full and partial observation regimes, giving the method theoretical footing. Empirical results are broad, with synthetic and multiple real datasets, and show strong gains over standard multi-task and multivariate GP baselines. The appendices give thorough algorithmic and complexity details."}, "weaknesses": {"value": "The computational complexity $O(n^3 T^3 log(n))$ of TOBO is cubic in both n **and** tensor size, which may severely limit practical scale. \n\nThe scalarization operators $L_f$ and $H_f$ are central in practice, but their choices, sensitivity, and domain-specific instantiations are only lightly discussed. \n\nBaselines omit state-of-the-art tensor GP surrogates with Kronecker structure and non-separable multi-output kernels beyond the selected MTGP/MVGP/MLGP such as proposed in \"Scalable High-Order Gaussian Process Regression\" (line 91)."}, "questions": {"value": "- How are $L_f$ and $H_f$ instantiated across tasks, and how sensitive are results to these choices or to non-Lipschitz scalarizations encountered in practice?\n- How are tensor ranks chosen for the core tensors (line 208), and what is the performance/overfitting trade-off as ranks vary under separable vs non-separable kernels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "4esbmGYxqG", "forum": "BgX4s1d2d0", "replyto": "BgX4s1d2d0", "signatures": ["ICLR.cc/2026/Conference/Submission15977/Reviewer_uCf6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15977/Reviewer_uCf6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761253292113, "cdate": 1761253292113, "tmdate": 1762926188199, "mdate": 1762926188199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates optimization problems involving tensor-output objective functions, where each function evaluation returns a multi-dimensional tensor instead of a vector, as in standard multi-output settings. The authors introduce a Tensor-Output Gaussian Process (TOGP) that captures both dependencies across inputs and structured correlations within and between tensor modes of outputs. The proposed TOGP generalizes and introduces two types of kernels: a separable kernel, which assumes constant correlations across tensor modes, and a non-separable kernel, which allows correlations within and across tensor modes to vary with the inputs. \n\nThe authors further apply the TOGP to Bayesian Optimization (BO) in two scenarios. In the fully observed setting (TOBO), all tensor entries are available at each iteration, and optimization proceeds via an Upper Confidence Bound acquisition function derived from the TOGP posterior. In the partially observed setting (TOCBBO), only a subset of tensor entries is observed per iteration, transforming the problem into a Combinatorial Bandit Bayesian Optimization (CBBO) framework. Here, the algorithm must select both the next input and the subset of tensor entries (the super-arm) to observe. The paper provides theoretical guarantees, including proofs of kernel validity and sublinear regret bounds for both TOBO and TOCBBO. Extensive evaluations include three synthetic and four real-world benchmark problems to compare against many multi-output GP baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "-\tThe idea is sound, which generalizes the well-known LMC model in multi-output (vector-output) GP into the tensor-input GPs.\n-\tThe paper is clearly written, with theoretical proof for the kernel validity and regret bound under certain assumptions.\n-\tThe empirical performance shows that TOGP has potential in modelling tensor-output functions, when compared to vector-output GPs."}, "weaknesses": {"value": "Some experiments are missing: \n- Lack of single-objective BO baselines: Since the target problem settings rely on a scalarization operator (L_f or H_f in the CBBO case) to convert tensor outputs into scalar objectives, standard single-objective BO methods (e.g., single-task GP combined with a UCB acquisition function) should, in principle, operate normally. The paper would be stronger if it compared TOGP-based optimization with such single-task GP baselines to demonstrate the benefit of modelling complex tensor dependencies rather than relying on a simpler approach.\n- Lack of single-objective BO baselines in the Bandit BO setting: Similarly, in the Bandit BO setting, the presence of the scalarization operator (L_f or H_f) allows single-objective BO algorithms to be applied directly. Prior works (e.g., Nguyen et al., 2020; Ru et al., 2020; Huang et al., 2022) already use this setup effectively, and including these as baselines would provide a fairer and more comprehensive comparison for TOCBBO.\n- Lack of multi-objective BO baselines: Because TOGP inherently models multiple correlated outputs, it would be valuable to assess its performance under a multi-objective optimization framework. Comparing against the SOTA MOBO methods using metrics such as the hypervolume indicator (e.g., Maddox et al., 2021; Daulton et al., 2022) would further complement the paper."}, "questions": {"value": "1. What is the runtime trade-off of the proposed TOGP? Since TOGP models complex interactions across multiple tensor modes, it likely incurs higher computational costs than both single-task GPs and multi-output GPs based on the LMC, due to the larger number of hyperparameters involved. Additionally, given that the problem settings already require a scalarization operator (L_f or H_f) to map tensor outputs to scalar objectives, which allows single-task GP to work just fine, it would be useful to analyze the accuracy–efficiency trade-off - that is, how much performance gain the tensor-output model provides compared to the reduced computational cost relative to a simpler single-task GP baseline.\n2. Does the proposed TOBO algorithm account for the diversity of solutions? Given that the target problems have multiple outputs, some of which may be conflicting, can TOBO handle such cases, identify a set of solutions that satisfy different objectives or trade-offs? This is a common consideration in MOBO literature, where the goal is not only to find the best single solution but also to capture the Pareto diversity among competing objectives."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wmaBgZ0qXr", "forum": "BgX4s1d2d0", "replyto": "BgX4s1d2d0", "signatures": ["ICLR.cc/2026/Conference/Submission15977/Reviewer_f43k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15977/Reviewer_f43k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959998814, "cdate": 1761959998814, "tmdate": 1762926187822, "mdate": 1762926187822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents two variants of GP-UCB for BO of tensor-valued objectives. Both model the objective using a tensor-output GP (TOGP) with the goal of capturing additional structure present in the tensor representation, with a kernel that is somewhat novel to capture correlation between modes. The first algorithm uses scalarization - basically a weighted sum of the ouputs - while the second does the same on a subset of outputs (partial observations) at each round. Convergence rates are analysed to show sub-linear regret, and some experimental results are presented."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The major strength/novelty of this paper appears to be the TOGP model, in particular the kernel used. And the kernel used for this GP is novel, at least to my understanding.\n\nThat said, at least the first algorithm presented does not seem particularly novel to me: it is basically a scalarizing multi-objective BO algorithm. Each element of the tensor represents an objective, and a linearization operator takes the place of scalarization. The second algorithm is arguably more novel, as it assumes partial observations of the tensor components at each iteration; however, past this, it again boils down to a scalarized MOBO formulation.\n\nExperiments seem to have improved since last time I reviewed this paper, and experimental result seem sound, so I am inclined toward accepting."}, "weaknesses": {"value": "See above"}, "questions": {"value": "Relevant questions were answered last time I reviewed this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "neaCDkWUWj", "forum": "BgX4s1d2d0", "replyto": "BgX4s1d2d0", "signatures": ["ICLR.cc/2026/Conference/Submission15977/Reviewer_AnPz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15977/Reviewer_AnPz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973313489, "cdate": 1761973313489, "tmdate": 1762926187391, "mdate": 1762926187391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper introduces Tensor-Output Gaussian Processes (TOGPs) that model structured tensor-valued outputs with novel separable and non-separable kernels capturing input-dependent correlations.\n- Building on TOGP, the authors propose Tensor-Output Bayesian Optimization (TOBO) using a UCB acquisition function for efficient optimization of expensive black-box tensor-output functions.\n- They further extend the framework to a Combinatorial Bandit Bayesian Optimization (TOCBBO) setting where only a subset of tensor outputs can be observed.\n- Both TOBO and TOCBBO are theoretically analyzed, with sublinear regret bounds.\n- Extensive synthetic and real-world experiments demonstrate that the proposed methods outperform several baselines in predictive accuracy and optimization effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The manuscript is well written with polished language that enhances readability and effectively communicates complex technical ideas.\n- It introduces a novel Tensor-Output Gaussian Process (TOGP) framework with separable and non-separable kernels.\n- The framework extends Bayesian Optimization to handle tensor-valued functions."}, "weaknesses": {"value": "- The paper's main novelty lies in the Gaussian Process modeling, while the Bayesian Optimization layer mainly adapts standard UCB and CMAB-UCB2 frameworks without introducing a fundamentally new acquisition strategy.\n- The experimental evaluation emphasizes predictive accuracy metrics (MSE, MAE, NLL) more than optimization-oriented ones, creating a mismatch with the paper’s stated BO focus.\n- The paper lacks runtime or computational efficiency results, reporting only asymptotic complexity analyses without empirical timing comparisons."}, "questions": {"value": "- The paper's primary contribution appears to lie in the Gaussian Process modeling rather than in the Bayesian Optimization framework itself. Could the authors clarify whether the main contribution is intended to advance GP modeling or BO methodology?\n- Given that ICLR allows for a revision, the authors might be able to revise or extend the work to more clearly strengthen its contribution to the BO literature?\n- While theoretical computational complexities are analyzed, no empirical runtime or efficiency results are provided. Could the authors report or estimate the practical computational overhead of TOGP and TOCBBO compared with existing multi-output GP or BO methods?\n- How does the improved predictive accuracy of TOGP quantitatively translate into better optimization results such as faster regret decay or fewer evaluations? Have the authors conducted any ablation or correlation analysis to demonstrate this link?\n- Could the authors consider extending their framework to more meaningful tensor structures, such as spatiotemporal tensors that capture both spatial and temporal dependencies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There is no particular ethical concern."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qTSrXB7m5p", "forum": "BgX4s1d2d0", "replyto": "BgX4s1d2d0", "signatures": ["ICLR.cc/2026/Conference/Submission15977/Reviewer_9BYU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15977/Reviewer_9BYU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147621030, "cdate": 1762147621030, "tmdate": 1762926186890, "mdate": 1762926186890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel Bayesian optimization framework for systems with tensor-valued outputs, a setting previously unaddressed in the BO literature. The core idea involves a Tensor-output Gaussian Process (TOGP) with two new kernel classes that capture complex, input-dependent correlations within the tensor structure, avoiding the limitations of simple vectorization. The authors also introduce a more challenging Combinatorial Bandit BO (CBBO) setting where only a subset of tensor elements contributes to the objective, and propose TOCBBO to handle it via a CMAB-UCB2 acquisition function that decouples continuous input selection from combinatorial super-arm selection. Theoretical guarantees are provided for the sub-linear regret of both methods. Extensive synthetic and real-world experiments demonstrate superior performance over baselines that use standard multi-output GPs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces the first Bayesian optimization framework specifically designed for tensor-output systems, developing novel tensor-output Gaussian processes that capture complex structural dependencies through specialized kernel designs.\n2. This paper provides strong theoretical guarantees, establishing sublinear regret bounds for both the standard tensor-output setting and the more challenging combinatorial bandit scenario with partial observations.\n3. The proposed methods show robust performance and consistently outperform existing approaches across diverse real-world applications."}, "weaknesses": {"value": "1. This paper does not provide computational complexity comparisons with existing methods. Additionally, the high O(n^3T^3) complexity of TOGP itself represents a significant limitation, as strategies for scaling to large tensor outputs are not discussed.\n2. The theoretical analysis relies on the assumption that the true function is a sample from the proposed TOGP. There is little discussion of whether this holds in practice or how to assess its validity in real-world applications.\n3. A more detailed explanation of Equation (3) in line 161 would be helpful. For instance, it would be useful to clarify whether the kernel function K(x,x′) is designed to capture both similarities in the input space X and correlations among the internal elements of the output tensor.\n4. One concern is that the modeling and algorithm design are claimed to target tensor-type data, yet the actual formulations (e.g., Equations (4), (5), and (8)) are carried out at the matrix level. Would this process compromise or distort the intrinsic tensor structure?"}, "questions": {"value": "See the weaknesses part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MOZg2rs7hA", "forum": "BgX4s1d2d0", "replyto": "BgX4s1d2d0", "signatures": ["ICLR.cc/2026/Conference/Submission15977/Reviewer_6Rc1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15977/Reviewer_6Rc1"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149390761, "cdate": 1762149390761, "tmdate": 1762926186388, "mdate": 1762926186388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}