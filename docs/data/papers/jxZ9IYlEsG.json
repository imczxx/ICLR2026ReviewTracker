{"id": "jxZ9IYlEsG", "number": 6745, "cdate": 1757994261887, "mdate": 1759897897225, "content": {"title": "Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs", "abstract": "Reasoning is a key capability for large language models (LLMs), particularly when applied to complex tasks such as mathematical problem solving. \nHowever, multimodal reasoning research still requires further exploration of modality alignment and training costs. Many of these approaches rely on additional data annotation and relevant rule-based rewards to enhance the understanding and reasoning ability, which significantly increases training costs and limits scalability. To address these challenges, we propose the Deliberate-to-Intuitive reasoning framework (D2I) that improves the understanding and reasoning ability of multimodal LLMs (MLLMs) without extra annotations and complex rewards. Specifically, our method sets deliberate reasoning strategies to enhance modality alignment only through the rule-based format reward during training. \nWhile evaluating, the reasoning style shifts to intuitive, which removes deliberate reasoning strategies during training and implicitly reflects the model's acquired abilities in the response. D2I outperforms baselines across both in-domain and out-of-domain benchmarks. Our findings highlight the role of format reward in fostering transferable reasoning skills in MLLMs, and inspire directions for decoupling training-time reasoning depth from test-time response flexibility.", "tldr": "", "keywords": ["Multimodal LLMs", "LLM Reasoning", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04c24c2c33e3245d46c649371faf178036efb6c0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces D2I (Deliberate-to-Intuitive), a lightweight training–inference paradigm for enhancing multimodal reasoning in large vision-language models. During training, the model is guided by structured format rewards, including region localization (LOC), textual justification (JUS), and structured parsing (PAR)—under Group Relative Policy Optimization, encouraging stronger visual grounding without requiring extra annotations. At inference time, these constraints are removed, allowing the model to generate answers more freely and effectively. Experiments on GEOQA-8K and Doc-Mix, evaluated across math (e.g., MathVista, MathVerse) and general benchmarks (e.g., MME, MMVet), show that D2I consistently outperforms supervised fine-tuning and GRPO baselines. This demonstrates that deliberate structured reasoning during training can lead to stronger intuitive reasoning at test time, offering a simple and scalable improvement strategy for multimodal models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear conceptual contribution: D2I formalizes the idea of training with deliberate structured reasoning while allowing flexible, intuitive reasoning at test time. This training–inference decoupling is conceptually neat and aligns with the “slow thinking → fast thinking” trend in recent reasoning research.\n\n2. Low-cost supervision: The format reward does not require additional human annotations—only template matching. This makes the approach lightweight and scalable.\n\n3. Consistent performance gains: 1.Across math and general benchmarks, D2I improves over baselines by a few points (often 1–8%), showing good transfer from deliberate training to intuitive inference."}, "weaknesses": {"value": "1. Limited ablations and scaling evidence: Experiments are confined to a single model (7B) and short training (150 steps). There’s no scaling study across steps, model sizes, or training datasets. Ablations of LOC/JUS/PAR are also limited.\n\n2. For applications beyond math/document VQA, especially in less structured multimodal tasks (e.g., open-ended scene understanding), is the D2I regime still beneficial, or are the format rewards too domain-specific?"}, "questions": {"value": "Seeing weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QAOU9ePr8J", "forum": "jxZ9IYlEsG", "replyto": "jxZ9IYlEsG", "signatures": ["ICLR.cc/2026/Conference/Submission6745/Reviewer_Dp4P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6745/Reviewer_Dp4P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559217023, "cdate": 1761559217023, "tmdate": 1762919029936, "mdate": 1762919029936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reasoning framework named \"Deliberate-to-Intuitive\" (D2I), aimed at addressing the challenges of poor modality alignment and high training costs faced by Multimodal Large Language Models (MLLMs) in complex reasoning tasks, such as mathematical problems."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The main contribution of this paper is quite ingenious. A major bottleneck in current MLLM reasoning research is the high cost of obtaining high-quality, fine-grained reasoning-annotated data. The \"lightweight\" training paradigm proposed in this paper—which uses only rule-based format rewards without relying on any additional human annotations or content-level supervision—is used to enhance complex visual reasoning capabilities.\n2. The evaluation is not limited to in-domain datasets but also covers multiple out-of-domain math benchmarks and general MLLM benchmarks, strongly demonstrating the method's generalization ability.\n3. Rather than stopping at accuracy gains, the paper investigates why D2I works. The analyses of Pass@k, entropy distributions, and token-distribution shifts are persuasive; together they suggest that D2I encourages more exploratory, diverse, and flexible generation strategies, thereby outperforming D2D’s more rigid outputs.\n4. The paper is clearly written and easy to understand."}, "weaknesses": {"value": "1. An interesting finding of the paper is that different strategies perform differently on various benchmarks (e.g., JUS performs well on MathVerse, while PAR performs better on MathVista and MATH-Vision). The authors attribute this to the task types of the benchmarks (e.g., PAR helps in understanding \"complex visual layouts\"). However, this association is debatable. The core of D2I lies in intuitive reasoning, meaning these strategies are not used at test time. Why, then, would forcing the model to output coordinates (LOC) during training make it perform better on intuitive reasoning tasks that do not require coordinates?\n2. The case study in Figure 3 is very insightful, showing that $D2I_{jus}$ successfully identified \"vertical angles,\" while other models (including $D2I_{loc}$ and $D2I_{par}$) incorrectly identified them as \"adjacent angles\" or \"linear pairs\". However, this case only shows the successes and failures among D2I variants. To more forcefully support the core argument that D2I is superior to D2D (i.e., that D2D lacks flexibility), it is crucial to show the outputs of the D2D models. For example, if a $D2D_{par}$ model could generate the correct image parsing output but still produced the wrong final answer, it would strongly demonstrate the limitations of the D2D paradigm itself, rather than the model's learning ability.\n3. The core premise of this paper is that \"format rewards\" are more scalable than \"content rewards\". This is a reasonable assertion. However, the experiments lack an upper-bound baseline (even a theoretical one) that uses \"content rewards\"."}, "questions": {"value": "Same as the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tL8oIDfm4H", "forum": "jxZ9IYlEsG", "replyto": "jxZ9IYlEsG", "signatures": ["ICLR.cc/2026/Conference/Submission6745/Reviewer_NXGx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6745/Reviewer_NXGx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560006119, "cdate": 1761560006119, "tmdate": 1762919029557, "mdate": 1762919029557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Deliberate-to-Intuitive (D2I), a training–inference paradigm for multimodal LLMs. During training, the model is guided by deliberate reasoning strategies and rewarded with rule-based format rewards (plus answer correctness), which strengthens modality alignment without extra annotations or complex preference models. At evaluation, the scaffolds are removed and the model responds intuitively, implicitly leveraging what it learned. On visual-math benchmarks (in-domain and out-of-domain), D2I outperforms baselines, suggesting that simple, programmatic format rewards can foster transferable multimodal reasoning and that training-time reasoning depth can be decoupled from test-time response style."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea directly addresses data and engineering bottlenecks in multimodal reasoning. Because the rewards are deterministic and easy to verify (presence/structure of tags, answer format), the approach is cheap to implement, simple to maintain, and broadly reproducible. This lowers the barrier to deploying reasoning-oriented training at scale and makes the method attractive beyond research prototypes.\n\n2. D2I’s separation between deliberate training and intuitive inference is conceptually tidy and practically useful. The scaffolds force the model to practice grounding and decomposition during training, yet inference discards them, reducing latency, token cost, and brittleness to low-quality intermediate outputs. The result is an appealing operational profile: you pay the complexity during training, then reap simpler, faster, and often more robust inference at deployment."}, "weaknesses": {"value": "1. The format rewards primarily check structure, including shape of <box>, <crucial>, <parse>, rather than semantic fidelity. A box that doesn’t truly cover the evidence, a plausible-sounding justification that is not grounded in the image, or a parse with internally inconsistent relations can still receive a reward. This risks teaching “good-looking” reasoning artifacts that do not guarantee genuine visual grounding.\n\n2. Most compelling results hinge on a single backbone/configuration and visual-math–centric benchmarks. The paper would be stronger with cross-architecture tests (other MLLM families and sizes), variance across random seeds, statistical significance, and detailed compute profiles. Without these, it’s hard to judge how robustly D2I generalizes or what the real cost–benefit curve looks like in production settings.\n\n3. Although test-time scaffolds are removed, prompts are still biased toward stepwise answers rather than truly concise, System-1–style outputs. The work would benefit from a more systematic exploration of response styles and their trade-offs in accuracy, hallucination, and usability, especially for applications that value brevity or strict format constraints at inference.\n\n4. On some general multimodal benchmarks, gains are weaker or mixed, suggesting domain- or scaffold-sensitivity."}, "questions": {"value": "See above weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mdEJVUTFI4", "forum": "jxZ9IYlEsG", "replyto": "jxZ9IYlEsG", "signatures": ["ICLR.cc/2026/Conference/Submission6745/Reviewer_kLio"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6745/Reviewer_kLio"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162437494, "cdate": 1762162437494, "tmdate": 1762919029090, "mdate": 1762919029090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces D2I framework to enhance the reasoning capabilities of MLLMs. The main idea is to train models to engage in deliberate, structured reasoning during training and switch to more flexible, intuitive reasoning during inference. This paper proposes three different strategies and only the reponse format is supervised in the training stage. Experiments results on on multiple benchmarks such as MathVerse and MathVista show that D2I framework can surpass D2D in most cases."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes three different reasoning strategies, which helps model to learn the alignment across image and text modals. Also the training stage focuses on format reward, which means additional human annotations or expensive data generation are not required, making the method cost-effective and scalable.\n2. Allowing flexible generation in inference stage is innovate, and the experiments results on in-domain dataset further support the effectiveness of this D2I framework."}, "weaknesses": {"value": "1. It looks like the citation format is somehow wrong. There are no brackets warping the citations, makes them mixed with other texts. Also, the figures are not well organized, for instance Fig 6 shows before Fig 5.\n2. The results on out-of-domain dataset shows that D2D achieve better performance than D2I models. However authors lack detailed description of this phenomenon."}, "questions": {"value": "1. The results on general OOD dataset like MME shows that D2D can achieve better results than all of the three D2I models. What may be the reasons?\n2. I'd like to know more about the the PAR strategy. For geometry tasks the textual explanation (JUS) and structrual parsing (PAR) may show great difference. However on other general VQA tasks how does PAR works? Especally there are no additional annotations to help the model learn GT structure language."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EeMJPQCssm", "forum": "jxZ9IYlEsG", "replyto": "jxZ9IYlEsG", "signatures": ["ICLR.cc/2026/Conference/Submission6745/Reviewer_a5bU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6745/Reviewer_a5bU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762197050432, "cdate": 1762197050432, "tmdate": 1762919028551, "mdate": 1762919028551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Deliberate-to-Intuitive (D2I) reasoning framework designed to enhance the understanding and reasoning capabilities of Multimodal Large Language Models (MLLMs). Specifically, the authors introduce three training strategies—Region Localization (LOC), Region Justification (JUS), and Parsing Consistency (PAR)—which are applied during training but removed during inference to strengthen the model’s reasoning ability. Experimental results demonstrate that D2I achieves improvements on both in-domain and out-of-domain benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is straightforward and easy to understand.\n2. The three methods sound reasonable for the training stage.\n3. The results achieve improvements on both in-domain and out-of-domain benchmarks."}, "weaknesses": {"value": "1. Adding citations in Line 094-096 about how deliberate reasoning behaviors are commonly adopted can help readers gain a better understanding of this field.\n2. The citation at Line 499 is incomplete.\n3. Line 394: MMe -> MME\n4. Lacks citations and discussion of previous reasoning MLLMs and related works, e.g., [1-5]\n\n[1] Vision-r1: Incentivizing reasoning capability in multimodal large language models\n\n[2] R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization\n\n[3] Video-R1: Reinforcing Video Reasoning in MLLMs\n\n[4] Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement\n\n[5] Vlm-r1: A stable and generalizable r1-style large vision-language model"}, "questions": {"value": "1. What about the results of I2I learning? Is it equivalent to common GRPO?\n2. What is the value of k in Figure 4?\n3. Entropy increases means randomness also increases, so how is reproducibility ensured? What is the fluctuation range of the results? \n4. Providing inference-time hyperparameters (e.g., temperature, top-p/top-k, and other relevant settings) is important for replication.\n5. The authors state that D2I leads to a higher entropy distribution, implying greater randomness in the model’s outputs. Could this be due to the difference between training prompts and inference prompts? Additionally, how do the authors interpret this increase in entropy — is higher entropy during inference considered beneficial or potentially harmful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UVHqwSPKYp", "forum": "jxZ9IYlEsG", "replyto": "jxZ9IYlEsG", "signatures": ["ICLR.cc/2026/Conference/Submission6745/Reviewer_TyaT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6745/Reviewer_TyaT"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6745/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762311355291, "cdate": 1762311355291, "tmdate": 1762919028187, "mdate": 1762919028187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}