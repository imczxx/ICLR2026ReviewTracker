{"id": "OZxlQjWu8S", "number": 1154, "cdate": 1756852624845, "mdate": 1759898224761, "content": {"title": "LINA: Exploring Linear Autoregressive Image Generative Models with Continuous Tokens", "abstract": "In this paper, we systematically explore how efficient \\underline{lin}ear attention mechanisms, as alternatives to full attention, should be designed in the context of \\underline{a}utoregressive image generative models with continuous tokens.\nWe offer \\textbf{\\model}, a simple and empirically validated text-to-image generation baseline built on pure linear attention, capable of rapidly generating high fidelity 1024$\\times$1024 images from user instructions. \nOur key contributions are: \n(1) An empirical study on scaling behavior \\wrt parameter counts.\nWe examine two crucial design choices: (i) normalization paradigms—division-based \\vs addition-based, and (ii) the use of a depthwise convolution module on image features for locality capacity enhancement. We then compare which variants of linear attention scale most effectively in autoregressive image generation. \n(2) KV gate mechanism: \nWe empirically find that applying data-independent learnable parameters to the key and value states, thereby enabling flexible memory management and benefiting sample quality. \nWe run thorough ablations on the KV gate design choices and visually show off the patterns it learns. \nBuilding on these designs, \\model achieves promising results on both class-conditional (C2I) and text-to-image (T2I) generation. Compared to diffusion models of similar scale and autoregressive models with full attention, \\model delivers competitive performance: FID of 2.18 on ImageNet ($\\sim$1.4B) and an overall score of 0.74 on GenEval ($\\sim$1.5B). Code and models will be open-sourced.", "tldr": "A successful practice of efficient linear autoregressive image generation model with continuous tokens.", "keywords": ["Linear attention; autoregressive image generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3a8ed72b774bdbbda234daa09f1bd11a511eacf.pdf", "supplementary_material": "/attachment/5733330bfe1d1f9851d39443dd8fcb26940b0b2c.zip"}, "replies": [{"content": {"summary": {"value": "This submission proposes several improvements aiming to make inference of autoregressive image generative models cheaper.\nTo achieve this, the authors replace common attention layers in recent NOVA model with linear attention and study several design choices for that.\nNamely, they compare division-based normalization with subtraction-based and add a convolutional layer to compensate the insufficient  capacity for local modeling.\nIn addition, learnable gates are proposed for attention heads, which seem to slightly improve the quality of generation.\n\nThe authors evaluate their model both with class-conditional generation on ImageNet and txt2img generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper considers an important problem of efficient models. The motivation for this research is clear, and the paper is mostly easy to read. Scaling behavior is examined, and it show some promise for the proposed method. Also, the comparison with other diffusion models and autoregressive models demonstrates that linear attention may be a viable alternative to the more expensive full attention."}, "weaknesses": {"value": "1. The presented ideas are not novel per se, although the empirical study of known techniques applied to a new problem can be beneficial for the community as well.\n2. It is not very clear from the text (Eq. 7), why KV gates cannot be learned just as part of the projection matrices $W_K$ and $W_V$ of the attention layer, especially if the kernel function is just a ReLU unit. While Tab. 1 demonstrates some improvement of using the gates on ImageNet, I wonder how well the model without gates performs in text-to-image task."}, "questions": {"value": "1. While the latency of the model without optimizations is slightly greater than the latency of FlashAttn-optimized NOVA (Tab. 3), I would ask to provide the numbers for the peak memory consumption. Is my understanding correct that it should be significantly lower for the model with linear attention?\n2. Kernel size of 5x5 is relatively large, even though it is a depthwise convolution. For example, this can be limiting for mobile hardware. How sensitive is the model to the design of this convolutional module?\n3. Please address the question about the importance of independently learned KV gates for ReLU kernel function."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WFYE7edshL", "forum": "OZxlQjWu8S", "replyto": "OZxlQjWu8S", "signatures": ["ICLR.cc/2026/Conference/Submission1154/Reviewer_vP1E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1154/Reviewer_vP1E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760984734100, "cdate": 1760984734100, "tmdate": 1762915692441, "mdate": 1762915692441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how to design efficient linear attention mechanisms specifically for autoregressive image generative models with continuous tokens, a setting that differs from ViT-based perception or diffusion-based generation due to its step-wise causal decoding process. The authors propose LINA, a pure linear-attention-based autoregressive generator capable of producing high-resolution (1024×1024) images with competitive quality and faster sampling compared to full-attention counterparts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper provides a feasibility study of applying linear attention to autoregressive image generative models, decomposing the problem into several sub-components and addressing them step by step.\n* The authors propose **LINA**, a text-to-image model that aims to balance efficiency and fidelity under a pure linear attention architecture.\n* Experimental coverage is extensive, and the design choices appear to be systematically ablated and reproducible."}, "weaknesses": {"value": "1. **Limited novelty.**\n   The main contributions primarily consist of empirical combinations and refinements of existing techniques:\n   i) exploring normalization strategies for linear attention and introducing depthwise convolution to compensate for locality;\n   ii) introducing a KV gate module.\n   While the observation that linear attention lacks local inductive bias and thus benefits from depthwise convolution is somewhat insightful, the work still lacks a theoretically grounded innovation or a fundamentally new mechanism.\n\n2. **Presentation issues leading to a technical report style rather than a focused scientific argument.**\n\n   * The preliminary section is overly long and the related work is placed at the end, which makes the structure feel misaligned. Foundational content such as MAR models does not require such an extended exposition.\n   * The two normalization variants, claimed as core design decisions, could be introduced more concisely under the Methods section without heavy narrative buildup.\n   * Motivation is repeated across Introduction and Methods, leading to redundancy.\n   * The paper jumps between methodology, implementation details, and experimental settings, instead of clearly separating method description and empirical validation.\n\n3. **Experimental presentation could be improved.**\n   The best results in tables should be bolded, and if possible, second-best results could be underlined to enhance readability.\n\n4. **Some generation results lack semantic consistency.**\n   For example, in Figure 7 of the appendix, the first example fails to preserve the concept \"jetpack\", which suggests that semantic grounding is not always stable."}, "questions": {"value": "Please refer to the \"Weakness\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OaAOQZdaQT", "forum": "OZxlQjWu8S", "replyto": "OZxlQjWu8S", "signatures": ["ICLR.cc/2026/Conference/Submission1154/Reviewer_wwwH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1154/Reviewer_wwwH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965954141, "cdate": 1761965954141, "tmdate": 1762915692328, "mdate": 1762915692328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LINA, a linear autoregressive image generation architecture employing continuous tokens, designed for efficiency and competitive image quality at resolutions up to 1024×1024. The authors comprehensively investigate design paradigms for linear attention—specifically, division-based vs. subtraction-based normalization—and the impact of locality augmentation using depthwise convolution (DWC). They introduce a data-independent KV gate mechanism to flexibly manage memory within linear attention layers. Extensive ablations, scaling experiments, and qualitative/quantitative evaluations are conducted on class-conditional and text-to-image generation benchmarks, with comparisons against recent diffusion and autoregressive baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The empirical study rigorously dissects the scaling behavior of different linear attention paradigms, providing actionable guidance on architecture design for autoregressive image generation.\n2. The paper is well written, with rigorous structure and clear logical flow.\n3. The experiments are comprehensive and carefully conducted, providing solid empirical support for the proposed approach."}, "weaknesses": {"value": "1. Limited Novelty in Core Mechanisms: While the empirical study on scaling and the introduction of KV gate are well-executed, the architectural changes (division-based normalization, DWC, gating) largely package and extend existing ideas from linear attention literature (Katharopoulos et al., Han et al., Qiu et al., Yang et al.). There is limited original theoretical derivation or clear demonstration that the KV gate delivers fundamentally different benefits compared to gating or locality modules already studied elsewhere.\n2. Despite referencing recent diffusion and linear attention efforts (Section 5), the paper insufficiently contextualizes and empirically compares to several directly related works on continuous-token autoregressive image generation.\n3.  Table 3, comparing LINA’s latency to FlashAttention, reveals little actual speedup—22s vs. 20s at 1024px, with FlashAttention on the full-attention baseline—and the observed latency parity is not explained in terms of FLOPs or memory.\n4. Minor Issues: Occasional dense or overloaded notation in equations (e.g., Section 3.3), some grammatical/typo issues persist (e.g., “Addition-based” sometimes called “subtraction-based”), reference style inconsistent in places, and a few claims (e.g., “negligible parameter overhead”) are asserted without accompanying numbers or clear resource comparison."}, "questions": {"value": "Please provide responses to the issues raised in my Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qzyRF527WB", "forum": "OZxlQjWu8S", "replyto": "OZxlQjWu8S", "signatures": ["ICLR.cc/2026/Conference/Submission1154/Reviewer_7uD7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1154/Reviewer_7uD7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978213915, "cdate": 1761978213915, "tmdate": 1762915692172, "mdate": 1762915692172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of designing efficient linear attention mechanisms for autoregressive image generative models.\nThe paper introduces LINA, a text-to-image generator based solely on linear attention, capable of producing high-quality 1024×1024 images efficiently. \nThrough systematic experiments, they analyze key design choices—normalization strategies and depthwise convolution for locality enhancement. In addition, the KV gate is proposed for flexible memory management. \nExtensive ablation and visualization studies demonstrate how these components improve scalability and sample quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper tackles an important problem of efficiency in autoregressive image generation, providing a viable solution based on linear attention. \n\n2. The paper is well-structured and easy to follow.  \n\n3. Comprehensive experiments across diverse model scales and benchmarks are well-conducted and appreciated."}, "weaknesses": {"value": "1. While the paper is strongly motivated by efficiency, the experiments mainly focus on generation performance. For example, efficiency comparisons (Table 3) are limited to latency and parameter count. Given that efficiency is the core motivation, a more in-depth analysis covering GPU memory usage, FLOPs, and runtime behavior is needed. Furthermore, the efficiency contribution of each proposed component (e.g., KV gate, depthwise convolution) should also be clarified. \n\n2. It is unclear whether LINA truly achieves competitive performance as claimed (line 39). In Table 4, LINA-H performs noticeably worse than the baseline NOVA, especially in 1024×1024 generation, raising doubts about the claimed competitiveness. \n\n3. Although the empirical findings are interesting, the analyses on normalization types and locality augmentation for linear attention appear generic rather than specific to autoregressive image generation. The identified trends seem consistent with existing results in other domains, thus limiting the novelty of the empirical insights. \n\n4. If LINA’s efficiency gains are comparable to those achieved by popular acceleration techniques such as FlashAttention, practitioners may not find it worthwhile to adopt a new architecture. It would be valuable to demonstrate the synergy between LINA and such existing acceleration methods."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qcHJD9E2rj", "forum": "OZxlQjWu8S", "replyto": "OZxlQjWu8S", "signatures": ["ICLR.cc/2026/Conference/Submission1154/Reviewer_UJ9T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1154/Reviewer_UJ9T"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762261170461, "cdate": 1762261170461, "tmdate": 1762915692052, "mdate": 1762915692052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}