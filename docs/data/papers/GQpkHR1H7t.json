{"id": "GQpkHR1H7t", "number": 17527, "cdate": 1758277177830, "mdate": 1759897168988, "content": {"title": "Learning from Failures: Understanding LLM Alignment through Failure-Aware Inverse RL", "abstract": "Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with human preferences, yet the underlying reward signals they internalize remain hidden, posing a critical challenge for interpretability and safety. Existing approaches attempt to extract these latent incentives using Inverse Reinforcement Learning (IRL), but treat all preference pairs equally, often overlooking the most informative signals: those examples the extracted reward model misclassifies or assigns nearly equal scores, which we term *failures*. We introduce a novel *failure-aware* IRL algorithm that focuses on misclassified or difficult examples to recover the latent rewards defining model behaviors. By learning from these failures, our failure-aware IRL extracts reward functions that better reflect the true objectives behind RLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselines across multiple metrics when applied to LLM detoxification, without requiring external classifiers or supervision. Crucially, failure-aware IRL yields rewards that better capture the true incentives learned during RLHF, enabling more effective re-RLHF training than standard IRL. This establishes failure-aware IRL as a robust, scalable method for auditing model alignment and reducing ambiguity in the IRL process.", "tldr": "We develop failure aware IRL to understand LLM alignment", "keywords": ["Inverse Reinforcement Learning", "Failures; Alignment", "LLMs", "RLHF"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cb236295bec17d0977e5160c6623dfe93c3bcdd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Failure-Aware Inverse Reinforcement Learning (FA-IRL), a new method to figure out the hidden reward signals that Large Language Models (LLMs) learn during alignment with human preferences (RLHF). Instead of treating all data points equally, FA-IRL explicitly identifies these \"failures\" and gives them more weight during training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper designs several experiments to verify that failures (hard cases) are important for reward recovery."}, "weaknesses": {"value": "The paper has limited scale and scope, using small parameter models and simple tasks like detoxification. It's unclear whether these findings will transfer to more general settings with more capable base language models and rewards. \n\nThe idea of using failure cases or hard examples is not novel (e.g., utilizing hard negatives for contrastive learning or leveraging challenging rollouts to enhance model reasoning through RL).\n\nRe-RLHF experiments show that the method still underperforms compared to the ground-truth reward signal—which makes sense since it's an approximated reward function. But this raises the question: **why does learning this signal matter? If the goal is interpretability or safety, how does IRL address these concerns**?"}, "questions": {"value": "When a hidden reward signal is unknown (in your case, the reward model), why is it important to use IRL to model such signals? What is the actual use of the proposed model?\n\nWill the proposed method improve the current LLM + RL training pipeline (whether it is RLHF with reward modeling or RLVR with verifiable rewards)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fnEoPsp5jM", "forum": "GQpkHR1H7t", "replyto": "GQpkHR1H7t", "signatures": ["ICLR.cc/2026/Conference/Submission17527/Reviewer_oyKk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17527/Reviewer_oyKk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761099425924, "cdate": 1761099425924, "tmdate": 1762927403968, "mdate": 1762927403968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Failure-Aware Inverse RL (FA-IRL) for auditing post-training incentives in LLMs. Instead of treating all preference pairs equally, FA-IRL identifies “failures”—misclassified or near-tie pairs—and gives them extra weight through a dual-path reward model: a base path trained on all pairs and a correction path trained only on failures, with a curriculum over a dynamic margin threshold and a decay schedule for the failure weight. The authors also give a simple theoretical result: adding stricter constraints on failure pairs shrinks the feasible reward set, mitigating non-identifiability. Empirically, using detoxification as the main case study, FA-IRL improves preference classification (e.g., higher F1/AUC), reduces STARC error, and—importantly—yields rewards that drive more effective re-RLHF (toxicity ≈ 6% with FA-IRL rewards vs ≈ 9% with standard IRL; ≈ 4% with ground-truth rewards). The study spans several small-to-mid models (e.g., Pythia-410M, SmolLM2-360M, Gemma-270M) and ~20k preference pairs per family, with subtype analyses and disagreement slices."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear lever with theory to match. Tightening margins on failure pairs reduces the reward ambiguity set; the empirical variance reductions and STARC gains are consistent with that story.\n\nActionable in practice. The curriculum + decayed λ make the method stable; the dual-path split isolates sharp corrections without destabilizing the base reward.\n\nDownstream win, not just offline metrics. The re-RLHF result (≈ 6% toxicity vs ≈ 9% with standard IRL) shows the rewards are operationally better, not only better-correlated.\n\nWhere it helps is where it should. Disagreement and subtype tables show most gains appear on genuinely hard slices (near-ties, insults/obscene categories), which fits the hypothesis.\n\nLimitations are acknowledged. Pair-mix sensitivity, scale bounds, and labeling issues are discussed rather than buried."}, "weaknesses": {"value": "Toxicity pipeline dependence. The “ground-truth” for PPO and some evals comes from public toxicity classifiers. That is practical but can imprint classifier bias; a second domain (e.g., factuality with unit tests, harmlessness with multi-rater human labels) would strengthen generality claims.\n\nFailure definition coupling. Margin-based failures depend on the current reward model; this can create feedback loops. A short analysis of false-positive failure mining (e.g., when ambiguity ≠ error) would be reassuring.\n\nLimited model scale and tasks. Results top out at ~410M and one main alignment task. Claims about broader “alignment auditing” should be scoped accordingly.\n\nEvaluation hygiene. For re-RLHF, report identical PPO hyper-params and wall-clock/GPU for each reward to rule out accidental training imbalance; include confidence intervals on the ≈ 6%/9% numbers."}, "questions": {"value": "Schedules \\& robustness. How sensitive are final STARC/AUC and re-RLHF toxicity to the margin threshold schedule $\\gamma_t$, failure sampling rate $p_t$, and $\\lambda$ decay? A small grid or randomsearch summary would help practitioners pick defaults.\n\nFailure quality control. Did you try hybrid mining (margin + small, diverse teacher set) to reduce spurious failures? How often do margin-flagged pairs later flip to non-failures as the model improves?\n\nGeneralization beyond toxicity. Any pilot results on a second verifiable domain (e.g., code with unit tests, math with programmatic checkers) to decouple from classifier biases?\n\nCost accounting. What is the extra compute for FA-IRL (failure mining + correction head) vs. standard IRL? For re-RLHF, please include identical budgets and report tokens $/ \\mathrm{s}$, hours, and seeds.\n\nWhich path does the work? If you freeze the base path after a warm-up and continue training only the correction path on failures, how much of the final gain remains? Conversely, if you zero the correction head at test time, how much performance drops?\n\nWhen failure density is low. In the pair-mix analysis, FA-IRL can underperform when informative $T \\rightarrow N T$ pairs are scarce. Do you recommend a minimum failure density or a switchback to standard IRL when $|F_{t}|$ falls below a threshold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fAKyPY4Rbz", "forum": "GQpkHR1H7t", "replyto": "GQpkHR1H7t", "signatures": ["ICLR.cc/2026/Conference/Submission17527/Reviewer_BAGb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17527/Reviewer_BAGb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952359760, "cdate": 1761952359760, "tmdate": 1762927403563, "mdate": 1762927403563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Failure-Aware Inverse Reinforcement Learning (FA-IRL) to recover the latent reward behind RLHF by explicitly identifying and up-weighting “failures”—preference pairs that are ambiguous or misclassified—during reward learning. Concretely, it uses a dual-path reward model over frozen texts embeddings; failures are detected via margin uncertainty or disagreement with supervised labels. Evaluated on detoxification, with preference pairs built from RealToxicityPrompts (base vs. aligned “expert”) and a held-out Jigsaw set, FA-IRL improves F1/AUC and reduces STARC error versus standard IRL, and when used for re-RLHF it lowers toxicity to ~6% (vs. ~9% with standard IRL; ~4% with ground-truth reward)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper brings a simple and effect idea to treating uncertain or wrong pairs as “failures” and giving them extra weight with a two-path reward, turning common near-ties and label noise into useful signal. \n* The method is clearly described and easy to re-produce, with a small theory piece that explains why focusing on failures reduces ambiguity and with careful experiments and ablations to back it up. \n* The gains show up in both standard metrics and downstream re-alignment (lower toxicity) across several model sizes, suggesting practical impact beyond this one detox task."}, "weaknesses": {"value": "* Preference pairs are synthetic (base vs. expert with the expert always preferred), which can make learning partly about distinguishing policies rather than general human preferences; include some human-curated pairs, allow non-expert-preferred pairs, and check if gains hold.\n* The dataset is small and targeted to detox (≈20k pairs per family, plus Jigsaw), so generality is unclear, including non-toxicity axis (e.g., factuality or refusal correctness) and report cross-task transfer could help.\n* The “ground-truth” reward that trains the expert policy is a public toxicity classifier, not human ratings; this risks learning the classifier’s biases rather than human preference, so add a small human eval could be useful and probe for bias (group-wise error, calibration) and agreement with humans. \n* Results use only a few small model families (≈135M–410M), so it’s unclear if gains hold for larger instruction-tuned models or other tasks. Including at least one 7B-class model would be needed for generality."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3vxIe8kA48", "forum": "GQpkHR1H7t", "replyto": "GQpkHR1H7t", "signatures": ["ICLR.cc/2026/Conference/Submission17527/Reviewer_586b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17527/Reviewer_586b"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972727751, "cdate": 1761972727751, "tmdate": 1762927403123, "mdate": 1762927403123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Failure-Aware IRL (FA-IRL): learn a base reward model plus a “failure” head that focuses on examples the model gets wrong or is uncertain about. The theory argues these extra constraints reduce reward ambiguity. Experiments are on detoxification with preference data and a downstream RLHF-style loop."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The core motivation (failures carry more information) is reasonable, and the theoretical framing is helpful.\n+ The method is conceptually simple (dual head + curriculum) and seems implementable.\n+ Empirically, there are consistent gains on the detox task and some downstream improvements."}, "weaknesses": {"value": "- Scope vs. claim. The paper is positioned as a general IRL/“auditing” framework, but the experiments focus on detoxification. To support the broader claim, one additional axis (e.g., helpfulness, refusals, factuality/hallucinations, or a non-toxicity harmlessness task) would greatly strengthen the paper.\n- Presentation clarity. Figure 1 and Algorithm 1 seem to be the core pieces, but I had trouble following them on a first pass. It would help to (i) explicitly refer back to Figure 1 in the main text and add a short walk-through of the example, and (ii) add a bit more intuition around Algorithm 1 (a brief “what each step is doing,” a variable glossary, and typical default settings).\n- Evaluation interpretation. STARC is defined, but it’s hard to know what a shift (e.g., 0.686 to 0.850) means physically. The narrative also jumps between in-domain preference tests, a held-out Jigsaw set, and downstream PPO toxicity. A small figure/table mapping STARC changes to toxicity deltas, and a short paragraph stitching these pieces together, would make the evaluation easier to interpret."}, "questions": {"value": "Apart from my concerns in the weaknesses:\n- How were the threshold schedule determined, failure weight schedule, sampling rate in practice (initial values, schedules, and sensitivities)?\n- Would it be possible to include some brief case studies showing how the failure head changes the decision vs. the base head (inputs, margins/scores, and the corrected outcome)? That would greatly help with evaluating the effectiveness of the method.\n- Do you see the failure head over-specialize to rare toxic templates? Can you show performance on unseen toxicity styles or paraphrases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "r72NIOJwFd", "forum": "GQpkHR1H7t", "replyto": "GQpkHR1H7t", "signatures": ["ICLR.cc/2026/Conference/Submission17527/Reviewer_ep8m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17527/Reviewer_ep8m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994750076, "cdate": 1761994750076, "tmdate": 1762927402670, "mdate": 1762927402670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}