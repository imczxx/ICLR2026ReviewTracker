{"id": "kkNr2niHtT", "number": 21578, "cdate": 1758319246084, "mdate": 1759896915321, "content": {"title": "Evaluating steering techniques using human similarity judgments", "abstract": "Current evaluations of Large Language Model (LLM) steering techniques focus on task-specific performance, overlooking how well steered representations align with human cognition. Using a well-established triadic similarity judgment task, we assessed steered LLMs on their ability to flexibly judge similarity between concepts based on size or kind. We found that prompt-based steering methods outperformed other methods both in terms of steering accuracy and model-to-human alignment. We also found LLMs were biased towards \"kind\" similarity and struggled with \"size\" alignment. This evaluation approach, grounded in human cognition, adds further support to the efficacy of prompt-based steering and reveals privileged representational axes in LLMs prior to steering.", "tldr": "Using a cognitive science inspired evaluation paradigm, we evaluate the effectiveness of various LLM representation-steering methods for inducing more human-like semantic alignment.", "keywords": ["cognitive science", "transformers", "large language models", "human-AI alignment", "human-centered AI", "steering", "cognitive benchmarking"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/56fc23c000d986f08c926ccc96b5d443a33327d0.pdf", "supplementary_material": "/attachment/7c7af3b3799e1f4b845b9d02ab175b5b18b62f49.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes a cognitively grounded evaluation of LLM steering methods using triadic similarity judgments. The task asks models to choose which of two items is more similar to a reference along an instructed dimension (size or kind). From many triplet judgments, the authors reconstruct two-dimensional embeddings using a crowd-kernel approach and then quantify model–human representational alignment via Procrustes correlation. Across Gemma2-9B and Gemma2-27B, prompt-based steering achieves the highest task accuracy and the best, though still limited, alignment; models show a default bias toward kind over size; and none of the interventions produce embeddings that align well with human size representations. The work argues for evaluation beyond task accuracy, emphasizing alignment between steered model representations and human cognition."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The experimental setup is coherent:\n- define triplets with mutually exclusive size vs kind decisions\n- collect at least 2,500 judgments per method\n- fit 2D embeddings with the crowd-kernel loss, and \n- compute squared Procrustes correlations between human-derived and model-derived embeddings. \n\nThe instruction formats, in-context learning vs zero-shot, and activation-based interventions are described clearly in the appendix with a consistent steering and evaluation pipeline. The separation of competence (accuracy on held-out triplets) and alignment (geometry similarity to human embeddings) is appropriate and addresses the common “performance vs representation” conflation.\n\nThe competence–alignment dissociation is carefully documented: prompting yields high task accuracy, yet alignment, especially for size, remains low. The kind-over-size default is convincingly supported by higher neutral-prompt accuracy and higher neutral-kind alignment relative to size. The paper’s hypothesis that humans allow leakage from irrelevant dimensions while well-prompted LLMs can more cleanly isolate task-relevant features is plausible and consistent with observed dissociations, although corroborating analyses would strengthen it. For example, measuring off-axis interference directly by training a linear probe on model residual states to predict kind from size prompts (and vice versa) would quantify leakage, and doing the same on human embeddings would create an explicit comparison.\n\nThe narrative fits with emerging evidence that simple prompting often rivals or beats more complex internal interventions and that model competence need not imply human-like internal representations; the paper’s added value is to anchor this in an interpretable task with a geometry-level alignment metric."}, "weaknesses": {"value": "- The decision to fix the embedding dimensionality at two could artificially compress structure and differentially affect methods; it would be helpful to report results across multiple dimensionalities with model selection via held-out triplets or information criteria, or to show that conclusions are stable at d=3–5.\n- Procrustes r^2 should be accompanied by uncertainty estimates, for example via bootstrapping triplets, and significance assessed against a permutation baseline that preserves triplet structure to rule out spurious correlations.\n- The selection of layers for the activation-based methods appears to be tuned on held-out accuracy and then reused for alignment estimation; a fully nested cross-validation would avoid any potential selection–evaluation coupling.\n- The parsing of model outputs requires precision to avoid leakage from surface-form priors; please detail how exact string matching was enforced and whether alternative surface forms (e.g., synonyms, capitalization) were encountered and handled."}, "questions": {"value": "- How many human participants contributed to each condition, what were per-participant trial counts, and how were judgments aggregated? Were within-participant reliabilities assessed, and is there an upper-bound “human-to-human” Procrustes r^2 to contextualize model alignment?\n- How sensitive are results to embedding dimensionality and to the choice of optimizer and regularization in the crowd-kernel fitting?\n- For activation-based methods, were multiple layers or compositions considered at inference time beyond the best single layer, and does multi-layer steering change the story?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "The study raises minimal direct ethical risks. It uses human judgments; the paper should explicitly state the consent procedure, compensation, and IRB/ethics approval if applicable. Since the work evaluates steering, it might indirectly influence how practitioners attempt to control models; discussing dual-use considerations and emphasizing that alignment to human-like representations is not synonymous with normative safety would be valuable."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YSXJCwnGX1", "forum": "kkNr2niHtT", "replyto": "kkNr2niHtT", "signatures": ["ICLR.cc/2026/Conference/Submission21578/Reviewer_mVQ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21578/Reviewer_mVQ2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763252946, "cdate": 1761763252946, "tmdate": 1762941841494, "mdate": 1762941841494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates the efficacy of Large Language Model (LLM) steering techniques by assessing how well the steered representations align with human derived representations. Both humans and LLMs were asked to judge which of two concepts was most similar to a target concept, along kind and size dimensions. LLMs were steered using 4 techniques and measured on accuracy and alignment. Prompting outperformed other steering methods. The work also discovered that LLMs are biased towards similarity judgements based on kind, and struggle with size. Outcome is that prompting is the most effective steering approach and LLMs exhibit predisposed representational axes i.e. kind vs. size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The study moves beyond task specific accuracy to evaluate the representational alignment of steered LLMs with human cognition.\n\nApplication of triadic similarity judgment task to both humans and LLMs and comparing results.\n\nThe use of the Round Things Dataset allows for a controlled investigation of how task context selectively emphasizes one dimension over the other.\n\nComprehensive comparison of steering methods.\n\nDiscovery of inherent LLM bias towards a specific axis over another via the inclusion of neutral prompt."}, "weaknesses": {"value": "The study relies exclusively on triadic similarity judgment, this is good for controlled isolation, but the results may not generalize to more complex application. \n\nWould be interesting to see how larger models evaluate. \n\nOrder effects. The authors mention that the experiments were run using only a single overall ordering. I understand the reasoning but given the knowledge that LLMs do suffer from ordering bias, it would have been better to at least report on results that included a mitigation for this phenomenon."}, "questions": {"value": "Any expectations on how the experiments would perform on larger models, or models with different architectures?\nAny theory as to why alignment with human size judgments was consistently poor?\nComments/theory as to why prompting was the most successful technique?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZlL2AR6usk", "forum": "kkNr2niHtT", "replyto": "kkNr2niHtT", "signatures": ["ICLR.cc/2026/Conference/Submission21578/Reviewer_vtyN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21578/Reviewer_vtyN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944142427, "cdate": 1761944142427, "tmdate": 1762941840873, "mdate": 1762941840873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper evaluates LLM steering methods (prompt and activation) on Gemma-2 models using a similarity task on the Round Things dataset. Prompting gives the highest accuract and strongest alignment to human reps. Unsteered models default to kind-based similarity and are not well-aligned."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ I like the question of evaluating whether steering interventions produce human-like representations\n\n+ The task design seems carefully constructed, and the implementation details are quite extensive"}, "weaknesses": {"value": "- Accuracy appears to be defined against decisions induced by the fitted human embedding rather than some other more reasonable measure like majority human response on the same triplet. Is this circular?\n\n- The experiments are lacking some important details about statistics, specifically the results in Section 4 don't appear to specify the underlying model (is it logistic regression?), confidence intervals, how multiple comparisons are handled, etc.\n\n- Experiment is focused only on two Gemma-2 models. Also, steering does not include simple interventions like LoRA or SFT which would help to separate if prompting's advantage is just instruction-following ability rather than a limitation of activation steering"}, "questions": {"value": "- Can the authors report model accuracy against majority human triplet responses in addition to the current embedding labels?\n\n- Were the 200 prompt pairs used for layer selection in task vectors and DiffMean disjoint from the 2400 pairs used for final evaluation? \n\n- How sensitive are the results to the chosen injection layer and magnitude?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ppM2ikCPcq", "forum": "kkNr2niHtT", "replyto": "kkNr2niHtT", "signatures": ["ICLR.cc/2026/Conference/Submission21578/Reviewer_FSmf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21578/Reviewer_FSmf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998160126, "cdate": 1761998160126, "tmdate": 1762941840628, "mdate": 1762941840628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}