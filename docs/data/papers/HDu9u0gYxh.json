{"id": "HDu9u0gYxh", "number": 13755, "cdate": 1758222084582, "mdate": 1759897415436, "content": {"title": "LEAP: Learning Expert Adaptation & Pruning for Task-Specialized MoE Language Models", "abstract": "Most deployed large language model applications benefit more from specialized models than from ever-larger generalists. While Mixture-of-Experts (MoE) models learn specialists and activate only a subset of experts per token, they typically retain far more experts than needed for any specific task. This inflates inference latency and memory usage without proportional performance gains. \nWe present LEAP (Learning Expert Adaptation and Pruning), a principled framework that decouples model structure from behavior through agentic optimization. Our approach uses a meta--reinforcement-learning Pruning Agent to search the combinatorial space of expert subsets, optimizing for both performance and efficiency to identify compact, task-specific expert configurations. After pruning, we reconfigure the original router as a Routing Agent and train it using PPO. Additionally, Active Learning identifies the most informative, high-uncertainty samples to accelerate model recovery and specialization.\nWe evaluate LEAP on Llama 4 Maverick (17Bx128E) and Qwen3-235B-A22B across three diverse tasks: HumanEval (code generation), GSM8K (mathematical reasoning), and XSum (summarization). LEAP retains $>94\\%$ of the original model quality while using $8\\times$ fewer activated experts per token. This translates to up to $\\mathbf{2.5\\times}$ faster per-token inference, $0.31\\times$ FLOPs, and $\\sim40\\%$ lower peak memory usage compared to the full 128-expert models. Our method establishes a Pareto-dominant accuracy--compute frontier, consistently outperforming SoTA techniques including frequency-based pruning, magnitude-based pruning, and vanilla fine-tuning approaches.\nAblation studies demonstrate that learned pruning significantly outperforms heuristic methods, active learning reduces labeled data requirements by $2.1\\times$, and PPO-based routing is essential for maintaining post-pruning performance. By transforming expert selection and routing into a closed-loop, learnable process, LEAP provides a practical pathway to specialized, efficient MoE models and advances toward scalable, agentic optimization of expert systems.\nCode: https://anonymous.4open.science/r/LEAP2-4668", "tldr": "", "keywords": ["MoE", "LLM", "Pruning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/34bdd60467d040be5ce1b2144a8097fde657a2cd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces LEAP, a two-stage, agent-driven framework for task-specializing large MoE LLMs. A meta‑RL Pruning Agent searches the combinatorial space of expert subsets with a reward that trades off task performance and model size; the model is then physically pruned and reindexed. A Routing Agent fine-tunes the gating network over the retained experts using PPO, while an active-learning sampler prioritizes high-uncertainty and diverse examples to accelerate adaptation. On Qwen3‑235B‑A22B and Llama‑4 Maverick (128 experts), LEAP retains >94% of full-model quality with only 16 experts, delivering roughly 2.5× faster inference, ~0.31 relative FLOPs, and markedly lower memory. Against frequency/magnitude pruning, vanilla fine-tuning, and several recent MoE specialization methods, LEAP shows better accuracy–efficiency trade-offs; ablations attribute most of the gains to the RL-based pruning, with additional lift from PPO routing and active learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear, deployment-motivated objective: reduce latency/memory of large MoEs while preserving task performance; the paper reports practical metrics (ms/token, peak memory, relative FLOPs).\n- Coherent system design: neat “structure–behavior” decoupling with meta‑RL for subset discovery and RL for runtime routing, tied together by an AL loop; implementation details like expert reindexing and temperature renormalization are sensible.\n- Strong empirical trade-offs: on two distinct MoE backbones, the 16‑expert configuration consistently offers a good Pareto point, outperforming heuristic pruning and prior MoE-specialization baselines at matched expert budgets.\n- Thorough ablations: removing any of pruning, routing PPO, or AL degrades results, supporting the claim that the gains come from the integrated pipeline rather than a single tweak.\n- Practicality: use of lightweight adapters (LoRA) for expert updates keeps memory manageable and stabilizes joint fine-tuning."}, "weaknesses": {"value": "- System-level novelty: the contribution is mainly an integration of known pieces (PPO, meta-RL framing, entropy/diversity AL); stronger evidence that the closed loop beats non‑RL or one‑shot optimizers is needed.  \n- Baseline breadth and fairness: add competitive non‑RL searches (evolutionary/greedy, bilevel/Lagrangian) and a supervised/distilled router; clarify whether “vanilla FT” retrains the router and matches data/epochs with LEAP+AL.  \n- Hyperparameters and tuning: report actual values and procedures for α/β/λ, temperature T, KL/entropy targets, Gumbel noise; include sensitivity curves and recommended ranges.  \n- Reward computation and cost: specify the validation split, evaluation frequency, normalization of Perf(Et), and any approximations (proxy sets, early stopping, incremental scoring); discuss compute/noise trade‑offs.  \n- Theory is thin: the O(NT) discussion restates runtime without guarantees; convergence claims lack variance, stopping criteria, and seed‑to‑seed stability.  \n- Evaluation scope: results center on HumanEval, GSM8K, and XSum; please complement with newer, harder, and better‑curated suites to ensure robustness over time and distributions.  \n- End‑to‑end cost/practicality: provide pruning+routing GPU hours/energy and AL overhead; include a simple break‑even analysis relating training cost to inference savings.  \n- Measurement context: document batch size, KV‑cache policy, tensor/TP degrees, and interconnect when reporting latency/memory; add a brief sensitivity study to hardware/parallel settings.  \n- Stability and reproducibility: report variance across seeds, overlap of selected expert sets (e.g., Jaccard), and typical failure modes.  \n- Layer‑wise selection: current practice appears to share one retained set and prune only the final affine head; evaluate a layer‑wise variant with adaptive k′ℓ and quantify gains vs. cost.  \n- Routing behavior: analyze capacity/load‑balancing under Gumbel‑TopK (drops/overflows) and compare PPO routing with a supervised/distilled router trained on the same data.  \n- Active‑learning bias: compare several acquisition policies and report per‑round coverage/diversity statistics to rule out selection bias.  \n- Boundary regimes: examine very small budgets (≤8 experts), long‑context/retrieval/multi‑turn settings, and low‑data scenarios where the method may degrade.  \n- Generalization breadth: consider cross‑domain or multilingual tasks to support the “task‑agnostic” claim."}, "questions": {"value": "- Reward design and tuning\n  - Please report the exact values and tuning procedures for α, β in Eq.(4) and λ in Eq.(14), along with the temperature T, KL/entropy targets, and Gumbel noise settings. Could you provide sensitivity curves and recommended ranges per task?\n\n- Perf(Et) computation and validation protocol\n  - How is Perf(·) estimated during meta‑RL? What validation split, evaluation frequency, and normalization are used? Do you employ proxy sets, incremental scoring, or early stopping to control cost/noise? An ablation on reward-estimation fidelity vs. wall‑clock would help.\n\n- Training budget and cost–benefit\n  - Please provide end‑to‑end GPU hours/energy for pruning and routing, AL overhead, and a simple break‑even analysis: under typical serving loads, how long until training costs are amortized by the inference savings?\n\n- Baseline fairness and breadth\n  - For “vanilla fine‑tuning,” is the router frozen or trainable? Are data/epochs matched to LEAP+AL? Can you add strong non‑RL searches (evolutionary/greedy, bilevel/Lagrangian) and a supervised/distilled router trained on the same data to isolate the benefit of PPO/meta‑RL?\n\n- Stability and reproducibility\n  - Across seeds, how consistent are the selected expert sets (e.g., Jaccard overlap) and final metrics (mean±std)? Please include failure modes and the conditions that trigger them.\n\n- Layer‑wise selection and adaptive k′ℓ\n  - Current implementation appears to share one retained set and prune only the final affine head. What is the gain/cost of learning per‑layer expert sets I(ℓ) and adaptive k′ℓ? Even a small‑scale study would quantify headroom.\n\n- Routing behavior and capacity\n  - Under Gumbel‑TopK exploration, do you observe token drops/overflows or imbalance? Please report load‑balancing statistics and compare PPO routing with a supervised/distilled router.\n\n- Active learning policy analysis\n  - How does entropy+diversity compare to uniform sampling and other AL policies? Please report coverage/diversity metrics across AL rounds and the impact on sample efficiency and final accuracy.\n\n- Evaluation scope and freshness\n  - Results focus on HumanEval, GSM8K, and XSum. Could you complement with newer and harder benchmarks (e.g., recent sanitized code/math suites and longer/varied summarization or instruction sets) to ensure robustness over time and distributions?\n\n- Measurement context and portability\n  - For latency/memory, please specify batch size, KV‑cache policy, tensor/TP degrees, and interconnect, and include a short sensitivity study across hardware/parallel settings.\n\n- Very small budgets and long‑context regimes\n  - Performance drops sharply at ≤8 experts. What drives the decline (expert interactions, routing instability, capacity)? How does LEAP behave under long‑context, retrieval‑augmented, or multi‑turn tasks?\n\n- Theory and guarantees\n  - Beyond the O(NT) runtime, can you offer convergence diagnostics (criteria, variance) or an optimality‑gap bound/empirical proxy? Even negative results would clarify scope.\n\n- Generalization breadth\n  - Any evidence on cross‑domain or multilingual transfer, or few‑shot data regimes? This would substantiate the “task‑agnostic” claim.\n\n- Reproducibility artifacts\n  - Will you release configs/logs/checkpoints (with anonymized links) sufficient to reproduce the pruning search, routing PPO, and AL selection? A minimal training recipe would be valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wxnCAOkTJy", "forum": "HDu9u0gYxh", "replyto": "HDu9u0gYxh", "signatures": ["ICLR.cc/2026/Conference/Submission13755/Reviewer_C38p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13755/Reviewer_C38p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761386427896, "cdate": 1761386427896, "tmdate": 1762924288434, "mdate": 1762924288434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LEAP, a method that transforms large general-purpose MoE models into compact, task-specialized variants. The approach uses a meta-reinforcement learning pruning agent to identify optimal expert subsets, followed by a routing agent trained with PPO and active learning to adapt the routing mechanism to the pruned architecture."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The core idea of using meta-RL for expert selection is solid and addresses real limitations of frequency/magnitude-based pruning heuristics. The results are impressive and the efficiency gains are substantial. The ablations clearly show each component matters, which strengthens the technical claims."}, "weaknesses": {"value": "(1) While the paper demonstrates LEAP's effectiveness on three tasks (code generation, mathematical reasoning, and summarization), this evaluation scope appears somewhat narrow for establishing the generalizability of the approach. The selected tasks all involve text generation and may not capture the full challenges in MoE specialization. It will be better to include evaluations on additional domains like question answering. \n\n(2) There is limited discussion of the actual computational cost required to train this meta-RL agent. The paper mentions 500 episodes for convergence (Section 4.3), but does not report the training time or the number of full model evaluations required during the pruning search phase. Given that each episode likely involves evaluating candidate expert subsets on validation data, this could represent a substantial cost that may limit the practical applicability of LEAP.\n\n(3) LEAP introduces numerous hyperparameters (like α, β, etc.) across its components, but the paper provides limited discussion of sensitivity to these choices for setting them on new tasks. The question of how to tune these hyperparameters on new tasks without extensive trial-and-error remains unaddressed.\n\n(4) While the paper acknowledges the possibility of per-layer expert subsets through the notation I^(ℓ), the primary implementation and all reported experiments appear to use a single, globally shared expert subset E* across all MoE layers (Section 3.2: \"in the simplest case, I^(ℓ) = E* for all ℓ\"). This assumes that the same experts are optimal for all layers, which may be overly restrictive because different transformer layers often serve distinct computational roles (early layers extract low-level features while deeper layers perform task-specific reasoning)."}, "questions": {"value": "I tried to access the code in the anonymous github repo, but got the 'File not found' error. This may occur due to an upload synchronization issue. The author should fix this issue in the next version."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cfZXhCOIe6", "forum": "HDu9u0gYxh", "replyto": "HDu9u0gYxh", "signatures": ["ICLR.cc/2026/Conference/Submission13755/Reviewer_j7UM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13755/Reviewer_j7UM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641481373, "cdate": 1761641481373, "tmdate": 1762924288116, "mdate": 1762924288116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The Mixture-of-Experts (MoE) architecture faces challenges such as high memory consumption and redundancy in expert modules. Addressing the issue of MoE pruning, this paper proposes the LEAP (Learning Expert Adaptation and Pruning) framework—a systematic framework that decouples model structure from model behavior through agent optimization. This method leverages a meta-reinforcement learning Pruning Agent to explore the combinatorial space of expert subsets, while optimizing both performance and efficiency to identify a compact, task-specialized expert configuration. Its superiority is demonstrated through comparisons with various baselines, including Full MoE, Frequency Pruning, Magnitude Pruning, and Vanilla Fine-Tuning, on Qwen3-235B-A22B and Llama 4 Maverick."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper innovatively formulates the pruning problem as a policy optimization task. By transforming the expert selection and routing process into a closed-loop, learnable workflow, it adopts a joint objective function to optimize the trade-off between performance and efficiency. The proposed method is insightful, particularly the RL Pruning Agent."}, "weaknesses": {"value": "- The authors integrate the RL Pruning Agent, data, and RL Router through a joint objective function. What is the necessity of this integration (please explain the interrelationships among the components)? Please elaborate on its advantages compared to the simple combination of A, B, and C (i.e., A+B+C).\n\n- Specific experimental details of comparative experiments (e.g., with MoE-Pruner) are missing, including information on the backbone, data, and pruning strategy. Were the results reproduced under consistent basic configurations? Please supplement relevant explanations. For instance, does MoE-Pruner adopt pure pruning or a combination of pruning and distillation?\n\n- The ablation experiments for the LEAP method are severely insufficient. For example, Table 7 lacks a comparison between the LEAP pruning model and pure pruning methods to validate the superiority of the proposed Pruning Agent (both Tables 5 and 7 only present comparisons between Full LEAP and pure pruning methods). Additionally, due to biases in data distribution, the advantages of data selection cannot be sufficiently demonstrated with only a small number of benchmarks; instead, they should be validated across more benchmarks to prove effectiveness. In fact, there is doubt that the performance improvement brought by LEAP stems from RL fine-tuning. A comparative experiment with Vanilla RL should be added to confirm its superiority. Consequently, the effectiveness of the proposed method cannot be verified based on the existing experiments. Furthermore, the paper lacks specific experimental details (including data and hyperparameters) for Vanilla Fine-Tuning.\n\n- Could more training details (such as reward and loss curves of the RL training process) be provided to enhance the credibility of RL-related methods?\n\n- The authors focus on task-specialized performance. Would post-training with small-scale models be a viable alternative? To address this question, post-training experiments with medium- and small-scale models of equivalent size (either Dense or MoE) should be supplemented."}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0fvG6oqCzK", "forum": "HDu9u0gYxh", "replyto": "HDu9u0gYxh", "signatures": ["ICLR.cc/2026/Conference/Submission13755/Reviewer_jdSZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13755/Reviewer_jdSZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789306381, "cdate": 1761789306381, "tmdate": 1762924287740, "mdate": 1762924287740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the expert pruning problem in Mixture-of-Experts (MoE) models, this paper proposes a novel framework named Learning Expert Adaptation and Pruning (LEAP), which transforms MoE optimization into a learnable, agentic process rather than relying on static heuristics. Specifically, it comprises three components: Pruning Agent, Active Learning, and Joint Fine-tuning. Results are analyzed from three perspectives: Task Performance, Efficiency, and Inference Latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper proposes a novel framework named Learning Expert Adaptation and Pruning (LEAP), which transforms the optimization of Mixture-of-Experts (MoE) models into a learnable, agentic process instead of relying on static heuristics. Specifically, it comprises three components: Pruning Agent, Active Learning, and Joint Fine-tuning.\n\nThis paper innovatively frames expert pruning as a policy optimization problem and addresses it with RL methods."}, "weaknesses": {"value": "1. The comparative experiments lack comparative analysis with recent works (e.g., MoNE). There is a deficiency in detailed configuration information for the reproduction of baseline-related experiments, and the relevant evaluation metrics are insufficient. It is expected that supplementary comparative experiments and detailed explanations will be provided.\n2. Could a comparison between the Pruning Agent and pure pruning methods be provided to demonstrate its superiority?\n3. There is a lack of comparative experiments with Vanilla RL, and the current improvements observed may stem from the inherent advantages of RL itself. Please provide the training details of RL.\n4. The Pruning Agent strategy that relies on specific datasets may result in pruned LLMs being strongly dependent on the dataset itself. Could this issue be discussed?\n5. Is the data strategy of Active Learning fair for other comparative experiments?"}, "questions": {"value": "No further questions, see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "prvhNdcT8z", "forum": "HDu9u0gYxh", "replyto": "HDu9u0gYxh", "signatures": ["ICLR.cc/2026/Conference/Submission13755/Reviewer_ATun"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13755/Reviewer_ATun"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13755/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893025004, "cdate": 1761893025004, "tmdate": 1762924287323, "mdate": 1762924287323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}