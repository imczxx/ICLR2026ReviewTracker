{"id": "Vb4nE3WWf5", "number": 11891, "cdate": 1758204506698, "mdate": 1759897548245, "content": {"title": "Unified In-Context Video Editing", "abstract": "Recent advances in text-to-video generation have sparked interest in generative video editing tasks. Previous methods often rely on task-specific architectures (e.g., additional adapter modules) or dedicated customizations (e.g., DDIM inversion), which limit the integration of versatile editing conditions and the unification of various editing tasks. In this paper, we introduce UNified In-Context Video Editing (UNIC), a simple yet effective framework that unifies diverse video editing tasks within a single model in an in-context manner. To achieve this unification, we represent the inputs of various video editing tasks as three types of tokens: the source video tokens, the noisy video latent, and the multi-modal conditioning tokens that vary according to the specific editing task. Based on this formulation, our key insight is to integrate these three types into a single consecutive token sequence and jointly model them using the native attention operations of DiT, thereby eliminating the need for task-specific adapter designs. Nevertheless, direct task unification under this framework is challenging, leading to severe token collisions and task confusion due to the varying video lengths and diverse condition modalities across tasks. To address these, we introduce task-aware RoPE to facilitate consistent temporal positional encoding, and condition bias that enables the model to clearly differentiate different editing tasks. This allows our approach to adaptively perform different video editing tasks by referring the source video and varying condition tokens \"in context\", and support flexible task composition. To validate our method, we construct a unified video editing benchmark containing six representative video editing tasks. Results demonstrate that our unified approach achieves comparable performance with task specialists and exhibits emergent task composition abilities.", "tldr": "a parameter-efficient and unified framework for video editing tasks", "keywords": ["video editing; video generation; diffusion models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/06bf17c69f7a87b2914254416cd603a680efcdb5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Whereas prior video editing methods often require inefficient video inversion or the design and training of complex control modules, this paper proposes an in-context approach based on a recently introduced DiT-based video generation model. The method concatenates multiple conditional signals and performs full attention over them, enabling a single unified model to handle diverse conditional editing tasks. To further support such a unified model, the authors introduce condition bias and task-aware RoPE, techniques that help distinguish different conditioning signals and improve performance. They also propose a sequential training scheme to make unified training feasible."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Unified model for diverse video editing tasks:** The paper models various video editing tasks under a single formulation and demonstrates that learning a unified video editing model is feasible. The two new techniques introduced specifically for unified training are effective and constitute an additional strength.\n- **Timely extension to video:** As development is progressing from image models toward video models, the proposed work arrives at an opportune time.\n- **Clear and intuitive writing:** The paper is easy to follow, and the motivations behind the methods are clear and intuitive."}, "weaknesses": {"value": "- **Slightly limited novelty:** Although the need to distinguish task-specific conditional signals in unified training is well-motivated and the proposed techniques appear appropriate, the overall design feels like a natural extension from a single signal to multiple signals. Moreover, the proposed methods do not seem tightly tailored to video editing, which makes the contribution feel somewhat less novel.\n\n- **Lack of detailed explanation for method and experiments:**\n    The sequential training scheme appears crucial to performance based on the presented results, yet it is not discussed in the method section. In addition, the paper does not specify which TTV base model is used during fine-tuning.    \n    Furthermore, in Table 6, the performance differences between D2–D4 and the baseline (D1) appear marginal depending on the metric, making it difficult to appreciate the improvements. Additional qualitative comparisons would help clarify the effectiveness of the two proposed components."}, "questions": {"value": "- In L473–474, what does “temporal modeling” refer to? Task-aware RoPE seems to improve reference alignment (as Table 6 suggests), whereas “temporal modeling” sounds more like a measure of video quality, making its use here somewhat unclear.\n- In L102, what is meant by “index collisions”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9BOZUP445l", "forum": "Vb4nE3WWf5", "replyto": "Vb4nE3WWf5", "signatures": ["ICLR.cc/2026/Conference/Submission11891/Reviewer_4wrd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11891/Reviewer_4wrd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746650916, "cdate": 1761746650916, "tmdate": 1762922905365, "mdate": 1762922905365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UNIC, a simple, adapter-free framework that unifies diverse video-editing tasks, such as ID insert/delete/swap, stylization, first-frame propagation, and re-camera control within one model by concatenating three token types (noisy video latent, reference-video tokens, and multi-modal condition tokens) into a single sequence processed by a DiT with full attention. UNIC features two key designs: Condition Bias (task-type embeddings) and Task-aware RoPE (per-task positional indexing), which mitigate task ambiguity and alignment conflicts. On a six-task benchmark, UNIC reports competitive or superior results to task specialists, shows emergent task composition, and includes ablations and efficiency analyses (e.g., step-cache)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. UNIC reformulates many video editing tasks as tokenized conditions concatenated with noisy and reference tokens.\n2. UNIC supports six representative tasks and demonstrates emergent compositions (e.g. stylization + re-camera)."}, "weaknesses": {"value": "1. I think the paper could benefit from reorganizing its structure for improved presentation. One of the most important parts of training video editing models is in its data construction pipeline, since paired before-and-after editing data is often difficult to obtain. Right now the main text barely has any description of training data, and all of descriptions are in the appendix.\n2. Following weakness 1, there are no statistics about the quantity of the training data in the main text. In Appendix D, there are statistics for some tasks, but not all tasks.\n3. While it is clear that both condition bias and task-aware RoPE benefit model performance, it seems that combining these two designs leads to weaker performance on the style transfer task, and the results of ArtFID and CFSD are even worse than not having the two designs at all. Can the author provide more intuition on this issue?"}, "questions": {"value": "1. In appendix E.3, \"All finetuning experiments start from a pre-trained model with 1B parameters and 28 sequential standard Diffusion Transformer (DiT) blocks.\" What is this base pretrained model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ServXaYb4s", "forum": "Vb4nE3WWf5", "replyto": "Vb4nE3WWf5", "signatures": ["ICLR.cc/2026/Conference/Submission11891/Reviewer_CmCf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11891/Reviewer_CmCf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862039549, "cdate": 1761862039549, "tmdate": 1762922904531, "mdate": 1762922904531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed UNIC, a finetuning framework that enables a video diffusion model to perform a wide range of video editing tasks in an in-context fashion without task-specific modules, adapters, or dedicated inversion pipelines in prior works. A unified video editing benchmark is built and reported UNIC achieves comparable performance with task specialists on the benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1) The framework is simple that it does not require any architecture changes or inversion overhead. By simply concatenating tokens for any desired editing behavior in finetuning, the framework enables new combinations and hybrid tasks. This highlights the originality.\n\nS2) This paper recognizes and operationalizes the fundamentally different natures of video editing tasks, then abstracts them into a unified, learnable token-based framework. Its treats task diversity as token diversity. The motivation is justified.\n\nS3) Smooth transitions in paper writing."}, "weaknesses": {"value": "W1) Inversion-free approaches like FlowEdit (ICCV 2025) or other direct injection or projection-based methods (where editing is possible without explicit reference-to-noise inversion or heavy architectural mods) are not discussed. The omission is notable, especially as the field moves rapidly toward more general and efficient editing without inversion bottlenecks. Adding the discussion inside would help contexturalize the problem. A more complete landscape would compare: 1) Inversion-based, 2) Adapter-based, 3) Inversion-free, and 4) Unified token-based models.\n\nW2) The paper did not mention which exact video diffusion model was tested in the context. It only mentioned finetuning on a general DiT-based video diffusion model pretrained with flow matching. It remains unclear for us to verify if this can be applied to general cases. Providing the details of the experiment would have enhanced the reproducibility."}, "questions": {"value": "Q1) W2 also raise an interesting question: If such a framework is finetuned on CogVideoX and also Wan 2.2, is it we should expect models with stronger pretrained video diffusion model would perform better? Or the gain is heavily relied on the finetuning? Would it be possible to share some empirical results?\n\nQ2) Why the in-text citations style are inconsistent across the paper? \n\nStyle 1 in Introduction: \"Current video editing methods primarily follow two strategies to inject reference video and control signals. As depicted in Fig. 2, one stream of methods, represented by Video-P2P (Liu et al., 2024b), AnyV2V (Ku et al., 2024), and FLATTEN (Cong et al., 2023), utilizes DDIM inversion for noise initialization to preserve the main structure of the reference video.\"\n\nStyle 2 Related Works: \"Similarly, FLATTEN Cong et al. (2023) utilizes optical flow to identify keypoints and injects their features to maintain motion fidelity. AnyV2V Ku et al. (2024) also leverages spatial, temporal, and CNN features gathered during inversion. While these approaches excel at retaining reference video information, they inherently require an additional stage for inversion, thereby increasing the overall inference cost and computational overhead.\"\n\nThe authors need to fix the in-text citations style (preferably style 1)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EIZsifZaNw", "forum": "Vb4nE3WWf5", "replyto": "Vb4nE3WWf5", "signatures": ["ICLR.cc/2026/Conference/Submission11891/Reviewer_MK6C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11891/Reviewer_MK6C"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927164523, "cdate": 1761927164523, "tmdate": 1762922904084, "mdate": 1762922904084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UNIC (UNified In-Context Video Editing)， a framework for unifying diverse video editing tasks (e.g., ID insertion/deletion/swap, stylization, re-camera control, propagation) within a single diffusion transformer model. \nInstead of relying on task-specific adapters or DDIM inversion, UNIC concatenates noisy video, reference video, and multi-modal condition tokens into a single token sequence, enabling unified learning through native transformer attention. Experiments on a unified benchmark of six tasks show that UNIC achieves competitive or superior results to baselines like VACE, AnyV2V, and ReCamMaster, while offering emergent task composition and parameter efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of in-context unification across multiple video editing tasks via token concatenation is elegant and clear\n\n2. The introduction of Condition Bias and Task-Aware RoPE is well-motivated and clearly implemented.\n\n3. The benchmark spans six distinct tasks, covering both local (ID editing) and global (stylization, propagation) settings. it is comprehensive .\n4. The demonstration of task composition is interesting, highlights that the model generalizes beyond discrete training tasks"}, "weaknesses": {"value": "1. The paper doesn’t compare against or reference newer state-of-the-art T2V and editing models like Wan series\n\n2. While the in-context formulation is elegant, much of the architecture directly borrows from existing full-attention DiT and OmniGen paradigms. The novelty primarily lies in combining these ideas for video, rather than a fundamentally new mechanism.\n\n3. Missing some important related work discussion like VEGGIE [1], which is also unified video editing framework + abilities in in-context video editing \n\n4. Some metrics (e.g., CLIP, DINO, ArtFID) are weak proxies for perceptual quality. The paper doesn’t include human evaluation or temporal consistency metrics (like VBench perceptual coherence, even though it is for video generation, it still captures the generation quality of a video editing model).\n\n[1] VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded Generation. ICCV25."}, "questions": {"value": "please see weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WMEvQMYn1g", "forum": "Vb4nE3WWf5", "replyto": "Vb4nE3WWf5", "signatures": ["ICLR.cc/2026/Conference/Submission11891/Reviewer_njmg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11891/Reviewer_njmg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136189974, "cdate": 1762136189974, "tmdate": 1762922903578, "mdate": 1762922903578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}