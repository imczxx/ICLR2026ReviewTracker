{"id": "UF1pV5JObZ", "number": 7438, "cdate": 1758021900927, "mdate": 1759897852821, "content": {"title": "Unlocking Universal Graph Knowledge in the View Space", "abstract": "Unlike recent foundation models in NLP and CV, which generalize across datasets through standardized inputs, graph foundation models remain limited to datasets with compatible feature spaces. Node features vary in both dimension and semantics across datasets, and many graphs lack features altogether, making parameter reuse for a graph-based model infeasible. To address this limitation, we introduce the view space, a new axis of graph representation where graphs of arbitrary sizes and feature specifications can be universally encoded. Building on this foundation, we propose Graph View Transformation (GVT), a parametric mapping that can transform the node-feature matrix of any graph through this shared space. We then construct Recurrent GVT (RGVT), a graph foundation model that serves as a substrate for universally shareable graph knowledge. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, RGVT outperforms GraphAny, the only universal graph model to date, by +8.93\\% and surpasses 12 individually tuned GNNs by at least +3.30\\%. These results demonstrate that learning in the view space is a theoretically grounded and practically effective framework for universal graph representation learning (UGRL). Code and datasets are available at https://anonymous.4open.science/r/RGVT.", "tldr": "We introduce the view space, a third axis of graph representation, where transformations along this axis capture universal graph knowledge transferable across graphs with arbitrary features.", "keywords": ["Graph Neural Networks", "Graph Foundation Models", "View Space", "Universal Graph Representation Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1bb521fa4b180a848acd5b6149652a57ac0f0b28.pdf", "supplementary_material": "/attachment/501239495e1a5421877857f92cea08096e2e5c3d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new way to train inductive gnn model by projecting the original features into a view space. By learning a shared projecter, the learned representation can be projected to desired task dimension through weight sharing. A recurrent variant is further adopted to deal with tasks requiring different hops."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. the problem is interesting\n2. the presentation is generally good \n3. the performance boost over graphany is clear, which demonstrates that universal representation is much better than directly aggregating the closed-form solution"}, "weaknesses": {"value": "1. I don't agree with the claim in the abstract. Graphany is obviously not the only universal graph model. First, you can't say it's universal. graph is not just about node classification. In their papers, they also don't claim they are graph foundation models (they just say fully-inductive classification). Second, if graphany is universal, they i don't see why others like OneForAll are not universal. \n2. In the introduction part, the motivation makes me feel like the authors think the feature heterogeneity is the main problem of building a graph foundation model. I strongly disagree with this viewpoint. FIrst, it's obvious that structure heterogeneity is much harder. For example, from a geometric perspective, the 1-order (entity-level) and 2-order (link-level) tasks are not compatible. Moreover, the homophily-heterophily problem is very challenging to solve. Second, in practice, the feature heterogeneity may be not a \"real\" problem. There are many relational foundation mdoels like kumorfm and tabular foundation models that can work on heterogeneous feature types. You just need a type-aware encoder, which works generally well. \n3. In the introduction part, authors point out that the way OneForAll takes results in clear performance loss. I also need to point out that the way GraphAny leads to even more performance loss by the inductive transformation. \n4. Section 3 is in general identical to Graphany. \n5. One obvious drawback of recurrent one is the expressivenss problem, You have to weight share the parameters across layers, and non-identical weight learning for heterophilous graph is very important (for example, in EvenNet) \n6. the experimental dataset are selected with relatively weak features. Vanilla GNN may be much better with better features like LLM-encoded ones. \n7. THis model can only work for node classification tasks, and can't do in-context learning, zero-shot learning, generation. I would say it's far away from a foundation model."}, "questions": {"value": "What's the main design component that makes GVT so much better than graphany? Better explain this in experiment sections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0adDrgZa0J", "forum": "UF1pV5JObZ", "replyto": "UF1pV5JObZ", "signatures": ["ICLR.cc/2026/Conference/Submission7438/Reviewer_M8Kc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7438/Reviewer_M8Kc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760811922217, "cdate": 1760811922217, "tmdate": 1762919556249, "mdate": 1762919556249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles cross-dataset generalization for graph learning by introducing a “view space” that can encode graphs of any size and feature specification, enabling parameter sharing across incompatible node feature spaces. The authors propose Graph View Transformation (GVT), a learned mapping that projects any node-feature matrix through this shared space, and build Recurrent GVT (RGVT) on top as a foundation model for universally shareable graph knowledge. Trained on OGBN-Arxiv and tested on 27 node classification benchmarks, RGVT outperforms GraphAny and beats a dozen tuned GNN baselines. The results suggest that learning in the view space provides a principled and effective route to general node classifier, with code and data released for reproducibility."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The heterogeneous feature space is an important problem for GFM. The proposed view-space formulation is a compact and novel solution.\n2. Theoreticall study on properties and expressivity results help justify the design.\n3. Extensive experiments show consistent improvements with statistical significance\n4. Writing is concise, figures are readable, and the narrative is easy to follow."}, "weaknesses": {"value": "1. The statement given by the title is too strong. The proposed method provides a general model for node classification. Edge level and graph level tasks are not explored, and coverage of graphs without node features or with edge features is not addressed.\n2. The choices for view finders is unclear. The paper does not clearly state how many views are used, how each view is constructed, or how sensitive performance is to these choices.\n3. There is no complexity analysis. It would be better to provide a comparison of the the training and inference cost with GraphAny and other baselines."}, "questions": {"value": "1. Is there a theoretical or empirical guideline for selecting the number of views?\n2. Can the framework extend to graphs without node features, to graphs with edge features, and to other downstream tasks such as link prediction and graph classification? If so, what changes are required in the view transformation or the predictor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ct0vb3N81G", "forum": "UF1pV5JObZ", "replyto": "UF1pV5JObZ", "signatures": ["ICLR.cc/2026/Conference/Submission7438/Reviewer_vCdh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7438/Reviewer_vCdh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766044649, "cdate": 1761766044649, "tmdate": 1762919555682, "mdate": 1762919555682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a critical problem for graph foundation models: datasets often have incompatible features. The authors solve this by proposing the view space, a novel representation axis that is independent of node features. They then introduce Graph View Transformation (GVT), a theoretically universal function that learns representations within this new space. GVT serves as the core building block for their RGVT foundation model. Impressively, experiments show the pre-trained, frozen RGVT model surpassed 12 specialized GNNs that were individually tuned for their specific tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novelty: The paper introduces the \"view space,\" a new representation paradigm for graphs. This is a significant conceptual leap, as it bypasses the central problem of feature heterogeneity (incompatible dimensions and semantics) rather than trying to fix it with alignment or projection, which is what most prior work does.\n2. Theoretical Grounding: The authors provide a solid theoretical foundation for their method. They mathematically prove that the proposed Graph View Transformation (GVT) achieves \"dual permutation equivariance\" (for both nodes and features), which formally guarantees its universality across graphs with arbitrary feature specifications.\n3. Strong Results: The experiments provide compelling evidence for the \"foundation model\" claim. A single, pre-trained, and frozen RGVT model, with only a lightweight predictor, was able to outperform 12 different, specialized GNN models that were fully and individually tuned for each of the 27 downstream tasks. This demonstrates a remarkable level of generalizable knowledge transfer."}, "weaknesses": {"value": "1. Overclaim. This manuscript claims that the proposed model is the first graph foundation model that is unlimited by the feature space barrier, enabling universal knowledge transfer. However, recent advances in text-free multi-domain graph pre-training generally do not struggle with the feature space heterogeneity, supporting the knowledge transfer across different graphs. Also, SAMGPT (WWW25) can be viewed as an initial success of GFM in this direction.\n2. Miss of related work. The authors state that `` GraphAny, the only universal graph model to date\". However, there has been a series of GFM attempts very recently.\n3. Weak evaluations. RGVT is validated in the node classification task. However, as a general-purpose foundation model, important tasks, such as link prediction, graph classification, and node clustering, are touched on in the experiment."}, "questions": {"value": "1. How does the model scale for graphs with no features?\n2. GVT processes each feature channel independently. How does the model learn interactions between different features if they never mix?\n3. By treating all features identically (to ensure feature equivariance), doesn't the model lose the ability to apply feature-specific logic, such as processing categorical and numerical features differently?\n4. How sensitive is performance to the specific set of view finders?\n5. How does the model's computational cost scale with a large number of features? A runtime benchmark against standard GNNs (which project the feature dimension to a small dimension) is missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DVGvmcsEJr", "forum": "UF1pV5JObZ", "replyto": "UF1pV5JObZ", "signatures": ["ICLR.cc/2026/Conference/Submission7438/Reviewer_85dN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7438/Reviewer_85dN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7438/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925028386, "cdate": 1761925028386, "tmdate": 1762919554948, "mdate": 1762919554948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}