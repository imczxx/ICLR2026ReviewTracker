{"id": "0vbYakkECY", "number": 4243, "cdate": 1757646215215, "mdate": 1759898044063, "content": {"title": "Draft-based Approximate Inference for LLMs", "abstract": "Optimizing inference for long-context large language models (LLMs) is increasingly important due to the quadratic compute and linear memory costs of Transformers. Existing approximate inference methods, including key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on coarse predictions of token or KV pair importance. We unify and extend recent work by introducing a framework for approximate LLM inference that leverages small draft models to more accurately predict token and KV pair importance. We provide novel theoretical and empirical analyses justifying lookahead-based importance estimation techniques. Within this framework, we present two new instantiations: (i) **SpecKV**, the first method to use lookahead with a small draft model to enable precise KV cache dropping, and (ii) **SpecPC**, which leverages draft model attention activations to identify and discard less important prompt tokens. Extensive experiments on long-context benchmarks demonstrate that our methods consistently achieve higher accuracy than existing baselines while retaining the same improvements in memory usage, latency, and throughput.", "tldr": "", "keywords": ["long-context", "sparse attention", "KV cache eviction", "prompt compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d50d2e93b0f6b3c4f1b7490d1017b89117edc9f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Draft-based Approximate Inference (DAI), a framework that leverages a small draft model to generate a short sequence of lookahead tokens, which are then used to estimate token or KV importance in large language models (LLMs).\nUnder this framework, the authors instantiate two methods:\n\nSpecKV — for lookahead-guided KV cache dropping\n\nSpecPC — for prompt compression\n\nThe key idea is to use the draft model’s predicted future queries to obtain a more accurate estimate of which tokens will be attended to in future steps, enabling more effective prefill-time cache compression.\nEmpirical results on RULER and LongBench show that the proposed methods outperform prior works such as SnapKV, LAQ++, and SpecPrefill, with reduced memory and latency.\nThe paper also provides theoretical bounds relating draft embedding quality to attention approximation error."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  Clean and implementable idea:\nExtends speculative decoding to approximate inference in a conceptually neat way, requiring minimal system changes.\n\n2.  Strong empirical results:\nOn both synthetic and long-context benchmarks, SpecKV yields clear gains (1–2 points improvement) over existing KV compression baselines, with ~40–50% memory savings.\n\n3.  Solid theoretical justification:\nProvides error bounds connecting the draft model’s embedding deviation to importance estimation error, filling a theoretical gap that earlier methods (e.g., SnapKV, LAQ++) lacked.\n\n4.  System efficiency:\nThe overhead of lookahead generation is minimal (<10% in prefill), while decoding latency and memory are significantly reduced."}, "weaknesses": {"value": "1. Limited conceptual novelty:\nThe main idea—using a smaller model to predict future attention patterns—is a natural extension of speculative decoding (FastGen, Medusa) and prior KV-dropping methods (SnapKV, LAQ++).\nThe improvement lies mainly in integration and theoretical refinement, not in introducing a new paradigm.\n2. Marginal accuracy gains:\nImprovements on benchmarks are moderate (1–2%), suggesting the practical benefit mainly comes from efficiency rather than substantial modeling advance."}, "questions": {"value": "How sensitive are the results to $n_{lookahead}$? The ablation in Appendix E.4 is informative, but further scaling analysis would be valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tbLZTt5HH4", "forum": "0vbYakkECY", "replyto": "0vbYakkECY", "signatures": ["ICLR.cc/2026/Conference/Submission4243/Reviewer_tK1Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4243/Reviewer_tK1Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761463889333, "cdate": 1761463889333, "tmdate": 1762917251130, "mdate": 1762917251130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework called draft-based approximate inference to optimize the efficiency of LLM in long context decoding. Existing methods, which approximate KV-cache discarding, sparse attention, and prompt compression, usually rely on rough prediction when estimating the importance of tokens or KV pairs. In contrast, the core idea of this paper is to use a small and cheap \"draft model\" to generate approximate future output (lookahead), so as to more accurately predict the importance of the current token or KV pair.\n\nIn this paper, two methods are introduced: \n\n1. SpecKV, whose goal is to use the \"look ahead\" capability of the draft model to more accurately discard unimportant parts in the kV cache, and combine sparse pre-filling to improve efficiency.\n\n2. SpecPC, whose goal is to use the draft model to directly determine which tokens in the input prompt are not important, and compress the prompt before sending it to the target model.\n\n\nA large number of experiments on the benchmark of rule and longbench show that speckv and specpc continuously achieve higher accuracy than the existing baseline methods under the fixed KV cache or prompt size limit."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides theoretical (Theorems 1 and 2) and experimental evidence to support the effectiveness of the \"look ahead\" based importance estimation. \n\n2. This framework unifies and extends the idea of using approximate future information to improve token importance estimation.\n\n3. The paper claims that its method achieves the current state-of-the-art accuracy in the long context benchmark under the constraint of a fixed KV cache or prompt size.\n\n4. Even if a weak draft model is used, this paper can perform well, far exceeding the performance of the draft model itself. Also, using a better draft model will further improve performance."}, "weaknesses": {"value": "1. The core of the whole framework is to use the draft model to approximate the behavior of the target model. Both theoretical analysis (Theorem 1) and experimental results (Fig. 10) show that the accuracy of the draft model directly affects the final performance. If a draft model that is small (low overhead) and similar enough to the target model (high accuracy) cannot be found, the effect of SpecKV and SpecPC may be compromised.\n\n2. Compared with methods such as SnapKV, which only pre-fills and compresses the target model once, the pre-filling steps of SpecKV are more complex, and the calculation cost is higher. Although the paper claims that the overall delay is reduced, this is mainly the benefit of the decoding stage.\n\n3. This paper also introduces a new memory occupation: it needs to load and store the weight of the draft model. Although this overhead is fixed and does not increase with the sequence length as KV cache, it is still an additional memory burden compared with methods that do not require a draft model.\n\n4. The main idea is similar to speculative decoding and previous LAQ++. It is more likely a technical extension.\n\n\nI have a borderline opinion on this paper and hope to see the rebuttal and other reviewers' comments."}, "questions": {"value": "How does SpecKV reduce the peak memory than LAQ++? In Algorithm 1, the target model still needs to store all the KV cache of the input sequence x and draft output y_draft."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vkGtve9tCR", "forum": "0vbYakkECY", "replyto": "0vbYakkECY", "signatures": ["ICLR.cc/2026/Conference/Submission4243/Reviewer_QfmW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4243/Reviewer_QfmW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622939927, "cdate": 1761622939927, "tmdate": 1762917250724, "mdate": 1762917250724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an effective \"Draft-based Approximate Inference\" framework that leverages a draft model for lookahead to more accurately estimate the importance of tokens and KV pairs, thereby improving long-context approximate inference. The authors further introduce SpecKV and SpecPC for KV cache dropping and prompt compression, respectively, and provide a thorough error analysis. A comprehensive set of experiments further demonstrates the effectiveness and application potential of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. By using a draft model to estimate the importance of tokens in the KV cache and prompt, the method achieves strong performance under controllable complexity.\n2. This work provide a clear theoretical analysis, demonstrating how embedding errors influence KV importance estimation errors (Theorem 1), and how output approximation under RIP or more general assumptions can upper bound attention approximation errors (Theorems 2 and 3).\n3. The experiments on LongBench and RULER Benchmarks are solid, with results that convincingly demonstrate the effectiveness of the proposed methods."}, "weaknesses": {"value": "1. For different input embeddings, are there any limitations to the applicability of Theorem 2?\n2. There appear to be some typo errors in Table 2."}, "questions": {"value": "1. I am curious whether using different types of draft models and target models would affect token selection (e.g., Qwen-2.5-0.5B + Llama-3-70B). Does this imply that draft models must be selected from the same model family as the target model?\n2. As shown in Figure 2, the benefits gained from the draft model vary across different tasks. Any explaination?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N8Slj3gPbO", "forum": "0vbYakkECY", "replyto": "0vbYakkECY", "signatures": ["ICLR.cc/2026/Conference/Submission4243/Reviewer_bFv3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4243/Reviewer_bFv3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920400273, "cdate": 1761920400273, "tmdate": 1762917250081, "mdate": 1762917250081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Draft-based Approximate Inference, leveraging lightweight draft models for lookahead-based token/KV importance estimation. It introduces SpecKV for KV cache dropping with sparse prefill and SpecPC for prompt compression, both supported by theoretical proofs and extensive experiments showing superior accuracy–efficiency trade-offs over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. First integration of draft-model lookahead into KV dropping and prompt compression, with theoretical justification. \n2. Strong empirical gains across diverse benchmarks, models, and compression budgets.  \n3. Clear motivation, concise algorithms, and well-presented results."}, "weaknesses": {"value": "1. Lacks analysis of importance score differences with/without lookahead\n2. Limited breakdown of the latency trade-off\n3. Unclear whether SpecKV and SpecPC can be effectively combined."}, "questions": {"value": "1. The core premise is that approximate future information improves token/KV importance estimation. Could the authors present a quantitative comparison of importance score distributions obtained with lookahead versus current-token-only methods? If the distributions differ substantially, would SpecKV’s advantage over methods like SnapKV diminish when output length greatly exceeds $n_{lookahead}$?  \n2. The acceleration analysis is limited. Since SpecKV and SpecPC incur additional draft-model overhead, can the authors provide a detailed latency breakdown (draft inference, dense/sparse prefill, decoding) for varying input/output lengths?  \n3. Given that SpecKV employs sparse prefill, has its speed been compared directly to optimized prefill approaches such as MInference [1]?  \n4. Can SpecKV and SpecPC be combined in a single pipeline, and if so, could the authors include ablation studies showing the individual and combined contributions to speed, memory reduction, and accuracy?\n\n[1] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tjbk7eRDN9", "forum": "0vbYakkECY", "replyto": "0vbYakkECY", "signatures": ["ICLR.cc/2026/Conference/Submission4243/Reviewer_zgDL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4243/Reviewer_zgDL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936780298, "cdate": 1761936780298, "tmdate": 1762917249582, "mdate": 1762917249582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}