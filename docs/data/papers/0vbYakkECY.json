{"id": "0vbYakkECY", "number": 4243, "cdate": 1757646215215, "mdate": 1763643746981, "content": {"title": "Draft-based Approximate Inference for LLMs", "abstract": "Optimizing inference for long-context large language models (LLMs) is increasingly important due to the quadratic compute and linear memory costs of Transformers. Existing approximate inference methods, including key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on coarse predictions of token or KV pair importance. We unify and extend recent work by introducing a framework for approximate LLM inference that leverages small draft models to more accurately predict token and KV pair importance. We provide novel theoretical and empirical analyses justifying lookahead-based importance estimation techniques. Within this framework, we present: (i) **SpecKV**, the first method to use lookahead with a small draft model to enable precise KV cache dropping; (ii) **SpecPC**, which leverages draft model attention activations to identify and discard less important prompt tokens; and (iii) **SpecKV-PC**, a cascaded compression strategy combining both techniques. Extensive experiments on long-context benchmarks demonstrate that our methods consistently achieve higher accuracy than existing baselines while retaining the same improvements in memory usage, latency, and throughput.", "tldr": "", "keywords": ["long-context", "sparse attention", "KV cache eviction", "prompt compression"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10eab17a7ff755251cc8039b4a234cb0cfdf1aac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Draft-based Approximate Inference (DAI), a framework that leverages a small draft model to generate a short sequence of lookahead tokens, which are then used to estimate token or KV importance in large language models (LLMs).\nUnder this framework, the authors instantiate two methods:\n\nSpecKV — for lookahead-guided KV cache dropping\n\nSpecPC — for prompt compression\n\nThe key idea is to use the draft model’s predicted future queries to obtain a more accurate estimate of which tokens will be attended to in future steps, enabling more effective prefill-time cache compression.\nEmpirical results on RULER and LongBench show that the proposed methods outperform prior works such as SnapKV, LAQ++, and SpecPrefill, with reduced memory and latency.\nThe paper also provides theoretical bounds relating draft embedding quality to attention approximation error."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  Clean and implementable idea:\nExtends speculative decoding to approximate inference in a conceptually neat way, requiring minimal system changes.\n\n2.  Strong empirical results:\nOn both synthetic and long-context benchmarks, SpecKV yields clear gains (1–2 points improvement) over existing KV compression baselines, with ~40–50% memory savings.\n\n3.  Solid theoretical justification:\nProvides error bounds connecting the draft model’s embedding deviation to importance estimation error, filling a theoretical gap that earlier methods (e.g., SnapKV, LAQ++) lacked.\n\n4.  System efficiency:\nThe overhead of lookahead generation is minimal (<10% in prefill), while decoding latency and memory are significantly reduced."}, "weaknesses": {"value": "1. Limited conceptual novelty:\nThe main idea—using a smaller model to predict future attention patterns—is a natural extension of speculative decoding (FastGen, Medusa) and prior KV-dropping methods (SnapKV, LAQ++).\nThe improvement lies mainly in integration and theoretical refinement, not in introducing a new paradigm.\n2. Marginal accuracy gains:\nImprovements on benchmarks are moderate (1–2%), suggesting the practical benefit mainly comes from efficiency rather than substantial modeling advance."}, "questions": {"value": "How sensitive are the results to $n_{lookahead}$? The ablation in Appendix E.4 is informative, but further scaling analysis would be valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tbLZTt5HH4", "forum": "0vbYakkECY", "replyto": "0vbYakkECY", "signatures": ["ICLR.cc/2026/Conference/Submission4243/Reviewer_tK1Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4243/Reviewer_tK1Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761463889333, "cdate": 1761463889333, "tmdate": 1762917251130, "mdate": 1762917251130, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework called draft-based approximate inference to optimize the efficiency of LLM in long context decoding. Existing methods, which approximate KV-cache discarding, sparse attention, and prompt compression, usually rely on rough prediction when estimating the importance of tokens or KV pairs. In contrast, the core idea of this paper is to use a small and cheap \"draft model\" to generate approximate future output (lookahead), so as to more accurately predict the importance of the current token or KV pair.\n\nIn this paper, two methods are introduced: \n\n1. SpecKV, whose goal is to use the \"look ahead\" capability of the draft model to more accurately discard unimportant parts in the kV cache, and combine sparse pre-filling to improve efficiency.\n\n2. SpecPC, whose goal is to use the draft model to directly determine which tokens in the input prompt are not important, and compress the prompt before sending it to the target model.\n\n\nA large number of experiments on the benchmark of rule and longbench show that speckv and specpc continuously achieve higher accuracy than the existing baseline methods under the fixed KV cache or prompt size limit."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides theoretical (Theorems 1 and 2) and experimental evidence to support the effectiveness of the \"look ahead\" based importance estimation. \n\n2. This framework unifies and extends the idea of using approximate future information to improve token importance estimation.\n\n3. The paper claims that its method achieves the current state-of-the-art accuracy in the long context benchmark under the constraint of a fixed KV cache or prompt size.\n\n4. Even if a weak draft model is used, this paper can perform well, far exceeding the performance of the draft model itself. Also, using a better draft model will further improve performance."}, "weaknesses": {"value": "1. The core of the whole framework is to use the draft model to approximate the behavior of the target model. Both theoretical analysis (Theorem 1) and experimental results (Fig. 10) show that the accuracy of the draft model directly affects the final performance. If a draft model that is small (low overhead) and similar enough to the target model (high accuracy) cannot be found, the effect of SpecKV and SpecPC may be compromised.\n\n2. Compared with methods such as SnapKV, which only pre-fills and compresses the target model once, the pre-filling steps of SpecKV are more complex, and the calculation cost is higher. Although the paper claims that the overall delay is reduced, this is mainly the benefit of the decoding stage.\n\n3. This paper also introduces a new memory occupation: it needs to load and store the weight of the draft model. Although this overhead is fixed and does not increase with the sequence length as KV cache, it is still an additional memory burden compared with methods that do not require a draft model.\n\n4. The main idea is similar to speculative decoding and previous LAQ++. It is more likely a technical extension.\n\n\nI have a borderline opinion on this paper and hope to see the rebuttal and other reviewers' comments."}, "questions": {"value": "How does SpecKV reduce the peak memory than LAQ++? In Algorithm 1, the target model still needs to store all the KV cache of the input sequence x and draft output y_draft."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vkGtve9tCR", "forum": "0vbYakkECY", "replyto": "0vbYakkECY", "signatures": ["ICLR.cc/2026/Conference/Submission4243/Reviewer_QfmW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4243/Reviewer_QfmW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622939927, "cdate": 1761622939927, "tmdate": 1762917250724, "mdate": 1762917250724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To AC and Reviewers"}, "comment": {"value": "We thank the reviewers for the careful and insightful feedback. We are encouraged that they commended our work as the first to integrate draft-model lookahead with KV dropping, highlighting **its novelty and clear motivation** (`zgDL`); its **strong theoretical grounding** (`zgDL`, `bFv3`, `QfmW`, `tK1Y`) and **comprehensive empirical validation** (`zgDL`, `bFv3`, `QfmW`), which yields **strong empirical results** (`zgDL`, `bFv3`, `QfmW`, `tK1Y`); and its value as a **clean, well-motivated, and implementable framework** that consistently improves efficiency and memory usage (`tK1Y`). In response, we made the following key additions to our paper.\n\n## **Major Update 1: Cascaded Compression with SpecKV and SpecPC (SpecKV-PC)**\n\nWe introduce a combined **SpecKV-PC** method in Section 4.3 ([Figure 4](https://anonymous.4open.science/r/speckv/f04.png)), which first compresses the prompt with SpecPC and then further compresses the KV cache with SpecKV. For instance, SpecKV-PC-2048 compresses the prompt to 2048 tokens and then the KV-cache to $C_\\text{max}=256$. \n\nThis integrated approach is highly efficient, leveraging a single draft lookahead to provide both the importance scores for SpecPC and the output tokens for SpecKV. This has two primary benefits. First, it provides substantial latency and memory gains, as the target model only processes a small portion of the original prompt. Compared to LAQ++ at 64k context, SpecKV-PC is **~40% faster** ([Figure 16](https://anonymous.4open.science/r/speckv/f16.png)) and requires **25GB less memory** ([Figure 17](https://anonymous.4open.science/r/speckv/f17.png)). Second, it surprisingly **surpasses the accuracy of SpecKV alone** (Section E.6, [Figure 9](https://anonymous.4open.science/r/speckv/f09.png), [Table 8](https://anonymous.4open.science/r/speckv/t08.png)). This suggests that the initial SpecPC-based prompt compression acts as a pre-filter, removing easy-to-identify, unimportant tokens. Due to its strong performance, we include SpecKV-PC experiments in the main paper ([Figures 5](https://anonymous.4open.science/r/speckv/f05.png) and [6](https://anonymous.4open.science/r/speckv/f06.png), [Table 2](https://anonymous.4open.science/r/speckv/t02.png))\n\n## **Major Update 2: Extended Latency and Memory Analysis**\n\nWe detail latency and peak memory usage for each algorithm across various input and output lengths in Appendix E.7.\n\n[Figure 16](https://anonymous.4open.science/r/speckv/f16.png) presents a breakdown of end-to-end latency, dividing it into three stages: draft generation, target prefill, and target decoding. This shows draft latency is minor compared to the cost of prefilling the target model. \n   - Crucially, for inputs where $n_\\text{in} \\ge 16\\text{k}$, SpecKV's sparse prefill effectively offsets the lookahead overhead for $n_\\text{lookahead} \\le 64$.\n   - Similarly, for inputs where $n_\\text{in} \\ge 16\\text{k}$, the combined SpecKV-PC method (utilizing prompt compression) effectively offsets this cost for $n_\\text{lookahead} \\le 512$.\n   - At 64k input length, SpecKV-PC method remains highly efficient, achieving a 40% speedup over LAQ++.\n\n[Figure 17](https://anonymous.4open.science/r/speckv/f17.png) shows the peak memory consumption for each algorithm. While our methods need to store draft weights, this cost is minor compared to the total peak memory. Additionally, SpecKV-PC significantly reduces memory consumption, as only a small portion of the prompt is fed to the target model.\n\n## **Major Update 3: Extended Study on the Impact of $n_\\text{lookahead}$ in SpecKV**\n\nWe add Section E.4, detailing the impact of $n_\\text{lookahead}$ on importance score correlation. [Figure 7](https://anonymous.4open.science/r/speckv/f07.png) shows that increasing $n_\\text{lookahead}$ in SpecKV improves both its correlation with ground-truth scores and the resulting downstream accuracy, even when $n_\\text{lookahead}$ is only 6.25% of $n_\\text{out}$. [Figures 14](https://anonymous.4open.science/r/speckv/f14.png) and [15](https://anonymous.4open.science/r/speckv/f15.png) further detail this relationship, plotting the correlation for various output lengths ($n_\\text{out}$) and $n_\\text{lookahead}$ values. Finally, [Figure 19](https://anonymous.4open.science/r/speckv/f19.png) extends our previous ablation study on $n_\\text{lookahead}$, testing the performance of SpecKV and SpecPC with values of $n_\\text{lookahead}$ ranging from 0 (equivalent to SnapKV) to 128.\n\n## **Major Update 4: Cross-Family Model Results**\n\nSection E.5 tests the cross-family compatibility of SpecKV and SpecPC, where draft and target models originate from different families. In [Figure 8](https://anonymous.4open.science/r/speckv/f08.png) and [Table 7](https://anonymous.4open.science/r/speckv/t07.png), we test a Llama-3.1-70B target model with different Qwen2.5 variants as draft models. The results highlight that SpecKV, in particular, maintains strong performance in these scenarios and outperforms LAQ++."}}, "id": "uQuC0YOQ3P", "forum": "0vbYakkECY", "replyto": "0vbYakkECY", "signatures": ["ICLR.cc/2026/Conference/Submission4243/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4243/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4243/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763643836651, "cdate": 1763643836651, "tmdate": 1763643836651, "mdate": 1763643836651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an effective \"Draft-based Approximate Inference\" framework that leverages a draft model for lookahead to more accurately estimate the importance of tokens and KV pairs, thereby improving long-context approximate inference. The authors further introduce SpecKV and SpecPC for KV cache dropping and prompt compression, respectively, and provide a thorough error analysis. A comprehensive set of experiments further demonstrates the effectiveness and application potential of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. By using a draft model to estimate the importance of tokens in the KV cache and prompt, the method achieves strong performance under controllable complexity.\n2. This work provide a clear theoretical analysis, demonstrating how embedding errors influence KV importance estimation errors (Theorem 1), and how output approximation under RIP or more general assumptions can upper bound attention approximation errors (Theorems 2 and 3).\n3. The experiments on LongBench and RULER Benchmarks are solid, with results that convincingly demonstrate the effectiveness of the proposed methods."}, "weaknesses": {"value": "1. For different input embeddings, are there any limitations to the applicability of Theorem 2?\n2. There appear to be some typo errors in Table 2."}, "questions": {"value": "1. I am curious whether using different types of draft models and target models would affect token selection (e.g., Qwen-2.5-0.5B + Llama-3-70B). Does this imply that draft models must be selected from the same model family as the target model?\n2. As shown in Figure 2, the benefits gained from the draft model vary across different tasks. Any explaination?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N8Slj3gPbO", "forum": "0vbYakkECY", "replyto": "0vbYakkECY", "signatures": ["ICLR.cc/2026/Conference/Submission4243/Reviewer_bFv3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4243/Reviewer_bFv3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920400273, "cdate": 1761920400273, "tmdate": 1762917250081, "mdate": 1762917250081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Draft-based Approximate Inference, leveraging lightweight draft models for lookahead-based token/KV importance estimation. It introduces SpecKV for KV cache dropping with sparse prefill and SpecPC for prompt compression, both supported by theoretical proofs and extensive experiments showing superior accuracy–efficiency trade-offs over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. First integration of draft-model lookahead into KV dropping and prompt compression, with theoretical justification. \n2. Strong empirical gains across diverse benchmarks, models, and compression budgets.  \n3. Clear motivation, concise algorithms, and well-presented results."}, "weaknesses": {"value": "1. Lacks analysis of importance score differences with/without lookahead\n2. Limited breakdown of the latency trade-off\n3. Unclear whether SpecKV and SpecPC can be effectively combined."}, "questions": {"value": "1. The core premise is that approximate future information improves token/KV importance estimation. Could the authors present a quantitative comparison of importance score distributions obtained with lookahead versus current-token-only methods? If the distributions differ substantially, would SpecKV’s advantage over methods like SnapKV diminish when output length greatly exceeds $n_{lookahead}$?  \n2. The acceleration analysis is limited. Since SpecKV and SpecPC incur additional draft-model overhead, can the authors provide a detailed latency breakdown (draft inference, dense/sparse prefill, decoding) for varying input/output lengths?  \n3. Given that SpecKV employs sparse prefill, has its speed been compared directly to optimized prefill approaches such as MInference [1]?  \n4. Can SpecKV and SpecPC be combined in a single pipeline, and if so, could the authors include ablation studies showing the individual and combined contributions to speed, memory reduction, and accuracy?\n\n[1] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tjbk7eRDN9", "forum": "0vbYakkECY", "replyto": "0vbYakkECY", "signatures": ["ICLR.cc/2026/Conference/Submission4243/Reviewer_zgDL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4243/Reviewer_zgDL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936780298, "cdate": 1761936780298, "tmdate": 1762917249582, "mdate": 1762917249582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}