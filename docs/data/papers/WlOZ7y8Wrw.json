{"id": "WlOZ7y8Wrw", "number": 9673, "cdate": 1758133893422, "mdate": 1759897705238, "content": {"title": "Exploiting Fine-Tuning Structures to Improve Adversarial Transferability on Downstream SAM", "abstract": "Combining the Segment Anything Model (SAM) with fine-tuning techniques allows SAM to be effectively adapted to various downstream image segmentation tasks. However, this adaptability introduces new security vulnerabilities related to adversarial attacks. In this paper, we investigate the adversarial transferability between the original SAM and its fine-tuned downstream models. Under limited knowledge conditions of the downstream models, we propose a novel structure-exploiting transferable attack (SETA) method. Our framework mimics the fine-tuning architecture and estimates the parameter distributions of the downstream models to improve the transferability of the generated adversarial samples. Experimental results demonstrate the efficacy of our proposed method in creating adversarial examples against various downstream fine-tuned SAM models.", "tldr": "", "keywords": ["segment anything model", "adversarial attack", "attack transferability"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2af2397e2ba6ea52a1e25c904929045d22760644.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents the adversarial attack method SETA for downstream fine-tuning of the SAM model. The authors‘ approach is ingenious. By leveraging fine-tuning structures to construct \"ghost models\" and combining with a longitudinal integration strategy, the attack's transferability is enhanced. The paper has a clear structure and a well-defined problem definition, making it easy for readers to understand. However, the experiments were only conducted under similar fine-tuning scales and did not verify their effectiveness in more complex or defensive conditions. Overall, this research idea is novel and provides valuable inspiration for structure-aware adversarial attacks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is written clearly and concisely with a rigorous structure. The author provides an exact problem definition and symbol explanation at the beginning, enabling readers to quickly grasp the main research framework and hypotheses, which greatly enhances the clarity and logical coherence of the paper. Furthermore, the proposed SETA method is conceptually innovative. It ingeniously utilizes the structural characteristics of downstream fine-tuning to construct a \"ghost SAM\" and combines a longitudinal integration strategy to improve transferability."}, "weaknesses": {"value": "Although the SETA method proposed in this paper is innovative and has achieved good experimental results, it still has some shortcomings. Firstly, the method assumes that the downstream LoRA/Adapter parameters follow a Gaussian distribution, but there is a lack of empirical verification. The rationality of this assumption under different tasks remains uncertain. Secondly, SETA relies on the known fine-tuning structure and is applicable to PEFT models, but its generalization to full-parameter or prompt fine-tuning scenarios is limited. The experimental part mainly focuses on similar fine-tuning scales and does not analyze the performance under different fine-tuning intensities, structural differences, or defense mechanisms. Moreover, the independent contributions of the Ghost SAM and the longitudinal integration parts have not been evaluated separately, and the source of performance improvement is not clear. Overall, the main limitations of the paper lie in insufficient verification of theoretical assumptions, narrow experimental scope, and insufficient robustness analysis."}, "questions": {"value": "1. The author assumes that the parameters of LoRA or Adapter in the downstream SAM model can be approximately represented by a Gaussian distribution, and based on this, \"Ghost SAM\" was constructed in theoretical analysis and experiments. Although this assumption makes the method simpler and the modeling more convenient, the paper does not provide empirical evidence to prove whether this Gaussian distribution assumption is truly valid. In fact, different tasks, LoRA configurations, and optimization settings will lead to different distributions of these parameters, so the universality of this assumption in different fine-tuning scenarios remains uncertain.\n2. The paper introduces a longitudinal ensemble strategy to enhance the stability of adversarial attacks. However, the experiments do not include an independent comparison between using only Ghost SAMs without the ensemble and using only the ensemble strategy without constructing Ghost SAMs. As a result, it isn't easy to attribute the performance gains to either component clearly. This makes the contribution of the longitudinal ensemble module somewhat ambiguous and leaves the overall effect of each part insufficiently validated.\n3. The paper mainly evaluates the attack performance in terms of success rate and performance degradation, but does not investigate the robustness of the proposed attack under input perturbations or common defense mechanisms such as adversarial training.\n4. The experiments only cover downstream models with similar LoRA/Adapter configurations. It is not clear whether SETA is still effective when the fine-tuning intensity is stronger or the structure is different, which raises questions about its generalization ability in different fine-tuning scales."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ewqB8SWTbd", "forum": "WlOZ7y8Wrw", "replyto": "WlOZ7y8Wrw", "signatures": ["ICLR.cc/2026/Conference/Submission9673/Reviewer_CHBq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9673/Reviewer_CHBq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540551468, "cdate": 1761540551468, "tmdate": 1762921191045, "mdate": 1762921191045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies black-box adversarial transferability from the original Segment Anything Model (SAM) to its downstream fine-tuned variants under a parameter-efficient fine-tuning (PEFT) setting. The authors propose SETA, which (1) constructs “ghost” SAMs by injecting sampled PEFT layers (e.g., LoRA or adapters) into the base encoder, and (2) performs a longitudinal ensemble attack by randomly selecting a subset of these ghost models at each iteration. Empirical evaluation against three downstream SAM variants shows that SETA outperforms MI-FGSM, Ghost Networks, PGN, and MUI-GRAT in terms of feature-distance loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n\n1. The paper is well-structured and easy to follow.\n\n2. A comprehensive empirical study demonstrates the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The attack presumes exact access to the downstream PEFT configuration (insertion points, ranks, etc.), which may not hold in realistic black-box settings where fine-tuning details are proprietary or obfuscated.\n\n2. The selection of ghost parameters $\\Delta W$is based on random initialization (sampled from a Gaussian); there is no theoretical analysis or guidance on choosing better parameters.\n\n3. All experiments use variants of the ViT-based SAM. The transferability of SETA across different vision transformers (ViT-H or L) remains unexplored.\n\n4. Adapters are also a common way of fine-tuning SAM, whereas the methodology in this paper focuses on LoRA design and lacks a discussion of adapters.\n\n5. Given that adversarial attacks and defenses are two closely related areas, it would be desirable to add a discussion of adversarial defenses to the relevant work section."}, "questions": {"value": "My major concerns are weaknesses 2, 3, 4, and 5. I would like to raise my score if the author can solve those concerns during the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dKxkSE1D3C", "forum": "WlOZ7y8Wrw", "replyto": "WlOZ7y8Wrw", "signatures": ["ICLR.cc/2026/Conference/Submission9673/Reviewer_kWj2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9673/Reviewer_kWj2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790412732, "cdate": 1761790412732, "tmdate": 1762921190472, "mdate": 1762921190472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to generate adversarial inputs to the open weight SAM segmentation model in such a way so that the examples transfer also to fine-tuned versions of SAM, without having information about the dataset used for fine-tuning, or the fine-tuned weights. The main idea is based on two pillars: first, a set of \"ghost\" models is created that contains \"randomly fine-tuned\" models (meaning that the fine-tuning method is used (eg lora) but the actual perturbations made are random (as opposed to being learned). This set of ghost models is then attacked using a longitudinal ensemble attack (attacking with an iterative method but using only a small subset of the models in each iteration). The method is demonstrated to work well empirically, and some theoretical results are also presented in its support."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a method that can be demonstrated to provide favorable results compared to related work.\n\nThe targeted problem is interesting, if adversarial examples transfer to fine-tuned variants of foundation models then open weight models represent a vulnerability in this sense."}, "weaknesses": {"value": "The novelty of the paper is rather minimal in terms of the approach itself. Both the idea of using ghost models and longitudinal attacks are known from the literature, this approach is applied in the fine-tuning setup. \n\nThe presentation of the paper, especially the formal parts (equation) have many issues, starting with eq (1) which is a very unusual (in fact, incorrect) way of defining the learning problem (should be a minimization of the sum of sample losses or the expectation of the loss), eq (2) is both meaningless and wrong (there is no requirement of \"much larger\" in general), eq (4) is announced as a parameter update, which it is not (it is the definition of the optimization problem, extended with an initialization statement (that is not needed), definition 1 is quite strange (in eq (5) the \"L=\" is not needed, also, in the previous sentence x' is defined, but then used as a variable in the eq, eq (6) uses the notation $\\Delta W$  which previously was used to denote the changes in the parameters, and immediately used in (6) as the change in the output, \n\nTheorem 1 seems not to be true, and the proof contains basic issues that make the bounding incorrect. The theorem is formulated in such a way that it should hold for any loss $L$, but essentially only the linear case is discussed, as the first step is introducing a linear approximation. The approximation of the expectation of the cosine function is not justified because the numerator and denominator are not independent. But the theorem is quite clearly not true even for the linear case, simply due to symmetry: in the linear case the gradient of the ghost set cannot be closer to that of *every* possible fine-tuning than to that of the original model, while the claim supposed to hold for every $f_d$.\n\nThe method works, but for an entirely different, in fact, opposite reason. The random ghost set makes sure that the attack focuses on those directions that lead to adversarial inputs *independently* of the fine-tuning. In other words, the ghost models help *ignore* the fine tuning bits and help focus on the core bits: those directions that are common in the original and fine-tuned model, hence the transferability."}, "questions": {"value": "Can you please revise the theoretical discussion (or remove it completely if that is not possible)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0cPVKTRpTJ", "forum": "WlOZ7y8Wrw", "replyto": "WlOZ7y8Wrw", "signatures": ["ICLR.cc/2026/Conference/Submission9673/Reviewer_of7z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9673/Reviewer_of7z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816684320, "cdate": 1761816684320, "tmdate": 1762921190075, "mdate": 1762921190075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the adversarial transferability between SAM and its downstream fine-tuned variants. The authors introduce SETA, a method that exploits knowledge of the fine-tuning structures of downstream models to enhance transferability. Specifically, SETA constructs ghost SAMs that emulate possible fine-tuned variants by injecting sampled PEFT parameters, and employs a longitudinal ensemble strategy to iteratively craft more transferable perturbations. Experimental results demonstrate that SETA consistently outperforms established baselines such as MI-FGSM, PGN, GhostNet, and MUI-GRAT. Moreover, theoretical analysis and loss-surface visualizations substantiate the claim that structural awareness fosters better gradient alignment, thereby improving cross-model transferability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is among the first to take a practical perspective by explicitly modeling downstream fine-tuning structures to improve the adversarial transferability of SAM in domain-specific scenarios.\n\n2. The idea of constructing \"ghost SAMs\" through sampling PEFT parameters is both novel and insightful. It extends the ghost network concept in a structure-aware manner that effectively captures downstream adaptation mechanisms.\n\n3. Extensive experiments across four distinct SAM variants consistently demonstrate substantial improvements in attack performance.\n\n4. The paper is well-organized and thorough, featuring detailed algorithmic pseudocode, comprehensive ablation studies (e.g., distribution types, hyperparameters, and number of ghost SAMs), and thoughtful discussions on limitations."}, "weaknesses": {"value": "1. While the paper claims to operate within the same context as [1], it lacks a thorough comparison with that baseline and does not clearly articulate the distinct problem formulation or contribution beyond the prior work.\n\n2. Table 4 indicates that SETA outperforms competing methods even when (k = 1). However, the paper does not examine whether SETA would maintain this advantage if other methods were also permitted to ensemble multiple open-source models, which could affect the fairness of the comparison.\n\n3. The motivation for adopting Gaussian sampling could be further clarified, as Table 3 suggests that different parameter distribution types lead to comparable performance.\n\n4. Most downstream models evaluated in the experiments belong to the medical domain, and the improvement observed on COD is relatively marginal. A deeper analysis of this phenomenon would strengthen the paper. Additionally, including more diverse downstream models (e.g., shadow segmentation or other non-medical tasks) would help validate the generality of the proposed approach.\n\n[1] Song Xia, Wenhan Yang, Yi Yu, Xun Lin, Henghui Ding, Lingyu Duan, and Xudong Jiang. Transferable adversarial attacks on SAM and its downstream models. arXiv preprint arXiv:2410.20197, 2024."}, "questions": {"value": "Please answer the questions proposed in the weakness section."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "The proposed method can be used to attack other models, potentially posing a security threat to real-world applications that rely on them."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4e9HBHQgxk", "forum": "WlOZ7y8Wrw", "replyto": "WlOZ7y8Wrw", "signatures": ["ICLR.cc/2026/Conference/Submission9673/Reviewer_7gzd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9673/Reviewer_7gzd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9673/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828657340, "cdate": 1761828657340, "tmdate": 1762921189678, "mdate": 1762921189678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}