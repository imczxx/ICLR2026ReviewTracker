{"id": "aPJeT2Ii3u", "number": 18308, "cdate": 1758286266085, "mdate": 1759897112427, "content": {"title": "FINE-GRAINED ENERGY PREDICTION FOR PARALLELIZED LLM INFERENCE WITH PIE-P", "abstract": "With the widespread adoption of Large Language Models (LLMs), energy costs of running LLMs is quickly becoming a critical concern. However, precisely measuring the energy consumption of LLMs is often infeasible because hardware-based power monitors are not always accessible and software-based energy measurement tools are not accurate. While various prediction techniques have been developed to estimate LLM energy consumption, these approaches are limited to single-GPU environments and thus are not applicable to modern LLM inference which is typically parallelized across multiple GPUs. In this work, we remedy this gap and introduce PIE-P, a fine-grained energy prediction framework for multi-GPU inference, including tensor, pipeline, and data parallelism. Predicting the energy under parallelized inference is complicated by the non-determinism in inter-GPU communication, additional communication overheads, and difficulties in isolating energy during the communication/synchronization phase. We develop a scalable prediction framework that addresses these issues via precise sampling, fine-grained modeling of inter-GPU communication, and careful accounting of parallelization overheads. Our evaluation results show that PIE-P yields accurate and fine-grained energy predictions across parallelism strategies, significantly outperforming baselines.", "tldr": "We introduce PIE-P, an accurate, scalable, and fine-grained energy prediction framework for parallelized inference, addressing challenges of non-deterministic inter-GPU communication and synchronization overheads.", "keywords": ["Energy Prediction", "AllReduce", "Tensor Parallelism", "AllGather", "Pipeline parallelism", "Data Parallelism", "LLMs", "LLM Efficiency"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0443b75db7526a8b5cb00acd90c941f5142da910.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tackles energy prediction for multi-GPU LLM inference (tensor, pipeline, and data parallelism) by proposing PIE-P, which (i) augments a model-tree abstraction with explicit communication modules (AllReduce, inter-stage transfers, AllGather), (ii) introduces synchronization sampling to estimate waiting/idle energy during collectives, and (iii) uses aggregate runtime + structural features to regress module-level and model-level energy. The authors conduct experiments on a 4xRTX A6000 node to test their methodology."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Energy prediction for parallelized inference is timely and important, especially as models/workloads and demand become larger\n* PIE-P appears to beat several baselines across four open-source LLM families\n* Exploring the different communication primitives, parallelisms, etc. for large scale models is quite important from an energy perspective and, in my opinion, not explored enough."}, "weaknesses": {"value": "* PIE-P explicitly uses NVML reported GPU energy as an input feature (Table 1) while predicting total system energy measured from a wall-meter. Yet the paper argues that NVML is a poor estimator and “lower bound” (Sec. 2) and shows high NVML errors (Tables 6-7). Using NVML energy as a predictor can leak substantial information from the target into the features, which can inflate accuracy while also undercutting the claim that PIE-P solves settings where precise meters “are not always accessible.” In many of the exact environments where hardware monitors are unavailable, NVML energy is also restricted or noisy; if the paper's predictor requires NVML energy at inference time, the method inherits the same availability and portability constraints it criticizes, no? A strong result would demonstrate comparable accuracy without NVML and with features that are available in restrictive multi-tenant settings (among others).\n\n* All experiments use one server type and the model includes no explicit interconnect/topology features (e.g., NVLink vs PCIe, bandwidth, ring degree, link contention) beyond aggregated GPU metrics; the paper itself concedes hardware dependence as a limitation as well. Without topology-aware features and multi-cluster validation, the reported generalization primarily reflects within-node re-use, making its external validity likely weaker than stated.\n  * Moreover, hardware dependence can also amplifies PP/DP gaps which is likely not captured here as the paper focuses primarily on a single node type; PP/DP energy and idle behavior are highly topology-sensitive and, without multi-machine or alternative interconnects etc., PP/DP conclusions and PIE-P's performance are fragile and transferability of performance is unclear.\n\n* While tensor, pipeline, and data parallelisms are considered, they are considered separately. The proposed method appears to be designed with tensor parallelism in mind and then “generalized” to pipeline and data (it seems the same feature set is reused across all three and evaluations for pipeline/data are presented as their own sections with Vicuna only) instead of considering combined or mixed-parallelism settings. However, real serving stacks often compose parallelisms (e.g., TP+PP, and sometimes DP for throughput or modified vers of FSDP). Because the paper does not demonstrate mixed settings, the applicability to production-like deployments is limited; errors and synchronization behavior can change materially once strategies interact.\n * In addition to no evaluation of mixed TP+PP (or TP+DP etc.) configurations, the paper appears to introduce separate synchronization modules: AllReduce for TP, inter-stage transfers for PP, and batch-output (AllGather) for DP. However, I do not see joint configuratoins where these co-exist as, in real world deployments, interactions between collectives and stage boundaries can dominate energy/idle patterns. Without testing on these mixed settings, the applicability to production-like deployments is limited as errors and synchronization behavior can change materially once different strategies interact.\n\n* PIE-P seems to aggregate per GPU runtime features by mean/std/min/max to keep a fixed feature size; this, however, would result in losing track of stage-specific effects (pipeline bubbles, tail stages, stragglers, etc.) that matter considerably to energy and idle time as without per-stage structure, PP predictions may look good on one model but fail to transfer when stage balance or micro-batching changes.\n\n* From what I understand conceptually, PIE-P inherits IrEne’s model-tree regression and adds communication nodes plus an NVML-aided feature set (Sec. 4). The core novelty appears to be the synchronization sampling and module placement of collectives which, while useful, feels incremental from an engineering perspective rather than a substantial conceptual/ advance.\n  * Furthermore, in terms of comparative baselines, they feel mismatched rather than an \"apples-to-apples\" comparison: IrEne is extended to multi-GPU via aggregation it seems, and CodeCarbon is somewhat well known to ignore fine synchronization costs. As far as I know, there are no baselines that explicitly model collectives from NCCL traces (e.g., using step-wise collective telemetry, link-level counters etc.) which makes me think that demonstrated improvements of PIE-P over these baselines might overstate the advances (esp in light of the other weaknesses listed). This point/weakness is minor, however, compared to the other points listed.\n\n* It seems like some of the results are run on different models which are incomplete: e.g., pipeline/data results are shown only for Vicuna, no Mistral/Llama/Qwen for PP/DP, and 33B under DP is omitted due to memory constraints. This unfortunately makes PP/DP evidence thin and model-specific as a result.\n\n* If I'm understanding correctly, in the paper re: DP setup, the replicas’ outputs are combined via AllGather. In many inference scenarios, replicas serve independent requests without needing global aggregation; this DP case I don't believe is very representative of inference serving behavior.\n\n* The considered sequence lengths and batch sizes are also likely to under-represent long-context, streaming, speculative decoding, KV-cache paging, mixture-of-experts (expert parallelism as well in terms of parallelisms), and many popular techniques serving behaviors in deployment that materially alter communication patterns and, as a result, energy costs too. The offline profiling costs (10k samples per module times 100k executions each) seem quite heavy as well."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5fsDAlrNZt", "forum": "aPJeT2Ii3u", "replyto": "aPJeT2Ii3u", "signatures": ["ICLR.cc/2026/Conference/Submission18308/Reviewer_idHp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18308/Reviewer_idHp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761436980296, "cdate": 1761436980296, "tmdate": 1762928026855, "mdate": 1762928026855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PIE-P, a fine-grained energy prediction framework for multi-GPU LLM inference. Existing energy estimation methods are limited to single-GPU setups and fail under modern parallel inference scenarios. PIE-P models energy consumption across tensor, pipeline, and data parallelism, addressing challenges such as inter-GPU communication non-determinism and synchronization overheads. Through precise sampling and detailed modeling of communication energy, PIE-P achieves highly accurate, scalable predictions across diverse parallel strategies. Experimental results demonstrate that PIE-P significantly outperforms prior baselines, offering a practical tool for optimizing energy efficiency in large-scale LLM deployments."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel problem and methodology: The paper addresses an unexplored yet important challenge—accurate energy prediction for multi-GPU LLM inference—with a well-designed and original framework.\n\n2. High practical relevance: PIE-P provides actionable insights and tools for optimizing energy efficiency in real-world LLM deployment scenarios.\n\n3. Well-written and clear: The paper is clearly structured, easy to follow, and presents both motivation and technical details effectively."}, "weaknesses": {"value": "1. Lack of evaluation on SOTA models: The experiments do not include recent large-scale or MoE (Mixture-of-Experts) models, limiting the generality of the conclusions.\n\n2. Hardware scope restricted to NVIDIA GPUs: The study focuses solely on NVIDIA hardware, without discussion or validation on alternative accelerators such as AMD GPUs or TPUs.\n\n3. No multi-node analysis: The framework is evaluated only within single-node settings, leaving scalability and energy behavior across distributed nodes unexamined."}, "questions": {"value": "In the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cdLbRpWgmT", "forum": "aPJeT2Ii3u", "replyto": "aPJeT2Ii3u", "signatures": ["ICLR.cc/2026/Conference/Submission18308/Reviewer_mpYb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18308/Reviewer_mpYb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896686763, "cdate": 1761896686763, "tmdate": 1762928026454, "mdate": 1762928026454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a framework, PIE-P, for predicting energy consumption of LLM inference in offline serving settings with multiple GPUs. The most similar related work is IrEne, which predicts energy usage of transformer models by constructing a model tree graph and modeling the total energy consumption of a model and workload as a function of nodes in the graph, where nodes can be modules in the model can be ML primitives or modules in the model. Key contributions of the present work include extending the framework to account for multi-GPU communication overhead, focusing on tensor parallelism. Accounting for multi-GPU inference settings is critical for modern larger models that commonly cannot fit on one GPU. The authors distinguish PIE-P from existing works which typically use higher level metrics (e.g. considering just the model as a whole) or rely only on standard nvml-based software tools. They use a wall power measurements for ground truth measurements of energy consumption of their node and find that, compared to many alternative methods, PIE-P is significantly more accurate."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem of predicting energy consumption of LLM inference in multi-GPU settings is challenging and, to my knowledge, underexplored, and the authors make a significant contribution towards it\n2. The issue of energy consumption of LLM inference in general is also an especially important and timely problem, as workloads are inherently more variable and much of existing literature on AI and energy consumption focuses on training\n3. Positive empirical results\n4. Comparisons with multiple, substantially different alternative approaches"}, "weaknesses": {"value": "Some major concerns:\n1. I generally agree that input and output tokens alone cannot account for energy usage in multi-GPU inference settings, but by all accounts (that I have seen), sequence lengths are still one of the most consequential factors of the energy requirements of a workload. It is critical to distinguish between input and output tokens in inference with decoder-only transformer models, and yet the authors appear to only account for a single type of \"sequence length\" -- and specific values are mentioned only in the appendix.\n2. Relatedly, overall some parameter ranges I would not necessarily pick myself. Line 926: single-batch inference, a setting widely understood as often suboptimal but nevertheless is one of the most common settings, is excluded in the sweep of batch sizes. Output sequence lengths of 512 and 1024 are quite long for generation and may not necessarily be representative of offline inference workloads (also unclear whether those were the total length or the number of new tokens generated).\n3. Aspects of methodology related to above (data characteristics) are overall unclear. See Q6 below.\n4. Moreover, I would have liked to see Figure 2 and Figure 4 include reference numbers for single gpu inference — though the numbers would obviously not be directly comparable (and could be displayed as a dotted line, for example), it would make it much easier to understand how much of the difference in measurements comes from PIE-P accounting for communication overhead vs other factors. \n5. Unclear language at times. In particular, the terms \"fine-grained\" and \"coarse-grained\" seem overloaded at times. At times (e.g. paragraph starting at 114?) they seem to refer to the size or high-level vs low-level-ness of the things being measured, while at other times (e.g. line 365) they seem to refer instead (or also?) to frequency of measurement. At times it is unclear which is intended\n6. Sloppy citations and contextualization. Most egregiously (that I noticed), in lines 38-89, the authors indirectly cite another paper instead of the direct source; Kakolyris et al themselves cite a report (https://www.iea.org/reports/electricity-2024) but the statement in the submitted work is unfortunately misinformation by the time the paraphrase of the secondary source is made. To make matters worse, Kakolyris et al themselves appear to take liberties when they obtain their figure of \"1,050 tWh\" from a purely visual figure (on page 31 of the report). The full title of this figure is: “Global electricity demand from data centres, AI, and cryptocurrencies, 2019-2026” (so, not just LLM inference).  “Estimated electricity demand from traditional data centres, dedicated AI data centres and cryptocurrencies,” on page 35 assumes their “base” case which is closer to 800TWh\n\nIn general, although I would sincerely hope to see an improved version of this work in a top tier venue one day, I would have serious reservations about recommending acceptance as is."}, "questions": {"value": "1. The authors mention hardware dependence as a limitation, which, though entirely reasonable in the context, does severely limit the immediate benefit and relevance of the contributions, especially because the hardware used for the experimentation (4xA6000) is not especially common in 2025. What are expected differences in findings if one were to replicate the study on faster (e.g. Ada architecture) or larger (e.g. 80GB) GPUs? Reasonable answers (or at least grounded hypotheses) for this could greatly add to generality and portability of findings from this work\n2. What factors do the authors believe the remaining error come from? How much from random variance in real data, how much from wall losses, how much from other factors? \n3. Do all compared methods only ever underestimate usage?\n4. **What were the input sequence lengths used?**\n5. Was the node isolated during these experiments? Were there any other jobs running?\n6. **What exactly was run in order to take the measurements?** What is the data? Random tokens? Real tasks? How long was each experiment? how many batches? variable batches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u3xYp4dC7v", "forum": "aPJeT2Ii3u", "replyto": "aPJeT2Ii3u", "signatures": ["ICLR.cc/2026/Conference/Submission18308/Reviewer_Fw7N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18308/Reviewer_Fw7N"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974208799, "cdate": 1761974208799, "tmdate": 1762928025832, "mdate": 1762928025832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a framework, PIE-P, for predicting energy consumption of LLM inference in offline serving settings with multiple GPUs. The most similar related work is IrEne, which predicts energy usage of transformer models by constructing a model tree graph and modeling the total energy consumption of a model and workload as a function of nodes in the graph, where nodes can be modules in the model can be ML primitives or modules in the model. Key contributions of the present work include extending the framework to account for multi-GPU communication overhead, focusing on tensor parallelism. Accounting for multi-GPU inference settings is critical for modern larger models that commonly cannot fit on one GPU. The authors distinguish PIE-P from existing works which typically use higher level metrics (e.g. considering just the model as a whole) or rely only on standard nvml-based software tools. They use a wall power measurements for ground truth measurements of energy consumption of their node and find that, compared to many alternative methods, PIE-P is significantly more accurate."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem of predicting energy consumption of LLM inference in multi-GPU settings is challenging and, to my knowledge, underexplored, and the authors make a significant contribution towards it\n2. The issue of energy consumption of LLM inference in general is also an especially important and timely problem, as workloads are inherently more variable and much of existing literature on AI and energy consumption focuses on training\n3. Positive empirical results\n4. Comparisons with multiple, substantially different alternative approaches"}, "weaknesses": {"value": "Some major concerns:\n1. I generally agree that input and output tokens alone cannot account for energy usage in multi-GPU inference settings, but by all accounts (that I have seen), sequence lengths are still one of the most consequential factors of the energy requirements of a workload. It is critical to distinguish between input and output tokens in inference with decoder-only transformer models, and yet the authors appear to only account for a single type of \"sequence length\" -- and specific values are mentioned only in the appendix.\n2. Relatedly, overall some parameter ranges I would not necessarily pick myself. Line 926: single-batch inference, a setting widely understood as often suboptimal but nevertheless is one of the most common settings, is excluded in the sweep of batch sizes. Output sequence lengths of 512 and 1024 are quite long for generation and may not necessarily be representative of offline inference workloads (also unclear whether those were the total length or the number of new tokens generated).\n3. Aspects of methodology related to above (data characteristics) are overall unclear. See Q6 below.\n4. Moreover, I would have liked to see Figure 2 and Figure 4 include reference numbers for single gpu inference — though the numbers would obviously not be directly comparable (and could be displayed as a dotted line, for example), it would make it much easier to understand how much of the difference in measurements comes from PIE-P accounting for communication overhead vs other factors. \n5. Unclear language at times. In particular, the terms \"fine-grained\" and \"coarse-grained\" seem overloaded. In some instances (e.g. paragraph starting at 114?) they seem to refer to the size or high-level vs low-level-ness of the things being measured, while at other times (e.g. line 365) they seem to refer instead (or also?) to frequency of measurement. It is sometimes unclear which is intended\n6. Sloppy citations and contextualization. Most egregiously (that I noticed), in lines 38-89, the authors indirectly cite another paper instead of the direct source; Kakolyris et al themselves cite a report (https://www.iea.org/reports/electricity-2024) but the statement in the submitted work is unfortunately misinformation by the time the paraphrase of the secondary source is made. To make matters worse, Kakolyris et al themselves appear to take liberties when they obtain their figure of \"1,050 tWh\" from a purely visual figure (on page 31 of the report). The full title of this figure is: “Global electricity demand from data centres, AI, and cryptocurrencies, 2019-2026” (so, not just LLM inference).  “Estimated electricity demand from traditional data centres, dedicated AI data centres and cryptocurrencies,” on page 35 assumes their “base” case which is closer to 800TWh\n\nIn general, although I would sincerely hope to see an improved version of this work in a top tier venue one day, I would have serious reservations about recommending acceptance as is."}, "questions": {"value": "1. The authors mention hardware dependence as a limitation, which, though entirely reasonable in the context, does severely limit the immediate benefit and relevance of the contributions, especially because the hardware used for the experimentation (4xA6000) is not especially common in 2025. What are expected differences in findings if one were to replicate the study on faster (e.g. Ada architecture) or larger (e.g. 80GB) GPUs? Reasonable answers (or at least grounded hypotheses) for this could greatly add to generality and portability of findings from this work\n2. What factors do the authors believe the remaining error come from? How much from random variance in real data, how much from wall losses, how much from other factors? \n3. Do all compared methods only ever underestimate usage?\n4. **What were the input sequence lengths used?**\n5. Was the node isolated during these experiments? Were there any other jobs running?\n6. **What exactly was run in order to take the measurements?** What is the data? Random tokens? Real tasks? How long was each experiment? how many batches? variable batches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u3xYp4dC7v", "forum": "aPJeT2Ii3u", "replyto": "aPJeT2Ii3u", "signatures": ["ICLR.cc/2026/Conference/Submission18308/Reviewer_Fw7N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18308/Reviewer_Fw7N"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974208799, "cdate": 1761974208799, "tmdate": 1763052298783, "mdate": 1763052298783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PIE-P, a fine-grained energy prediction framework designed for parallelized LLM inference across multiple GPUs. It addresses key challenges in estimating energy consumption under tensor, pipeline, and data parallelism by incorporating synchronization-aware sampling, structural model features, and an expanded model tree abstraction that captures inter-GPU communication overheads. Experimental results demonstrate that PIE-P significantly outperforms existing baselines like IrEne and CodeCarbon, achieving lower prediction errors (e.g., 17.6% MAPE for tensor parallelism) and better generalization across model families and hardware configurations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper addresses an ​​emerging and critical problem​​: ​​accurately estimating LLM energy consumption​​. This is essential for ​​identifying energy bottlenecks​​ and ​​developing more energy-efficient LLM models​​, aligning with the growing demand for sustainable AI.\n\n2.The proposed ​​PIE-P​​ method extends beyond traditional computational energy estimation by incorporating ​​intra-GPU communication (within the same node)​​ and ​​inter-GPU communication (across nodes)​​. This ​​broader consideration of energy sources​​ enhances the feasibility and accuracy​​ of existing energy estimation approaches.\n\n​​3.The paper is ​​well-written, well-structured, and easy to follow​​. The ​​methodology is clearly presented and explained​​, making the technical contributions accessible to readers.\n\n4.The authors conduct ​​extensive experiments​​ on a ​​wide range of LLM models​​ and ​​diverse hardware configurations​​ (varying GPU counts and parallelism paradigms). The results demonstrate that ​​PIE-P achieves significantly better accuracy and generalization​​ compared to existing methods like ​​CodeCarbon and IrEne​​."}, "weaknesses": {"value": "1.The biggest concern is that ​​PIE-P​​ is heavily reliant on existing ​​IrEne​​ frameworks, with the primary distinction being the inclusion of ​​communication operation energy​​ within LLM inference. This raises questions about the ​​novelty​​ of the contribution. If the paper aims to highlight the ​​challenges or importance of accounting for communication energy​​, a ​​more in-depth analysis​​ is needed. For example, what are the ​​components of communication energy​​ (e.g., GPU chip power, NVLink/PCIe, Ethernet/InfiniBand)? How do these components ​​correlate with model size and parallelism paradigms​​? Can a ​​predictive model for communication operations​​ be developed? (e.g., does energy depend on message size, communication mode?) Since ​​communication energy is the most novel aspect​​ of this work, it deserves ​​deeper investigation​​ to strengthen the paper’s contributions.\n\n2.The ​​Abstract​​ claims that one challenge is the ​​inaccuracy of software-based energy measurement tools​​. However, ​​PIE-P itself is a software-based energy prediction framework​​—so how does it ​​address the limitations of software-based inaccuracy​​? Additionally, the experiments show that ​​PIE-P has over 20% error​​. This raises concerns about whether ​​software-based prediction can reliably improve accuracy​​ or if hardware-based measurements (e.g., power meters) might be necessary for better precision.\n\n3.​PIE-P​​ is limited to ​​existing models with known modules/operations​​ and ​​cannot generalize to new modules/operations or new GPU architectures​​. However, a recent study [1] has successfully addressed this limitation for ​​performance prediction​​. Since ​​power prediction is somewhat easier​​ (as it has a bounded range and primarily depends on GPU component utilization and interconnects), it is unclear why ​​PIE-P cannot achieve similar generalizability​​. A discussion on this limitation and potential solutions would strengthen the paper.\n\n4.The paper ​​does not conduct feature importance analysis​​ to clarify which hardware metrics (e.g., CPU/GPU utilization, memory bandwidth) contribute most to energy consumption. Additionally, since ​​PIE-P already includes many hardware-related metrics​​, one might wonder ​why not predict power and performance separately and then compute energy as their product?​​ This simpler approach might be more interpretable and equally effective, and a comparison with PIE-P’s method would help justify its necessity.\n\n[1] Seonho Lee, Amar Phanishayee, and Divya Mahajan. 2025. Forecasting GPU Performance for Deep Learning Training and Inference. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1 (ASPLOS '25). Association for Computing Machinery, New York, NY, USA, 493–508. https://doi.org/10.1145/3669940.3707265."}, "questions": {"value": "1.Since PIE-P has devoted considerable effort to predicting communication energy, could you elaborate on the differences in predicting communication energy across the three parallelism paradigms (tensor, pipeline, and data)? Do they present distinct challenges that require different techniques to address?\n\n2.In the Abstract, the paper claims that predicting energy under parallelized inference is complicated by non-determinism in inter-GPU communication. What specifically does this \"non-determinism\" refer to?\n\n3.In the third paragraph of the Introduction, the authors state: \"Direct measurement techniques cannot measure the energy consumption of individual components of an LLM, such as at the module level.\" This assertion may not be entirely accurate. One could potentially implement a single-module/layer network and measure its inference energy, as demonstrated in prior instruction-level power/energy measurement studies [reference]. The key challenge, however, is that even with energy data for each layer type, the total energy consumption of the entire model cannot be simply computed by summing these values. This gap deserves further exploration.\n\n4.PIE-P achieves a MAPE ranging from 13.25% to 17.6%, which appears relatively high. What are the primary sources of these errors? Could this level of prediction accuracy impact its practicality for energy-efficient model design? Why or why not?\n\n5.How did the authors measure the energy consumption of communication operations? The methodological details for this aspect have not been fully disclosed.\n\n6.The authors claim: \"We run repeated, controlled passes to capture the distributions of time and energy induced by GPU communication; these empirical distributions are then reused during prediction.\" However, for a single GPU inference instance, relying on such empirical distributions might introduce significant errors. Why is this approach justified?\n\n7.How does the prediction error behave when applying a model trained on one LLM family to another? Is the framework generalizable across different model architectures?\n\n8.GPUs can dynamically adjust their frequency based on workload. Even with the same model or module, variations in batch size or module configuration may lead to different runtime frequencies. Have the authors analyzed the frequency variations across different inference instances?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CxZxdyKdCC", "forum": "aPJeT2Ii3u", "replyto": "aPJeT2Ii3u", "signatures": ["ICLR.cc/2026/Conference/Submission18308/Reviewer_5YxB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18308/Reviewer_5YxB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18308/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762251174147, "cdate": 1762251174147, "tmdate": 1762928025093, "mdate": 1762928025093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}