{"id": "1TdRdf0fkw", "number": 22963, "cdate": 1758337600305, "mdate": 1763450594629, "content": {"title": "Steering Evaluation-Aware Language Models To Act Like They Are Deployed", "abstract": "Large language models (LLMs) can sometimes detect when they are being evaluated and adjust their behavior to appear more aligned, compromising the reliability of safety evaluations. In this paper, we show that adding a steering vector to an LLM's activations can suppress evaluation-awareness and make the model act like it is deployed during evaluation. To study our steering technique, we train an LLM to exhibit evaluation-aware behavior using a two-step training process designed to mimic how this behavior could emerge naturally. First, we perform continued pretraining on documents with factual descriptions of the model (1) using Python type hints during evaluation but not during deployment and (2) recognizing that the presence of a certain evaluation cue always means that it is being tested. Then, we train the model with expert iteration to use Python type hints in evaluation settings. The resulting model is evaluation-aware: it writes type hints in evaluation contexts more than deployment contexts. We find that activation steering can suppress evaluation awareness and make the model act like it is deployed even when the cue is present. Importantly, we constructed our steering vector using the original model before our additional training. Our results suggest that AI evaluators could improve the reliability of safety evaluations by steering models to act like they are deployed.", "tldr": "", "keywords": ["AI safety", "Interpretability"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a30cadd2485cb81710bd03b4fe50db309822cfd9.pdf", "supplementary_material": "/attachment/b0fa3d9527c68d5f1bd3000cc862f91c50c0a2f9.zip"}, "replies": [{"content": {"summary": {"value": "- This paper investigates evaluation awareness by introducing a steering vector that neutralizes the model's sensitivity to evaluation contexts, allowing controlled testing of evaluation effects.\n\n- The authors train models that explicitly differentiate between evaluation and deployment settings through three distinct approaches:\n(1) Type hints, the model produces type hints only during evaluation, not in deployment; (2) Wood Labs, introducing a signal that explicitly indicates an evaluation setting; and (3) Emoji usage, the model avoids using emojis during evaluation but includes them during deployment.\n\n- The experiments show that activation steering can successfully induce deployment behavior even when the model is operating in an evaluation context."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors design a well-controlled experimental setup to demonstrate that activation steering effectively removes evaluation awareness from the model. For instance, they introduce Wood Labs to ensure robustness against direct prompt-based cues of evaluation, and they extract the steering vector not from a model fine-tuned on the type-hint task but from the pre-fine-tuned model.\n\n- The results confirm the effectiveness of activation steering from the perspective of LM evaluation behavior."}, "weaknesses": {"value": "- The experiments are limited to relatively simple and structurally binary tasks, such as whether the model produces type hints or uses emojis, behaviors that are easy to control and evaluate. **However, real-world LM evaluations assess multiple dimensions, including completeness, coherence, and fluency, across a broad spectrum of tasks such as grounding, instruction-following, and planning.**\n\n- The study examines only the direct effect on the target behavior (i.e., type-hint generation), **without exploring potential trade-offs introduced by activation vector manipulation**. In practical evaluation scenarios, such activation steering could inadvertently affect fluency or alter behavior on unrelated tasks, leading to unintended performance degradation or biased assessment outcomes.\n\n- The effectiveness of activation steering has already been demonstrated in several prior studies. The contribution would be more substantial if the authors validated the robustness of activation steering on more realistic evaluation tasks."}, "questions": {"value": "How does activation steering perform on test sets that reflect the evaluation dimensions used in real LLM benchmarks? It would also be important to examine what kinds of capability trade-offs it introduces, and how it compares to prompt-based control when those trade-offs are taken into account."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J0Y6T9tj1y", "forum": "1TdRdf0fkw", "replyto": "1TdRdf0fkw", "signatures": ["ICLR.cc/2026/Conference/Submission22963/Reviewer_1uCF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22963/Reviewer_1uCF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768156865, "cdate": 1761768156865, "tmdate": 1762942453988, "mdate": 1762942453988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Large language models (LLMs) can sometimes detect when they are being evaluated and adjust their behavior to appear more aligned, compromising the reliability of safety evaluations. In our paper, we applied activation steering to make evaluation-aware models act like they are deployed during testing. We validate our steering method by training a model that acts differently in evaluation and deployment contexts. Our model can recognize a certain “evaluation cue.” When the cue is present, we found that simple prompts cannot make the model act like it is deployed, but activation steering could. \n\nReviewers noted several key strengths of our paper:\n\n1. Reviewers recognized that mitigation evaluation awareness is an important research problem. Reviewer fTiZ says that we “address a critical challenges in AI safety.” Reviewer KjZs calls it an “important and timely contribution: evaluation awareness is an increasingly concerning issue.”\n2. Reviewers found the experimental setup to be convincing and well-designed. Reviewer 1uCF says we have “a well-controlled experimental setup,” reviewer HqgU notes that “the experiments methodology is very clear and uses all the appropriate baselines,” and reviewer fTiZ states that “the approach is pretty well thought and technically sound. The experiments are thorough and the results are promising.” \n   1. Reviewer HqGU also noted that we studied the “model accuracy degradation when” applying activation steering.\n3. Reviewers found the paper to be extensive and easy to follow. Reviewer fTiZ states that the paper is “well-written and the figures/plots are effective.” Reviewer HqgU says that “code and technical details shared are very comprehensive.”\n\nTo address reviewer concerns and improve our manuscript, we have made the following updates, now reflected in the PDF:\n- We reran our model organism expert iteration process to make it more standardized.\n- We further optimized our steering setup (see Appendix G), sweeping over different steering layers and contrastive prompts, which help address concerns from reviewers HqgU and fTiZ.\n- We generally improved the paper writing and cited the new Claude Sonnet 4.5 system card (released after the submission deadline). Anthropic researchers used activation steering to suppress evaluation awareness during the pre-deployment testing of Claude Sonnet 4.5. \n- We ran some additional ablations, including additional red teaming of our model organism (Appendix I.1.1), more detailed results throughout expert iteration (Appendix K), and results on a simpler model organism (Appendix B). \n\n**None of our core results changed as a result of our new experiments.** While we’ve improved the phrasing in each paragraph, **we have the same four core results plots in the updated pdf (Figures 1, 3, 4, 5 in the new version correspond to Figures 1,2,3,4 in the old version).** The most significant change is that steering is more effective after we’ve optimized it a bit more. \n\nWe’ve addressed concerns by specific reviews in detailed responses to their comments below."}}, "id": "ZNUHaajbJT", "forum": "1TdRdf0fkw", "replyto": "1TdRdf0fkw", "signatures": ["ICLR.cc/2026/Conference/Submission22963/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22963/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22963/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763450925391, "cdate": 1763450925391, "tmdate": 1763450925391, "mdate": 1763450925391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose activating steering as a technique for controlling LLM evaluation awareness, constructing steering vectors from contrastive pairs. They find that steering the model towards \"deployment\" decreases certain behaviors when an environment cue is present to a comparable amount of removing the environment cue. They suggest that this technique could be used to suppress evaluation awareness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Important and timely contribution: evaluation awareness is an increasingly concerning issue\n- progression from toy example setup to natural world-example\n- empirical results are convincing, even though they are not extremely comprehensive (steering changes the model's expressed beliefs and behavior)\n- multiple baseline comparisons"}, "weaknesses": {"value": "- uncertain how this generalizes to frontier systems in the wild\n- type hint presence is a very simple signal, it might not capture deeper strategic deception or more complex behaviors\n- potential for adaption and counter-steering by the model (although this is acknowledged by the authors)"}, "questions": {"value": "- How robust is the steering when the model is queried over long dialogue?\n- Could a capable model learn to detect the steering itself?\n- Could the steering direction be identified automatically (e.g. via probes) rather than via manually curated contrastive prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FRCRwn7405", "forum": "1TdRdf0fkw", "replyto": "1TdRdf0fkw", "signatures": ["ICLR.cc/2026/Conference/Submission22963/Reviewer_KjZs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22963/Reviewer_KjZs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942684744, "cdate": 1761942684744, "tmdate": 1762942453751, "mdate": 1762942453751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LLMs are sometimes evaluation aware. This paper investigates using contrastive activation steering to trick a model into eliciting deployment model behaviour during evaluations. The authors successfully finetune a model to write Python type hints whenever it thinks it is being evaluated. And then show that steering the model towards deployment decreases this type hint rate to the same level as normal deployment, suggesting that the model does not think it is being evaluated.\nIt is a significant contribution in particular for safety evaluations, where identifying and mitigating awareness is critical to the veracity of the results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper proposes a very straightforward lightweight method to detect model awareness during evaluation. \n- The experiments methodology is very clear and uses all the appropriate baselines, and examines the random steering case. Code and technical details shared are very comprehensive.\n- It studies the model accuracy degradation when using the method.\n- As tested, the presented method outperforms simple prompting at reducing the type hint rate."}, "weaknesses": {"value": "- The paper could benefit from using a safety related benchmark since that would be the type of evaluations for which it would be most impactful.\n- Only one model is finetuned."}, "questions": {"value": "- Have you tried evaluating your set up with a safety related benchmark (e.g. WMDP)?\n- Can you expand on how you expect these results generalises to other models and evaluation tasks?\n- Was there an ablation on which layers to use for difference vectors injection?\n\n- Have studied the steering vector from the fine-tuned model and compared it to that of the pre-fine-tuned model? It would be interesting if perhaps there was no behaviour change."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KccZYktCL7", "forum": "1TdRdf0fkw", "replyto": "1TdRdf0fkw", "signatures": ["ICLR.cc/2026/Conference/Submission22963/Reviewer_HqgU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22963/Reviewer_HqgU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986841794, "cdate": 1761986841794, "tmdate": 1762942453560, "mdate": 1762942453560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigate into the problem of \"evaluation-aware\" LLM evaluation, which hypothesize that LLMs sometimes are aware of they are under evaluation (not actual deployment). The paper propose to use contrastive activation steering as a method to control evaluation awareness in LLMs. The authors train a model organism using synthetic document fine-tuning plus expert iterations (i.e.,, fine-tune on carefully selected prompt-response pairs) to exhibit the evaluation-aware behavior, on the coding task with python type hint writing. The main method is to constructing steering vectors from simple contrastive prompt pairs and subtract/add the vector during inference time. The experiments shows that activation steering successfully elicits deployment behavior even when evaluation cues are present in the prompt, whereas the direct prompting fails. Applying the steering vector can reduce the type hint rates from 90% to 40-50%, which matches the true deployment behavior."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper tries to address a critical challenges in AI safety, i.e., evaluation awareness undermines the reliability of safety evaluation before deployment. The evaluation awareness is a very interesting angle and the paper formulate the problem in a very clear way. The paper is well-written and the figures/plots are effective. I can see a few other strengths from the paper as the follows:\n\n1. The experimental design is strong. The model organism approach is methodologically sound. Different from prior work, the authors train a model with known ground-truth behavior differences between evaluation and deployment. This can clearly assess whether steering actually changes behavior rather than suppressing verbalizations of the evaluation awareness in the chain-of-thoughts process. Also, the authors well design the synthetic document fine-tuning (SDF) process, which avoids artificially strengthening evaluation/deployment representation comparing to directly fine-tuning with these examples. Overall the approach is pretty well thought and technically sound. \n\n2. The experiments are thorough and the results are promising. The paper includes multiple controls to explore the true effect introduced by the steering vector, such as comparing it with random noise vector, prompting baselines and test across expert iterations rounds etc. The results are quite strong, it shows that applying the steering vector can reduce the type hint rates from 90% to 40-50%, which matches the true deployment behavior."}, "weaknesses": {"value": "1. The scope of the evaluation is relatively limited. Particularly, the paper only evaluate one model (Llama Nemotron 49B) for one primary behavior (python type hint writing). The impose the question of how much the proposed method can be generalized to more model families, models trained differently and more tasks. This would limit the confidence in the method's broad applicability. Evaluation on frontier models (such as DeepSeek R1 etc.) will certainly help mitigate this concern. \n\n2. The paper lacks the discussion of the \"capability\" change due to the steering. The setup from the paper can show the effectiveness of the evaluation-awareness, however, the activation steering essentially changed the model behavior, how do we ensure the model we evaluated (with the steering) will perform the same way (or at least very similarly to) the original models in terms of the model accuracy? The evaluation awareness is only one aspect, however, the quality of the output before and after the steering should be the same. Otherwise, it will significantly decrease the value introduced by the method (as the model we are evaluating are NOT the same model we will deploy if the output quality vary too much)."}, "questions": {"value": "1. Have we evaluated the model generation quality for the model with and without activation steering? Are they on par with each other? I would strongly encourage the authors to include the quality comparison for the model  generation before and after activation steering in the main paper. \n\n2. How sensitive is the method to the specific wording of contrastive pairs? Have you tested paraphrased versions or systematically varied the prompts? How would that impact the steering performance?\n\n3. How would you construct the steering vector in the real world applications, where you won't know the ground truth deployment behavior? How we are going to pick the right contrastive prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vbOtCg1k46", "forum": "1TdRdf0fkw", "replyto": "1TdRdf0fkw", "signatures": ["ICLR.cc/2026/Conference/Submission22963/Reviewer_fTiZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22963/Reviewer_fTiZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058548495, "cdate": 1762058548495, "tmdate": 1762942453215, "mdate": 1762942453215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}