{"id": "1TdRdf0fkw", "number": 22963, "cdate": 1758337600305, "mdate": 1759896838392, "content": {"title": "Steering Evaluation-Aware Language Models To Act Like They Are Deployed", "abstract": "Large language models (LLMs) are sometimes aware when they are being evaluated and change their behavior accordingly. This poses a challenge for AI development, as it undermines the reliability of safety evaluations. In this work, we study activation steering as a technique for controlling LLM evaluation awareness. Specifically, we construct steering vectors using simple contrastive pairs of inputs such as ``You are talking to a real user'' and ``You are talking to an AI evaluator.'' We test this technique on an LLM trained using synthetic document fine-tuning and expert iteration to exhibit evaluation-aware behavior. The model recognizes a specific cue as always indicating evaluation and writes Python type hints only during evaluation, not deployment. This model almost always writes type hints when this evaluation cue is present. Steering the model towards deployment without removing the evaluation cue decreases the type hint rate to the same level as removing the cue and directly telling the model it's deployed. However, directly telling the model that it is deployed without removing the evaluation cue does not reduce the type hint rate at all. These findings suggest that activation steering is a promising technique to suppress evaluation awareness and improve our ability to conduct evaluations of frontier models.", "tldr": "", "keywords": ["AI safety", "Interpretability"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9c179984a48bcf9f9ac0d4cbfdf5f5fae2a9bd7f.pdf", "supplementary_material": "/attachment/b0fa3d9527c68d5f1bd3000cc862f91c50c0a2f9.zip"}, "replies": [{"content": {"summary": {"value": "- This paper investigates evaluation awareness by introducing a steering vector that neutralizes the model's sensitivity to evaluation contexts, allowing controlled testing of evaluation effects.\n\n- The authors train models that explicitly differentiate between evaluation and deployment settings through three distinct approaches:\n(1) Type hints, the model produces type hints only during evaluation, not in deployment; (2) Wood Labs, introducing a signal that explicitly indicates an evaluation setting; and (3) Emoji usage, the model avoids using emojis during evaluation but includes them during deployment.\n\n- The experiments show that activation steering can successfully induce deployment behavior even when the model is operating in an evaluation context."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors design a well-controlled experimental setup to demonstrate that activation steering effectively removes evaluation awareness from the model. For instance, they introduce Wood Labs to ensure robustness against direct prompt-based cues of evaluation, and they extract the steering vector not from a model fine-tuned on the type-hint task but from the pre-fine-tuned model.\n\n- The results confirm the effectiveness of activation steering from the perspective of LM evaluation behavior."}, "weaknesses": {"value": "- The experiments are limited to relatively simple and structurally binary tasks, such as whether the model produces type hints or uses emojis, behaviors that are easy to control and evaluate. **However, real-world LM evaluations assess multiple dimensions, including completeness, coherence, and fluency, across a broad spectrum of tasks such as grounding, instruction-following, and planning.**\n\n- The study examines only the direct effect on the target behavior (i.e., type-hint generation), **without exploring potential trade-offs introduced by activation vector manipulation**. In practical evaluation scenarios, such activation steering could inadvertently affect fluency or alter behavior on unrelated tasks, leading to unintended performance degradation or biased assessment outcomes.\n\n- The effectiveness of activation steering has already been demonstrated in several prior studies. The contribution would be more substantial if the authors validated the robustness of activation steering on more realistic evaluation tasks."}, "questions": {"value": "How does activation steering perform on test sets that reflect the evaluation dimensions used in real LLM benchmarks? It would also be important to examine what kinds of capability trade-offs it introduces, and how it compares to prompt-based control when those trade-offs are taken into account."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J0Y6T9tj1y", "forum": "1TdRdf0fkw", "replyto": "1TdRdf0fkw", "signatures": ["ICLR.cc/2026/Conference/Submission22963/Reviewer_1uCF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22963/Reviewer_1uCF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768156865, "cdate": 1761768156865, "tmdate": 1762942453988, "mdate": 1762942453988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose activating steering as a technique for controlling LLM evaluation awareness, constructing steering vectors from contrastive pairs. They find that steering the model towards \"deployment\" decreases certain behaviors when an environment cue is present to a comparable amount of removing the environment cue. They suggest that this technique could be used to suppress evaluation awareness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Important and timely contribution: evaluation awareness is an increasingly concerning issue\n- progression from toy example setup to natural world-example\n- empirical results are convincing, even though they are not extremely comprehensive (steering changes the model's expressed beliefs and behavior)\n- multiple baseline comparisons"}, "weaknesses": {"value": "- uncertain how this generalizes to frontier systems in the wild\n- type hint presence is a very simple signal, it might not capture deeper strategic deception or more complex behaviors\n- potential for adaption and counter-steering by the model (although this is acknowledged by the authors)"}, "questions": {"value": "- How robust is the steering when the model is queried over long dialogue?\n- Could a capable model learn to detect the steering itself?\n- Could the steering direction be identified automatically (e.g. via probes) rather than via manually curated contrastive prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FRCRwn7405", "forum": "1TdRdf0fkw", "replyto": "1TdRdf0fkw", "signatures": ["ICLR.cc/2026/Conference/Submission22963/Reviewer_KjZs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22963/Reviewer_KjZs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942684744, "cdate": 1761942684744, "tmdate": 1762942453751, "mdate": 1762942453751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LLMs are sometimes evaluation aware. This paper investigates using contrastive activation steering to trick a model into eliciting deployment model behaviour during evaluations. The authors successfully finetune a model to write Python type hints whenever it thinks it is being evaluated. And then show that steering the model towards deployment decreases this type hint rate to the same level as normal deployment, suggesting that the model does not think it is being evaluated.\nIt is a significant contribution in particular for safety evaluations, where identifying and mitigating awareness is critical to the veracity of the results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper proposes a very straightforward lightweight method to detect model awareness during evaluation. \n- The experiments methodology is very clear and uses all the appropriate baselines, and examines the random steering case. Code and technical details shared are very comprehensive.\n- It studies the model accuracy degradation when using the method.\n- As tested, the presented method outperforms simple prompting at reducing the type hint rate."}, "weaknesses": {"value": "- The paper could benefit from using a safety related benchmark since that would be the type of evaluations for which it would be most impactful.\n- Only one model is finetuned."}, "questions": {"value": "- Have you tried evaluating your set up with a safety related benchmark (e.g. WMDP)?\n- Can you expand on how you expect these results generalises to other models and evaluation tasks?\n- Was there an ablation on which layers to use for difference vectors injection?\n\n- Have studied the steering vector from the fine-tuned model and compared it to that of the pre-fine-tuned model? It would be interesting if perhaps there was no behaviour change."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KccZYktCL7", "forum": "1TdRdf0fkw", "replyto": "1TdRdf0fkw", "signatures": ["ICLR.cc/2026/Conference/Submission22963/Reviewer_HqgU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22963/Reviewer_HqgU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986841794, "cdate": 1761986841794, "tmdate": 1762942453560, "mdate": 1762942453560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigate into the problem of \"evaluation-aware\" LLM evaluation, which hypothesize that LLMs sometimes are aware of they are under evaluation (not actual deployment). The paper propose to use contrastive activation steering as a method to control evaluation awareness in LLMs. The authors train a model organism using synthetic document fine-tuning plus expert iterations (i.e.,, fine-tune on carefully selected prompt-response pairs) to exhibit the evaluation-aware behavior, on the coding task with python type hint writing. The main method is to constructing steering vectors from simple contrastive prompt pairs and subtract/add the vector during inference time. The experiments shows that activation steering successfully elicits deployment behavior even when evaluation cues are present in the prompt, whereas the direct prompting fails. Applying the steering vector can reduce the type hint rates from 90% to 40-50%, which matches the true deployment behavior."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper tries to address a critical challenges in AI safety, i.e., evaluation awareness undermines the reliability of safety evaluation before deployment. The evaluation awareness is a very interesting angle and the paper formulate the problem in a very clear way. The paper is well-written and the figures/plots are effective. I can see a few other strengths from the paper as the follows:\n\n1. The experimental design is strong. The model organism approach is methodologically sound. Different from prior work, the authors train a model with known ground-truth behavior differences between evaluation and deployment. This can clearly assess whether steering actually changes behavior rather than suppressing verbalizations of the evaluation awareness in the chain-of-thoughts process. Also, the authors well design the synthetic document fine-tuning (SDF) process, which avoids artificially strengthening evaluation/deployment representation comparing to directly fine-tuning with these examples. Overall the approach is pretty well thought and technically sound. \n\n2. The experiments are thorough and the results are promising. The paper includes multiple controls to explore the true effect introduced by the steering vector, such as comparing it with random noise vector, prompting baselines and test across expert iterations rounds etc. The results are quite strong, it shows that applying the steering vector can reduce the type hint rates from 90% to 40-50%, which matches the true deployment behavior."}, "weaknesses": {"value": "1. The scope of the evaluation is relatively limited. Particularly, the paper only evaluate one model (Llama Nemotron 49B) for one primary behavior (python type hint writing). The impose the question of how much the proposed method can be generalized to more model families, models trained differently and more tasks. This would limit the confidence in the method's broad applicability. Evaluation on frontier models (such as DeepSeek R1 etc.) will certainly help mitigate this concern. \n\n2. The paper lacks the discussion of the \"capability\" change due to the steering. The setup from the paper can show the effectiveness of the evaluation-awareness, however, the activation steering essentially changed the model behavior, how do we ensure the model we evaluated (with the steering) will perform the same way (or at least very similarly to) the original models in terms of the model accuracy? The evaluation awareness is only one aspect, however, the quality of the output before and after the steering should be the same. Otherwise, it will significantly decrease the value introduced by the method (as the model we are evaluating are NOT the same model we will deploy if the output quality vary too much)."}, "questions": {"value": "1. Have we evaluated the model generation quality for the model with and without activation steering? Are they on par with each other? I would strongly encourage the authors to include the quality comparison for the model  generation before and after activation steering in the main paper. \n\n2. How sensitive is the method to the specific wording of contrastive pairs? Have you tested paraphrased versions or systematically varied the prompts? How would that impact the steering performance?\n\n3. How would you construct the steering vector in the real world applications, where you won't know the ground truth deployment behavior? How we are going to pick the right contrastive prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vbOtCg1k46", "forum": "1TdRdf0fkw", "replyto": "1TdRdf0fkw", "signatures": ["ICLR.cc/2026/Conference/Submission22963/Reviewer_fTiZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22963/Reviewer_fTiZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058548495, "cdate": 1762058548495, "tmdate": 1762942453215, "mdate": 1762942453215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}