{"id": "wCBNxp1qWe", "number": 24727, "cdate": 1758359734496, "mdate": 1759896752423, "content": {"title": "Online Fitting Connectome-constrained Drosophila Whole-brain Model Reproduces Critical Resting-state Dynamics", "abstract": "The rapid growth of large-scale synaptic connectome maps and neural activity datasets has created an urgent need for connectome-constrained whole-brain models that can fit and interpret experimentally recorded neural data. A promising approach to bridge this gap is to train biologically inspired models using backpropagation through time (BPTT), which enables data-driven optimization of unknown model parameters. However, BPTT is inherently an offline method, with memory requirements that grow linearly with simulation time, making it impractical for training large-scale whole-brain networks over biologically relevant timescales. To address this challenge, we introduce an online learning framework for fitting whole-brain models using online gradient-based optimization. By updating parameters in a strictly forward-time manner, our method reduces memory consumption to a single time step, scaling only with the number of parameters rather than the entire temporal sequence. Using this framework, we construct a Drosophila whole-brain network comprising over 130,000 neurons and millions of synapses, where the network topology is fixed from the FlyWire connectome, and unknown parameters such as synaptic weights and cellular time constants are optimized to match in vivo resting-state neural activity. Our results show that this approach enables the training of large-scale Drosophila models over experimental timescales on a single GPU, a feat that is computationally prohibitive with BPTT. Remarkably, the optimization not only captures target dynamics but also spontaneously produces synaptic weight distributions that closely match empirical connectome statistics and drives the network toward a hallmark feature of resting state -- critical dynamics. Together, this work establishes an online, scalable, and data-driven framework for integrating anatomical and functional datasets, paving the way toward mechanistic whole-brain models at unprecedented scales.", "tldr": "", "keywords": ["whole-brain model; connectome; online learning; resting-state neural activity; critical dynamical state"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/33b936e4b5386c547e085e26926d84b7147cf08b.pdf", "supplementary_material": "/attachment/37441622d195d03533ed99d06c2309cbc24efa0c.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles memory-efficient parameter estimation in biologically plausible models of Drosophila (but really neural models more generally) neural networks.  The paper makes the observation that BPTT is not feasible for such large networks, and so applies a recent online method to reduce the memory footprint (at the expense of biasing gradients).  Some examination of the learned parameters are presented."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. I think this is a super promising line of work.  Developing memory- and work-efficient algorithms for fitting neural models is super exciting, because it allows us to study the variability in the underlying model between specimens, before/after specimen training etc.  If we could do this efficiently, then the model itself also becomes a learnable \"parameter\", which allows model discovery to come to the fore.  \n2. Actually building a mega-scale model of an entire connectome and hooking it up to an inference and learning algorithm is no small undertaking. It's great to aim big and try and fit the whole thing; so many papers would try and fit smaller-scale models. Kudos to the authors.\n3. The authors try and examine and pull apart the results with some depth and breadth, which can/could lead to some interesting insights down the road."}, "weaknesses": {"value": "# Summary\nUltimately, I do not feel like this paper is ready for publication.  This paper falls into an awkward position:  it is a (from a methodological angle, at least) fairly vanilla application of an existing methodology, to a model that predominantly exists; some new model components are presented, but there is no empirical evaluation of their validity.  In contrast, the paper angles itself as tackling one of the biggest open problems in neuroscience, and so there is quite a high bar for evidence for that.  While it is a promising start, the empirical results aren't as convincing as I would need to see to for the claimed \"science\", and the methodological/modeling innovations are lacking in a pure machine learning sense.  I really encourage the authors to continue this line of work though, because if you're successful this would be an _enormous_ breakthrough.  Good luck\n\n\n# Weaknesses (roughly in descending order of significance)\n1. The experimental evidence is weak.  It seems like the evidence hinges on Figure 3 (see my comments below).  I am not convinced a good generative model has been recovered, and the correlation plots are a fairly blunt instrument for extracting neuroscientific understanding.  From a methodological standpoint, I'd like to see much more analysis of the bias and variance of estimators, how these scale with data length and across methods/approximations, how accurately parameters are recovered for various synthetic network sizes etc.  On the neuroscience side, I'd love a thorough examination of some functional assemblies:  if we can't say \"this parameter is interesting\", then what even is the point of biophysically accurate models!  Comparing distributions of learned parameters to hypothetical parameters is not especially convincing either.  For instance, Lappalainen study activations under different inputs, where the expected activation pattern is known but not trained against.  This is the sort of super compelling neuroscientific evidence and insight that is required to really make this a compelling neuroscience contribution.\n\n2. On the experiments themselves:  it seems like most of the experiments are in relation to 500 time steps of training data for 73 neuropil regions?  This is a total of 36,500 observations (a lot of which are highly correlated).  If I understand correctly, your model has over 100k states and 15M connections.  Therefore, this model is unbelievably over-parameterized, even with the connectome constraints.  I am therefore _very_ dubious as to any insights drawn from the model, because this model could, essentially, fit any smooth system in a multitude of ways.  For instance, I would love to see a correlation analysis of learned parameters across different random initializations.  If the learned parameters are highly variable, then that is evidence that little reproducible scientific insight is being generated (despite getting good training reconstructions).\n\n3. There is some evidence for this in Figure 3, in fact, where the training reconstructions are perfect; but the test reconstructions are comparatively very poor.  This suggests to me that the training set is being overfit to, and results in less-than-ideal test predictions.  You can get good correlation by accurately predicting the quiescent point.  I'd like to see a self-normalized or cumulative distribution normalized analyses to really look into how accurately, in a more robust probabilistic/calibration sense, the predictions are.  See my question below, I don't understand the role or implementation of $F^{neuropil}$, but is there a chance that is driving the test reconstructions, and not the dynamics?  I'd like to see an ablation of that. \n\n4. The neural model you present is quite low fidelity -- it is essentially a single-compartment, net-excitatory-only, non-linear model of calcium dynamics.  Calcium dynamics are not the driver neural dynamics, and also act as a low-pass filter to potential dynamics.  As a result I question whether \"correctly\" fitting such a mechanistic model that is over simplified would ever lead to really meaningful neuroscientific insight.  This is the sort of really top-end insight that is needed when your paper sets out to do real science. \n\n5. The explanation of the model and inference methods are quite unclear (in my opinion).  I think these sections would benefit heavily from overhauling and improving the writing.  There is hardly any introduction or discussion of the strengths, weaknesses or applicability of DTRL. The exposition around the firing rate resolution (Fig 1D), as well as eq. (3), are especially unclear.  I'd love to really understand the neurobiological underpinnings of these models and why they are so useful, but the exposition, unfortunately, is exceptionally poor.  It is really difficult to get excited for a model where there are large parts of it I simply cannot parse out.  \n\n6. The paper takes quite a long time to get going, with notation, parameters equations etc spread over several pages.  IMO the paper could really benefit from a short \"Problem Setup\"-esque section that quickly, compactly and concretely defines the problem setup early on.  This would include parameters and their spaces;  transition and emission functions with their arguments and outputs etc. \n\n7. I get that this is essentially an application of D-RTRL to Drosophila data, but I would still like a more thorough examination (or evidence) of BPTTs non-applicability.  For instance, Aicher+ [UAI, 2020] explore truncating BPTT, trading off bias with cost.  There is some exposition on the difference between BPTT and D-RTRL in Section 4.1, but it's very short and not super informative.  Right now I would not be convinced into using your method for this problem.  Examination of this wider literature and really zooming in on the applicability of different methods (with experiments) would really shore up your methodological footing.  \n\n8. I am just about willing to believe Figure 5a, but that first and last point are doing a lot of work for that correlation/LoBF, some kind of sensitivity analysis/random repeats would be nice here just to ensure results are not buoyed by outliers.  Similarly, I do not find the avalanche results particularly compelling.  None of the result actually seem to follow a power law, and even if they did, I think its weak evidence without a lot more exposition on how these measurements were obtained/real-world validation/ablations etc.  i.e. Are these phenomena just a feature of networks generally?  What model components are critical to generate this effect? \n\n9. A batch size of 16 is huge in some domains.  Sometimes LLMs (for instance) use a batch size of one! \n\n10. Figure 4c is interesting, but why do we believe they should be aligned?  The whole hypothesis of this paper is that we can't know the functional weight ahead of time, and so the more right the hypothesis is, the worse this result becomes.  I, admittedly, do not know how to remedy this, but it is a difficult wrinkle in the analysis.\n\n11. I don't think some of the claims are borne out:  e.g. the y=x reference line claim in 433."}, "questions": {"value": "Q1. Does $f$ being ReLU mean that there can be no _net_ inhibition?  I.e., individual neurons can inhibit as much as other neurons have excited?\n\nQ2. Are the observations really at 1.2**Hz** (cf. Line 214).  This seems incredibly low to me, to the point where I would be amazed that we can resolve _any_ neural dynamics!  \n\nQ3. What is the low-rank approximation in Section 4.1?  This experiment needs much more introduction and discussion.\n\nQ4. Were multiple random seeds used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IN3dBsbczu", "forum": "wCBNxp1qWe", "replyto": "wCBNxp1qWe", "signatures": ["ICLR.cc/2026/Conference/Submission24727/Reviewer_2GYa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24727/Reviewer_2GYa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623277799, "cdate": 1761623277799, "tmdate": 1762943177534, "mdate": 1762943177534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors fit a connectome-constrained recurrent model of the drosophilia brain with single-neuron resolution. To avoid prohibitively large memory demands of BPTT, they perform this fitting using D-RTRL, an approximation that allows for memory-efficient parameter updating. They argue that their fitted model yields decent reconstruction of dynamics and functional connectivity at the level of neuropils. Further, they argue that their fitted weights reconstruct summary statistics and weight distribution properties of those found in the actual connectome. Finally, they argue that their fitted model reproduces observed criticality phenomena such as neuronal avalanches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The writing is reasonably clear, and the model formulation is easy to understand. \n- To the best of my knowledge, this paper presents the first connectome-constrained recurrent model of drosophilia neural dynamics that is fitted at the individual neuron level and the whole-brain scale."}, "weaknesses": {"value": "- The claim that the fitted model actually significantly reproduces drosophilia neural dynamics is quite unconvincing (Fig. 3). While it is indeed a tall task to actually predict neural activity well many timesteps into the future, the test-time predictions seem shockingly poor, especially for a connectome-informed model. The paper's claim that the model captures the true dynamics beyond the train set is vastly overstated, as segments labeled \"a\" and \"b\" feel cherry-picked. Summary statistics like functional connectivity, on the other hand, are much easier to estimate, and don't even require a dynamical model to obtain a reasonable estimate, so I find it only mildly impressive that the model slightly improves over the baseline of using the train-time functional connectivity estimate. \n- The implicit claim made by this paper is that the inclusion of detailed connectome information at the level of individual synapses and neurons yields a model that would otherwise not be able to reproduce various macroscopic properties of true drosophilia neural dynamics. Setting aside my concern above that the fitted model does not achieve this convincingly, the evidence that connectome constraints are playing a large role in shaping the properties of the final fitted model is also weak. In particular, there are no null models that are compared to. There are many sensible null models that one could use that should be equally easy to train: randomizing $\\text{sgn}_{ij}$, randomizing the overall connectivity graph imposed, partial randomizations that preserve in-degree, out-degree, total number of outgoing excitatory synapses, etc. Also, a simple RNN with units at the level of neuropils feels like a very accessible baseline. I strongly suspect that at least one of these null models could reproduce many of the macroscopic findings like the final weight distribution, moderate functional connectivity estimation gains, and criticality.\n- Limitations of the approach do not seem to be acknowledged."}, "questions": {"value": "- If graded connectome weights (as opposed to binary) are available, as suggested by Fig. 4, why are they not used in any way? If not for direct constraints on the weights, at least for initializations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LKLWNze6Z1", "forum": "wCBNxp1qWe", "replyto": "wCBNxp1qWe", "signatures": ["ICLR.cc/2026/Conference/Submission24727/Reviewer_4rxR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24727/Reviewer_4rxR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777653594, "cdate": 1761777653594, "tmdate": 1762943177297, "mdate": 1762943177297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a firing rate network model based on the FlyWire drosophila connectome with learnable weights and time constants, but fixed synaptic polarity. The authors note that BPTT memory usage scales as O(N * t) and instead use the D-RTRL method implemented within the BrainScale framework, which scales as O(N). The model is trained with per-neuropil calcium imaging data acquired in a resting state at 1.2 Hz and for 73 neuropils. Post-training evaluations show biologically plausible behavior of the trained network."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clearly written, easy to read, and cites relevant prior work.\n- Building scalable connectome-constrained models is an important area of research.\n- Evaluation of the trained model includes multiple aspects (functional connectivity, synaptic weight distributions, and criticality).\n- The paper includes source code."}, "weaknesses": {"value": "- Limited novelty, and diversity of the study and evaluation. The paper applies an existing algorithm to a new dataset and shows that multiple high level properties (avalanche statistics, synapse strength distribution, neuropil-level functional connectivity) have reasonable values. To make a strong statement about the applicability of D-RTRL to this type of models, ideally the authors would also compare to prior results obtained with BPTT, such as those of (Chen et al., 2022) or (Lappalainen et al., 2024).\n- The model includes a direct and learnable input from every neuropil to every neuron (Eq. 3), in addition to the inputs driven by the connectome. This is not biologically realistic and not justified well. The paper should explain this better, explicitly include the parameter count with and without this term, and include an ablation where this term is dropped, as well as analysis of resulting weights where the term is kept (for instance, how often does it happen that a neuron couples to neuropils with no existing synaptic connectivity?).\n- Limited control experiments. The authors attempt training their model with a linear readout instead of the standard connectome-weighted one (Eq. 4). Ideally one would also see experiments where the weight matrix itself (in I_conn) is perturbed to show that the real connectivity matters."}, "questions": {"value": "- line 135: \"BPTT, while effective, remains biologically implausible (Lillicrap et al., 2020)\". This sentence seems out of place. Biological implausibility is not a reason why the modeling approach cannot be extended to the whole brain scale.\n- line 427: \"where synaptic spine counts ...\": very few neurons in the fly brain have spines. Was the term \"spine\" intended here?\n- line 259: \"numerous sources of input remain unaccounted for during resting states\": what are those sources?\n- Fig. S5 is truncated\n- Fig. 2B shows that loss is lower with BPTT; does that matter?\n- Please report the timestep in the main text; otherwise \"biologically realistic\" timescales are hard to interpret.\n- The deconvolution model used assumes spiking neurons; but not all neurons of the fly are spiking; how is this accounted for?\n- Compared to the real connectome distribution, there are too many strong connections. Can you comment on why that might be the case?\n- How much memory would be needed to train the model with BPTT? Could you use multiple GPUs and compare the results?\n- Is there any data showing that training over longer horizons helps? How could that be quantified? Fig. 2B does not show an obvious gain with longer time horizons.\n- Isn't the R=0.75 of predicted train/test activity, which is higher than R=0.474 for real data, a sign of overfitting rather than \"robust temporal consistency\"?\n- Are the trained time constants biologically plausible?\n- Synaptic strengths can be estimated from EM. Could you please include an ablation in which they are kept fixed and a comparison between the actual strengths and the post-optimization ones in your study, computed at the synapse level? (e.g. a distribution of the real-trained weight difference).\n- The paper uses 138k neurons, but measures only the averaged outputs for 73 neuropils. Could one build a much simpler and lower-dimensional model that only uses mesoscale neuropil-level activity instead of tracking individual neurons? How would that impact the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YVrBcz1QHZ", "forum": "wCBNxp1qWe", "replyto": "wCBNxp1qWe", "signatures": ["ICLR.cc/2026/Conference/Submission24727/Reviewer_MzjY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24727/Reviewer_MzjY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970509585, "cdate": 1761970509585, "tmdate": 1762943177087, "mdate": 1762943177087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "his paper introduces an online learning framework to train a large-scale, connectome-constrained model of the Drosophila brain. Instead of using backpropagation through time (BPTT), which is computational intensive, the method updates parameters in a forward, online way, allowing training of 130K neurons and millions of synapses on a single GPU. The model fits real resting-state calcium imaging data and reproduces functional connectivity and neural dynamics. Interestingly, it also shows emergent biological features such as heavy-tailed synaptic weights and critical-state dynamics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Motivation:** This work is scientifically grounding, it is rooted in realistic structural constraints (FlyWire connectome) and supported by mechanistic biophysical modeling of neurons and neuropil aggregation.\n2. **Method:** It introduces an online learning framework that scales connectome-based neural fitting to whole-brain level, resolving BPTT’s scalability bottleneck, allows effective parameter tuning on a single GPU.\n3. **Biological relevance:** This model allows to fit data but reproduces critical-state dynamics, linking structural connectivity, optimized weights, and emergent function in a unified mechanistic system."}, "weaknesses": {"value": "1. **Predictive performance:** While the model reproduces broad activity patterns, there remains a visible mismatch between predicted and ground-truth neural traces, as well as differences in functional connectivity between training and test phases (Fig. 3). This indicates that the fitting may not fully capture detailed neural dynamics.\n\n2. **Generalization:** The work focuses primarily on resting-state data, leaving generalization to stimulus-driven or task-based brain dynamics unexplored. \n\n3. **Scientific insights:** Although the model reproduces criticality and synaptic weight distributions, it does not reveal new mechanistic principles or biological findings beyond reproducing known phenomena. And the presentation and writing style read more like a technical system report than a research paper emphasizing scientific discovery or hypothesis testing.\n\n4. **Baselines**: The paper lacks systematic comparison with alternative modeling frameworks (e.g., mechanistic models, recurrent networks) or other biologically plausible learning rules."}, "questions": {"value": "1. What factors cause the mismatch between training and test functional connectivity (Fig. 3)? Is this due to overfitting, noise in calcium imaging data, or limits of the model’s expressiveness?\n\n2. Could the authors discuss how their framework could be adapted or tested under task-driven or sensory-evoked conditions? This would clarify the general applicability of the method.\n\n3. Beyond reproducing criticality, can the model be used to generate new predictions or hypotheses about how structural connectivity shapes dynamics? \n\n4. Include more implementation details for reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jj8m3SJ3HX", "forum": "wCBNxp1qWe", "replyto": "wCBNxp1qWe", "signatures": ["ICLR.cc/2026/Conference/Submission24727/Reviewer_fvHS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24727/Reviewer_fvHS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982637461, "cdate": 1761982637461, "tmdate": 1762943176843, "mdate": 1762943176843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}