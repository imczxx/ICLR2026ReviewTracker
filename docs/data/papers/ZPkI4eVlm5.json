{"id": "ZPkI4eVlm5", "number": 5788, "cdate": 1757935246911, "mdate": 1759897953572, "content": {"title": "TRACEBench: Personalized Function Calling Benchmark Based on Real-World Human Interaction", "abstract": "Function calling has emerged as a central paradigm for augmenting the capabilities of Large Language Models (LLMs) by enabling them to transcend inherent limitations, particularly in accessing real-time information. Personalized tool utilization is essential for LLMs to adaptively select and invoke tools based on individual user profiles and historical interactions. However, most current benchmarks primarily rely on LLMs to simulate user interaction histories rather than using real-world interaction data, and these histories are typically short in length, offering limited evaluation of the model's ability to understand long-context inputs. \nIn this work, we introduce TRACEBench, a benchmark designed to evaluate LLMs‚Äô function calling capabilities in terms of tool, parameter, and temporal context personalization. \nA significant difference from prior work is our data sourcing strategy: \\textit{TRACEBench is built upon authentic user interaction histories collected from human volunteers}, which provides a realistic foundation of user behavior and has been anonymized to protect user privacy.\nFurthermore, we build long tool-use records to facilitate the evaluation and optimization of tool-augmented models' long-context understanding capabilities. \nWe perform reverse generation of user instructions from target tool calls, varying the level of instruction specificity to simulate different degrees of personalization. Extensive experiments offer insights into improving personalized LLM agents.\nOur code is available\\footnote{\\url{https://anonymous.4open.science/r/TRACEBench-5CC4}}.", "tldr": "", "keywords": ["Personalization", "Tool Use", "LLM Benchmarks", "Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5fbb5b990e30088c8f68f46130d11fa6cd98a5fb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- This paper introduces TRACEBench to evaluate LLMs function / tool calling abilities in personalized and long-context scenarios. It notes the limitation of existing benchmarks that relying on short, simulated user histories by instead using real, anonymized interaction data from human volunteers. \n- Introduced benchmark assesses personalization across tool selection, parameter use, and temporal context, and includes reverse generated user instructions with varying specificity to test how well models adapt to different personalization levels.\n- The benchmark is across 3 dim viz. Tool Personalization (implicit user preference from historical data), Parameter Personalization (lack of info or details / under constructed user queries) and Temporal Context Personolization (dynamic preference based on short term history, like recently used tools)"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Paper actually uses real user data from human users so this should reflect in the quality of their benchmark against existing benchmarks.\n-"}, "weaknesses": {"value": "See questions"}, "questions": {"value": "1. My main qualms with this work is the motivation. Tool calling and user personalization of LLM's responses are two orthogonal items. They should be evaluated on them differently. I don't find this benchmark useful and well motivated. In the example 1, in Fig 1, I find it unrealistic that user provides instruction to order an \"Luckin Iced Americano\" but w/o mentioning via which app (so tool can be selected). Like, then why not also infer which store for \"Iced Americano\" in this case. I'm sorry, but this is not making sense to me. If the example was, where user themselves have little idea on their preference and not just \"its implicitly buried\" then maybe I would understand. But tools are general things, and they're only handfuls that are overloaded for many users. So this doesn't make sense to me.\n1. How is Temporal Context Personalization (TCP) is not subset of Tool Personalization (TP)? Aren't they both based on user pref and their past data, just that TCP looks only at most recent things and TP at everything from past / user info.\n1. In Table 4, nothing is underlined or bold, even though the caption says they mean best and 2nd best respectively. \n1. The results miss many models from both closed and open source like GPT5, Gemini, Claude, Qwen3, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "This paper contains data from actual human volunteers. I am marking this to let S/AC know since I'm not aware how it works. This does not effect my review score or points whatsoever."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kF6mgLz7sn", "forum": "ZPkI4eVlm5", "replyto": "ZPkI4eVlm5", "signatures": ["ICLR.cc/2026/Conference/Submission5788/Reviewer_ygdZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5788/Reviewer_ygdZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761451600271, "cdate": 1761451600271, "tmdate": 1762918262430, "mdate": 1762918262430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the problem of personalization in LLM function calling, arguing that current research predominantly focuses on general-purpose tool use while overlooking user-specific needs. It is the first to systematically propose and formalize three types of personalization: Tool Personalization, Parameter Personalization, and Temporal Context Personalization. The authors construct the TRACEBench benchmark based on real, anonymized user interaction histories. A multi-stage pipeline (tool preparation, data collection, anchor behavior selection, and query generation) is used to create the dataset. Extensive experiments evaluate numerous open-source and closed-source models, revealing significant shortcomings in current models, especially in inferring parameters and leveraging long interaction histories."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper provides the first clear and structured definition of personalized function calling, categorizing it into three distinct dimensions (Tool, Parameter, Temporal Context) with formal definitions. This establishes a solid theoretical foundation for future research in this area.\n2.TRACEBench is built upon genuine, anonymized user interaction logs, unlike many existing benchmarks that rely on LLM-simulated data. This significantly enhances the benchmark's reliability, realism, and practical relevance.\n3.The experimental design is thorough. Beyond overall performance, it includes valuable analyses of the impact of query vagueness and interaction history length on model performance. These analyses effectively pinpoint the current bottlenecks in models for personalized function calling."}, "weaknesses": {"value": "1.While the use of real data is commendable, the dataset is relatively small, comprising only 10 users and 1,815 queries. This limited scale may affect the generalizability of the evaluation results and their statistical significance.\n2.Although the introduction of quantifiable vagueness levels is innovative, the queries themselves are synthesized by an LLM. This approach might introduce model-specific biases, potentially compromising the objectivity of the evaluation.\n3.While Temporal Context Personalization is proposed as a core dimension, the experiments do not include targeted comparisons or a separate, detailed analysis to specifically validate and dissect model performance on this capability. The discussion of results for this aspect remains somewhat high-level."}, "questions": {"value": "1.\tThe dataset currently includes only 10 users. Are there plans to expand the user base to improve diversity and representativeness? Is the inclusion of cross-cultural or multilingual user data being considered for future work?\n2.\tDuring the query generation process, how did you ensure semantic and logical consistency across different vagueness levels? Was human validation or any other method employed to control and verify the quality of the generated queries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l4ZZGssAT5", "forum": "ZPkI4eVlm5", "replyto": "ZPkI4eVlm5", "signatures": ["ICLR.cc/2026/Conference/Submission5788/Reviewer_zaJH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5788/Reviewer_zaJH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923347002, "cdate": 1761923347002, "tmdate": 1762918262140, "mdate": 1762918262140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main contents and contributions of this paper can be summarized as,\n\nFunction calling lets LLMs use tools in real time, but most benchmarks rely on short, simulated histories, limiting evaluation of personalization and long-context understanding. This paper introduces TRACEBench, built from anonymized, authentic interaction histories and long tool-use records, to assess tool, parameter, and temporal-context personalization. This paper reverse-generates instructions from target tool calls at varying specificity and reports experiments that reveal practical insights for designing personalized LLM agents (code available)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of this paper can be summarized as,\n\n- The authors open-source the code to facilitate readers‚Äô reproduction (replication) of their results.\n\n- This paper formalizes personalized function calling (tool, parameter, temporal), present TRACEBench built from real human histories to evaluate all three, and reports extensive experiments that benchmark LLMs and highlight challenges and future directions."}, "weaknesses": {"value": "The main weaknesses / questions of this paper can be summarized as,\n\n- Section 3.1 contains numerous imprecise and erroneous descriptions, *e.g.*, \n     - They define the candidate set as $ \\mathcal{T} = \\{(d(t_1), ..., d(t_N))\\} $ (docs), but then write $ t \\in \\mathcal{T} $. That‚Äôs inconsistent: $ \\mathcal{T} $ should be tools, not their docs. \n     - $ p $ should be a parameter assignment/argument tuple drawn from a tool-specific domain, not an untyped ‚Äúset‚Äù. \n     - The argmax domain ignores that the admissible parameters depend on the chosen tool.\n     - If documentation is used (as it typically is), it should appear in the conditioning; otherwise the probability term is underspecified.\n     - Equation (2) does not incorporate temporal information.\n\n- Section 3.2 contains numerous imprecise and erroneous descriptions, *e.g.*, \n     - ‚ÄúFunctionally equivalent tools‚Äù is used but no equivalence relation is specified, so the candidate subset is ill-posed.\n     - ‚ÄúWe term this phenomenon as tool personalization‚Äù ‚Üí ‚ÄúWe term this phenomenon tool personalization.‚Äù\n     - The text says preferences depend on non-functional attributes (price, speed, service) and user context/history, but none of these variables appear in the notation.\n     - The relation $\\succ_{(u,q)}$ has no stated\nproperties (e.g., totality, transitivity). If it is only a partial order, a unique maximizer need\nnot exist; tie-breaking is unspecified.\n\n- The key issues in Section 3.3,\n    - It‚Äôs unclear whether ùê¥ A is a set of parameter slots, a vector of values, or a tool-specific domain; parameters should be tool-dependent.\n    - Using linear-algebraic ‚Äúspan‚Äù for text/structured history is ill-defined; information dependence should be probabilistic or via a measurable mapping.\n    - No formal notion of time or decay; $\\mathcal{H}_u$ lacks timestamps and recency weighting.\n    - What happens if history is uninformative or conflicting is unspecified.\n    - The probability/utility that selects parameters is missing.\n    - Binary existence is too weak for $\\alpha$. Moreover, real cases involve multiple parameter slots; notation should handle vectors and per-slot missingness.\n\n- The key issues in Section 3.4,\n    - The authors mention ‚Äúshort-term‚Äù but never define a window $k$ or decay; timestamps are absent.\n    - The authors define $(c_i,o_i)$ but the rule uses only $\\{p_{r-k},\\dots,p_{r-1}\\}$; outputs $o_j$ (often the key temporal signal) are ignored.\n    - Writing $\\alpha\\in p_r$ and $\\alpha=f(\\{p_{r-k},\\dots,p_{r-1}\\})$ mixes a scalar with heterogeneous parameter vectors across tools, without a mapping between parameter spaces.\n    - ‚ÄúDoes not belong to long-term interests‚Äù lacks an operational criterion (no threshold/test to separate short-term from profile-based effects).\n    - What happens if recent history is uninformative or conflicting is unspecified?\n\n- In section 4.1, the authors say APIs are collected from real apps, but also that GPT-4o generates ‚Äúinitial designs and tool descriptions.‚Äù That makes the tools partly synthetic, undermining the claim of realism and threatening reproducibility/version drift.\n\n- ‚Äú8 scenarios‚Äù are asserted without selection criteria, diversity checks, or statistics (tools per domain, endpoints, arg types, auth modes). External validity is unclear (e.g., JD/Taobao only).\n\n- ‚ÄúManually check and optimize‚Äù lacks protocol, annotator count, QA criteria, and inter-rater agreement‚Äîhard to reproduce.\n\n- Are calls live, mocked, or sandboxed? No mention of rate limits, failures, retries, caching, or version pinning; results won‚Äôt be replicable.\n\n- The text mixes ‚ÄúAPP level‚Äù with API endpoints; it‚Äôs unclear what the agent actually calls.\n\n- No confirmation of ToS compliance, licensing for brand APIs, or mitigation of LLM prior knowledge leakage when GPT-4o drafts tools.\n\n- The authors assert ‚Äúauthentic, anonymized interaction data from volunteers,‚Äù but give no cohort size, demographics, languages, collection duration, domains per user, or basic stats (sessions/user, tool-calls/session, length distributions). No recruitment method, inclusion/exclusion criteria, compensation, or bias analysis; external validity is unknown.\n\n- ‚ÄúTemporal/logical consistency‚Äù lacks operational rules, metrics, annotator protocol, inter-rater agreement, or error rates; discarding ‚Äúincomplete/corrupted‚Äù sessions may bias away from realistic edge cases.\n\n- The authors don‚Äôt say how ‚Äútrue‚Äù tool choice/parameters/temporal dependencies are labeled or inferred from history.\n\n- Mapping logs to a unified API schema can erase vendor-specific heterogeneity (args, defaults, failure codes) that tool personalization depends on; no audit of information loss.\n\nOverall, introducing personalization is a promising idea; however, the paper reads rather rushed. Many parts feel awkward‚Äîespecially the definitions of equations and concepts‚Äîwhile the experimental design is mediocre and the analysis lacks depth."}, "questions": {"value": "The issues are mainly reflected in the weaknesses: the manuscript contains numerous unprofessional and non-rigorous descriptions, along with many incomplete sections; the authors are advised to substantially revise and improve the work before resubmission."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9fTu0CUUvP", "forum": "ZPkI4eVlm5", "replyto": "ZPkI4eVlm5", "signatures": ["ICLR.cc/2026/Conference/Submission5788/Reviewer_PWPU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5788/Reviewer_PWPU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929679760, "cdate": 1761929679760, "tmdate": 1762918261805, "mdate": 1762918261805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TRACEBench, a benchmark designed to evaluate personalized function calling capabilities in Large Language Models. The authors identify three key dimensions of personalization: tool personalization (selecting between functionally similar tools based on user preferences), parameter personalization (inferring missing parameters from long-term user habits), and temporal context personalization (reasoning over short-term dynamic context to infer parameters). The benchmark is derived from user interaction collected from human volunteers, rather than synthetic LLM-generated data. The authors construct a dataset comprising 1,815 queries from 10 users across 8 scenarios. They evaluate various open-source and closed-source models, finding that current LLMs struggle significantly with inferring correct parameter values, particularly as query vagueness increases."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper makes a valuable contribution by formally defining and distinguishing three types of personalization in function calling. The distinction between parameter personalization (long-term habits) and temporal context personalization (short-term dynamic context) is particularly insightful\n- The use of real human interaction logs collected from volunteers makes the benchmark valuable. The manual review process for anchor behavior selection and query generation further adds credibility."}, "weaknesses": {"value": "- The evaluation is restricted to a narrow range of models, with open-source models limited to the 7-8B parameter range. Given the importance of this benchmark, evaluation should include larger open-source models (13B, 70B variants) and additional API-based models to provide a more comprehensive assessment of the current state of personalized function calling capabilities.\n- The low performance on format following of xLAM, ToolACE, and watt-tool models raises concerns about the evaluation setup. These models perform strongly on other benchmarks, such as BFCL, suggesting that they may not have been tested with their native chat templates.\n- The paper would benefit from including sample data points with actual model outputs in an appendix. Showing concrete examples of queries at different vagueness levels, the corresponding ground truth function calls, and actual model generations would help readers better understand both the benchmark's challenges and the nature of model failures\n- Figure 3 uses a radar chart to visualize the performance across different vagueness levels. It is difficult to interpret and compare across models. It would help if the authors could explore other ways of presenting the results, such as with a bar chart or in a table.\n- The data collection methodology around real-user logs lacks crucial details, such as the recruitment criteria, geographic distribution, or demographic characteristics.. Without these details, concerns about selection bias and generalizability remain unaddressed.\n- The paper will also benefit from providing some practical guidance on where function-calling models currently lack and how to improve them in the future. Exploration of various prompting strategies to improve performance will also be beneficial."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u8689ZCtnn", "forum": "ZPkI4eVlm5", "replyto": "ZPkI4eVlm5", "signatures": ["ICLR.cc/2026/Conference/Submission5788/Reviewer_ar7G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5788/Reviewer_ar7G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5788/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762024826184, "cdate": 1762024826184, "tmdate": 1762918261521, "mdate": 1762918261521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}