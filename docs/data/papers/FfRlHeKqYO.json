{"id": "FfRlHeKqYO", "number": 11756, "cdate": 1758203550827, "mdate": 1759897556992, "content": {"title": "Hilbert Attention for Image Generation with Diffusion Models", "abstract": "Designing sparse attention for diffusion transformers requires reconciling two-dimensional spatial locality with GPU efficiency, a trade-off that current methods struggle to achieve. Existing approaches enforce two-dimensional spatial locality but often incur uncoalesced memory access. We present HilbertA, a 2D-aware and GPU-efficient sparse attention mechanism. HilbertA reorders image tokens along Hilbert curves to achieve a contiguous memory layout while preserving spatial neighborhoods, and employs a sliding schedule across layers to enable long-range information propagation without repeated or uncoalesced memory access. To further enhance cross-tile communication and positional awareness, HilbertA introduces a small central shared region. Implemented in Triton, HilbertA delivers comparable image quality with significant acceleration over prior methods on Flux.1-dev, demonstrating the feasibility of hardware-aligned two-dimensional sparse attention for high-resolution image generation. HilbertA delivers attention speedups of $2.3\\times$ when generating 1024 $\\times$ 1024 images, and up to $4.17\\times$ at 2048 $\\times$ 2048, while achieving image quality comparable to or surpassing baselines.", "tldr": "HilbertA maps 2D image tokens onto a Hilbert curve for coalesced GPU memory access, adds a layer-wise sliding schedule plus a small central shared region to maintain long-range and cross-tile context.", "keywords": ["Diffusion Model; Sparse Attention"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/afe70a64254401773cbd23b615c85406fe999adb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes HilbertA, a sparse attention mechanism for image diffusion models that preserves the 2D spatial locality of sparse attention tokens to achieve GPU-efficient implementation by leveraging the Hilbert space-filling curve. The proposed HilbertA reorders patchified tokens along a Hilbert curve to maintain spatial locality and partition them into local tiles. It then applies L sliding window cycles to enable cross-tile communication."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper presents a novel application of the Hilbert curve for GPU-efficient sparse attention computation by effectively preserving 2D spatial locality.\n- This paper clearly justifies the choice of the Hilbert curve by comparing it against various space-filling curves using two spatial locality metrics: edge average stretch and geometric distortion error."}, "weaknesses": {"value": "- This paper argues that HilbertA can achieve better speedup than CLEAR even with lower sparsity level by preserving 2D spatial locality. Therefore, it is important to evaluate the latency of HilbertA and CLEAR under fair conditions. While the Triton kernel design for HilbertA is described in detail, the latency evaluation setup for CLEAR is not clearly explained. Hence, the fairness of the latency comparison is uncertain.\n- The advantage of HilbertA over the conventional CLEAR appears marginal for resolution 1024. In Table 1, for resolution 1024, CLEAR with r=8 and HilbertA with 4 tiles achieve similar end-to-end latencies (12.85s vs. 12.83s), and the image quality metrics are also comparable. \n- The main evaluation with fine-tuning is reported only on Flux.1-dev, leaving the generalization capability of the proposed method unclear. \n- The term “Seq Len” in Table 2 is used without a clear definition. The experimental setup does not specify how many pixels correspond to a single token, making it difficult to correlate sequence length with image resolution."}, "questions": {"value": "1) Could you provide evaluation results on additional models (e.g., Stable Diffusion 3.5 Large, DiT-XL/2)?\n\n2) How is the latency of CLEAR measured? Please clarify the evaluation setup, including what kind of kernel implementation is used for the latency measurement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kW6iaATnMb", "forum": "FfRlHeKqYO", "replyto": "FfRlHeKqYO", "signatures": ["ICLR.cc/2026/Conference/Submission11756/Reviewer_k4Tv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11756/Reviewer_k4Tv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795521534, "cdate": 1761795521534, "tmdate": 1762922784605, "mdate": 1762922784605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new sparse attention mechanism for attention layers in the image generation framework. Specifically, they adopt the Hilbert scan order to reorder the tokens, making the memory continuous and enabling local attention. Besides, they propose to adopt different offsets and a central shared region to enhance and gather cross-tile information. Experiments demonstrate its efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. adopting hilbert order for sparse attention seems non - trivial.\n2. the presentation is clear and easy to follow."}, "weaknesses": {"value": "1. Missing comparison with Swin Transformer. This paper defines a sparse attention mode as each token focusing on interacting with the tokens within a local region. However, this kind of attention is already studied in the Swin Transformer, where each token is restricted to communicate with tokens in the same window. In this view, we highly doubt that using the simple shifted window mechanism would meet all the requirements of the sparse attention in this paper. Each window is local, memory continuous, and could exchange cross - window information using the shifting mechanism. The authors should clearly explain the differences and conduct the experiments to verify the results. \n\n2.  The metric GDE is not reasonable. Since the metric is computed for the whole sequences while the sparse attention only performs attention within a local region (such as the tiling operation in Hiber Attention). This misalignment makes the result in Fig.3 unsolid.\n\n3. Missing quantitative comparison with other scan orders. The author claims \"HilbertA reorders image tokens along Hilbert curves to achieve a contiguous memory layout while preserving spatial neighborhoods\", However, it seems all the scan orders in Fig.3 can achieve a contiguous memory layout while preserving spatial neighborhoods using tiling and sliding. Missing comparison with those scan orders leads to an unsolid conclusion.\n\n4. Unsolid experiment results. Although the Albert Attention achieves higher efficiency, its results on the image quality lag behind the competitors (especially for Sparse Attention). Why not adjust the hyper - parameter to achieve a better performance and show the latency at that setting? Besides, it is weird that Sparse Attention has a longer latency than Flux.1 - dev since Flux.1 - dev adopts the full attention. It seems that the Sparse Attention implementation is not optimal using Triton code. In such a case, we doubt that the high - efficiency performance of Hilbert Attention comes from the Triton implementation.\n\nWe indeed are very interested in this method. However, the above concerns also indeed hinder the acceptance. If the author addresses all the concerns, we would like to raise the score to 6 or higher."}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JyEoIHn3g6", "forum": "FfRlHeKqYO", "replyto": "FfRlHeKqYO", "signatures": ["ICLR.cc/2026/Conference/Submission11756/Reviewer_pCbB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11756/Reviewer_pCbB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839842025, "cdate": 1761839842025, "tmdate": 1762922783918, "mdate": 1762922783918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Hilbert Attention, a sparse attention mechanism designed for diffusion transformers to handle high-resolution image generation. Hilbert Attention rearranges image tokens using Hilbert curves, which helps keep memory access efficient and spatially coherent on GPUs. The method combines local attention tiles, sliding windows, and a central shared region to improve information flow. The author claimed that it speeds up attention computations while keeping the generated image quality acceptable compared to other methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of rearranging tokens using Hilbert curves is interesting and helps make GPU memory usage more efficient.\n- The method is practical, combining simple components like local tiles and sliding windows in a coherent way.\n- Clear experimental results demonstrating real efficiency improvements.\n- The analysis of the method (like attention patterns and receptive field) is insightful.\n- Applying the method to higher resolutions or multi-image conditions, as suggested, could further highlight its usefulness."}, "weaknesses": {"value": "- Table 1 is crowded and hard to read.\n- The generated image quality still noticeably drops compared to dense methods, limiting immediate practical use.\n- The experimental settings at 1024×1024 and 2048×2048 resolutions may not fully demonstrate the advantages of Hilbert Attention, larger or more complicated scenarios might show clearer benefits.\n- Important training details like LoRA ranks and dataset specifics are missing.\n- Testing Hilbert Attention on video generation (mentioned briefly in the paper) could help demonstrate its broader usefulness."}, "questions": {"value": "1. What LoRA ranks did you use in training?\n2. Could you give more details on the training dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MIg2DheFUr", "forum": "FfRlHeKqYO", "replyto": "FfRlHeKqYO", "signatures": ["ICLR.cc/2026/Conference/Submission11756/Reviewer_bhkz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11756/Reviewer_bhkz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963873839, "cdate": 1761963873839, "tmdate": 1762922783532, "mdate": 1762922783532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Hilbert Attention, a tailored token purmutation order for local window attention to ensure contiguous memory access when reading tokens within in the same window. To eusure cross-window communication, the paper introduces a sliding mechanism. Experiments demonstrate that the effectiveness of the proposed method on practical acceleration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Compared with the original form of window attention, the proposed method provides a smart way to accelerate it by permuting the token order.\n2. The analysis of the optimality of the Hilbert order is valid.\n3. The writing is generally clear."}, "weaknesses": {"value": "1. The physical meaning of the proposed attention method when $offset\\neq0$ is not clear. Although it can bring cross-tile communication, it is not natural. It is like a simple but not elegant extension of $offset=0$ and we have to accept it.\n2. The experiment results are not so strong. From the main table, the advantage of the proposed method is not significant in terms of neither performance nor efficiency. And there are many artifacts in the visualized samples. For example, in Fig. 5, the glass behiind the pizza is unnatural, which could originate from the ineffectiveness of the cross-window communication mechanism.\n3. The qualitative results of higher-resolution images like 2K are not sufficient."}, "questions": {"value": "Is it possible to combine the proposed permutation order with the rolling strategy in the original SWIN Transformer to devise more effective solutions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uGpk4wFEly", "forum": "FfRlHeKqYO", "replyto": "FfRlHeKqYO", "signatures": ["ICLR.cc/2026/Conference/Submission11756/Reviewer_Qg5H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11756/Reviewer_Qg5H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11756/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992776315, "cdate": 1761992776315, "tmdate": 1762922782862, "mdate": 1762922782862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}