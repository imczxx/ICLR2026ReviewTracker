{"id": "gzi13q8Wa8", "number": 13826, "cdate": 1758223251417, "mdate": 1759897410108, "content": {"title": "Understanding Memory in Neural Networks through Fisher Information Diffusion", "abstract": "Information retention and transmission are fundamental to both artificial and biological neural networks. We present a general theoretical framework showing how information can be maintained on dynamically stable manifolds that evolve over time while preserving the geometry of inputs. In contrast to classical memory models such as Hopfield networks, which rely on static attractors, our approach highlights evolving stable subspaces as the substrate of memory. A central contribution of our work is the use of dynamic mean-field theory to uncover a new principle: operating at criticality (spectral radius $\\approx 1$) is necessary but not sufficient for reliable information retention. Equally crucial—yet overlooked in prior studies—is the alignment between the input structure and the stable subspace. The theory leads to simple initialization rules that guarantee stable dynamics at the edge of chaos. We validate these rules in basic recurrent networks, showing that Fisher information–optimized initialization accelerates convergence and improves accuracy in sequential memory tasks, including the copy task and sequential MNIST compared to standard random initialization. Together, these results provide both principled design guidelines for recurrent networks and new theoretical insight into how information can be preserved over time.", "tldr": "", "keywords": ["Fisher information", "information diffusion", "initialization"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d52f89926ff348db809c8d6fb4c77ef908e7cfd.pdf", "supplementary_material": "/attachment/9e7819b663a13b34b7436cacfd6c050cad4183c1.zip"}, "replies": [{"content": {"summary": {"value": "This work presents a general theoretical framework illustrating how information can be maintained on dynamically stable manifolds that evolve over time while preserving the geometry of inputs. In contrast to Classical Associative Memory or Hopfield networks, which rely on static attractors, the paper highlights evolving stable subspaces as the substrate of memory."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It's an interesting work which explores Fisher Information with respect to the dynamical regimes of a recurrent neural network, whose neurons are split into distinct subpopulations, and helps illustrate the fact that ***memory*** can defined by how well the differences between stimuli are preserved as the network's activity evolves.\n\n2. Using the Fisher information, the network's weights can be initialized with this information, leading to better performance or training regime according to Fig. (6)."}, "weaknesses": {"value": "1. Limited experimentation. For example, in Fig. (6), there are no comparisons with typical weight initialization techniques. The paper does not also demonstrate whether the initialization method would provide any benefit to more complex RNN architectures like LSTMs, GRUs, or even State-Space Models (SSMs).\n\n2. Despite being interesting, it is difficult to see the big picture of the paper here, due to limited experimentation. Does the trend of Fig. (6) still hold for other datasets or tasks?\n\n3. As the authors state in their limitations, the entire framework is focused on how information is encoded and preserved. It offers no insight into the decoding mechanism."}, "questions": {"value": "1. This idea of dividing a set of neurons into $M$ distinct subpopulations is quite interesting. Could this be related to multi-head attention in some way?\n\n2. Indeed, the memory in Associative Memory networks is generally fixed, but they can be trained. Do you think you can use your framework to help eliminate some of the fixed **memories** that can cause spurious states in such networks? Your framework places an emphasis on the dynamics while AM emphasizes storage capacity; there must be a bridge between the two. Also, what about Dense Associative Memory (as your work did not mention about it)? \n\n3. Are the used images actually from CIFAR-10? I am very unfamiliar with the images you used. Should they not be 32 x 32 resolution and belonging to one of the these 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UXGZoTMMx6", "forum": "gzi13q8Wa8", "replyto": "gzi13q8Wa8", "signatures": ["ICLR.cc/2026/Conference/Submission13826/Reviewer_woJv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13826/Reviewer_woJv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761515799397, "cdate": 1761515799397, "tmdate": 1762924351009, "mdate": 1762924351009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a dynamical systems perspective on working memory in recurrent neural networks (RNNs). It offers a principled framework to study memory through Fisher Information (FI), quantifying how information about an input impulse of magnitude theta evolves over time. \n\nThe authors analyze how FI propagates through the network, how it relates to information retention and geometry preservation—that is, how pairwise distances d(xi,xj) between inputs are maintained during recurrence—and how these insights inspire initialization strategies  for improving information retention in RNNs. The experiments show the impact of initialization in that convergence is faster and final accuracy in higher in benchmark tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper presents a principled perspective on working memory in RNNs, grounded in information geometry with the concept of fisher information.  \n\nIt derives insights from first principles, i.e., brings a rigorous framework to analyze dynamics of retention of information using Fisher Information.\n\nS2. The use of information geometry is elegant and well-motivated. It provides clear geometric intuition. I believe it is also novel (in this context) although the limited related works make novelty hard to assess (see weaknesses).\n\nS3. The text is clearly written and easy to follow, even when introducing technical concepts."}, "weaknesses": {"value": "W1. The related work section is a bit weak, making it difficult to assess the paper’s novelty. Important prior studies are missing.\n\nFor example works on how memory arises from distributed neural dynamics (Cavanagh et al., 2018; Spaak et al., 2017; Meyers et al., 2008; Stroud et al., 2024; Brennan & Proekt, 2023; Kurtkaya et al., 2025). The paper should review existing studies of short-term, long-term, and working memory in RNNs, and clarify what analytical tools those works used.\n\nThe use of Fisher Information should be better situated within prior work in machine learning: where and how has FI been applied before, both in RNNs and other neural network types?\n\nRelatedly, several references are missing publication years, suggesting that the literature review could have been handled with greater care.\n\nW2. The broader goal of the paper is unclear: does it aim to model biological mechanisms of memory or to improve existing AI models? Either way, it would need either biological validation or corresponding baselines.\n\nW3. Experiments are insufficient (or insufficiently described).\n\nAre the results presented been tested across several random seeds? (esp for fig 5+6)?\n\nWhat are competitive approaches that have been proposed to preserve information retention in RNNs? How does the FI initialization compare to them?\n\nW4. Mathematics could be presented more rigorously.\n\nThe claim “This connection explains why preserving local geometry, maintaining stability at criticality, and ensuring Fisher information flow are mathematically equivalent conditions.” seems to strong since there is no theorem that proves the equivalence, only empirical results on a small set of RNNs."}, "questions": {"value": "Q1. It seems that the paper studies *working memory* and not *short-term memory*. Can you explain what distinguishes one from the other and why this paper is on working memory and not short-term memory? They seem related.\n\nQ2. Why was this specific RNN formulation chosen? Several variants exist (e.g., leaky currents, leaky firing rates), do the findings generalize across them? Is the equation in (1) the most biologically faithful representation? If explaining biology is not the goal, then more experiments across other architectures should be included.\n\nQ3. Can you clarify the mathematical connection between Fisher Information and geometry preservation? It seems to be explained in Appendix A.5, but it should be presented and discussed in the main text.\n\nQ4. Where does the “Fisher diffusion operator” come from? Is it a standard concept or newly introduced by the authors?\n\nQ5. Why is preserving the input stimulus considered desirable? Whether this is beneficial might depends on the specific task?\n\nQ6. How do the results vary with respect to the delay length (time over which information must be retained)? Does the initialization strategy need to change for longer delays?\n\nMinor: Typo between Twait and Tdelay."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f1uKoMAvfP", "forum": "gzi13q8Wa8", "replyto": "gzi13q8Wa8", "signatures": ["ICLR.cc/2026/Conference/Submission13826/Reviewer_WrqF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13826/Reviewer_WrqF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901282972, "cdate": 1761901282972, "tmdate": 1762924350457, "mdate": 1762924350457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Present a method for analyzing information flow in a recurrent network to determine its ability to maintain information about the stimuli, in a \"working memory\"-like setting. Going beyond a persistent representation of the stimuli by converging into a \"fixed-point\" representation, the method can describe dynamic maintenance of information as it propagates in the recurrent network. The theoretical work is based on a measure of Fisher information, which the authors can calculate exactly only for block matrices of either 2x2 structure or a feed-forward-only structure. They identify the need to operate at criticality, as well as maintain enough alignment between the inputs and the network's activations. This analysis provides a concrete suggestion for the \"correct\" scaling of values on network initialization."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed model combines a random structure (iid Gaussian) with a structure using a block structure, thus allowing for interesting results while maintaining some analytic handle, and can capture information propagation around the network, beyond the persistent activity option.\n * A beautiful mathematical framework where the non-linear network activation can be represented as a linear diffusion of the Fisher information. \n * An interesting \"optimal scaling\" of values on initialization."}, "weaknesses": {"value": "* A missing reference to a seminar contribution to this question, [White, Lee, Sompolinsky 2004].\n * The conclusion that \"architectures predicted to optimize Fisher retention also preserve stimulus geometry more effectively\" needs to be better supported; it is currently demonstrated qualitatively, from the similarity of the top and bottom rows of Figure 2.\n * The results in Figure 3 seem to suggest that optimality is achieved in a fine-tuned range of parameters, thus putting the relevancy of the results to any application at risk. The authors seem to suggest that there is a \"natural way\" to be in the correct range by using \"carefully placed feedback stabilizes and modulates\" in section 4, but do not provide a conclusive criterion for achieving such stability.\n * The results presented in section 5 in support of the theory are not impressive. A better baseline compared to \"no init\" should have been presented, where weights initialization is performed using a previously proposed method (e.g., Xavier initialization)."}, "questions": {"value": "* How would you suggest achieving the operational regime without fine-tuning of parameters (e.g., \"carefully placed feedback\")?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0i78JPnUpo", "forum": "gzi13q8Wa8", "replyto": "gzi13q8Wa8", "signatures": ["ICLR.cc/2026/Conference/Submission13826/Reviewer_gb9t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13826/Reviewer_gb9t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991080128, "cdate": 1761991080128, "tmdate": 1762924349817, "mdate": 1762924349817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}