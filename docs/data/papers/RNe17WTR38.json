{"id": "RNe17WTR38", "number": 23592, "cdate": 1758346055248, "mdate": 1759896805977, "content": {"title": "Self-Evolving Language Models via Simple Generator-Verifier Games", "abstract": "Post-training language models often depends on costly external signals such as human annotations or domain-specific rewards. As an alternative, we explore model self-evolution through the lens of simple generator–verifier games. A single base model plays both roles---generating candidate solutions and verifying/improving their quality---to construct preference data for fine-tuning. To extract reliable signals from noisy self-verification, we leveraging _thresholded majority voting_, which approximates high-precision preference pairs. The approach enables self-evolution on  synthetic logical reasoning and realistic mathematical reasoning tasks, even when models initially perform poorly. For example, on the Knights and Knaves benchmark, accuracy rises from 31.0% to **40.7%** with single-turn verification, **42.2%** with multi-turn verification, **44.1%** with iterative training, and **44.8%** with curriculum learning. Notably, models trained only on easier instances generalize effectively to harder test data, demonstrating _emergent easy-to-hard generalization_. These results show that simple generator-verifier games can unexpectedly enhance reasoning in small models, offering a new perspective on concurrent research in self-improvement and RL with verifiable rewards.", "tldr": "We analyze self-improvement with self-generated data.", "keywords": ["self-training", "post-training", "language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/066bc19a1204c188798ef1036ca490bdb793c681.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a self evolution framework in which a single language model acts as both generator and verifier to construct preference data for self improvement. The method introduces two variants, SimpleGV (single turn verification) and RevisionGV (multi turn verification), and applies iterative DPO. The authors evaluate the approach mainly on mathematical reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly written and conceptually easy to follow."}, "weaknesses": {"value": "**Limited novelty.**\\\nThe proposed framework of self-generation and self-verification is not new. Similar iterative preference learning or self-rewarding approaches have already been explored in prior works such as [1,2,3,4,5]. The paper differs only in minor details (e.g., thresholded verification, curriculum scheduling), which do not constitute meaningful methodological novelty. It also overlaps heavily with [6] in both structure and training procedure.\n\n**Lack of proper comparison and discussion.**\\\nThe paper does not compare against the most relevant prior works on self-improvement or self-rewarding LMs. There is also little discussion on how the proposed method differs conceptually or empirically from existing iterative preference optimization frameworks.\n\n**Weak experimental validation.**\\\nThe experiments are narrow in scope (e.g., training on a single dataset, using only two models, and evaluating mostly on math relaative easy reasoning tasks). There are no experiments on other domains (e.g., coding) or stronger reasoning benchmarks (e.g., AIME, AMC). As a result, the claims of general self-evolution remain unsubstantiated.\n\n**Cost inefficiency.**\\\nThe generator–verifier framework requires multiple generations and verification passes, increasing computational cost significantly. However, the observed performance gains are modest, suggesting poor cost–performance trade-off.\n\n**Overall.** The method appears to be a minor extension of existing self-reward/self-evolution frameworks, without demonstrating clear advantages or generality.\n\nReference\\\n[1] Self-Rewarding Language Models\\\n[2] Self-consistency preference optimization\\\n[3] Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge\\\n[4] CREAM: Consistency Regularized Self-Rewarding Language Models\\\n[5] ARIES: Stimulating Self-Refinement of Large Language Models by Iterative Preference Optimization\\\n[6] ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification"}, "questions": {"value": "See the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QegaM9gbok", "forum": "RNe17WTR38", "replyto": "RNe17WTR38", "signatures": ["ICLR.cc/2026/Conference/Submission23592/Reviewer_b2eD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23592/Reviewer_b2eD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761023958555, "cdate": 1761023958555, "tmdate": 1762942726490, "mdate": 1762942726490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies generator-verifier games where the same instruction‑tuned model acts as a generator that proposes multiple answers and a verifier that judges whether each answer is correct. Preference pairs are then formed and used to fine‑tune the model with DPO. The paper studies two variants: SimpleGV, which involves single‑turn, verifier as a judge with thresholded majority voting, and RevisionGV, which uses multiple turns, generator revises responses using verifier feedback. The method is evaluated on synthetic KK puzzles and math benchmarks, with strong KK improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Timely problem: Overall the problem the paper aims to solve is good. Reducing dependence on human labels or domain-specific verifiers is very important, and a single model generator/verifier is very a simple formulation.\n* Clear Writing: The paper does well at explaining how they form preference pairs and provides concrete settings. I found this easy to follow.\n* Ablations: The ablations on KK were really useful and important giving a sense of how the synthetic task behaves."}, "weaknesses": {"value": "I think this paper needs additional evaluations to be stronger and seems unfinished. I list my concerns below:\n\n* Improvements on Real Datasets: The paper finds really small improvements on datasets like GSM-8K or MATH-500 in comparison to the synthetic task. I think this needs more analysis, see one concern below on this thread.\n* Distribution Leakage: I was not familiar with OpenThoughts3, but I looked into it and found that it was really close in distribution to GSM-8K and MATH-500. It aggregated reasoning problems. I wonder if this leads to inflated gains and the modest gains seen are from using OpenThoughts3 rather than the set up seen here. \n* Hyperparameters: How are hyperparameters selected? My reading right now is that they are selected using the testing set... \n* Verifier Signal: The core claim of the paper is that we can extract reliable signals from noisy self-verification via thresholded voting. But, when measured against a strong model with access to ground truth, the unsupervised verifier is only ~60% accurate on KK with Gemma‑4B. At scale, this is far from real preference labels. Even when making the model larger, you're still topping out at around 91%. \n* Small Models: In the abstract, the authors claim that the approach \"enhances reasoning in small models\" yet Gemma‑1B shows little to no benefit. In the limitations section, this is acknowledged as well. I would reduce claims on generality in the abstract implied upfront. \n* Novelty: Conceptually, I am having trouble differentiating between this method and self-refinement. To me, the paper is in this way, purely empirical. The major problem with this is that the main improvement the paper demonstrates is with KK and very limited improvement on the math benchmarks. Without stronger gains or deeper analysis, the contribution doesn't feel very strong to me.,"}, "questions": {"value": "* Could the authors discuss OpenThoughts3 and its training distribution a bit more?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vseL6b8iMm", "forum": "RNe17WTR38", "replyto": "RNe17WTR38", "signatures": ["ICLR.cc/2026/Conference/Submission23592/Reviewer_Lso3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23592/Reviewer_Lso3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914021227, "cdate": 1761914021227, "tmdate": 1762942726197, "mdate": 1762942726197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Simple Generator–Verifier (GV) Games, a self-evolution framework that enables a single language model to generate and verify its own outputs without external supervision. By constructing high-confidence preference pairs through thresholded majority voting and optimizing via DPO, the model self-improves its reasoning ability. The authors demonstrate consistent gains in reasoning accuracy and easy-to-hard generalization across multiple benchmarks, showing that simple self-verification can approach supervised performance efficiently."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper conducts extensive experiments across multiple benchmarks, model scales, and ablation settings, demonstrating robustness and reproducibility.\n\n2. The problem setting—learning from unlabeled prompts and unverifiable tasks—is timely and important.\n\n3. The idea of turning a model’s own verification capability into a generator–verifier game, rather than relying solely on majority voting, is novel."}, "weaknesses": {"value": "1. The paper lacks sufficient explanation of baseline methods such as INTUITOR, Absolute Zero (AZR), and AZR-Coder—it is unclear how these baselines are implemented or differ from the proposed approach.\n\n2. Although the related work section mentions Absolute Zero and R-Zero, there is no direct empirical or conceptual comparison, making it difficult to assess the advantage of the proposed method.\n\n3. Without a comparison to an external or supervised verifier, it is hard to evaluate how much of the improvement comes from self-verification itself versus incidental effects of DPO fine-tuning.\n\n4. Different tables use different model backbones (e.g., Table 1 uses Qwen2.5-7B-Instruct, while Table 4 uses Gemma-3-1B-it), raising concerns about consistency and fairness in comparison.\n\n5. Figure 2 shows that the SimpleGV verifier accuracy improves over the base model, but since the verifier is not explicitly trained, it is unclear why or how this improvement emerges.\n\n6. In Table 1, several baselines perform worse than their original reports (e.g., GRPO with Qwen2.5-7B drops from 90.2 → 82.9), yet no interpretation or justification is provided.\n ---\nPresentation.\\\na. The gray-highlighted rows in the tables are difficult to interpret .\\\nb. The numbers in Figure 5 are too small and hard to distinguish.\\\nc. The texts in Table 3 are overly long, making it hard to understand each configuration clearly"}, "questions": {"value": "1. How would performance change if the verifier were trained or fine-tuned separately rather than sharing parameters with the generator?\n\n2. Is the model update applied only to the generator role, or does it indirectly improve verification ability as well?\n\n3. Could the authors clarify whether iterative GV training leads to verifier drift (i.e., changes in its reliability over iterations)?\n\n4. Could the authors provide more explanation and evidence for lines 264–266 — specifically, what causes “redundancy and verifier noise to begin to dominate” beyond a moderate model size?\n\n5. Could the authors include an error case analysis to better illustrate the failure modes of self-evolution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HCPLEDkuyA", "forum": "RNe17WTR38", "replyto": "RNe17WTR38", "signatures": ["ICLR.cc/2026/Conference/Submission23592/Reviewer_pUW8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23592/Reviewer_pUW8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943245733, "cdate": 1761943245733, "tmdate": 1762942725881, "mdate": 1762942725881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes as test time inference method relying on majority voting with an LLM as a judge utilized for a single-turn and multi-turn setup."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper evaluates on several empirical domains such as knights and knaves and also performs ablations over thresholds and data and model sizes"}, "weaknesses": {"value": "- LLM as a judge and Majority Voting/Ensembling has been well studied in prior work (e.g [1]) thus is unclear the main contributions of this work with respect to these approaches\n- Several baselines (e.g MCTS) from the test time inference literature hasn't been compared to in this work, making it hard to judge the contribution of the proposed method\n\n[1] Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters"}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UCioUZ8Bmt", "forum": "RNe17WTR38", "replyto": "RNe17WTR38", "signatures": ["ICLR.cc/2026/Conference/Submission23592/Reviewer_eTrK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23592/Reviewer_eTrK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762102133042, "cdate": 1762102133042, "tmdate": 1762942725624, "mdate": 1762942725624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}