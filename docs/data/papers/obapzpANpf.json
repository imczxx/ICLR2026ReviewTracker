{"id": "obapzpANpf", "number": 5697, "cdate": 1757927635977, "mdate": 1759897960073, "content": {"title": "DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery", "abstract": "The rapid advancement of large language models has fundamentally shifted the bottleneck in AI development from computational power to data availability—with countless valuable datasets remaining hidden across specialized repositories, research appendices, and domain platforms. As reasoning capabilities and deep research methodologies continue to evolve, a critical question emerges: can AI agents transcend conventional search to systematically discover any dataset that meets specific user requirements, enabling truly autonomous demand-driven data curation? We introduce DatasetResearch, the first comprehensive benchmark evaluating AI agents' ability to discover and synthesize datasets from 208 real-world demands across knowledge-intensive and reasoning-intensive tasks. Our tri-dimensional evaluation framework reveals a stark reality: even advanced deep research systems achieve only 22% score on our challenging DatasetResearch-pro subset, exposing the vast gap between current capabilities and perfect dataset discovery. Our analysis uncovers a fundamental dichotomy—search agents excel at knowledge tasks through retrieval breadth, while synthesis agents dominate reasoning challenges via structured generation—yet both catastrophically fail on ''corner cases'' outside existing distributions. These findings establish the first rigorous baseline for dataset discovery agents and illuminate the path toward AI systems capable of finding any dataset in the digital universe. Our benchmark and comprehensive analysis provide the foundation for the next generation of self-improving AI systems. The code and dataset will be open-sourced soon.", "tldr": "", "keywords": ["Benchmark", "AI Agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/30a3516c6473d5ccc988ff134c924af3cc55cfca.pdf", "supplementary_material": "/attachment/3f3c4d8827b9ead8e7a9e39c74d47c00e80724a9.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces DATASETRESEARCH, a benchmark for evaluating agent systems on demand-driven dataset discovery and synthesis. It curates 208 real-world dataset demands (from Hugging Face and Papers with Code) and pairs each with a reference dataset and reference metadata (MetaTriplets). Agents are assessed along three axes: (i) metadata alignment (o3-judged semantic similarity), (ii) few-shot performance (1/3/5-shot), and (iii) fine-tuning performance on LLaMA-3.1-8B, with scores normalized by the reference-data upper bound. Baselines span search agents, synthesis agents (o3-generated 500-sample datasets), and deep research agents. Results show a clear split: search excels on knowledge-based tasks, synthesis on reasoning-based tasks, while all methods struggle on the harder DatasetResearch-pro subset (best ≈0.22), highlighting substantial headroom for hybrid and more generalizable approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Turns “find or build the dataset that matches a natural-language demand” into a measurable benchmark with paired reference data and metadata, covering the full path from requirements to downstream utility.\n\nCombines intrinsic (metadata similarity) and extrinsic (few-shot and fine-tuned task performance) measures, with normalization that enables comparison across heterogeneous NLP tasks.\n\nSystematically contrasts search, synthesis, and deep research paradigms, revealing a knowledge vs. reasoning specialization and consistent failure on corner cases—useful guidance for designing future hybrid agents."}, "weaknesses": {"value": "- The same model family (o3) is used to generate reference metadata/demands, parse discovered data, and score alignment—inviting self-consistency bias rather than genuine agreement, and masking contamination through stylistic echoing.\n- Overreliance on closed-source systems (o3 for synthesis/judging; GPT-4o/Deep Research variants for search) undermines reproducibility, accessibility, and cost realism. Results may reflect vendor-specific capabilities rather than agent design quality.\n- No systematic comparisons with open retrieval stacks (e.g., BM25 + dense retrievers/ColBERT), open reasoning LMs (e.g., Llama-3.x-70B, Mistral-Large-Instruct, Qwen2.x/3-Instruct), or open toolformer/agent frameworks.\n- Despite broad claims, coverage is text-only across six NLP tasks; no CV/audio/tabular/time-series/multimodal demands; limited external validity.\n- The benchmark is closer to a controlled template-matching exercise that reproduces a known target than to open-world dataset discovery. In practice, the “task” is largely to find (or approximate) someone else’s already-curated dataset given a stylized natural-language demand. But in real settings, you usually don’t have such ready-made, perfectly matched datasets—you have to prospect, acquire, clean, align schemas, and handle licensing/privacy—so the setup falls well short of real-world data discovery."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The benchmark overlooks copyright/licensing risks—agents may retrieve or synthesize data that copies gated or restricted sources without permission or proper attribution."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oJqNgdx3Kp", "forum": "obapzpANpf", "replyto": "obapzpANpf", "signatures": ["ICLR.cc/2026/Conference/Submission5697/Reviewer_A7PE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5697/Reviewer_A7PE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760673447834, "cdate": 1760673447834, "tmdate": 1762918202878, "mdate": 1762918202878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper points out that relevant data forms a crucial bottleneck to advance AI models. The authors seek to answer if “conventional data search” methods can be replaced with “AI-agent search.” To answer this question, they propose a new benchmark aimed at evaluating AI agents’ ability to discover and synthetically generate datasets. Sourcing data from Hugging Face and papers with code, the authors built a benchmark that automatically generates data demands split between knowledge-intensive and reasoning-intensive tasks. They use this benchmark to evaluate several leading models and conclude that current models do not perform well. The authors further show that “search agents” do better at knowledge tasks while “synthesis agents” outperform on reasoning tasks. Both agent types perform poorly on “corner cases.”"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "[**significance**] The authors correctly identify data and data discovery as an important challenge to improving AI models. Efforts aimed at creating benchmarks designed to isolate capabilities useful for automating such challenges is an important endeavor."}, "weaknesses": {"value": "[**clarity**]\n- The paper (rightfully) emphasizes the importance of data to further advance AI models. Unfortunately, the problem specification is overly vague, entangling different challenges and data use cases. For example, the abstract mentions “countless valuable datasets [...] and domain platforms]” (l13-14], but does not specify if these are hidden due to access constraints or limitations attributable to search algorithms. \n- The word “synthesis” is used several times in the introduction without defining its meaning.\n- The experimental setup misses many important details\n\n\n[**quality**]\n- The related work section is severely lacking. For example, it appears to completely ignore decades of information retrieval research. The literature on synthetic data generation similarly lacks any discussion of core concepts like diversity, complexity, and quality. Also absent is the extensive literature on “retrieval augmented generation” (RAG) based systems.\n- The core contribution of this work is a new benchmark designed to simulate real-world data discovery. However, the methodology used to create this benchmark appears to have various questionable aspects (see questions). \n- The paper seemingly uses OpenAI’s o3 model for every step of the pipeline. This reviewer fears that any takeaways or analysis is therefore overly biased and does not necessarily generalize.\n- Reported metrics lack confidence intervals.\n- In Section 5.2, the authors write “we identify that [...] instruction-following capabilities” (l431-448). As OAI o3 is used to generate data, this can simply reflect existing data knowledge of o3, rather than any relation to retrieved or “discovered” data. This is an important confounding factor not accounted for in the empirical evaluation.\n\n[**significance**] The authors claim that their work provides “the foundation for the next generation of self-improving AI systems” (l29-30), which is a lofty claim that does not appear to be supported by empirical or theoretical evidence."}, "questions": {"value": "Q1. Confusing notation: “Given a natural language [...] the specified demand D” ( l146-148), in this notation, what is the subscript “d” in S_d? Do the authors mean a “set” of datasets? This continues in l150-153, where now “r_i” is used without introduction.\n\n\nQ2. In Section 3.2, Step 1, the authors write that “gated” datasets are used to mitigate data leakage. What evaluation was performed to check against data leakage? \n\n\nQ3. In Section 3.2, Step 2 and 3, a number of filtering steps are performed to narrow down the dataset candidates. If the goal of this challenge is to measure models on “realistic” conditions, these steps appear to strongly bias the remaining set towards an unrepresentative sample.\n\n\nQ4. In Section 3.2, Step 6-7: What were the rejection criteria to decide if a dataset was “unsuitable for fine-tuning” (l236), and what were the criteria used to check if the generated meta data and demand descriptions are faithful to the underlying data and ecologically valid? (l235-241)\n\n\nQ5. In Section 3.2, l250-260, the authors propose a binary classification of knowledge-based vs. reasoning-based tasks. Yet, to this reviewer, it appears that *many* queries require a combination of these two. Could you please provide the systematic rubrics used to annotate these tasks? Were any consistency checks performed, e.g., cross-annotator agreement?\n\n\nQ6. In 3.3 it is claimed that using OpenAI’s o3 model for scoring both reference and discovered metadata mitigates potential scoring biases (l296-l297). This claim lacks evidence, e.g., are the reference and discovered metadata distributions similar? Is scoring consistent across different types of underlying data? Is scoring robust across multiple samples and/or prompts?\n\n\nQ7. The authors report a “Normalized Score” (l321), which finetunes a model on a reference dataset and uses this as the “theoretical maximum performance achievable” (l316-317). First, this assumes that a reference dataset contains both a train and test subset. Second, this reviewer sees no reason why combining one or multiple other datasets could not lead to a better performance. For example, training a model on a challenging math dataset and evaluating it on a simpler reference dataset fits this scenario. As such, what is the difference of dividing the S_{\\text{eval}} by an arbitrary fixed number, given that scores are now “on a scale from 0 to 1, or higher” (l322)? \n\n\nQ8. A “synthesis agent” uses OAI o3 to generate 500 data samples (l357): How? What criteria are used to evaluate these samples?\n\n\nQ9. How is finetuning done?\n\n\nQ10. Key experimental setup details are missing to explain how the “deep research” systems of various providers were evaluated. The text mentions manual actions: what were these?\n\nSuggestions:\n- typo: “evaluable” (l204)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LoWNxf4lmh", "forum": "obapzpANpf", "replyto": "obapzpANpf", "signatures": ["ICLR.cc/2026/Conference/Submission5697/Reviewer_Puqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5697/Reviewer_Puqg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834006002, "cdate": 1761834006002, "tmdate": 1762918202402, "mdate": 1762918202402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores a novel agent application that utilizes LLMs to discover or synthesis datasets that meets specific user requirements. Its major contribution is the introduction of a dataset discovery benchmark covering 208 types of demands covering knowledge-intensive and reasoning-intensive tasks. The paper hopes the benchmark and analysis can benefit the progress of self-improving AI systems."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "If well-justified, dataset discovery would be an interesting direction for LLM agents to explore."}, "weaknesses": {"value": "1. The paper still needs in-depth justification on the motivation of dataset discovery demands. It is always intriguing to utilize LLM-based agents for exploring different applications. However, it is still lacking examples of practical use cases for human users to utilize dataset discovery agents.\n\n2. The paper claims the dataset discovery agent shows interesting demands related to knowledge-intensive tasks or reasoning-intensive tasks. However, both tasks have mature strategies regarding data exploration/building. For instance, RAG and search agent related techniques are widely used in knowledge-intensive tasks; and reasoning tasks involves data synthesis (e.g., WizardMath) and RL-related long CoT & test-time scaling strategies. It is unclear how the proposed data discovery agent differs from these widely used existing methods on resolving related tasks.\n\n3. The benchmark results in Table 2 seems incomplete. It misses multiple open-source models like QWen, DeepSeek, etc, and other popular models like Gemini and Claude series. I would also be interested to see performances of GPT-4.1 and GPT-5 models."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qfXiLYoKCT", "forum": "obapzpANpf", "replyto": "obapzpANpf", "signatures": ["ICLR.cc/2026/Conference/Submission5697/Reviewer_XC7Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5697/Reviewer_XC7Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989474706, "cdate": 1761989474706, "tmdate": 1762918201921, "mdate": 1762918201921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce a benchmark designed to evaluate AI agents’ ability to autonomously discover or synthesize datasets given natural-language task requirements. \n\nThe benchmark consists of 208 real-world NLP dataset, with reference datasets and metadata for objective comparison. Models are assessed via metadata alignment, few-shot performance, and fine-tuned downstream results. Experiments show a clear split: search-based agents excel at knowledge-oriented tasks, while synthesis-based models achieve superior performance on reasoning tasks. The work provides the first systematic evaluation pipeline for demand-driven data discovery."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. First comprehensive framework targeting automated data discovery—a growing but under-studied problem.\n\n2. Uses gated datasets + reference metadata, preventing leakage and reflecting real research workflows.\n\n3. Combines metadata scoring, few-shot results, and fine-tuning—much richer than single-metric evaluation."}, "weaknesses": {"value": "1. Used OpenAI o3 to generate both reference/discovered metadata and judges metadata similarity. This creates a closed loop that may favor o3’s rather than true task fit. \n\n\n2. When starting from gated datasets, it prevents agents from downloading the ground-truth data. This structurally disadvantages search agents (vs. synthesis) and conflates ``access policy'' with ``discovery ability.'' \n\n3. Data scope is narrow: NLP-only and text-only."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mXKagyoisF", "forum": "obapzpANpf", "replyto": "obapzpANpf", "signatures": ["ICLR.cc/2026/Conference/Submission5697/Reviewer_83LS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5697/Reviewer_83LS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762203340436, "cdate": 1762203340436, "tmdate": 1762918201628, "mdate": 1762918201628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}