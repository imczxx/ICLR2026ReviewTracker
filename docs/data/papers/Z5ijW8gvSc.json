{"id": "Z5ijW8gvSc", "number": 6958, "cdate": 1758003410449, "mdate": 1763715258951, "content": {"title": "Recycling Pretrained Checkpoints: Orthogonal Growth of Mixture-of-Experts for Efficient Large Language Model Pre-Training", "abstract": "The rapidly increasing computational cost of pretraining Large Language Models necessitates more efficient approaches. Numerous computational costs have been invested in existing well-trained checkpoints, but many of them remain underutilized due to engineering constraints or limited model capacity. To efficiently reuse this \"sunk\" cost, we propose to recycle pretrained checkpoints by expanding their parameter counts and continuing training. We propose orthogonal growth method well-suited for converged Mixture-of-Experts model: interpositional layer copying for depth growth and expert duplication with injected noise for width growth. To determine the optimal timing for such growth across checkpoints sequences, we perform comprehensive scaling experiments revealing that the final accuracy has a strong positive correlation with the amount of sunk cost, indicating that greater prior investment leads to better performance. We scale our approach to models with 70B parameters and over 1T training tokens, achieving 10.66\\% accuracy gain over training from scratch under the same additional compute budget. Our checkpoint recycling approach establishes a foundation for economically efficient large language model pretraining.", "tldr": "By reusing the sunk cost from existing pretrained models, we design a orthogonal growth framework that efficiently builds large-scale MoEs with reduced computational cost..", "keywords": ["Model Growth", "LLM", "Pre-training", "Mixture-of-Experts", "Sunk Cost"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f05afea57d3e4bbc9c755c58091b57f2ece54f7.pdf", "supplementary_material": "/attachment/43a33baa84c9061ea2d5293ee8821b88eb1e7af4.zip"}, "replies": [{"content": {"summary": {"value": "This paper explores depth and width expansions for upcycling of pretrained MoE LLMs. For depth, each existing transformer layer is duplicated in place rather than appending new blocks. The paper empirically shows this depth expansion better retains layer structure and yields stronger downstream accuracy than stacking. For width expansion, the paper duplicates existing experts and adds Gaussian noise to new experts and router logits to promote divergence. Depth upcycling is compared against width upcycling under matched training compute budgets, demonstrating that depth expansion outperforms expert addition. The paper also investigates the best point during pretraining to upcycle, and compares upcycling to training from scratch and continued pretraining."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1- Some of the results, including the empirical comparison between depth and width upcycling, in the context of MoEs are insightful.  Assuming active compute is eventually matched appropriately, the insights from this comparison are informative in large-scale model design and pretraining scaling strategies.\n\n2- The authors make a clear effort to match total training FLOPs across different strategies, which is a strong experimental design choice. \n\n3- The paper experiments with depth and width upcycling at a practically relevant scale. The fact that the upcycled models perform competitively under controlled compute in this scale makes the findings more compelling.\n\n4- The related work section covers most of the key prior work. While a few recent methods could be cited more explicitly (e.g., [1] and [2] in weaknesses), the overall coverage is strong.\n\n5- Even though an LLM is used for polishing writing, the paper is written in a concise and easy to understand style."}, "weaknesses": {"value": "1- Important details are missing or unclear. The number of active parameters for both the base and upcycled MoE models is never stated, only the number of active experts (in the appendix). Figure 1’s pretrain(new) configuration lacks explicit parameter counts, and there is no 70B pretrain result in Figure 11 or the appendix (the tables). These omissions make it difficult to verify the fairness and reproducibility of the reported comparisons. The authors should make sure they include every detail for the readers to understand the experimental settings.\n\n2- The depth and width upcycling comparisons are potentially unfair. The width-scaled model likely activates fewer parameters per token than the depth-upcycled one, which undermines claims of equal compute. The authors should include a matched-active-parameter width baseline, ensuring equal per-token compute. Additionally, a fixed top-k variant is missing, which is important for isolating architectural differences from increased compute. Matching only training FLOPs is not sufficient, since inference efficiency is the key motivation for MoE models. \n\n3- Several plots add little insight or are poorly explained.  Figure 5 evaluates models immediately after upcycling (before continued training) providing no practical signal. Why would you upcycle a model and immediately deploy it? The x-axis labeled training FLOPs is not clear.  Are the points for different model sizes? or different points during training? In general the figure doesn't contribute to the goal of this paper. Also the setting of figure 5 is not clearly explained in the paper. Figure 7 similarly offers an unsurprising conclusion (more training results in better performance?) without new insights. Figure 8 already captures the relevant comparison more cleanly. These figures could be omitted or clarified to improve focus.\n\n4- (Minor) The methodological contributions are incremental. Adding noise to cloned experts has been explored in [1] (as briefly covered in the related work), and duplicating layers in-place was proposed in [2]. The paper’s main difference is applying these known ideas to MoE rather than dense models. Explicitly acknowledging these works and clarifying the scope of genuine novelty would strengthen the positioning. \n\n[1] Drop-Upcycling: Training Sparse Mixture of Experts with Partial Re-initialization, Nakamura et al., 2025\n\n[2] Transformer Layer Injection: A Novel Approach for Efficient Upscaling of Large Language Models, James Vo, 2024"}, "questions": {"value": "In addition to the question in the weakness section:\n\n1- Why was a learning rate schedule used instead of a fixed learning rate? Resuming training from different checkpoints along the LR annealing curve (especially near or past the LR floor) could significantly alter training dynamics during upcycling. This seems to be reflected in the paper’s own experiments, where upcycling from late checkpoints under a shared schedule results in degraded performance. If the goal is to isolate the effect of the growth method itself, shouldn’t other variables (like the learning rate) be held fixed or explicitly tuned per checkpoint? Otherwise, it’s unclear whether the observed effects are due to the upcycling start point or artifacts of the LR schedule mismatch.\n\n2- Did the authors try a combination of width and depth scaling at the same time? I understand this expands the design space but a simple experiment that doubles the model size by dividing the added parameters between new experts and new layers could be interesting.\n\n3- Did the authors try training both the upcycled model and the larger model pretrained from scratch to convergence and compare the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5wYKVz2wXN", "forum": "Z5ijW8gvSc", "replyto": "Z5ijW8gvSc", "signatures": ["ICLR.cc/2026/Conference/Submission6958/Reviewer_eGL6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6958/Reviewer_eGL6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760577915980, "cdate": 1760577915980, "tmdate": 1762919184568, "mdate": 1762919184568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical issue of high computational costs associated with pretraining large language models (LLMs). The authors propose a framework to \"recycle\" existing, well-trained checkpoints by expanding them into larger, more capable models, thereby leveraging the \"sunk cost\" of prior computation. The work specifically focuses on Mixture-of-Experts (MoE) architectures and introduces two orthogonal growth strategies.\nThe authors conduct experiments showing the superiority of their proposed methods. They investigate the optimal timing for growth, concluding that growing from more converged checkpoints (higher sunk cost) yields better final performance. The framework's scalability is demonstrated by successfully growing a 17B MoE model to a 70B model, which significantly outperforms a from-scratch baseline trained with the same additional compute budget."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "*   The paper tackles a highly relevant and practical problem. Finding more compute-efficient ways to scale LLMs is crucial for the sustainability and accessibility of AI research. \n*   The analysis of growth timing (Section 4) is a key strength. The experiments clearly demonstrate a positive correlation between the amount of prior training and the final performance of the grown model, providing a valuable and actionable insight for practitioners.\n*   The paper makes a salient point about the difference in growth strategies for models in early vs. late training stages. The finding that \"interposition\" is superior for converged models due to preserving structural properties is an interesting and well-argued contribution.\n*   The scalability experiment, growing a model from 17B to 70B parameters on a 1-trillion-token scale, is impressive and provides strong evidence for the practical viability of the proposed framework."}, "weaknesses": {"value": "2.  **Inadequate Ablation Studies for Key Design Choices:**\n    The framework's effectiveness relies on several critical design decisions that are not fully justified through ablation studies.\n    *   **Width Growth Initialization:** The paper demonstrates that \"copying with noise\" is superior to noiseless copying. However, a much stronger and more intuitive baseline is to **randomly initialize the new experts** while retaining the original ones. Comparing against this baseline is necessary to determine whether the advantage comes from knowledge inheritance (via noisy copies) or simply from effective symmetry-breaking.\n    *   **Orthogonality and Order of Growth:** The paper claims depth and width growth are \"orthogonal\" but only demonstrates a fixed \"depth-then-width\" sequence in the large-scale experiments. This does not sufficiently prove orthogonality, as the order of operations could matter in practice. An experiment testing the reverse order (\"width-then-depth\") is needed to validate this claim and explore potential performance differences."}, "questions": {"value": "1.  You convincingly argue that the \"interposition\" method is superior for **converged** models, contrasting this with prior work like [4] that applies \"stacking\" in earlier training phases. Could you elaborate on where this tipping point might be? Is there a stage in early-to-mid training where \"stacking\" performs comparably or even better?\n\n2.  For width growth, you used Gaussian noise to break symmetry. Did you experiment with other noise types or alternative symmetry-breaking techniques? Specifically, how does the proposed \"copy-and-noise\" approach compare empirically to the baseline of randomly initializing the new experts?\n\n3.  In your 70B experiment, you applied a \"depth-then-width\" strategy. What is the intuition behind this order? Given that depth growth enhances functional complexity while width growth expands model capacity, could the sequence of these operations fundamentally impact the final model's capabilities or its training dynamics?\n\n\n[1] Liu D, Wang Z, Wang B, et al. Checkpoint merging via bayesian optimization in llm pretraining[J]. arXiv preprint arXiv:2403.19390, 2024.\n\n[2] Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for efficient transformer training. arXiv preprint arXiv:2303.00980, 2023a.\n\n[3] Peihao Wang, Rameswar Panda, and Zhangyang Wang. Data efficient neural scaling law via model reusing. In International Conference on Machine Learning, pp. 36193–36204. PMLR, 2023b.\n\n[4] Du W, Luo T, Qiu Z, et al. Stacking your transformers: A closer look at model growth for efficient llm pre-training[J]. Advances in Neural Information Processing Systems, 2024, 37: 10491-10540."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6gtvuSHfLG", "forum": "Z5ijW8gvSc", "replyto": "Z5ijW8gvSc", "signatures": ["ICLR.cc/2026/Conference/Submission6958/Reviewer_wSPh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6958/Reviewer_wSPh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760712492658, "cdate": 1760712492658, "tmdate": 1762919183992, "mdate": 1762919183992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper utilizes two main techniques to upcycle a pretrained (MoE) model to increase the capacity of the model:\n- interpositional layer copying for depth growth\n- expert duplication for width growth\nThey claim that utilizing these methods leverage the sunk cost of pretrained models to accelerate learning.\nThese methods first modify the model architecture, before continual pretraining on the model.\n\nThe depth-growth method is inspired by the fact that the norm of each layer is more aligned to each other when using the interpositional stacking method.\nThe width growth method is by duplicating expert and adding some noises to it.\n\nThe largest concern is that the proposed separate methods have been shown to be ineffective in previous work, and it is not clear why they work in the paper's proposal."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The experiments conducted are at fairly large scale: 70B model with over 1T training tokens.\n- The paper is quite easy to follow with clear organization, with ablation studies to support their arguments (with caveats; see below)"}, "weaknesses": {"value": "- The utilized methods for depth growth and width growth are well-known and have been shown to be less effective. It is unclear what differences are made in the paper that make them work:\n  - depth growth: [2405.15319] show that naive stacking works best; while the paper claims that it is suboptimal for converged models, the argument is not convincing, as no detailed comparison is made showing under what condition (at what sunk cost, budget, model size etc.) stacking works best vs interpositional stacking. Also, models in  [2405.15319] are trained with over the chinchilla-suggested token number and are quite overtrained.\n  - width growth: In [2212.05055,2406.06563,2409.02060,2502.03009], it was shown that adding gaussian noises does not lead to performance improvement. [2502.03009] further shows that it is more difficult to upcycle a model from an overtrained base model. These results contradict the claim of the paper.\n- When studying the impact of sunk cost, the paper should also use checkpoints with LR fully decayed, instead of intermediate checkpoints from a single run. This is more useful as in practice, one upcycle a model which has finish training (LR fully decayed).\n- Design choices are quite ad-hoc, e.g., increasing the depth/width by a factor of 2 (not 3, 4, etc)."}, "questions": {"value": "Please address the weaknesses mentioned above and clarify if there is any misunderstanding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RNpbll6r0X", "forum": "Z5ijW8gvSc", "replyto": "Z5ijW8gvSc", "signatures": ["ICLR.cc/2026/Conference/Submission6958/Reviewer_6wRb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6958/Reviewer_6wRb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6958/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633563712, "cdate": 1761633563712, "tmdate": 1762919183410, "mdate": 1762919183410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}