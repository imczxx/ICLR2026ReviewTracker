{"id": "CQ2n2H5x6c", "number": 10606, "cdate": 1758177126154, "mdate": 1762995923519, "content": {"title": "AdaST: Adaptive Semantic Transformation of Visual Representation for Training-free Zero-shot Composed Image Retrieval", "abstract": "Composed Image Retrieval (CIR) aims to retrieve a target image given a reference image and a textual modification instruction. The textual instruction specifies the desired modification, while the remaining visual attributes are preserved for consistency. Recent research has focused on training-free methods that leverage image generation models to synthesize proxy images by combining a reference image with a textual modification. However, this approach is computationally expensive and time-consuming, while relying solely on text queries often results in the loss of crucial visual details. To address these issues, we propose Adaptive Semantic Transformation (AdaST), a new training-free method that transforms reference image features into proxy features guided by text. It preserves visual information more efficiently without relying on image generation. To achieve finer-grained transformation, we introduce an adaptive weighting mechanism that balances proxy and text features, enabling the model to exploit proxy information only when it is reliable. Our method is lightweight and can be seamlessly applied to existing training-free baselines in a plug-and-play manner. Extensive experiments demonstrate that it achieves state-of-the-art performance on three CIR benchmarks while avoiding the heavy cost of image generation and incurring only marginal inference overhead compared to text-based baselines.", "tldr": "We propose an efficient and effective feature-level transformation method for Zero-shot Composed Image Retrieval.", "keywords": ["Zero-shot Composed Image Retrieval", "Training-free", "Multi-modal", "VLM", "LLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/4cf0db7179c3beb9828122c9fc00b479f65013b2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Adaptive Semantic Transformation (AdaST), a training-free method for composed image retrieval. The core idea is to transform the reference image into a proxy feature embedding guided by the difference between the reference caption and the target caption. The similarity score based on the proxy feature is incorporated into the original scores via a gating scheme. Experiments on several datasets demonstrate the effectiveness and efficiency of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written, with clear background, method descriptions, and experimental analysis.\n\n2. The method is highly adaptable and can be integrated into existing approaches in a plug-and-play manner.\n\n3. The method operates directly in feature space, runs fast, and has strong practical value."}, "weaknesses": {"value": "1. The paper demonstrates the effectiveness of the proxy feature mainly through experimental and visualization results, but lacks deeper or alternative explanations, such as feature visualizations and statistics of score distributions before and after applying proxy similarity.\n\n2. The ablation study is relatively simple and lacks ablations on other hyperparameters.\n\n3. There is no analysis of failure cases: in what situations is the proxy most effective, and in what situations might it have negative effects?"}, "questions": {"value": "The designs of Equations (8) and (9) are quite intuitive. What are their design principles and specific meanings? For example, in Equation (9), why is the adaptive weight multiplied by $S_{T_t}$? Are there other options?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sBx4Cn5OLr", "forum": "CQ2n2H5x6c", "replyto": "CQ2n2H5x6c", "signatures": ["ICLR.cc/2026/Conference/Submission10606/Reviewer_garP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10606/Reviewer_garP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809193151, "cdate": 1761809193151, "tmdate": 1762921871101, "mdate": 1762921871101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "aflXPnmuah", "forum": "CQ2n2H5x6c", "replyto": "CQ2n2H5x6c", "signatures": ["ICLR.cc/2026/Conference/Submission10606/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10606/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762995922397, "cdate": 1762995922397, "tmdate": 1762995922397, "mdate": 1762995922397, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a training-free zero-shot combined image retrieval method called Adaptive Semantic Transformation(AdaST), aimed at addressing the trade-off between visual detail preservation and computational efficiency in existing methods. AdaST does not rely on image generation, but instead performs semantic transformations of reference image features through text guidance in the feature space of a pre-trained vision-language model, generating a \"proxy feature\" to approximate the target image. The method also introduces an adaptive similarity fusion mechanism that dynamically balances the contributions of proxy features and text features, achieving state-of-the-art performance across multiple CIR benchmarks while significantly improving inference speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is training-free, relying entirely on a pre-trained model without the need for additional labeled data or fine-tuning.\n2. Extensive experiments demonstrate efficient inference by avoiding the high-cost image generation process, significantly outperforming generative methods in terms of inference speed.\n3. The method is plug-and-play, seamlessly integrating into existing training-free CIR methods, enhancing their performance."}, "weaknesses": {"value": "1. The gating threshold m, weight λ, and β all require manual grid-search, yet the main text describes them as \"adaptive,\" which is not fully accurate.\n2. The influence of different LLMs on the results has not been adequately explored.\n3. There is a spelling error in the term \"yeilds\" in the experimental section, which should be \"yields\"; also, \"domain\" in the INTRODUCTION may need to be changed to the plural form \"domains\". It is recommended to carefully review the entire manuscript for such issues.\n4. The plus signs in Table 2 have inconsistent styles; it is advised to check and adjust the formatting throughout the document to ensure consistency and conformity.\n5. The manuscript lacks examples of LLM prompts; it is recommended to include relevant examples to enhance reproducibility and transparency."}, "questions": {"value": "1. I hope the authors can address each of the weaknesses and explain how this work is not merely repetitive.\n2. What is the difference from SEIZE? It is necessary to clarify the similarities and differences between AdaST and SEIZE in terms of feature transformation, similarity fusion, and other aspects.\n3. How does AdaST compare qualitatively with SEIZE? Could more visual examples be provided to demonstrate AdaST's advantages in detail preservation and semantic alignment?\n4. Will the code be open-sourced? The release of the code is crucial for reproducibility and credibility. Is there a plan for publication?\n5. Does \"plug-and-play\" imply post-processing? Is it subsequent processing based on the output of the original method, or does it fully replace the original process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NTUREaos2V", "forum": "CQ2n2H5x6c", "replyto": "CQ2n2H5x6c", "signatures": ["ICLR.cc/2026/Conference/Submission10606/Reviewer_nn1V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10606/Reviewer_nn1V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908316224, "cdate": 1761908316224, "tmdate": 1762921870621, "mdate": 1762921870621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AdaST, a method that predicts target-image features for two-stage training-free CIR. The authors aim to leverage a text-driven manifold augmentation strategy to avoid the high cost of pixel-level generation in ZS-CIR. Experiments on CIRCO, CIRR, and Fashion-IQ report SOTA performance while running ~186× faster than pixel-level, generation-based IP-CIR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe idea of this paper is intuitive.\n2.\tIt is interesting to predict target-image information at the feature level rather than the pixel level.\n3.\tExperiments on CIRCO, CIRR, and Fashion-IQ achieve SOTA performance."}, "weaknesses": {"value": "1.\tLack of novelty. The motivation of this paper, which predicts target information at the feature level instead of the pixel level, has been explored in PrediCIR [1], which is more efficient than the proposed method; the authors overlook a proper comparison and acknowledgement. This raises concerns that the paper does not offer sufficient new insight to the community.\n2.\tLimited technology contribution. While feature-level target prediction has been studied [1], this paper appears to adopt TextManiA’s [2] text-driven manifold augmentation to realize it. Moreover, the key module, the Adaptive Similarity module, closely mirrors LDRE’s [3] Adaptive Semantic Ensemble (Sec. 4.2.2). Thus, the technical contribution seems insufficient.\n3.\tConcerns of the generalizability. he method seems restricted to a two-stage, training-free CIR pipeline. Recent work suggests one-stage approaches are the trend in training-free CIR, offering higher efficiency and avoiding information loss [3]. This further weakens the contribution.\n4.\tConcerns about the hallucination issues. The output of AdaST is based on VLM (i.e.,BLIP-2), a MLLM-generated results without CoT process, which make me concern the hallucination problem. Such issues could significantly impact the retrieval results. A more detailed analysis of hallucination risks is needed.\n5.\tNeed more analysis experiments. For example, have a visualization experiment to show the benefit of the text-driven manifold augmentation (e.g., add a decoder to generate the corresponding target image).\n6.\tMissing implementary details. The pipeline has many components, and the code is not provided. Including pseudocode is recommended.\n7.\tInsufficient ablation studies. For example, what is the performance with open-source MLLM (i.e., Qwen-VL-72B)? What is the influence of other gating methods? What is the influence of other Optimal Scaling methods?\n8.\tIncomplete benchmarking. Comparisons on CLIP-B/L for Fashion-IQ are missing, although these are common in recent ZS-CIR evaluations.\n\nOverall, the novelty and technical contribution appear limited, and analyses on generalizability and hallucination are missing. Therefore, I gave the Reject recommendation. I believe the paper should have a revision to address these concerns.\n\nReferences\n\n[1] Tang Y, Yu J, Gai K, et al. Missing target-relevant information prediction with world model for accurate zero-shot composed image retrieval[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 24785-24795.\n\n[2] Ye-Bin M, Kim J, Kim H, et al. Textmania: Enriching visual feature by text-driven manifold augmentation[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 2526-2537. \n\n[3] Sun Z, Jing D, Lu Z. CoTMR: Chain-of-Thought Multi-Scale Reasoning for Training-Free Zero-Shot Composed Image Retrieval[J]. arXiv preprint arXiv:2502.20826, 2025."}, "questions": {"value": "1.\tCan the proposed method be used in one-stage, training-free CIR?\n2.\tWhy not compare against feature-level target-prediction methods for CIR?\n3.\tWhat is the performance with open-source MLLMs (e.g., Qwen-VL-72B)?\n4.\tWhat is the influence of alternative gating methods?\n5.\tWhat is the influence of different optimal-scaling strategies?\n6.\tWhat is the performance on CLIP-B/L for Fashion-IQ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6uAnwExDIO", "forum": "CQ2n2H5x6c", "replyto": "CQ2n2H5x6c", "signatures": ["ICLR.cc/2026/Conference/Submission10606/Reviewer_BEwH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10606/Reviewer_BEwH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976653610, "cdate": 1761976653610, "tmdate": 1762921870208, "mdate": 1762921870208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the training-free Adaptive Semantic Transformation (AdsST) approach for the task of composed image retrival, which transforms reference image features into proxy features with the guidance of text, preserving visual information through feature-level transformation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized and easy to follow.\n2. Extensive abaltion expereiments are conducted to demonstrate the effectiveness of the proposed AdaST method."}, "weaknesses": {"value": "1. The paper claims that existing methods are computationally expensive, but no efficiency evaluations are presented to show AdaST's improvements on computational cost.\n2. The architecture of the proposed AdaST method is not new, more theoretical insight should be provided to strength the novelty of AdaST."}, "questions": {"value": "1. Efficiency evaluations on computational cost should be provided to validate the effectiveness of AdaST.\n2. More theoretical insight of the proposed method should be presented to strength the novelty of AdaST."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NAkNutUo1k", "forum": "CQ2n2H5x6c", "replyto": "CQ2n2H5x6c", "signatures": ["ICLR.cc/2026/Conference/Submission10606/Reviewer_Hjcc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10606/Reviewer_Hjcc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10606/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762256683987, "cdate": 1762256683987, "tmdate": 1762921869859, "mdate": 1762921869859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}