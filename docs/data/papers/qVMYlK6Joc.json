{"id": "qVMYlK6Joc", "number": 16634, "cdate": 1758267010575, "mdate": 1763721796459, "content": {"title": "Greedy Information Projection for LLM Data Selection", "abstract": "We present Greedy Information Projection (GIP), a principled framework for choosing training examples for large language model fine-tuning. GIP casts selection as maximizing mutual information between a compact subset of examples and task-specific query signals, which may originate from LLM quality judgments, metadata, or other sources. Under a jointly Gaussian model of data and query embeddings, the objective has a closed form and naturally balances quality and diversity. We show that optimizing this score is equivalent to maximizing the projection of the query embedding matrix onto the span of the selected data, yielding a geometric explanation for the co-emergence of quality and diversity. Building on this view, we develop a fast greedy matching-pursuit procedure with efficient projection-based updates. On instruction-following and mathematical reasoning datasets, GIP selects compact subsets that match full-data fine-tuning while using only a small fraction of examples and compute, unifying quality-aware and diversity-aware selection for efficient fine-tuning.", "tldr": "Greedy Information Projection (GIP) uses a mutual-information objective with efficient greedy algorithms to pick small, high-value, diverse training subsets from general query signals, matching full-data performance at a fraction of the cost.", "keywords": ["data selection", "large language model", "fine-tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e4120f1c6256a1db7553009b9e3a37190229452a.pdf", "supplementary_material": "/attachment/30cf55811bc1cd54efc73028fac499f34d0fa66d.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a data selection method for efficient fine-tuning of LLMs on instruction-finetuning and mathematical reasoning tasks. For each example x in the dataset D, the method takes (1) a feature embedding of x; and (2) a set of “scores” for that example. The scores in (2) may, for example, be a set of quality scores for that particular datapoint x generated by a variety of larger LLMs, human annotations on helpfulness or accuracy, etc. \n\nThe goal of the proposed method is to select a subset of D to fine-tune a language model on, hopefully trading off the diversity in feature space with “quality” (captured by the scores). To do this, the paper proposes utilizing a query embedding matrix Q, chosen to connect the feature space and score space (1 and 2 above). Then, a theoretical result (Theorems 1+2) states that maximizing the mutual information between the a subset of the feature space and the query matrix will have performance guarantees in terms of the quality (w.r.t. the scores). Diversity comes implicitly through maximizing entropy.\n\nOptimizing mutual information is challenging in practice. Instead, the paper proposes a greedy approximation algorithm based on matching pursuit. As I understand it, by working the dual space, the proposed algorithm can avoid constructing the matrix Q explicitly. Intuitively, the algorithm identifies residuals which are not yet captured by the spanning set S, and then continuously updates the set greedily to reduce the magnitude of the square sum residuals.\n\nExperimental results show that the method can perform similarly to other proposed data selection methods for instruction-finetuning and mathematical reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed approach is quite intuitive and has some nice theoretical properties. I really liked the framing and relaxation of the hard to optimize objective.\n\nThe paper overall is well written and understandable."}, "weaknesses": {"value": "Overall, I am not convinced by the experimental evaluations presented in the paper. I will mainly focus on data in the appendix, since the data in the main paper is identical but without standard error bars. At a high level, I believe that random subset selection performs as well or better than the proposed method. Here are some particular things I noticed for each model:\n\n**Qwen3-8B**\n1. GSM8k results are not convincing (overlapping standard errors with baselines and random subset)\n2. MT-Bench results for 2% data overlap with random 2% subset SEs. 1% random is not reported.\n3. BBH results for MP+SC and MP+MA with 2% of the data looks to be clearly better than random and better than all other tested baselines. Results for 1% data with other baselines is not reported. \n\n**Mistral-7B**\n1. GSM8k results at least do not overlap standard errors with the other baseline methods, but do overlap standard errors with the random method.\n2. MT-Bench results for 2% of data have overlapping SEs with random subset selection. Random with 1% of the data is not reported.\n3. BBH results overlap with random selection for 2% data. 1% random is not reported.\n\n**Qwen-4B**\n1. MT-Bench results have random performing nearly identically or better than proposed methods.\n2. BBH results have the best proposed method (MP+SC) performing in a non-overlapping way with random, but errors overlap for other baselines.\n\nFine-tuning with a random 2% matches or outperforms the finetuning with the full dataset quite often (Qwen3-8B MT-Bench, Qwen-4B MT-Bench, Mistral-7B MT-Bench, Mistral-7B BBH). Is this not a red-flag evaluation-wise? Should we be expecting a random 2% of the data to be performing as well as full fine-tuning?\n\nIt may be helpful to run statistical significance tests in order to see the p-value of the results. This may be necessary to convince the reader that they are statistically significant (see discussion below).\n\nFurthermore, I am not sure what the standard errors are being reported over. In Appendix J, the paper says that it computes standard errors as SE = std / \\sqrt{n} for all benchmarks. However, in Appendix C reproducibility (line 565), the paper says that the standard error is over four different prompts for MT-Bench, and for GSM8k / BBH the paper reports 95% Wilson binomial CIs. Which error metric is reported where? Are the errors reported over different prompts or subsamples of the full dataset / re-doing finetuning? Re-running fine-tuning with a random 2% of the dataset should not be too expensive to do with LoRA (since the number of examples is on the order of ~1K).\n\nI also believe that the paper could still stand to benefit from studying the active learning literature more carefully (e.g., Bodo et al. 2011). Although the paper claims that “most active learning methods are designed for traditional supervised learning and may not directly transfer to the large-scale generative modeling setting”, there are still methods which can be applied. For example, k-means clustering in the embedding space (since we are already computing embeddings for the proposed method), and then selecting representative points as the centroids.\n\nFurther Suggestions\n1. Change the LHS figure 1 to be understandable without the caption. The current “1x = twice as good” regime is quite confusing at a glance. Instead, you can have a clearer figure if you simply plot performance on the y axis and % data on the x axis. Then, you can show multiple baselines and your proposed method. At a glance, this would show a clearer improvement.\n2. Appendix at end of paper (after appendix) instead of end of main body. I think it is usually customary to include the appendix at the end of the main paper, instead of at the end of the appendix. Furthermore, you have some tables that bleed into the appendix (\\flush or \\newpage should be able to fix this).\n3. Perhaps some explicit quantitative comparisons between the proposed method and prior work could show that the proposed algorithm has a better “quality” and “coverage” trade-off within the data.\n\nBodo et al., 2011. Active Learning with Clustering."}, "questions": {"value": "Most questions are listed implicitly in the weaknesses section.\n\n1. Is it possible to introduce an explicit hyperparameter for controlling the trade-off between quality (w.r.t scores) and diversity (w.r.t. embeddings)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tOO1ILWsNq", "forum": "qVMYlK6Joc", "replyto": "qVMYlK6Joc", "signatures": ["ICLR.cc/2026/Conference/Submission16634/Reviewer_Jjz6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16634/Reviewer_Jjz6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772926975, "cdate": 1761772926975, "tmdate": 1762926700451, "mdate": 1762926700451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GIP, a data selection framework that trains task-specific models. They maximize the mutual information between a dataset and corresponding quality features (that can be from LLM evaluators or self-consistency estimates). Theoretically, they show that this problem can be reduced to finding the maximum projection of the quality features onto the selected data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper solves a timely problem\n- Their results show their method can outperform using the full dataset in a handful of cases\n- The algorithm is creative"}, "weaknesses": {"value": "- If quality scores are indeed from an LLM, inference across a dataset could be expensive, without significant performance boosts.\n- It's not immediately intuitive to me, but it is unclear how much time this method would take in comparison to the baselines. This is a concern in particular because MP+ does not significantly outperform the baselines in some tables. At least if the cost is reduced compared to these works, MP would be justified.\n- The experimental set up in Table 3 does not include other baselines, so the claims are a bit incomplete.\n- The chosen models are roughly the same size (7B and 8B)"}, "questions": {"value": "See weaknesses, please."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "5KgTn3EcNS", "forum": "qVMYlK6Joc", "replyto": "qVMYlK6Joc", "signatures": ["ICLR.cc/2026/Conference/Submission16634/Reviewer_Q6Nm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16634/Reviewer_Q6Nm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803421619, "cdate": 1761803421619, "tmdate": 1762926700093, "mdate": 1762926700093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "For the task of LLM fine-tuning data selection, the proposed GIP framework operates by extracting sample feature vectors and combining them with per-sample quality scores (such as those derived from GPT-4o multi-attribute assessments or dataset self-compression scores) to derive task-specific query signals Q. It then utilizes a greedy matching pursuit (MP) sampling method to maximize the projection of Q onto the span of the selected data to ensure coverage of the required query directions, thereby balancing both quality and diversity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The framework models data selection as maximizing mutual information between the selected subset and query signals within a single, unified information-theoretic objective, which holds the advantage of balancing quality and diversity instead of taking the sequential approach.\n2. A fast greedy matching pursuit approximation algorithm is proposed to solve the approximate dual problem. This MP approach uses efficient, projection-based updates."}, "weaknesses": {"value": "1. The total runtime complexity includes a substantial initial $O(m^2d)$ cost for precomputing the data inner product matrix. This renders the method computationally challenging to scale to truly massive datasets in practice, suggesting the claim of \"nearly linear\" scaling is an overstatement as the linearity only holds after the quadratic precomputation.\n2. To achieve efficiency, the method relies on linearization, optimizing an upper bound/trace approximation of the determinant objective, and using a greedy selection approach. This introduces inherent limitations, as the selected subset is only an approximation rather than the globally MI-optimal solution.\n3. The empirical validation is only validated on three benchmarks. While several strong baselines are included, the results lack comparisons against other similar methods, potentially limiting the generalizability and persuasiveness of the empirical findings. Studies that are highly related, for example:\n- What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning\n- LESS: Selecting Influential Data for Targeted Instruction Tuning\n- QuRating: Selecting High-Quality Data for Training Language Models\n- DataMan: Data Manager for Pre-training Large Language Models\n- Rule-based data selection for large language models"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v8oaMEvwUp", "forum": "qVMYlK6Joc", "replyto": "qVMYlK6Joc", "signatures": ["ICLR.cc/2026/Conference/Submission16634/Reviewer_1qNM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16634/Reviewer_1qNM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939221571, "cdate": 1761939221571, "tmdate": 1762926699168, "mdate": 1762926699168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Greedy Information Projection, an information-theoretic framework for selecting training examples for large language model fine-tuning. The core idea is to formalize data selection as maximizing mutual information between selected data embeddings and task-specific query embeddings, which can encode LLM evaluation signals or metadata. Under a jointly Gaussian assumption, this MI objective has a closed-form expression and admits a geometric interpretation: it maximizes the projection of the query embedding matrix onto the span of selected data, balancing quality and diversity. The authors derive a greedy matching pursuit algorithm that approximates this objective efficiently, scaling linearly in data size. Empirically, GIP achieves comparable performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper presents a principled and unified information-theoretic framework for LLM data selection, casting the problem as mutual information maximization under a joint Gaussian model. The formulation connects data quality and diversity within a single objective and provides a clear geometric interpretation through projection onto the span of selected data. The proposed greedy matching pursuit algorithm is conceptually simple, computationally efficient, and scalable to realistic dataset sizes. Empirically, the method achieves comparable or slightly better performance than full-data fine-tuning using few data, demonstrating data efficiency."}, "weaknesses": {"value": "1. The theoretical framework heavily relies on the assumption that data and query embeddings are jointly Gaussian, which is unlikely to hold for real LLM embeddings. \n\n2. Evaluations are conducted only on relatively small instruction-tuning datasets and mid-sized models (7B–8B). Could the author provide results on larger datasets and LLM models?\n\n3. The method comparison is narrow, particularly on reasoning tasks. More recent or SOTA baselines are required to assess how much improvement comes from the proposed framework itself.\n\n4. The performance gains on benchmarks such as BBH and GSM8K are limited or even below full-data baselines, and the improvements do not seem statistically significant."}, "questions": {"value": "Please refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ExJzJ6HkId", "forum": "qVMYlK6Joc", "replyto": "qVMYlK6Joc", "signatures": ["ICLR.cc/2026/Conference/Submission16634/Reviewer_QoPy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16634/Reviewer_QoPy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959737761, "cdate": 1761959737761, "tmdate": 1762926698640, "mdate": 1762926698640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Greedy Information Projection (GIP) for selecting small, high-value subsets to fine-tune LLMs. Each candidate example is embedded as a vector \\(F_i\\), and the authors build “query” signals \\(Q\\) from either LLM-judge attributes (e.g., helpfulness/accuracy/reasoning) or simple self-scores. Modeling \\((F,Q)\\) as jointly Gaussian, they show that maximizing \\(I(Z_Q; Z_{F_S})\\) over subsets \\(S\\) is equivalent to minimizing a projection volume:\n\\\\[\n\\min_{S}\\; \\det\\!\\big(Q^\\top (I - P_S)\\,Q\\big), \\qquad\nP_S := F_S(F_S^\\top F_S)^{-1}F_S^\\top,\n\\\\]\nwhich naturally balances quality (alignment with \\(Q\\)) and diversity (avoiding redundant span). They optimize this with a matching-pursuit–style greedy rule and efficient rank-1 updates on a precomputed Gram matrix, making it practical at scale. On instruction-following (e.g., Alpaca \\(\\\\rightarrow\\) MT-Bench/BBH) and math (GSM8K), GIP-chosen subsets at \\(\\\\sim\\)1–20\\\\% of the data match or surpass full-data fine-tuning, reducing compute.\n\n**Main contributions**\n- A unified, information-theoretic objective that makes the quality–diversity trade-off fall out of the math (no hand-tuned weights).  \n- A simple, scalable greedy algorithm (matching-pursuit updates) that accepts heterogeneous scoring signals.  \n- Empirical evidence*that compact, information-rich subsets can replace most of the dataset without hurting performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper tackles a very practical question, how to get full data performance from a much smaller, smarter subset, and does so with a clean, unified formulation. Casting selection as mutual information between data embeddings and “query” signals gives a principled way to make quality and diversity fall out of the same objective, rather than balancing hand tuned terms. The Gaussian projection view makes the geometry intuitive: choose examples whose span covers the directions encoded by the scores, so you align with what matters while avoiding redundancy.\n\nTechnically, the derivation is coherent and traceable: mutual information leads to a projection determinant objective, which admits a tractable relaxation and a matching pursuit style greedy rule with efficient rank 1 updates. The algorithm is at the right level of sophistication, faithful to the math yet lightweight enough to scale, and the setup naturally accommodates multiple signals (LLM judge attributes, self scores, metadata) just by adding columns in Q tied to F. Empirically, the story is consistent across models and tasks: subsets on the order of 1 to 20\\% routinely meet or beat full data fine tuning, with uncertainty reported, and the baseline suite spans quality driven, diversity driven, and influence style methods, which makes the comparisons credible.\n\nThe paper reads clearly. The geometric intuition (“span the query directions”) makes both the objective and the greedy choice easy to internalize, the structure (objective $\\\\rightarrow$ algorithm $\\\\rightarrow$ complexity $\\\\rightarrow$ results) is logical, and figures and tables speak directly to the core claims. As a contribution to practice, the method is drop in friendly: teams already producing any form of per example signal can plug it in without re architecting their pipeline. If these gains hold under broader stress tests (different embeddings, noisier judges, larger pools), this work can shift best practice toward principled small subset fine tuning rather than defaulting to “use everything.”"}, "weaknesses": {"value": "The central theory rests on a jointly Gaussian, approximately linear coupling between data embeddings and query signals. The paper does not probe robustness to misspecification of this assumption. A concrete way to strengthen this is to add stress tests where either the embedding map is deliberately distorted (e.g., random rotations, dimension reduction, or adversarial noise) or the score vectors are corrupted or biased, and then measure both objective values and downstream fine tuning performance. Reporting how selection quality degrades as the coupling diverges from linearity would clarify when the method is safe to deploy.\n\nScalability is promising in asymptotic terms but under documented in practice. Precomputing and storing the Gram matrix scales as $O(m^2 d)$ time and $O(m^2)$ memory, which can dominate at pool sizes common in instruction tuning. The paper would benefit from wall clock and peak RAM curves versus pool size and subset size on a single machine, with details on batching, sharding, or approximate kernels if used. A microbenchmark comparing the relaxed objective to the original determinant on small pools would also help quantify the approximation gap introduced by the trace relaxation.\n\nThe experimental section supports the main story but could be more diagnostic. Key ablations are missing or are only discussed briefly. In particular, add a sweep over the ridge regularizer, an embedding choice study across strong encoders, and a comparison of different score sources, including intentionally noisy or biased judges and mixtures of attributes. For selection stability, report the intersection over union of selected sets across seeds and minor scoring perturbations. For claims that small subsets meet or beat full data, provide confidence intervals with multiple training seeds and, where evaluations rely on LLM judges, include judge reruns to show variance across scorers.\n\nPositioning relative to the closest alternatives could be sharper. Diversity only methods such as determinantal point processes, mutual information or submodular selection aimed at target relevance, and influence or gradient based selection offer nearby formulations. A small scale study that directly optimizes the non relaxed determinant, a diversity only selection, and a low cost influence style baseline would better isolate what the proposed objective contributes beyond existing ideas. Where datasets contain demographic or topical heterogeneity, include simple distributional coverage or fairness checks so that the quality signal does not collapse minority phenomena.\n\nReproducibility artifacts could be more turnkey. To maximize reuse, release prompts, judge templates, scoring scripts, and exact decoding settings, document token budgets and hardware hours per run, and clarify external data or license constraints. Finally, provide a short decision guide for practitioners that explains how to choose the subset size, how to scale or combine multiple score columns, and when to re select as new data arrives."}, "questions": {"value": "Question 1: Robustness to modeling assumptions. The core theory assumes a jointly Gaussian, approximately linear coupling between data embeddings and query signals. How sensitive is selection quality and downstream performance when this coupling is misspecified? Please add stress tests where the embedding map is distorted or the score vectors are corrupted or biased, and report both objective values and fine tuning outcomes.\n\nQuestion 2: Determinant versus trace relaxation. The greedy rule optimizes a relaxed trace objective that upper bounds the determinant form. On small pools where the determinant is tractable, how closely do the selections and objective values match? Please provide a micro study quantifying the approximation gap and any conditions under which the relaxation may mislead the greedy choice.\n\nQuestion 3: Scalability in practice. Precomputing and storing the Gram matrix scales quadratically in the pool size. Please include wall clock time and peak memory as functions of pool size and subset size on a single machine, along with any batching, sharding, or approximate kernel tricks you used. A brief guide to practical limits would be very helpful.\n\nQuestion 4: Score source ablations. The method can ingest heterogeneous signals. Please include a comparison among different score sources, for example LLM judge attributes, self scores, metadata proxies, and mixtures, and a sweep over deliberate noise or bias injected into the scores to assess robustness.\n\nQuestion 5: Embedding choice and regularization. How does performance vary with the embedding model family and dimension, and with the ridge regularizer strength in the projection computation? A sweep over encoder choices and the regularization parameter would clarify sensitivity and help practitioners choose defaults.\n\nQuestion 6: Selection stability. Across different random seeds, minor scoring perturbations, and small changes in embedding initialization, how stable are the selected subsets? Please report intersection over union across runs and discuss whether instability, if present, affects downstream results.\n\nQuestion 7: Statistical support for small subset claims. For cases where a small subset matches or exceeds full data, please provide confidence intervals across multiple training seeds and, when using LLM based evaluation, include judge reruns to quantify scorer variance. This would strengthen the evidence behind the headline claims.\n\nQuestion 8: Comparison to closest alternatives. To sharpen positioning, could you add a controlled study against diversity only selection such as determinantal point processes, a targeted mutual information or submodular selection method, and a lightweight influence style baseline on the same pools? This would isolate what the proposed objective contributes beyond nearby ideas.\n\nQuestion 9: Distributional coverage and fairness. When source data contain demographic or topical heterogeneity, how do selected subsets track the original distribution? Please include simple coverage or fairness checks and discuss whether the quality signal risks collapsing minority phenomena.\n\nQuestion 10: Practical guidance. Could you add a short decision guide on how to choose the subset size, how to scale or combine multiple score columns, and when to re select as new data arrives? Concrete defaults and heuristics would ease adoption.\n\nQuestion 11: Data and evaluation artifacts. To maximize reproducibility, will you release prompts, judge templates, scoring scripts, decoding settings, and exact token and hardware budgets per run, along with any license constraints on data reuse?\n\nQuestion 12: Iterative use and extensions. Have you tried using the method in a loop where a model is briefly fine tuned on the selected subset and scores are then recomputed before a second selection pass? If so, does an iterative strategy offer additional gains, and are there theoretical or practical barriers to such extensions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jZNs9CCTNj", "forum": "qVMYlK6Joc", "replyto": "qVMYlK6Joc", "signatures": ["ICLR.cc/2026/Conference/Submission16634/Reviewer_zxEg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16634/Reviewer_zxEg"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966136968, "cdate": 1761966136968, "tmdate": 1762926698001, "mdate": 1762926698001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}