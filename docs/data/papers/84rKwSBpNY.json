{"id": "84rKwSBpNY", "number": 22883, "cdate": 1758336626169, "mdate": 1759896841443, "content": {"title": "Bias as a Virtue: Rethinking Generalization under Distribution Shifts", "abstract": "Machine learning models often degrade when deployed on data distributions different from their training data. Challenging conventional validation paradigms, we demonstrate that higher in-distribution (ID) bias can lead to better out-of-distribution (OOD) generalization. Our Adaptive Distribution Bridge (ADB) framework implements this insight by introducing controlled statistical diversity during training, enabling models to develop bias profiles that effectively generalize across distributions. Empirically, we observe a robust negative correlation where higher ID bias corresponds to lower OOD error—a finding that contradicts standard practices focused on minimizing validation error. Evaluation on multiple datasets shows our approach significantly improves OOD generalization. ADB achieves robust mean error reductions of up to 26.8% compared to traditional cross-validation, and consistently identifies high-performing training strategies, evidenced by percentile ranks often exceeding 83.4%. Our work provides both a practical method for improving generalization and a theoretical framework for reconsidering the role of bias in robust machine learning.", "tldr": "This paper demonstrates that strategically increasing in-distribution bias can improve out-of-distribution generalization, and introduces the Adaptive Distribution Bridge (ADB) framework to achieve this by managing training data diversity", "keywords": ["Distribution shifts", "Out-of-distribution generalization", "Bias-variance tradeoff", "Model selection", "Statistical diversity", "Optimal transport", "Adaptive Distribution Bridge"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd714dcc95d1fe7cd5c6ca3af6630877c34b418d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method for mitigating distribution shifts under a model in which the mean of the error under distribution shift is shifted. Under this model, the author’s show that there can be an inverse correlation between ID and OOD error. They suggest a training approach that identifies permutations of training examples for which batches have a high divergence from the general population. As I understand it, the goal is to induce bias during training in a way that results in a model with lower ID performance and higher OOD performance, as it does not overfit to training distribution."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- This paper studies an important problem of training approaches with robustness to distribution shifts\n- The paper’s numerical results look strong"}, "weaknesses": {"value": "- The paper is quite unclear in terms of both the setup and method (see the following comments).\n- The “mean shift model” is not well-motivated nor clearly explained. Why does it make sense to assume a certain model for the errors of the source and target distributions? This seems like a model-dependent quantity, i.e., for different models, we have different error distributions for ID and OOD?\n- The method section describes a scheme for binning permutations of the training data and then training on permutations with large average deviation. It is unclear why we should be training on permutations with large average deviation would help. I explained my understanding of the intuition in my summary, but this does not seem well-motivated.\n- The idea that ID and OOD errors might be negatively correlated is not as novel as is presented by the authors, and in fact is the foundation for many prior works on robustness to distribution shift [1,2]. This paper has the same broad intuition as these prior works, but with different setup assumptions (this “mean shift model”) which is not well-motivated and with a method that is less well-motivated.\n- The benchmarks used in this paper are non-standard, making it difficult to assess the strength of their method and compare to prior works.\n\n[1] https://arxiv.org/abs/1911.08731\n\n[2] https://arxiv.org/abs/1907.02893"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EWAkJsUD0W", "forum": "84rKwSBpNY", "replyto": "84rKwSBpNY", "signatures": ["ICLR.cc/2026/Conference/Submission22883/Reviewer_YcZ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22883/Reviewer_YcZ7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761515325163, "cdate": 1761515325163, "tmdate": 1762942424523, "mdate": 1762942424523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission considers the generalization, connecting in-distribution (ID) bias and out-of-distribution (OOD) generalization. The main point is that ID bias can lead to better OOD generalization.  Specifically, a higher ID bias corresponds to lower OOD error. Motivated by this, an approach called the Adaptive Distribution Bridge (ADB) is introduced to enforce controlled statistical diversity during training: it creates bias profiles that generalize across distributions. By doing so, ADB achieves robust mean error reductions compared to traditional cross-validation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ It is interesting to study the connection between ID bias and OOD generalization. The empirical observation that ID bias can lead to better OOD generalization is also interesting. The submission also provides a theoretical study (Section 3) to study this and discuss how this observation appears (Lines 151 - 156)\n\n+ ADB introduces controlled statistical diversity during training by modifying data permutations (training order). It uses optimal transport distances (Sinkhorn distance with debiasing) to quantify how far each training batch diverges from the global data distribution."}, "weaknesses": {"value": "- [**Limited Domain Generalization**] The major concern is that ABD is only evaluated on regression-based tabular and molecular datasets. It is unknown how it performs on classification tasks and vision or language domains. As the claim is broad and general, it would be better to discuss whether the analysis holds for classification and other data types.\n\n- [**Disuccion on Latent Representations**] The ABD heavily relies on latent representations learned via VAEs. Then, how to make sure the latent space is expected? Poorly trained VAEs may produce unreliable diversity signals.\n\n- [**Sampled Distribution Types**] The experiments simulate distribution shifts by using stratified sampling to construct OOD test sets that are deliberately different from the training data. It is not clear how such a distribution shift connects with real-world distribution shifts. For example, well-known datasets designed for studying distribution shifts (e.g., WILDS, DomainNet, PACS, BREEDS). Or other real-world shifts of tabular and molecular would be better if they are discussed in the experiments.\n\n- [**Heavy Computational Cost**] According to Lines 422 - 425, the computational cost is heavy (e.g., 266 GPU hours for one experiment). Is there any way to speed up? What is the cost of the standard random sampling? Please discuss this computational cost."}, "questions": {"value": "- There are several shift types: covariate shift, label shift, and concept shift.  How does ADB perform across different shift types?\n\n- The observation mentions that the negative correlation between ID and OOD errors emerges when the shift $\\Delta$ is large. How does ADB perform when the distribution shift is moderate or small? \n\n- 500 permutations per experiment are used. How does the performance and stability of ADB change if fewer permutations are used (e.g., 50 or 100)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O2c18HCWqH", "forum": "84rKwSBpNY", "replyto": "84rKwSBpNY", "signatures": ["ICLR.cc/2026/Conference/Submission22883/Reviewer_eS4m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22883/Reviewer_eS4m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558701246, "cdate": 1761558701246, "tmdate": 1762942423699, "mdate": 1762942423699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem raised from the difference between the training distribution and the inference distribution. The authors claim that bystrategically increasing ID bias, the model can achieve significantly better OOD generalization. The Adaptive Distribution Bridge (ADB) framework is proposed to control the statistical diversity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors provide theoretical proof to support the claim that higher ID bias leads to reduced OOD error.\n\n2. ADB framework is proposed to control the distribution shifts\n\n3. Extensive experiments are conducted to support the findings"}, "weaknesses": {"value": "1. If I understand correctly, the proof in 3.1 assumes a simplified model, does the conclusion generalize to more complicated settings?\n\n2. What if $\\Delta$ is not known? How can the author determine if $k < \\alpha \\Delta $?\n\n3. Compuational cost might prohibit applications: \"processing all 500 permutation paths required 266.5 total GPU hours with the batchwise approach versus 740 hours with the cumulative approach\""}, "questions": {"value": "1. How is the global distribution obtained? If it's from the whole training set, would that introduce extra bias because it includes the low, medium and hight deviation samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s5BRdmSn2E", "forum": "84rKwSBpNY", "replyto": "84rKwSBpNY", "signatures": ["ICLR.cc/2026/Conference/Submission22883/Reviewer_mf88"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22883/Reviewer_mf88"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863587748, "cdate": 1761863587748, "tmdate": 1762942423410, "mdate": 1762942423410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discovers that a negative correlation can exist between ID and OOD error under significant distribution shift between training and testing data, challenging the conventional assumption that minimizing in-distribution (ID) error is the optimal path to good out-of-distribution (OOD) generalization. They propose an Adaptive Distribution Bridge (ADB) framework to improve OOD generalization."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The observation provided in this paper is interesting. It gives a novel and counterintuitive core Idea that ID error is negatively correlated with OOD error. If validated, it represents a significant shift in paradigm.\n\n* The ADB framework is described with a precise algorithm and two distinct computational approaches (Cumulative and Batchwise)."}, "weaknesses": {"value": "* The theoretical analysis is questionable. The assumption that $b$ is non-negative is not reasonable. With an unknown distribution shift, the bias can not always reduce OOD error. Similarly, simply define $U(b) = (b-\\Delta)^2$ is also questionable, $U(b)$ could also be $(b+\\Delta)^2$. \n\n* Limited empirical evidence to validate the proposed method: With a questionable analysis, the intuition of the proposed framework is similar to previous works that train the model to learn stable features across different training data distributions (for example, IRM). However, there is no comparison between the proposed method and proper baseline methods focusing on improving OOD generalization.\n\nTo my understanding, the negative correlation is a result of models overfitting to the training data, which is not a new phenomenon. The assumption of the paper is infeasible; one can not assume that the bias from the training distribution is towards the test distribution (especially when the test distribution is generally unknown)."}, "questions": {"value": "* In lines 144-149, the bias parameter is restricted to non-negative values. How can one determine whether the high ID error is a result of underfitting or overfitting? It seems more likely that a higher bias $b$ would lead to a higher OOD error $(b+\\Delta)^2$.\n\n* The intuition of the proposed framework is not new. Could the authors provide experimental results comparing the proposed method with other OOD generalization methods? Take a famous example, such as Invariant Risk Minimization (IRM)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CiczJNgLyO", "forum": "84rKwSBpNY", "replyto": "84rKwSBpNY", "signatures": ["ICLR.cc/2026/Conference/Submission22883/Reviewer_CanL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22883/Reviewer_CanL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978781704, "cdate": 1761978781704, "tmdate": 1762942423095, "mdate": 1762942423095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}