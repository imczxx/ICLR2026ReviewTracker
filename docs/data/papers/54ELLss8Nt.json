{"id": "54ELLss8Nt", "number": 5197, "cdate": 1757864417160, "mdate": 1763731491455, "content": {"title": "DKRF: Dynamic Knowledge Reasoning for Out-of-Distribution Generalization in Mobile GUI Agents", "abstract": "Graphical User Interface (GUI) agents demonstrate significant potential in cross-application tasks, yet their performance often drops sharply when facing out-of-distribution (OOD) scenarios (e.g., unseen task, different layout, etc.) in the open world. \nPrevious methods, modular agent frameworks and end-to-end native agents, are designed based on in-distribution (ID) mobile data, whether through manual designed modules or specially collected training sets, while neglecting the adaptability to diverse data in potential OOD mobile scenarios.\nTo overcome these limitations, we propose Dynamic Knowledge Reasoning Fine-tune (**DKRF**), a paradigm that shifts the agent's core capability from memorizing ID patterns to reasoning dynamically with external knowledge.\nDuring training, the model *explicitly* receives dynamic knowledge (e.g., *trajectories of similar tasks* or *reusable meta-functions*) and need to *incorporate* this knowledge in its reasoning chain, thereby learning to make knowledge-driven decisions. \nBased on DKRF, 1) we train an end-to-end native agent, **DKR-GUI**, and 2) further propose a modular agent framework, **MA-DKR**, which uses DKR-GUI as the planning core combined with knowledge retrieval and an executing agent to achieve collaboration between complex reasoning and precise execution. \nExperiments on multiple mobile benchmarks show that both DKR-GUI and MA-DKR significantly outperform existing methods, achieving an average 9.2\\% improvement in success rate in OOD mobile scenarios while also maintaining state-of-the-art performance in ID mobile tasks. \nOur results demonstrate that dynamic knowledge reasoning provides a general and effective solution for OOD generalization, highlighting its potential as a foundation for robust, knowledge-driven interactive agents.", "tldr": "Instead of just memorizing, our GUI agent learns how to reason with external knowledge to solve tasks in new environments.", "keywords": ["GUI-Agent", "out-of-distribution", "Multimodal Large Language Models", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/123a555768fe12b47b26b7a87a0e9989ec4a03ec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "DKRF addresses the poor out-of-distribution generalization of mobile GUI agents, and encourages agents to reason with external “dynamic knowledge” rather than memorizing in-distribution patterns. The authors also develop DKR-GUI, an end-to-end agent fine-tuned using retrieved trajectories and meta-functions, and extend it to MA-DKR where DKR-GUI serves as a planning agent paired with a separate executor. Experiments on AITZ, Android Control, CAGUI, and Kairos indicate improved OOD performance without degrading in-distribution accuracy, and ablations suggest that both knowledge components and decoupling planning from execution contribute to the gains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focuses on improving out-of-distribution generalization in mobile GUI agents. Adapting retrieval-augmented reasoning to this domain gives the work relevance to real-world application.\n2. The paper presents its problem setup, agent design, and experimental protocol clearly, and it conducts evaluations on both ID and OOD mobile benchmarks. The gains over baseline agents indicate practical value, and the ablations help illustrate the complementary roles of different knowledge sources."}, "weaknesses": {"value": "1. While the paper introduces DKRF and MA-DKR as new frameworks, many of the proposed components appear closely aligned with existing paradigms in RAG-based agents and modular LLM frameworks. Could the authors further clarify which parts of the framework [1][2][3] are genuinely novel in terms of algorithmic design or learning methodology, beyond integrating retrieval into the GUI agent pipeline?\n2. Although the paper claims that DKRF enables dynamic knowledge–conditioned reasoning for OOD generalization, the “dynamic knowledge” is still retrieved from pre-existing trajectories within the training corpus. The method relies on static prior data rather than genuinely adapting to unseen task structures or novel interface layouts. It is also unclear whether the model can handle tasks for which no semantically related trajectories exist. \n\n[1] Xu, Ran, et al. \"Retrieval-augmented GUI Agents with Generative Guidelines.\" arXiv preprint arXiv:2509.24183 (2025).\n[2] Loo, Gowen, et al. \"MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation.\" arXiv preprint arXiv:2509.03891 (2025).\n[3] Li, Yanda, et al. \"Appagent v2: Advanced agent for flexible mobile interactions.\" arXiv preprint arXiv:2408.11824 (2024)."}, "questions": {"value": "1. While you mention that the ground-truth thought t^* “must explicitly reference and utilize” the dynamic knowledge D_k, could you clarify how this requirement is actually ensured during data construction or generation? For example, is there any explicit constraint, filtering, or verification mechanism to prevent the model from producing thoughts that overlook or minimally rely on D_k?\n2. In cases where the provided dynamic knowledge D_k conflicts with the model’s prior knowledge or pretrained reasoning patterns, how is such inconsistency handled during training or inference? Is there any mechanism to resolve or prioritize between external knowledge and the model’s internal priors?\n3. In Table 5, adding a planning agent sometimes brings only marginal or even negative gains, and in a few cases GPT-4o performs worse than the “None” setting. It would be helpful if the authors could clarify what might be driving these outcomes"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "6jTsxWLSej", "forum": "54ELLss8Nt", "replyto": "54ELLss8Nt", "signatures": ["ICLR.cc/2026/Conference/Submission5197/Reviewer_Guzq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5197/Reviewer_Guzq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760731524072, "cdate": 1760731524072, "tmdate": 1762917939268, "mdate": 1762917939268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "general reply"}, "comment": {"value": "We appreciate the reviewer's critical question concerning OOD generalization. We understand the divergence primarily centers on **whether a scenario is still considered \"OOD\" if a small amount of task-relevant external knowledge is provided during inference.** This issue lacks systematic discussion in GUI Agent, and we hope to clarify our methodology and its necessity.\n\n1. **OOD in the GUI Domain is \"Dual-Dimensional\"; Providing External Knowledge Does Not Invalidate OOD Status**\n\nWe categorize OOD generalization into two dimensions:\n\n(1) **Zero-Knowledge OOD**: The traditional definition of OOD:\n+ Tasks and UI are out-of-distribution;\n+ No supplemental knowledge is provided during inference.\n\n(2) **Knowledge-Adaptive OOD—Our Focus**: \n\nDefined as:\n+ Tasks/UIs are out-of-distribution, and the model obtains and dynamically analyzes limited, unseen external knowledge (e.g., similar trajectories, operational examples) during inference to complete the new task.\n\nIt is necessary to emphasize that:\n+ The tasks and UIs of CAGUI and Kairos were never present in the training set, establishing strict out-of-distribution validity;\n+ The \"dynamic knowledge\" provided during inference is likewise absent in the training data;\nTherefore, \"OOD under knowledge conditions\" is not a relaxation of the OOD criteria but a more accurate characterization of real-world application scenarios.\n\n2. The Open Nature of the GUI Environment Necessitates 'Knowledge-Adaptive OOD'\n\nThe GUI environment exhibits the following characteristics:\n+ The number of Apps is massive, and versions are updated frequently;\n+ The UI state space is complex and continuously drifts;\n+ There are a large number of rare elements and long-tail semantics;\n+ Complete training set coverage is nearly impossible, and frequent fine-tuning carries high costs and risks (catastrophic forgetting).\n\nAs emphasized by UGround [5]:\n+ The core difficulty for GUI Agents lies not in the model but in the environment's change rate far exceeding data construction capability.\n\nIn most practical applications, humans rely on minimal external references (e.g., tutorials, history records) when confronting new interfaces. Consequently, utilizing small amounts of external knowledge to adapt to OOD environments is more aligned with real-world needs. \n\n**This concept is entirely consistent with the phenomenon of conditional generalization in multimodal models [1], prompt-driven adaptation in ICL [2], and the paradigm of solving OOD problems through external documentation in Retrieval-Augmented Generation (RAG) [3].**\n\n3. DKRF's Core Innovation: Training the Model \"How to Use Knowledge,\" Not Merely \"How to Retrieve Knowledge\"\n\nExisting RAG/GUI-RAG[6,7] methods typically assume that models can naturally utilize retrieved content during inference, yet extensive research often indicates that models lack stable knowledge integration capability.\n\nTo address this, DKRF explicitly constructs during the training phase:\n\n+ Training samples that include dynamic knowledge;\n+ Mandating that the reasoning chain forcefully references and integrates this knowledge;\n+ Thus enabling the model to directly learn a transferable \"knowledge integration function.\"\n\nThis design is consistent with the RAG practice of \"teaching the model how to use retrieved content through rationales\"[3] , and shares similar logic with \"structural OOD generalization that adapts to new target classes through open-set testing\" in visual tracking[4].\n\nTherefore, DKRF is not simply adding retrieval; **it explicitly models \"how to reason with external knowledge\" within the training objective**. This is the fundamental reason the model can improve  in completely unseen UIs/tasks.\n\n4. Modular Framework Further Validates DKRF's Generalizability\nWe integrated DKR-GUI as the Planning Agent into a UGround-style modular architecture, and experimented with various executing agents.\nResults show:\n+ Different executing models (including those with weaker grounding capability) benefit significantly;\n+ Indicating that the skill learned by DKRF is not a \"trick in a certain model,\" but a reusable, transferable OOD knowledge integration and planning capability.\n\nIn sum, this paper's OOD definition guarantees task/UI distribution novelty and explicitly models the universally required \"adaptation under external knowledge conditions\". DKRF enables stable knowledge-driven reasoning in OOD scenarios by explicitly teaching the model how to understand and integrate dynamic knowledge during training, a point fully supported in ICL, RAG, and visual literature."}}, "id": "pRuNIa4niB", "forum": "54ELLss8Nt", "replyto": "54ELLss8Nt", "signatures": ["ICLR.cc/2026/Conference/Submission5197/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5197/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5197/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763684265174, "cdate": 1763684265174, "tmdate": 1763686288326, "mdate": 1763686288326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "general reply"}, "comment": {"value": "We appreciate the reviewer's critical question concerning OOD generalization. We understand the divergence primarily centers on **whether a scenario is still considered \"OOD\" if a small amount of task-relevant external knowledge is provided during inference.** This issue lacks systematic discussion in GUI Agent, and we hope to clarify our methodology and its necessity. In the revised manuscript, we have incorporated the reviewers' suggestions, and the corresponding modifications are highlighted in blue.\n\n1. **OOD in the GUI Domain is \"Dual-Dimensional\"; Providing External Knowledge Does Not Invalidate OOD Status**\n\nWe categorize OOD generalization into two dimensions:\n\n(1) **Zero-Knowledge OOD**: The traditional definition of OOD:\n+ Tasks and UI are out-of-distribution;\n+ No supplemental knowledge is provided during inference.\n\n(2) **Knowledge-Adaptive OOD—Our Focus**: \n\nDefined as:\n+ Tasks/UIs are out-of-distribution, and the model obtains and dynamically analyzes limited, unseen external knowledge (e.g., similar trajectories, operational examples) during inference to complete the new task.\n\nIt is necessary to emphasize that:\n+ The tasks and UIs of CAGUI and Kairos were never present in the training set, establishing strict out-of-distribution validity;\n+ The \"dynamic knowledge\" provided during inference is likewise absent in the training data;\nTherefore, \"OOD under knowledge conditions\" is not a relaxation of the OOD criteria but a more accurate characterization of real-world application scenarios.\n\n2. The Open Nature of the GUI Environment Necessitates 'Knowledge-Adaptive OOD'\n\nThe GUI environment exhibits the following characteristics:\n+ The number of Apps is massive, and versions are updated frequently;\n+ The UI state space is complex and continuously drifts;\n+ There are a large number of rare elements and long-tail semantics;\n+ Complete training set coverage is nearly impossible, and frequent fine-tuning carries high costs and risks (catastrophic forgetting).\n\nAs emphasized by UGround [5]:\n+ The core difficulty for GUI Agents lies not in the model but in the environment's change rate far exceeding data construction capability.\n\nIn most practical applications, humans rely on minimal external references (e.g., tutorials, history records) when confronting new interfaces. Consequently, utilizing small amounts of external knowledge to adapt to OOD environments is more aligned with real-world needs. \n\n**This concept is entirely consistent with the phenomenon of conditional generalization in multimodal models [1], prompt-driven adaptation in ICL [2], and the paradigm of solving OOD problems through external documentation in Retrieval-Augmented Generation (RAG) [3].**\n\n3. DKRF's Core Innovation: Training the Model \"How to Use Knowledge,\" Not Merely \"How to Retrieve Knowledge\"\n\nExisting RAG/GUI-RAG[6,7] methods typically assume that models can naturally utilize retrieved content during inference, yet extensive research often indicates that models lack stable knowledge integration capability.\n\nTo address this, DKRF explicitly constructs during the training phase:\n\n+ Training samples that include dynamic knowledge;\n+ Mandating that the reasoning chain forcefully references and integrates this knowledge;\n+ Thus enabling the model to directly learn a transferable \"knowledge integration function.\"\n\nThis design is consistent with the RAG practice of \"teaching the model how to use retrieved content through rationales\"[3] , and shares similar logic with \"structural OOD generalization that adapts to new target classes through open-set testing\" in visual tracking[4].\n\nTherefore, DKRF is not simply adding retrieval; **it explicitly models \"how to reason with external knowledge\" within the training objective**. This is the fundamental reason the model can improve  in completely unseen UIs/tasks.\n\n4. Modular Framework Further Validates DKRF's Generalizability\nWe integrated DKR-GUI as the Planning Agent into a UGround-style modular architecture, and experimented with various executing agents.\nResults show:\n+ Different executing models (including those with weaker grounding capability) benefit significantly;\n+ Indicating that the skill learned by DKRF is not a \"trick in a certain model,\" but a reusable, transferable OOD knowledge integration and planning capability.\n\nIn sum, this paper's OOD definition guarantees task/UI distribution novelty and explicitly models the universally required \"adaptation under external knowledge conditions\". DKRF enables stable knowledge-driven reasoning in OOD scenarios by explicitly teaching the model how to understand and integrate dynamic knowledge during training, a point fully supported in ICL, RAG, and visual literature."}}, "id": "pRuNIa4niB", "forum": "54ELLss8Nt", "replyto": "54ELLss8Nt", "signatures": ["ICLR.cc/2026/Conference/Submission5197/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5197/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5197/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763684265174, "cdate": 1763684265174, "tmdate": 1763731644540, "mdate": 1763731644540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two key contributions: (1) DKRF, a framework for training an MLLM-based agent to call meta functions; and (2) MA-DKR, which leverages the DKRF-trained agent as a planner, augmented with few-shot RAG. The primary novelty lies in enhancing the agent’s meta-level capabilities and knowledge retrieval skills, enabling robust OOD generalization even when trained on a limited dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea is interesting, combing tool-call SFT and few-shot RAG in GUI agent.\n2. The paper is clearly written and easy to follow.\n3. The ablation experiments are well-organized."}, "weaknesses": {"value": "1. **Insufficient Baseline Comparison**  \n   The current evaluation lacks several important baselines. It is recommended to include more RL-based models (e.g., UI-R1 [1], GUI-R1 [2], InfiGUI-R1 [3]) as well as representative closed-source models (e.g., GPT and Claude) to better demonstrate DKRF’s capabilities and relative performance.\n\n2. **Benchmark Coverage**  \n   The evaluation currently omits dynamic mobile benchmarks. In particular, AndroidWorld [4] should be incorporated to more convincingly validate DKRF’s OOD generalization ability.\n\n3. **Related Work Coverage**  \n   The related work section is incomplete. Additional works on tool-call training (e.g., ToolRL [5], Tool-Star [6], [7], ARPO [8]), GUI tool calls (e.g., CoAct-1 [9]) and GUI few-shot learning (e.g., LearnAct [10]) should be reviewed and discussed. The authors are encouraged to compare their method with these approaches and explicitly highlight DKRF’s novelty in this context.\n\n4. **Experimental Analysis**  \n   The experimental section could be strengthened by visualizing the call frequency of meta functions before and after DKRF training, to better assess the improvement in meta-function utilization.\n\n5. **Training Details for Reproducibility**  \n   More details on DKRF’s training process should be provided to enhance reproducibility. Suggested additions include training examples, learning curves.\n\n6. **Paper Presentation**  \n   The layout of tables (particularly in Appendix C) should be revised. Adjusting table size and placement would improve readability and presentation quality.\n\n---\n\n**References**\n\n[1] UI-R1: Enhancing Efficient Action Prediction of GUI Agents by Reinforcement Learning  \n[2] GUI-R1: A Generalist R1-Style Vision-Language Action Model For GUI Agents  \n[3] InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners  \n[4] AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents  \n[5] ToolRL: Reward is All Tool Learning Needs  \n[6] Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning  \n[7] Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment  \n[8] Agentic Reinforced Policy Optimization  \n[9] CoAct-1: Computer-using Agents with Coding as Actions  \n[10] LearnAct: Few-Shot Mobile GUI Agent with a Unified Demonstration Benchmark"}, "questions": {"value": "It would be informative to include additional ablations on the training methodology. For example, why not train the meta functions using GRPO directly on the 10K dataset? Such experiments could provide further insights into the design choices and validate the superiority of the proposed approach.\n\n---\n\nIf all of the concerns listed above are thoroughly addressed in a revised version, I would be willing to raise my rating score to 6."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XdavRu4wdq", "forum": "54ELLss8Nt", "replyto": "54ELLss8Nt", "signatures": ["ICLR.cc/2026/Conference/Submission5197/Reviewer_uGHn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5197/Reviewer_uGHn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761298215635, "cdate": 1761298215635, "tmdate": 1762917939027, "mdate": 1762917939027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DKRF, a framework that enables GUI agents to generalize to out-of-distribution tasks by retrieving and reasoning over dynamic knowledge from prior trajectories and meta-functions.\nBuilt upon this idea, the authors design DKR-GUI and MA-DKR, achieving strong generalization and state-of-the-art performance across multiple mobile GUI benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper studies an interesting and important problem improving the generalization of GUI agents through dynamic knowledge reasoning, which is both novel and practically meaningful.\n2. The proposed framework is well-developed and comprehensive and demonstrating solid performance across multiple benchmarks."}, "weaknesses": {"value": "1. The paper suffers from clarity issues in definitions and notation. Many key terms are introduced long before being clearly defined (e.g., in Section 3.2), which makes it difficult to follow the framework. The meanings of several symbols such as $\\mathcal{K}$ and $k$ in all $D_k$s​ are unclear, and the paper occasionally mixes $i$ and $I$, which confuses the formulation.\n\n2. The benefit of using meta-function is not well explained. Its role in the overall framework lacks intuitive motivation, and empirically, it seems to contribute only marginal improvement, making it hard to assess its necessity.\n\n3. Although the method is claimed to primarily improve out-of-distribution generalization, the experimental results show similar gains in both in-distribution and OOD settings, which weakens the strength of this claim."}, "questions": {"value": "1. How does the proposed framework handle unseen knowledge in truly OOD tasks? If prior knowledge remains directly applicable to these unseen tasks, does that imply that the OOD setting in the benchmark is not genuinely out-of-distribution?\n\n2. In Table 4, the comparison with other planning-based agents raises fairness concerns. Are all models evaluated under the same conditions (e.g., fine-tuning vs. zero-shot)? Please clarify whether the baselines were evaluated in zero-shot mode, and how this might affect the comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9XxPGi3xeX", "forum": "54ELLss8Nt", "replyto": "54ELLss8Nt", "signatures": ["ICLR.cc/2026/Conference/Submission5197/Reviewer_UTCj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5197/Reviewer_UTCj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727242141, "cdate": 1761727242141, "tmdate": 1762917938562, "mdate": 1762917938562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dynamic Knowledge Reasoning Fine-tune (DKRF), a paradigm designed to enhance out-of-distribution (OOD) generalization for mobile GUI agents by shifting the learning objective from pattern memorization to knowledge-conditioned reasoning. The authors train an end-to-end agent that explicitly leverages retrieved trajectories and meta-functions during training, and further present a modular variant that combines the agent with a retrieval module and an executing agent. Experiments on four mobile GUI benchmarks show consistent improvements in OOD success rate while maintaining competitive performance on in-distribution tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets a well-recognized challenge in GUI agent research, robust OOD generalization, and positions dynamic reasoning as a principled solution beyond scaling or memorization.\n\n2. Results span multiple benchmarks and baselines. Moreover, the proposed method consistently improves OOD performance while preserving ID performance."}, "weaknesses": {"value": "1. The approach relies on a stronger teacher model to generate reasoning traces and dynamic knowledge, which increases computational cost and raises fairness concerns, as external knowledge beyond the original training data is introduced.\n2. The reported OS-ATLAS results appear lower than those reported in the original paper (e.g., at least ~71% SR on Android Control in the original results).\n3. In real-world scenarios, retrieval may produce irrelevant or low-quality trajectories.. How such knowledge affects the proposed method.\n4. The paper lacks qualitative visualization or cases showing how the proposed method plans and executes.\n5. The work focuses only on mobile GUI OOD scenarios. It would be useful to clarify whether the method generalizes to desktop GUI environments (e.g., OS-World) and what makes the mobile case special.\n6. Can authors report efficiency analyses about the proposed method, e.g., data annotation, training and inference cost, etc."}, "questions": {"value": "Please see the Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zhxpj8Rbql", "forum": "54ELLss8Nt", "replyto": "54ELLss8Nt", "signatures": ["ICLR.cc/2026/Conference/Submission5197/Reviewer_cZN5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5197/Reviewer_cZN5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827627454, "cdate": 1761827627454, "tmdate": 1762917938291, "mdate": 1762917938291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}