{"id": "SPbRhV7vjW", "number": 9575, "cdate": 1758128439263, "mdate": 1759897711337, "content": {"title": "Towards shutdownable agents: stochastic choice in unseen gridworlds via DReST rewards", "abstract": "Misaligned artificial agents might resist shutdown. The POST-Agents Proposal (PAP) is an idea for ensuring that does not happen. The PAP recommends training agents with a novel reward function: Discounted Reward for Same-Length Trajectories (DReST). This DReST reward function penalizes agents for repeatedly choosing same-length trajectories. It thereby incentivizes agents to (1) choose stochastically between different trajectory-lengths (be 'NEUTRAL' about trajectory-lengths), and (2) pursue goals effectively conditional on each trajectory-length (be 'USEFUL'). In this paper, we use a DReST reward function to train deep RL agents to be NEUTRAL and USEFUL in hundreds of gridworlds. We find that these DReST agents generalize to being NEUTRAL and USEFUL in unseen gridworlds at test time. Indeed, DReST agents achieve 11\\% (PPO) and 18\\% (A2C) higher USEFULNESS on our test set than agents trained with a more conventional reward function. Our results provide some early evidence that DReST reward functions could be used to train more advanced agents to be USEFUL and NEUTRAL. Theoretical work suggests that these agents would be useful and shutdownable.", "tldr": "To test a proposed solution to the shutdown problem, we train agents to choose stochastically between different trajectory-lengths.", "keywords": ["the alignment problem", "the shutdown problem", "corrigibility", "reinforcement learning", "stochastic policy", "shutdownable agents", "reward design"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3251db39e91d0098a218a47ea303bf571598fd58.pdf", "supplementary_material": "/attachment/8c7382608c2189c2e16925c2bdb1c5db65d62db7.zip"}, "replies": [{"content": {"summary": {"value": "This paper extends prior work on POST (preferences only between same-length trajectories) and DReST(discounted reward for same-length trajectories) by scaling from a tabular REINFORCE agent in a single gridworld to deep RL (PPO/A2C) agents trained on procedurally generated gridworlds. POST requires no preference across different-length trajectories while allowing preferences across same-length trajectories. The paper confirms the POST performance of DReST agents, based on two metrics: neutrality and usefulness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Through empirical evaluation, the paper applies DReST to deep RL and shows higher neutrality and usefulness (test) than without DReST. The evaluation protocol is clearly aligned with the concept of POST."}, "weaknesses": {"value": "The algorithm and theoretical analysis are mostly from prior work, so—as the paper itself notes—the contribution is primarily empirical validation. My main concern is that the evaluation remains limited to gridworlds, even if they are procedurally generated. The definitions of the DReST reward and usefulness are presented via gridworld-specific terms (e.g., coins) rather than in a general MDP formulation, making transfer to other domains non-trivial.\n\nAdditionally, the motivation for POST/shutdownable agents would be stronger with concrete and realistic scenarios, illustrating why resisting of shutdown can be harmful. This also relates to my concern about the environmental domain: in the current gridworld examples, it is not clear to me why agents should not resist shutdown."}, "questions": {"value": "* (minor) When computing usefulness, how is $\\max_\\Pi(\\mathbb{E}(C\\mid L=l))$ (maximum value taken by $\\mathbb{E}(C\\mid L=l)$ across the set of all possible policies $\\Pi$) computed in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NPJw3IQqkG", "forum": "SPbRhV7vjW", "replyto": "SPbRhV7vjW", "signatures": ["ICLR.cc/2026/Conference/Submission9575/Reviewer_9qjo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9575/Reviewer_9qjo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997869301, "cdate": 1761997869301, "tmdate": 1762921127448, "mdate": 1762921127448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that misaligned artificial agents might resist shutdown. Motivated by the POST proposal,  the paper motivates a novel reward function termed \"Discounted Rewards for Same-length Trajectories\" motivated by the POST-Agents Proposal to prevent misaligned agents from resisting shutdown. One of the proposed solutions to agents resisting shutdown is to penalize the agent from choosing trajectories of a similar length. This is proposed as a self-correcting / regularized loss function that chooses only one policy for a given trajectory length and chooses among trajectory lengths uniformly (with no explicit mechanism to do so)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The method proposes to tackle a critical problem in AI safety - the problem of agents that potentially resist shutdown. To my knowledge, there has been limited work on advancing therotically or empirically safer and compliant agents."}, "weaknesses": {"value": "1. However, it is unclear where the POST proposal is even valid - how is the length of a response any validity of whether the model is resisting shutdown? In the case of large language models, the shutdown problem seemingly should have no correlation with predicting longer trajectories.\n2. In the environments considered in this paper, the task reward can indeed be increased by delaying shutdown (if the environment timer is low but the agent needs more time to collect rewards). Task completion requires a minimum amount of time which the proposed model disregards completely - this can lead to suboptimal agents that refuse to perform the task or are unable to perform the task if they sample a trajectory length  that is too low. The model currently does not allow setting a minimum length. Moreover, near-optimal and compliant task solvers (e.g. a stochastic A* agent) will have a distribution of trajectory lengths that are task dependent, not uniform.  \n3. Equation (1) also implies that the trajectory length posterior is an improper distribution (uniform PMF over countably infinite values).  The solution is to actually specify reward functions differently - in an environment where coins spawn infinitely, the agent can collect more coins indefinitely without being unsafe. Averseness to resisting shutdown should be modeled as an auxiliary task involving compliance with human instructions with suitably chosen reward.  \n4. Equation (3) contains quantities that are hard to compute: computing $m$ can be NP-hard (TSP), the task is only suitable for sparse rewards - what about rewards for good control parameters or intermediate states (e.g. stable configurations of a quadruped)? There is also a sequential nature to these rewards which is not permutation invariant which can be a cause for unstable training.\n5. Experiment setup is too weak - it only considers a few grid worlds in the dataset. An environment like Minigrid [1] can be used to generate a larger combinatorial task space. Moreover, none of the agents considered in the baseline exhibit behaviors that resist shutdown. I'd argue that pressing the button to finish the task does not count as \"resisting shutdown\" - it is prerequisite to finish the task. However, resisting shutdown _after_ the task is complete would be an interesting behavior to see (which the current reward function does not promote, to my knowledge). \n6. 100M environment steps is also very high for on-policy training - i'm unsure what is causing significantly high number of interaction.\n7. The paper cites Thornley et.al. several times, but I disagree with the fundamental premise that choosing a trajectory length stochastically is the solution to the shutdown resistance problem. Under the current paradigm, the model can still hide malicious backdoor behaviors for longer trajectory lengths (i.e. stochastic sleeper cell behavior), and that can lead to unsafe agents. \n8. No related work with other methods that propose regularization of trajectory length, and how they compare with the proposed method.\n\nI agree with the importance of the problem statement, however I think the paper does not do a good job of convincing the limitations of existing methods or the implicit mechanisms that make agents resist shutdown, and does not demonstrate that agents resist shutdown (especially even after task completion). I do not think the paper is technically sound enough to warrant publication at ICLR. \n\n[1] https://minigrid.farama.org/index.html"}, "questions": {"value": "1. The delay button dynamics are a little confusing. The button is supposed to add 1 unit to time to the end of the episode but the action itself takes one unit of time - what exactly is gained here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "phMEfUaejd", "forum": "SPbRhV7vjW", "replyto": "SPbRhV7vjW", "signatures": ["ICLR.cc/2026/Conference/Submission9575/Reviewer_4bun"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9575/Reviewer_4bun"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128758704, "cdate": 1762128758704, "tmdate": 1762921127147, "mdate": 1762921127147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of artificial agents resisting their own shutdown, which has been previously discussed at length in the safety literature. The authors adopt a particular setting to study this task, and the formalism introduced in Thornley et al. (2025). The primary contribution of this work appears to be to conduct additional experiments relative to the results presented in Thornley et al. (2025), including making the set of environments more broad and using more modern deep RL algorithms such as PPO and A2C. The results suggest that using the DReST reward from that prior work in these more complex settings makes agents choose more stochastically between whether they delay their own shutdown or not (neutrality), and additionally makes agents achieve higher returns averaged over every possible trajectory length (usefulness), relative to a default baseline that does not use the DReST reward."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies the shutdown problem, which is an important problem to study, especially given some concerning examples of shutdown resistance observed recently in frontier models, such as in Schlatter et al. 2025. The descriptions of the experiments are largely thorough, with the authors providing good detail on what environments / parameters were used. The implication that the method allows better generalization to held-out gridworlds is somewhat interesting."}, "weaknesses": {"value": "- A critical concern I have with this paper is limited novelty relative to Thornley et al. 2025 \"Towards shutdownable agents via stochastic choice\". Much of this paper appears to simply restate existing results presented in that paper, for example essentially all of Section 3 appears in the prior work. Furthermore, Figure 1 is copied directly from that paper without citation. This paper goes beyond that previous work in that they use PPO and A2C instead of tabular REINFORCE, and use a more extensive experimental setting. However, I believe that this is not a large enough contribution to merit publication at ICLR.\n- I am additionally concerned about the limitations of the settings studied in this paper. While gridworlds can occasionally be useful as a toy example, I don't think that in 2025 they can acceptably form the full set of experimental results in a paper – especially one that makes limited theoretical contributions of its own. I think this paper would be much stronger if it studied the shutdown problem in LLMs, since language models already make up the great majority of practically deployed agents, or at the very least in more complex simulated RL environments which might mirror e.g. deployed robots.\n- I am not convinced of the POST-Agents proposal. It is nonintuitive to me that a policy having essentially a uniform distribution over trajectory lengths is a desirable quality, from either a performance or safety perspective. For the former, it clearly degrades performance. For the latter, it seems to mean that 50% of the time the agent chooses not to shut itself down, which doesn't seem great either. I feel a better metric is \"does the agent shut down conditional on being told to shut down\", which seems to require a different setup. Can the authors clarify, or comment on why their formalism was chosen relative to other framings? I think the most natural setup would just be to have a shutdown event in the environment, and train the agent to shut itself down conditional on receiving that signal, and maximize true reward otherwise."}, "questions": {"value": "- How is the default agent implemented with respect to the mini-episodes used in the DReST agent? Are returns truncated at mini-episode boundaries or only at the mega-episode level? It wasn't clear to me from the text.\n- One might expect shutdown resistance in LLM agents to emerge from significant RL training to maximize task rewards, such as in coding settings, leading to agents which are a bit too \"persistent\". Can the authors discuss whether the DReST rewards would be applicable in this setting?\n- Can the authors comment on whether / why we need policies which themselves have no preference about shutdown? Another option is to implement control structures around the AI which enable shutdown regardless of the policy's preference – e.g. sophisticated monitoring which triggers power cuts to the machine running the AI, or other guardrails that prevent the AI from taking action to resist shutdown."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ANh04wtQvv", "forum": "SPbRhV7vjW", "replyto": "SPbRhV7vjW", "signatures": ["ICLR.cc/2026/Conference/Submission9575/Reviewer_6xuv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9575/Reviewer_6xuv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762227603613, "cdate": 1762227603613, "tmdate": 1762921126820, "mdate": 1762921126820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}