{"id": "zvw9Hiwa0i", "number": 774, "cdate": 1756817677613, "mdate": 1759898242421, "content": {"title": "Beyond Scattered Acceptance: Fast and Coherent Inference for DLMs via Longest Stable Prefixes", "abstract": "Diffusion Language Models (DLMs) promise parallel generation via iterative denoising, yet their practical speed is often throttled by \\emph{schedulers} that accept scattered high-confidence tokens, fragmenting KV caches and forcing repeated local repairs. We present \\emph{Prefix Absorption}, a training-free inference principle operationalized by the \\emph{Longest Stable Prefix} (LSP) scheduler. In each iteration, LSP performs a single forward pass to locate the longest left-aligned run whose predictions are both high-margin and temporally stable, then snaps the candidate boundary to natural structural delimiters (e.g., punctuation or code boundaries) before atomically committing the block. This prefix-first topology preserves a single frozen/active boundary, converts KV updates into contiguous appends, and concentrates attention on a rapidly shrinking suffix. As a consequence, the active sequence length decays geometrically and the total work bends from an effectively cubic $O(N^3)$ regime toward near-quadratic $O(N^2)$ while maintaining coherence. On code generation (HumanEval, MBPP) and complex reasoning (GSM8K, GPQA) with LLaDA-8B and Dream-7B, LSP substantially reduces end-to-end latency and denoiser calls while matching or improving task quality relative to strong scattered-acceptance baselines. Ablations isolate the gains to LSP’s core components—adaptive block sizing, structural boundary snapping, and the prefix-first commitment topology—demonstrating that faster DLM inference can be achieved without retraining and is complementary to existing diffusion schedules.", "tldr": "", "keywords": ["diffusion language model", "efficent", "block"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1f40b0e462af3c84a35b62655602abd47c9b3d4b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper fixes “scattered acceptance” in DLM decoding by committing one contiguous left-aligned block each step via the Longest Stable Prefix (LSP): compute per-token margins in a single pass, choose a threshold to absorb ~25–50% of the active suffix, snap to delimiters, and atomically commit. This maintains a single frozen/active boundary, keeps KV cache contiguous, and empirically cuts latency/denoiser calls on LLaDA-8B and Dream-7B with near-parity quality (GSM8K, GPQA, HumanEval, MBPP).\n\nMy primary expertise is outside language diffusion models. I’ve done a careful read, but please weigh my field-specific comments accordingly."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The single prefix-first boundary is an elegant topology that aligns algorithmic coherence with KV-cache locality. \n\n2. The design is concrete—margin-based stability, adaptive thresholds, delimiter snapping, and a guaranteed-progress fallback.\n\n3. Empirically it yields 1.5–3× speedups with near-parity quality across reasoning and code tasks, with ablations isolating each component’s effect."}, "weaknesses": {"value": "1. LSP’s delimiter snapping introduces heuristic dependencies that could be brittle across tokenizers and vocabularies.\n\n2. Theoretical framing is light, with no formal bounds on early-commit errors or convergence.\n\n3. The evaluation does not quantify repair costs when early-committed tokens later require substantial rewriting."}, "questions": {"value": "See the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "S2yKSVZccp", "forum": "zvw9Hiwa0i", "replyto": "zvw9Hiwa0i", "signatures": ["ICLR.cc/2026/Conference/Submission774/Reviewer_Cvwn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission774/Reviewer_Cvwn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705533332, "cdate": 1761705533332, "tmdate": 1762915602311, "mdate": 1762915602311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Identifies scattered acceptance as a key bottleneck in improving efficiency of DLMs causing slow gather operations.\nProposes:\n1. LSP scheduler, where prefixes of tokens are committed instead of scattered commitment. This ensures the KV cache is not fragmented leading to efficient computation reuse.\n2. Adaptive thresholding to find the longest stable prefix instead of using a fixed threshold.\n3. Structural boundary snapping\n\nThe results show significant speedup and at the same time some quality gains that support the strength of the proposed changes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Identifies an important problem\n- Proposes a practical and elegant solution, especially because it is training free.\n- Demonstrates strong speedup performance and sometimes slight quality gains.\n- Thorough ablation studies."}, "weaknesses": {"value": "- The proposed method is less of a DLM and more of a blockwise autoregressive decoding.\n- The additional proposals (structural snapping) are not optional solutions, they are critical patches to get blockwise decoding to work.\n- Structural snapping is domain specific and might not perform well always. How is the performance on CJK?\n- The prefix commitment is irreversible, which means one of the most important advantages of DLMs is gone.\n- There is no mention on KV cache update of the committed sequence later on since bidirectional attention can impact it.\n- Tasks that need more fixing of the previously generated sequence (for example, creativity tasks) should be evaluated upon."}, "questions": {"value": "- See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QwLl106fWc", "forum": "zvw9Hiwa0i", "replyto": "zvw9Hiwa0i", "signatures": ["ICLR.cc/2026/Conference/Submission774/Reviewer_jKnz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission774/Reviewer_jKnz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866698508, "cdate": 1761866698508, "tmdate": 1762915602108, "mdate": 1762915602108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free approach for improving inference efficiency in diffusion language models (DLMs) by dynamically selecting the longest stable prefix based on confidence measured over a decoding window, thereby reducing cache fragmentation and redundant computations. The method achieves faster and more coherent generation compared to standard DLM decoding schedules."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Simple and effective approach that mitigates cache fragmentation without retraining.\n- Practical efficiency gains demonstrated across multiple pretrained DLMs.\n- Clear experimental reporting and consistent evaluation settings.\n- Compatible with existing architectures, requiring minimal modification."}, "weaknesses": {"value": "- The main novelty lies in using left windowed confidence instead of position-wise confidence, which is conceptually similar to autoregressive commitment heuristics.\n- The prefix-first decoding constraint may limit diffusion’s flexibility for editing, in-fill, or parallel token generation tasks.\n- The geometric decay rule for active suffix length and its thresholding lacks theoretical or empirical grounding.\n- GSM8K is a relatively simple benchmark for 7B-scale models; evaluating on AMC or AIME would better assess reasoning capability.\n- Table 1 could include more detail on what contributes to the speedup (e.g., cache locality vs. adaptive thresholding).\n- The robustness of suffix-length selection across model scales, sequence lengths, and task domains is not discussed."}, "questions": {"value": "- What motivates the geometric decay assumption for suffix-length determination?\n- Can the approach generalize to non-sequential or in-fill decoding tasks?\n- How sensitive is the thresholding to model scale or dataset domain?\n- Could the authors provide a breakdown of runtime gains (cache locality vs. token reuse)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uwF1oFFJwD", "forum": "zvw9Hiwa0i", "replyto": "zvw9Hiwa0i", "signatures": ["ICLR.cc/2026/Conference/Submission774/Reviewer_Yt8W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission774/Reviewer_Yt8W"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939388011, "cdate": 1761939388011, "tmdate": 1762915601933, "mdate": 1762915601933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies \"scattered acceptance\" as a primary bottleneck hindering the inference speed of Diffusion Language Models (DLMs), arguing that it leads to both algorithmic inefficiency and severe KV cache fragmentation. To address this, the authors propose the Longest Stable Prefix (LSP), a training-free and model-agnostic scheduling paradigm based on monolithic prefix absorption. In each step, LSP atomically commits the longest possible contiguous and stable prefix of the active sequence, identified via an adaptive, confidence-based mechanism and aligned to natural structural boundaries. This prefix-first topology maintains a contiguous KV cache and ensures a geometric decay in the computational workload. Experiments on challenging code and reasoning tasks demonstrate that LSP substantially accelerates inference (up to 3.4x) while preserving, and in some cases improving, generation quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The left-to-right commitment strategy dramatically improves KV cache efficiency.\n  \n2. The adaptive sizing mechanism intelligently modulates generation speed based on model confidence, achieving a superior speed-quality balance compared to fixed-size strategies.\n  \n3. The method achieves significant inference acceleration without sacrificing, and in some cases even improving, generation quality.\n  \n4. Its training-free and model-agnostic nature makes the method highly practical and broadly generalizable across different DLMs."}, "weaknesses": {"value": "1. **Limited Comparative Baselines:** The empirical evaluation primarily compares LSP against \"Full decoding,\" which serves as a quality baseline rather than a competitive speed-oriented one. The paper does not include a direct comparison against other contemporary DLM acceleration techniques, making it difficult to position LSP's performance within the existing state-of-the-art.\n  \n2. **Insufficient Hyperparameter Analysis:** The paper lacks a sensitivity analysis for its key hyperparameter, the fractional acceptance interval [α, β]. While the authors claim this parameter is robust, no data is provided to substantiate this, leaving the tuning effort required for new models or tasks an open question."}, "questions": {"value": "In Figure 1, the diagram for t=0 shows a sequence of logit margins starting with [0.2, 0.8, 0.4, 0.6, ...]. It then states that a stability threshold (τ) of 0.6 is chosen, resulting in a commitment of 3 tokens. This appears inconsistent with the paper's definition of a stable prefix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pxdsACWrHL", "forum": "zvw9Hiwa0i", "replyto": "zvw9Hiwa0i", "signatures": ["ICLR.cc/2026/Conference/Submission774/Reviewer_kHHK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission774/Reviewer_kHHK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762442214600, "cdate": 1762442214600, "tmdate": 1762915601724, "mdate": 1762915601724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}