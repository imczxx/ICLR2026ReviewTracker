{"id": "nspzrcvzcB", "number": 4654, "cdate": 1757735995857, "mdate": 1759898021424, "content": {"title": "Entropy-Monitored Kernelized Token Distillation for Audio-Visual Compression", "abstract": "We propose a method for audio-visual knowledge distillation. Existing methods typically distill from the latent embeddings or outputs. The former requires matching feature dimensions, if not the same architecture, between teacher and student models while the latter supports any teacher-student pairing, but tends to be less performant. Unlike them, we do not explicitly distill from the latent embeddings or outputs, but the pairwise relationships between embeddings across samples for each modality; this is realized as a kernel, which is the crux of our method, ``Kernelized Token Distillation (KTD)''. Specifically, we tokenize and embed the input for a given modality, and compute the Gram matrix across tokens, from which we distill. As audio and visual modalities afford different information for a task, we adaptively modulate distillation by measuring the entropy of each modality, leading to an Entropty-Monitored Kernelized Token Distillation (EM-KTD) scheme. Our method allows flexibility in complexity of kernel function to model relationships across tokens, which are selectively distilled to ensure high-fidelity supervision for the student. We evaluate EM-KTD on VGGSound and AVS-Bench, where we use 94\\% fewer parameters than the teacher while preserving 96.9\\% in performance for audio-visual event recognition and 96.5\\% on audio-visual segmentation.", "tldr": "", "keywords": ["Audio-Visual Learning", "Multimodal Learning", "Efficient Machine Learning", "Knowledge Distillation", "Audio-Visual Classification", "Audio-Visual Segmentation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad4cc4d42b8f2d2ab391c18a3035a4111182c5d3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "For network-agnostic audio-visual knowledge distillation, this paper proposes the Entropy-Monitored Kernelized Token Distillation (EM-KTD). Specifically, the Kernelized Token Distillation (KTD) distills pairwise relationships between latent tokens, captured in a Gram matrix. Then, an Entropy-Monitored (EM) scheme is proposed to selectively distill knowledge. It dynamically weighs the distillation loss for each modality (audio, visual, fused) based on the entropy of its feature embeddings. Experiments are conducted on audio-visual event classification and segmentation tasks, demonstrating the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The core idea of KTD is novel and well-motivated. Using kernelization to transfer the geometric structure of the teacher's latent space without requiring feature dimension alignment is an elegant solution to a common problem in knowledge distillation.\n- The method is validated on two distinct and challenging audio-visual tasks: classification and segmentation.\n- The results are impressive, showing that EM-KTD can compress a teacher model by over 90% while retaining nearly all of its performance, clearly demonstrating the practical utility of the proposed method."}, "weaknesses": {"value": "- The kernelization step has a computational complexity of $O(N^2)$ with respect to the number of tokens $N$ for each instance. While the paper shows strong performance, a more explicit discussion of the training time trade-offs in the main text would be beneficial. \n- The paper states that for the Entropy Monitor, \"additional task heads... are trained to minimize the cross entropy loss\" on the frozen teacher model (Sec. 3.3). This is a crucial implementation detail that lacks clarity. It is unclear *when* these linear probes are trained. Are they pre-trained on the entire dataset before the student distillation process begins? Or are they trained concurrently? A more detailed explanation of this procedure is needed for reproducibility and to fully understand the method's mechanics.\n- As shown in Table 2, the EM-KTD is not much superior to KTD in the S4 segmentation task.\n- The Figure can be improved. For example, it is challenging for readers to understand the 'entropy-monito' mechanism in Figure 2."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8HYpARKMR4", "forum": "nspzrcvzcB", "replyto": "nspzrcvzcB", "signatures": ["ICLR.cc/2026/Conference/Submission4654/Reviewer_YgiM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4654/Reviewer_YgiM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760553533122, "cdate": 1760553533122, "tmdate": 1762917493742, "mdate": 1762917493742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Entropy-Monitored Kernelized Token Distillation (EM-KTD), a novel framework for compressing audio-visual models. The method features two key components: 1) Kernelized Token Distillation (KTD), which distills the pairwise relationships between latent tokens captured in a Gram matrix, making the approach architecture-agnostic and highly expressive. 2) An Entropy-Monitored (EM) scheme that adaptively weights each modality's contribution based on its predictive entropy, ensuring high-fidelity supervision. This framework establishes a new state-of-the-art on two audio-visual tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper proposes a novel EM-KTD framework. By operating on the space of token relationships, it enables flexible and architecture-agnostic knowledge transfer.\n\n2.The method demonstrates state-of-the-art performance on audio-visual benchmarks, achieving a 94% parameter reduction while retaining over 96% of the teacher's performance.\n\n3.The claims are well-supported by comprehensive comparative experiments and thorough ablation studies that validate the contributions of each component."}, "weaknesses": {"value": "1.The O(N²) computational complexity of the pairwise kernelization step may limit the method's scalability to tasks with more input tokens.\n\n2.The EM scheme relies on entropy-loss-based classification tasks. This might limit its direct applicability to other tasks (e.g. regression tasks).\n\n3.The paper's validation on heterogeneous teacher-student architectures is a key strength, but this experimental context is detailed only in the appendix. Highlighting this setup in the main text would better frame the results and underscore the method's flexibility. To provide a more comprehensive evaluation, an additional experiment in a homogeneous setting is recommended to evaluate the method's performance when architectures are matched.\n\nTypos:\n\n1. The formulation of the Huber loss in Equation (2) has two minor errors: it is missing an equals sign, and the condition should be ||p - q|| < 1 instead of ||p, q|| < 1."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DfYakFHVZL", "forum": "nspzrcvzcB", "replyto": "nspzrcvzcB", "signatures": ["ICLR.cc/2026/Conference/Submission4654/Reviewer_VPsJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4654/Reviewer_VPsJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761386111675, "cdate": 1761386111675, "tmdate": 1762917493095, "mdate": 1762917493095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed EM-KTD, a novel framework to compress audio-visual models with knowledge distillation. The proposed method expands the idea of MTST (i.e., distilling pairwise relationships between token embeddings) to audio-visual understanding models, and makes the following improvements:\n\n1. Measures the similarity between latent embeddings with kernel functions rather than cosine similarity.\n2. Replaces KL-divergence loss with Huber loss, eliminating the need to use masking and Softmax mapping as in [MTST.](http://MTST.In) In this way, the student model can replicate the geometry of teacher latent space more precisely.\n3. Proposes Entropy Monitor to adjust the loss weight of each modality entropy, based on entropy of uni-modal classification predictions.\n\nThe authors evaluated the proposed method on two audio-visual understanding tasks: event classification and segmentation. Experiment shows a clear advantage of EM-KTD over both vanilla training and previous distillation baselines, providing an resource-efficient solution for audio-visual understanding."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposed an effective solution to the latent dimension mismatch problem in audio-visual knowledge distillation.\n2. Reasons for core design choices are clearly explained evidently supported by experiments, such as why removing softmax mapping (Appendix A) and why distilling token relationships within one instance in each modality (Appendix B).\n3. The proposed model is architecture-agnostic and supports various kernel functions (linear, polynomial, RBF), allowing trade-offs between computation complexity and performance."}, "weaknesses": {"value": "1. Compatibility to long training samples: Calculating the Gram matrix is an $O(N^2)$ operation, which may be computationally inefficient, especially when masking is not applied as in MTST. This might affect effectiveness of the proposed method in modalities with variable input lengths, like audio.\n2. While the advantage of KTD over MTST is concrete, the advantage of EM-KTD over KTD is comparably less significant.\n3. Figure 2 is a bit confusing. The “fusion modality” are not depicted and mechanism of entropy-weighted loss is ambiguous."}, "questions": {"value": "Lacking Multi-modal Distillation Baselines: Although EM-KTD is designed for audio-visual knowledge distillation, all baselines included are not specifically designed for multi-modal knowledge distillation. Can the proposed method be compared with multi-modal knowledge distillation methods mentioned in Related Work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6uSnoRPAP9", "forum": "nspzrcvzcB", "replyto": "nspzrcvzcB", "signatures": ["ICLR.cc/2026/Conference/Submission4654/Reviewer_rDf2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4654/Reviewer_rDf2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810262732, "cdate": 1761810262732, "tmdate": 1762917492895, "mdate": 1762917492895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Traditional knowledge distillation methods either require alignment of the teacher-student structure or fail to effectively utilize the internal structural information of modalities, and they lack an adaptive mechanism for the varying amounts of information across different modalities. To address these issues, the paper proposes Kernelized Token Distillation (KTD): instead of distilling the features themselves, it distills the similarity structure among tokens within a single sample, achieving structure-agnostic cross-modal distillation. Additionally, the paper introduces the Entropy-Monitored mechanism: by using the classification entropy of each modality from the teacher model to dynamically adjust the distillation weights and suppress the interference of low-information modalities. The paper validates the effectiveness on the VGGSound and AVS-Bench datasets, maintaining a considerable level of teacher performance even when the student model has significantly fewer parameters."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The EM-KTD method uses the \"structure among tokens\" as the distillation target, avoiding the dimension matching problem. The motivation for the entropy supervision mechanism is reasonable, and the overall method is innovative. The clarity of the paper is acceptable, but it lacks intuitive explanations for \"kernelized token\" and \"Gram matrix.\" It is suggested to optimize Figure 1 and Figure 2 to emphasize that the similarity matrix comes from multiple tokens within a single sample. The experimental quality of the paper is solid, with comprehensive comparisons to current mainstream methods and extensive ablation experiments. As a general method, if the paper could validate it on more modalities and tasks, it would further enhance its significance."}, "weaknesses": {"value": "1. The paper's explanation of the entropy prediction head $g_m(⋅)$ is insufficient. Firstly, in the methods section, it does not describe which stage of the process the entropy predictor is trained in. In the experimental section, it does not mention the source of the weights for $g_m(⋅)$. In the ablation part, it seems that the impact of the structure of $g_m(⋅)$ on the distillation results is not discussed.\n\n2. The explanation of dataset labels and information on line 192 is not clear enough. Clarifying the meanings of $n$ and $N$ in the Dataset section would make the paper more understandable.\n\n3. Validating the method on more datasets would more fully demonstrate its generalizability."}, "questions": {"value": "1. I still have some confusion about the entropy monitor $g_m(⋅)$. Firstly, how is it trained? If the task is not classification, how should the model evaluate the entropy?\n\n2. Although the paper tries linear, polynomial, and RBF kernel functions, it does not provide a systematic selection criterion or adaptive mechanism. When used for different tasks or datasets, is it necessary to manually select the kernel function?\n\n3. The paper mainly focuses on the visual and audio modalities. I am curious whether, as a general method, it would yield similar results on tasks involving other modalities (such as visual-text, audio-text, etc.). If this could be validated, it would well demonstrate the method's versatility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NN1XquAL1I", "forum": "nspzrcvzcB", "replyto": "nspzrcvzcB", "signatures": ["ICLR.cc/2026/Conference/Submission4654/Reviewer_6Yud"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4654/Reviewer_6Yud"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811539003, "cdate": 1761811539003, "tmdate": 1762917492394, "mdate": 1762917492394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}