{"id": "oKB0CacHaM", "number": 1694, "cdate": 1756907856793, "mdate": 1759898194219, "content": {"title": "Progressive Online Video Understanding with Evidence-Aligned Timing and Transparent Decisions", "abstract": "Visual agents operating in the wild must respond to queries precisely when sufficient evidence first appears in a video stream, a critical capability that is overlooked by conventional video LLMs evaluated in offline settings. The shift to an online, streaming paradigm introduces significant challenges: a lack of decision transparency, the difficulty of aligning response timing with visual evidence, and the need to maintain a global, causally consistent understanding under tight computational budgets. To address these issues, we propose a novel framework that decouples reasoning control from memory integration. We introduce Thinking-QwenVL, an instantiation of this framework with two core components. First, the Active Thinking Decision Maker (ATDM) is a transparent reasoning controller that externalizes its decision process using observable progress ($\\boldsymbol{\\rho}$) and confidence ($\\boldsymbol{c}$) metrics. This allows it to precisely time its response $t_r$ to match the first-sufficient-evidence timestamp $t^\\star$ while streaming its reasoning to the user. Second, the Hierarchical Progressive Semantic Integration (HPSI) module acts as an efficient memory system. It employs a set of learnable, multi-level aggregation tokens that are propagated across clips to build a rich, global cognitive state without exceeding token budgets. Extensive experiments demonstrate the effectiveness of ATDM and HPSI, e.g., Thinking-QwenVL improves the accuracy of the previous state-of-the-art from 67.63\\% to 71.60\\% on the StreamingBench benchmark.", "tldr": "Progressive, Causal Online Video Understanding with Evidence-Aligned Timing and Transparent Decisions Methods", "keywords": ["Online video understanding; Video Question Answering; Vision-Language Models; Decision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a25b18d80d7f3ba58de6abb83f38b2829e118905.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel framework for online video understanding in streaming scenarios, where visual agents must respond to queries precisely when sufficient evidence first emerges in a video stream. The proposed system, Thinking-QwenVL (built on Qwen2.5-VL-7B), decouples reasoning control from memory integration through two core modules: 1. Active Thinking Decision Maker (ATDM): A transparent reasoning controller that decomposes queries into sub-questions, tracks observable metrics like progress (ρ) and confidence (c), and self-triggers reflections for cross-clip causal updates. 2. Hierarchical Progressive Semantic Integration (HPSI): An efficient memory system using learnable multi-level aggregation tokens inserted at different decoder depths with structured sparse attention. Extensive experiments on online benchmarks (e.g., StreamingBench, OVOBench, OVBench, RTVBench) show significant improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper effectively bridges the gap between offline and real-world streaming video understanding by emphasizing evidence-aligned timing and transparency.\n2. HPSI's hierarchical aggregation reduces token overhead while preserving cross-clip relations and causal consistency, making it suitable for long videos under tight budgets.\n3. Extensive evaluations across multiple benchmarks validate the approach, with clear improvements over state-of-the-art models."}, "weaknesses": {"value": "1. Built on Qwen2.5-VL-7B, the results may not generalize to smaller or different architectures. What about the performance in smaller-sized models.\n2. Focuses primarily on visual evidences potentially lacks diversity. Have you considered the multi-modal settings?"}, "questions": {"value": "What about the computational overhead of the multi-stage ATDM process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bA3HTRqh74", "forum": "oKB0CacHaM", "replyto": "oKB0CacHaM", "signatures": ["ICLR.cc/2026/Conference/Submission1694/Reviewer_EPyy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1694/Reviewer_EPyy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657256095, "cdate": 1761657256095, "tmdate": 1762915858128, "mdate": 1762915858128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Thinking-QwenVL, a framework for streaming video understanding. It consists of two core components: 1) Active Thinking Decision Marker (ATDM) : a module that determines when to provide an answer based on task progress and confidence. 2) Hierarchical Progressive Semantic Integration (HPSI): an efficient memory system for streaming videos that uses multi-level learnable aggregation tokens to capture video content effectively. Thinking-QwenVL demonstrates strong performance across various online and offline video understanding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written and of high quality.\n\n- The proposed method is both powerful and elegant. The hierarchical processing of visual signals allows progressive handling of long video features from coarse to fine levels. Moreover, decomposing questions into sub-questions and leveraging confidence scores enhances flexibility while preserving the base model’s capabilities.\n\n- The experiments are extensive and comprehensive; including both online and offline benchmarks effectively demonstrates the framework’s robustness and versatility."}, "weaknesses": {"value": "Despite its effectiveness, several concerns and missing details remain:\n\n- In HPSI, it is unclear how the authors decided on the number of aggregation levels (i.e., three). Does this configuration balance efficiency and accuracy? From Table 4, the performance gain from levels 2–3 appears marginal, as the third row (only level 1) performs comparably to the full method. Further clarification would help justify the design choice.\n\n- In ATDM, the use of sub-questions and confidence scores is crucial, yet the paper lacks sufficient detail to understand how the module operates. Prior work [1] shows that sub-questioning can increase complexity or even degrade performance if applied indiscriminately. Have authors faced such issues? Furthermore, how were the thresholds (e.g., 0.85 and 0.5) for confidence scores in Parts 4 and 5 determined? A clearer interpretation is needed.\n\n- In Table 1, Thinking-QwenVL performs worse than its backbone model (Qwen2.5-VL), but this issue is not addressed, which contradicts the claim in L410–414.\n\n- The recent SOTA method [2] is missing from the comparison, and Thinking-QwenVL shows inferior performance to [2].\n\n*I would consider revising the rating if I misunderstood any part, and the authors clarify these issues in the rebuttal phase.*\n\n**References**\n\n[1] Confidence-guided Refinement Reasoning for Zero-shot Question Answering, arXiv 2025\n\n[2] StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant, arXiv 2025"}, "questions": {"value": "See the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mKO0s7yeYZ", "forum": "oKB0CacHaM", "replyto": "oKB0CacHaM", "signatures": ["ICLR.cc/2026/Conference/Submission1694/Reviewer_arDz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1694/Reviewer_arDz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803583111, "cdate": 1761803583111, "tmdate": 1762915857980, "mdate": 1762915857980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of online video understanding, focusing on two critical but often overlooked aspects: evidence-aligned response timing and decision transparency. The authors argue that conventional video LLMs, typically evaluated in offline settings, are ill-suited for real-world streaming scenarios where an agent must respond precisely when sufficient evidence becomes available. To tackle this, they propose a novel framework, Thinking-QwenVL, which decouples reasoning control from memory integration. The framework has two core components: The Active Thinking Decision Maker (ATDM): a reasoning controller that externalizes its decision process using explicit progress ($p$) and confidence ($c$) metrics, allowing it to align its response time ($t_r$) with the first-sufficient-evidence timestamp ($t^*$). The Hierarchical Progressive Semantic Integration (HPSI) module: an efficient memory system that uses multi-level aggregation tokens to maintain a global cognitive state under tight computational budgets. The paper demonstrates strong empirical results, achieving a new state-of-the-art on the StreamingBench benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper clearly articulates and tackles the critical, practical challenges of response timing and decision transparency in streaming video analysis, which is a major step towards real-world applications.\n\n- The ATDM module provides an effective framework for making the model's decision-making process transparent and quantifiable through progress ($p$) and confidence ($c$) scores. This is a significant strength for user trust and controllability.\n\n- The HPSI module is a technically sound and well-executed solution for maintaining long-term, causally consistent context within a limited token budget, as demonstrated by its strong performance on long-video benchmarks.\n\n- The paper achieves state-of-the-art results on several challenging online video benchmarks, most notably StreamingBench, providing strong evidence for the effectiveness of the proposed Thinking-QwenVL framework. The ablation studies are thorough and convincing."}, "weaknesses": {"value": "- The primary weakness is the lack of analysis on the computational latency introduced by the ATDM module. The five-part reasoning process (generating instructions, decomposing the question, captioning, extracting answers, and reflecting) seems to require multiple LLM inference steps for each incoming video clip. This could create a significant processing bottleneck in a true real-time scenario, a concern that is not adequately addressed in the paper.\n\n- The 5-part ATDM process, while transparent, appears highly structured and heavily engineered. This raises questions about its robustness and generalizability. How dependent is this structure on the specific base model (Qwen2.5-VL) and extensive prompt engineering? It's unclear if this complex chain-of-thought would transfer effectively to other video LLMs without significant re-tuning.\n\n- The concept of \"Active, Self-triggered Thinking\" (Part 5 of ATDM) is an interesting idea but is not described in sufficient detail. The paper mentions it is triggered by low confidence or major semantic shifts, but the exact trigger conditions (e.g., thresholds, detection mechanisms) and the concrete steps of the \"reflection\" process are not clearly defined. This makes the mechanism less reproducible and its contribution harder to assess."}, "questions": {"value": "- Could you provide an analysis of the wall-clock latency or computational overhead (e.g., number of forward passes per second of video) introduced by the ATDM module? How does this compare to simpler streaming models like Flash-VStream or Dispider, and how might it impact real-time performance?\n\n- Can you elaborate on the development process for the 5-part ATDM prompt structure? How sensitive is the model's performance to the specific wording and structure of these prompts? Have you experimented with applying ATDM to other base models to test its generalizability?\n\n- Could you provide a more detailed explanation of the \"self-triggered reflection\" mechanism? What are the specific criteria for triggering it (e.g., confidence thresholds, how are \"major semantic shifts\" detected)? What does the model do during reflection (e.g., does it re-process past clips, revise the question decomposition)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "psozfTtL1z", "forum": "oKB0CacHaM", "replyto": "oKB0CacHaM", "signatures": ["ICLR.cc/2026/Conference/Submission1694/Reviewer_iwc4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1694/Reviewer_iwc4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909653083, "cdate": 1761909653083, "tmdate": 1762915857829, "mdate": 1762915857829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Thinking-QwenVL, a new framework for online, evidence-aligned video understanding, addressing the key challenge of determining when a model should respond based on visual evidence as it streams. The framework explicitly separates reasoning control from memory integration, leading to two key components: Active Thinking Decision Maker (ATDM) and Hierarchical Progressive Semantic Integration (HPSI). The authors evaluate Thinking-QwenVL across several online and offline benchmarks (StreamingBench, OVOBench, RTVBench, OVBench, MLVU, VideoMME) and report significant improvements, e.g., +3.97% over Dispider on StreamingBench and +4.9% on OVOBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work tackles a largely under-explored but crucial problem — real-time, evidence-aligned video understanding — distinct from traditional offline long-video reasoning.\n2. The idea of decoupling reasoning control from memory integration is conceptually strong and practically motivated. The multi-level aggregation mechanism (HPSI) is an elegant solution for progressive semantic integration, offering a new perspective beyond standard pooling or RAG-based methods.\n3. The paper is clearly written and well-structured, with informative visual diagrams (e.g., Fig. 1–4) illustrating timing, aggregation, and decision flow. Each module (ATDM, HPSI) is described step-by-step, making a complex architecture accessible."}, "weaknesses": {"value": "1. While empirically strong, the paper could benefit from more conceptual analysis of why ATDM’s quantitative progress and confidence signals yield better evidence alignment. For instance, is the model implicitly learning uncertainty calibration or temporal gating?\n2. The paper claims transparency and interpretability, but does not include user studies or objective interpretability metrics to support these claims (e.g., human evaluation of decision clarity or correctness of rationales).\n3. Although ATDM aims for real-time decision-making, the paper does not explicitly report latency, throughput, or computational overhead compared to simpler baselines. Such results would help assess deployment feasibility for real-world streaming agents.\n4. The ablations treat ATDM and HPSI largely separately, but it would be informative to analyze their synergy — for example, whether ATDM decisions remain robust if memory integration is partially disabled or simplified."}, "questions": {"value": "1. For the reference in Latex, please use \\citep instead of \\cite to fix the reference issue in the main paper so that \"Xun et al. (2025)\" could be \"(Xun et al. 2025)\"\n2. Have you conducted human or expert evaluations on whether ATDM’s progress/confidence signals improve user trust or understanding compared to black-box baselines?\n3. What is the per-frame or per-second latency of Thinking-QwenVL relative to Dispider and Flash-VStream? How does hierarchical aggregation affect token throughput?\n4. How does the model behave when the input stream contains missing frames, abrupt scene transitions, or noisy temporal cues? Does ATDM maintain stability in such conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6dUJb4CJJ5", "forum": "oKB0CacHaM", "replyto": "oKB0CacHaM", "signatures": ["ICLR.cc/2026/Conference/Submission1694/Reviewer_5jps"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1694/Reviewer_5jps"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1694/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983029827, "cdate": 1761983029827, "tmdate": 1762915857695, "mdate": 1762915857695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}