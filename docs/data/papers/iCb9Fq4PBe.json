{"id": "iCb9Fq4PBe", "number": 18490, "cdate": 1758288266054, "mdate": 1759897099934, "content": {"title": "Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful paradigm to improve Large Language Models on reasoning tasks such as coding, math or logic. To assess the reasoning boundary (the fraction of problems a model can solve) researchers often report pass@k at large sampling budgets. Recent results reveal a crossover phenomenon: while RLVR models outperform the base model at small k values, the base model usually outperforms them when sampling a very large number of completions. This has been interpreted as evidence that base models have a larger reasoning boundary. We argue that on tasks with discrete answer spaces, such as math with numeric outputs, pass@k at large k reflects the increasingly higher chance of success in the limit of the number of trials rather than genuine reasoning, and can therefore be misleading. We propose cover@tau, which measures the fraction of problems that a model can solve for which at least a tau proportion of completions are correct. Unlike pass@k, cover@tau captures reasoning under an explicit reliability threshold: models that rely on random guessing degrade rapidly as tau increases. We evaluate several RLVR models using cover@tau-based metrics and illustrate how the relative rankings of popular algorithms change compared to pass@1, offering a different perspective on reasoning boundaries.", "tldr": "We introduce a new metric, cover@tau, to measure the reasoning boundary of LLMs at explicit reliability thresholds", "keywords": ["LLMs for reasoning", "reinforcement learning with verifiable rewards", "evaluation metrics", "math datasets"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1530a870dc1d3716225fff3cf1d29cd1de4adf4c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Cover@τ, a new evaluation metric intended to complement or replace Pass@k when assessing the reasoning performance of Reinforcement Learning with Verifiable Rewards (RLVR) models. The authors argue that Pass@k can be misleading for tasks with small discrete answer spaces, such as mathematical reasoning, because increasing k artificially raises success rates through random chance rather than genuine reasoning ability. They formally define Cover@τ as the fraction of tasks for which at least a τ fraction of completions are correct, show that Pass@k is a Beta-weighted integral of Cover@τ, and empirically evaluate various RLVR models (GRPO, GSPO, PPO-GAE, KL-Cov, GRPO-Unlikeliness) on OMEGA and Reasoning Gym datasets. The results suggest that Cover@τ provides a more nuanced view of “breadth” versus “depth” of reasoning capabilities, favoring exploration-preserving algorithms such as KL-Cov."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "###  Clear problem statement and motivation:\n\nThe paper articulates a genuine issue with Pass@k saturation for small answer spaces and provides both intuitive and formal justification.\n\n### Elegant theoretical connection:\n\nThe derivation showing Pass@k as a weighted average over Cover@τ via a Beta(1, k) distribution is mathematically neat and offers interpretability.\n\n### Visualization clarity:\n\nThe Pass@k vs. Cover@τ plots (Figures 1–3) effectively demonstrate the saturation and trade-off phenomena, supporting the paper’s main claims."}, "weaknesses": {"value": "### Limited novelty beyond metric reformulation:\n\nWhile Cover@τ is conceptually appealing, it is essentially a restatement of existing majority or consistency metrics (maj@k, cons@k) with a continuous threshold. The main novelty lies in the interpretation, not in the metric itself. This weakens the contribution for a top-tier venue.\n\n### Overstated theoretical contribution:\n\nThe mathematical results (Proposition 1–2 and corollaries) are straightforward consequences of calculus and probability theory. They provide insight but do not represent new theory on reasoning evaluation or RLVR optimization.\n\n### ncomplete problem diagnosis: \nThe paper identifies a real issue with Pass@k but the solution doesn't fully address it:\n\n- The problem occurs specifically when answer spaces are small and uniformly distributed. The paper doesn't systematically characterize when Pass@k is problematic vs. when it's fine.\n\n- For many reasoning tasks (especially open-ended or continuous domains), the random guessing problem is minimal, making the motivation less universal than presented.\n\n- No analysis of task properties that determine whether random guessing is a concern.\n\n### Practical utility unclear:\n\n- The paper shows that rankings can differ across τ values, but doesn't establish when practitioners should care about different τ levels.\n\n- No clear recommendation for which τ to use. Should we report a τ profile or a single number? If a single number, how do we choose it?\n\n- The claim that Cover@τ is \"more informative\" is not well-supported. Information ≠ utility. Practitioners need guidance on decision-making, not just more curves."}, "questions": {"value": "I know my review is kind of critical. But would be pleased to see your response over:\n\n- In practical applications (e.g., code generation, math competition problems), how often is random guessing actually a problem?\n\n- How should practitioners choose τ for their use case?\n\n- Can you provide guidance on when Cover@τ should replace Pass@k vs. complement it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lz4ApuBJho", "forum": "iCb9Fq4PBe", "replyto": "iCb9Fq4PBe", "signatures": ["ICLR.cc/2026/Conference/Submission18490/Reviewer_uWMw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18490/Reviewer_uWMw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760483917602, "cdate": 1760483917602, "tmdate": 1762928188203, "mdate": 1762928188203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Cover@τ, a reliability-thresholded extension of Pass@k that measures the fraction of problems solved with at least a τ proportion of correct completions. The metric is used to analyze reasoning boundaries of RLVR models and slove limitations of Pass@k at large sampling numbers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is clearly written, with well-organized sections and consistent notation.\n2. The topic of evaluation metrics for reasoning stability is important for RLVR research community."}, "weaknesses": {"value": "The core idea of thresholded generalization of Pass@k is not novel. The recently published ACL 2025 paper “Are Your LLMs Capable of Stable Reasoning?” introduced an almost identical formulation, G-Pass@k and mG-Pass@k, with the same motivation and nearly equivalent equations. That paper also showed that Pass@k is a special case as τ→0 and provided extensive stability analyses.\n\nThe current submission does not cite or discuss that prior work, which gives a misleading impression of originality. Mathematically, Cover@τ and G-Pass@kτ differ only in notation. Hence, the contribution appears incremental without proper acknowledgment or comparison."}, "questions": {"value": "As mentioned in weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "The submission introduces a metric (Cover@τ) that is conceptually and mathematically equivalent to the G-Pass@k metric proposed in the ACL 2025 paper “Are Your LLMs Capable of Stable Reasoning”, without any citation or acknowledgment. This raises potential research integrity concerns regarding proper attribution of prior work."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IAGAfyBafz", "forum": "iCb9Fq4PBe", "replyto": "iCb9Fq4PBe", "signatures": ["ICLR.cc/2026/Conference/Submission18490/Reviewer_Nwpw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18490/Reviewer_Nwpw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898192279, "cdate": 1761898192279, "tmdate": 1762928187548, "mdate": 1762928187548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Cover@τ, a new evaluation metric for assessing reasoning abilities of LLMs that accounts for reliability thresholds. The authors argue that Pass@k at large k is misleading for tasks with discrete answer spaces, as it conflates lucky guesses with genuine reasoning. They demonstrate that Pass@k is a Beta(1,k)-weighted average of Cover@τ, introduce the OMEGA and Reasoning Gym benchmarks, and evaluate several RLVR methods showing different breadth-depth trade-offs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear problem formulation**: Effectively articulates the degeneracy of Pass@k \n   at large k with concrete examples (Figure 1), showing how models can achieve \n   Pass@k=1 through random guessing on tasks with small answer spaces. The Beta(1,k)-weighted interpretation of Pass@k \n   (Proposition 1) provides valuable insight into why Pass@k is biased toward low-τ \n   regions, emphasizing \"lucky hits\" over reliability. Proposition 2 shows that \n   Cover@τ dominance implies Pass@k dominance but not vice versa.\n\n2. **Breadth-depth framework**: The explicit trade-off between coverage (low τ) and \n   reliability (high τ) is conceptually clean and practically interpretable.\n\n3. **Systematic RLVR evaluation**: First comprehensive comparison of RLVR methods \n   (GRPO, PPO, GSPO, KL-Cov, Unlikeliness) using reliability-aware metrics, revealing \n   that entropy-preserving methods (KL-Cov) achieve better stability."}, "weaknesses": {"value": "1. **Insufficient acknowledgment of prior work**: The recent work \"Are Your LLMs \nCapable of Stable Reasoning?\" (Liu et al., arXiv:2412.13147, Dec 2024) proposed \nG-Pass@k, which measures the same concept—coverage at reliability threshold τ. \nWhile your Cover@τ provides cleaner theoretical formulation and focuses on RLVR \nmethods, the submission should:\n- Discuss the relationship between Cover@τ and G-Pass@k\n- Clarify whether this is concurrent/independent work\n- Compare the estimation approaches (your implicit empirical frequency vs. their \n  explicit hypergeometric distribution)\n). \n2. **Shallow RLVR analysis**:\nWhy does preventing entropy collapse (KL-Cov) improve Cover@τ specifically, how do training hyperparameters affect the breadth-depth trade-off, what Cover@τ profile should practitioners target during training? \n3. **Missing critical experiments**: Data contamination/overfitting effects on Cover@τ (present in G-Pass@k)\nSample complexity analysis: how does estimation variance change with n and k?\nAblation on AUC+ vs. alternative aggregation methods\n4. **Limited practical guidance**: The paper shows Cover@τ reveals different rankings but doesn't provide actionable recommendations for τ selection based on application requirements."}, "questions": {"value": "See the questions in **Weaknesses** part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hbvJFbPviQ", "forum": "iCb9Fq4PBe", "replyto": "iCb9Fq4PBe", "signatures": ["ICLR.cc/2026/Conference/Submission18490/Reviewer_QhEP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18490/Reviewer_QhEP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959308515, "cdate": 1761959308515, "tmdate": 1762928186660, "mdate": 1762928186660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that Pass@k at large k can be misleading for reasoning tasks with discrete answer spaces, because success can arise from random guessing without reliable reasoning. The authors propose Cover@τ, the fraction of problems for which at least a τ proportion of sampled completions are correct. They prove that Pass@k is a Beta weighted average of the Cover@τ curve, so Pass@k emphasizes very low τ and ignores reliability, and they also introduce a pairwise comparison measure AUC⁺_cover. Experiments on OMEGA and Reasoning Gym show that rankings of RLVR methods change when we look at coverage under reliability thresholds, with methods that preserve exploration and entropy doing better at higher τ. The crossover plots and the tables in this paper support the claim that Cover@τ reveals different capability trade-offs than Pass@k."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Cover@τ provides a simple but powerful reliability controlled view. The identity that Pass@k is a Beta weighted integral over τ makes the bias of large k very clear and easy to communicate to practitioners. This theory part is clean and correct from first principles.\n2. On OMEGA Probability, the figure on page 2 shows Pass@k saturates as k grows because the answer support is small, while the Cover@τ curve decreases smoothly and separates models by reliability.\n3. The work addresses the widely reported crossover between RLVR and base models under Pass@k and provides a principled reason why this happens."}, "weaknesses": {"value": "1. To estimate per-problem success rates, the paper uses very large K, up to 8196 samples, but the statistical uncertainty of the empirical proportions is not analyzed. Confidence bands for Cover@τ and sensitivity to K and temperature are important for fair comparisons, especially when τ is small.\n2. All main results are on math with numeric answers. Code tasks with test suites and other verifiable domains would strengthen the claim that the metric generalizes across RLVR use cases. The ongoing literature already evaluates both math and code when discussing reasoning boundaries.\n3. Experiments start from a single family and size, Qwen 2.5 7B Instruct, so it is hard to know if the findings hold for other families or larger models. For example, due to contaminations, Qwen2.5 7B may perform very differently from Llama. Several reports show that behavior under RLVR depends on the base model and training recipe."}, "questions": {"value": "1. How sensitive are your Cover@τ curves to the sample size K, decoding temperature, and nucleus top p. Can you add bootstrap confidence intervals or a Bayesian treatment that reports uncertainty on coverage across τ.\n2. Can you include a code generation study, for example HumanEval or other verifiable suites, and compare Cover@τ with CoT Pass@k and maj@k there. This would help show whether the metric behaves similarly beyond numeric answers.\n3. Could you report results on at least one more base model family or size. Some recent papers suggest that the interplay between RLVR and base distribution varies across models.\n\nMy final rating depends on the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nB7AiIBKBT", "forum": "iCb9Fq4PBe", "replyto": "iCb9Fq4PBe", "signatures": ["ICLR.cc/2026/Conference/Submission18490/Reviewer_FVku"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18490/Reviewer_FVku"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18490/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970727991, "cdate": 1761970727991, "tmdate": 1762928182189, "mdate": 1762928182189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}