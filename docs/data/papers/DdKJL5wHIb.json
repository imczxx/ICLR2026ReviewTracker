{"id": "DdKJL5wHIb", "number": 13527, "cdate": 1758218920451, "mdate": 1759897430848, "content": {"title": "Generalizable Geometric Image Caption Synthesis", "abstract": "Multimodal large language models (MLLMs) have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most symbolic data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting accuracy-guided RLVR to refine captions for symbolically synthesized geometric images, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset GeoReasoning-10K achieves non-trivial performance gains, yielding accuracy improvements of 2.8\\%–4.8\\% in non-geometric subtasks of MathVista and MathVerse. This generalization ability is further validated in MMMU, where significant improvements of 2.4\\%–3.9\\% in Art \\& Design and Tech \\& Engineering tasks are observed.", "tldr": "", "keywords": ["MLLM", "Data Generation", "Geometry"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1672fc6bd79a00a9616454a091a4060b1cd8d3d4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GeoReasoning-10K, a dataset of 10,000 high-quality image-caption pairs designed to enhance the cross-modal reasoning capabilities of multimodal large language models (MLLMs) in geometric problem-solving. The authors propose a novel Geo-Image-Textualization framework, integrating Reinforcement Learning with Verifiable Rewards (RLVR) to iteratively refine data quality and model performance. Experimental results demonstrate significant improvements in both in-domain tasks (e.g., geometry, algebra) and out-of-domain generalization (e.g., art, design)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper proposes the Geo-Image-Textualization framework, which combines reinforcement learning and reward mechanisms (RLVR) to significantly improve the semantic alignment between geometric images and textual descriptions.\n2.\tComprehensive experiments across 3 benchmarks (MathVista, MathVerse, MMMU) with multiple trials and error bars"}, "weaknesses": {"value": "1.\tFigure 1 is unconvincing. How can training on geometry enable the model to calculate \"the age difference between King Richard III and Queen Anne Neville\"? This generalization lacks theoretical support. A more plausible explanation is that increased model scale (more training) improves general capabilities, rather than transfer from geometric reasoning. Additionally, Figure 1 is poorly placed—not referenced until Section 4.3.\n2.\tThe method description needs more details: how are the geometric relationships constructed? Does it cover all core concepts of a standard geometry curriculum? What are the RELATION SAMPLING rules? \n3.\tExplicitly encoding semantic relationships within the image is not surprising, as existing datasets (e.g., MAVIS, TR-COT) achieve similar effects. It's unclear how its quality compares to them. Additionally, the paper lacks discussion on how this annotation impacts generalization.\n4.\tThe improvement in experimental results in Table 1 is minimal, with significant overlap in standard deviation.\n5.\tThe citation of Figure 5 in Section 3.3 seems to be incorrect."}, "questions": {"value": "See the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pFrmK6zTlY", "forum": "DdKJL5wHIb", "replyto": "DdKJL5wHIb", "signatures": ["ICLR.cc/2026/Conference/Submission13527/Reviewer_hdrJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13527/Reviewer_hdrJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652366097, "cdate": 1761652366097, "tmdate": 1762924132359, "mdate": 1762924132359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the poor geometric reasoning and cross-modal generalization capabilities of multimodal large language models (MLLMs), stemming from the lack of high-quality, fully aligned geometry image-text datasets and the poor generalization of existing symbolic synthesis pipelines. To solve these issues, the authors propose a reinforcement learning-based framework called Geo-Image-Textualization and construct a dataset named GeoReasoning-10K (10,000 fully aligned geometry image-caption pairs). The framework integrates a Reinforcement Learning with Verifiable Rewards (RLVR) phase, which iteratively optimizes dataset quality and model performance through alternating caption refinement and model retraining. Experimental results show that MLLMs trained on GeoReasoning-10K achieve significant accuracy improvements on in-domain benchmarks (MathVista, MathVerse) and out-of-domain tasks (MMMU’s Art & Design, Tech & Engineering)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper demonstrates strong originality through targeted innovation in addressing unmet needs and creative fusion of existing methods. First, it identifies a critical gap in multimodal large language model (MLLM) research—poor geometric reasoning due to misaligned image-text data—an underexplored niche despite MLLMs’ success in general tasks.\n2. The RLVR (Reinforcement Learning with Verifiable Rewards) Phase reimagines reinforcement learning (RL) for dataset refinement: instead of using RL to optimize model inference directly, it creates a \"data-model co-evolution loop\"—a creative combination of RL, captioning, and reasoning evaluation.\n3. Experimentally, the authors use rigorous ablation studies (e.g., testing with/without RLVR, with/without cold-start) to isolate the impact of each component, avoiding conflation of results. They validate on multiple benchmarks (MathVista, MathVerse, MMMU) across in-domain and out-of-domain tasks, ensuring conclusions are not benchmark-specific."}, "weaknesses": {"value": "1. The paper presents GeoReasoning-10K as a \"high-quality aligned dataset,\" yet it offers insufficient detail about how well the dataset covers complex geometric edge cases—coverage that is essential for evaluating true model generalization. In contrast, earlier geometric reasoning datasets such as GeoQA explicitly document their diversity in edge cases, whereas GeoReasoning-10K lacks similar transparency. To address this, the authors should publish a detailed breakdown of the dataset, categorizing samples by geometric type (e.g., 2D vs. 3D, Euclidean vs. non-Euclidean), complexity (e.g., number of components, number of inference steps required), and other relevant dimensions.\n2. The core innovation of the RLVR phase is the composite reward function $R = \\lambda_r \\cdot R_{\\text{reasoning}} + (1 - \\lambda_r) \\cdot R_{\\text{caption}}$ (with $\\lambda_r = 0.7$), but the paper fails to validate whether both components are truly necessary—for example, by testing variants that use only one of the two rewards. Moreover, the choice of $\\lambda_r = 0.7$ appears arbitrary, as the paper provides no justification or analysis for this hyperparameter setting.\n3. The RLVR Phase involves 5 rounds of \"caption generation → reward scoring → model retraining\" (each retraining epoch uses 10K samples). However, the paper does not discuss Computational efficiency. How much compute (GPU hours, memory) is required for the full RLVR pipeline?"}, "questions": {"value": "1. You report improved performance on non-geometric tasks (MMMU’s Art & Design, Tech & Engineering)—could you clarify: (1) Which subcategories of these tasks benefit most? (2) Do these tasks share structural similarities with geometric reasoning (e.g., visual feature extraction, relational deduction)?\n2. Could you conducted any probing experiments (e.g., attention visualization, feature layer analysis) to confirm that the same model capabilities are transferred as a result of training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7MIHn1lFM1", "forum": "DdKJL5wHIb", "replyto": "DdKJL5wHIb", "signatures": ["ICLR.cc/2026/Conference/Submission13527/Reviewer_183W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13527/Reviewer_183W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809445486, "cdate": 1761809445486, "tmdate": 1762924131990, "mdate": 1762924131990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the lack of high-quality geometry image-text pair datasets and further enhance the geometric reasoning capabilities of MLLMs, this paper proposes a novel geometric image description generation framework, Geo-Image-Textualization, and releases a high-quality dataset, GeoReasoning-10K. Utilizing the RLVR framework combined with the RAFT method, the authors iteratively optimize both data quality and model capability. Experiments demonstrate that the optimized model exhibits significant improvements across multiple geometric tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors propose a novel iterative \"data-model\" optimization paradigm, using RL methods to iteratively enhance both dataset and model quality.\n2. The experiments are sufficiently comprehensive. The results show that the proposed method not only improves performance on geometric problems but also demonstrates promising enhancements in other out-of-domain areas."}, "weaknesses": {"value": "Although the authors' results show great potential, there are still some limitations:\n1. GeoReasoning-10K contains only 10,000 samples, which is significantly smaller than existing synthetic datasets like AutoGeo-100K or GeoPeP-200K. Furthermore, the \"image-text alignment\" of proposed dataset is not clearly demonstrated (beyond the experimental results). It remains unclear whether the performance improvement stems primarily from data quality or the RLVR process itself; this requires further experimental validation. The authors should apply RLVR optimization to the same AutoGeo-100K or GeoPeP-200K datasets to see if the results can surpass those achieved with the proposed GeoReasoning dataset.\n\n2. The Reasoning Reward relies on an external model to judge answer correctness. The model might generate plausible but actually incorrect descriptions. If the reward model itself has biases or errors in geometric reasoning, these could be amplified; the overall robustness of RLVR needs further substantiation.\n\nIf authors can detail these questions, I will consider raise my score."}, "questions": {"value": "1. See weakness.\n2. Although optimized via RL, the dataset size is 10K, which remains relatively small compared to other datasets. Further discussion on its efficiency and scalability is needed to convincingly demonstrate the effectiveness of the authors' proposed data generation pipeline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rNTABOQeFw", "forum": "DdKJL5wHIb", "replyto": "DdKJL5wHIb", "signatures": ["ICLR.cc/2026/Conference/Submission13527/Reviewer_esbc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13527/Reviewer_esbc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897649865, "cdate": 1761897649865, "tmdate": 1762924131632, "mdate": 1762924131632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a geometric data generation pipeline with reinforcement learning with verifiable rewards (RLVR). RLVR helps generate captions of higher quality that effectively capture the key features of synthetic geometric images. Models trained on their proposed dataset achieve better performance on in-domain geometric tasks and out-of-distribution math and general tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- They propose a geometry dataset with 10,000 image-text pairs.\n- They propose an RLVR framework to refine the captions generated for the geometric dataset. \n- They not only improve performance on geometric benchmarks, but also enhance performance on standard math tasks and the MMMU task."}, "weaknesses": {"value": "- The writing is a bit confusing, and some parts could be overstated. For example, it says \"GeoReasoning-10k achieves non-trivial performance gains\" (Ln 022). How can a benchmark have performance gains, and why is the improvement \"non-trivial\"? They also said the dataset is \"fully-aligned\" (Ln 082) without statistical support.\n- Compared other datasets of 100k and 200k scales (e.g., AutoGeo-100k, GeoPeP), it only has 10k data which is much smaller. Since the data is very effective and generated by an automatic pipeline, it would be better to scale much of the other data.\n- It lacks detailed analyses. Naturally, training on 10,000 synthetic domain-specific data can not help out-of-distribution performance that much. It would be very interesting to see the reason behind it.\n- It is concerning that for the geometry tasks, training on their GeoReasoning only has marginal improvement."}, "questions": {"value": "- Among all the generated data, what is the result of human validation? How is the hallucination rate?\n- What is the computation and time used for data generation? Is it possible to upscale it to 100k? At that scale, would it still help performance on out-of-distribution benchmarks?\n- How can training on your geometric synthetic data help performance on other tasks? On what cases does it improve, and on what cases does it drop?\n- Compared to other geometric datasets, after training on georeasoning, what are the advantages and disadvantages of the geometry split?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bs0EyARGYv", "forum": "DdKJL5wHIb", "replyto": "DdKJL5wHIb", "signatures": ["ICLR.cc/2026/Conference/Submission13527/Reviewer_owqy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13527/Reviewer_owqy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954315779, "cdate": 1761954315779, "tmdate": 1762924131012, "mdate": 1762924131012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}