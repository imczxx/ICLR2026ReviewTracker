{"id": "KwvE9rSts7", "number": 6380, "cdate": 1757976122472, "mdate": 1759897918411, "content": {"title": "Hidden in Order: A Theoretical and Empirical Dissection of Positional Bias in LLMs", "abstract": "Large Language Models (LLMs) demonstrate strong capabilities but remain limited by positional bias defined as the tendency to treat content differently based on where it appears in a sequence. This bias creates subtle but consequential distortions in tasks that demand equal treatment of all inputs, such as multi-document reasoning. Although positional effects have been observed, no unified approach has existed to measure them systematically. We present Xayna, a framework of 11 complementary methods spanning information-theoretic, geometric, and probabilistic perspectives. Xayna evaluates internal representations, attention flows, and output behavior across models including GPT-4o, Gemini 2.5, Llama 3, Claude 3.7, Mixtral 8Ã—22B, and DeepSeek R1. Results show consistent patterns: (i) identical content yields divergent representations with near-zero cosine similarity; (ii) attention flows disproportionately from later to earlier tokens; and (iii) primacy dominates behavior, with position-1 preferences reaching 0.892 in Llama 3. Xayna provides both a toolkit for quantifying positional bias and a foundation for developing position-aware evaluation and mitigation strategies, advancing the pursuit of more reliable and equitable language technologies.", "tldr": "", "keywords": ["Positional Bias", "Large Language Models (LLMs)", "Representation Similarity"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a05b320dea6ec495147e31d0d7d147b3c59294b7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Xayna, a pioneering framework for systematically diagnosing and quantifying positional bias in Large Language Models. Addressing the critical need for a unified approach, the authors have developed a comprehensive toolkit of 11 complementary methods that provide a holistic view of how a model's behavior is influenced by position bias."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The evaluation is exceptionally comprehensive, approaching the problem from multiple angles, including external behavior, internal states, and mechanistic analysis. Each aspect is assessed using a variety of methods. So, the results are rich, consistent, and reliable, providing significant directional value for future research on position bias.\n\n2. While a significant body of prior work on position bias exists, it has been scattered, fragmented, unsystematic, and often lacking in quantitative analysis. This paper consolidates these disparate lines of inquiry into a unified toolkit and provides standardized quantitative metrics, which will be of great convenience to future researchers.\n\n3. The use of clean, synthetically controlled datasets effectively eliminates potential noise.\n\n4. The study evaluates a wide range of models, including both closed-source and open-source alternatives."}, "weaknesses": {"value": "1. In many evaluation methods, two similar documents are concatenated and fed into the model. This setup, however, is not representative of real-world use cases, where it is more common to combine multiple documents containing diverse information.\n\n2. The paper does not present particularly novel conclusions or discoveries about position bias. It just summarizes or confirms previous findings."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BYpotPDZRo", "forum": "KwvE9rSts7", "replyto": "KwvE9rSts7", "signatures": ["ICLR.cc/2026/Conference/Submission6380/Reviewer_5w3F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6380/Reviewer_5w3F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760766887672, "cdate": 1760766887672, "tmdate": 1762918667474, "mdate": 1762918667474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper's primary contribution is the introduction of Xayna, a novel and systematic framework designed to detect, quantify, and analyze positional bias in Large Language Models (LLMs). Addressing the lack of a unified approach to measure this bias , Xayna integrates 11 complementary methods spanning information-theoretic, geometric, and probabilistic perspectives. This framework evaluates bias across a model's internal representations, attention flows, and final output behavior , revealing that identical content can yield divergent representations and that models often exhibit a strong preference for content appearing earlier in a sequence. Ultimately, Xayna serves as both a practical toolkit for quantifying positional bias and a theoretical foundation for developing future evaluation and mitigation strategies to create more reliable and equitable LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "the first \"unified approach\" to \"measure [it] systematically\". Its originality stems not from inventing a single new technique, but from the creative and comprehensive formulation of the Xayna framework, which synthesizes 11 complementary methods from diverse fields like information theory, geometry, and Bayesian statistics into a single diagnostic pipeline.\n\nThe 11 methods are not redundant but are shown to \"capture unique information\" by analyzing bias at every level: \"behavioral,\" \"internal representation,\" and \"mechanistic\". This is supported by a high-quality, custom-generated dataset designed to isolate position as the sole variable and is robustly demonstrated across a suite of the most advanced LLMs (GPT-40, Llama 3, Claude 3.7, etc.). This complex contribution is delivered with remarkable clarity; the paper clearly defines the problem, logically structures the 11 methods of the Xayna framework."}, "weaknesses": {"value": "The experiments are confined to a custom, simplified dual-document comparison task designed to isolate position. Although this ensures high internal validity, it fails to demonstrate how these 11 bias metrics generalize to the more complex, open-ended, and \"consequential\" multi-document reasoning tasks.  For instance, the work would be strengthened by applying the Xayna framework to at least one existing mitigation technique (e.g., data shuffling, positional-aware finetuning) to demonstrate that the framework can not only detect bias but also verify its reduction, thereby closing the loop from diagnosis to treatment. Moreover, adding some baselines like PINE is useful as well because ultimately the positional bias problem depends on the use cases while PINE already mitigates the problems in multi-document retrieval QA tasks, which can serve as a strong baseline. I am not sure whether Causal Positional Intervention Analysis is a reasonable formulation because the problem is less of a causal inference one and there is no identification, yet the counterfactual is easily observable."}, "questions": {"value": "Could the authors provide any evidence or even a theoretical discussion on how they expect the Xayna framework's 11 metrics to behave in a more realistic, open-ended task?\n\nCould the authors clarify the relationships between these 11 metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U02GNeFjA4", "forum": "KwvE9rSts7", "replyto": "KwvE9rSts7", "signatures": ["ICLR.cc/2026/Conference/Submission6380/Reviewer_CFjv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6380/Reviewer_CFjv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760849358488, "cdate": 1760849358488, "tmdate": 1762918667174, "mdate": 1762918667174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problems of positional bias in LLMs. It proposes a framework named Xayna that performs 11 methods to systematically detect, quantify, and analyze how LLMs treat content differently based on its position in a sequence.\nThrough experiments on mainstream models using semantically equivalent but differently ordered document pairs, the study confirms that positional bias is a pervasive and significant phenomenon. \nBesides, Xayna provides a diagnostic toolkit for understanding and evaluating positional bias in LLMs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a systematic and comprehensive framework for analyzing positional bias. It combines 11 complementary methods to probe bias at the behavioral, internal representational, and mechanistic levels.\n2. Each of the 11 methods is distinctly defined, and the results are well-organized across a wide range of state-of-the-art LLMs (GPT-4o, Llama 3, Claude 3.7, etc.), which demonstrates the broad applicability of the framework and the pervasiveness of the problem."}, "weaknesses": {"value": "1. The framework's a prior theoretical foundation might be weak. The paper doesn't provide a clear argument for why these specific 11 metrics were chosen over countless other possibilities. It's unclear if this set is comprehensive, minimal, or simply a convenient collection.\n2. The core experimental setup relies almost exclusively on sequences of two documents of 5k words . This setup fails to adequately probe the \"lost in the middle\" phenomenon, which is one of the most widely cited and problematic forms of positional bias[1,2]. This bias manifests in longer sequences (e.g., N > 3, 128k input contexts) where middle positions are demonstrably ignored. The current findings may not generalize to these more complex, long-context scenarios.\n3. It's unclear whether Xayna's 11 metrics are practically useful. For instance, it's unknown if these metrics are sensitive enough to detect a reduction in bias after a mitigation strategy is applied. It is also unclear if different metrics would show differential improvement, which would be key to understanding how a mitigation strategy works (e.g., does it fix the behavior but not the internal representation?).\n\nRefs:\n[1] Zhang, Zhenyu, et al. \"Found in the middle: How language models use long contexts better via plug-and-play positional encoding.\" Advances in Neural Information Processing Systems 37 (2024): 60755-60775.\n\n[2] Wang, Meiyun, et al. \"Lost in the Distance: Large Language Models Struggle to Capture Long-Distance Relational Knowledge.\" Findings of the Association for Computational Linguistics: NAACL 2025. 2025."}, "questions": {"value": "1. Could you please elaborate on the a priori theoretical model of positional bias that guided the selection of these specific 11 metrics? Why is this set chosen over other possible metrics? For example, what is the theoretical justification for using Centered Kernel Alignment (CKA) for PIRA or KL Divergence for TPBD  specifically?\n2. Could you conduct additional experiments to show the positional bias effect by testing the Xayna framework on longer sequences (e.g., N=5, N=10) and placing a single target document at varying positions (first, middle, last)?\n3. Are the metrics sensitive enough to detect a reduction in bias? The authors may include an experiment where they apply a known mitigation strategy (even a simple one, like fine-tuning with permuted data) to an open-source model and use Xayna to quantify the change in bias scores."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NxngYV2oL2", "forum": "KwvE9rSts7", "replyto": "KwvE9rSts7", "signatures": ["ICLR.cc/2026/Conference/Submission6380/Reviewer_eVQr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6380/Reviewer_eVQr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761375659416, "cdate": 1761375659416, "tmdate": 1762918666785, "mdate": 1762918666785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "the paper proposes a toolkit of metrics for evaluating the position bias of LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A diverse (though not sure of comprehensiveness) set of analysis toolkit for measuring the position biases in any Transformer-based LLM.\n1. The inter-metric correlation analysis provides an interesting prespective. However, overall speaking, the metrics do not correlate with each other."}, "weaknesses": {"value": "1. \"near-zero cosine similarity\" does no necessarily mean non-equivalent representations, as they could well be different but equivalently interpreted features, as long as their differences are in the kernel of subsequent linear projections (or other criteria).  Therefore statements such as \"Lower CKA scores indicate greater dissimilarity due to position.\" is less rigorously supported.\n1. Does A' always lose critical information for answering the question? Otherwise the metrics might be over-tolerating by averaging on these cases: a difference in behavior can always be attributed to positional bias, but no difference in behavior does not necessarily mean there is no positional bias because the model could still be biased towards the first document, but A and A' just always have the same information. Therefore, the score might only serve as an upper bound of \"position unbiasedness\" (or equivalently speaking, a lower bound of position bias). We could not get an accurate estimate on how good or bad the scores are.\n1. Evaluation is only on QA which is a bit limited compared to the introduction section which lists other domains like \"summarization, or comparative analysis\". Especially, \"comparative analysis\" is an interesting domain where a positional bias is strictly unwanted."}, "questions": {"value": "1. In the abstract and intro, when listing examples for unwanted position biases, \"multi-document reasoning\" does not seem a perfect example.\n1. On the other hand, \"Positional Entropy Analysis (PEA)\" and TPBD seem a bit course-grained, is it? Looks like a sufficient but non-necessary condition for position bias. This item is not a weakness point but I am just curious.\n1. To clarify, in figure 4a, do both the variance (lengths of the horizontal bars) and the absolute value (the mean) indicate positional bias, either in a distributed or homogeneous way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TxPdNSW0SJ", "forum": "KwvE9rSts7", "replyto": "KwvE9rSts7", "signatures": ["ICLR.cc/2026/Conference/Submission6380/Reviewer_Snox"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6380/Reviewer_Snox"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6380/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762022974703, "cdate": 1762022974703, "tmdate": 1762918666332, "mdate": 1762918666332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}