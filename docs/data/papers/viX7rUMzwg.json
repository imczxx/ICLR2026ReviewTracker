{"id": "viX7rUMzwg", "number": 8785, "cdate": 1758098122484, "mdate": 1759897764092, "content": {"title": "AnyLayout: Versatile Advertising Poster Layout Generation with MLLMs", "abstract": "Layout design is a fundamental aspect of visual communication, widely used in advertising, publishing, and digital media. Recent datasets and methods, including content-agnostic and content-aware approaches, have advanced automatic layout generation, and large language models (LLMs) and multi-modal LLMs (MLLMs) have further improved performance. However, most existing methods focus on predicting bounding boxes for limited design elements on fixed backgrounds, which restricts their capability to tackle diverse instruction-driven tasks in real-world applications. To address these limitations, we introduce **AnyLayout-120K**, a large-scale instruction-driven dataset for multimodal layout generation. It offers: (1) *Task Diversity*—comprising four instruction-driven sub-tasks that encompass multimodal design elements such as multi-lingual text, visual/textual product, logos and background underlays; (2) *Rich Annotations*—including user instructions, multimodal inputs and spatial annotations; (3) *Downstream Compatibility*—where, in addition to the layout of individual elements, we propose composite layouts that capture the overall design, integrating both details and semantics. These composite layouts can be seamlessly incorporated into text-to-image (T2I) models for end-to-end generation. Alongside this dataset, we develop 7 geometry-aware evaluation metrics that assess spatial precision and adherence to design principles, ensuring a more comprehensive evaluation. Furthermore, utilizing this dataset, we establish a strong baseline based on MLLMs, achieving state-of-the-art performance. The dataset, metrics, and baseline will be released to support future research in instruction-driven layout design.", "tldr": "", "keywords": ["Multi-modal Vision", "Large Multimodal Models", "Mining of Visual", "Multimedia and Multimodal Data"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7837104f12891f1f63d34e8f7198198b1b0a0eb1.pdf", "supplementary_material": "/attachment/415404c5f8de1a7e30b419b5e6ab068d062ac9ca.pdf"}, "replies": [{"content": {"summary": {"value": "This paper focuses on 4 layout generation tasks, including solgan layout generation, textual product layout generation, visual product layout generation, and visual background layout generation. A new dataset, i.e., AnyLayout-120k, is proposed with detailed annotations. Several evaluation metrics are further adopted to ensure the layout quality assessment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed dataset construction process is a reasonable solution for improving the layout generation performance. The released dataset could be useful for future research in the community."}, "weaknesses": {"value": "1. The motivation of this paper is to create new datasets to support multiple layout generation tasks. However, it is somewhat overclaimed. Many existing works can also support different layout generation tasks in a unified model.\n2. The technical contribution of this paper is relatively limited. While the proposed dataset belongs to a large-scale one, all data comes from the combination of existing public datasets (CGL, PKU, AutoPoster, CreatiDesign). In addition, the metrics used in this paper belong to commonly used design rules. I cannot see new insights from the dataset construction and evaluation processes. \n3. The experimental evaluations are not thorough. The current evaluations only compare with two LLM-based methods, lacking detailed evaluations with closely related works, e.g., [1-5].\n\n[1] Zhang, Hui, et al. \"Creatidesign: A unified multi-conditional diffusion transformer for creative graphic design.\" arXiv preprint arXiv:2505.19114.\n\n[2] Zhang, Hui, et al. \"Creatilayout: Siamese multimodal diffusion transformer for creative layout-to-image generation.\" ICCV 2025.\n\n[3] Horita, Daichi, et al. \"Retrieval-augmented layout transformer for content-aware layout generation.\" CVPR 2024.\n\n[4] Hsu, HsiaoYuan, and Yuxin Peng. \"PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation.\" CVPR 2025.\n\n[5] Chen, Haoyu, et al. \"Posta: A go-to framework for customized artistic poster generation.\" CVPR 2025."}, "questions": {"value": "1. This paper claims that the proposed formulation is model-agnostic. How about the performance of the proposed method on different backend MLLMs?\n2. How about the details of the Similarity, IoU, and Human Check process? Is each case checked by one human reviewer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EWQKE342Qd", "forum": "viX7rUMzwg", "replyto": "viX7rUMzwg", "signatures": ["ICLR.cc/2026/Conference/Submission8785/Reviewer_F6mx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8785/Reviewer_F6mx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813027062, "cdate": 1761813027062, "tmdate": 1762920559611, "mdate": 1762920559611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors study how to map user intentions into poster layouts. The condition includes textual descriptions, product images, background images, and so on. To achieve this, they construct AnyLayout-120K, which contains pairwise (instruction, layout) samples and covers four layout generation tasks: slogan layout, textual product layout, visual product layout, visual background layout generation. In addition, they develop 7 geometry-aware evaluation metrics specifically designed for product-centered poster design. Using AnyLayout-120K, the authors establish a strong baseline using Qwen2.5-VL-7B through supervised fine-tuning, achieving state-of-the-art layout generation performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly articulates limitations of existing approaches and motivates the need for flexible, instruction-driven layout generation. By introducing novel product-centered metrics, the work provides a comprehensive evaluation on layout quality. Qualitative results across the 4 tasks demonstrate the effectiveness of AnyLayout."}, "weaknesses": {"value": "- Qualitative comparison is not included. The authors only visualize the qualitative results of AnyLayout. However, it is also necessary to show the results of baseline methods, such as PosterLlama and PosterLLaVA. \n- No discussion or comparison with recent powerful image generators. For example, GPT-4o and Gemini can also produce visually appealing posters from user instructions. Does AnyLayout perform better than these models?\n- Is it a fair comparison to evaluate PosterLlama and PosterLLaVA in zero-shot mode? To be more specific, the proposed model is trained on 4 datasets (PKU, AP, CGL, CD), while the two baseline models only see some of them. Although the authors explain the I/O format mismatch issue on line 376, it is still not convincing enough. Why not unify the I/O formats across different datasets and retrain these baselines?\n- Regarding the dataset construction, Figure 2 shows that some visual elements are not fully shaped (e.g., fruits). How do the authors address these issues, or do the authors not perform additional processing? While it is understandable (due to occlusion between elements), including such data may severely degrade the dataset quality.\n- The technical contributions of the work are relatively limited. The model is standard SFT on Qwen2.5-VL without novel designs. The main contribution is the dataset and task formulation.\n- Although the work introduces novel metrics, they are mainly geometric and rule-based. Aesthetic quality assessment of the output posters is missing (e.g., user study, VLM judge)."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f4qdEcbWvO", "forum": "viX7rUMzwg", "replyto": "viX7rUMzwg", "signatures": ["ICLR.cc/2026/Conference/Submission8785/Reviewer_p7bZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8785/Reviewer_p7bZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010626224, "cdate": 1762010626224, "tmdate": 1762920559180, "mdate": 1762920559180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AnyLayout-120K, a large-scale instruction-driven dataset for advertising poster layout generation, and proposes an MLLM-based layout generation model built upon Qwen2.5-VL. The authors define four sub-tasks (Slogan Layout, Textual Product Layout, Visual Product Layout, and Visual Background Layout)  and claim that their formulation extends layout generation from fixed-canvas bounding-box prediction to versatile, instruction-driven settings. The paper also presents seven geometry-aware evaluation metrics for assessing layout quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**1. Comprehensive dataset curation.**\n\nThe authors combine several existing datasets (PKU, CGL, AutoPoster, CreatiDesign) into a unified benchmark with 120K samples, offering a consistent structure for evaluating multimodal layout generation tasks.\n\n**2. Clear experimental organization.**\n\nThe paper provides extensive quantitative results and ablation studies across multiple baselines, including PosterLlama and PosterLLaVA, along with newly proposed metrics. The experimental section is well structured.\n\n**3. Consistency in data–metric–model pipeline.**\n\nThe data construction process, the geometry-aware metrics, and the MLLM-based model share a coherent design philosophy focused on spatial precision and controllability."}, "weaknesses": {"value": "**1. Limited novelty in task formulation.**\n\nDespite the claim of introducing a “versatile, instruction-driven” layout generation task, the actual formulation only revisits bounding-box prediction for existing subtasks (e.g., slogan placement, product alignment). The work does not go beyond layout-level reasoning or full poster synthesis.\n→ Prior works such as PosterLayout, AutoPoster, PosterLlama, and PosterLLaVA already addressed similar subtasks under more comprehensive frameworks.\n\n**2. Dataset largely repurposes existing resources.**\n\nAnyLayout-120K is mainly a recombination of existing datasets with minor instruction rewriting. It does not introduce new design domains, visual styles, or genuinely user-driven layout intentions. The dataset’s incremental contribution appears insufficient for a new benchmark paper.\n\n**3. Trivial task difficulty.**\n\nThe paper only evaluates layout-level box generation, which is far simpler than current end-to-end layout-to-image or design synthesis tasks. The task complexity is low, and the proposed evaluation metrics focus solely on geometric alignment rather than perceptual or aesthetic quality.\n\n**4. Weak modeling contribution.**\n\nThe proposed model is a fine-tuned version of Qwen2.5-VL. There are no architectural innovations  (the method essentially serializes JSON-like layout data for supervised fine-tuning). The claimed improvements are likely due to dataset size and instruction tuning rather than algorithmic novelty.\n\n**5. Outdated research direction.**\n\nWith the rapid progress of end-to-end poster generation frameworks, such as Planning and Rendering (Li et al., 2023), PosterMaker (Gao et al., CVPR 2025), and POSTA (Chen et al., CVPR 2025), modern systems can already synthesize complete poster images directly from multimodal inputs, integrating layout planning, background rendering, and text composition in a unified pipeline.\nIn contrast, this paper focuses solely on layout-level box prediction, without engaging with the visual realization or style aspects that dominate current poster generation research.\nAs a result, the proposed dataset and benchmark feel out of step with current research trends, offering limited practical value or novelty in the context of today’s diffusion- and LLM-based end-to-end design automation landscape.\n\n**6. Evaluation fairness issues.**\n\nCompeting baselines (PosterLlama, PosterLLaVA) are evaluated in zero-shot mode, while the proposed model is fine-tuned on the target dataset, which makes the reported gains difficult to interpret as genuine superiority."}, "questions": {"value": "1. How do you justify the need for a layout-only benchmark when most recent poster generation works handle end-to-end image synthesis including layout, texture, and style?\n\n2. Could you elaborate on how your “instruction-driven” component goes beyond simply adding textual prompts? Are there examples of compositional or multi-turn reasoning?\n\n3. Have you evaluated whether the proposed geometry-aware metrics correlate with human aesthetic judgments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cKRmU6aAjH", "forum": "viX7rUMzwg", "replyto": "viX7rUMzwg", "signatures": ["ICLR.cc/2026/Conference/Submission8785/Reviewer_bAPd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8785/Reviewer_bAPd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153988271, "cdate": 1762153988271, "tmdate": 1762920558629, "mdate": 1762920558629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AnyLayout-120K, a large-scale instruction-driven dataset for multimodal advertising poster layout generation, along with 7 geometry-aware evaluation metrics and a unified baseline model based on multi-modal large language models (MLLMs)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes the AnyLayout-120K Dataset, a high-quality, annotated, large-scale dataset that covers multiple layout generation tasks and is beneficial to the future development of the layout generation field. \n2. This paper introduces 7 geometry-aware metrics to assess spatial accuracy and compliance with design principles from dimensions such as product centrality, size ratio, and occlusion. They are more comprehensive than traditional metrics. \n3. This paper establishes a baseline model that generates layouts in natural language-formatted composite sequences. It supports four sub-tasks, leverages cross-task synergies via multi-task training, and achieves state-of-the-art performance."}, "weaknesses": {"value": "The workload of this article is evident, and the dataset and metrics make significant contributions to the future development of the field. However, the method for constructing the baseline seems overly simple and intuitive, failing to propose obvious improvements related to the characteristics of the poster layout generation task."}, "questions": {"value": "1. It is recommended to supplement subjective evaluation results of layouts generated by this method and comparative methods for more comprehensive validation. \n2. It is suggested to further elaborate on both the methodological innovations and limitations of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a57tfcJH8o", "forum": "viX7rUMzwg", "replyto": "viX7rUMzwg", "signatures": ["ICLR.cc/2026/Conference/Submission8785/Reviewer_crb6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8785/Reviewer_crb6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8785/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762258117641, "cdate": 1762258117641, "tmdate": 1762920558293, "mdate": 1762920558293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}