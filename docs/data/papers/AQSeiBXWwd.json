{"id": "AQSeiBXWwd", "number": 20182, "cdate": 1758303362515, "mdate": 1759896993605, "content": {"title": "Diverging Flows: Detecting Out-of-Distribution Inputs in Conditional Generation", "abstract": "Flow matching models are able to learn complex conditional distributions from data. Nevertheless, they do not model the distribution of the conditioning itself, which means they can confidently generate samples from conditioning inputs that are not in the training distribution. In this work, we introduce _Diverging Flows_, an approach to train flow matching models that enables a single model to detect OOD conditions, without hindering its generative capabilities. _Diverging Flows_ augments standard flow matching training with a contrastive objective that learns to separate the velocity fields produced by in- and out-of-distribution conditions, effectively modeling the conditions' distribution, and practically enforcing an effective telltale sign during the generation process. At inference time, we combine this signal with conformal prediction to obtain statistically valid OOD decisions. Additionally, _Diverging Flows_ does not require real OOD data, enabling fully self-contained training on the target domain. The results indicate that _Diverging Flows_ is competitive with other OOD detection methods while preserving the predictive quality of the underlying flow model. Ultimately, these results pave the way in adopting generative models as safe and robust predictors in high-stakes domains like weather forecasting, robotics, and medical applications.", "tldr": "", "keywords": ["flow matching", "out-of-distribution detection"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0d0c0a23e80f9284d37825d5c5294e165d3337ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Diverging Flows, a flow-matching approach for conditional generation that can detect OOD conditions. The method augments flow matching with a contrastive objective, ensuring that predicted velocity fields for ID and OOD conditions are separated during training. Experiments include a 2D toy task, image reconstruction OOD benchmarks, and a weather-forecasting setting. Results suggest high OOD detection performance while preserving or slightly improving reconstruction quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The direction of calibrating uncertainty in generative models is important for making conditional generation safer and more reliable.\n\nS2. The technical components are clearly described and easy to follow.\n\nS3. The toy example in Section 5.1 clarifies the core concept and how divergence manifests along trajectories.\n\nS4. The paper candidly reports limitations of the proposed approach."}, "weaknesses": {"value": "W1. The problem setting feels somewhat removed from common practical goals. In many applications, diffusion or flow models are expected to generalize to novel conditions like \"an astronaut riding a horse on mars\"; this work instead trains a generator that intentionally fails under OOD conditions to flag them. The paper would benefit from concrete scenarios where this behavior is clearly advantageous.\n\nW2. Prior work, DiffPath, has already used diffusion or flow trajectories for OOD detection. This paper can be read as an extension of that idea to flow matching with conditional generation; the incremental novelty feels modest.\n\nW3. Some comparisons appear potentially unfair. In Table 1, the DiffPath baseline uses a single model trained on CelebA for RGB experiments, while the proposed method is trained separately on each dataset."}, "questions": {"value": "Q1. Why does the triplet-style component in your loss also improve reconstruction quality? An intuitive explanation would help.\n\nQ2. Can you offer any theoretical guarantee or clear conditions under which the contrastive training should improve OOD detection without hurting generation?\n\nQ3. The introduction motivates safety-critical settings such as weather forecasting and robotics. Under OOD inputs, however, the model is designed to produce diverging flows; what prevents these predictions from being harmful in practice, and how is risk actually mitigated at deployment time?\n\nQ4. For Table 2, are reconstruction errors computed on a held-out test set that was not seen during training?\n\nQ5. Compared with DiffPath, what are the pros and cons in terms of the practical usage (e.g. runtime)?\n\nQ6. In Table 1, would it be possible to compare against DiffPath models trained individually on each dataset to ensure a fairer baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C0hVzeeaOO", "forum": "AQSeiBXWwd", "replyto": "AQSeiBXWwd", "signatures": ["ICLR.cc/2026/Conference/Submission20182/Reviewer_aXV4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20182/Reviewer_aXV4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761270889387, "cdate": 1761270889387, "tmdate": 1762933692436, "mdate": 1762933692436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposed a new method based on (conditional) flow matching to detect out of distribution (OOD) data, using only in-distribution (ID) training data. The main idea is to use the variance of the vector field at multiple time steps (learned through a contrastive variant of flow matching) as a scoring function. Experiments on standard image benchmarks and a weather forecasting dataset seem to confirm the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- a new OOD detection algorithm based on (conditional) flow matching and contrastive learning\n\n- experiment comparison against multiple diffusion based OOD detection baselines on standard image benchmarks"}, "weaknesses": {"value": "- unclear how the context for in-distribution data is generated, on both training and test sets. \n\n- problem setup is questionable: why insisting on training only on in-distribution data? Anyone who is serious about OOD should at least try to collect a small amount of OOD data, for otherwise we are doing ourselves a disservice and making the problem unnecessarily underdefined and challenging. I recognize many prior works followed the same setup and I am curious to hear the authors' reasons. For some theoretical discussions on this problem setup, see e.g. https://openreview.net/forum?id=sde_7ZzGXOE and https://proceedings.mlr.press/v139/zhang21g.html.\n\n- heavier computational cost for both training and inference: for training, one needs to use a bigger network to account for the context, while for testing, multiple time steps need to be simulated and averaged. Can the authors comment on scalability and training and test time comparisons? Ideally, one should compare to the unconditional flow matching model on the amount of in-distribution data needed to achieve certain performance (e.g., the assumption on Line 052)."}, "questions": {"value": "My main concern is that the authors did not describe how the context c during training and testing is generated and how its choice (including dimension) affects the experimental results. This is a crucial missing piece that will affect my final evaluation. \n\nI also suggest running this ablation study: during testing, what happens if we disable the context (e.g., provide the same context for all test samples)? Can your method still achieve good performance? In other words, is the context actually doing the heavy lifting?\n\nIt is surprising that in Table 1, DF+FGSM achieved AUROC 1 on multiple datasets, implying that those pairs of datasets have no overlap of support at all. If this is the case, many existing generative methods (including those based on likelihood) should perform well too. To the contrary, FM likelihood performed very poorly. Do the authors have any explanation of this observation? How is FM likelihood trained and tested? \n\nIn the second paragraph of Introduction, the authors made a big deal on quantifying uncertainty, but in the end the authors merely employed (split) conformal prediction to address this issue. This is a bit disappointing since the same (split) conformal prediction could literally be applied to any existing OOD detection method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vx1B64Idyw", "forum": "AQSeiBXWwd", "replyto": "AQSeiBXWwd", "signatures": ["ICLR.cc/2026/Conference/Submission20182/Reviewer_jvGi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20182/Reviewer_jvGi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761439049486, "cdate": 1761439049486, "tmdate": 1762933692065, "mdate": 1762933692065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a framework which simultaneously generates conditional samples and also determine if the conditioning is out of distribution. They do this by creating Gaussian noise or adversarial perturbations of real samples and use a contrastive loss to push OOD conditioned samples away from in-distribution samples. At test time, the instability in the curve of the predicted velocity field grants a Flow Smoothness Score metric which, once surpassing a certain threshold which is obtained through conformal prediction, allows the model to flag whether the sample is in-distribution or OOD. They evaluate on a few standard image datasets which contrast against one another."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\nTheir approach allows them to train a single model for OOD detection without specific OOD data needed within the training procedure.\n\nThey show surprisingly strong results in terms of AUROC (see weaknesses section for a caveat here).\n\nGeneration quality seems to not be impacted by the inclusion of the OOD detection algorithm."}, "weaknesses": {"value": "Weaknesses:\nThe AUROC numbers reported are extremely close to 1 on all the benchmarks. This might be due to the datasets being completely different when trying to direct OOD samples, whereas in the real world the application may not be as clean. In particular, the MNIST vs KMNIST case shows a severe drop in performance which is irregular for a model which should otherwise be performing very well across everything else. This might indicate that the model is not actually robust to OOD detection as claimed.\n\nSome ablation study results regarding the inclusion of the dual-conditioning portion vs the contrastive loss could be helpful in determining the specific contributions of the paper.\n\nDatasets included are somewhat weak; the weather forecasting is done for single step only. Current weather forecasting experiments in particular focus on much longer time horizons and multi-step prediction.\n\nThe authors mention in the limitations that the training with the contrastive loss can be unstable, but fail to mention how heavy the impact of the instability is, which can make applying this model particularly hard."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4JviAAbPgG", "forum": "AQSeiBXWwd", "replyto": "AQSeiBXWwd", "signatures": ["ICLR.cc/2026/Conference/Submission20182/Reviewer_CdUW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20182/Reviewer_CdUW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872073116, "cdate": 1761872073116, "tmdate": 1762933691727, "mdate": 1762933691727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to detect out-of-distribtuons (OOD) conditions in a conditional generative model (here, in the flow matching framework).\n\nThe proposed method, called Diverging Flows, seeks to identify OOD conditionings without relying on the model’s output, but instead by computing scores directly from the velocity fields of the flow matching model.\nIt relies on *contrastive learning*, i.e. it adapts the standard flow matching loss by adding 2 regularization terms.\nThese two terms aim at pulling apart positive conditionings from negative ones. \n\nThe paper suggests two strategies to build negative samples: either pure noise or adversarial samples. Thus, it does not rely on an extra OOD dataset.\nThe training objective encourages the model to produce similar instantaneous velocity fields for in-distribution (ID) conditionings and divergent velocity fields for OOD conditionings.\n\nThe OOD detection then relies on an ad-hoc score that measures the variation of the velocity field along the ODE trajectory: the more it varies, the less likely the conditioning is ID."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The OOD detection problem is interesting.\n- The presentation is clear, making the paper easy to follow.\n- The method shows empirical effectiveness, and it is applied to real data (weather forecasting)."}, "weaknesses": {"value": "**On the OOD Score**\nIn Section 3.2, I think there is a confusion between:\n- what happens during training where one regresses against the conditional velocity field $u^\\mathrm{cond} = x_1 -x_0$ on interpolated $x_t$ \n- what happens during inference, where the goal is to follow the total velocity field which is defined as $u_t = \\mathbb E[u^\\mathrm{cond}  | x_t]$.\n\nThe sentence ''at each step the model should strive to follow this straight trajectory'' is confusing: at inference, the trajectories have no reason to be straight (see e.g. Multisample Flow Matching: Straightening Flows with Minibatch Couplings, Pooladian et al. or Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow, Lie et al. for a discussion on straightness).\nAs the OOD score is based on a straightness hypothesis; this should be clarified in the paper.\n\n**On conformal prediction** The authors use the term conformal prediction at several places in the paper to explain that their method can be used on ''safety-critical'' usages and that it is ''statistically valid''.\nFrom my understanding, the threshold on the scores (that decides if the data is ID or OOD) is set as a quantile on a calibration set made of ID data.\nI think this does not bring any guarantee on what happens for OOD data.\nBesides, as it is presented as an important asset of the method, I think this part deserves more details both regarding the theory (do we have some guarantess? if yes, they should be described) and in practice (e.g. in the experiments, what is the size of the calibration set?).\n\n**On generation performance** The claim that the modified loss “does not degrade the predictive performance of the underlying FM model” could be more deeply evaluated. For example, Table 2 reports a reconstruction error, but as this is a generative model, computing the FID is also an important metric to assess the quality of generated samples."}, "questions": {"value": "1. In the experiment made Section 5.2, ID conditionings are exactly samples from the training data. \nAs stated in the paper, the method is somehow underemployed in this setting (it's rather used as a first benchmark against other baselines). Yet, it would be interesting to add tasks such as image restoration or style transfer (where the degraded images / the styles are the conditionings), it would be a more relevant demonstration of the method's utility.\n\n2. Does the proposed loss really fit into the standard contrastive learning framework ?\nUsually, positive samples are generated by applying well-chosen transformations to true samples, while negative samples correspond to other samples from the train set. Here, there is no positive samples (just the baseline sample) and the negative samples are new conditionings. I think this distinction deserves some clarification.\n\n3. For the weather forecasting experiments, are the same hyperparameters and architecture used as in the RGB image experiments? What are the sizes of the training, calibration, and test sets? This information should be provided in the appendix.\n\n**Minor comments**\n\n- The introduction to Flow Matching (FM) could be slightly clarified: $u_t^\\mathrm{cond}= x_1-x_0$ is the conditional velocity field  (conditonned on $(x_0,x_1)$) and what we actually aim to learn is $u_t = \\mathbb E[u^\\mathrm{cond}  | x_t]$.\n\n- On the beginning of section 3, introducing conditional generation. The fact that it is enough to train a velocity field $u_\\theta(x, c)$ to generate $q(x|c)$ using the standard FM loss comes from the hypothesis made in Equation (4): the underlying assumption is that $p_t(x_t|x_1)$ is independant of $c$ conditionnaly on $x_1$, which leads to Equation (3). As it is now stated (with Eq (3) that appears before Eq (4)), it is not so clear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P0PCSNnI4I", "forum": "AQSeiBXWwd", "replyto": "AQSeiBXWwd", "signatures": ["ICLR.cc/2026/Conference/Submission20182/Reviewer_DmnZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20182/Reviewer_DmnZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20182/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921163837, "cdate": 1761921163837, "tmdate": 1762933691288, "mdate": 1762933691288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}