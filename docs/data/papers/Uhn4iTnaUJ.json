{"id": "Uhn4iTnaUJ", "number": 10524, "cdate": 1758174612978, "mdate": 1763010069591, "content": {"title": "Actor-Critic without Actor", "abstract": "Actor-critic methods constitute a central paradigm in reinforcement learning (RL), coupling policy evaluation with policy improvement. While effective across many domains, these methods rely on separate actor and critic networks, which makes training vulnerable to architectural decisions and hyperparameter tuning. Such complexity limits their scalability in settings that require large function approximators. Recently, diffusion models have recently been proposed as expressive policies that capture multi-modal behaviors and improve exploration, but they introduce additional design choices and computational burdens, hindering efficient deployment. We introduce Actor-Critic without Actor (ACA), a lightweight framework that eliminates the explicit actor network and instead generates actions directly from the gradient field of a noise-level critic. This design removes the algorithmic and computational overhead of actor training while keeping policy improvement tightly aligned with the critic’s latest value estimates. Moreover, ACA retains the ability to capture diverse, multi-modal behaviors without relying on diffusion-based actors, combining simplicity with expressiveness. Through extensive experiments on standard online RL benchmarks, ACA achieves more favorable learning curves and competitive performance compared to both standard actor-critic and state-of-the-art diffusion-based methods, providing a simple yet powerful solution for online RL.", "tldr": "We introduce Actor-Critic without Actor (ACA), a lightweight framework that eliminates the explicit actor network and generates actions directly from the gradient field of a noise-level critic.", "keywords": ["online reinforcement learning", "diffusion models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f802b6d19d1e94bd11c1daf45a42382c68e89ffa.pdf", "supplementary_material": "/attachment/11b962fc2352aa418ba31864d14b135f6faeb521.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents an Actor-Critic without Actor, a lightweight framework that removes the explicit actor network and directly generates actions from the gradient field of a noise-level critic."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes an actor-critic algorithm without an actor and demonstrates its effectiveness by comparing it with several baselines across multiple tasks."}, "weaknesses": {"value": "1. The paper lacks theoretical proofs for the proposed method, including: 1) Convergence analysis of the proposed method, and 2) Why the proposed method can outperform existing actor-critic methods and diffusion-based DRL methods.\n2. How does the proposed method perform compared to actor-less DRL methods, such as the DQN series, on Atari tasks?\n3. From Figures 5 and 6, the returns of the proposed method on Ant-v4, Walker2d-v4, Humanoid-v4, Swimmer-v4, Pusher-v4, and HalfCheetah-medium-v2 decrease below the baseline as learning progresses, showing no clear advantage.\n4. The method should be compared with more recent SOTA actor-critic methods, including at least three standard actor-critic methods published in the last two years, besides SAC.\n5. The diffusion-based DRL baseline is incomplete and should be compared with more baselines, such as \"Ren A Z, Lidard J, Ankile L L, et al. Diffusion Policy Policy Optimization\" (arXiv preprint arXiv:2409.00588, 2024).\n6. Offline-to-online DRL needs validation on more tasks, such as Hopper, Ant, and Walker2d.\n7. Besides comparing parameter counts, it is also important to compare actual hardware efficiency, such as wall-clock throughput, FLOPs, and VRAM usage, with more SOTA standard actor-critic algorithms and diffusion-based DRL methods."}, "questions": {"value": "1. The paper lacks theoretical proofs for the proposed method, including: 1) Convergence analysis of the proposed method, and 2) Why the proposed method can outperform existing actor-critic methods and diffusion-based DRL methods.\n2. How does the proposed method perform compared to actor-less DRL methods, such as the DQN series, on Atari tasks?\n3. From Figures 5 and 6, the returns of the proposed method on Ant-v4, Walker2d-v4, Humanoid-v4, Swimmer-v4, Pusher-v4, and HalfCheetah-medium-v2 decrease below the baseline as learning progresses, showing no clear advantage.\n4. The method should be compared with more recent SOTA actor-critic methods, including at least three standard actor-critic methods published in the last two years, besides SAC.\n5. The diffusion-based DRL baseline is incomplete and should be compared with more baselines, such as \"Ren A Z, Lidard J, Ankile L L, et al. Diffusion Policy Policy Optimization\" (arXiv preprint arXiv:2409.00588, 2024).\n6. Offline-to-online DRL needs validation on more tasks, such as Hopper, Ant, and Walker2d.\n7. Besides comparing parameter counts, it is also important to compare actual hardware efficiency, such as wall-clock throughput, FLOPs, and VRAM usage, with more SOTA standard actor-critic algorithms and diffusion-based DRL methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ozSHaiDpZc", "forum": "Uhn4iTnaUJ", "replyto": "Uhn4iTnaUJ", "signatures": ["ICLR.cc/2026/Conference/Submission10524/Reviewer_EhSP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10524/Reviewer_EhSP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761534112186, "cdate": 1761534112186, "tmdate": 1762921803897, "mdate": 1762921803897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We sincerely appreciate the reviewers’ thoughtful and constructive feedback. Because the experiment was conducted under a tight schedule, we made an error that affected part of our paper. For this reason, we have decided to withdraw the submission.\n\nWe will carefully incorporate the reviewers’ comments, address the identified issue, and prepare a more improved and reliable version of the paper. We sincerely appreciate your time and understanding."}}, "id": "HLaO3KhJUC", "forum": "Uhn4iTnaUJ", "replyto": "Uhn4iTnaUJ", "signatures": ["ICLR.cc/2026/Conference/Submission10524/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10524/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763010068858, "cdate": 1763010068858, "tmdate": 1763010068858, "mdate": 1763010068858, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel reinforcement learning (RL) framework called Actor-Critic without Actor (ACA). Traditional Actor-Critic (AC) methods rely on two separate networks—an actor and a critic. This separate design complicates training, is sensitive to hyperparameters, and increases computational and memory requirements, limiting its scalability. The ACA framework addresses these problems by completely removing the explicit actor network. Instead, it employs a \"critic-guided denoising process\": actions are generated by directly sampling from the gradient field of a \"noise-level critic.\""}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper eliminates the algorithmic and computational overhead of actor training and requires significantly fewer parameters than standard AC methods and diffusion-based methods.\n\nACA ensures that policy improvements (action generation) are always closely aligned with the SOTA value estimate of the critic, eliminating the \"policy lag\" found in traditional methods.\n\nThis framework retains the ability to capture diverse, multimodal behaviors (similar to diffusion models) without requiring a separate, computationally expensive actor network."}, "weaknesses": {"value": "1.The method appears to share a significant conceptual foundation with existing work, particularly Q-Score Matching (QSM). As established, QSM's core idea is to train a diffusion model (the policy $\\epsilon_{\\theta}$) to match the Q-function's gradient field ($\\nabla Q$). In contrast, the ACA method seems to primarily omit the gradient-matching step. Instead of training a separate $\\epsilon_{\\theta}$ to approximate this field, ACA directly utilizes the raw gradient of the critic, $\\nabla_{a_t} Q_{\\phi}(s, a_t, t)$, as the noise prediction $\\hat{\\epsilon}$. Given this, ACA appears to function less as a novel algorithmic paradigm and more as a simplification or a direct ablation of QSM.\n\n2.The authors' repeated claims that the ACA framework is \"lightweight\"  and \"parameter-efficient\"  are highly misleading. The primary justification for this claim is the removal of the actor network, which reduces the total parameter count. This argument, however, ignores a critical trade-off: the method merely transfers the computational overhead from parameter storage (the actor network) to sampling latency (the iterative denoising process). In practical reinforcement learning applications, particularly in real-time control, inference latency (wall-clock time) is often a more severe bottleneck than the parameter count.\n\n3.The entire theoretical foundation of the ACA framework, as articulated in Sec 3.1, rests upon a critical assumption: that its \"critic-guided denoising process\" (Definition 1)  can effectively sample from the target Boltzmann distribution $\\hat{\\pi} _ t \\propto \\exp(wQ _ {\\phi})$ .However, the paper conspicuously fails to provide any theoretical guarantees regarding the convergence of this sampling process. The policy $\\pi _ Q$ is an implicit policy, defined entirely by this sampler. It is well-established that such samplers, whether MCMC or diffusion-based, face significant challenges when sampling from complex, non-convex energy landscapes, which the Q-function $Q _ {\\phi}$ represents. It is highly questionable whether the use of a finite and fixed number of steps, such as $T=20$, is sufficient to ensure the sampler converges to the stationary distribution. The authors should have provided an analysis to justify this choice, perhaps referencing empirical investigations similar to those in Fig 2 of the SDAC paper.\n\n4.Considering the inference time, the non-converged performance on MuJoCo is insufficient to support the authors' SOTA claims. I need to see the performance metrics after training 3M iterations for each algorithm on different tasks until convergence.\n\n5.This study lacks experimental comparison with the recent SOTA DIME, and I'd like to know the relationship between the Q-gradient field guidance proposed in the DACERv2 paper and yours? They seem very similar.\n\n6.Could the author provide results comparing the time taken for single-step inference?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8SvmwfXQZr", "forum": "Uhn4iTnaUJ", "replyto": "Uhn4iTnaUJ", "signatures": ["ICLR.cc/2026/Conference/Submission10524/Reviewer_GtS2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10524/Reviewer_GtS2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826792225, "cdate": 1761826792225, "tmdate": 1762921803594, "mdate": 1762921803594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new reinforcement learning framework, Actor-Critic without Actor (ACA). ACA eliminates the explicit actor network; instead, actions are generated directly from the gradient field of a noise-level critic, effectively transforming the critic into both a value estimator and an implicit policy generator. Inspired by diffusion model guidance, ACA formulates action sampling as a denoising process guided by $ \\nabla_a Q(s, a, t) $. The critic is optimized with a two-term objective: (i) fitting terminal ($t=0$) temporal-difference targets, and (ii) regressing noisy-step values toward their denoised counterparts, thereby learning a noise-aware gradient field for action generation. Empirically, ACA demonstrates strong multi-modal coverage in a synthetic 2D bandit task and achieves competitive, sample-efficient performance across various MuJoCo benchmarks, while using significantly fewer parameters than diffusion-based actor–critic baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of removing the actor is conceptually elegant and effectively reduces both parameter and hyperparameter complexity. Training a critic that provides value estimates across different noise levels offers a principled way to stabilize the gradients used during denoising. This design also eliminates the “policy lag,” ensuring that the generated actions immediately reflect the most recent critic updates. Moreover, ACA naturally induces high-entropy, multi-modal behaviors without requiring explicit entropy regularization."}, "weaknesses": {"value": "1.\tWhile ACA motivates the Boltzmann-form policy $ \\hat{\\pi} \\propto \\exp(w Q) $ and draws intuition from classifier and diffusion guidance, the intermediate energy used in the framework serves as an approximate estimator. The paper lacks a quantitative analysis of the accuracy of this intermediate energy approximation, which is crucial for validating ACA, since implicit policy sampling directly relies on the gradient field of the noise-level critic. If the noise-level critic is not accurately learned, the resulting actions derived from its gradients may be unreliable.\n2.\tThe connection between the trained noise-level critic, the denoising-based sampling dynamics, and overall policy improvement remains informal. The paper provides no convergence or consistency analysis for the full training loop.\n3.\tSince the actions used for Bellman backups are themselves generated by the critic through denoising, a feedback loop is formed. Although the paper introduces gradient normalization and target networks to mitigate instability, it does not analyze under what conditions this loop converges or fails.\n4.\tIterative denoising in ACA is computationally more expensive than the single forward pass of an amortized actor. While the paper reports parameter savings, it only partially quantifies wall-clock training cost or action-generation throughput. The multi-step denoising process may hinder real-time deployment and large-batch training efficiency."}, "questions": {"value": "1.\tDoes the critic-guided denoising policy $\\pi_Q$ converge to the optimal policy under certain assumptions?\n2.\tGuidance weight (w) and diffusion steps (T) require manual tuning. Is there any adaptive scheduling ?\n3.\tCan ACA be accelerated with fewer denoising steps or learned step-size schedules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "msr5ShTitR", "forum": "Uhn4iTnaUJ", "replyto": "Uhn4iTnaUJ", "signatures": ["ICLR.cc/2026/Conference/Submission10524/Reviewer_VFZK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10524/Reviewer_VFZK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10524/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926538610, "cdate": 1761926538610, "tmdate": 1762921803068, "mdate": 1762921803068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}