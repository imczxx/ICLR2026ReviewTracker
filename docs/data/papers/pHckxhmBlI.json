{"id": "pHckxhmBlI", "number": 20965, "cdate": 1758312104787, "mdate": 1759896949474, "content": {"title": "Neyman-Pearson Classification under Both Null and Alternative Distributions Shift", "abstract": "We consider the problem of transfer learning in Neyman–Pearson classification, where the objective is to minimize the error w.r.t. a distribution $\\mu_1$, subject to the constraint that the error w.r.t. a distribution $\\mu_0$ remains below a prescribed threshold. While transfer learning has been extensively studied in traditional classification, transfer learning in imbalanced classification such as Neyman–Pearson classification has received much less attention. This setting poses unique challenges, as both types of errors must be simultaneously controlled. Existing works address only the case of distribution shift in $\\mu_1$, whereas in many practical scenarios shifts may occur in both $\\mu_0$ and $\\mu_1$. We derive an adaptive procedure that not only guarantees improved Type-I and Type-II errors when the source is informative, but also automatically adapt to situations where the source is uninformative, thereby avoiding negative transfer. In addition to such statistical guarantees, the procedures is efficient, as shown via complementary computational guarantees.", "tldr": "", "keywords": ["Imbalanced classification", "Transfer Learning", "Neyman-Pearson Classification."], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac2f2f1003f52206a8836e62f39cf414707ccd12.pdf", "supplementary_material": "/attachment/9a494ce81c3fef39366c289ea4a5d701436d0820.zip"}, "replies": [{"content": {"summary": {"value": "This paper provides an algorithm, to perform NP classification under distribution in the case of transfer learning where for both class 0 and 1 data there is a small amount of target data from our distribution of interest and a larger amount of source data from an unknown but potentially similar distribution. The NP classifier is shown to satisfy the Type I error constraints with high probability up to some error term $\\epsilon$ (while at least matching the error bound given by just using Type I error) as well at least matching the Type II error given by a classifier just produced on the target data and improving upon it when the source and target are similar.\n\nAt a high level this procedure works by finding optimal classifiers on both the source and target data (with a relaxation of the Type I error on the source data to ensure some intersection exists) then takes the final classifier to be the intersection of the near optimal classifiers on both source and target data within this set (if such a classifier exists.)"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This work effectively extends the work of Kalan \\& Kpotufe (2024); Kalan et al. (2025) to the case of distribution shift on the class 0 data. Furthermore the theoretical results illustrate meaningful improvement from the inclusion of source the data from both classes provided the transfer moduli is sufficiently small.\n\n* Strong theoretical results are provided both for the optimal classifier under the schema as well as a classifier given by a practical algorithm solving the optimisation problem.\n\n* The relative error induced by the difference in source and target distributions is nicely related to the transfer exponent, a pre-existing measure of distribution shift.\n\n* Experimental results on real world data illustrate improvement when the source and target data overlap and minimal downside when the target and source data do not."}, "weaknesses": {"value": "* The additional error on the target Type I error $\\epsilon_{0,T}$ depends upon the Rademacher complexity $B_{\\mathcal{H}}$ a quantity which while bounded for may function classes is not often practically known. This makes it difficult to use these results to give meaningful finite sample, high probability bounds on the Type I error for a specific case. This is in contrast to works such as Tong (2013) and Tong et al. (2018) which use a sample splitting procedure to project the problem down to learning the threshold of a score function, effectively making $\\mathcal{H}$ have VC dimension 1.\n\n* The paper lacks a proper conclusion or discussion of future directions/limitations of the work which I feel would enhance the paper, especially is it is a very dense technical work.\n\n* The experimental results do not illustrate a case where improper handling of the source data harms Type I error, in both cases the lowest Type I error (theoretically the value we are most keen to control) are mostly given by exclusively working with the source data.\n\n* While I appreciate that it is just a very technical work. I feel it could be improved by trying to streamline notation. E.g. later in the paper, the sets $\\mathcal{H}', \\mathcal{H}'_1, \\mathcal{H}'_0$ are re-written as constraints on the functions $g$. Could these functions instead be introduced earlier therefore requiring fewer subsets of $\\mathcal{H}$ to be defined?\n\n* Some small errata (I think):\n    * Line 343 -- Definition of $\\hat{\\theta}_{T,\\alpha}$ not defined and I believe it should be $-6\\epsilon_{1,T}$ is the definition of $g_{1,T}$. \n    * Line 361 -- Typo, ``uniformly'' written twice.\n    * Line 356 -- Slackness should be $\\xi$? $\\delta$ not used and neither is $\\epsilon$ (though by CP-Solver's later use $\\epsilon$ should appear.)\n    * Line 420 -- Technically, (12) only finds $\\hat{h}$ when $\\mathcal{H}'_1\\cap\\mathcal{H}'_0\\neq\\varnothing$ and I feel this would be clearer if it was clarified (I appreciate that this is clarified in the algorithm but would benefit from clarification here as well.)\n\n## References\nXin Tong. A plug-in approach to neyman-pearson classification. *The Journal of Machine Learning\nResearch*, 14(1):3011–3040, 2013.\n\nXin Tong, Yang Feng, and Jingyi Jessica Li. “Neyman-Pearson Classification\nAlgorithms and NP Receiver Operating Characteristics”. In: *Science Advances 4.2*\n(Feb. 2, 2018),"}, "questions": {"value": "Can this be extended to the case where some relationship between the target and source distributions are known and can be leveraged for example the density ratio or some other relationship?\n\nAre there illustrative examples of how the transfer modulus or transfer exponent vary as the source and target distributions differ from one another?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vkFyNPCglH", "forum": "pHckxhmBlI", "replyto": "pHckxhmBlI", "signatures": ["ICLR.cc/2026/Conference/Submission20965/Reviewer_dxrU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20965/Reviewer_dxrU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760714112958, "cdate": 1760714112958, "tmdate": 1762939062695, "mdate": 1762939062695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Abstract: This paper investigates transfer learning within the Neyman–Pearson (NP) classification framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "great theoretical results"}, "weaknesses": {"value": "**Weaknesses.**\nThe paper does not adequately discuss its limitations. In particular, the theoretical generalization analysis appears to rely on boundedness assumptions on the loss or surrogate loss, which is a common but restrictive condition.\n\nMoreover, there are alternative transfer learning settings—such as fine-tuning and related approaches discussed in [1] and [2]—that are not compared against the proposed setup. Including such a comparison would clarify where the NP transfer setting stands relative to more standard transfer pipelines.\n\nFinally, the motivation for studying the Neyman–Pearson formulation in the transfer learning context is not fully articulated. It would help to explain why NP constraints are especially relevant here, and to describe how competing approaches (e.g. $\\alpha$-ERM or fine-tuning) would perform or be adapted in this setting.\n\n[1] Aminian, Gholamali, Łukasz Szpruch, and Samuel N. Cohen. “Understanding Transfer Learning via Mean-field Analysis.” arXiv:2410.17128 (2024).\n[2] Bu, Y., Aminian, G., Toni, L., Wornell, G. W., & Rodrigues, M. (2022). “Characterizing and understanding the generalization error of transfer learning with Gibbs algorithm.” AISTATS 2022, pp. 8673–8699."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qqN2B681WL", "forum": "pHckxhmBlI", "replyto": "pHckxhmBlI", "signatures": ["ICLR.cc/2026/Conference/Submission20965/Reviewer_r8Y4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20965/Reviewer_r8Y4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860656987, "cdate": 1761860656987, "tmdate": 1762939061591, "mdate": 1762939061591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study Neyman-Pearson classification in the context of transfer learning, where both source and target domains provide positive and negative samples. The objective is to minimize the false negative (Type-II) rate while ensuring the false positive (Type-I) rate remains below a specified threshold on the target distribution. While prior work has mainly focused on the case where the negative distributions are identical between source and target, this paper explores the more general scenario where they differ. The primary contribution is an adaptive error bound on the target false negative rate, which interpolates between using only target samples and leveraging additional source samples. The authors further propose a computational algorithm with matching error guarantees, assuming convexity for both the loss function and classifier class. Experimental results validate the algorithm's adaptive control of the Type-II error while adhering to the target Type-I error constraint."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper addresses Neyman-Pearson classification within the transfer learning framework, which is highly relevant to the conference. The Neyman-Pearson classification setup is practical due to its importance in high-stakes real-world applications, and transfer learning is motivated by the scarcity of labeled samples in the target domain.\n\nThe authors present an algorithm with error bound guarantees that avoids negative transfer, even when both the negative and positive source and target distributions differ. As previous works are limited to the setting where the negative source and target distributions are identical, this represents a theoretical contribution to a more general scenario.\n\nThe resulting error bound is characterized by a newly proposed quantity assessing the transferability of source samples, namely, the transfer modulus. This measure has the potential to capture non-polynomial associations between the source and target distributions, whereas previous transfer exponents are restricted to polynomial relationships.\n\nThe authors also provide a computational algorithm achieving the same error bound guarantee. Even under convexity assumptions, they demonstrate a polynomial runtime algorithm for transfer Neyman-Pearson classification."}, "weaknesses": {"value": "The practical scenarios in which $R_{\\varphi,\\mu_1,T}(h^\\ast_{S,T,\\alpha}) - R_{\\varphi,\\mu_1,T}(h^\\ast_{T,\\alpha})$ is small are unclear. If this term is not small, using only target samples dominates the rate. In such cases, the benefit of transfer learning is not adequately demonstrated. Since $h^\\ast_{S,T,\\alpha}$ is defined as the maximizer of $R_{\\varphi,\\mu_1,S}$, the excess error between $h^\\ast_{S,T,\\alpha}$ and $h^\\ast_{T,\\alpha}$ could remain large even when the source and target distributions are similar. The justification for using $h^\\ast_{S,T,\\alpha}$ as the pivot in the error bound should be explained more clearly.\n\nThe experimental results suggest that false positive rate control is not adaptive. Theorem 1 asserts that the additive error in false positive rate control is adaptive, meaning it is bounded by the minimum of the error rates for using only target samples and the transfer method. However, Figure 3 shows that the false positive rate error is comparable to that of the method using only target samples. This discrepancy between theory and experiment should be addressed.\n\nThe paper's structure could be improved. For example, the setup section (Section 3) combines the problem formulation of Neyman-Pearson classification under the transfer learning framework, discussion of prior results, and challenges in constructing the proposed algorithm. A clearer separation of these components would be beneficial."}, "questions": {"value": "- In what concrete scenarios is the excess term $R_{\\varphi,\\mu_1,T}(h^\\ast_{S,T,\\alpha}) - R_{\\varphi,\\mu_1,T}(h^\\ast_{T,\\alpha})$ provably small? Can you characterize such cases via properties of the source/target distributions?\n- Why is $h^\\ast_{S,T,\\alpha}$ the right pivot for the analysis? Could similar or tighter bounds be derived by pivoting around alternatives such as a target‑only optimizer on a mixed/importance‑reweighted distribution, and how would that affect the rates?\n- Theorem 1 suggests adaptive control of the false positive rate (Type‑I), yet Figure 3 appears comparable to the target‑only method. What explains this discrepancy, and under what conditions should we expect adaptivity to be visible empirically?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "d8VMDiLxpC", "forum": "pHckxhmBlI", "replyto": "pHckxhmBlI", "signatures": ["ICLR.cc/2026/Conference/Submission20965/Reviewer_oLLY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20965/Reviewer_oLLY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980885673, "cdate": 1761980885673, "tmdate": 1762939061045, "mdate": 1762939061045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies transfer learning for Neyman–Pearson (NP) classification when both class-conditional distributions may shift between source and target domains. It proposes a two-stage adaptive procedure: (i) calibrate a source-side Type-I threshold `α̂_S` so that the source constraint aligns with the target NP constraint, thereby pruning hypotheses that would violate the target Type-I bound; (ii) within this restricted set, leverage source class-1 data (and target data) to further reduce Type-II risk. The paper derives generalization bounds stated via a *transfer modulus*—functions `ϕ₀` and `ϕ₁` translating source performance to target—and shows recovery of prior results when `μ₀,S = μ₀,T`. It also provides a computational oracle via a sequence of convex programs (SGDA-style) with gradient-complexity guarantees, and evaluates the method on two climate datasets where locations define domain shifts. :contentReference[oaicite:0]{index=0}"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I was not previously familiar with the NP classification setting, but I find both the problem and the authors’ extensions interesting. The authors also provide sufficient theoretical guarantees for their proposed algorithm, along with supportive simulation studies. In terms of novelty and substance, I believe the paper is worthy of publication at ICLR, although this is not my primary area of expertise."}, "weaknesses": {"value": "1. It is somewhat difficult to discern a strong novelty in the transfer-learning extension, even though the authors present solid theoretical results and simulations."}, "questions": {"value": "1. The theory relies on a convex hypothesis class $H$ and convex, Lipschitz, bounded surrogate losses, whereas the experiments use an MLP. Do MLPs constitute a convex hypothesis class?\n2. Can we derive similar theoretical guarantees for transfer learning under an overlapping-support assumption between the source and target datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CjpzGP57Lc", "forum": "pHckxhmBlI", "replyto": "pHckxhmBlI", "signatures": ["ICLR.cc/2026/Conference/Submission20965/Reviewer_Wj1g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20965/Reviewer_Wj1g"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20965/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981413348, "cdate": 1761981413348, "tmdate": 1762939060251, "mdate": 1762939060251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}