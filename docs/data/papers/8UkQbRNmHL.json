{"id": "8UkQbRNmHL", "number": 6370, "cdate": 1757974743370, "mdate": 1763069795523, "content": {"title": "How Animals Dance (When You're Not Looking)", "abstract": "We present a framework for generating music-synchronized, choreography aware animal dance videos. Our framework introduces choreography patterns---structured sequences of motion beats that define the long-range structure of a dance---as a novel high-level control signal for dance video generation. These patterns can be automatically estimated from human dance videos. Starting from a few keyframes representing distinct animal poses, generated via text-to-image prompting or GPT-4o, we formulate dance synthesis as a graph optimization problem that seeks the optimal keyframe structure to satisfy a specified choreography pattern of beats. We also introduce an approach for mirrored pose image generation, essential for capturing symmetry in dance. In-between frames are synthesized using an video  diffusion model. With as few as six input keyframes,  our method can produce up to 30 seconds dance videos across a wide range of animals and music tracks.", "tldr": "", "keywords": ["animal dance video generation; choreography pattern; long video generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e6541ac023bddbdd48120915450ca93de76cfbd9.pdf", "supplementary_material": "/attachment/2c19546de829feca4f40ae03794a2c1ff3b31c46.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors propose a framework which 1) generate dance keyframes from text prompts for different animals,\n2) extract music tempo and adjust the keyframes, 3) perform a video inbetweening generation for the final video.\n\nWe are seeing nice and cute videos where animals are dancing to the music."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. I think it is overall a nicely written graphics paper with a clear and concise introduction.\nThe combination use of dynamic time warping, pose clustering, RAFT, graph optimization and\ngenerative video models show that the authors have a in depth understanding of the graphics pipeline, traditional and newly emerging generative approaches.\n\n2. The authors are honest about limitations and future work directions.\nThey discuss the failure cases (motion artifacts from video diffusion, background inconsistency) and future work directions.\n\n3. A comprehensive set of user studies are conducted to evaluate the quality of the generated videos."}, "weaknesses": {"value": "1. The motion / animation quality is not very ideal. \nThere're visually very noticeable artifacts and jumps in the motion.\nWhile this might not be fair to compare indiviual researchers to industrial product teams which have far bigger budgets and resources,\nI think in terms of the visual quality and the motion smoothness for music conditioned video generation,\nthis demo is not as good as the online entertainment applications released on social media platforms like TicTok.\n\n2. The keyframe formulation does not feel intuitive or natural,\nand the applications require a relatively static background and camera view,\nwhich limits the level of freedom in entertainment of the proposed method.\n\n3. The engineering pipeline is too long and thus making it unscalable.\nA large number of components are added and a huge ton of hyperparameters are introduced,\nwith which it hard to tell how the proposed method can be generalized to other datasets and tasks outside this specific application.\n\n4. There's relative limited contribution in terms of the novelty and the significance of the proposed method.\nWhile this provides a solid engineering pipeline, it might not be most suitable for the submission of ICLR."}, "questions": {"value": "1. how long does one generation takes; \nIs there a break down of each module and human effort?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8CKB1tszwr", "forum": "8UkQbRNmHL", "replyto": "8UkQbRNmHL", "signatures": ["ICLR.cc/2026/Conference/Submission6370/Reviewer_ssGY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6370/Reviewer_ssGY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953835317, "cdate": 1761953835317, "tmdate": 1762918660755, "mdate": 1762918660755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "pfCTAh1Sbw", "forum": "8UkQbRNmHL", "replyto": "8UkQbRNmHL", "signatures": ["ICLR.cc/2026/Conference/Submission6370/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6370/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763069794696, "cdate": 1763069794696, "tmdate": 1763069794696, "mdate": 1763069794696, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel framework for generating music-synchronized animal dance videos using a small set of keyframes. The method leverages text-to-image models (or GPT-4o) to generate initial animal pose keyframes, then formulates dance synthesis as a graph optimization problem to align poses with rhythmic beats extracted from reference dance videos. To enhance visual coherence, a mirrored pose generation strategy is proposed for capturing bilateral symmetry in dance movements. The final continuous animation is produced using a video diffusion model for in-between frame synthesis. The system can produce up to 30-second dance clips from as few as six input keyframes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* Combining animal motion generation, musical beat synchronization, and keyframe optimization is original and creative.\n* The mirrored pose generation step addresses a real aesthetic issue in animal dancing (natural symmetry)."}, "weaknesses": {"value": "* The mirrored pose generation step makes the generation unreal.\n* I have seen the demos in the website, and I find that the generated result looks too stiff and lacks detail."}, "questions": {"value": "Compared with end-to-end approaches like [*Animate Anyone*](https://arxiv.org/pdf/2311.17117), I think focusing only on choreography patterns loses many important details. Can the current framework handle this issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4zgqOBYYna", "forum": "8UkQbRNmHL", "replyto": "8UkQbRNmHL", "signatures": ["ICLR.cc/2026/Conference/Submission6370/Reviewer_kGx5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6370/Reviewer_kGx5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973399918, "cdate": 1761973399918, "tmdate": 1762918660088, "mdate": 1762918660088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on generating animal dance videos from music.\nGiven an input music track, the system outputs a video of an animal dancing in synchronization with the rhythm.\n\nUnlike AnimateAnyone, this paper adopts a two-stage pipeline:\nfirst generating keyframes, and then arranging dance units along the music beats via graph-based computation.\nAccording to the paper, this design aims to achieve more stable image quality and to avoid the motion artifacts that can occur in models such as AnimateX.\n\nHowever, from another perspective, the results seem quite limited in scope.\nMost of the generated dances still resemble human-like movements performed by animals, rather than exhibiting species-specific motion patterns, such as tail dynamics or the non-limb-based movements of snakes, spiders, or fish.\nThis raises questions about the applicability and generalization capacity of the proposed framework."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of first generating keyframes and then arranging dance units according to the music through a graph-based warping strategy is quite interesting. This design helps maintain stable image quality while producing coherent video sequences.\n\n2. The writing is clear and easy to follow, and the presented demos are very cute and engaging.\n\n3. The experimental results are comprehensive and well-presented."}, "weaknesses": {"value": "1. From the results, the generated dances still appear limited to humanoid structures and behaviors. However, this paper claims to focus on animals. Why, then, does the framework rely on a human-based model (SMPL)? Can it be applied to non-humanoid species such as fish, snakes, reptiles (e.g., dinosaurs), or spiders?\nThis leads to a more fundamental question: where does this framework actually generalize?\nIf the intended motion domain remains largely human-like, why not directly adopt existing methods such as AnimateX, which are designed for articulated human motion?\nThe authors mention that AnimateX tends to produce artifacts for large motions. Then why not instead constrain the generation to smaller, more controlled music-to-motion segments?\n\n2. From a novelty perspective, the idea of treating dance units as graph vertices and their connections as edges has already been explored in 3D choreography systems, such as ChoreoMaster [1].\nWhat is fundamentally different about the proposed approach compared to those prior works, in terms of principle or formulation?\n\n[1] Chen et al., SIGGRAPH 2021, ChoreoMaster\n\nGenerally, while the results are indeed cute, they are not particularly convincing me this is really useful. Similar effects, such as dancing animals or expressive pet animations, can already be achieved using tools like Animate Anyone.\nThe current demos do not convincingly demonstrate broader capability.\n**If the authors could somehow make out unconventional examples (a dancing spider or fish) it would make the contribution much more compelling, and I would be happy to raise my score accordingly.**"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aBkbrBxaS7", "forum": "8UkQbRNmHL", "replyto": "8UkQbRNmHL", "signatures": ["ICLR.cc/2026/Conference/Submission6370/Reviewer_hxcn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6370/Reviewer_hxcn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994577211, "cdate": 1761994577211, "tmdate": 1762918659623, "mdate": 1762918659623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel framework for generating long-form, music-synchronized, and choreography-aware animal dance videos. The core innovation is the introduction of \"choreography patterns\" as a high-level control signal, which are automatically extracted from reference human dance videos. Starting with a small set of generated keyframes of an animal, the method formulates the dance synthesis as a graph optimization problem. The final video is rendered by synthesizing in-between frames using a video diffusion model. The authors demonstrate the ability to create dance videos up to 30 seconds long across various animals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a creative application of \"dancing animals\" and addresses a practical limitation of current models, which lack intuitive controls for creating coherent, long-duration motion. \n\nDecomposing the complex task into a graph optimization problem is an effective strategy. It decouples the high-level temporal structure (the choreography) from the low-level frame generation (the diffusion model), making the overall problem more tractable.\n\nThe paper also includes a well-designed perceptual user study comparing the proposed method against a baseline (Animate-X), which provides valuable, quantifiable insights into the method's pros and cons."}, "weaknesses": {"value": "1. The core innovation in \"choreography patterns\" for long-range control has been previously explored in the 3D skeleton animation domain (e.g., ChoreoMaster). The main contribution here appears to be a combination of \"automated choreography extraction from video\" (also a relatively established task) with \"keyframe-based video generation\", which lacks a fundamental methodological breakthrough.\n\n2. The demo reveals less fluid generated motion compared to the baseline with a distinct \"pose-to-pose\" quality, which ultimately undermines the overall quality of the final video. Fig. 4 also reveals the trade-off between \"appearance & motion naturalness\" and \"motion fluidity\" compared to the baseline. This suggests the limitation of the keyframe-based graph approach than methods driven by per-frame guidance. Compared to the complex dance genres explored in the broader dance synthesis field (e.g., hip-pop, waltz, ballet), the complexity and artistic expressiveness of the generated dances are rudimentary. The framework's capability to handle more advanced and nuanced choreography remains unproven.\n\n3. While direct baselines for animal dance generation are scarce, its core dance synthesis capability can still be evaluated against more sota methods in avatar dance. Concatenated baseline (\"skeleton-driven animation + motion retargeting\") is also available (e.g., Skinned Motion Retargeting). The absence of such a comparison weakens the paper's claims about its method's superiority.\n\n4. The paper addresses a relatively narrow application scenario. Beyond creating amusing short videos for social media, the underlying technical motivation and broader utility are not well-articulated, making the research feel somewhat niche and constrained.\n\n5. The paper's claim of generating videos up to 30 seconds long is potentially misleading. This is not achieved by a single model capable of understanding and generating long-term dynamics but an engineered composition with an independent video in-betweening model."}, "questions": {"value": "Do you consider the lack of motion fluidity to be an inherent limitation of the graph-search-plus-in-betweening paradigm? \n\nIs the framework able to handle more complex and less repetitive choreography, such as the rapid, consecutive movements or fine-grained body control?\n\nHow do you position your contribution in the context of true, coherent long-video generation? What are the advantages of your approach compared to works that attempt to model long-term dependencies within a single, unified model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5XeKecIDeb", "forum": "8UkQbRNmHL", "replyto": "8UkQbRNmHL", "signatures": ["ICLR.cc/2026/Conference/Submission6370/Reviewer_a366"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6370/Reviewer_a366"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996862196, "cdate": 1761996862196, "tmdate": 1762918658491, "mdate": 1762918658491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}