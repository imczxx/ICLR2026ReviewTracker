{"id": "Ia0EZQgalH", "number": 701, "cdate": 1756774200796, "mdate": 1763359048364, "content": {"title": "Pushing Toward the Simplex Vertices: A Simple Remedy for Code Collapse in Smoothed Vector Quantization", "abstract": "Vector quantization, which discretizes a continuous vector space into a finite set of representative vectors (a *codebook*), has been widely adopted in modern machine learning. Despite its effectiveness, vector quantization poses a fundamental challenge: the non-differentiable quantization step blocks gradient backpropagation.\n\n*Smoothed* vector quantization addresses this issue by relaxing the hard assignment of a codebook vector into a weighted combination of codebook entries, represented as the matrix product of a simplex vector and the codebook. Effective smoothing requires **two properties**:\n\n1. smoothed quantizers should remain close to a onehot vector, ensuring tight approximation, and\n2. all codebook entries should be utilized, preventing *code collapse*.\n\nExisting methods typically address these desiderata separately. By contrast, the present study introduces **a simple and intuitive regularization that promotes both simultaneously** by minimizing the distance between each simplex vertex and its $K$-nearest smoothed quantizers. Experiments on representative benchmarks&mdash;including discrete image autoencoding and contrastive speech representation learning&mdash;demonstrate that the proposed method achieves more reliable codebook utilization and improves performance compared to prior approaches.", "tldr": "This study proposes a simple and effective regularization method for smoothed vector quantization that simultaneously prevents code collapse and enforces tight approximation.", "keywords": ["vector quantization", "code collapse", "discrete autoencoder", "smoothing"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eab8dfc55f281144d6866282b005c7cf34a81cbb.pdf", "supplementary_material": "/attachment/51735f88af81344c16e98e8260cf117ec82745da.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the non-differentiability of hard quantization by smoothing assignments while also aiming to avoid code collapse. It reformulates VQ as smoothing of one-hot vectors and adds a simple KNN regularization that pulls smoothed assignments toward all simplex vertices (using L2 or cross-entropy), compatible with softmax or Gumbel-softmax. On ImageNet autoencoding and wav2vec2 pretraining, the method shows reasonable performance against the reported baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written. The technical parts are clear and detailed.\n- Smoothing the VQ process is an interesting idea. It provides new insight and motivates a simple KNN regularization to reduce code collapse. The loss is simple and effective.\n- Experiments confirm the effectiveness of the proposed method across tasks."}, "weaknesses": {"value": "- First, the experimental scale is limited, and the paper mostly compares its own variants. There are few comparisons to recent methods, and the related work leans on pre‑2023 citations, with the rotation trick as the only recent item. As a result, the basis feels thin for an active area like VQ codebook learning.\n- Second, more experiments are needed to study how batch size influences the KNN regularization. For example, compute the loss using groups within the same batch and analyze the trends. This would show whether the method scales to larger settings.\n- Third, include an ablation of the Gumbel‑Softmax temperature to make the study more complete and to clarify its interaction with the proposed loss."}, "questions": {"value": "- Please refer to the Weaknesses section. Overall I appreciate the technical contribution, but because the experiments appear insufficient and the discussion of recent related work is limited, I will keep the current score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ao5Ve1IooV", "forum": "Ia0EZQgalH", "replyto": "Ia0EZQgalH", "signatures": ["ICLR.cc/2026/Conference/Submission701/Reviewer_249k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission701/Reviewer_249k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761013886317, "cdate": 1761013886317, "tmdate": 1762915586456, "mdate": 1762915586456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the issue of code collapse in smoothed vector quantization by proposing a simple K-nearest neighbor (KNN) regularization that simultaneously enforces tight smoothing and uniform code utilization. Empirical evaluations on discrete image autoencoding and contrastive speech representation learning demonstrate that the proposed approach achieves nearly complete codebook utilization and competitive or superior reconstruction quality compared to baselines including straight-through estimation, Gumbel-softmax, and perplexity-based regularization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The illustration and explanation of smoothed vector quantization from a simplex perspective provide a clear geometric intuition that unifies and clarifies connections with prior approaches. \n2. The proposed KNN regularization is an intuitive idea that offers strong empirical benefits compared to prior entropy or perplexity-based regularization methods."}, "weaknesses": {"value": "1. While the method is designed to improve both smoothing tightness and codebook utilization, the paper does not provide quantitative analysis or visualization to support the claim about improved smoothing tightness. \n2. The KNN loss is acknowledged to be memory-inefficient, but the paper does not include any experiments or analysis to quantify the computational overhead or explore mitigation strategies."}, "questions": {"value": "1. How does the proposed regularization scale to very large codebooks (e.g., M = 16k ~ 64k)? Have you explored memory-efficient approximations such as partial vertex sampling or approximate KNN search? \n2. How sensitive are the results to the choice of K (number of nearest neighbors)? Is there a principled way to select K relative to the codebook size or batch size? \n3. While the paper demonstrates improved code utilization and reconstruction performance, does this lead to improvements in downstream tasks (e.g., image generation or text-to-speech synthesis)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zCEiSffl0d", "forum": "Ia0EZQgalH", "replyto": "Ia0EZQgalH", "signatures": ["ICLR.cc/2026/Conference/Submission701/Reviewer_PY24"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission701/Reviewer_PY24"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959701730, "cdate": 1761959701730, "tmdate": 1762915585793, "mdate": 1762915585793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new regularization method for smoothed vector quantization that simultaneously promotes near one-hot assignments and prevents codebook collapse. Their method primarily varies from prior methods in its choice of anchors and neighbours. More specifically, they use codebook entries as anchors and data as neighbours. Using this, the KNN-based regularizer encourages samples to cluster near all vertices, ensuring full codebook utilization and tight smoothing. They test their method on two VQ tasks (auto-encoding and contrastive learning) and show that their method performs on par with or better than the baseline methods in different codebook settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Their method naturally incorporates all the entries of the codebook, which ensures maximum codebook utilization by design.\n2. They achieve 100% codebook utilization among all the tasks they train, while performing on par or better than baseline methods\n3. The regularization loss they propose is quite flexible and can be combined with other backbones, such as Gumbel softmax, L2 or cross entropy losses, etc."}, "weaknesses": {"value": "1. Missing comparison with other baseline methods such as Group VQ, SimVQ etc.,\n2. Since they use the codebook entries as anchors, some of the datapoints might not contribute to the regularization loss\n3. Their method does not perform the best in some settings like in table 1, 64x64x3 setting, where the STE method performs the best"}, "questions": {"value": "1. The paper might improve by adding more baseline methods to show how their method compares with current state-of-the-art methods\n2. How does the method perform on Image generation (diffusion, flow-based, or autoregressive) or language models?\n3. How sensitive is the performance of the method in terms of K\n4. A dedicated ablation study and an explanation for the choice of hyperparameters can help in reproducibility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4FhJBJZhtR", "forum": "Ia0EZQgalH", "replyto": "Ia0EZQgalH", "signatures": ["ICLR.cc/2026/Conference/Submission701/Reviewer_Zwav"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission701/Reviewer_Zwav"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962462234, "cdate": 1761962462234, "tmdate": 1762915585163, "mdate": 1762915585163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper proposes a simple yet effective KNN-based regularization method to stabilize smoothed vector quantization. A well-behaved quantizer should produce one-hot–like assignments while maintaining balanced codebook usage. The proposed approach achieves both by penalizing the distance between each simplex vertex and its K nearest soft assignments. This encourages all codebook entries to be actively used while keeping each assignment sharply concentrated near a single vertex.\n\n\n- As a result, the method mitigates the train-test mismatch that occurs in perplexity-based regularization, where training uses soft assignments but inference requires hard quantization.\n\n\n- Experiments on discrete image autoencoding and contrastive speech representation learning demonstrate that the proposed regularizer effectively prevents code collapse, achieves near-complete code utilization, and delivers improved reconstruction and representation quality compared with other baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Empirical results support that KNN is superior to Perplexity regularizer, as it effectively reduce the mismatch between soft / hard assignment when training and testing.\n\n- Experiment results in representation learning highlights the robustness of the proposed method when adopted with cross-entropy (KNN-CE), which achieves 100% usage in both single and dual codebooks, where other methods fail.\n\n- The paper is generally clear. The Related Work section provides not only a concise overview of recent advances in vector quantization but also a coherent narrative that situates the proposed method within the broader literature. It effectively summarizes the fundamental components of VQ, such as the quantization step, gradient estimation, and smoothing strategies, making it accessible and informative even to readers new to the topic."}, "weaknesses": {"value": "- The paper lacks an ablation study on the number of neighbors (K), which is a key hyperparameter of the proposed regularizer.\n\n\n- There is limited qualitative results in the experiment section. Beyond the simplex visualizations, it would be valuable to include additional qualitative examples such as reconstructed images or assignment-map visualizations to illustrate the impact of KNN-based over Perplexity regularization.\n\n\n- Lack of experimental settings: details such as the exact loss components (reconstruction, codebook, commitment) and weighting used for baselines like STE, RE, and Perplexity regularization should be declared."}, "questions": {"value": "- The experimental results show that the proposed method combined with L2 distance performs noticeably worse than KNN + CE, particularly on the contrastive learning task. My main concern is about the method’s robustness to the choice of distance metric:\n\nIs the proposed regularizer sensitive to the form of the distance function?\n\nCould you provide theoretical intuition or gradient-level analysis explaining why CE behaves better than L2 in contrastive settings?\n\nAdditional experiments or ablation studies comparing both distances under controlled settings would strengthen the claim.\n\n- Please provide further analysis on the influence of number of neighbors (K):\n\nWhen K is small, can the method still mitigate the soft/hard assignment mismatch observed in Perplexity regularization?\n\nPlease include an ablation study on varying K and compare it with other baselines to measure effectiveness of the proposed method.\n\n- Because the value of K directly affects GPU memory usage, please report experiments for both baselines and the proposed method under similar GPU resource."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rUkWvf3nsz", "forum": "Ia0EZQgalH", "replyto": "Ia0EZQgalH", "signatures": ["ICLR.cc/2026/Conference/Submission701/Reviewer_Ssva"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission701/Reviewer_Ssva"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023529802, "cdate": 1762023529802, "tmdate": 1762915584678, "mdate": 1762915584678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}