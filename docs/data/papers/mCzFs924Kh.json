{"id": "mCzFs924Kh", "number": 4679, "cdate": 1757744057476, "mdate": 1763357658634, "content": {"title": "Revisiting Out-of-Distribution Detection: Angular Separation Learning as a Powerful and Simple Baseline", "abstract": "Out-of-Distribution (OOD) detection is a critical safety requirement for deploying deep neural networks in open-world environments. While recent advances increasingly rely on more computationally intensive training methods involving synthetic outliers, contrastive objectives, or specialized loss functions, their gains often come with substantial computational overhead and implementation complexity.\nIn this work, we revisit the fundamentals of OOD detection and uncover a key flaw in common distance-based detectors: sensitivity to feature magnitude. We show that low-norm OOD samples can appear closer to in-distribution (ID) class centroids than actual ID samples, evading detection.\nTo address this, we introduce **Angular Separation Learning (ASL)**, a simple and highly effective strategy that applies $\\ell_2$-normalization to features before the final classification layer. This modification compels the network to optimize for angular separation, achieving robust feature learning without additional regularization mechanisms, synthetic samples, or costly negative mining.\nThrough extensive experiments on diverse benchmarks, we demonstrate that ASL not only matches but often surpasses state-of-the-art methods, especially in challenging near-OOD scenarios, while maintaining training efficiency. Our results indicate that a minimalist rethink of standard training can achieve superior OOD performance, prompting a re-evaluation of the complexity-to-performance trade-off in OOD detection.", "tldr": "", "keywords": ["Out-of-Distribution Detectionm", "Angular Separation Learning", "Feature Normalization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed8bce7eb67cba72b65b0886cb76653aaebc5128.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "## Summary\nThe paper proposes Angular Separation Learning (ASL) as a simple, training-time technique for OOD detection: normalize feature representations before the classification head to emphasize angular decisions. The authors hypothesize that feature-norm is an overlooked factor; they analyze Mahalanobis distance, highlight failure cases with low-norm OOD features and show strong performance on multiple benchmarks. They also compare with post-hoc normalization methods (e.g., Mahalanobis++) underperform ASL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Simplicity.** Very simple algorithm and presented in a clear way.\n- **Low overhead, strong results.** A lightweight add-on achieving strong performance.\n- **Broad evaluation.** Covers many cases; Layerwise analysis done on Figure 5 is particularly a nice add.\n- **Fair comparisons.** Side-by-side with state-of-the-art post-hoc scoring methods, highlighting gains attributable to the training scheme."}, "weaknesses": {"value": "- **Overstated claims.**  \n  - Calling other training-based methods “complex” (line 15) / “exotic” (line 24) is too strong. Please tone down language unless quantitatively/qualitatively supported.  \n  - Line 111: [1], [2], [3] are works that already studied Mahalanobis and its corrections. Calling it “under-studied” is not accurate.  \n  - Line 407: Real-world practicality is an overstatement. One would not use a model with FPR95 ~ 20% on safety-critical applications.\n\n- **Figure 1 clarity.**  \n  Low-norm OOD representations are closer to ID is a strong observation, but I find Figure 1 unclear: how exactly computed, how thresholds selected, which backbone/dataset used, and whether repeated across datasets/backbones.\n\n- **Motivation vs. cosine head.**  \n  If the goal is angular learning, a cosine-based classifier (normalized features and normalized weights) is a straightforward alternative. Authors mention applying weight decay to bound weight norms, but do not justify not normalizing weights. Please add an ablation to evaluate:\n  - Whether norm cancellation and a truly cosine classifier improve OOD detection.\n  - Whether cosine model feature representations align with Section 3.2’s intuition.\n  - If cosine performs worse, provide reasoning and supporting experiments if it does not agree with the intuition provided.\n\n- **Low-norm failure analysis.**  \n  Beyond Fig. 1, selecting the 95% ID validation threshold, collecting false-positive OODs, and plotting their norm distribution alongside ID validation norms would be an easy validation for the main argument that we need to solve the low norm OOD sample symptom.\n\n- **Related work (post-hoc).**  \n  It is a rather crowded line of research. Therefore, the coverage lags the literature. Please consider [4] for strong and simple baselines and extensive evaluation and also [5], [6], [7].\n\n- **Norm as signal vs. hypothesis.**  \n  Line 150 cites a work using norm as a discriminative signal, which seems at odds with the main hypothesis. Either show [8] does not apply consistently or cite works aligned with your stance (e.g., [2], [5], [9]).\n\n- **Intuition around Eq. (1).**  \n  The discussion around line 198 is confusing. In ideal CE, ID feature and class mean coincide (therefore, distance becomes 0). Interpreting the equation \"holding everything else constant\" may mislead; many configurations allow high class separation and tight clusters with large norms. The issue seems more about ambiguous CE feature assignments to ID/OOD than \"MDS sensitivity to norm\" per se; consider analysis grounded in training dynamics (possibly with synthetic settings).\n\n- **Training efficiency.**  \n  Include confidence intervals from repeated runs after warmup. The table is concerning as there’s no clear reason LogitNorm should be slower than ASL. Control all hyperparameters (especially batch size) if reporting wall-time."}, "questions": {"value": "- Figure 3 is aesthetically nice but uses large space; consider rearranging to save space.\n- In Figure 1, what metric defines \"nearest class mean\"? L2 or cosine?\n- In imbalanced classes, weight/feature norms may contain priors probabilities if we interpret CE trained models as posterior predictors (~ $p(y) \\cdot p(x|y)$). How does normalization behave in this setting?\n- Table 3 (CIFAR-100): PALM has the lowest FPR, yet both ASL and PALM are bolded.\n- Can ASL be straightforwardly combined with activation shaping (e.g., ReAct, ASH)? It would be interesting to see interactions.\n- Since training/optimization is involved, please report confidence intervals with repeated experiments over different random seeds. Any reason this was not done?\n\n\n## References\n[1] Ren et al., 2021. *A simple fix to Mahalanobis distance for improving near-OOD detection*. arXiv:2106.09022.  \n[2] Mueller & Hein, 2025. *Mahalanobis++: Improving OOD Detection via Feature Normalization*. arXiv:2505.18032.  \n[3] Shi et al., 2013. *Improved relative-transformation PCA based on Mahalanobis distance*. Acta Automatica Sinica.  \n[4] Bitterwolf, Mueller, Hein, 2023. *In or out? Fixing ImageNet OOD detection evaluation*. arXiv:2306.00826.  \n[5] Demirel, Fumero, Locatello, 2024. *Out-of-Distribution Detection with Relative Angles*. arXiv:2410.04525.  \n[6] Liu & Qin, 2025. *Detecting OOD through the lens of neural collapse*. CVPR.  \n[7] Ammar et al., 2023. *NECO: Neural collapse based OOD detection*. arXiv:2310.06823.  \n[8] Zhang & Xiang, 2023. *Decoupling maxlogit for OOD detection*. CVPR.  \n[9] Sun et al., 2022. *OOD detection with deep nearest neighbors*. ICML."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BE7w2JgzOa", "forum": "mCzFs924Kh", "replyto": "mCzFs924Kh", "signatures": ["ICLR.cc/2026/Conference/Submission4679/Reviewer_puLH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4679/Reviewer_puLH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760978787763, "cdate": 1760978787763, "tmdate": 1762917509777, "mdate": 1762917509777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Angular Separation Learning (ASL), a simple modification to standard classification training for Out-of-Distribution (OOD) detection. By applying normalization to feature vectors before the final linear layer, the method enforces an angular decision geometry that alleviates the well-known sensitivity of distance-based OOD detectors (in particular of Mahalanobis detection score) to feature magnitude. The authors demonstrate that this minimalist change yields state-of-the-art or superior performance on several OOD benchmarks, especially for near-OOD cases. The paper argues that small architectural choices can achieve competitive OOD robustness without added complexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe method is extremely lightweight and well-motivated by a geometric analysis.\n2.\tASL consistently matches or outperforms more complex OOD detection approaches, particularly in the challenging near-OOD regime.\n3.\tThe work prompts a healthy reconsideration of whether the field’s increasing methodological complexity is always justified.\n4.\tThe method aligns the loss used during training with the score employed for InD/OoD separation."}, "weaknesses": {"value": "1.\tThe idea of feature normalization and angular margin learning is not new (similar principles exist in SphereFace for example). The main novelty lies in reinterpreting and applying them to OOD detection.\n2.\tThe method shows clear improvements in near-OOD detection; however, a more thorough analysis is needed to assess its effectiveness on far-OOD scenarios and its impact on overall classification accuracy.\n3.\tThe impact of normalization on confidence calibration and threshold is not discussed."}, "questions": {"value": "1.\tAngular separation learning has been previously explored in the literature, notably in *SphereFace: Deep Hypersphere Embedding for Face Recognition* (Liu et al., CVPR 2017). Although SphereFace was not designed for OOD detection, its underlying rationale is closely related, as it also enforces angular-based feature learning on a hypersphere and discusses the Mahalanobis distance for classification. SphereFace should therefore be cited and discussed in relation to the proposed method. In particular, the loss functions of ASL and SphereFace should be explicitly compared and analyzed, as both promote hyperspherical feature learning but differ in how they enforce angular separation (standard normalized cross-entropy vs. multiplicative angular-margin loss).\n\n2.\tThe mechanics of MDS failure are well-motivated for near-OOD samples. However, losing access to feature norm information may negatively impact both classification accuracy and far-OOD detection. While the accuracies of models trained on CIFAR-10 and CIFAR-100 are reported in the appendix, they should also be clearly mentioned alongside all experimental results, especially those presented in the main body of the paper. Regarding far-OoD detection, the results presented in Fig. 4 show a strong instability with respect to parameter $\\lambda$ (on individual datasets) meaning that the method could exhibit poor performance in some particular situations. Could you elaborate on the strategy to choose the value of $\\lambda$ that should not be guided by experimental results on particular datasets ?\n\n3.\tCould you provide an analysis of the features distribution to visually verify the claimed hyperspherical geometry and the relative positioning of InD and OOD samples?\n\n4.\tThe estimation of $\\mu_c$ and $\\Sigma$ is a key factor in the effectiveness of MDS. Could you clarify how these estimates were computed (e.g., number of samples, and whether training, validation, or test data were used)?\n\n5.\tNeural Collapse (mentionned in the paper) is typically observed for in-distribution features, where class features collapse toward their centroids on a hypersphere. Given that Angular Separation Learning enforces $\\ell_2$-normalization, did you observe a similar collapse behavior around class centroids for in-distribution samples, and how does this interact with the positioning of out-of-distribution features? \n\nAngular Separation Learning (ASL) is a lightweight and well-motivated approach that demonstrates strong empirical performance, particularly for near-OOD detection. While the method is effective and clearly presented, its novelty is limited, with prior work (e.g., SphereFace) exploring similar angular feature normalization. The analysis of far-OOD performance, feature-space visualization, and hyperparameter sensitivity is incomplete. Overall, the paper provides a valuable incremental contribution to OOD detection, but it primarily reinforces existing intuitions rather than introducing a fundamentally new methodology."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S5JrWb2tBL", "forum": "mCzFs924Kh", "replyto": "mCzFs924Kh", "signatures": ["ICLR.cc/2026/Conference/Submission4679/Reviewer_zrv2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4679/Reviewer_zrv2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761147902956, "cdate": 1761147902956, "tmdate": 1762917509211, "mdate": 1762917509211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the foundation of out-of-distribution (OOD) detection and identifies a critical weakness of Mahalanobis-based methods — their sensitivity to feature norms. The authors propose Angular Separation Learning (ASL), a minimalist yet effective training strategy that applies $\\ell_2$-normalization to features before the final classification layer. This simple modification encourages angular discrimination between classes, effectively mitigating the failure mode of distance-based detectors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed Angular Separation Learning (ASL) involves only a minimal modification—$\\ell_2$ normalization of features before the cross-entropy loss—yet consistently surpasses much more complex training schemes (contrastive, synthetic outlier generation, or regularization-heavy methods). This reinforces the key message that complexity is not always correlated with OOD robustness, positioning ASL as a strong and practical new baseline.\n\n2. The paper provides a unifying perspective connecting normalization, angular margin optimization, and contrastive representation learning. Proposition 1 formally shows that ASL implicitly performs prototype-based contrastive learning in angular space, without explicit pair mining or temperature tuning. This theoretical lens could inspire a rethinking of numerous normalization and contrastive frameworks.\n\n3. The experiments are extensive, covering both convolutional and transformer backbones, and include near-/far-OOD as well as covariate shift scenarios."}, "weaknesses": {"value": "1. The novelty of the proposed method appears somewhat limited, as normalization-based mechanisms for OOD robustness have already been explored in prior works such as LogitNorm (Wei et al., 2022), T2FNorm (Regmi et al., 2024a), and MD++ (Müller & Hein, 2025). It would strengthen the paper if the authors could further highlight the unique aspects and contributions of their approach compared to these existing methods.\n\n2. Proposition 1 provides a qualitative statement about angular separation, but its proof sketch is not rigorous and lacks an explicit link to OOD generalization.\n\n3. In the proposed ASL framework, features are projected onto the angular space via $\\ell_2$-normalization before classification. Could the authors further clarify why discarding the feature magnitude and relying solely on angular information is beneficial for OOD detection?"}, "questions": {"value": "Please carefully check the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XEv9qGnjXB", "forum": "mCzFs924Kh", "replyto": "mCzFs924Kh", "signatures": ["ICLR.cc/2026/Conference/Submission4679/Reviewer_RM6N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4679/Reviewer_RM6N"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762202733314, "cdate": 1762202733314, "tmdate": 1762917508793, "mdate": 1762917508793, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the out-of-distribution (OOD) detection problem. It proposes that recent work rely on complex training paradigms, which significantly limit the computational efficiency. Moreover, it discover that the existing methods neglect the influence of feature magnitude, which may lead to incorrect detection of OOD samples. To address this problem, it proposes a new method called ASL, which encourages robust feature learning by adding a L2-normalization before the classifier. The proposed method is evaluated on multiple datasets, where the performance achieves SOTA in most cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation of this paper is reasonable. It reveals the reason behind the failure of existing distance-based detection methods by empirical studies.\n- The proposed method is simple while effective. This will improve the genealizability of the proposed method.\n- The experimental results are strong. Also, the authors have conducted extensive ablation studies and additional analysis to make the proposed method more convincing.\n- Theoretical analysis is provided to improve the solidness."}, "weaknesses": {"value": "- The proposed method combines a feature normalization and a linear classifier. What about directly modifying the linear classifier into a cosine classifier or some other distance-based classifier? Using cosine classifier for OOD detection is already studied by previous works, so what is the difference between previous work and this study?\n- The training efficiency is an important strength of this paper, which, however, is only mentioned in the section of experiments. Is it possible to add some explanations or analyses in the section of method to highlight this advantage?\n- The font sizes of some figures are inharmonious (e.g. Figure 15). It is better to improve the fonts.\n\n[1] Hyperparameter-free out-of-distribution detection using cosine similarity."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dWGX0sdkn0", "forum": "mCzFs924Kh", "replyto": "mCzFs924Kh", "signatures": ["ICLR.cc/2026/Conference/Submission4679/Reviewer_82FN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4679/Reviewer_82FN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762260832604, "cdate": 1762260832604, "tmdate": 1762917508389, "mdate": 1762917508389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}