{"id": "RdEgywr1eA", "number": 8283, "cdate": 1758077361342, "mdate": 1759897794389, "content": {"title": "On-Policy RL with Optimal Reward Baseline", "abstract": "Reinforcement learning algorithms are fundamental to align large language models with human preferences and to enhance their reasoning capabilities. However, current reinforcement learning algorithms often suffer from training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models. In this work, we propose On-Policy RL with Optimal reward baseline (OPO), a novel and simplified reinforcement learning algorithm designed to address these challenges. OPO emphasizes the importance of exact on-policy training, which empirically stabilizes the training process and enhances exploration. Moreover, OPO integrates a practically feasible formulation of the optimal reward baseline that minimizes gradient variance. We evaluate OPO on mathematical reasoning benchmarks. The results demonstrate its superior performance and training stability without additional models or regularization terms. Furthermore, OPO achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses. These results highlight OPO as a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks. The OPO implementation is integrated into the VeRL library.", "tldr": "We introduce OPO, a novel RL algorithm that uses exact on-policy training and a practical optimal reward baseline to achieve superior performance, increased training stability, and diverse outputs.", "keywords": ["Reinforcement learning for large language models", "On-policy optimization", "Reward baseline", "RL training stability", "Policy entropy"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5508e3db995618dcbf404a9cd265be9b5ee7963f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes On-Policy RL with Optimal reward baseline (OPO), a reinforcement learning algorithm. OPO aims to address training instability and computational inefficiency encountered in current methods for aligning large language models (LLMs) and enhancing LLM reasoning capabilities. The algorithm incorporates two main features: exact on-policy training and a practical formulation of an optimal reward baseline. The optimal reward baseline minimizes policy gradient variance. Experimental results on mathematical reasoning benchmarks indicate that OPO has improved performance and training stability. OPO operates without requiring auxiliary models or explicit regularization terms. The algorithm shows lower policy shift and higher output entropy, resulting in more diverse and less repetitive generated responses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The algorithm saves the need for auxiliary models (like value networks) and explicit regularization terms (such as KL divergence or entropy bonuses), simplifying the training pipeline.\n\n2. Experimental results show that OPO achieves improved performance on mathematical reasoning benchmarks and maintains stable training dynamics with lower policy shift and higher output entropy."}, "weaknesses": {"value": "1. The formula for the optimal baseline, which weights the reward by the squared magnitude of the score function, is a very well-known result in variance reduction for policy gradient methods (REINFORCE) [1]. This considerably undermines the originality of this work.\n\n2. Around Eq. 10, the authors make two strong and unjustified assumptions: one is the orthogonal token gradients, and the other is the same distribution of different squared token gradient norms. The first assumption hardly holds in the context of sequence models (LLMs) because:\n\na) Shared Parameters: All tokens in the sequence are generated by the exact same set of parameters ($\\theta$). The gradient $\\mathbf{g}_t$ is the effect of token $y_t$ on all parameters. Since a single word (like ``not'') might significantly influence the probability distribution for many subsequent words, the gradients are highly correlated across time, meaning that they are far from orthogonal.\n\nb) Context Dependence (Autoregressive Structure): The gradients $\\mathbf{g}_t$ and $\\mathbf{g}_k$ are dependent because \n\n$\\\\pi_\\\\theta(y_k|y_{1:k-1}, x)$ depends on the entire history, which itself depends on the parameters. In an autoregressive model, the policy for generating $y_k$ relies on the embeddings generated by $y_{1:k-1}$, making the gradients intrinsically linked.\n\nc) Covariance Matrix is Non-Diagonal: If the gradients were truly orthogonal, the expected Fisher Information Matrix (which is related to $E[(\\\\sum \\mathbf{g}_t)(\\\\sum \\mathbf{g}_k)^T]$) would be a diagonal matrix, but in reality, it is highly structured and full of covariance terms.\n\nThe second assumption is also unreliable. The gradient magnitude $\\\\|\\mathbf{g}_t\\\\|^2$ is inversely related to the policy's confidence in its prediction $y_t$. In brief, high confidence corresponds to a small gradient magnitude, while low confidence corresponds to a large gradient magnitude. \n\n\n3. Mathematical expression incorrectness: the ``squared gradients $(\\cdot)^2$'' in Eqs. 8 and 9 seem to be corrected as squared l_2 norm $\\\\|\\cdot\\\\|^2$.\n\n[1] Barto, A. G. (2021). Reinforcement learning: An introduction. by richard's sutton. SIAM Rev, 6(2), 423.\n\nBased on these significant problems, this work does not meet the quality standard of ICLR yet."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cJgFXrixNY", "forum": "RdEgywr1eA", "replyto": "RdEgywr1eA", "signatures": ["ICLR.cc/2026/Conference/Submission8283/Reviewer_8d7i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8283/Reviewer_8d7i"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838262114, "cdate": 1761838262114, "tmdate": 1762920215492, "mdate": 1762920215492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes \"On-Policy RL with Optimal Reward Baseline,\" highlighting the importance of on-policy reinforcement learning algorithms while introducing a novel reward term to minimize gradient variance. Ultimately, OPO achieves smaller policy shifts and higher output entropy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The design of the reward based on minimizing gradient variance is highly innovative.\n2. OPO achieves relatively stable training curves without requiring KL penalty or entropy regularization."}, "weaknesses": {"value": "1. The theoretical assumptions in the paper—that \"the gradients of different tokens are approximately orthogonal\" and \"the norm of the gradient for each token follows the same distribution\"—lack justification. There is neither empirical statistical validation nor theoretical support, casting doubt on their reliability.\n2. A significant portion of current RL algorithms are already based on on-policy methods. If the authors aim to emphasize the importance of on-policy learning, it would be more appropriate to compare with inherently off-policy algorithms like DPO. Additionally, while the authors critique PPO for not being entirely on-policy, a comparison with PPO remains valid."}, "questions": {"value": "1. I believe that an increase in KL divergence does not necessarily indicate training instability, as deviation from the initial distribution is inevitable during training. Therefore, OPO's slower rate of KL increase may only suggest more conservative optimization. How do the authors view this point? I am also curious about the effects of directly using KL penalty or entropy penalty.\n2. I observed that even after RL training, the performance improvement is not significant. What might be the reason for this? Are there results with more training steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DubrKL1uvw", "forum": "RdEgywr1eA", "replyto": "RdEgywr1eA", "signatures": ["ICLR.cc/2026/Conference/Submission8283/Reviewer_5pnU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8283/Reviewer_5pnU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840852919, "cdate": 1761840852919, "tmdate": 1762920214582, "mdate": 1762920214582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose OPO, which is a combination of GRPO with an added baseline to reduce variance."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "I found the paper to be lacking in technical novelty. It is essentially a rebranding of various previous recipes in a new form.\n\nHowever, I found the interest in Variance Reduction techniques in LLM finetuning to be quite refreshing."}, "weaknesses": {"value": "A Substantial Literature review has been established on variance reduction techniques, though they are good for theoretical results; incorporating baselines rarely changes anything. This is kind of reflected in the training dynamics, as shown in Figure 1. Honestly, I am a bit surprised to see that both on-policy and off-policy training have similar error plots, but I guess that's OK. I might be biased in my opinion, but I think that variance reduction techniques bring little value in RL, and this paper has failed to convince me otherwise. \n\nI think the work would be better suited as a theory paper, see\n1. Peter L Bartlett and Jonathan Baxter, Estimation and approximation bounds for gradient-based reinforcement learning,\n2. Evan Greensmith, Peter L Bartlett, and Jonathan Baxter, Variance reduction techniques for gradient estimates in reinforcement learning,\n3. P. Marbach and J. N. Tsitsiklis, Simulation-based optimization of markov reward processes,\n4. P. Marbach and J. N. Tsitsiklis, Simulation-based optimization of markov reward processes,\n5. Jonathan Baxter and Peter L Bartlett, Infinite-horizon policy-gradient estimation,\n6. Peter L Bartlett and Jonathan Baxter, Estimation and approximation bounds for gradient-based reinforcement learning,"}, "questions": {"value": "Q1. Can you explain how you derived equation 10 from the assumptions?\nQ2. Any particular reason you are including $\\log \\pi$ in the OPO objective? There seems to be a lack of transparancy between the text before which includs GRPO and variance reduction to the formulation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W5ikIzPuJQ", "forum": "RdEgywr1eA", "replyto": "RdEgywr1eA", "signatures": ["ICLR.cc/2026/Conference/Submission8283/Reviewer_ivp9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8283/Reviewer_ivp9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892417069, "cdate": 1761892417069, "tmdate": 1762920214114, "mdate": 1762920214114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes On-Policy Reinforcement Learning with Optimal reward baseline (OPO), a novel and simplified reinforcement learning algorithm. There are two key innovations: 1) exact on-policy training to ensure that every policy update is computed with freshly sampled trajectories to maintain stability and prevent entropy collapse; and 2) a practical approximation of the optimal reward baseline that minimizes gradient variance by weighting rewards by sequence length. The authors evaluate OPO on mathematical reasoning benchmarks (e.g., MATH-500, AIME 2024, AIME 2025) and show that OPO achieves higher accuracy, lower policy shifts as indicated by reduced KL, and higher output entropy, resulting in more diverse, less repetitive generations. These results show the potential of OPO as a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well motivated, addressing an important research problem, RL algorithms often suffering from training instability due to loose on-policy constraints.\n2. The paper is well written, the derivation of the proposed OPO algorithm is easy to follow and understand.\n3. Optimal reward baseline is simple and straightforward to implement, making it easy to incorporate into various RL algorithms.\n4. The empirical results seems convincing: experiments on math benchmarks show that OPO achieves higher accuracy, reduced KL, and greater output entropy"}, "weaknesses": {"value": "1. The evaluation is limited to math benchmarks only; it would be nice to include results on code, science, STEM, or instruction-following benchmarks as well.\n2. A key assumption in the derivation of OPO is that the gradients of different tokens are approximately orthogonal and that the gradient norms follow the same distribution. There is a lack of explanation and evidence for why this assumption would hold.\n3. While the authors demonstrate the improvement brought by the optimal reward baseline when integrated into GRPO, there is a lack of comparison and discussion on whether it would be compatible with other RL algorithms.\n4. While the authors demonstrate better performance with exact on-policy training, this approach can be computationally expensive compared to off-policy variants that reuse trajectories. The paper lacks discussion and ablation on compute cost.\n5. The paper lacks comparisons against other strong RL baselines such as PPO, DPO, and Dr. GRPO."}, "questions": {"value": "Can you illustrate more on the assumption in the derivation of OPO that \"the gradients of different tokens are approximately orthogonal and the norm of the gradient for each token follows a same distribution\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ozhbuWGK51", "forum": "RdEgywr1eA", "replyto": "RdEgywr1eA", "signatures": ["ICLR.cc/2026/Conference/Submission8283/Reviewer_Akfd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8283/Reviewer_Akfd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8283/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037990020, "cdate": 1762037990020, "tmdate": 1762920213354, "mdate": 1762920213354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}