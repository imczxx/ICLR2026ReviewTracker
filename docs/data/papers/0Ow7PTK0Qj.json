{"id": "0Ow7PTK0Qj", "number": 25400, "cdate": 1758367590471, "mdate": 1759896722305, "content": {"title": "FastEdit: Low-Rank Structured Regularization for Efficient Model Editing", "abstract": "When new knowledge emerges, it is crucial to efficiently update large language models (LLMs) to reflect the latest information. However, state-of-the-art methods widely adopted in the model editing community --- such as MEMIT, PRUNE, and AlphaEdit --- suffer from prohibitively slow editing speeds, often taking 6 to 14 hours to sequentially edit just 2000 facts on models like LLaMA-3-8B, making real-time updates impractical, especially as model scale increases. Moreover, they require extensive pre-computation to sample pre-edit knowledge --- a step that can take over 24 hours --- severely limiting their deployability. In this paper, we present \\textbf{FastEdit}, a highly efficient editing framework that enables rapid and scalable model updates. Our key insight is to exploit the low-rank structure inherent in editing updates through a structured regularizer, allowing us to avoid costly inversions via the Sherman-Morrison-Woodbury (SMW) identity. This drastically accelerates the computation of update matrices while preserving edit quality. Crucially, \\textbf{FastEdit} requires only a small number of pre-edit samples, reducing both memory and computational overhead. On 2000 sequential edits, \\textbf{FastEdit} completes the process in just \\textbf{1 hour} -- an order of magnitude faster than prior work -- without sacrificing accuracy. Our method significantly lowers the barrier to practical model editing, enabling timely and scalable knowledge updates in large models.", "tldr": "", "keywords": ["Large Language Models", "Model Editing", "Knowledge Updating"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a9b25e95a586621fb175980502f410c29b8a691d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes **FastEdit**, a model-editing framework that introduces a **low-rank plus diagonal (LR+D) structured regularization** for efficient and stable knowledge updates in large language models (LLMs).  \nBy leveraging the **Sherman–Morrison–Woodbury (SMW) identity**, the method reduces the cubic-time matrix inversion in existing editing frameworks (e.g., MEMIT, PRUNE, AlphaEdit) to a low-rank \\(O(dr^2)\\) computation.  \nA periodic spectral compression strategy is further introduced for sequential edits to maintain bounded rank and computational cost.  \nExperiments on GPT2-XL, GPT-J, and LLaMA-3 show up to **10× faster editing** with comparable factual accuracy."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important bottleneck — the inefficiency of model editing — and offers a structured, implementable solution. FastEdit achieves order-of-magnitude acceleration (5×–10×) and memory reduction (17GB vs. 22GB) across three large models without harming edit precision.The use of the LR+D covariance model and the Sherman–Morrison–Woodbury identity is mathematically sound and clearly derived. The closed-form update (Eq.7) is clean and easy to reproduce. The writing is clear, and the method is straightforward to implement, making it useful for practitioners seeking faster model editing."}, "weaknesses": {"value": "1. **The claimed acceleration may be overstated.**  \n\n   The reported “10× speedup” considers only the matrix inversion phase but not the **entire editing pipeline**, including the computation of  \\(v\\) and the LR+D covariance estimation step. When these additional costs are included, the overall acceleration is likely to be much smaller.  \n\n\n2. **The observed improvement mainly stems from SMW algebraic simplification.**  \n   The acceleration is primarily achieved through the **Sherman–Morrison–Woodbury (SMW)** identity, a standard algebraic transformation widely used in low-rank approximation and online inverse computations.  \n   Hence, the performance gain should be interpreted as an **engineering optimization**, rather than a genuine algorithmic innovation. \n\n\n3. **The safety metrics (eₜ, sₜ) are not novel.**  \n   Similar orthogonality-based interference measures were already discussed in previous works.  \n   The metrics in FastEdit share nearly identical definitions, differing mostly in naming and visualization.  \n\n4. **Compression advantage is marginal in batch editing scenarios.**  \n   The claimed benefit of periodic spectral compression mainly appears in **sequential single editing**, where many edits are applied one-by-one.  \n   However, in **batch editing** (e.g., editing 100 facts simultaneously as in AlphaEdit), both MEMIT and FastEdit may exhibit similar runtime reductions, suggesting that compression contributes little to efficiency in such cases."}, "questions": {"value": "See Above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VdeHz0wMaD", "forum": "0Ow7PTK0Qj", "replyto": "0Ow7PTK0Qj", "signatures": ["ICLR.cc/2026/Conference/Submission25400/Reviewer_mcCT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25400/Reviewer_mcCT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760690817319, "cdate": 1760690817319, "tmdate": 1762943422537, "mdate": 1762943422537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents FastEdit, a method to fix the speed bottleneck in model editing. State-of-the-art methods are slow due to large $O(d^3)$ matrix inversions. The authors' key insight is to assume the pre-edit knowledge representation follows a Low-Rank plus Diagonal (LR+D) structure. This assumption allows them to replace the standard regularizer with a structured one that can be inverted efficiently using the Sherman-Morrison-Woodbury (SMW) identity, dropping the computational complexity to $O(dr^2)$. They also use SVD-based compression for sequential editing and a fused covariance estimate to cut down on pre-computation time. Experiments show FastEdit is about 10x faster than baselines on LLaMA-8B and achieves comparable or better editing performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a critical, practical bottleneck. Current editing times are a major blocker for real-world use. A 10x speedup is a significant engineering contribution that makes real-time editing much more feasible.\n\n2. The core technical idea is elegant. Using the expected LR+D structured regularizer to enable the SMW identity is a smart and principled way to achieve the speedup, moving beyond just brute-force computation.\n\n3. The fused covariance estimation is another important practical win. The 24+ hour pre-computation of prior work is a huge hidden cost, and reducing it to minutes by combining a data-driven estimate with a structural prior is a big step for deployability.\n\n4. The safety analysis in Section 5.3, with the $e_t$ and $s_t$ metrics, provides a nice geometric intuition for why some sequential editing methods fail over time, adding a good diagnostic tool to the paper."}, "weaknesses": {"value": "1. The novelty feels a bit thin. This seems less like a new framework and more like applying a standard low-rank approximation to the covariance matrix in the MEMIT objective. Using SMW for this is a classic linear algebra trick.\n    \n2. The sequential editing comparison looks like a strawman. As described in Appendix C.2 the baselines are adapted to accumulate all past keys making their matrix inversion rank grow linearly. This guarantees they will be slow.\n    \n3. The periodic SVD compression for sequential editing is a lossy heuristic. There's no analysis of its impact on catastrophic forgetting. For instance does the model forget edit #1 after 2000 edits?\n    \n4. The results are a clear speed-accuracy trade-off. FastEdit is competitive but it doesn't win on accuracy. It underperforms PMET on Llama-3 Generality for example."}, "questions": {"value": "1. Regarding Appendix C.2 am I understanding correctly that you made the baselines accumulate all past keys into the matrix for inversion? This seems to create an unfair comparison. Why not compare against a standard incremental SMW update for the baselines as well?\n    \n2. Did you test for catastrophic forgetting caused by your periodic SVD compression? I'm curious what the accuracy on the _first_ 100 edits is after all 2000 edits are finished.\n    \n3. How did you choose the crucial $r_0$ rank for the Llama-3 and GPT experiments? The appendix only shows a sensitivity analysis for GPT2-XL but this parameter seems central to the method's performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wR1UnvYIGm", "forum": "0Ow7PTK0Qj", "replyto": "0Ow7PTK0Qj", "signatures": ["ICLR.cc/2026/Conference/Submission25400/Reviewer_CCmG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25400/Reviewer_CCmG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824275691, "cdate": 1761824275691, "tmdate": 1762943422171, "mdate": 1762943422171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FastEdit, a fast and structured model editing framework for large language models. FastEdit exploits the empirical low-rank structure of FFN key representations within Transformers, introducing a regularization scheme that enables efficient closed-form updates via the Sherman-Morrison-Woodbury (SMW) identity. The method achieves a dramatic reduction in both computational and memory costs without sacrificing edit precision or generalization. Experimental results across several benchmarks and models (GPT2-XL, GPT-J, Llama-3) show that FastEdit supports rapid, scalable sequential editing while maintaining model robustness and editing safety."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- A central strength is FastEdit's dramatic acceleration of large-scale editing—Figure 1 highlights orders-of-magnitude time reduction for performing 2,000 sequential edits, reducing editing latency from many hours to under two for state-of-the-art LLMs. This is a genuine practical advance addressing a severe real-world bottleneck in model editing.\n- Table 1 and Table 2 deliver comprehensive comparisons on CounterFact and ZsRE. FastEdit matches or outperforms strong baselines (e.g., MEMIT, AlphaEdit, PRUNE) on editing efficacy, specificity, and generalization—in many cases closing the efficiency gap without loss of edit success. The wider applicability is supported by experiments on three model architectures.\n- The adaptation of all baselines for sequential editing is clearly described, supporting meaningful comparison and reproducibility."}, "weaknesses": {"value": "- The SMW-based approach relies on good estimation of the LR+D structure from a (now-small) sample of pre-edit keys. While Section 4 and Appendix A provide justification for the low-rank model, the main paper does not thoroughly analyze what happens if the pre-edit data is excessively sparse/noisy, or if the singular value spectrum is not strongly decaying—the potential for bias or regularizer miscalibration is not deeply probed.\n- Some of the notation describing periodic spectral compression and SVD-based rank truncation in Algorithm 1 (Appendix B) are terse and could be more explicitly connected to the mathematical formulation in the main sections. For instance, the reuse of symbols for compressed key matrices may confuse practitioners unfamiliar with the area."}, "questions": {"value": "Same as Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h0zRPaBQFH", "forum": "0Ow7PTK0Qj", "replyto": "0Ow7PTK0Qj", "signatures": ["ICLR.cc/2026/Conference/Submission25400/Reviewer_BLSK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25400/Reviewer_BLSK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914539401, "cdate": 1761914539401, "tmdate": 1762943421913, "mdate": 1762943421913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets the practical efficiency bottlenecks of model editing in LLMs. It proposes FastEdit, which exploits an inherent low-rank-plus-diagonal (LR+D) structure in the edit update to accelerate computation. Specifically, the regularization term is rewritten using a structural prior $U U^\\top + D$, which enables efficient inverses via the Sherman–Morrison–Woodbury identity. For sequential edits, the method maintains a low computational cost by periodically compressing the accumulated keys to keep the low rank. Experiments indicate that it can achieve faster editing while maintaining editing quality comparable to that of prior editors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Although low-rank modeling and SMW are established, the paper integrates them in the model editing setting to deliver tangible speedups without changing the editing quality.\n2. Leveraging the FFN down-projection SVD as a prior to estimate the key covariance is a clever way to reduce the number of samples and preprocessing time."}, "weaknesses": {"value": "1. In many editors, the dominant per-edit cost stems from optimizing the value vectors $V$, and the inverse is often a smaller fraction. FastEdit primarily accelerates the inverse step.\n\n2. As the number of edits grows, the rank of accumulated keys may increase. While periodic compression is proposed, the paper does not yet demonstrate that performance remains reliable at larger scales.\n\n3. The method introduces several hyperparameters. The combined sensitivity and robustness of outcomes to these choices is insufficiently characterized."}, "questions": {"value": "1. Please evaluate substantially more edits to test whether periodic compression continues to preserve efficacy.\n\n2. How exactly is ''editing time'' measured—end-to-end (including data loading and forward), per-edit from the first optimization step, or only the inverse step? More details would help highlight the method's contribution.\n\n3. How does FastEdit’s editing time compare to the parameter-preserving methods like GRACE? For sequential editing, SimIE should also be included, as it has demonstrated strong performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kAg9ViFHdm", "forum": "0Ow7PTK0Qj", "replyto": "0Ow7PTK0Qj", "signatures": ["ICLR.cc/2026/Conference/Submission25400/Reviewer_ES6g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25400/Reviewer_ES6g"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963180921, "cdate": 1761963180921, "tmdate": 1762943421574, "mdate": 1762943421574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}