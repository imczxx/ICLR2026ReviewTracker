{"id": "MCnbUQsSmo", "number": 3230, "cdate": 1757382702746, "mdate": 1759898100973, "content": {"title": "CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers", "abstract": "Visual generation quality has been greatly promoted with the rapid advances in diffusion transformers (DiTs), which is attributed to the scaling of model size and complexity. However, these attributions also hinder the practical deployment of DiTs on edge devices, limiting their development and application. Serve as an efficient model compression technique, model post-training quantization (PTQ) can reduce the memory consumption and speed up the inference, with inevitable performance degradation. To alleviate the degradation, we propose CLQ, a cross-layer guided orthogonal-based quantization method for DiTs. To be specific, CLQ consists of three key designs. First, we observe that the calibration data used by most of the PTQ methods can not honestly represent the distribution of the activations.  Therefore, we propose cross-block calibration (CBC) to obtain accurate calibration data, with which the quantization can be better guided. Second, we propose orthogonal-based smoothing (OBS), which quantifies the outlier score of each channel and leverages block Hadamard matrix to smooth the outliers with negligible overhead. Third, we propose cross-layer parameter searching (CLPS) to search. We evaluate CLQ with both image generation and video generation models and successfully compress the model into W4A4 with negligible degradation in visual quality and metrics. CLQ achieves 3.98x memory saving and 3.95x speedup with real-world deployment testing. Our code will be released soon.", "tldr": "A novel PTQ method for DiTs on visual generation.", "keywords": ["Quantization", "Diffusion Transformer"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5dad93a5e78ec669188ada44f3e494283f4db71a.pdf", "supplementary_material": "/attachment/944e3592fedaed88640af2d51e63316ddee09198.pdf"}, "replies": [{"content": {"summary": {"value": "Authors propose CLQ, a method that addresses outliers in DiT-based diffusion transformers, in turn improving quantization. They claim three novel contributions as part of CLQ:\n1. A novel cross-block calibration method that can reduce quantization error across blocks\n2. An \"orthogonal-based smoothing\" method, that groups channels into blocks of similar magnitudes, before applying Hadamard to each block\n3. \"Cross-layer parameter searching\" for setting quantization parameters\n\nThey show W4A4 compression with low deterioration and claim SOTA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper has clear strengths:\n* Important problem, from both a cost, energy, and latency perspective, for which even small improvements can have a massive impact\n* W4A4 performance is promising\n* method possibly lighter than methods that use end-to-end training"}, "weaknesses": {"value": "**1. Contribution.**\n\nI have some doubts about the contributions.\n\n**CBC.** It makes sense to take earlier errors into account. However, it seems to me odd that the input *and* the output is changed. Would it not make more sense to keep the target of each block (the FP output of the full FP model) the same, but only change the input (i.e. to the quantized models' previous layers' output)? Now, it seems to me that actually there could still be error quantization---e.g. if previous blocks have some error, it would make sense that the new block would try to \"correct\" for this error (instead of trying to match the FP block's output given the error as input).\n\n**OBS**. I'm not sure I understand OBS and am not convinced by the novelty claim of OBS. Could the authors elaborate on the advantage of the permutation, considering the Hadamard mixes across all channels anyway? Also, the authors write:\n>  [L224] However, previous studies (Lin et al., 2024; Ashkboos et al., 2024) typically adopted dynamic\napproaches to construct the rotation matrix, which are time-consuming and hardware-unfriendly. In\ncontrast, we novelly propose using a static approach to further enhance the role of rotation matrices.\n\nI'm unsure what the authors mean, exactly. QuaRot (Ashkboos et al) use Hadamard transforms, which are completely static. Other works (e.g. SpinQuant, FlatQuant, OstQuant, FPTQuant) train some transformations, but they are also completely fixed during inference. The \"OBS\" method seems to just consist of a permutation (e.g. used in DuQuant) and Hadamard transform (used by almost everyone), so could the authors please elaborate on what the contribution is?\n\n**CLPS.** This seems interesting, but I'm not sure this is better than end-to-end training or even per-block learning of activation and weight clipping (e.g. FlatQuant). Authors state\n> [L269] Directly relying on the final model output for this optimization would be computationally prohibitive, as each layer would require a complete forward pass, and the VAE part needs to be included, which is computationally expensive.\n\nThis argument is incorrect---all layers can be trained at the same time. End-to-end training has been used successfully in e.g. SpinQuant, OstQuant, FPTQuant. It requires just a full forward and backwards pass through the model to update all layer weights, not as authors claim a full forward pass for each *layer* independently. There may still be an advantage of a block-by-block approach (e.g. to keep only one block in memory and increase the batch size), however, the current approach is very expensive---for each layer $L_O$, we require multiple forward passes through that layer *and the next layers*, **and** we need to do this for the different quantization grids of layer $L_O$. This seems quite an elaborate method, that would scale poorly for large candidate grids (i.e. large $S_r$ and $S_l$). It is also unclear how this method would work when there are many quantizers in each block (including channel-wise scaling using different quantizers for each row)---do we find a grid for each quantizer in the block independently? This would scale exponentially in the number of quantizers per block.\n\n**2. Experimental details**\n\nThere are not enough details to understand the results section. For Section 4.2 (Table 1), what quantization setting is used (W4A4?), and how is the Naive baseline implemented (e.g. no transforms at all?)? What data is used for calibrating CLPS and CBC? What is the cost of calibration compared to other methods? \n\n**3. Insufficient evidence for contributions**\n\nThe results look good, but I'm concerned that evidence for the contributions is insufficient. \n1. There are only video tasks, not image, which make it difficult to compare to literature numbers. The Table 3 results for ViDiT-Q don't match their paper's.\n2. Authors also ignore and do not discuss transforms/rotations with learneable parameters, e.g. SpinQuant, FlatQuant, SVDQuant, that perform vastly better than e.g. QuaRot. \n3. I very much appreciate the current paper's ablations, but it would have been very useful to also have a direct comparison with other methods. E.g., (1) OBS vs standard Hadamard vs some learnable transform (e.g DuQuant, but preferably FlatQuant), (2) end-to-end training vs CLPS+CBC vs just per block training, (3) cost of training CLPS+CBC vs cost of training end-to-end vs per-block training but learnable transforms (preferably FlatQuant). I understand you can't do everything under the sun, but I currently do not see enough evidence that OBS is better than existing (trainable) transforms, and that CLPS+CBC is better/cheaper than some light QAT (in particular, per-block training of transforms or short end-to-end training)\n\n\nI'm very much willing to increase my score if the authors address my concerns."}, "questions": {"value": "See Weaknesses. In particular,\n* How does CLQ compare to methods that train transforms (e.g. FlatQuant or OstQuant)\n* How expensive is CLQ to calibrate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mzrGpIKOhP", "forum": "MCnbUQsSmo", "replyto": "MCnbUQsSmo", "signatures": ["ICLR.cc/2026/Conference/Submission3230/Reviewer_szxA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3230/Reviewer_szxA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760527630764, "cdate": 1760527630764, "tmdate": 1762916613192, "mdate": 1762916613192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CLQ as a combination of three post-training quantization steps aiming to improve performance on low bit width regimes. Namely the paper introduces Cross Block Calibration (CBC) to collect more accurate calibration data, Orthogonal-Based Smoothing (OBS) to better spread the outlier channels and Cross-Layer Parameter Search (CLPS) as a local procedure to tune the quantization hyper-parameters. \nOverall the combination of these steps improves the image and video generation quality on the Open-Sora and PixArt-$\\alpha$ architectures, with significant improvements especially in the W4A4 regime."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper introduces a series of changes for the PTQ procedure of DiT architectures that result in substantial improvements in low bit width settings. \n\n* Figure 2 does a good job in summarizing the proposed method, making the general idea clear from the beginning."}, "weaknesses": {"value": "* Some sections of the paper are not clearly described, namely:\n   * The procedure to generate $S$ and $H$ in lines 242-261 is quite ambiguous.\n   * The plots in Figure 3 are not entirely described.  Without an explanation of the quantities in the legend and on the x and y axis, it is quite challenging to interpret. \n* The paper proposes 3 variations of standard PTQ procedures that include many hyper-parameters. However, the paper does not provide much intuition or motivation regarding each choice. The ablations included in the paper are also not covering the impact of each suggested modification in detail. \n* Some crucial details required to interpret and reproduce the results are missing from the main text (see questions)."}, "questions": {"value": "1. What is the intuition behind swapping the channels so that the ones with outliers are mixed together? Is this procedure more effective than doing the opposite (i.e. making sure that the outliers are distributed uniformly in each block to spread them across more channels)? How does OBS compare to vanilla block Hadamard (no S) or Hadamard with random permutations (randomized Hadamard)? How does this change for different block size? \n \n\n2. The paper mentions that, considering the limited depth of one transformer block, during calibration, only the previous block is quantized. What is the reason behind this choice? Does it reduce the calibration time? How much of an impact does it make compared to quantizing all previous blocks instead in terms of the end2end model performance? \n\n \n\n3. Can the authors clarify the reasons behind the choice of using the layer with the largest variance located at most 3 blocks from the current one when setting the quantization range? Why is the L1 norm of the quantization a good target for this procedure? Does this procedure favor layers with larger norm (since the error is also scaled up)?  Can the authors provide more intuition behind this choice and a comparison with block-wise optimization?\n\n4. How much time/compute does the proposed CLQ calibration procedure require compared to the baselines included in this work? Which layers and activations are quantized in each model reported in Table 3 and 4? How is the 3.95x speedup reported in the abstract estimated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gGNcPWMMg2", "forum": "MCnbUQsSmo", "replyto": "MCnbUQsSmo", "signatures": ["ICLR.cc/2026/Conference/Submission3230/Reviewer_nt1d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3230/Reviewer_nt1d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761310263387, "cdate": 1761310263387, "tmdate": 1762916612269, "mdate": 1762916612269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a cross-layer guided orthogonal-based quantization method for DiTs. The approach focuses on optimizing calibration data, rotation matrices, and cross-layer parameter search to improve W4A4 quantization performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed OBS module is shown to be effective through ablation studies.\n- The visualization results are informative and clearly presented."}, "weaknesses": {"value": "- The related work section omits several important DiT quantization baselines, such as SVDQuant, PTQ4DiT, and QDiT. These baselines are also missing from the experimental comparisons.\n- The paper claims a 3.98× memory saving and 3.95× speedup in the abstract and introduction, but no inference results are provided in the main text or appendix to support these claims."}, "questions": {"value": "- Since CBC operates online, what is the additional inference cost introduced in terms of latency?\n- As the proposed method is inspired by DuQuant, how does its performance compare directly with DuQuant under similar settings?\n- Could you clarify the setup used for memory usage and speedup measurements? Also, how do these results compare with baselines such as ViDiT-Q?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WULaiFov79", "forum": "MCnbUQsSmo", "replyto": "MCnbUQsSmo", "signatures": ["ICLR.cc/2026/Conference/Submission3230/Reviewer_tS79"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3230/Reviewer_tS79"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907504704, "cdate": 1761907504704, "tmdate": 1762916611256, "mdate": 1762916611256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a cross-layer guided orthogonal-based quantization method for DiTs. Three key techniques are introduced: cross-block calibration, orthogonal-based smoothing, and cross-layer parameter searching. Experiments were conducted on both image and video generation tasks and demonstrate good results on W4A4 settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The performance at 4-bit (W4A4) is excellent.\n2. Experiments were conducted on both image generation and video generation models, demonstrating the method's generalizability."}, "weaknesses": {"value": "1. The first contribution (cross-block calibration) is a standard practice in LLMs (e.g., GPTQ). It is less common in diffusion models because diffusion calibration requires multi-timestep data (i.e., running a full forward pass, e.g., 50 steps, for each sample). In this paper, quantizing each transformer block involves re-executing this sampling step, which I suspect is extremely slow. The paper also lacks an analysis of algorithmic efficiency.\n\n2. Similarly, the third contribution (cross-layer parameter searching) likely involves the multi-timestep issue. To determine the output error of subsequent layers, is it also necessary to perform a multi-step forward pass?\n\n3. The specific quantizer settings (e.g., granularity, symmetric/asymmetric) are not specified in the main text.\n\n4. The models tested seem somewhat outdated and few in number. I suggest the authors refer to other related works and conduct tests on models such as wan, cognex, and flux.\n\n5. More comparison methods need to be included, such as DVDQuant[1] and SVDQuant[2], which are both post-training methods.\n\n6. The quantization settings for the ablation study are not specified. Furthermore, the \"naive method\" baseline is not defined, which is very confusing. Given the excellent W4A4 results, why not provide the ablation study under W4A4 settings?\n\n7. Regarding the special orthogonal matrix used in the paper, its latency overhead compared to other orthogonality-based methods is not discussed.\n\n[1] DVD-Quant: Data-free Video Diffusion Transformers Quantization\n\n[2] Svdquant: Absorbing outliers by low-rank components for 4-bit diffusion models"}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ge31F0k8f9", "forum": "MCnbUQsSmo", "replyto": "MCnbUQsSmo", "signatures": ["ICLR.cc/2026/Conference/Submission3230/Reviewer_CWXF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3230/Reviewer_CWXF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3230/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975687986, "cdate": 1761975687986, "tmdate": 1762916610982, "mdate": 1762916610982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}