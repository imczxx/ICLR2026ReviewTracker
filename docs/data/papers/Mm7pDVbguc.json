{"id": "Mm7pDVbguc", "number": 7005, "cdate": 1758004712428, "mdate": 1759897878418, "content": {"title": "Fine-tuning with RAG for Improving LLM Learning of New Skills", "abstract": "Large language model (LLM) agents deployed for multi-step tasks frequently fail in predictable ways: attempting actions with unmet preconditions, issuing redundant commands, or mishandling environment constraints. While retrieval-augmented generation (RAG) can improve performance by providing runtime guidance, it requires maintaining external knowledge databases and adds computational overhead at every deployment. We propose a simple pipeline that converts inference-time retrieval into learned competence through distillation. Our approach: (1) extracts compact, reusable hints from agent failures, (2) uses these hints to generate improved teacher trajectories via one-shot retrieval at episode start, and (3) trains student models on these trajectories with hint strings removed, forcing internalization rather than memorization. Across two interactive benchmarks, ALFWorld (household tasks) and WebShop (online shopping), distilled students consistently outperform baseline agents, achieving up to 91\\% success on ALFWorld (vs. 79\\% for baselines) and improving WebShop scores to 72 (vs. 61 for baselines), while using 10-60\\% fewer tokens than retrieval-augmented teachers depending on the environment. The approach generalizes across model scales (7B/14B parameters) and agent architectures (ReAct/StateAct), demonstrating that retrieval benefits can be effectively internalized through targeted fine-tuning without permanent runtime dependencies.", "tldr": "We show that hints extracted from agent failures can be used to train retrieval-augmented teachers and then distilled into students without hints, yielding agents that match or exceed RAG performance on ALFWorld and WebShop while fewer tokens.", "keywords": ["Agentic AI", "LLM", "Prompt Distillation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2411ae3a9f8334f934a387475383e85ea8ae2da8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper reframes retrieval-augmented generation (RAG) as a training-time teacher rather than a runtime dependency, which is an interesting idea. The authors propose a four-stage pipeline, and the resulting “distilled” agents internalize the retrieval benefits and achieve better performance while using fewer tokens per episode."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, the main message that retrieval can be treated as a tempoaray training teacher rather than a permanent runtime component is interesting and useful. The paper is clearly written and the automatic pipeline seems practical and deployment-friendly."}, "weaknesses": {"value": "1. Generalization: The main concern lies in the generalization of the findings. As the authors acknowledge, they evaluate only on two text-based environments (ALFWorld and WebShop) using Qwen-2.5 7B/14B as the base models and single-seed runs. Although the reported results are consistent, it would be more convincing to include evaluations on more diverse benchmarks and multiple random seeds.\n2. Re-ranking biase: The use of a quantized Qwen-2.5 7B model for scoring hint relevance may introduce bias, as it belongs to the same model family as the student and teacher agents. It would be preferable to use a different model family or an unquantized version for re-ranking to reduce potential bias.\n3. Training costs: The training process relies on repeated GPT-4o calls for hint generation and the quantized Qwen-2.5 7B for hint scoring. This setup likely increases training costs and may slow down the process due to waiting for external feedback."}, "questions": {"value": "1. Does the quantized Qwen-2.5 7B provide reliable hint ranking? Is there any ablation study on the effect of hint ranking or retrieval quality?\n2. Have the authors tested whether the distilled model maintains its advantage when evaluated on unseen environments or different task distributions? Could the retrieved hints unintentionally leak task-specific answers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "S0pv1N7Td8", "forum": "Mm7pDVbguc", "replyto": "Mm7pDVbguc", "signatures": ["ICLR.cc/2026/Conference/Submission7005/Reviewer_pQJR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7005/Reviewer_pQJR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861780146, "cdate": 1761861780146, "tmdate": 1762919218171, "mdate": 1762919218171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method to **distill retrieval-augmented generation (RAG)** into fine-tuned LLMs, eliminating runtime retrieval while retaining its benefits. By extracting corrective hints from agent failures and using them once during training to generate improved teacher trajectories, the model learns to internalize guidance instead of depending on external retrieval. Tested on ALFWorld and WebShop, the distilled models achieve **notably higher success (up to 91%) and efficiency (10–60% fewer tokens)** than baselines. The approach is **simple, effective, and generalizes across architectures**, though scalability and evaluation breadth remain limited."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation: The paper tackles a real limitation of RAG—its runtime cost and dependency on external databases—by proposing a training-time alternative that internalizes retrieval benefits.\n\n2. Strong empirical results: Distilled models achieve 91% success on ALFWorld (vs. 79% baseline) and 72.4 score on WebShop (vs. 60.9 baseline) while using 10–60% fewer tokens.\n\n3. Generalization across setups: Demonstrated effectiveness for both 7B and 14B models and two agent architectures (ReAct, StateAct)."}, "weaknesses": {"value": "* **Limited novelty:** While well-executed, the core idea—distilling knowledge from prior retrieval or failures—is conceptually close to existing self-improvement and distillation frameworks (e.g., FireAct, Reflexion). The contribution lies more in engineering and integration than in fundamental innovation.\n* **Narrow experimental scope:** All experiments are conducted only on **Qwen-2.5 models (7B/14B)**, raising concerns about generality across architectures (e.g., Llama, Mistral etc).\n* **Dependence on GPT-4o for hint extraction:** The pipeline still relies on a powerful external model to generate failure-driven hints, which may limit scalability and reproducibility."}, "questions": {"value": "1. why the distilled student model performs better than the teacher model (RAG)? Are there any hyper-parameter optimization included?\n2. why use LoRA instead of full-finetuing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EVetcI1L5B", "forum": "Mm7pDVbguc", "replyto": "Mm7pDVbguc", "signatures": ["ICLR.cc/2026/Conference/Submission7005/Reviewer_6c7r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7005/Reviewer_6c7r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762035431699, "cdate": 1762035431699, "tmdate": 1762919217751, "mdate": 1762919217751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Despite their strong performance on reasoning benchmarks, large language models (LLMs) tend to perform poorly on multi-step tasks without preconditioning on task actions and environment state spaces. LLMs augmented with retrieval-augmented generation (RAG) have demonstrated strong performance but require maintaining external knowledge bases and incur additional computational cost at runtime. In this work, the authors propose a framework that generates improved reasoning trajectories from a teacher model using RAG for hint selection. These generated trajectories are then used to enhance the performance of a student model on interactive reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of this paper are listed below:\n1. This work successfully demonstrates that the hints generated with the help of hints lead to better performance.\n2. The distilled model demonstrates better token efficiency."}, "weaknesses": {"value": "The weaknesses of this work are as follows:\n\n1. Supervised Fine-Tuning (SFT) is known to cause overfitting to the training domain; hence, the observed performance gains may not generalize well.\n\n2. The performance of RAG remains low on both benchmarks. It would be valuable to explore how these failure cases could be leveraged in RL training to learn more generalizable reasoning patterns.\n\n3. The hints are generated using API calls to an external LLM, which raises questions about the quality and reliability of the generated hints.\n\n4. An LLM is also used for reranking the retrieved hints, but it is unclear why a more traditional and cost-effective retrieval pipeline was not employed instead.\n\n5. For the 14B model, the performance improvement is evident only on the ALFWorld benchmark.\n\n6. The approach is evaluated on only two benchmarks; extensive experiments on additional datasets are necessary to validate its general applicability."}, "questions": {"value": "1. Please outline the benefits of using an LLM for hint reranking and retrieval.\n\n2. Please provide additional information regarding the quality of the hints generated by the LLM.\n\n3. Please include evaluations on additional challenging interactive benchmarks, such as WebArena [1].\n\n[1] Zhou, Shuyan et al. “WebArena: A Realistic Web Environment for Building Autonomous Agents.” arXiv abs/2307.13854 (2023)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MU594MsRP4", "forum": "Mm7pDVbguc", "replyto": "Mm7pDVbguc", "signatures": ["ICLR.cc/2026/Conference/Submission7005/Reviewer_zA5j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7005/Reviewer_zA5j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045997798, "cdate": 1762045997798, "tmdate": 1762919217224, "mdate": 1762919217224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel approach to *retrieval-augmented fine-tuning* where Retrieval-Augmented Generation (RAG) is not used merely at inference but as a **training-time teacher**. The authors introduce a pipeline that leverages **failure-driven hint extraction** and **hint-based RAG distillation** to improve the downstream agent’s reasoning and efficiency without requiring retrieval at runtime.\n\nThe method operates in three main stages:\n\n1. **Failure Mining and Hint Extraction** – The base agent’s failed trajectories are analyzed using GPT-4o to produce short, typed *hints* (e.g., “try opening the `{container}` before placing `{object}`”), generalizing the error into reusable advice.\n2. **Hint Retrieval and RAG Teacher Generation** – At training time, a small set of relevant hints ((k \\leq 3)) is retrieved once per episode and injected into the teacher prompt. The RAG teacher then produces improved, successful trajectories.\n3. **Hint-Removed Distillation** – The student model (fine-tuned with LoRA) is trained on the teacher’s improved trajectories, but with hints *removed*, encouraging it to internalize the corrections rather than rely on retrieval tokens.\n\nEmpirical results on **ALFWorld** and **WebShop** demonstrate that the distilled students achieve comparable or superior success rates to RAG-based agents, e.g., up to *91% success* on ALFWorld while reducing inference token usage and removing retrieval dependency entirely.\n\nThe core insight is that *retrieval can serve as a teacher rather than a crutch*: by extracting and reusing failure-derived knowledge during training, models can **inherit RAG’s strengths while maintaining fine-tuned efficiency**.\n\nThis perspective is complementary to methods such as **RAFT (Retrieval-Augmented Fine-Tuning)**, which enhance RAG performance at inference time; in contrast, this work uses RAG supervision to **eliminate the need for retrieval at deployment**."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. **Conceptual Novelty (Turning RAG into a Training-Time Teacher)**\n   The paper introduces a genuinely fresh perspective on retrieval-augmented learning. Instead of relying on RAG *at inference*, it reinterprets RAG as a *supervision mechanism* using retrieved, failure-derived hints to generate improved demonstrations and distilling them into the model. This effectively *converts external retrieval into internal reasoning skill*, a direction that feels both original and timely.\n\n2. **Failure-Driven Knowledge Extraction Pipeline**\n   The automatic hint extraction from failed trajectories (via GPT-4o) is one of the paper’s strongest contributions. By converting raw failure logs into short, typed hints with placeholders, the method operationalizes the idea of *learning from one’s own mistakes*. This makes the system data-efficient and highly interpretable, i.e., each hint represents a distilled form of experiential correction.\n\n3. **Practical and Deployment-Friendly Design**\n   The approach strikes an excellent balance between performance and efficiency. The distilled student models retain the accuracy of RAG agents while dropping retrieval costs and context overhead. In practice, this means fewer API calls, lower latency, and more predictable runtime behavior which are key deployment benefits often ignored in academic RAG work.\n\n4. **Empirical Results Support the Hypothesis**\n   On both **ALFWorld** and **WebShop**, the results clearly show that the distilled agents outperform or match the RAG baseline while using fewer tokens. The ablations (e.g., number of retrieved hints (k), LoRA scale) are sensible and validate design choices. The efficiency frontier plots strongly convey that the gains are not just anecdotal but structural.\n\n5. **Methodological Clarity and Reproducibility**\n   Despite involving multiple moving parts (hint extraction, re-ranking, teacher generation, and LoRA distillation), the paper presents its pipeline in a clean, reproducible manner. Each stage is modular, making it easy to replicate or adapt for other domains.\n\n6. **Complementarity with RAFT and Other RAG–Fine-Tuning Hybrids**\n   From a broader research standpoint, this work complements approaches like **RAFT**. While RAFT fine-tunes models to use retrieval *during inference*, this paper focuses on distilling retrieval’s benefits *into the model weights*. This complementarity opens exciting ground for hybrid pipelines, i.e., RAFT for retrieval robustness; this is for retrieval-free competence.\n\n7. **High Potential for Generalization Beyond Current Benchmarks**\n   The approach is architecture-agnostic and task-flexible. Any domain with observable failures (reasoning, code, robotics, or web agents) could feasibly benefit from failure-driven hint distillation. It’s a general recipe for turning external supervision into internal skill formation."}, "weaknesses": {"value": "1. **Dependence on Expensive Hint Generation (GPT-4o)**\n   The hint extraction process relies heavily on GPT-4o to produce concise, structured hints from failed trajectories. While conceptually elegant, this introduces a non-trivial *cost and dependency barrier*. It’s unclear how well the pipeline performs if the hint generator is weaker (e.g., Claude, GPT-3.5, or an open-source model). An ablation on hint-source quality would be crucial to validate the approach’s robustness and accessibility.\n\n2. **Limited Domain Scope and Missing Generalization Evidence**\n   Evaluation is restricted to **ALFWorld** and **WebShop**, both of which are text-based and relatively synthetic. There’s no evidence of transfer to other environments or domains (e.g., tool-use, programming, or reasoning tasks). Without cross-domain or out-of-distribution experiments, it’s difficult to gauge whether the distilled behaviors generalize or merely specialize to the training distribution.\n\n3. **Single-Seed Results and Lack of Statistical Rigor**\n   The reported results appear to rely on single-seed runs. Given the stochasticity of both retrieval and LLM rollouts, variance across seeds could be substantial. Multi-seed evaluation and confidence intervals would strengthen empirical reliability.\n\n4. **One-Shot Retrieval Constraint**\n   The design retrieves hints only *once* at episode start. While this simplifies the setup, it limits adaptivity in longer-horizon or dynamic tasks where new context emerges mid-trajectory. This constraint could lead to brittle performance in environments requiring sequential replanning or dynamic retrieval updates.\n\n5. **Potential Overfitting to Hint Styles**\n   Even though hints are removed during student fine-tuning, the student may still internalize surface-level templates or stylistic priors specific to the GPT-4o hint format. The absence of tests on noisy, paraphrased, or conflicting hints leaves open the question of whether the distilled competence reflects *true abstraction* or *pattern mimicry*.\n\n6. **Missing Comparison to RAFT or Other RAG-Fine-Tuning Baselines**\n   The paper positions itself as complementary to RAFT (Retrieval-Augmented Fine-Tuning), yet no direct empirical comparison is provided. A controlled RAFT baseline or a hybrid experiment where RAFT pretraining precedes hint distillation would help clarify relative strengths. Without this, the distinction remains conceptual rather than quantitative.\n\n7. **Scalability and Practicality of the Failure Mining Loop**\n   The method assumes the availability of structured failure logs and deterministic access to environment feedback. This is feasible in ALFWorld-like simulators but less practical in noisy real-world pipelines (e.g., user-facing chat agents). Some discussion or lightweight simulation of such conditions would make the method’s applicability clearer.\n\n8. **Lack of Long-Term Behavior Evaluation**\n   The experiments measure immediate task success, but not whether the distilled policies preserve *stability* or *general reasoning improvements* over multiple episodes or domains. A longitudinal or continual-learning evaluation would have given deeper evidence that the model truly internalized RAG-derived knowledge."}, "questions": {"value": "1. **Hint Generator Robustness**\n   How sensitive is your pipeline to the quality of the hint generator?\n\n   * Have you tested weaker or smaller LLMs (e.g., GPT-3.5, Claude, or open-source models like Mistral-7B) for hint extraction?\n   * If performance degrades, what specific aspects, i.e., hint conciseness, structure, or accuracy, most affect downstream learning?\n     *A convincing ablation here would clarify whether the pipeline is viable without expensive closed-source LLMs.*\n\n2. **Cross-Domain Generalization**\n   Can the distilled agent transfer knowledge from one environment to another?\n\n   * For instance, could a model trained on ALFWorld hints generalize to unseen household task variations or WebShop-like reasoning tasks without additional RAG?\n   * Do the hints encode reusable “task primitives” or are they too domain-specific?\n     *Understanding the degree of reusability of the learned hints would help assess the true scope of this method.*\n\n3. **Statistical Rigor and Variance Reporting**\n   Are the reported results averaged over multiple seeds?\n\n   * Given that retrieval and LLM outputs introduce randomness, have you measured performance variance across runs?\n   * If not, could the observed improvements (e.g., 91% vs. 82%) be within expected variance margins?\n     *Providing mean ± std metrics would significantly improve credibility.*\n\n4. **Comparison to RAFT and Retrieval-Based Fine-Tuning Methods**\n   Have you considered comparing directly to RAFT or hybridizing with it?\n\n   * For instance, pretraining the backbone with RAFT, then applying your hint-distillation pipeline to internalize common retrieval patterns?\n   * Do you expect the two methods to be additive, or would RAFT’s retrieval-conditioned fine-tuning conflict with your retrieval-free student training?\n     *A small-scale RAFT baseline or hybrid experiment would clarify the relationship and strengthen your positioning.*\n\n5. **One-Shot Retrieval Limitation**\n   Why restrict hint retrieval to a single step at episode start?\n\n   * Did you attempt multi-step retrieval or adaptive hint updates mid-trajectory?\n   * If not, is this primarily a cost/complexity trade-off or a deliberate design choice to enforce self-sufficiency?\n     *Clarifying this would help understand whether the framework can extend to more dynamic, long-horizon tasks.*\n\n6. **Overfitting and Hint Style Bias**\n   The student model is trained on teacher trajectories with hints removed—but could it still overfit to stylistic patterns introduced by the RAG teacher?\n\n   * Have you tested robustness under paraphrased or intentionally noisy hints?\n   * Do hints produced by different LLMs (with different tone or structure) affect final student performance?\n     *This would clarify whether the distilled competence reflects genuine abstraction rather than surface imitation.*\n\n7. **Scalability of Failure Mining in Real-World Systems**\n   Your pipeline assumes access to structured failure trajectories and outcome signals.\n\n   * How feasible is it to apply this approach to free-form, user-facing LLM interactions where “failure” is subjective or weakly labeled?\n   * Could implicit feedback (e.g., user corrections or re-prompts) serve as proxy failures for hint extraction?\n     *Discussing this would extend the method’s relevance beyond simulator benchmarks.*\n\n8. **Long-Term Learning and Continual Adaptation**\n   Once a student is distilled, can the pipeline be repeated iteratively as new failures appear?\n\n   * Does performance plateau after one distillation round, or can further RAG-assisted hint extraction continue improving the student?\n     *Evidence of continual learning potential would make the approach even more compelling.*\n\n9. **Compute and Efficiency Accounting**\n   Could you share concrete compute/token usage breakdowns for each stage (failure mining, hint extraction, RAG teacher runs, student fine-tuning)?\n\n   * This would help readers balance efficiency gains at inference against additional training overhead.\n\n10. **Interpretability of the Learned Knowledge**\n    Since hints are typed and structured, have you tried aligning the student’s internal representations with these hint categories post-training?\n\n     * For example, probing whether the model has implicitly learned “open-container” or “check-object-state” concepts.\n     *Such analysis could offer a unique window into how RAG-derived supervision shapes internal reasoning.*"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xEhjMnaeuO", "forum": "Mm7pDVbguc", "replyto": "Mm7pDVbguc", "signatures": ["ICLR.cc/2026/Conference/Submission7005/Reviewer_yhnQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7005/Reviewer_yhnQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762108523867, "cdate": 1762108523867, "tmdate": 1762919216770, "mdate": 1762919216770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}