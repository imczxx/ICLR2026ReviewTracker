{"id": "86PhVA7veh", "number": 21708, "cdate": 1758320724097, "mdate": 1759896907634, "content": {"title": "Change of Thought: Adaptive Test-Time Computation", "abstract": "Standard Transformers apply a fixed amount of computation to every token, limiting their expressive power, while more powerful iterative approaches often introduce significant architectural complexity and cost. We introduce Fixed-Point Self-Attention (FPSA), a parameter-free, drop-in replacement for self-attention that enables a model to adaptively ``think longer'' by iteratively refining each layer's representations to a fixed point. We train this recurrent process end-to-end using implicit differentiation, ensuring that memory usage during training and inference remains constant and identical to a standard Transformer layer, regardless of the number of refinement steps. Without adding any parameters, FPSA significantly improves strong baselines like BERT-Base and ELECTRA-Base on the GLUE and SQuAD v2.0 benchmarks. We demonstrate similar consistent gains for vision (ViT-B/16) and vision-language models, achieving accuracy improvements of up to 20\\%. This performance boost comes at a modest computational cost: a median of 3--6 refinement steps results in a $\\approx1.6\\times$ GFLOPs and $\\approx1.3-1.4\\times$ latency overhead compared to an equivalent BERT-Base model. Analysis shows FPSA dynamically allocates compute to challenging inputs and converges to stable fixed points. Furthermore, integrating FPSA into language models improves performance on complex reasoning tasks like GSM8K, BBH, and LogiQA. Ultimately, FPSA bridges the gap between fixed-computation and iterative reasoning, offering a powerful building block that adaptively allocates compute while preserving architectural simplicity.", "tldr": "FixedPoint SelfAttention replaces token autoregression with latent alignment refinement enabling iterative reasoning within self-attention,by updating alignment matrices through fixed-point iterations, it achieve 25% improvement with parameters reuse", "keywords": ["deep learning architecture", "fixed point iteration", "adaptive test-time"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6bde904f12569564ccb5cfd150c5f71ff9e17714.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Fixed-Point Self-Attention (FPSA), a parameter-free and computationally adaptive replacement for the standard self-attention mechanism in Transformers. Instead of applying a fixed number of transformations per token, FPSA iteratively refines the attention outputs until convergence to a fixed point, allowing “adaptive computation” per token or per head. The method uses implicit differentiation to train efficiently, maintaining a constant memory footprint regardless of iteration depth. Empirical results show notable improvements across multiple domains: NLP (GLUE, SQuAD v2.0), vision (ImageNet, image restoration), and multimodal tasks, as well as reasoning benchmarks for large language models (GSM8K, BBH). The approach claims up to +20% relative accuracy improvements with ~1.3–1.6× compute cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper evaluates FPSA across multiple settings — encoder-only Transformers, decoder-only LLMs, and vision/multimodal models — showing consistent gains.\n\n2. The authors provide a clear convergence analysis showing contractivity of attention mappings under spectral normalization and pre-LN. The implicit differentiation approach is rigorously justified."}, "weaknesses": {"value": "1. While appendices provide algorithm sketches, more implementation-level specifics (e.g., PyTorch pseudocode, convergence thresholds per dataset, training schedules) are needed for full reproducibility.\n\n2. The main baselines (BERT, ELECTRA) are standard, but the paper does not compare against other recent adaptive computation methods (e.g., ACT, MoD, or LayerDrop) on exactly matched compute budgets.\n\n3. The ablations focus on iteration counts and convergence, but lack sensitivity studies for hyperparameters such as spectral norm bound σ, halting threshold ϵ, or gradient clipping T.\n\n4. Results for 7B models are promising but brief; the paper would benefit from discussion of FPSA behavior at larger scales (70B+) or under distributed inference constraints.\n\n5. For long-context tasks, does FPSA’s advantage hold beyond 8k tokens (e.g., 32k)?"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l7Qsd4CmEl", "forum": "86PhVA7veh", "replyto": "86PhVA7veh", "signatures": ["ICLR.cc/2026/Conference/Submission21708/Reviewer_2aHb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21708/Reviewer_2aHb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761358171799, "cdate": 1761358171799, "tmdate": 1762941898418, "mdate": 1762941898418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Fixed-Point Self-Attention (FPSA)—a parameter-free, drop-in replacement for self-attention that iteratively refines the attention outputs inside each layer until a fixed point, trained end-to-end via implicit differentiation so the memory footprint is constant w.r.t. the number of refinement steps. Empirically, swapping standard attention for FPSA improves strong, size-matched baselines (e.g., BERT-Base, ELECTRA-Base) on GLUE/SQuAD, shows gains for ViT and VL models, and yields small but consistent boosts when integrated into 7B decoder-only LLMs (LLaMA-2/Mistral) on GSM8K/BBH/LogiQA—without adding parameters. Overhead is modest (median ~3–6 inner steps, ~1.6× GFLOPs and ~1.3–1.4× latency vs. BERT-Base)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed structure is simple and maintains constant memory. Iterative refinement within the attention sublayer trained via implicit differentiation; avoids storing the inner unroll and heavy checkpointing. Architectural simplicity is preserved. \n\n2. The proposed structure improves the performance with the same parameters.  It improves size-matched encoder baselines on GLUE/SQuAD and shows benefits for ViT/VL, supporting generality.\n\n3. The writing of the paper is clear. It clearly conveys the main idea of the new structure."}, "weaknesses": {"value": "1. The main idea of the proposed iterative structure is to scale the computation. In other words, the performance gain comes with the increased training and inference computation. The training cost is not reported in the current work. The inference computation comparison is missing. Almost all the results in the paper show that the proposed structure performs better than vanilla attention with more computation. However, a fair comparison is to constrain the computation budget of both models, which is missing in the current work.\n\n2. The baselines for the proposed algorithm are not sufficient. The ''iterative'' or ''looped'' structures are widely proposed in the LLM community, .e.g, the looped transformer [1]. Why we only iterate the attention calculation instead of the other parts or the whole network? Such a comparison is not presented.\n\n3. The paper considers some non-causal transformers, .e.g., ViT. However, a line of works shows that pure attention iteration suffers from low-rankness [2]. This is exactly the proposed structure does. Whether the proposed structure also suffer from the low-rankness? If not, please explain the reason.\n\n[1] Giannou A, Rajput S, Sohn J, et al. Looped transformers as programmable computers[C]//International Conference on Machine Learning. PMLR, 2023: 11398-11442.\n\n[2] Dong Y, Cordonnier J B, Loukas A. Attention is not all you need: Pure attention loses rank doubly exponentially with depth[C]//International conference on machine learning. PMLR, 2021: 2793-2803."}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f8ebac1SuW", "forum": "86PhVA7veh", "replyto": "86PhVA7veh", "signatures": ["ICLR.cc/2026/Conference/Submission21708/Reviewer_Wvsf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21708/Reviewer_Wvsf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761673427205, "cdate": 1761673427205, "tmdate": 1762941898115, "mdate": 1762941898115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Fixed-Point Self-Attention (FPSA), a new Transformer-based architecture based on fixed-point iteration. This pushes the individual layer to be in fixed point, and this basically works like looping while value state is fixed. The authors extensively experimented across various settings to support FPSA's superiority."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed approach seems quite reasonable and good, based on fixed point iteration method.\n- This paper has comprehensive analysis and results, to show how FPSA actually works and the performance under various downstream tasks."}, "weaknesses": {"value": "- First, I feel like the overall presentation could be much enhanced. Some useful explanation and experimental results are hidden in Appendix parts (e.g., Fixed-point iteration is not explained in main body properly, some qualitative results as well). It would be good to re-place contents clearly for the reader.\n- As the author mentioned, this mechanism seems having high relation to recursive / looped transformer architecture. There is lack of discussion to recent papers. And I'm curious about comparison to them. For example, comparison with Figure 1c or 1d, and ablation results without fixing value state.\n- All layers seem like being updated by fixed-point iteration. What will happened if you select certain layers only? Some redundancy can be existed like current architecture, but this pushed too much for all layers.\n- With this paradigm, can pruning be more critical? I feel like redundancy gets disappeared for each layer, which means that we can fully leverage the \"depth\" of models while loosing some chances to prune.\n- Selecting some layers would be the solution though, inference latency and throughput seem inefficient. Especially, how could you deal with batched inference? Some tokens should wait for the other incomplete tokens at certain depth always."}, "questions": {"value": "See above Weakness parts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qUBGJQWpza", "forum": "86PhVA7veh", "replyto": "86PhVA7veh", "signatures": ["ICLR.cc/2026/Conference/Submission21708/Reviewer_Tf1q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21708/Reviewer_Tf1q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891570353, "cdate": 1761891570353, "tmdate": 1762941897790, "mdate": 1762941897790, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Fixed-Point Self-Attention, a parameter-free, drop-in replacement for the standard self-attention mechanism. The core idea is to iteratively refining the attention alignment matrix within a single layer until it converges to a fixed point. The authors propose to train end-to-end using implicit differentiation, which keeps memory usage constant."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper offers a more granular approach to adaptive computation compared to prior work that typically repeats entire blocks.\n\n2. The use of implicit differentiation is a major technical strength, making the iterative approach practical by avoiding the memory explosion that would occur with standard backpropagation through time. The compute-matched comparisons (Appendix G) is nice."}, "weaknesses": {"value": "1. The claim of being parameter-free is a bit misleading. While FPSA adds no new learnable model weights, it introduces several crucial hyperparameters that require tuning: the convergence tolerance $\\epsilon$, the maximum number of iterations $K_max$, and the gradient clipping threshold. The learned halting variant (FPSA-LH) further adds a small gating MLP and a ponder cost hyperparameter. The paper lacks a sensitivity analysis for these hyperparameters, which seem critical to the method's performance and efficiency."}, "questions": {"value": "1. Can authors provide a trade-off between the additional number of layers required in simple transformer layer compare to the fixed-point approach proposed here to match the same loss? Mainly, having more iteration seems that should reduce the number of layers in total. But it has not been really discussed in terms of achieving the same or comparable loss.\n\n2. Could authors elaborate why they decided to choose a static value across iterations? What happens if it also gets updated?\n\n3. In Table 4, it doesn't seem that Self-Transformer is providing any helpful improvement, and the enhancement in SR could also be just due to the increase of FLOPs. Can authors compare the results in this table more fairly?\n\n4. Minor typo: Eq 1, I think it should be concatenation over $h$. In Table 5, I think for Top1 acc, the underline should be for the first row.\n\n5. A clarification question: What are the colors in right plot of Fig 3?\n\nSuggestions:\nI would suggest to include the compute-matched results of Appendix G in the main paper. It makes the point of using the approach more clear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n41pncXylJ", "forum": "86PhVA7veh", "replyto": "86PhVA7veh", "signatures": ["ICLR.cc/2026/Conference/Submission21708/Reviewer_J788"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21708/Reviewer_J788"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21708/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976982292, "cdate": 1761976982292, "tmdate": 1762941897509, "mdate": 1762941897509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response by Authors"}, "comment": {"value": "The authors sincerely thank all reviewers (J788, Tf1q, Wvsf, 2aHb) for their insightful and rigorous feedback. We are encouraged that reviewers recognized the **generality** of our approach (Reviewer 2aHb), the **technical strength of implicit differentiation** (Reviewer J788), and the **comprehensive analysis** across domains (Reviewer Tf1q).\n\nThe review process has been incredibly productive. Based on your collective feedback regarding **compute-matched fairness, architectural novelty, and scaling**, we have pointed out the supporting experiments in the appendices and have additionally generated **three additional experimental results** and clarified key theoretical distinctions.\n\n### 1. Major Clarification: The \"Compute-Matched\" Pareto Frontier\nA shared concern (Reviewers Wvsf, 2aHb, J788) was whether FPSA provides efficiency gains or simply uses \"more compute.\"\n* We direct attention to **Appendix G and Figure 4**, which explicitly plot **Accuracy vs. GFLOPs**.\n* **Result:** FPSA establishes a new Pareto frontier. At every fixed budget (41, 48, 59 GFLOPs), FPSA outperforms non-adaptive baselines (Deeper/Wider Transformers) and adaptive baselines (ACT, Depth-Adaptive). This confirms that our gain comes from *better allocation* of compute, not just *more* compute.\n\n### 2. New Experiments Conducted During Rebuttal\nTo address specific concerns about scaling, efficiency, and theory, we have added the following results:\n\n**A. Zero-Shot Context Extrapolation (Addressing Reviewers 2aHb, Wvsf)**\n* *Question:* Does the advantage hold beyond 8k tokens?\n* *New Result:* We evaluated a model trained on 8k context on unseen lengths (**16k, 32k, 64k**) by adaptively increasing the iteration cap.\n* *Outcome:* While the fixed-depth baseline collapsed (**3.2% recall @ 64k**), FPSA maintained robust performance (**71.4% recall @ 64k**) purely by \"thinking longer\" to filter noise. This validates FPSA as a mechanism for length generalization.\n\n**B. Empirical Rank Analysis (Addressing Reviewer Wvsf)**\nUnlike a stack of pure attention layers which collapses to rank $\\approx 0.01$, FPSA maintains a healthy rank (**>0.92**) even after 24 iterations. This confirms that our architectural choices (Static Value $V$, Residuals, Pre-LN) successfully prevent representation collapse.\n\n**C. Training Throughput & The \"Memory Wall\" (Addressing Reviewer Tf1q)**\nWhile FPSA is slower per-sample, its **Constant $O(1)$ Memory** (via Implicit Differentiation) allows for **4x larger batch sizes** (256 vs 64) before OOM compared to a Deeper Baseline. This results in **+22% higher total token throughput** during training.\n\n### 3. Structural Novelty: Why Iterate *Inside* the Head?\nReviewers Tf1q and Wvsf asked for comparisons to **Looped Transformers/Universal Transformers**. We clarify that FPSA is distinct because it decouples **Alignment Recurrence** from **Feature Recurrence**.\n* **Looped Transformers:** Iterate the whole block (Attention+MLP). High cost per step, unstable training.\n* **FPSA (Ours):** Iterates only the **Alignment Matrix** ($\\mathcal{A}$) while keeping Values ($V$) static.\n* *Benefit:* This ensures **Local Contractivity** (guaranteeing convergence) and allows us to use Implicit Differentiation (constant memory), which is mathematically intractable for full-block recurrence.\n\n### 4. Presentation Improvements\nWe agree with Reviewers Tf1q and J788 that key insights were buried. In the updated revision, we have:\n* Moved the **Fixed-Point formulation** to the main Method section.\n* Promoted **Figure 10 (Token-wise Convergence Heatmap)** to the main body to qualitatively prove that the model allocates compute to semantic complexity.\n\nWe believe these new results and clarifications conclusively address the concerns raised, demonstrating that FPSA is a stable, efficient, and theoretically sound advance in adaptive computation."}}, "id": "rxILRwF10t", "forum": "86PhVA7veh", "replyto": "86PhVA7veh", "signatures": ["ICLR.cc/2026/Conference/Submission21708/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21708/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission21708/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763490214037, "cdate": 1763490214037, "tmdate": 1763493589189, "mdate": 1763493589189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}