{"id": "Zz2gtWX8wn", "number": 9929, "cdate": 1758149712300, "mdate": 1763749692152, "content": {"title": "ReviewScore: Misinformed Peer Review Detection with Large Language Models", "abstract": "Peer review serves as a backbone of academic research, but in most AI conferences, the review quality is degrading as the number of submissions explodes. To reliably detect low-quality reviews, we define misinformed review points as either \"weaknesses\" in a review that contain incorrect premises, or \"questions\" in a review that can be already answered by the paper. We verify that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce ReviewScore indicating if a review point is misinformed. To evaluate the factuality of each premise of weaknesses, we propose an automated engine that reconstructs every explicit and implicit premise from a weakness. We build a human expert-annotated ReviewScore dataset to check the ability of LLMs to automate ReviewScore evaluation. Then, we measure human-model agreements on ReviewScore using eight current state-of-the-art LLMs and verify moderate agreements. We also prove that evaluating premise-level factuality shows significantly higher agreements than evaluating weakness-level factuality. A thorough disagreement analysis further supports a potential of fully automated ReviewScore evaluation.", "tldr": "We introduce ReviewScore, a new evaluation of peer review quality, focusing on detecting misinformed review points.", "keywords": ["Peer Review Evaluation", "Argument Evaluation", "Critical Thinking", "Logic", "Large Language Models", "Neurosymbolic Approaches"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ebd8a879a2e77b129f526e020155dc8d80c1c7e8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the degrading quality of peer review in AI conferences, a problem driven by the explosive growth in paper submissions. The authors aim to reliably detect low-quality review by defining \"misinformed review points\" as either reviewer \"questions\" that are already answerable by the paper or \"weaknesses\" that are incorrect or based on incorrect premises. The primary contribution is ReviewScore, a new evaluation criterion to automatically identify these misinformed points. The paper introduces base ReviewScore for foundational scoring and a more effective advanced ReviewScore, which breaks arguments down to evaluate the factuality of each individual premise. To enable this, the authors also propose an automatic argument reconstruction engine that uses LLMs to extract all explicit and implicit premises from a review. To validate this automated system, the authors built a human expert-annotated ReviewScore dataset and measured the performance of eight state-of-the-art LLMs against it. The results showed moderate human-model agreement and confirmed that evaluating premise-level factuality is significantly more reliable than evaluating weakness-level factuality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper introduces a refined approach to reviewing AI conference papers by providing a more detailed examination of each review point. \n* The paper proposes the ADVANCED REVIEWSCORE, which evaluates the factuality of individual premises within arguments, proving to be more effective than simply evaluating weaknesses at a high level."}, "weaknesses": {"value": "1. While the paper introduces the REVIEWSCORE system to detect misinformed review points, its classification of errors into two categories: questions and weaknesses, appears too simplistic. There are likely more nuanced types of errors in peer reviews that this system does not currently address. \n2. Although the paper presents REVIEWSCORE to detecting misinformed review points, it fails to adequately compare its method with existing peer review evaluation systems.\n3. The paper’s experimental evaluation relies on a small and homogeneous dataset consisting of 40 ICLR submissions. While this serves as a proof of concept, the limited size and scope of the dataset restrict its generalizability. The dataset's lack of diversity in terms of conferences and academic domains may not reflect the wide range of review styles and subjects encountered across various fields."}, "questions": {"value": "1. What is the rationale behind limiting the classification to just these two categories? Are there other types of misinformed review points, such as misunderstandings of methodologies, misinterpretation of results, or reviewer biases, that could be considered?\n2. Could you provide a more in-depth comparative analysis to highlight the strengths and limitations of REVIEWSCORE in relation to other state-of-the-art review quality assessment tools? How does REVIEWSCORE perform in comparison to other AI-based approaches for identifying misinformed review points?\n3. How do you plan to address the generalizability of your findings given the limited scope of the dataset? Additionally, there could be biases introduced during the human annotation process. Could you expand the dataset to include a wider range of conferences and academic domains to ensure robustness? Also, could the inclusion of external data sources (e.g., citation counts, feedback from future users) strengthen the accuracy and completeness of the dataset?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7xc4pJPceE", "forum": "Zz2gtWX8wn", "replyto": "Zz2gtWX8wn", "signatures": ["ICLR.cc/2026/Conference/Submission9929/Reviewer_ywZt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9929/Reviewer_ywZt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795533397, "cdate": 1761795533397, "tmdate": 1762921382050, "mdate": 1762921382050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces REVIEWSCORE, a framework to automatically detect misinformed review points in peer reviews from AI conferences. A misinformed review point is defined as either (1) a question already answered by the paper or (2) a weakness based on incorrect or unsupported claims. The authors develop two scoring systems: BASE REVIEWSCORE and ADVANCED REVIEWSCORE\n\nThe authors construct a human-annotated dataset of review points from 40 ICLR 2021–2023 papers, labeled by 15 graduate students. They then evaluate agreement between these human annotations and predictions from 8 SOTA LLMs (GPT-4, Claude 2, PaLM 2, Gemini, etc.), finding that ADVANCED REVIEWSCORE achieves higher alignment than BASE, especially in weakness detection."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Well-defined problem: The concept of “misinformed review points” is clearly formalized and reflects real concerns authors face, such as reviewers asking questions that are already answered in the paper.\n* Argument reconstruction engine: The paper presents a novel approach that extracts both explicit and implicit premises from weaknesses, enabling premise-level factuality evaluation—a step beyond most prior work.\n* Empirical study with LLMs: Eight LLMs are evaluated on the REVIEWSCORE dataset, with quantitative results showing moderate human-model agreement, and ADVANCED scoring improves over BASE."}, "weaknesses": {"value": "* Insufficient dataset size: The dataset only covers 40 ICLR submissions across three years (~0.4% of the total), which is far too small to support general conclusions or train reliable automated systems. Most findings should be considered preliminary.\n* Limited diversity and experience of annotators: All annotations are from graduate students. There’s no evidence that experienced reviewers, ACs, or SACs were consulted—this may bias the labeling toward an author-centric view of review quality.\n* No practical deployment pathway: The paper does not explore how REVIEWSCORE could be integrated into real conference workflows (e.g., as part of rebuttal, AC dashboards, or review filtering), nor does it simulate such use cases."}, "questions": {"value": "* How well does your argument reconstruction engine perform on an external dataset or human-reconstructed arguments?\n* Have you tested REVIEWSCORE on reviews from other venues to see how it generalizes beyond the training data?\n* Could REVIEWSCORE be used as part of a rebuttal assistant or reviewer feedback system? If yes, what would that pipeline look like?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0Mxuu8siD8", "forum": "Zz2gtWX8wn", "replyto": "Zz2gtWX8wn", "signatures": ["ICLR.cc/2026/Conference/Submission9929/Reviewer_ht6d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9929/Reviewer_ht6d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859627666, "cdate": 1761859627666, "tmdate": 1762921381763, "mdate": 1762921381763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to detect low-quality reviews by defining a misinformed review point, i.e., ReviewScore. Specifically, the misinformed review point includes a question stated in a review that can already be answered by the paper, or a weakness stated in a review is incorrect or contains incorrect premises regarding the paper. After that, the authors build a human expert-annotated benchmark to evaluate the ability of current SOTA LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The problem studied in this paper is valuable. \n2. The paper is well-organized."}, "weaknesses": {"value": "1. My main concern is the proposed definition 2 (Misinformed Review Point), which is the core assumption of this paper. The claim \"a question stated in a review can already be answered by the paper\" may be caused by many other cases, not just the misinformed point. For example, maybe the paper is poorly written, the clarity is misleading, or the reviewer is not familiar with this area, and so on. If this paper is poorly written\n2. We believe that, in the peer-review system, scholars evaluate their academic level rankings based on the \"consistency assumption\", i.e., scholars with stronger abilities usually have stronger persuasiveness for evaluating others, and these scholars can also obtain higher achievements [1]. Therefore, we believe the \"review quality\" is mainly determined by the human/expert preference. And the \"low-quality review problem\" is a system problem.  \n\n\n[1] PICO: PEER REVIEW IN LLMS BASED ON CONSISTENCY OPTIMIZATION. ICLR'2025."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ThNoEqv9IN", "forum": "Zz2gtWX8wn", "replyto": "Zz2gtWX8wn", "signatures": ["ICLR.cc/2026/Conference/Submission9929/Reviewer_TFcU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9929/Reviewer_TFcU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918289665, "cdate": 1761918289665, "tmdate": 1762921381488, "mdate": 1762921381488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ReviewScore, a novel framework and dataset for detecting misinformed peer review points in academic paper reviews. They are defined as (1) questions already answered in the paper or (2) weaknesses based on incorrect premises. \n\nThe authors verify that a significant fraction of review points (15.2% of weaknesses, 26.4% of questions) are misinformed. \n\nTo automate this detection, they proposes a two-stage evaluation approach:\n1. Base ReviewScore, measuring unanswerability and factuality on a 5-point scale.\n2. Advanced ReviewScoreE, which decomposes review points into arguments and reconstructs their premises via an automatic argument reconstruction engine, checked for validity (using a SAT solver) and faithfulness (via LLM feedback loops).\n\nA human-annotated dataset of 657 review points (ICLR 2021–2023) is created to benchmark human–model agreement across eight modern LLMs (Claude, GPT-4/5, Gemini, LLaMA-3, etc.). Experiments show moderate agreement (F1≈0.4–0.5, κ≈0.3–0.4), and the paper demonstrates that premise-level analysis significantly improves over direct weakness-level evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Problem Definition: the paper identifies misinformed reviews as a measurable and impactful aspect of review quality — a previously underexplored issue. The operational definitions (answered question / incorrect premise) are specific yet widely applicable.\n\n2. Methodological Innovation: The automatic argument reconstruction engine combining LLMs, formal logic translation, and a SAT solver is a creative and rigorous integration of symbolic reasoning and LLM capabilities. The validity–faithfulness feedback loops are well-motivated and thoughtfully designed.\n\n3. High-quality Dataset: The authors build a trustworthy expert-annotated dataset (657 review points, 1,748 premises) with strong documentation and process controls (cross-annotation, consensus, training sessions)."}, "weaknesses": {"value": "1. Limited Agreement and Practical Readiness: Despite the substantial improvement over the baseline, the overall human-model agreement remains moderate (F1 ~0.45, Kappa ~0.35). This level of reliability is likely insufficient for fully autonomous deployment and would require human oversight, potentially reducing the net efficiency gains.\n\n2. Subjectivity in Annotations: Inter-annotator α=0.30–0.43 is low, suggesting high ambiguity in human judgments of factuality. The paper’s conclusions about “moderate agreement” might partly reflect human inconsistency rather than LLM capability.\n\n3. Dataset Scope and Diversity: Only ICLR reviews are used; the framework’s generality to NLP, CVPR, or smaller venues is unclear. Annotators are graduate students rather than senior researchers — potentially limiting domain depth.\n\n4. Evaluation Simplifications Models were provided only the main paper text, not figures or appendices, which could substantially affect factual verification. The binary misinformed/not-misinformed threshold is somewhat arbitrary; alternative calibration curves could provide richer insight.\n\n5. Argument Reconstruction Validation: While impressive, the argument reconstruction quality is mainly assessed via one model (Claude 3.7). Broader ablations (e.g., GPT-5 vs open models) or human inspection metrics could strengthen confidence."}, "questions": {"value": "1. How consistent are human labels for the same review points across groups (e.g., claim vs argument disagreements)?\n2. Could the reconstruction engine generalize to other reasoning tasks (e.g., debate analysis, scientific claim verification)?\n3. Could incorporating citation graphs or author responses systematically (rather than as an optional experiment) raise reliability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics issue detected."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "42tS1loj06", "forum": "Zz2gtWX8wn", "replyto": "Zz2gtWX8wn", "signatures": ["ICLR.cc/2026/Conference/Submission9929/Reviewer_GJmX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9929/Reviewer_GJmX"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762416929618, "cdate": 1762416929618, "tmdate": 1762921381195, "mdate": 1762921381195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}