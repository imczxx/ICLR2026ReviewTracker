{"id": "imxSI4yUZo", "number": 21385, "cdate": 1758316987817, "mdate": 1762957381981, "content": {"title": "Green Pruning: Layer Interdependence-Aware CNN Pruning for Resource Efficiency", "abstract": "The rising computational demands of pruning algorithms have heightened challenges about their energy consumption and carbon footprint in convolutional neural networks. We address these challenges from two perspectives. First, we introduce new evaluation metrics for pruning: a Resource Efficiency (RE) metric, which quantifies the computational cost required to achieve a target accuracy, and a system-agnostic framework for assessing the relative carbon efficiency of pruning algorithms. Together, these metrics enable fair and consistent comparisons of pruning methods with respect to both efficiency and sustainability. Second, we present a \\textbf{green pruning technique}, a data-free method that explicitly models inter-layer dependencies to provide a more reliable filter selection criterion. To further minimize computational overhead, our approach incorporates a low-complexity oblivious algorithm that leverages weak submodularity, ensuring efficiency without requiring iterative dataset passes.", "tldr": "", "keywords": ["Convolutional Neural Networks", "Structured Filter Pruning", "Model Compression Methods", "Best Approximation", "Resource Efficiency"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ffa332e4b19fb4fd8963ed4d543d0198a271f0c5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper argues that we should also reduce the cost of the pruning method itself for CNN pruning. To this end, the authors propose a resource efficiency metric and a data-free green pruning technique."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It is interesting to see some discussions about the cost of the pruning method."}, "weaknesses": {"value": "- Only focusing on the cost of the pruning method is not enough; the pruning method only needs to be conducted for once, but the pruned model will run for thousands or even millions of times. \n- More recent baselines are needed for a direct comparison."}, "questions": {"value": "- For Eq.(5) and Eq.(6), we see that the proposed method only focuses on the cost of the pruning method. In most cases, the pruning method only needs to be conducted once. However, after the pruned model is deployed in an application/production scenario, it will be run for thousands or even millions of times. For example, the current LLMs API, millions of people will call them around the world. The pruning cost is like $1\\times Cost_{prune}$, but the cost for the deployed model is like $N_{calls}\\times Cost_{model}$. In  most cases, $N_{calls} >> 1$. When designing a resource efficiency metric for green pruning, the reviewer thinks we should also consider this. The current metric is not comprehensive.\n\n- Following the above point, in Table 2, the reviewer thinks the authors should also tell us the absolute value of the cost of pruning and the FLOPs of the model after pruning, not only the RE ratio. It would give us a better understanding of the pruning cost and the FLOPs of the pruned model. It is hard to get something from the RE ratio; the ACC and MAC are at different orders of magnitude, making the RE ratio something hard to understand.\n\n- In Table 3, the reviewer finds that the most recent method is from the year 2021. Are there any more recent works? For example, RED++ and IPBM in Table 2, which come from the year 2023. Considering this is ICLR 2026, could the authors add some reference to methods in the years 2024 and 2025 for a straight comparison? This will better help us understand the effectiveness of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9UDrF1JSNk", "forum": "imxSI4yUZo", "replyto": "imxSI4yUZo", "signatures": ["ICLR.cc/2026/Conference/Submission21385/Reviewer_jBNX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21385/Reviewer_jBNX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885825075, "cdate": 1761885825075, "tmdate": 1762941737233, "mdate": 1762941737233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "7TKDYxDCuO", "forum": "imxSI4yUZo", "replyto": "imxSI4yUZo", "signatures": ["ICLR.cc/2026/Conference/Submission21385/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21385/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762957380689, "cdate": 1762957380689, "tmdate": 1762957380689, "mdate": 1762957380689, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Response:This paper addresses two critical challenges in CNN pruning: high environmental cost and poor feasibility on edge devices. Its core contributions are twofold: 1) Proposing novel evaluation metrics—Resource Efficiency (RE), which quantifies the trade-off between pruned network accuracy and pruning computational cost, and Relative Carbon Efficiency (RCE), a system-agnostic metric enabling fair comparison of pruning methods’ sustainability by using FLOPs as a hardware-independent proxy for carbon footprint. 2) Developing a data-free \"green pruning\" method that explicitly models inter-layer dependencies to improve filter selection reliability and adopts a low-complexity oblivious algorithm leveraging γ-weak submodularity, avoiding iterative dataset passes. Experiments on ResNet-50, ResNet-56, and ResNet-110 claim to match SOTA accuracy while reducing computational overhead and carbon footprint by orders of magnitude compared to baselines like APIB, ThiNet, and HRank."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The inclusion of theorems (e.g., upper bound on output error in Theorem 4.1, weak submodularity in Theorem 4.2) provides a formal basis for the pruning method, distinguishing it from heuristic-only approaches.\n\n2. Unlike prior work that links carbon footprint evaluation to specific hardware (hindering cross-method comparisons), the paper’s RCE metric is system-agnostic. It uses FLOPs (a hardware-independent proxy for runtime) to enable fair pruning method comparisons across setups without re-implementation on the same hardware."}, "weaknesses": {"value": "1. The paper attributes improved performance to inter-layer dependency modeling but does not test a \"no inter-layer\" variant (e.g., using only current-layer norms). Without this, it is impossible to confirm if layer interdependence adds value beyond the oblivious algorithm or norm-based selection.\n\n2. The paper includes post-pruning fine-tuning but does not account for its energy/carbon cost in RCE. Fine-tuning can dominate overall sustainability, so excluding it makes RCE an incomplete measure of real-world impact.\n\n3. Experiments are limited to ResNet variants. Larger models (e.g., ViT) may expose scalability issues (e.g., increased complexity in layer dependency calculations) that the paper does not address.\n\n4. This paper didn't provide any background or preliminaries of submodularity, which makes the theoretical parts in the main paper hard to follow.\n\n5. Some typos:\n\n* \"Accoriding\" and \"equation equation 5\" in line 143;\n* In (8), it should be $||F||$, not $|F|$;\n* ?? in line 866;\n* (?) in line 934."}, "questions": {"value": "1. In line 152: Why do you need to deploy a pruning method on the devices? Isn't it more common to prune the trained model on the server/cloud side and deploy the pruned model on the devices?\n\n2. In (8), what's the insight for the design of this important function? In particular, why is it defined as $\\max$ of the two norms, not multiplication or any other operation?\n\n3. How does your method scale to larger architectures (e.g., ViT)? Do layer dependency calculations become computationally prohibitive for these models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sn8H54BmUg", "forum": "imxSI4yUZo", "replyto": "imxSI4yUZo", "signatures": ["ICLR.cc/2026/Conference/Submission21385/Reviewer_8fi7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21385/Reviewer_8fi7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895183841, "cdate": 1761895183841, "tmdate": 1762941736855, "mdate": 1762941736855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a resource-efficient and environmentally-conscious pruning method for CNNs that explicitly models inter-layer dependencies. The authors propose two novel evaluation metrics: Resource Efficiency (RE) and Relative Carbon Efficiency (RCE) to assess pruning methods beyond traditional accuracy-FLOPs trade-offs. The core technical contribution is a γ-weakly submodular importance function based on filter norms across adjacent layers, solved using a low-complexity oblivious algorithm. Experiments on ResNet architectures demonstrate competitive accuracy with substantially reduced computational overhead during the pruning process itself."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. RE and RCE metrics address an important gap in pruning literature by considering sustainability\n\n2. Green AI and sustainable ML are increasingly important"}, "weaknesses": {"value": "1. The paper claims exceptional computational efficiency compared to APIB (Guo et al., 2023), which is an automatic pruning method. However, there exist many other **norm-based pruning methods** that can also obtain filter rankings very quickly without requiring greedy search. For instance, simple L1-norm pruning or L2-norm pruning can rank all filters in a single pass by computing norms independently for each filter, potentially achieving even faster computation than the proposed method. These approaches similarly avoid iterative dataset passes and greedy algorithms. Therefore, it is unclear whether the added complexity of considering inter-layer dependencies provides sufficient benefit to justify the method's claim. \n\n2. Incomplete carbon footprint analysis. While RCE is system-agnostic, the paper doesn't validate actual energy consumption measurements. The FLOP-to-energy conversion assumes a constant relationship, which may not hold across different operations or hardware states.\n\n3. Based on point 1 and 2, I believe there is a huge gap between the title and actual contribution.\n\n4. The compared methods are old. From line 412- 416, \"HRank (Lin et al., 2020), RED++ (Yvinec et al., 2023), ThiNet (Luo et al., 2018), GAL (Lin et al., 2019), SSS (Huang and Wang, 2018), L1 (;and Hans Peter Graf, 2017), F-ThiNet (Tofigh et al., 2022), He et al., (He et al., 2017), GDP (Lin et al., 2018), NISP (Yu et al., 2018), DECORE (Alwani et al., 2021), FilterSketch (Lin et al., 2021), and APIB (Guo et al., 2023).\" None of these is recent. The authors should cite and compare papers in the year 2024 and 2025."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kObub4bexU", "forum": "imxSI4yUZo", "replyto": "imxSI4yUZo", "signatures": ["ICLR.cc/2026/Conference/Submission21385/Reviewer_ujYe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21385/Reviewer_ujYe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917948216, "cdate": 1761917948216, "tmdate": 1762941736501, "mdate": 1762941736501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "# GREEN PRUNING\n\n## Summary\n\nWhile studying pruning methods, we typically measure the reduction in inference-time computation (FLOPs) due to pruning. Instead, this work measures the computation cost of performing the pruning itself — that is, the computation cost of selecting the filters for pruning. Furthermore, it measures the cost of pruning in terms of carbon footprint, using a system-agnostic framework.  \n\nTwo measures are proposed: **Runtime Carbon Efficiency (RCE)** and **Resource Efficiency (RE)**.  \n- **RCE** is defined as the ratio between the number of operations required by two pruning methods.  \n- **RE** is defined as the ratio between accuracy and the number of multiply–accumulate (MAC) operations required in the pruning process.  \n\nFinally, a pruning method termed the *Oblivious Algorithm* has been proposed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "## Strengths\n\n- The paper delves into an important area — the pruning efficiency of pruning methods themselves.  \n- The two proposed measures, **RCE** and **RE**, seem easy to implement and offer a system-agnostic strategy for evaluating pruning methods.  \n- Tables 1, 2, and 3 present a convincing case that the proposed solution is superior compared to other baselines."}, "weaknesses": {"value": "## Comments\n\n- It is not clear how RCE and RE result in a reduction of the carbon footprint. While some relationship is established in Equations (1)–(5), there are no results that demonstrate the actual amount of reduction in carbon footprint.\n\n- The claim of a system-agnostic framework is also not evaluated. An analysis showing the true carbon footprint of a particular pruning method across multiple hardware platforms and its comparison with the proposed strategy is needed to better understand the effectiveness of the proposed metrics and algorithm.\n\n- The Oblivious Algorithm, though offering low complexity due to its simplified nature, presents results in Tables 1, 2, and 3 that convincingly demonstrate the superiority of the proposed solution compared to other baselines. However, since it is based on the L₂-norm of filters, it is unclear how it achieves lower computational cost or eventually a lower carbon footprint.\n\n- Furthermore, the computations depend on the number of epochs required by each method to achieve the reported results, but there is no discussion of computation or carbon footprint per epoch.\n\n- While the method is compared with SOAT approaches, a more categorical comparison such as with Knowledge Distillation, Structured online/offline pruning, recent methods for transformers could be useful in also help in evaluating whether ti can be extended to more recent models such as those based on transformers.\n\n## Minor Comments\n\n- In Equation (3), it is not clear what PUE refers to.  \n- The paper is 9 pages long, whereas the Appendix extends over 12 pages, which is excessive. Only relevant information should be included in the Appendix, and more important information should be incorporated into the main paper."}, "questions": {"value": "Please refer to Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VgSnZrv9CS", "forum": "imxSI4yUZo", "replyto": "imxSI4yUZo", "signatures": ["ICLR.cc/2026/Conference/Submission21385/Reviewer_6uSH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21385/Reviewer_6uSH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762069464977, "cdate": 1762069464977, "tmdate": 1762941736162, "mdate": 1762941736162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}