{"id": "XJ3T70nELl", "number": 4227, "cdate": 1757642328792, "mdate": 1759898046054, "content": {"title": "Turn That Frown Upside Down: FaceID Customization via Cross-Training Data", "abstract": "Existing face identity (FaceID) customization methods perform well but are limited to generating identical faces as the input, while in real-world applications, users often desire images of the same person but with variations, such as different expressions (e.g., smiling, angry) or angles (e.g., side profile). This limitation arises from the lack of datasets with controlled input-output facial variations, restricting models’ ability to learn effective modifications.\n\nTo address this issue, we propose CrossFaceID, the first large-scale, high-quality, and publicly available dataset specifically designed to improve the facial modification capabilities of FaceID customization models. Specifically, CrossFaceID consists of 40,000 text-image pairs from approximately 2,000 persons, with each person represented by around 20 images showcasing diverse facial attributes such as poses, expressions, angles, and adornments. During the training stage, a specific face of a person is used as input, and the FaceID customization model is forced to generate another image of the same person but with altered facial features. This allows the FaceID customization model to acquire the ability to personalize and modify known facial features during the inference stage. Experiments show that models fine-tuned on the CrossFaceID dataset retain its performance in preserving FaceID fidelity while significantly improving its face customization capabilities.\n\nTo facilitate further advancements in the FaceID customization field, our code, constructed datasets, and trained models are fully available to the public.", "tldr": "", "keywords": ["Face identity (FaceID) Customization", "Datasets", "Text-to-image", "Diffusion Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/af86a4b463417daab3eb37ca8059a7ce152d5937.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CrossFaceID, a large-scale dataset containing 40,000 text–image pairs collected from approximately 2,000 individuals, with around 20 images per person. The dataset covers a wide range of facial attributes, including pose, expression, viewing angle, and accessories, and is designed to facilitate research on FaceID customization and facial editing.\n\nHowever, the image authorization and copyright status are unclear, which raises serious concerns regarding data legality and ethical compliance. In addition, the annotations were not manually verified, suggesting potential issues with annotation accuracy and consistency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a large-scale dataset (CrossFaceID) with 40,000 text–image pairs from 2,000 individuals, providing a valuable resource for training and evaluating FaceID customization models.\n\n2. The dataset includes diverse facial variations (e.g., poses, expressions, angles, and accessories), which could help improve model generalization."}, "weaknesses": {"value": "1. The images appear to lack explicit authorization, leading to major concerns about copyright and data usage rights. This issue may significantly limit the dataset’s public accessibility and research applicability.\n\n2. The annotations were not manually verified, which raises doubts about the reliability and precision of the textual descriptions. Poor annotation quality could undermine the dataset’s effectiveness in training high-fidelity generative models.\n\n3. The results shown in Figure 4 do not appear convincing. Some generated examples (e.g., Case 3 and Case 4) look unnatural and fail to demonstrate a clear advantage over existing methods."}, "questions": {"value": "1. The images appear to lack explicit authorization, leading to major concerns about copyright and data usage rights. This issue may significantly limit the dataset’s public accessibility and research applicability.\n\n2. The annotations were not manually verified, which raises doubts about the reliability and precision of the textual descriptions. Poor annotation quality could undermine the dataset’s effectiveness in training high-fidelity generative models.\n\n3. The results shown in Figure 4 do not appear convincing. Some generated examples (e.g., Case 3 and Case 4) look unnatural and fail to demonstrate a clear advantage over existing methods."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "This paper introduces a dataset containing 40,000 text–image pairs collected from approximately 2,000 persons.\nThe images appear to lack explicit authorization, leading to major concerns about copyright and data usage rights."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XwF7TB4A2q", "forum": "XJ3T70nELl", "replyto": "XJ3T70nELl", "signatures": ["ICLR.cc/2026/Conference/Submission4227/Reviewer_fcwF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4227/Reviewer_fcwF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737699137, "cdate": 1761737699137, "tmdate": 1762917238876, "mdate": 1762917238876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper follows a data-centric approach and collects a dataset of 40,000 of text image pairs. They use the collected dataset to fine-tune existing models to generate diverse facial variations (expression and pose) of a given input identity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The collected dataset of image-text pairs for customization is valuable and can enable further research in this area.\n- The qualitative results look realistic and show that they can indeed generate different expressions and head poses based on instructions\n- The quantitative results of their fine-tuned models show consistent improvement over their base model counterparts and do not seem to degrade."}, "weaknesses": {"value": "- There is limited technical novelty. Mostly just fine-tunes a model with a new dataset.\n- The dataset only contains a very limited set of facial features.\n- There's no stratified analysis, only aggregate results. Given the highly imbalanced facial features in the dataset, the customization performance results could be heavily dominated by just one feature that the model can do very well (e.g., smiling).\n- Currently, there are no convincing experiments to validate the diversity. It would be nice to incorporate this, especially since fine-tuning can sometimes also have the unintended consequence of reduced diversity. \n- InstantID also allows for novel view synthesis using a reference view. It would be nice to have a comparison with this and also show the diversity of head pose / angles that your model can generate as compared with InstantID. \n- It would also be nice to see an experiment showing performance as a function of the number of fine tuning images."}, "questions": {"value": "- How does the highly imbalanced distribution of facial features affect customization performance for each feature?\n- How many examples of each feature do we need for fine tuning to capture a specific facial feature?\n- Were the identities for fine-tuning different from the identities for testing? It was not explicitly clear in the paper.\n- Also, are there visual examples showing performance for a bit more out-of-distribution faces, such as non-celebrity faces, other races, non front facing poses, face accessories and more extreme make ups? It would be good to see the model's robustness to these inputs."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "They collect face images of approximately 2000 celebrities from the web. \nNot sure if this necessitates an ethics review, but mentioning here just in case."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OBTqRJw7cN", "forum": "XJ3T70nELl", "replyto": "XJ3T70nELl", "signatures": ["ICLR.cc/2026/Conference/Submission4227/Reviewer_m2YN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4227/Reviewer_m2YN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823006281, "cdate": 1761823006281, "tmdate": 1762917238602, "mdate": 1762917238602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper collects a dataset, called CrossFaceID, which is specifically designed to improve the facial modification capabilities of FaceID customization models. CrossFaceID consists of 40,000 text-image pairs of 2,000 persons. Based on the collected dataset, the authors design a cross-training strategy to fine-tune the FaceID customization model, allows it to acquire the ability to personalize and modify known facial features during the inference stage."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well structured and easy to follow. The construction process of the CrossFaceID dataset is introduced in detail. \n\n2. Experimental results appear to demonstrate the effectiveness of the CrossFaceID dataset and the proposed cross-training strategy."}, "weaknesses": {"value": "1. One of my major concerns lies in the main contribution of this paper. The authors claim that the CrossFaceID dataset is a large-scale and high-quality dataset, but it only contains 40,000 text-image pairs of 2,000 persons. Compared with existing text-to-image person generation datasets (e.g., CosmicMan), the data volume of CrossFaceID is relatively smaller. Moreover, the data collection process follows a standard practice, so I cannot get any novel insight from this dataset.\n\n2. For the proposed cross-training strategy, the experiments in Figure 4 show that it can empower the model to generate the results that can more accurately align with the text prompt, such as “wearing a sunglass” in case 2 and “wearing a mask” in case 4. However, from the technical details of the cross-training strategy, I cannot find any specific mechanism to achieve this text-image alignment. I believe that it cannot be realized by just changing the ID prompt. \n\n3. The motivation of this paper is relatively weak. The authors claim that their motivation is “Existing face identity (FaceID) customization methods perform well but are limited to generating identical faces as the input, while in real-world applications, users often desire images of the same person but with variations, such as different expressions (e.g., smiling, angry) or angles (e.g., side profile).” However, based on my knowledge, RealisID [1] is a scale-robust and fine-controllable ID customization method, which can generate face images with different facial expressions, head poses, face locations, face scales, and even multiple faces. \n\n[1] Sun Z, Du F, Chen W, et al. RealisID: Scale-Robust and Fine-Controllable Identity Customization via Local and Global Complementation[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2025, 39(7): 7158-7166.\n\n4. In the experiments, the authors pre-trained the competing methods by using “our curated LAION dataset”. How to construct this dataset? Is there any description and reference for this dataset?"}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "The authors collect a dataset that contains 4,0000 face images from approximately 2,000 celebrities. Will it raise some concerns about human privacy?"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NHkNipu5q5", "forum": "XJ3T70nELl", "replyto": "XJ3T70nELl", "signatures": ["ICLR.cc/2026/Conference/Submission4227/Reviewer_QtiN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4227/Reviewer_QtiN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762081951934, "cdate": 1762081951934, "tmdate": 1762917238335, "mdate": 1762917238335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}