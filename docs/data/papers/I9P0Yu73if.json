{"id": "I9P0Yu73if", "number": 2312, "cdate": 1757056650869, "mdate": 1759898156325, "content": {"title": "Reinforcement Reward Model with Policy Feedback", "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward hacking, a phenomenon that policy models  exploit spurious reward patterns instead of faithfully capturing human intent.\nPrior work to mitigate reward hacking primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward hacking.\nTo address these limitations, we propose R2M (Reinforcement Reward Model), a novel lightweight RLHF framework.\nSpecifically, we aim to go beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, we enhance the reward model by incorporating the evolving hidden states of the policy (namely policy feedback).\nwe redesign the scoring head of the reward model to integrate policy feedback and introduce a corresponding iterative lightweight training phase, utilizing real-time policy feedback to enable adaption to policy distribution shifts.\nNotably, without modifying the core RLHF algorithms, simply integrating R2M enables the reward model to achieve iterative distribution alignment with accurate reward allocation,  yielding 4.8\\% to 5.6\\% win rate improvement on dialogue tasks and 6.3\\% win rate improvement on document summarization tasks, while introducing marginal computational cost.\nThis work points to a promising new direction for improving the performance of  reward models through real-time utilization of feedback from policy models.", "tldr": "", "keywords": ["Reinforcement Learning", "RLHF", "Reward Model"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9692f03c349c5d85daedf5bbed234186b3dd8af9.pdf", "supplementary_material": "/attachment/cd30f28776c202e510809447a4c887296d507fc3.zip"}, "replies": [{"content": {"summary": {"value": "The submission proposes to address reward hacking by using not only the outputs of the policy in the reward model, but additionally using the hidden state of the policy as input for the reward model.\nThe authors propose a network structure that allows this and propose to fine-tune the reward model after each policy update with synthetic labels.\nExperiments on ultrafeeedback and the tl;dr dataset show an improvement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* preventing reward hacking is an important problem setting\n * The idea of using the hidden state (intermediate activations) instead of only the output of the policy is well motivated and could be helpful"}, "weaknesses": {"value": "* The main weakness is that the experiments do not sufficiently support the claim that the inclusion of the policy's hidden state indeed improves performance. The proposed approach includes two main parts: 1) add policy hidden state to RM input, 2) update RM with pseudo labels after each policy update. The ablations do not compare to the case of a normal RM, which is updated with pseudo labels after each policy update.\nThere is one ablation which replaces the policy information with random noise and performs worse, but inserting random noise into the RM hinders training and thus is not a fair comparison. Instead, comparing to a fixed input would make more sense, or instead simply comparing to an ablation that only finetunes the head of a standard RM.\n\n* Experiments are not sufficiently rigorous. They are based on a single random seed each and, judging from Figure 5, training is not converged for either method. Hyper-parameters have seemingly not been optimized and are not fully reported.\n\n* The writing is unclear at times, with undefined symbols, missing details and undefined metrics. \n\nMinor issues:\n * Figure 1: The similarity measure is not specified. It is also not clear how the examples of reward hacking and not reward hacking were obtained.\n * \"Hidden state\" of the policy are never clearly defined. It seems to be the output of the last layer before the LM head, but this should be specified. This choice is also not \n * L92: pi_old is never used\n * L93: set X is not defined\n * L97: RLOO does not use groups, Hu 2025 is the correct citation for groups\n * L187: Reference to Section 2 is incorrect"}, "questions": {"value": "* Why is only the hidden state of the policy used, rather than the output token distribution or the intermediate activations? If we anthropomorphize the LM a bit, it seems reasonable that the \"intention to reward hack\" would arise in earlier or intermediate layers, not in the final layer.\n * How was Figure 1 created?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l5dqx0vjr7", "forum": "I9P0Yu73if", "replyto": "I9P0Yu73if", "signatures": ["ICLR.cc/2026/Conference/Submission2312/Reviewer_Vyu8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2312/Reviewer_Vyu8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761289303191, "cdate": 1761289303191, "tmdate": 1762916189612, "mdate": 1762916189612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets reward hacking in RLHF by proposing R2M, a lightweight framework that redesigns the reward model’s scoring head to incorporate the hidden states of the current policy model. The reward model is trained with pairwise preferences using a Bradley–Terry objective. Experimental results show the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses an important practical issue in RLHF (reward hacking) with a relatively simple architectural change.\n\n2. Integrating policy hidden states into the reward scorer is an interesting and potentially useful idea that could improve reward-model calibration to the policy’s distribution."}, "weaknesses": {"value": "1. Dependence on Bradley–Terry preference modeling: While pairwise preference RMs are standard, the paper does not situate R2M against verifiable reward approaches that are increasingly dominant for math and code. These verifiable rewards are often lighter-weight by grounding the reward in external correctness.\n\n2. Computational overhead: The method requires modifying the reward model and iteratively training both the policy and reward models. It is unclear whether the coupling increases training cost, instability, or maintenance complexity compared to standard RLHF pipelines. A clear accounting of compute, memory, and wall-clock time, plus sample efficiency, is missing.\n\n3. Limited scenarios and benchmarks: Evaluation is restricted to dialogue and TL;DR, without coverage of standard open benchmarks for LLM evaluation (e.g., MT-Bench, Arena-Hard, AlpacaEval 2, Opencompass). \n\n4. Metrics do not directly measure reward hacking: Reported metrics seem to focus on reward-model scores or internal alignment proxies, making it hard to substantiate claims about reduced reward hacking."}, "questions": {"value": "Multi-reward scenario:\n\n1. How does R2M extend to multiple objectives (e.g., helpfulness, harmlessness, calibration, style) or task-specific rewards (math, code correctness, factuality)?\n\n2. Can the scoring head be designed to aggregate multiple reward signals (scalarization, mixture-of-experts, or listwise ranking) while preserving robustness against hacking?\n\nBenchmark coverage:\n\n1. How R2M performs on public benchmarks such as arena\n\n2. How does R2M perform relative to verifiable reward on math and code tasks?\n\n3. How robust are the results across different base models, training scales, and domains?\n\n\nMeasuring reward hacking:\n\n1. How is “reward hacking” defined in this work?\n\n2. What metrics directly quantify reward hacking beyond reward-model scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KaOaadwhSI", "forum": "I9P0Yu73if", "replyto": "I9P0Yu73if", "signatures": ["ICLR.cc/2026/Conference/Submission2312/Reviewer_4wJU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2312/Reviewer_4wJU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559444537, "cdate": 1761559444537, "tmdate": 1762916189339, "mdate": 1762916189339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes policy model forward hidden states for responses with and without reward hacking and finds notable differences. Building on this finding, it proposes R2M, a reward model that takes both the response and the policy model’s hidden states as inputs to mitigate reward hacking. The method further introduces a Group Reward Entropy Bradley–Terry loss to iteratively update R2M. Experiments on *AlpacaEval2* and *TL;DR* report performance gains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation analysis is sound. Studying reward hacking through the lens of policy hidden states is novel.\n2. The R2M architecture leverages policy hidden states to alleviate reward hacking, and its effectiveness is validated on *AlpacaEval2* and *TL;DR*."}, "weaknesses": {"value": "#### Motivation\n1. The identified reward-hacking patterns need more quantitative evidence. Do the conclusions hold at larger scales?\n\n2. How do reward hacking type and prompt type relate to *similarity matrix*? If the same prompt triggers both length hacking and formatting hacking, do hidden states remain similar?\n\n3. Does the *similarity matrix* of hidden states change under policy distribution shift? As training progresses, do the conclusions still hold?\n\n#### Method\n1. The method uses a *bootstrap* scheme to train/update the reward model. On line 242, “we select only the samples with the highest and lowest scores...” Considering that when reward hacking occurs, the reward of the response that shows reward hacking will be higher than that of the normal response, this selection may cause the optimization in **Equation (4)** to push in the wrong direction.\n\n2. The method *pre-sets* the number of training rounds $ T $. In practical RL, convergence rounds are uncertain; it is difficult to determine $T$ *a priori*, making it hard to judge whether the key R2M component is actually taking effect. More *ablation* on $T$ is recommended.\n\n3. The crucial *Sequence-to-Token Cross-Attention* in R2M lacks a formal, equation-level definition.\n\n#### Baselines\n1. The evaluation benchmarks are limited; broader assessments on MT-Bench, Arena-Hard, IFEval, etc., would be more convincing.\n2. In Table 2, R2M is compared *against* RLOO, GRPO, ReMax, etc. Given the claim that R2M can be plugged into **any REINFORCE-based RLHF framework**, a more appropriate comparison is to apply R2M within RLOO, GRPO, ReMax, etc., and compare R2M-augmented vs normal reward model within each framework.\n\n### Experiments\n1. The experiments on reward hacking are insufficient and not fully quantitative. Since the paper’s main claim is to mitigate reward hacking, experiments should focus more on this—beyond space hacking—including length hacking, pattern/template hacking, and others.\n2. In Table 4, the WR (%) of RLOO is significantly lower than SFT, which undermines a fair comparison and may indicate RL was not conducted properly.\n3. The computational cost analysis needs to be more formal. In practice, jointly updating the policy and the reward model can make convergence difficult. The paper should report the number of steps to converge for a fair comparison.\n\n### Typos\n1. Line 246 (Eq. 4): $y_{i,w}$ and $h_{i,w}$ — the index $w$ is undefined.\n2. Line 252: *foward* → *forward*.\n3. Line 334: *increse* → *increase*."}, "questions": {"value": "1. R2M passes the policy model’s hidden state to the reward model, but there is no explicit signal indicating whether that hidden state corresponds to a hacked response. How does R2M learn to distinguish  hacked cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "m5buObq6rI", "forum": "I9P0Yu73if", "replyto": "I9P0Yu73if", "signatures": ["ICLR.cc/2026/Conference/Submission2312/Reviewer_msVd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2312/Reviewer_msVd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887204573, "cdate": 1761887204573, "tmdate": 1762916189115, "mdate": 1762916189115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces policy feedback into reward scoring stage. Here, policy feedback refers to taking the hidden state embedding of the policy and cross attends to such embeddings from the reward computation. When reward is adapted this way they show performance improvement during the policy optimization process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is interesting in that it introduces much richer interactions between policy and reward, allowing for policy to pass on richer info into reward computation, and they show improvements over a few baselines. The idea itself is somewhat novel."}, "weaknesses": {"value": "I think the paper lacks in a few important aspects such as technical solidity, presentation clarity etc which I will detail in the questions below."}, "questions": {"value": "==== **algorithm 1** ====\n\nI am a bit confused by the logic in algorithm 1, where it seems that we initially sample and generate scores $y_i$ from the RM and later will regress RM against the very same reward labels $y_i$ but using updated hidden state from the policy. I suppose this is the adaptive strategy highlighted in this work? i.e., not only is there an architectural improvement (cross attention to hidden state), there is also an online adaptation to ensure RM is in sync with policy.\n\nI worry that this updating process will be a bit sensitive to hypers related to training the RM online because it seems that too strong of a learning rate might erase the original RM's information. After all it's a form of self-distillation with labels from a previous version of the RM itself, so I think it is useful to ablate and show how sensitive this process is to various hyperparameters used for updating RM.\n\n==== **an architectural alone baseline** ====\n\nRelated to the above I think another important baseline is, what if you don't do the online adaptation and simply just do the cross attention using the policy feedback. This is already a form of online adaptation but does not require updating and self-distilling RM online.\n\nI think another baseline is to train and update the RM once at the beginning of policy training, and keep its weights fixed throughout. This experiment will let us know whether fully online adaptation is necessary.\n\nOverall I feel I am not sure where the performance gains are because of so many changes all at once, and it is useful to ablate each part to elicit where the true performance gains lie and if some parts of the algorithm are not necessary (in light of improvement in Fig 4).\n\n==== **Fig 4 and mechanisms behind the improvement** ====\n\nI am generally intrigued but meanwhile a bit surprised at the improvements in Fig 4. I think overall it's not clear why the improvements are possible in the first place simply because of the adaptation proposed in the paper. Indeed, there is more feedback from policy to RM and hidden state cross attention allows for much richer information, but at the end the RM distills back into its own labels and it's not clear to me new information has been distilled into the policy. Maybe the cross attention somehow gives more info to the RM? But this does not provide new source of \"ground truth\" to the RM itself, so where do the gains come from intuitively? Does it come from a smoother optimization landscape from the RM? Does it come from a different effective hyper-parameter from the RL run?\n\nI don't know answers to the above but I think more ablations are warranted for a better understanding. Just putting together a stack of system with performance the source of gains is good but understanding is more important."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KMNK4FftKW", "forum": "I9P0Yu73if", "replyto": "I9P0Yu73if", "signatures": ["ICLR.cc/2026/Conference/Submission2312/Reviewer_2sd2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2312/Reviewer_2sd2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762598972921, "cdate": 1762598972921, "tmdate": 1762916188887, "mdate": 1762916188887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}