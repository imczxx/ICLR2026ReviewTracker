{"id": "vmiqjqo08s", "number": 17777, "cdate": 1758280435147, "mdate": 1763677411406, "content": {"title": "Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm", "abstract": "Bidirectional Associative Memory (BAM) trained by Bidirectional Backpropagation (B-BP) suffer from poor robustness and sensitivity to noise and adversarial attacks. To address it, we propose a novel gradient-free training algorithm, the Bidirectional Subspace Rotation Algorithm (B-SRA), designed to improve the robustness and convergence behavior of BAM. Through comprehensive experiments, two key principles, orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA), are identified as central to enhancing the robustness of BAM. Motivated by these insights, new regularization strategies are introduced into B-BP, yielding models with significantly improved resistance to corruption and adversarial perturbations. We conduct an ablation study across different training strategies to determine which approach achieves a more robust BAM. Additionally, we evaluate the robustness of BAM under various attack scenarios and across increasing memory capacities, including the association of 50, 100, and 200 pattern pairs. Among all strategies, the SAME configuration—which combines OWM and GPA—achieves the highest resilience. Our findings suggest that B-SRA and carefully designed regularization strategies lead to more reliable associative memories and open new directions for building resilient neural architectures.", "tldr": "", "keywords": ["robust neural network", "subspace rotation algorithm", "bidirectional associative memory"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5cf4ec63292080cbbf241d4b844a9fe42199ab47.pdf", "supplementary_material": "/attachment/c11505463772f71317d2fdb086c82723e4a7a51e.zip"}, "replies": [{"content": {"summary": {"value": "This paper extends the Subspace Rotation Algorithm to Bidirectional Associative Memory (BAM), proposing B-SRA, and integrates both OWM and GPA regularization methods to enhance the training process. The proposed approach effectively improves the robustness of BAM against noise and adversarial attacks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed improvements are effective, with clear methodological explanations and comprehensive experimental results that strongly support the claims."}, "weaknesses": {"value": "1. While the paper utilizes two well-established regularization methods, OWM and GPA, their direct application without significant adaptation or novel integration strategy does not constitute a strong innovative contribution.\n2. The research focus of this paper, Bidirectional Associative Memory (BAM), appears to be a relatively niche area. Based on the introduction provided, it seems to have attracted limited research attention in recent years. Furthermore, the experimental results presented do not sufficiently demonstrate strong practical potential for BAM. It is necessary to provide a more detailed justification of BAM's research significance and application prospects.\n3. The organization of the paper deviates from conventional structure, with the experiments section occupying a disproportionately large portion of the content, while the methodology section lacks sufficient detail. It is recommended to condense the definition of BAM and the stability analysis, possibly relocating them to an appendix. Additionally, the source of the stability analysis in section 2.1 should be clearly stated—whether it is an original contribution or derived from existing work."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QVseqvYIzH", "forum": "vmiqjqo08s", "replyto": "vmiqjqo08s", "signatures": ["ICLR.cc/2026/Conference/Submission17777/Reviewer_xneK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17777/Reviewer_xneK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750183408, "cdate": 1761750183408, "tmdate": 1762927617855, "mdate": 1762927617855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel algorithm for robust training of Bidirectional Associative Memory (BAM), named B-SRA (Bidirectional Subspace Rotation Algorithm), which is gradient-free and inspired by recent work on subspace rotation in Hopfield networks. The authors identify two principles, orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA), as critical to robustness and incorporate them as regularizers into the original gradient-based BAM training (B-BP). Experimental results across multiple datasets and attack scenarios demonstrate clear gains in robustness, particularly when both regularizers are applied (SAME configuration)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clarity: The algorithmic description and core concepts are clearly presented and easy to follow, particularly for the new training method.\n\nOriginality: B-SRA is a compelling alternative to traditional B-BP, offering better robustness and convergence without the need for gradient-based optimization.\n\nEmpirical Validation: The experimental setup is extensive, with clear improvements shown in both adversarial robustness and noise resilience. The ablation study is particularly helpful in dissecting the roles of different regularization components."}, "weaknesses": {"value": "Theoretical Scope Limitation: While the linear-case analysis is insightful, the paper would be stronger with theoretical justification or approximation results for more commonly used nonlinear BAM architectures. The lack of formal results in such settings limits the generality of the claims.\n\nClarity in Experimental Tables: The tables could benefit from clearer formatting. It is not always obvious which values represent robustness performance or regularization metrics, and which direction (higher/lower) is better. A clearer legend or visual emphasis on best results would improve readability.\n\nRedundancy in Experimental Presentation: The inclusion of many variations and datasets is thorough, but at times excessive. A more concise presentation—e.g., one table summarizing robustness across all methods and datasets—would help the reader focus on key comparisons."}, "questions": {"value": "1. Why does the regularized B-BP (SAME) sometimes outperform B-SRA? I believe a more elaborate discussion of this phenomenon would be a valuable addition. More broadly, a deeper reflection on the experimental results could greatly strengthen the narrative, e.g., what they suggest about the nature of robustness in BAM, and how each method contributes.\n\n2. Could B-SRA be used as an initialization for B-BP? This might combine the robustness and fast convergence properties of B-SRA with the adaptability of gradient-based optimization. Was this hybrid approach considered or tested?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "aMjBw9Thow", "forum": "vmiqjqo08s", "replyto": "vmiqjqo08s", "signatures": ["ICLR.cc/2026/Conference/Submission17777/Reviewer_mRLp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17777/Reviewer_mRLp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900148396, "cdate": 1761900148396, "tmdate": 1762927617396, "mdate": 1762927617396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of improving the robustness and stability of Bidirectional Associative Memory (BAM) networks, which are designed to learn two-way associations between paired patterns.  It extends the Subspace Rotation Algorithm (SRA)—previously used for Restricted Hopfield Networks—to Bidirectional Associative Memory (BAM), yielding a gradient-free training method\nThey introduce two regularization techniques for B-BP through the usage of Orthogonal Weight Matrix (OWM) to encourage orthogonal weights to preserve signal norms and suppress noise, and Gradient Pattern Alignment to align gradients with data patterns in order to make learning more stable and resistant to perturbations. They run experiments under gaussian noise and different adversarial attacks to demonstrate the resilience of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- While previous works introduced the Subspace Rotation Algorithm (SRA) for Restricted Hopfield Networks (RHN), this applies SRA to BAMs. \n- The algorithm is well explained and easy to implement with pseudo code \n- The authors also propose gradient pattern alignment (GPA) for aligning the gradient of the loss with the stored input patterns. Previous works do not apply GPA to associative memory training. The authors jointly apply Orthogonal Weight Matrix (OWM) regularization and GPA. - Evaluations are done both accuracy and bitwise error under perturbations"}, "weaknesses": {"value": "- Positioning  - Need more clarity on the contribution. The work is an adaptation of SRA to BAM\nOrthogonality and gradient-input alignment style terms exist in broader literature; using them for training of BAM is reasonable but also incremental.\n- No direct comparisons to Dense Associative Memories / Modern Hopfield Networks or to orthogonality-promoting training in neural networks. \n- The authors claim that B-SRA enhances the robustness and convergence speed. But, do not provide any timing or iteration count plots to back this claim."}, "questions": {"value": "- See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yLri4lzQ5H", "forum": "vmiqjqo08s", "replyto": "vmiqjqo08s", "signatures": ["ICLR.cc/2026/Conference/Submission17777/Reviewer_L9rJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17777/Reviewer_L9rJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987529691, "cdate": 1761987529691, "tmdate": 1762927616932, "mdate": 1762927616932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the poor robustness of Bidirectional Associative Memory (BAM) networks trained with standard Bidirectional Backpropagation (B-BP). The authors first introduce a novel, robust, gradient-free trainer, the Bidirectional Subspace Rotation Algorithm (B-SRA), which demonstrates inherent resilience to noise and adversarial attacks. By analyzing B-SRA, they identify two key principles responsible for this robustness: maintaining Orthogonal Weight Matrices (OWM) and achieving Gradient-Pattern Alignment (GPA). The authors then propose these principles as novel regularization terms for the standard B-BP algorithm. Extensive experiments on pattern association tasks (MNIST, Chinese script) under various noise and adversarial attacks (FGSM, PGD) demonstrate that B-BP with both OWM and GPA regularizers (the \"SAME\" strategy) achieves the highest level of robustness, significantly outperforming standard B-BP and even the B-SRA method that inspired it, especially at larger memory capacities."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's strongest contribution is its scientific method. It proposes a robust gradient-free algorithm (B-SRA), performs a root-cause analysis to determine why it's robust (OWM + GPA), and then successfully ports those principles to fix the vulnerable B-BP algorithm.\n2. The ablation in Sec 4.3.2 is excellent. It cleanly isolates the individual contributions of OWM (the ORTH strategy) and GPA (the ALIGN strategy) and demonstrates that both are required for full robustness (the SAME strategy). Table 1, which measures the OWM and GPA values for each strategy, provides a direct link between the model's properties and its performance.\n3. The proposed \"SAME\" strategy (B-BP + OWM + GPA) is shown to be highly resilient. It achieves near-perfect retrieval under strong masking, noise, and adversarial attacks (FGSM, PGD) where the baseline B-BP and even the ALIGN-only models fail completely.\n4. The paper shows that the \"SAME\" strategy scales well with increased memory capacity (from 50 to 200 pattern pairs) and network depth (to 5 layers). In fact, its robustness improves with scale, outperforming B-SRA, which degrades as capacity increases."}, "weaknesses": {"value": "1. The paper focuses exclusively on Bidirectional Associative Memory (BAM), which is a classic but relatively niche architecture. The authors state an intent to apply these principles to Transformers and modern Hopfield networks as future work, but the paper presents no evidence that these findings will transfer.\n2. The experiments use low-resolution, bipolarized images (MNIST, Chinese script) . While standard for testing associative memory, this is far from the complex, high-dimensional data where robustness is a critical issue today (e.g., in computer vision or language modeling).\n3. Algorithm 1 is explicitly for a 3-layer BAM. The 200-pair experiment uses a 5-layer BAM. The paper never explains how B-SRA's SVD update (Algorithm 1) or the OWM/GPA regularizers are applied in this deeper, multi-layer setting. This is a significant methodological omission."}, "questions": {"value": "1. Algorithm 1 is for a 3-layer BAM. How were the B-SRA algorithm and, more importantly, the OWM and GPA regularizers adapted for the 5-layer BAM used in the 200-pattern capacity test?\n2. The \"SAME\" strategy (B-BP+OWM+GPA) was the most robust. What were the $\\lambda_{ortho}$ and $\\lambda_{align}$ (Appendix A.1) values used in the experiments? How sensitive is the model's robustness to these new hyperparameters?\n3. Why did the \"DIFF\" strategy (OWM + opposing GPA) perform so much worse than \"SAME\"? Table 1 shows its OWM/GPA metrics look decent, but Figure 2 shows it fails under noise. This implies the direction of the GPA is critical, which is a key finding that seems under-emphasized.\n4. Your conclusion suggests applying OWM and GPA to Transformers. Have you performed any preliminary experiments to see if these principles hold; e.g., does enforcing OWM on the FFN layers or Q/K/V matrices in an attention block improve its adversarial robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6Oo5PjI4hl", "forum": "vmiqjqo08s", "replyto": "vmiqjqo08s", "signatures": ["ICLR.cc/2026/Conference/Submission17777/Reviewer_X79Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17777/Reviewer_X79Z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762029297798, "cdate": 1762029297798, "tmdate": 1762927616440, "mdate": 1762927616440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the Bidirectional Subspace Rotation algorithm (B-SRA), a gradient-free method for training Bidirectional Associative Memories (BAMs). B-SRA extends the Subspace Rotation Algorithm (SRA) from Restricted Hopfield Networks (RHN) to Bidirectional Associative memories. The claims that B-SRA improves the robustness and convergence behavior of BAMs relative to Bidirectional Backpropagation (B-BP). It mentions that a set comprehensive experiments show that orthogonal weight matrices (OWM) and gradient pattern alignment (GPA) are key to the robustness of BAMs. Based on this, the paper claims to introduced regularization techniques to that significantly improved B-BP's resistance to corruption and adversarial perturbation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper tries to improve the robustness of BAMs. This topic is significant because of BAM's suitability for modular neuromorphic hardware design and robust learning. These and other potential benefits of BAMs have led to an increase in interest within the AI research community."}, "weaknesses": {"value": "1) Unsupported claim about Bidirectional Backpropagation (B-BP): The paper makes the strong claim, in the abstract and introduction, that \"B-BP suffers from poor robustness and sensitivity to noise and adversarial attacks\". But the paper cites the Lin et. al 2024 paper to support this claim even though the Lin et. al 2024 paper does not mention B-BP at all, it instead discusses unrelated associative memories.  So the criticisms of B-BP are lack support and the author(s) appear to confuse B-BP with BAM.\nFurther, typing the string \"Noise bidirectional backpropagation\" in Google Scholar produces the 2019 paper in the journal Neural Networks titled \"Noise-boosted bidirectional backpropagation and adversarial learning\". This 2019 B-BP paper demonstrates not just that B-BP is robust to noise, but actually shows how B-BP benefits from blind and non-blind noise injections, see, for instance, the noise plots in Figure 4 and related noise summaries in Tables 8-10. The authors have simply mischaracterized B-BP and provide no support for their central claim.\n\n2) Further unsupported claims about the paper findings: The authors claim to have conducted \"comprehensive experiments\" and \"multiple experiments\",  without presenting this claimed data that they have introduced new \"regularization strategies ...\" into B-BP without producing a mathematical description of \"regularization\" of B-BP. Regularization is a form of penalized or constrained optimization. The paper does not state any such optimization. Again, going to Google Scholar, one finds at least one paper on B-BP Regularization with Hidden Bayesian Priors: \"Hidden Priors for Bayesian Bidirectional Backpropagation\", and the 2023 proceeding of the IEEE SMC. So, again, the authors fail to support their claim, or even clearly define it.\n\n3) Insufficient information on OWM and GPA: There is insufficient information about these two principles. It is important to clarify what they mean in the context of BAM training because they are central to the design of the new regularization strategies in this paper."}, "questions": {"value": "1)  Could you clarify your claim about Bidirectional Backpropagation (B-BP) ?  (See number 1 under weaknesses)\n\n2)  Please respond to the point on B-BP regularization?  (See number 2 under weaknesses)\n\n3). Add more information about key components of the paper: SRA, OWM, and GPA.\n\n4) Are there experimental results illustrating the impact of OWM and GPA on the robustness of BAMs in settings other than B-BP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xBo5caxaHe", "forum": "vmiqjqo08s", "replyto": "vmiqjqo08s", "signatures": ["ICLR.cc/2026/Conference/Submission17777/Reviewer_DAJk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17777/Reviewer_DAJk"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762148452316, "cdate": 1762148452316, "tmdate": 1762927615730, "mdate": 1762927615730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}