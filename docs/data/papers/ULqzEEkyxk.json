{"id": "ULqzEEkyxk", "number": 25363, "cdate": 1758367193656, "mdate": 1759896723589, "content": {"title": "LLMs Leak Training Data Beyond Verbatim Memorization via Membership Decoding", "abstract": "Extracting training data from large language models (LLMs) exposes serious memorization issues and privacy risks. Existing attacks extract data by generations, followed by membership inference. However, extraction attacks do not guide such generations, and the extraction scope of member data is limited to the greedy decoding scheme. Only verbatim memorized member data is being audited in this process. And a majority of member data remains unexplored, even if it is partially memorized. In this work, we define a new notion of memorization, $k$-amendment-completable, to measure the degree of partial memorization. Greedy decoding can only extract \n$0$-amendment-completable sequences, which are verbatim memorized. To address the limitation in generation, we propose a membership decoding scheme, which introduces membership information to guide the generation process. We formulate the training data extraction problem as an iterative member token inference problem. The token distribution is calibrated with membership information at each generation step to explore member data. Extensive experiments show that membership decoding can extract novel member data that haven't been studied before. The proposed attack manifests that the privacy risk in LLMs is underestimated.", "tldr": "", "keywords": ["Membership Inference Attacks", "Privacy", "LLMs", "Data Extraction Attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e14ceb430f82c81d1d021fc97c331ca3d9d12bcb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "I list the main contributions of this work as following:\n\n- Introduction of a new memorization notion termed \"k-amendment-completable,\" which quantifies partial memorization by measuring how many tokens require amendment during greedy decoding to generate the actual training sequence. \n- A membership decoding framework that treats the training data extraction problem as an iterative sequence of token-level membership inference attacks. Rather than performing membership inference after generation, this approach affects the generation process itself using membership information.\n- A novel token-level membership inference attack score based on maximizing the posterior probability of observing the member prefix, which calibrates token distributions using reference models and unifies several existing MIA methods under a common framework."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper builds on a strong theoretical framework for understanding partial memorization in language models with proper mathematical explanation\n- Identifying and quantizing non-verbatim memorization has been an important problem statement in AI security and privacy for long and this paper tackles this exact problem, making this work significant for the field\n- Proposes a new way to identify non-verbatim memorization in LLMs, overcoming one of the primary limitations that of attacks which use greedy decoding only"}, "weaknesses": {"value": "- There is a lack of ablations in this study and the evaluation is restricted to the Pythia model family, which represents older and smaller-scale architectures compared to contemporary models\n- Table 2 is very unclear, why are most of the positions are blank, these missing results seriously puts into question the validity of the results of this study\n\nOverall, this research paper presents a great novel idea and framework but fails to present solid empirical evidence for it's effectiveness. The scope of the experiments is very limited and the results presented do not seem strong enough.\n\nThere is a HARD setting mentioned in the paper, however, no results could be found for this setting in the paper"}, "questions": {"value": "Kindly let us know if you could present any more results/values to support this paper\n\nCould you provide a reasoning for the somewhat similar performance of this method to Minus on k=1 for HackerNews, Pile-CC\n\nWhy weren't other open-source models like OLMo et cetera tried?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZJhHtSmsyx", "forum": "ULqzEEkyxk", "replyto": "ULqzEEkyxk", "signatures": ["ICLR.cc/2026/Conference/Submission25363/Reviewer_2NzT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25363/Reviewer_2NzT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552254108, "cdate": 1761552254108, "tmdate": 1762943414389, "mdate": 1762943414389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers the problem of extracting training data from Large Language Models (LLMs), following the way of first reconstructing sequences then checking their membership in the training data. The proposed method is to introduce membership information into the decoding process to guide the generation of training sequences, such that the generated sequences are more likely to be in the training data. The authors define a new concept, k-amendment-completable, to quantify the degree of partial memorization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The studied problem is important and relevant.\n- The paper is generally well-written."}, "weaknesses": {"value": "- The definition of k-amendment-completable may not fully capture the memorization behavior of LLMs.\n- The approach of breaking down sentence-level membership inference into token-level inference may lead to ambiguities."}, "questions": {"value": "The definition of k-amendment-completable does not fully capture the memorization behavior of LLMs, as it fails to account for the length of the prefix. In the extreme case where the prefix comprises almost the entire sequence except for one token, even a 0-amendment-completable sequence does not necessarily indicate that the model has memorized it.\n\nDecomposing sentence-level membership problems into token-level ones does not seem reasonable. Consider the following scenario: LLMs are trained on massive text corpora in which many sentences share common prefixes. For example, the prefix “In conclusion,” could appear in numerous different sentences. If we attempt to infer membership at the token level, the next token following “In conclusion,” could vary widely depending on the specific sentence. Therefore, inferring membership at the token level may lead to ambiguous or incorrect conclusions about whether the entire sentence was part of the training data. How do the authors address this potential ambiguity in their token-level membership inference approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nFw97cFmpc", "forum": "ULqzEEkyxk", "replyto": "ULqzEEkyxk", "signatures": ["ICLR.cc/2026/Conference/Submission25363/Reviewer_Z27i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25363/Reviewer_Z27i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581025530, "cdate": 1761581025530, "tmdate": 1762943414003, "mdate": 1762943414003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel approach to extract training data from large language models (LLMs) beyond verbatim memorization. The authors define a new notion of memorization called \"k-amendment-completable\" to measure the degree of partial memorization. They propose a membership decoding scheme that guides the generation process to extract non-verbatim memorized data by leveraging membership information at each generation step. The paper demonstrates that larger models memorize more training data than smaller models and shows that their membership decoding approach can extract novel member data that hasn't been studied before. The authors also introduce a new evaluation framework that measures extraction accuracy based on k values."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: The paper introduces a novel concept (\"k-amendment-completable\") that provides a fine-grained measure of partial memorization, addressing a significant gap in the literature beyond verbatim memorization.\n\nS2: The membership decoding scheme is well-motivated and theoretically sound, providing a systematic way to extract non-verbatim memorized data by incorporating membership information at each generation step.\n\nS3: The paper provides empirical evidence that larger models memorize more training data than smaller models, which is an important finding for understanding privacy risks in LLMs.\n\nS4: The evaluation framework (measuring extraction accuracy by k values) provides a more nuanced understanding of what data can be extracted from LLMs, moving beyond traditional verbatim extraction.\n\nS5: The paper makes a compelling argument that a majority of member data remains unexplored even if it's partially memorized, which significantly expands the scope of privacy risks in LLMs."}, "weaknesses": {"value": "W1: The paper lacks sufficient comparison with existing methods that aim to extract non-verbatim memorized data, making it difficult to fully assess the novelty and superiority of the proposed approach.\n\nW2: The evaluation is limited to a few datasets (HackerNews, Pile-CC, PubMed Central, ArXiv) and model sizes (1B, 1.4B, 2.8B, 6.9B, 12B), which limits the generalizability of the findings to other LLM architectures and training data.\n\nW3: The paper doesn't thoroughly discuss the practical implications of the proposed attack for real-world privacy risk assessment, particularly how the extraction accuracy translates to actual privacy risks in deployed models.\n\nW4: The theoretical justification for the membership decoding approach could be strengthened with more detailed mathematical analysis and comparison to related work.\n\nW5: The paper doesn't address potential defenses against the proposed attack, which would provide a more complete picture of the privacy implications."}, "questions": {"value": "Q1: Could you provide a more detailed comparison between your membership decoding approach and existing methods for extracting non-verbatim memorized data? This would help clarify the novelty and advantages of your approach.\n\nQ2: How would the proposed method perform on a wider range of datasets and model architectures beyond those tested in the paper? A more comprehensive evaluation would strengthen the generalizability of your findings.\n\nQ3: Could you explore the practical implications of your findings for real-world privacy risk assessment? How do the extraction rates at different k values translate to actual privacy risks in deployed LLMs?\n\nQ4: How does the proposed membership decoding approach scale with model size and complexity? A more detailed analysis of the computational cost and time requirements would be valuable for practical implementation.\n\nQ5: Could you discuss potential defenses against the proposed attack? This would provide a more complete picture of the privacy implications and help guide future work on privacy-preserving LLMs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OPXmfIPUg9", "forum": "ULqzEEkyxk", "replyto": "ULqzEEkyxk", "signatures": ["ICLR.cc/2026/Conference/Submission25363/Reviewer_MaY6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25363/Reviewer_MaY6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815064459, "cdate": 1761815064459, "tmdate": 1762943413765, "mdate": 1762943413765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the training data leakage risk in Large Language Models (LLMs). The authors point out that existing extraction attacks (e.g., Carlini et al., 2021) heavily rely on \"verbatim memorization\" and \"greedy decoding,\" which significantly underestimates the true privacy risk of these models. To address this, the paper presents two core contributions: (1) A new memorization metric, \"k-amendment-completable,\" to finely quantify \"partial memorization.\" (2) A new attack framework, \"Membership Decoding,\" which reframes extraction as an \"in-generation,\" iterative, \"token-level membership inference\" (MIA) problem, calibrated by a reference model. Experiments on Pythia demonstrate that this method can successfully extract partially memorized sequences (where the 'k' value is 1 or 2) undiscoverable by greedy decoding, confirming a broader data leakage risk."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Paradigm Shift\n- Fine-grained & Effective Metric\n- Solid Formulation\n- Strong Empirical Evidence"}, "weaknesses": {"value": "- Limited Attack Scope\n- Dependency on Heuristics\n- Need for Reference Model"}, "questions": {"value": "The authors have proposed a novel and important framework for evaluating and extracting \"partially memorized\" data from LLMs, which is crucial for understanding their privacy boundaries. The paper is well-argued, the experiments are solid, and this work opens up a new and valuable research direction.\n\nHowever, I do have the following comments:\n- The main limitation is the scope of the 'k' value. As shown in Table 2, the effectiveness is currently almost exclusively limited to 'k=1, 2'. I hope the authors can discuss in more detail the core challenges of extending this to larger 'k' values. Is it merely because the signal weakens as 'k' increases? Or is there a combinatorial explosion problem? Suggestion: Have the authors considered combining \"Membership Decoding\" with Beam Search, retaining the 'B' highest-MIA-score candidate sequences at each step, rather than just the Top-1?\n- The attack's effectiveness relies heavily on the assumption that the correct member token must be within the Top-20 most probable tokens. This is a strong constraint. Suggestion: Could the authors add an analysis in the appendix examining what proportion of failed 'k=1, 2' extractions were due to the true member token falling out of this Top-20 set? This would help us understand the bottleneck of this heuristic.\n- The necessity of a reference model (Pythia-170m) limits the attack's universality in a fully black-box scenario. Suggestion: Have the authors considered (or do they plan for future work) reference-free calibration methods? For example, using the target model itself with dropout, or approximating the denominator (the probability of the token appearing) using token statistics from a large corpus.\n- The choice of 'a=0.5' is presented as a \"trade-off.\" Could the authors provide a sensitivity analysis for 'a' (e.g., across 0, 0.25, 0.5, 0.75, 1.0)? This would make the robustness of Equation 8 more convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CO88qlclNL", "forum": "ULqzEEkyxk", "replyto": "ULqzEEkyxk", "signatures": ["ICLR.cc/2026/Conference/Submission25363/Reviewer_nsBE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25363/Reviewer_nsBE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961840882, "cdate": 1761961840882, "tmdate": 1762943413526, "mdate": 1762943413526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}