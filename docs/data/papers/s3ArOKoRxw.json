{"id": "s3ArOKoRxw", "number": 21015, "cdate": 1758312849111, "mdate": 1759896946956, "content": {"title": "Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs", "abstract": "Multi-agent systems (MAS) built on Large Language Models (LLMs) are being used to approach complex problems and can surpass single model inference. However, their success hinges on navigating a fundamental cognitive tension: the need to balance broad, divergent exploration of the solution space with a principled, convergent synthesis to the optimal solution. Existing paradigms often struggle to manage this duality, leading to premature consensus, error propagation, and a critical credit assignment problem that fails to distinguish between genuine reasoning and superficially plausible arguments. To resolve this core challenge, we first propose the Multi-Agent Exploration–Synthesis framework Through Role Orchestration (Maestro), a principled paradigm for collaboration that structurally decouples these cognitive modes. Maestro uses a collective of parallel Execution Agents for diverse exploration and a specialized Central Agent for convergent, evaluative synthesis. To operationalize this critical synthesis phase, we then introduce Conditional Listwise Policy Optimization (CLPO), a reinforcement learning objective that disentangles signals for strategic decisions and tactical rationales. By combining decision-focused policy gradients with a list-wise ranking loss over justifications, CLPO achieves clean credit assignment and stronger comparative supervision. Experiments on mathematical reasoning and general problem-solving benchmarks demonstrate that Maestro, coupled with CLPO, consistently outperforms existing state-of-the-art multi-agent approaches, delivering absolute accuracy gains of 6\\% on average and up to 10\\% at best.", "tldr": "", "keywords": ["Multi-Agent", "Structured Collaboration", "Policy Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/377203654ee064697e59af7e44a7c306976f2e46.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces MAESTRO, a collaborative reasoning paradigm that structurally separates divergent exploration from convergent synthesis, then broadcasts the endorsed solution to guide the next round. To train the selector without entangling “which answer to pick” and “how to justify it,” the authors propose Conditional Listwise Policy Optimization (CLPO): a decision‑focused policy‑gradient for the choice tokens plus a listwise ranking loss over rationales, with KL and entropy regularizers. On math and general benchmarks, MAESTRO+CLPO reports consistent gains over strong single‑ and multi‑agent baselines and an ablation shows CLPO outperforms SFT and GRPO variants. (Fig. 1 & contributions, p. 2; Sec. 3.2–3.3, pp. 4–6; Table 1, p. 6; Tables 2–3, p. 7)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "--  Importance of the problem. The paper pinpoints a central tension in multi‑agent LLMs—divergent exploration vs convergent critique—and argues that collapsing these modes causes premature consensus, error propagation, and muddled credit assignment.\n\n-- Clear motivation and design. MAESTRO operationalizes exploration–synthesis via (i) many parallel Execution Agents, (ii) a single Central Agent for discriminative selection, and (iii) broadcast to condition later rounds. The figure and text make the flow easy to follow; the coverage–identification factorization (with a tail bound on success probability) adds a crisp objective lens. (Fig. 1 & bullets, p. 2; Sec. 3.2, pp. 4–5). \n\n-- Theoretical lens for robustness. The coverage–identification view defines p_t (any correct candidate in the slate) and q_t (selector picks a correct one conditional on coverage) and proves a cumulative reliability inequality: if p_t\\!\\ge p and q_t\\!\\ge q a.s., then success within R rounds is \\ge 1-(1-pq)^R; equivalently R\\ge \\tfrac{1}{pq}\\log \\tfrac{1}{\\delta} achieves 1-\\delta. (Sec. 3.2, p. 5; Appx. B, pp. 16–17). \n\n-- Claims fits with the evidence. The paper not only shows accuracy but also decomposes into coverage and identification (Table 4) to substantiate that improvements stem from better selection with CLPO (higher q_t). (Table 4, p. 19)."}, "weaknesses": {"value": "-- Closest prior art not directly compared. While the paper cites broad multi‑agent and RL lines, it does not empirically compare to recent equilibrium/game‑theoretic coordination methods (e.g., multi‑agent equilibrium/consensus games) that also provide convergence/guarantee‑style arguments\n\n-- Scope of theory vs practice. The coverage–identification bound is elegant, but p_t,q_t are latent and not estimated online; the paper does not connect training dynamics to measured changes in p_t and q_t beyond end‑state decompositions, nor analyze CLPO convergence properties. (Sec. 3.2–3.3, pp. 4–6; Table 4, p. 19). \n\n-- Baseline parity & training differences. On LLaMA‑8B, the central policy receives SFT/GRPO/CLPO fine‑tuning, whereas many baselines are prompt‑only frameworks; although the authors include “MAESTRO w/ SFT/GRPO/CLPO” rows, parity (e.g., giving debate/routing systems similar optimization) is not explored. (Table 1 & setup, pp. 6–7; Appx. C.1, pp. 18–19)."}, "questions": {"value": "Besides weakness above, i have some questions to discuss with author:\n\nBeyond the post‑hoc Table 4, can you estimate p_t and q_t online during training to verify the reliability inequality’s predictions (e.g., plot pq vs. rounds and success probability)? (Sec. 3.2, p. 5; Table 4, p. 19).  \n\nFig. 2 shows selection’s superiority. Does this persist when the candidate pool is noisier (e.g., larger K with weaker agents) or when rationales are masked/noised? (Sec. 4.2, p. 8).\n\nTransfer beyond math: Any early results on tool‑using or planning tasks where end‑state verifiability is weaker, to test CLPO’s rationale‑ranking leverage? (Sec. 4.1–4.2, pp. 6–8)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5qeS2TGMeH", "forum": "s3ArOKoRxw", "replyto": "s3ArOKoRxw", "signatures": ["ICLR.cc/2026/Conference/Submission21015/Reviewer_d9LG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21015/Reviewer_d9LG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899349905, "cdate": 1761899349905, "tmdate": 1762999980107, "mdate": 1762999980107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Maestro, a technique for using a multi-agent LLM setup to improve performance on a range of benchmarks. The paper’s method contributes (1) a method for enabling divergence of solutions considered followed by convergence to a final solution, and (2) a modified RL technique for fine-tuning such approaches. They demonstrate superior performance on a range of benchmarks and perform a range of ablation and analysis experiments."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "I overall find this paper fairly strong.\n\nOriginality: both the multi-agent framework, sans RL, and the RL are to my knowledge novel contributions. Further, I think the way the method is constructed is principled in a way that has clear relation to and interpretable differences from prior work — it’s a natural but still creative addition.\n\nQuality: the experiments are extensive. The headline comparisons are good and the analysis experiments help answer a range of natural questions (how does it scale with number of agents? How does it compare to natural baselines for building consensus?). The performance decomposition into coverage and identification rates does a nice job of tying the experiments to methodological considerations introduced earlier.\n\nClarity: I generally found this work quite clear.\n\nSignificance: the results appear to be quite strong, with uniform improvement across settings and evidence that the method’s components all help."}, "weaknesses": {"value": "Part of this stems from a question: I’d like to better understand how you control for inference budget. The appendix states that you match for collaboration budget (rounds, agents, and generations). Can one match all three, for all baselines? If so, how? Related, I’m trying to contextualize the results of Fig 2 with Fig 1: if I’m understanding correctly, the difference between SC and Central-Gen+SC is that Central-Gen+SC allows the convergence stage access to all of the divergent stage generations, then it gets to generate a number of candidates on its own (is that right?). Given that this seems to perform much better to SC…is the computational budget matched? Obviously computational budget matching would be helpful to consider here, given how some baselines might scale.\n\nRelated to this, it’s interesting that there appears to be degradation after a point in scaling agents, samples, and rounds. The intuition for this makes sense, but at the same time, suggests that this scales less well than naive baselines.\n\nI also wish that some of the experiments (namely the Tab 2 and 3, in particular) were run on benchmarks that are a bit less saturated."}, "questions": {"value": "I’d largely like help with understanding how baselines were compute matched, and my other scaling questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sgeBZK1ixe", "forum": "s3ArOKoRxw", "replyto": "s3ArOKoRxw", "signatures": ["ICLR.cc/2026/Conference/Submission21015/Reviewer_VEx9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21015/Reviewer_VEx9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964324737, "cdate": 1761964324737, "tmdate": 1762999980326, "mdate": 1762999980326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MAESTRO, a multi-agent framework that explicitly separates divergent exploration (many execution agents generate K candidate reason-answer pairs) from convergent synthesis (a central agent selects one candidate and broadcasts it to guide later rounds). The core learning contribution is Conditional Listwise Policy Optimization (CLPO), which disentangles the update on decision tokens (which candidate to endorse) from rationale tokens (why), combining a choice-focused policy-gradient term with a listwise ranking loss over justifications, plus KL and entropy regularization. The authors analyze coverage vs identification (pt, qt) and give a simple success bound over T rounds. They report gains over single-agent and multi-agent baselines across common STEM+coding benchmarks. they also compare central generation vs central selection, arguing selection identifies correct candidates more reliably."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The motivation is clear and problem is well-framed. The divergent vs convergent tension in collaborative LLMs is well articulated, and MAESTRO maps directly onto that cognitive split. This makes the system design easy to reason about. The credit assignment issue has been a pain point in the field of LLM RL, and any attempt to solve this problem is an appreciated effort.\n- The paper is well-written and well articulated."}, "weaknesses": {"value": "- Claimed improvements of about 1% over GRPO on math/coding/MMLU (~2% on AMC) are too small to justify the substantially more contrived objective and orchestration. I find it hard to see these increments as significant improvement over GRPO.\n- The compute parity are under-specified for both all baselines. t’s unclear whether baselines (incl. GRPO) operate under matched total sampling budget (agent count × K × rounds), identical decoding setups, and equal reference-KL constraints. Without strict budget parity and detailed token-/call-level accounting, comparisons risk favoring MAESTRO merely by extra sampling/coordination."}, "questions": {"value": "- The framework rebroadcasts the endorsed candidate each round, although an $\\epsilon$-mixture is used to retain some base-policy exploration, how does entropy collapse across rounds?\n- How does $\\epsilon$ affect training stability"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9xTVyjZzwp", "forum": "s3ArOKoRxw", "replyto": "s3ArOKoRxw", "signatures": ["ICLR.cc/2026/Conference/Submission21015/Reviewer_uZoA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21015/Reviewer_uZoA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976960968, "cdate": 1761976960968, "tmdate": 1762999980578, "mdate": 1762999980578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Multi-agent systems can be more powerful than single LLM agents alone. Yet, they are difficult to get right because of difficulties in diverging exploration of the solution space by different agents, and convergent synthesis of the found solutions. Particularly, credit assignment is hard for such systems. Therefore, they propose Multi-Agent Exploration Synthesis framework through Role Orchestrating (MAESTRO) in combination with Conditional Listwise Policy Optimization (CLPO) to enable effective multi-agent systems. Their experiments on commonly used reasoning tasks show gains compared to previous approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors address an important problem in multi-agent LLM systems, that is, divergent exploration followed by convergent synthesis of the explored solution space.\n2. The proposed MAESTRO framework seems sound and nicely decomposes exploration and convergence.\n3. The proposed CLPO RL objective effectively decomposes decisions and reasons to enable more effective credit-assignment.\n4. They provide interesting analyses in section 4.2 on the effect of collaboration mechanisms, parts of the CLPO loss, and the size of the agent population."}, "weaknesses": {"value": "1. It is unclear how diverse the generated solutions in Phase 1 really are. Is there any way to enforce more diversity in this phase?\n2. The proposed CLPO objective (despite good motivation) consists of 4 loss terms with individual weights. However, there is no analysis on the sensitivity of the weighting terms of the proposed components. This would be valuable for the reader to better understand the difficulty of tuning CLPO.\n3. For RL tuning, the paper primarily focuses on the small-scale models (Llama3 3/8B and Qwen 3B/7B). It is unclear how well their recipes transfer to larger-scale models. I understand the difficulty of doing that, given that MAESTRO requires many more rollouts due to the multi-agent setup. However, this is important to understand better. \n4. MAESTRO requires many more rollouts due to the multi-agent setup. However, there is no wall-clock/FLOP analysis in the paper compared to using only a single agent. This would be important for the reader to understand better the tradeoffs between using multiple agents and using a single agent rollout. Generally, I would like to see a FLOP budget or wall-clock time column for all methods in Tables 1,2, and 3. It is possible that the increased scores and the result of the increased compute spent.  \n5. For self-consistency, debate, etc., did you control for the FLOP budget? I.e., do these methods spend a comparable amount of compute to MAESTRO, or is this way less? I think this is an important comparison, especially because improvement margins are slim for some tasks.\n6. I appreciate the ablation studies in Sections 4.2 and 4.3. Interestingly, performance declines as more agents are added, more samples are generated per agent, and more rounds are conducted. This raises concerns regarding the scalability of your recipe. I would be interested in your thoughts on this. \n\nMinor: \n- In section 3, both the question and the identification probability are denoted as $q$."}, "questions": {"value": "Some questions are listed in the Weakness section. Here are some more questions: \n1. Why does the performance on AMC, GSM8K decline as the number of agents increases? What would happen for more than 5 agents? Does this result limit the scalability of your method? How might this change for larger-scale models? \n3. For the number of collaboration rounds, this effect is even more pronounced (Figure 6 in the appendix). What is your intuition why that’s the case, and how can it be overcome?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pvlX5MQvhR", "forum": "s3ArOKoRxw", "replyto": "s3ArOKoRxw", "signatures": ["ICLR.cc/2026/Conference/Submission21015/Reviewer_3wvd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21015/Reviewer_3wvd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21015/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990536748, "cdate": 1761990536748, "tmdate": 1762999980392, "mdate": 1762999980392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}