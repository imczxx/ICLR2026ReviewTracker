{"id": "kGgHappUSR", "number": 18853, "cdate": 1758291539550, "mdate": 1759897077676, "content": {"title": "ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering", "abstract": "Recently, large language models have shown remarkable reasoning capabilities through long-chain reasoning before responding. However, how to extend this capability to visual reasoning tasks remains an open challenge. Existing multimodal reasoning approaches transfer such visual reasoning task into textual reasoning task via several image-to-text conversions, which often lose critical structural and semantic information embedded in visualizations, especially for tasks like chart question answering that require a large amount of visual details. To bridge this gap, we propose ChartReasoner, a code-driven novel two-stage framework designed to enable precise, interpretable reasoning over charts. We first train a high-fidelity model to convert diverse chart images into structured ECharts codes, preserving both layout and data semantics as lossless as possible. Then, we design a general chart reasoning data synthesis pipeline, which leverages this pretrained transport model to automatically and scalably generate chart reasoning trajectories and utilizes a code validator to filter out low-quality samples. Finally, we train the final multimodal model using a combination of supervised fine-tuning and reinforcement learning on our synthesized chart reasoning dataset and experimental results on four public benchmarks clearly demonstrate the effectiveness of our proposed ChartReasoner. It can preserve the original details of the charts as much as possible and perform comparably with state-of-the-art open-source models while using fewer parameters, approaching the performance of proprietary systems like GPT-4o in out-of-domain settings.", "tldr": "We present ChartReasoner, a code-driven framework that effectively bridges the gap between visual chart data and textual reasoning by leveraging structured ECharts representations to enhance chart reasoning capabilities.", "keywords": ["reasoning", "multimodal QA", "reinforcement learning", "multimodality"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b1d3829101b9d2a4cfae5cfc524de88df392a453.pdf", "supplementary_material": "/attachment/1d1267d1a34b9501d4de3cab0a960549d9c65832.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ChartReasoner, a two-stage, code-driven framework designed to enhance long-chain reasoning for MLLMs on Chart Question Answering (ChartQA) tasks. The authors argue that conventional image-to-text methods lose critical information. Their proposed solution first uses a model, Chart2Code, to translate a chart image into a high-fidelity, structured ECharts code representation. This stage is supported by a new synthetic dataset of 110K image-code pairs. In the second stage, this Chart2Code model is applied to existing benchmarks (ChartQA, ChartBench, etc.) to generate code, which is then fed to a long-chain reasoning LLM (DeepSeek-R1) to generate reasoning paths. These paths, filtered by final answer correctness, form a new 140K-sample dataset called ChartThink. The final ChartReasoner model is then trained on this dataset using a combination of Supervised Fine-Tuning (SFT) and Reinforcement Learning (GRPO). The central idea is that code serves as a superior, lossless intermediate modality for complex reasoning."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Interesting Idea: The core idea of using executable code (ECharts) as a symbolic, intermediate representation to bridge the visual-textual modality gap is interesting.\n\n2. Substantial Dataset Contributions: The paper introduces two large-scale datasets: Chart2Code for image-to-code translation and ChartThink for code-based reasoning. The creation and release of these resources are a significant contribution to the community."}, "weaknesses": {"value": "1. High Risk of Train-Test Overlap: This is the most critical issue. The paper states that the ChartThink training dataset is constructed by processing samples from existing benchmarks, including ChartQA and ChartBench. It then evaluates the final model on the test sets of these same benchmarks, labeling them as \"in-domain.\" The paper provides no clarification on whether it excluded the official test splits of these benchmarks during the creation of its training data. Without this explicit separation, there is a high probability of data contamination, where the model has been trained on samples derived from the test set it is being evaluated on. This potential data leakage makes the reported results on these benchmarks unreliable and possibly invalid.\n\n2. Questionable Necessity of the Intermediate Code Step: Although the idea is interesting, the paper's core premise is conceptually questionable. The ability of the Chart2Code model to generate accurate, detailed ECharts code from an image implies that it has already achieved a deep and structured understanding of the chart's components, layout, and data. If the model already possesses this rich internal representation, the necessity of an explicit, separate code-generation step is unclear. It seems plausible that a model with this level of visual parsing capability could be trained to reason directly on its internal representations, making the two-stage pipeline an unnecessarily complex and potentially inefficient detour. The paper fails to justify why this \"extra step\" is indispensable.\n\n3. Quality of Generated Reasoning Traces: The reasoning paths in the ChartThink dataset, which form the basis for the final model's training, are generated automatically by prompting an LLM (DeepSeek-R1). The only quality control measure is to filter out samples where the LLM's final answer does not match the ground truth. This is a very weak form of supervision. It does not guarantee that the reasoning path itself is correct, logical, human-like, or even the most efficient way to solve the problem. The final ChartReasoner model is therefore trained via imitation learning on potentially flawed, unnatural, or suboptimal reasoning logic, which limits the quality and robustness of what it can learn."}, "questions": {"value": "1. Can you please explicitly clarify how you handled the data splits from benchmarks like ChartQA and ChartBench when constructing the ChartThink training dataset? Specifically, did you ensure that no data (images, questions, or answers) from the official test splits were used in any part of your training data generation pipeline?\n\n2. Could you provide a stronger justification for the necessity of the intermediate code generation step? If a model is capable of generating accurate code, it already understands the chart's structure deeply. Why is it not possible to train this model to reason directly, and what evidence do you have that the explicit code-based reasoning is superior?\n\n3. Beyond filtering by final answer correctness, what steps did you take to validate the quality, logical correctness, and naturalness of the LLM-generated reasoning paths in the ChartThink dataset? How can you be sure your model is not simply learning to mimic flawed or unnatural reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zqg3kNxQti", "forum": "kGgHappUSR", "replyto": "kGgHappUSR", "signatures": ["ICLR.cc/2026/Conference/Submission18853/Reviewer_UpE9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18853/Reviewer_UpE9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760684770658, "cdate": 1760684770658, "tmdate": 1762930820799, "mdate": 1762930820799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the problem of chart understanding and argue that existing methods, which convert charts into textual representations, often result in the loss of structured information. To mitigate this, they propose using ECharts code as an intermediate representation. Specifically, they first train a Chart2Code model, which is then used to construct ChartThink inference data. This data is subsequently employed in a two-stage training process for the chart understanding model, resulting in improved performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and comprehensive, presenting a clear and detailed methodology.\n2. The proposed Chart2Code the ChartThink dataset provide valuable resources for the community."}, "weaknesses": {"value": "1. The proposed approach is largely incremental and lacks substantial novelty, with the main contribution being the construction of datasets.\n2. The performance gains are limited; for example, the method underperforms compared to Chart-R1[1] on ChartQA.\n3. Unlike approaches based on Python code, which are more widely applicable, the method relies on ECharts templates. This limits its ability to handle complex or non-standard charts, as well as real-world data not generated with ECharts.\n4. The improvement from RL compared to SFT on ChartQA and ChartBench is minimal, and the authors do not provide a discussion of this observation.\n\n[1]: Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner"}, "questions": {"value": "1. Can the authors clarify the key differences between the proposed method and prior approaches?\n2. Regarding the ChartThink dataset, what is the evaluation procedure for the reasoning chains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NdO3e4xkl6", "forum": "kGgHappUSR", "replyto": "kGgHappUSR", "signatures": ["ICLR.cc/2026/Conference/Submission18853/Reviewer_iLux"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18853/Reviewer_iLux"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728132034, "cdate": 1761728132034, "tmdate": 1762930820310, "mdate": 1762930820310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for chare reasoning by converting the charts into code for accurate understanding. It first introduces the Chart2Code to convert charts into ECode, then builds ChartReasoning for reasoning training. The resulted model ChartReasoner achieves the best performance compared with baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of this paper is clear, demonstrating the significance of chart reasoning.\n2. The performance is good, demonstrating the effectiveness of the method."}, "weaknesses": {"value": "1. In the Chart2Code stage, how to ensure that the code could preserve all information of the charts that the texts could not do? In the quality filtering stage, will there conduct a comparison between the raw chart and the chart that the code corresponds to?\n2. In Fig.12-15, if there lacks digital annotation in the charts, could the LLM generates accurate approximation for the data in the ECode?\n3. In the ChartThink construction process, it seems the reasoning process has not been verified, only the answer is checked to be correct.\n4. The detailed definition of the reward function in the GRPO has not been clearly introduced.\n5. The ablation on SFT and GRPO is supposed to be analyzed."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FINuKAhCmA", "forum": "kGgHappUSR", "replyto": "kGgHappUSR", "signatures": ["ICLR.cc/2026/Conference/Submission18853/Reviewer_Gkjs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18853/Reviewer_Gkjs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893434232, "cdate": 1761893434232, "tmdate": 1762930818946, "mdate": 1762930818946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ChartReasoner, a procedure for training multimodal large language models (MLLMs) to improve their reasoning capabilities on chart question answering tasks. First, the Chart2Code MLLM is trained to translate chart images into Apache ECharts code. Then, the ChartThink dataset is created by pooling chart question answering tasks from four previous datasets, translating the charts to Apache ECharts code using Chart2Code, and using a reasoning LLM to annotate each task with a detailed reasoning trace. Finally, the ChartReasoner model is trained on the ChartThink dataset."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n- The methodology is presented clearly, with enough details to reproduce the dataset generations and model training.\n- The annotated reasoning traces in the ChartThink dataset could be useful for future work.\n- The method itself seems intuitive, with the motivation being clear of \"bridging\" the text-vision modality gap by using code as an intermediate modality."}, "weaknesses": {"value": "Weaknesses\n- It is unclear if translating charts to Apache ECharts code has any tangible performance improvement. There are many existing reasoning LLMs which can take image inputs, including the QvQ-preview model the authors include in their main results. Why not simply pass the chart image itself to these multimodal reasoning LLMs and ask them to generate the reasoning trace?\n- The gains from the ChartReasoner training are very minimal over Qwen2.5-VL 7B, which was the model used for finetuning. ChartReasoner only improves Qwen2.5-VL's performance by 1.9% on average across the four datasets, despite training on in distribution data from 2/4 evaluation datasets. \n- The authors mention multimodal long-chain reasoning approaches such as R1-OneVision and Vision-R1 in the related works section, but don't compare their method to these methods.\n- The Chart2Code performance comparison is questionable, as there is no indication as to how well GPT-4V is able to judge the faithfulness of a chart rendered from code to the original chart."}, "questions": {"value": "- Could the authors clarify the advantages of the ECharts framework over other popular plotting libraries such as matplotlib?\n- It would be nice to know how many tokens the other models in the main results used on average, to compare with ChartReasoner.\n- How does the performance of the SFT + GRPO pipeline compare to a GRPO only pipeline (or in general, how much does training on annotated reasoning traces improve performance)?\n- The authors should explicitly state the reward function used during GRPO."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lY9XOr79RE", "forum": "kGgHappUSR", "replyto": "kGgHappUSR", "signatures": ["ICLR.cc/2026/Conference/Submission18853/Reviewer_9r4X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18853/Reviewer_9r4X"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762038875362, "cdate": 1762038875362, "tmdate": 1762930818533, "mdate": 1762930818533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}