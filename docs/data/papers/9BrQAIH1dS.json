{"id": "9BrQAIH1dS", "number": 21446, "cdate": 1758317655554, "mdate": 1759896921539, "content": {"title": "A predictive coding model of hippocampo-neocortical interactions involved in memory replay", "abstract": "The neocortex and the hippocampus are two complementary learning systems which interact during memory construction and consolidation. The hippocampus stores episodic memories coming from the neocortex passing through the entorhinal cortex, and later replays them back to the neocortex to transform them into semantic memory during memory consolidation. It is thought that memory replay is a generative process, involved in imagining, because new episodes can also be generated and instantiated in the neocortex. Here we present a computational model of hippocampal-neocortical interactions based on a predictive coding network with two hidden layers, which are mapped to the visual cortex and the entorhinal cortex. Improving on a previous implementation of this network, our simulations provide a mechanistic account of memory replay in the neocortex.", "tldr": "", "keywords": ["predictive coding", "neocortex", "hippocampus", "episodic memory", "semantic memory", "generative replay", "neuroscience", "generative model"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/05ece36840bf3b989d8189520055bd41bc1a8ab3.pdf", "supplementary_material": "/attachment/a82897e45a51b324fd6783e4440ef8edf8f263aa.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a 2-layered predictive coding model for neocortical reconstruction and replay. Based on theories and experimental observations that memory replay is driven by a top-down generative model, the predictive coding network successfully developed representations of MNIST digits and generated both memory and generative replays. The paper also studied the impact of the size of the hidden layer.\n\nOverall, this paper presented some interesting experiments and discoveries of PCN in modeling memory replay, but failed to address its relevant literature and lacked analytical/experimental depth."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "I found the class-conditioned generation from PCN's latent representations with multivariate Gaussian fitting interesting. Previous works (e.g., Oliviers et al. 2024) have discovered generative modeling with Langevin dynamics but may have missed this simple yet potentially effective approach to enable PCNs to generate unseen images from a latent distribution."}, "weaknesses": {"value": "Despite the interesting discovery mentioned in strengths, this paper generally lacks both depth and width to be accepted as publication. \n\nProblem with width:\n- The authors have overlooked a considerable body of literature modeling memory in hippocampus using predictive coding networks (see below), which have largely covered 1) how memory and generative replay can be achieved in a PCN; 2) how memory can be stored both hierarchically and recurrently; 3) how the recurrent HPC and hierarchical EC can be combined in a single model. I strongly suggest the authors to carefully review this field and understand the proposed architecture's similarity and difference to these models\n- The paper lacks a broader biological discussion on how the proposed architecture implies about the cortex e.g., is there evidence of error neurons in LGN, VC and EC? What experimentally verifiable predictions have this model made?\n\nProblem with depth:\n- The hierarchical structure of PCN has been extensively studied: in essence, the 2-layered model proposed here is no different from Rao and Ballard's (1999) original predictive coding model of visual cortex - does it mean LGN and VC alone can already perform memory replay? What is the role of multiple layers between LGN and EC if a single layer can already achieve replay/memory? Along this line, I'd suggest the authors to explore more the role of depth, rather than width, of PCNs e.g. in an 8-layer PCN, what are the respective roles of each intermediate layer?\n- The experiments were only performed on MNIST. Today's PCNs can alreay have many layers and different architectures such as convolutional layers (see Pinchetti et al. 2025) to accommodate larger datasets such as ImageNet. Performing these experiments may not add value to the proposed model per se, but will definitely add value to its generalizability and applicability.\n- The analysis of results are rather superficial. For example, Fig5a can benefit from a quantitative evaluation using FID scores of generations; the TSNE plots are not very informative as MNIST digits are known to be quite separable even in image space; some metrics of separability would help.\n\nOther comments:\n- Section 4 is basically a specific case of the equations in Section 3; instead of repeating Section 3, I'd describe in Section 4 how exactly the replay is achieved in mathematical terms.\n\nReferences:\n- Tang, Mufeng, et al. \"Recurrent predictive coding models for associative memory employing covariance learning.\" PLoS computational biology 19.4 (2023): e1010719.\n- Salvatori, Tommaso, et al. \"Associative memories via predictive coding.\" Advances in neural information processing systems 34 (2021): 3874-3886.\n- Salvatori, Tommaso, et al. \"Learning on arbitrary graph topologies via predictive coding.\" Advances in neural information processing systems 35 (2022): 38232-38244.\n- Tang, Mufeng, Helen Barron, and Rafal Bogacz. \"Sequential memory with temporal predictive coding.\" Advances in neural information processing systems 36 (2023): 44341-44355.\n-Oliviers, Gaspard, Rafal Bogacz, and Alexander Meulemans. \"Learning probability distributions of sensory inputs with Monte Carlo predictive coding.\" PLOS Computational Biology 20.10 (2024): e1012532."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "h5ZO1J2f0F", "forum": "9BrQAIH1dS", "replyto": "9BrQAIH1dS", "signatures": ["ICLR.cc/2026/Conference/Submission21446/Reviewer_siHR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21446/Reviewer_siHR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761256445167, "cdate": 1761256445167, "tmdate": 1762941782702, "mdate": 1762941782702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a predictive coding model of episodic and generative replay involving the hippocampus and neocortex. The work spans a broad range of prominent ideas in predictive coding (Friston, Rao, Ballard etc), complementary learning systems & memory consolidation (McClelland etc), hippocampal replay, generative modelling such as VAEs and their application to memory consolidation (e.g. Spens & Burgess).\n\nThe highest level of the predictive coding hierarchy (containing the abstract latent states, MNIST digits in this paper) can be sampled from (as in VAE) to generate experiences - this is framed by the authors as performing the role of the hippocampus (which is not explicitly modelled in this work). From my understanding this differs from the role of HPC in systems consolidation models (e.g. Spens) where both HPC and neocortex receive sensory input x and each try to reconstruct the input x or predict a target y; in this way, hippocampal replays can provide the input/outputs for training the neocortical network. Here instead, the hippocampus is cast as generating a sample of neocortical (EC) activity itself i.e. the latents of the VAE.\n\nThe central result is that when activity memories are sampled at the top level (analogous to hippocampal replay), the network generates qualitatively recognisable samples from that class. Classes are separable clusters in each level when projected into low dimensions with t-SNE. More neurons in the highest latent layer increases the expressiveness of the abstract latent state and as a result the generated replays are more precise."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The mathematical description and diagram of the model are clear and easy to follow, as is the main text.\n\nThe work improves upon the result from Fontaine & Alexandre by demonstrating non-overlapping digit clusters in the latent spaces.\n\nTo my knowledge it is a novel idea to frame the top level of the PC hierarchy as a hippocampus, although the significance is not clear"}, "weaknesses": {"value": "I'm skeptical of the novelty of using PCNs to generate samples - they are generative models so it must follow that they can generate predictions, e.g. by clamping the top level. I also believe PCNs have been shown to accurately model MNIST digits. Unless I'm mistaken, the novel contribution then is relating the generation via sampling to hippocampus - however, beyond conception of this idea, the work in its current form does little to go beyond the (I believe) established fact that PCNs can generate predictions from sampling. \n\nI believe it would also be helpful to relate back to memory consolidation, given that it is a core part of the story told in the abstract and introduction - in the current model, HPC cannot be used to train the neocortex since it does not generate inputs/outputs (as in Spens & Burgess) upon which cortex can be trained. If the model could be developed in such a way that these generative replays are functionally useful for training the PCN or downstream neocortical networks then it would be much more compelling. I believe one of the results of systems consolidation is that generalisable facts (mappings from x->y) get abstracted into neocortex but that irregular/incongruent facts are stored in hippocampus - perhaps in this PC model there would be interesting analyses of error neurons in these cases, or even that the error responses themselves are what get stored in hippocampus?\n\nI feel in general it could benefit from a bit more 'meat' - the suggestions above are only suggestions but I do feel exploring the consolidation part further, or how predictive coding (via an extension to your modelling) can shed new light into consolidation - e.g. psychological phenomena, error driven responses etc. Perhaps a multimodal network could help? I think there might be much more compelling arguments you can make than focusing on the layer size!"}, "questions": {"value": "- How is replay error computed? Is it the reconstruction error in the top-down regime when the network predicts input using a previously seen latent sample?\n- Second layer is just a linear layer - is it surprising that performance is the same with L=1 when the linear layer doesn't add any computational power?\n- Why does validation error increase with larger layer 0? It seems to contradict the fact that classification \n- Does the validation error trend in figure 6 contradict that of fig 3?\n- \"However, little work has been done in computational neuroscience to study how this theory can account for the functions\nof the neocortex, in a biologically plausible way.\" - is this true? I was under the impression that predictive coding theory and modelling literature was extensive, focused on biological plausibility, and that PCNs have been used to model a variety of functions (though predominantly in the sensory domain)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J9pdoflnHP", "forum": "9BrQAIH1dS", "replyto": "9BrQAIH1dS", "signatures": ["ICLR.cc/2026/Conference/Submission21446/Reviewer_fWGk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21446/Reviewer_fWGk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838550754, "cdate": 1761838550754, "tmdate": 1762941782097, "mdate": 1762941782097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a computational model of memory replay, framing it as a computational process based on predictive coding (PC). This work uses a 3-level hierarchical predictive coding network (PCN) trained on the full MNIST dataset. The model's layers are explicitly mapped to a simplified visual hierarchy: the input layer (level 2) is mapped to the LGN, the hidden layer (level 1) to the visual cortex (VC), and the top layer (level 0) to the entorhinal cortex (EC).The paper demonstrates two distinct forms of memory replay: (a) Experience Replay: This involves clamping a previously stored latent representation (from an item in the training set) onto the top EC layer. To allow this top-down signal to generate a percept, the precision of the input LGN layer is set to zero. This gates the network from bottom-up sensory input and allows the top-down prediction to flow down, generating a replayed image at the input. (b) Generative Replay: This is presented as a novel form of replay. The authors fit a class-conditioned multivariate Gaussian distribution to the entire latent space of the EC layer (using all stored representations from the training set). By sampling new latent codes from this Gaussian, they can then use the same top-down replay mechanism to generate novel, unseen exemplars of the digits.The central and most interesting finding of the paper is a non-obvious trade-off: increasing the number of hidden units in the top (EC) layer ($n_0$) worsens the network's ability to reconstruct novel inputs (the validation error increases). However, this same change improves the quality of its replayed images (the replay error decreases) and simultaneously improves the linear separability of the latent representations, as measured by a simple logistic regression classifier."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper does an excellent job explaining PC and mapping it clearly to a specific cognitive function (memory replay) and its supposed neuro-anatomical substrate (LGN, VC, EC). The distinction between the \"perception\" (encoding) and \"replay\" (decoding/generative) pathways is elegant. The explanation of the mechanism for replay—clamping the top layer and \"gating\" the bottom layer by setting its precision ($\\Sigma_2^{-1}$) to zero—is a biologically-plausible hypothesis for empirical phenomenon observed in the literature.\n\nThe central discovery of a trade-off between reconstruction and replay quality/separability is the most valuable part of this paper in my opinion. The finding that increasing top-level units hurts reconstruction while helping replay and separability suggests a functional specialization in a hierarchy: lower-level layers may be optimized for precise fidelity, while higher-level layers are optimized for abstract, separable representations that are good for replay and classification. This finding, in isolation, is a useful contribution to the learning representation community, as it provides a concrete example of how a hierarchical system might resolve the tension between detailed reconstruction and abstract categorization.\n\nThe work is well-grounded in neuro-inspired principlesn and relevant empirical observations. It provides a mechanistic, more biologically plausible account of generative memory replay that does not rely on biologically implausible mechanisms like backpropagation. It connects a known cognitive function (replay) to a specific, testable neural mechanism (top-down generation, precision-gated)."}, "weaknesses": {"value": "Although the results stated in strength are interesting, the authors could have done a much better job framing their contribution wrt the literature, as some of the interpretations in the submission is directly related to, if not identifical with prior, unreference works. Here are a few critical omissions:\n\n1. https://arxiv.org/abs/2109.08063. This work is quite critical as it's one of the earliest works to connect memory/replay to PC with a computational model & experiments on CV benchmarks on MNIST. It also makes the connection of PC to the hippocampus as a \"memory index and generative model\". \n\n2. https://philpapers.org/rec/EDLPPC. This thesis explicitly reconstructs the overarching conceptual argument this paper relies on: that PC is a \"unifying theory of perception, imagination, memory, and dreaming,\" implemented across the brain's hierarchies. It explains imagination as \"offline perception\" and dreaming as \"unconstrained imagination\" by modulating sensory input/precision—the exact mechanism this paper uses for replay. Although the computational model is less sophisticated than the one presented in the submission, it is crucial to ground the discussion by properly attributing the intellectual lineage of the idea that PC could be a unified theory for various mental functions.\n\n3. https://direct.mit.edu/neco/article/37/8/1373/131383/Predictive-Coding-Model-Detects-Novelty-on. This paper follows the same theme of hierarchical abstraction as the current submission. It uses PCNs to show that different layers detect novelty at different levels of abstraction: \"sensory (pixel) novelty\" in low layers versus \"semantic (digit) novelty\" in high layers. This directly overlaps with the current submission's theme of finding separable representations in the top (EC) layer. Moreover, the idea that different brain hierarchy/areas can correspond to different layers in a hierarchical PCN is already fleshed out in that paper, which the current submission claims without references."}, "questions": {"value": "My biggest question would be how would your contextualize the findings and novelty wrt the literature such as discussed in weaknesses. Also, the core finding of the trade-off in Figure 3 is interesting and what is your hypothesis for why this occurs? Why does a wider top-level layer ($n_0$) hurt reconstruction (increase validation error) while improving replay and separability? Does this imply that the top layer is being forced to learn more abstract, compressed representations that discard instance-specific variance (which hurts reconstruction) but better capture the class-defining essence (which aids separability and replay)? I'm currently on the fence regarding accpet/reject of this paper and may be swayed if my questions are addressed sufficiently."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ssP0wTqGyF", "forum": "9BrQAIH1dS", "replyto": "9BrQAIH1dS", "signatures": ["ICLR.cc/2026/Conference/Submission21446/Reviewer_wSYg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21446/Reviewer_wSYg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848811263, "cdate": 1761848811263, "tmdate": 1762941781600, "mdate": 1762941781600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a biologically motivated predictive coding model to simulate memory replay and generative imagination in hippocampo-neocortical interactions. Using the traditional predictive coding frameworks, it studies both episodic replay of stored memories and generative replay to simulate imagination. They provide detailed quantitative and qualitative evaluations on MNIST, analyzing reconstruction accuracy, replay quality, and latent class separability, offering insights about how network architecture impacts replay dynamics and memory consolidation processes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper aligns predictive coding architectures with biologically realistic neural circuitry from the Rao and Ballard algorithm (paper not cited!). It distinguishes episodic replay (recall) from generative replay (imagination), contributing to emerging efforts to model both memory consolidation and creative generation within a unified predictive coding system."}, "weaknesses": {"value": "There is little novelty in this work: there is a large body of literature that tests predictive coding networks on associative memory problems. First, from the more \"ML\" side of image storage and retrival, there is a work that uses the same framework you propose, but goes beyond the MNIST experiment you performed, all the way to ImageNet [1]. Then, there are multiple extensions: the first uses a categorical prior on the last layer to provide the model with exponential capacity [2], another one, more focused on the neurosciences, maps this model to different brain regions, developing a theory of how recurrent PCNs can be attractor points [3]; a third one updates the original algorithm to provide the PCN with a write-erase system that allows you to free the memory of datapoints you are not interested in [4]. There are also extensions on the use of PCNs for temporal data [5], and grid cells [6]. \n\n[1] Salvatori, Tommaso, et al. \"Associative memories via predictive coding.\" Advances in neural information processing systems 34 (2021): 3874-3886.\n\n[3] Tang, Mufeng, et al. \"Recurrent predictive coding models for associative memory employing covariance learning.\" PLoS computational biology 19.4 (2023): e1010719.\n\n[4] Yoo, Jinsoo, and Frank Wood. \"Bayespcn: A continually learnable predictive coding associative memory.\" Advances in Neural Information Processing Systems 35 (2022): 29903-29914.\n\n[5] Tang, Mufeng, Helen Barron, and Rafal Bogacz. \"Sequential memory with temporal predictive coding.\" Advances in neural information processing systems 36 (2023): 44341-44355.\n\n[6] Tang, Mufeng, Helen Barron, and Rafal Bogacz. \"Learning grid cells by predictive coding.\" arXiv preprint arXiv:2410.01022 (2024)."}, "questions": {"value": "How does your model differ from that of Salvatori et al.?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6mOnnDqucS", "forum": "9BrQAIH1dS", "replyto": "9BrQAIH1dS", "signatures": ["ICLR.cc/2026/Conference/Submission21446/Reviewer_YYAe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21446/Reviewer_YYAe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855807924, "cdate": 1761855807924, "tmdate": 1762941780959, "mdate": 1762941780959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}