{"id": "Y2tKygPoUp", "number": 13136, "cdate": 1758213971366, "mdate": 1763770612557, "content": {"title": "LIME: Link-based user-item Interaction Modeling with decoupled xor attention for Efficient test time scaling", "abstract": "Scaling large recommendation systems requires advancing three major frontiers: processing longer user histories, expanding candidate sets, and increasing model capacity. While promising, transformers' computational cost scales quadratically with the user sequence length and linearly with the number of candidates. This trade-off makes it prohibitively expensive to expand candidate sets or increase sequence length at inference, despite the significant performance improvements.\n\nWe introduce \\textbf{LIME}, a novel architecture that resolves this trade-off. Through two key innovations, LIME fundamentally reduces computational complexity. First,  low-rank ``link embeddings\" enable pre-computation of attention weights by decoupling user and candidate interactions, making the inference cost nearly independent of candidate set size. Second, a linear attention mechanism, \\textbf{LIME-XOR}, reduces the complexity with respect to user sequence length from quadratic ($O(N^2)$) to linear ($O(N)$).\n\nExperiments on public and industrial datasets show LIME achieves near-parity with state-of-the-art transformers but with a 10$\\times$ inference speedup on large candidate sets or long sequence lengths. When tested on a major recommendation platform, LIME improved user engagement while maintaining minimal inference costs with respect to candidate set size and user history length, establishing a new paradigm for efficient and expressive recommendation systems.", "tldr": "We compress user sequence representation via xor-attention, and cache item representation via decoupled target attention, to scale up candidate size and user history length in recommendation systems.", "keywords": ["decoupled attention", "recommendation system", "test time scaling", "xor attention", "linear attention", "ranking"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e875274ee61fe557999ab1d0847c4efcb8ef2844.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents LIME, which aims to solve the complexity of standard Transformer with self-attention for tackling long user sequence. Theoretically, it reduces the theoretical complexity from $O(N^2)$ to $O(N)$. Empirically, it demonstrates that the speedup happens given long sequence ($N\\geq 4096$)"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a figure immediately after abstract to showcase their empirical results.\n\nThe paper is written well with fluent narrative.\n\nThe paper targets the quadratic complexity of MHA, which is a significant problem especially in Recsys.\n\nOnline AB test results are provided."}, "weaknesses": {"value": "According to Figure 1, MHA outperforms the proposed LIME for user sequence lengths $N<4096$, and similarly outperforms LIME-XOR for $N<8192$. This implies that LIME provides limited acceleration benefits within these ranges, which are typical in real-world recommender systems. Since users with longer behavioral sequences (>4096 or 8192) constitute a small fraction, the practical advantage of LIME in terms of acceleration appears limited under realistic conditions.\n\nAn ablation study is essential to demonstrate the contribution of individual components in LIME to both acceleration and accuracy. The paper currently lacks such analysis. The authors are encouraged to include an ablation section, comparing LIME against both the standard Transformer and state-of-the-art accelerated Transformer variants, and to report results on model accuracy and computational complexity.\n\nIt is of great importance  to add comparisons with more recently developed accelerated attention mechanisms. These comparisons should evaluate both accuracy and efficiency across a comprehensive range of hyperparameters, including user sequence length, number of candidates, and QK dimensionality. The comparisons in the existing literature should also be unified to include these aspects.\n\nThe manuscript should include a discussion section addressing the limitations of the proposed approach and outlining potential directions for future research to address these shortcomings."}, "questions": {"value": "In comparison with the baseline MSA, the authors should specify the details of its implementation. It is unclear whether MSA is implemented using the modern FlashAttention mechanism (which could be essential for fairness). The authors are advised to clarify this point and, if FlashAttention is used, to indicate the specific version employed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NYC2MZ9Ya4", "forum": "Y2tKygPoUp", "replyto": "Y2tKygPoUp", "signatures": ["ICLR.cc/2026/Conference/Submission13136/Reviewer_VtZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13136/Reviewer_VtZ9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624898486, "cdate": 1761624898486, "tmdate": 1762923856154, "mdate": 1762923856154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LIME addresses the efficiency-expressiveness trade-off in large-scale recommendation systems via two innovations: low-rank link embeddings and XOR attention (LIMEXOR). Link embeddings decouple user-item interactions, enabling offline pre-computation of attention weights to make inference latency nearly independent of candidate set size. LIMEXOR reduces self-attention complexity from quadratic to linear by restricting interactions between user history and link embeddings. Experiments on public and industrial datasets show LIME matches the accuracy of expressive models (e.g., HSTU) while achieving 10× lower inference latency, with online A/B tests yielding up to 37.9% gains in user engagement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper simultaneously resolves two bottlenecks—candidate set scaling (via pre-computed attention weights) and long user history modeling (via linear XOR attention).\n- Near-SOTA Accuracy with Low Latency: LIME-XOR closes the performance gap with HSTU across tasks (video completion, watch time) while maintaining latency that is orders of magnitude lower.\n- Practical Deployment Value: Its decoupled inference pipeline (offline item pre-computation + online user processing) aligns with real-world recommendation systems, and online A/B tests confirm tangible user engage"}, "weaknesses": {"value": "- Link Count Sensitivity: Performance scales with link count, but small (e.g., 8–16) underperforms, and large may offset efficiency gains\n- Limited Shorter Sequence Advantage: On datasets with short user histories (e.g., Taobao Ads, max length 50), LIME-XOR performs only slightly better than LIME-MHA and lags HSTU. It seems that its linear attention gains are less impactful for small N.\n- Cold-Start Unaddressed: The framework relies on historical user interactions to personalize link embeddings, rarely consider cold-start users/items."}, "questions": {"value": "How does LIME perform when link embeddings are initialized with task-specific priors (e.g., item category clusters) instead of random normal distributions?\n\nCould a dynamic link count (adjusted per user’s history length) balance efficiency and performance, and what mechanisms would prevent overfitting to short sequences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ldGYiCLu9K", "forum": "Y2tKygPoUp", "replyto": "Y2tKygPoUp", "signatures": ["ICLR.cc/2026/Conference/Submission13136/Reviewer_619M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13136/Reviewer_619M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843635516, "cdate": 1761843635516, "tmdate": 1762923855877, "mdate": 1762923855877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LIME, a novel recommendation architecture aimed at improving efficiency while maintaining performance. The core contribution lies in decoupling the internal structure of attention and restructuring modules through two key components: LIME-MHA and LIME-XOR. LIME-MHA precomputes and caches candidate–link attention weights offline, effectively replacing the traditional cross-attention process with a matrix multiplication between precomputed item-side link matrices and personalized link embeddings. LIME-XOR further enhances efficiency by reducing self-attention complexity from quadratic to linear via an XOR masking scheme for user–link interactions. Extensive experiments on public and industrial datasets, including online A/B tests, confirm significant efficiency improvements and competitive or superior accuracy compared to transformer baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Novel architectural design that effectively resolves the efficiency–expressiveness trade-off in large-scale recommendation.\n- Clear technical innovations in decoupling attention (LIME-MHA) and introducing efficient XOR masking (LIME-XOR).\n- Detailed experimental verification across public and industrial datasets, with both offline and online A/B test results provided.\n- Demonstrated scaling properties and robustness in latency under very large candidate sets and long user histories."}, "weaknesses": {"value": "If I haven't misunderstood, the LIME-MHA part actually replaces the attention mechanism with a well designed matrix multiplication between user and candidate parts, manifested as the matrix multiplication of one matrix TL^t (independent of the user's item-side information) and the second one L^P (user-side personalized information). Therefore, this approach will definitely significantly enhance the speed of reasoning. However, based on the results of this article, its performance is also very strong. Does this prove that the personalized matrix multiplication is equally or even more effective than MHA in some recommendation aspects? This is a question worth exploring."}, "questions": {"value": "If I haven't misunderstood, the LIME-MHA part actually replaces the attention mechanism with a well designed matrix multiplication between user and candidate parts, manifested as the matrix multiplication of one matrix TL^t (independent of the user's item-side information) and the second one L^P (user-side personalized information). Therefore, this approach will definitely significantly enhance the speed of reasoning. However, based on the results of this article, its performance is also very strong. Does this prove that the personalized matrix multiplication is equally or even more effective than MHA in some recommendation aspects? This is a question worth exploring."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OIOiB24Taz", "forum": "Y2tKygPoUp", "replyto": "Y2tKygPoUp", "signatures": ["ICLR.cc/2026/Conference/Submission13136/Reviewer_ZWMY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13136/Reviewer_ZWMY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895385751, "cdate": 1761895385751, "tmdate": 1762923855520, "mdate": 1762923855520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}