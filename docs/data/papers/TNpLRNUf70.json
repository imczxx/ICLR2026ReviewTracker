{"id": "TNpLRNUf70", "number": 22321, "cdate": 1758329629451, "mdate": 1759896872491, "content": {"title": "Telescope: Improving Zero Shot Detection of LLM Generated Content With Token Repetition", "abstract": "Distinguishing Large Language Model (LLM) generated text from human writing is a critical and difficult challenge. While LLMs are trained to write like humans, we hypothesize that this training leaves an indelible mark. LLMs develop a particularly strong aversion to token repetition very early in training. This bias persists as a \"Vestigial Heuristic'' (a developmental artifact) that is activated in LLM-generated text, separating LLM from human writing. To probe this phenomenon, we introduce Telescope Perplexity, a metric that evaluates the token repetition of the model, $P(s_i | s_{1:i})$. Our empirical investigation reveals that the Telescope Perplexity signature emerges early in pre-training, and Telescope Perplexity empirically enables highly effective zero-shot LLM detection. We show state-of-the-art or competitive performance across diverse datasets (including modern evaluation sets we introduce), reference models, and perturbation schemes with greater efficiency than other methods.", "tldr": "We find LLMs have a unique aversion to repeating words (a \"Vestigial Heuristic\" from early training) and develop a highly effective method to detect LLM-generated text by measuring these repetition patterns.", "keywords": ["Fairness", "Accountability", "and Transparency", "Generative Models", "Text Analysis", "Natural Language Processing", "Benchmarks", "Learning Theory"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f069d3c546f96822ad7dcbd7f282d8ed6eec3046.pdf", "supplementary_material": "/attachment/d4dc981594f82c176514abc9ae69b33fe139e906.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the detection of content generated by large language models. The authors propose a new metric called Telescope Perplexity (TPPL), designed to capture a vestigial heuristic, an aversion to token repetition formed and retained during the early training stages of LLMs. The study verifies that this “vestigial heuristic” emerges early and persists across training in several open‑source models (e.g., Pythia, Amber‑7B). It further demonstrates that this signal can effectively distinguish human‑written from machine‑generated text across different architectures, corpora, and model outputs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes the Vestigial Heuristic hypothesis, tracing the detectability of LLM‑generated text back to early‑stage training biases, and provides empirical validation.\n- Includes ablation studies such as adversarial perturbation and transfer tests."}, "weaknesses": {"value": "- The Related Work section provides a biased and incomplete review of detector progress, omitting many significant works (e.g., Fast‑DetectGPT [1]). It also lacks systematic categorization and hierarchical structure. The authors should consult survey papers for a more comprehensive and organized discussion.\n- In Section 4.1.1, the authors claim that a major challenge is the lack of datasets generated by more advanced models in the literature—but which specific literature? To my knowledge, many cutting‑edge datasets (e.g., DetectRL [2]) already rely entirely on commercial models. Why not use such datasets instead? Judging from Table 2, both DetectLLM and PPL already achieve strong performance on most datasets, which indirectly suggests that the datasets used here may be relatively easy and requires more stressful testing.\n- The method shows a noticeable performance drop (AUROC ≈ 0.88) on structured text such as news. When thresholds are transferred (transfer test), the F‑measure declines, indicating a need for recalibration across domains. Therefore, a systematic cross‑domain transfer comparison is necessary to validate the robustness of the method. However, the paper does not appear to include any corresponding tables, figures, or comparisons with baselines.\n- The ICLR paper template used appears to be incorrectly formatted, and the writing is quite disorganized, making it difficult to extract concrete details.\n\n[1] Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature, ICLR 2024\n\n[2] DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios, NeurlPS 2024"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S9Hzv8RbdA", "forum": "TNpLRNUf70", "replyto": "TNpLRNUf70", "signatures": ["ICLR.cc/2026/Conference/Submission22321/Reviewer_qj1u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22321/Reviewer_qj1u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756530336, "cdate": 1761756530336, "tmdate": 1762942169141, "mdate": 1762942169141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Telescope Perplexity, a perplexity-style metric that measures the likelihood of repeating the last token in the context window. The authors suggest using this metric to differentiate LLM-generated from human-generated text in a zero-shot setting. The intuition is that LLMs learn early in pre-training to strongly avoid repeating tokens and that this persists throughout the training stages. The authors show empirical results across a range of datasets, including newly generated datasets using recent LLMs. The method is compared to other zero-shot approaches and claims superior performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles an important problem, differentiating LLM- from human-generated text, using a simple but very interesting approach, focusing on token repetitions.\n- The hypothesis and the proposed metric are clearly described. \n- The observation that LLMs learn and retain a strong aversion to token repetition early in training appears novel and interesting; however, I am concerned whether the large perplexity values throughout the process are strong evidence to support this claim.\n- Extensive empirical evaluation, covering multiple datasets, both legacy and newly generated text, and multiple reference models."}, "weaknesses": {"value": "- **Choice of benchmark and metric**: The reported AUROCs for the proposed method and the existing baselines (including standard perplexity and Binoculars) seem extremely high, close to 0.99 for most datasets. This would either imply that the detection of AI-generated text is a solved problem or that the setup is not realistic.\n- **Unclear conceptual link between aversion to repeat tokens and usefulness for detection**: The authors argue that LLMs develop a persistent aversion to token repetition, which leads to high Telescope Perplexity for both human- and LLM-generated text. It remains unclear to me why a reference LLM would assign higher Telescope Perplexity specifically to LLM-generated text? Could the effect instead be driven by confounding variable(s) (e.g., typos, grammatical errors, LLM’s overusage of certain words)?\n- **Insufficient details for the newly generated datasets**. Section 4.1.1 does not make it explicit that Claude is also used to generate datasets. I understand that the authors have used models (GPT4o and Deepseek) to generate new datasets based on each of the existing ones (GB Essay, GB News, and GB Creative), however, the dataset of GB News generated with GPT4o mini is possibly missing. A discussion about why it is better to ask the model to rewrite the human-generated text instead of giving it the same question or general topic as the human had when they were writing the text, and if it introduces possible biases in the analysis, would be appreciated.\n- **Some claims lack citations**. For example, the authors claim that rewriting is a difficult environment for AI detection models, and that if a reference LLM is perplexed and has never seen anything like it before, that means that other language models likely haven’t seen anything like it in their training data. The implication that all language models have largely overlapping training data holds across present and future models is non-obvious to me.\n- **Insufficient details for the “AI humanizer” method**. The “AI Humanizer” has not been released and has not been reproducibly described. Additionally, the paper does not discuss robustness to intentional fine-tuning of the target LLM to minimize Telescope Perplexity (e.g., via DPO/RLHF), which seems a direct threat model that needs to be addressed.\n- **Minor**:\n    - Appendix organization and writing quality could be improved.\n    - Some typos remain (e.g., “behviors” in Section 3.2 and “... calibrated 8.” in Section 9.5)\n    - Table 10 mentions bold entries but contains none.\n    - Table 14 formatting (double lines) is inconsistent with other tables."}, "questions": {"value": "(also see weaknesses)\n- Why is the mean AUROC across reference models (Table 2) chosen over the maximum? If the goal is detecting LLM- versus human-generated text, wouldn’t the best-performing reference model provide more actionable insight?\n- What is the motivation for using a logistic regression to determine the threshold for Transferability F1-Score?\n- What’s the impact of typos and grammatical errors in the input text?\n- Why does Binoculars Score have statistically significantly better performance than Telescope Perplexity in other languages (Table 12)? Could this difference be linked to linguistic properties or method-specific biases?\n- While I agree that the omission of exponentiation in Telescope Perplexity does not affect detection performance since the exponential function is monotonic, I am concerned about the large Telescope Perplexity values. Given that the **non-exponentiated** log-likelihood starts around 10 at the beginning of training (Fig. 2) and increases to roughly 12, could these large values be primarily due to noise?\n- It looks like the competitor methods show interesting, but opposite trends in performance with respect to text length: for the Detect LLM Text dataset, performance drops sharply around 300 tokens before improving with longer inputs, whereas for the HC3 Plus dataset, performance sharply increases around 200 tokens and then declines (Fig. 2). Could you share any insights into what drives these sharp inflection points, and why the trends differ between the two datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W6GVWuiyMr", "forum": "TNpLRNUf70", "replyto": "TNpLRNUf70", "signatures": ["ICLR.cc/2026/Conference/Submission22321/Reviewer_hSV2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22321/Reviewer_hSV2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842516364, "cdate": 1761842516364, "tmdate": 1762942168874, "mdate": 1762942168874, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the “Vestigial Heuristic” hypothesis, proposing that large language models (LLMs) develop an early-stage bias against token repetition while learning bigram patterns, and that this bias persists throughout training. The authors argue this residual heuristic explains why LLMs strongly avoid repetition even when unnecessary. To test this, they propose Telescope Perplexity, a novel metric that measures a model’s likelihood of repeating tokens, providing a focused probe into this aversion. Extensive experiments show Telescope Perplexity effectively distinguishes human-written from LLM-generated text, achieving strong zero-shot detection performance and robustness across modern models and datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper introduces a creative and conceptually fresh hypothesis — the “Vestigial Heuristics” perspective — suggesting that aversions to token repetition arise early during LLM training as a byproduct of bigram modeling and persist into later training stages. However, the empirical evidence is shallow and non-deterministic, leaving the hypothesis weakly supported.\n2) The authors provide reasonably careful empirical validation. The experiments test multiple modern LLMs (e.g., GPT-4o Mini, DeepSeek-V3), showing some robustness and competitiveness in zero-shot detection tasks."}, "weaknesses": {"value": "1) The paper introduces Telescope Perplexity as conceptually distinct from standard perplexity, yet the difference (conditioning on the same token rather than the next) seems mathematically minor and somewhat arbitrary. The rationale for why this formulation specifically isolates the repetition heuristic is not rigorously established. Additionally, the calculation of M(s_i|s_{1:i}) is not provided. Given that the scoring models are trained to predict next token, the calculation of the Telescope probability is not obvious.\n2) Although the paper reports “extensive testing,” the experiments focus narrowly on zero-shot text detection performance. For example, only three baselines are presented, data are generated by narrow decoding strategies and settings, scenarios under paraphrasing and adversarial attacks are not considered. \n3) The review about related work is incomplete given that many perplexity based zero-shot detectors are not compared. For example, DetectGPT, Fast-DetectGPT, LRR, Glimpse, etc.\n4) The exceptionally high scores of the simple perplexity baseline suggest that there maybe some distribution bias in the data settings. It would be valuable addition to have some datasets representing paraphrasing and adversarial attacks."}, "questions": {"value": "LN016: What is the indelible mark? If the concept was not well presented, the hypothesis will be vague.\n\nLN055: What is the point of the Vestigial Heuristic? How does it relate to your metric design?\n\nLN100: Hard to understand the formula M(s_i|s_{1:i}). How is the probability calculated using the model M?\n\nLN156: There are plenty of zero-shot detectors using perplexity. For example, DetectGPT, Fast-DetectGPT, LRR, Glimpse, etc. \n\nLN192: Except the raise during the initial stage, no significant patterns from the lines. What is the connect between these results and your heuristics? Do they justify the hypothesis?\n\nLN270: Data from LRMs like gpt-o1 or deepseek-r1 would be valuable addition.\n\nLN280: The settings of temperature with 0.7 and top-p with 0.9 are narrow, which make the text distribution skewed and easy for detection. A wide spectrum of the decoding strategies and settings are expected to simulate real scenarios.\n\nLN338: Which size of SmolLM is used here? I would expect to see the typical setting in the main results, such as falcon7b used by Binoculars or gpt-neo-2.7B by Fast-DetectGPT.\n\nLN344: The baselines are insufficient to support the claim. Additionally, the results seem weird that the perplexity performs better than Binoculars. Is there any deeper reason?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9t1oxALO9B", "forum": "TNpLRNUf70", "replyto": "TNpLRNUf70", "signatures": ["ICLR.cc/2026/Conference/Submission22321/Reviewer_LrEe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22321/Reviewer_LrEe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903307482, "cdate": 1761903307482, "tmdate": 1762942168699, "mdate": 1762942168699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors claim to have developed a new LLM detection method, Telescope, based on the higher token repetition aversion in LLM-generated texts compared to human-generated texts. The authors test their methods on basic LLM-generated texts against several existing LLM-detection methods, such as Binoculars, DetectLLM, and Perplexity, using the AURoC metric to argue their method's SotA performance. The authors also claim that their method exhibits excellent properties in real-world detection tasks, such as locality and generalization."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors introduce a new and useful signal for LLM text detection - token repetition aversion, which, to the best of my knowledge, has not been previously documented.The authors introduce a new useful signal for LLM text detection - token repetition aversion, that to the best of my knowledge, has not been documented before."}, "weaknesses": {"value": "- The authors do not seem to be aware, or at least mention, the existence of a generation control parameter called \"repeat penalty\", applied by default to models upon generation. This parameter penalizes token repetition to prevent model degeneration and can be easily adjusted to avoid detection by the authors' method. Without evaluating their method's robustness against this metric, the detection method's relevance or performance cannot be claimed, especially as sampling methods that automatically optimize repetition penalties are emerging [1]. Moreover, the \"Vestigal Heuristic\" hypothesis cannot be claimed without accounting for this parameter.\n\n- The authors only evaluated their method's resilience against a single evasion attack, which is insufficient. Evasion attacks are the most problematic current issue in the context of LLM detection, as exemplified by the existing literature on LLM detectors benchmarking, notably the RAID paper [2]. As such, the performance of their method in a real-world setting cannot be evaluated.\n\n- The authors do not seem to be aware of the foundational results in LLM detection, demonstrating that AUROC is not an informative metric with LLM detection [3]. In fact, due to the lack of access to true labels for LLM detector operators, the relevant metric is TPR at a fixed low FPR, for which most recent detectors have been optimized, potentially lowering their AUROC to achieve it. As such, the metric they used to argue the superiority of their method is irrelevant.\n\n- The authors argue that Fine Web is human-written data and is a distinct training corpus. Fine Web is a subsample of Common Crawl, which makes a substantial part of other training model datasets, such as the GPT4 family (80%> as per the GPT4 technical report)\n\n- I strongly suggest that the authors review the paper's wording and the background literature. The original citation for LLM pretraining being useful for downstream tasks (what you term as \"understanding\" on L34) was originally claimed by the ELMo paper [4], which is the correct citation for such claims. Similarly, the correct canonical citation for LLM texts being undetectable to humans is [5]. Overall, please review the wording of the introduction to ensure that the claims are factual and consistent with prior scientific literature, and that the coverage of existing literature is comprehensive and consistent with the claims. \n\n- Similarly, the code supporting the paper should be shared through an anonymized repository, such as https://anonymous.4open.science/ or an anonymous GitHub repository with PPI removed.\n\n[1] Huang, D., Nguyen, T., Liausvia, F., & Wang, Z. (2025). RAP: A Metric for Balancing Repetition and Performance in Open-Source Large Language Models. North American Chapter of the Association for Computational Linguistics.\n\n[2] Dugan, L., Hwang, A., Trhlik, F., Ludan, J.M., Zhu, A., Xu, H., Ippolito, D., & Callison-Burch, C. (2024). RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors. ArXiv, abs/2405.07940.\n\n[3] Carlini, N., Chien, S., Nasr, M., Song, S., Terzis, A., & Tramèr, F. (2021). Membership Inference Attacks From First Principles. 2022 IEEE Symposium on Security and Privacy (SP), 1897-1914.\n\n[4] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (2018). Deep Contextualized Word Representations. NACL 2018\n\n[5] Ippolito, D., Duckworth, D., Callison-Burch, C., & Eck, D. (2019). Automatic Detection of Generated Text is Easiest when Humans are Fooled. Annual Meeting of the Association for Computational Linguistics."}, "questions": {"value": "- Drastically different sampling temperatures are used in the generation of ESL dataset and Ghostbuster replication with GPT-4o mini and Deepseek. Why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2FHrFV8F60", "forum": "TNpLRNUf70", "replyto": "TNpLRNUf70", "signatures": ["ICLR.cc/2026/Conference/Submission22321/Reviewer_EUSN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22321/Reviewer_EUSN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985508444, "cdate": 1761985508444, "tmdate": 1762942168356, "mdate": 1762942168356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}