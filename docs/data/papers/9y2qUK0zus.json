{"id": "9y2qUK0zus", "number": 16629, "cdate": 1758266958556, "mdate": 1759897228664, "content": {"title": "A Unified Objective for On-Policy Reinforcement Learning in Stationary and Non-Stationary Environments", "abstract": "A fundamental dichotomy between the discounted and average return has long existed in the field of deep reinforcement learning (DRL). Algorithms based on the average return assume the existence of stationary state distribution and often struggle in non-stationary or episodic settings. In contrast, algorithms optimizing the discounted return are well-suited for non-stationary tasks but may learn suboptimal policies in long-term stationary settings due to the inherent bias introduced by the discount factor. This forces practitioners to select an objective based on the specific environment, thereby limiting the development of general and robust DRL algorithms. We introduce the \\textbf{$k$-sliding-window return}, a novel objective that bridges these two criteria. We instantiate this concept with a practical on-policy algorithm, $k$-sliding-window PPO ($k$SW-PPO). Besides, we provide theoretical analysis showing that the loss of our objective converges to that of the average return while maintaining a bounded bias relative to the discounted return. We validate our claims through experiments on a suite of MuJoCo continuous control tasks. The results demonstrate that $k$SW-PPO achieves performance competitive with average-return PPO in stationary environments, while matching the performance of its discounted-return counterpart in non-stationary settings. Our results establish the $k$-sliding-window return as a unified objective that eliminates the need for an a priori choice between discounting and averaging, which we hope to inspire the development of more robust and general-purpose DRL algorithms.", "tldr": "", "keywords": ["Reinforcement Learning", "Discounted Return", "Average Return", "On-policy", "Stationary State Distribution"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/56104c5d04309d3804d0f48c3f18e7ea250c7bc1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a new objective, k-sliding window cumulative rewards, for learning the optimal policies in both “stationary” and “non-stationary” environments. The paper starts by pointing out the main difference between Swimmer and Reacher environments and then formally defines the k-sliding window objective for PPO algorithm. This objective is further compared with the discounted objective and the average reward objective."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well-written and easy to understand. The motivation example clearly illustrates the “stationary” and “nonstationary” environments and how the performance of policy differs if different objectives are used to optimize policies. The paper presents both theoretical and empirical analysis of the proposed objective."}, "weaknesses": {"value": "I have some major concerns on the technical soundness and evaluation rigorousness of the paper in its current version. \n### Lack of technical soundness\nThere might be some errors in the assumptions and theorems. Some theoretical analysis may not be sound: \n1. In assumption 1, Ergodic means $P^\\pi$ is **irreducible** and aperiodic.\n2. **Theorem 1 may be wrong** (or there might be some constraints over k that hasn’t been specified correctly). One can verify this by setting $k=1$: the right hand side is now zero, which forces $V_k^{\\pi}(s) – V_{k}^{\\pi_{old}} = L_k(\\pi, \\pi_{old}, s)$. Based on the definition of $L_k$ in this theorem, $L_k$ differs from the first term on the right hand size of the equation in Lemma 1. \n3. It’s unclear why the analysis focuses on the gaps of different losses? **The analysis in both Section 4.2 and 4.3 seems irrelevant if the new objective is introduced to optimize policies in both “stationary” and “nonstationary” environments**. Shouldn’t the analysis be focused on how the objective can be more effective than discounted return if the environments are “non-stationary”? Further, in Theorem 2, “the upper bound is minimized at $k=\\frac{1}{1-\\gamma}$, which may not imply anything about the policies learned by the new objective, as opposed to the discounted return objective. Likewise in Theorem 3, if “the magnitude of the policy update … diminishes” (which means $\\pi_{old} = \\pi$?) then both $\\bar{L}$ and $L$ are effectively zero and the bound can be trivial. Also, why would it matter if “$L_k$ approximates $\\bar{L}_k$? if the goal here is to approximate $\\bar{L}_k$ then one can simply use the average return as the objective. \n\n\n### Evaluation rigorousness\n1. the main conclusion about the performance dichotomy between average and discounted return comes from **a very limited number of tests** on Swimmer and Reacher **only using PPO algorithm**. First, the reported performance on Swimmer-v5 in Figure 1(b) has a very large variance. It’s hard to draw the conclusion that “average-return PPO excels in the stationary Swimmer environment”. Even in Figure 2 (the main results), the variance of k-sliding-window and average remains very high. Second, since the paper only reports the results from PPO, it’s unclear if the performance gap reported in Figure 1(b) is really caused by the different objectives. It can be simply that hyperparameters in PPO are not robust to the change of objectives. The paper should reports more analysis with more different types of reinforcement learning methods. \n\n2. in the experiment section, the proposed method benefits from some optimized tricks on GAE, L1 value loss, and Beta distribution, however **none of which have been ablated**. It would be a bit hard to see if the proposed objective can really yield a performant PPO and how these optimized tricks contribute to the final performance. Also **the baseline method can be quite weak**. The paper reported that “the vanilla PPO” has been used as the baseline for the discounted-return case. It seems this PPO baseline does not have any tricks used by the proposed method and can thus be in disadvantage. It’s also odd to not include a baseline simply because “Tang et al. (2021) lacks a public implementation”. The authors might need to reproduce Tang et al. (2021) for a comparison. \n\n3. the theoretical analysis implies that when $k$ is set to $\\frac{1}{1-\\gamma}$, the concerned bounds “is minimized”. However, in the experiments, “k was set to 50 for Swimmer, Walker2d, and Hopper; 30 for HalfCheetah; and 10 for Reacher and Pusher.” **It seems the value of $k$ was somehow tuned for each environment, not consistent with the theoretical implications**. The ablation study on k also does not reflect this."}, "questions": {"value": "1. Line 151: “da” is not needed\n2. Line 210: $d^\\pi_{k, s}(s’)$ may not be a probability distribution as it’s not normalized. (it used to be called state visitation measures)\n3. Line 211 to 214: I guess this lemma can only be applied to $k\\geq 2$? Otherwise the summation index would be problematic. \n4. Line 353: “its unbiased nature …” why the objective is unbiased and in what sense? \n5. Line 355: “structural similarities with the discounted return…” what are the structural similarities? \n6. There are some terminology issues: I think it’s a bit odd to call Swimmer environment stationary and Reacher environment non-stationary. Clearly, both environments are stationary since the underlying transition dynamics and reward functions don’t change over time (namely the simulation itself is not changing over time). I guess the authors meant that the states in these two environments are of different types: the states in Swimmer are recurrent while the states in Reacher are transient? \n7. “discount factor $\\gamma$ inherently biases the objective towards short-term performance” but would k-sliding window then biases the objective towards k-steps performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mZw6eKe4bd", "forum": "9y2qUK0zus", "replyto": "9y2qUK0zus", "signatures": ["ICLR.cc/2026/Conference/Submission16629/Reviewer_Sq2C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16629/Reviewer_Sq2C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866728982, "cdate": 1761866728982, "tmdate": 1762926696857, "mdate": 1762926696857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper attempts to tackle a fundamental issue of RL: the dichotomy between discounted and average-return objectives. The proposed $k$-sliding-window return is an intuitive concept that defines the sum of the next $k$ rewards as the value function. The paper provides bounds on the loss difference between using $k$-step return and previous returns when updating the same policy. Additionally, the proposed algorithm, kSW-PPO, which incorporates the $k$-step return into PPO, exhibits comparable performance on several MuJoCo tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper aims to address a very fundamental problem, providing an intuitive solution. \n\nThe algorithm and result are clearly explained.\n\nAll equations are clearly presented with well-defined symbols."}, "weaknesses": {"value": "The fundamental definition of the task is not clear. What are stationary and non-stationary environments?\n\nThe newly introduced hyperparameter is not robust across tasks, requiring the same tuning efforts to choose between average return and discounted return.\n\nThe theoretical result does not provide an analysis of the algorithm's convergent behavior. (Check the Question section.)"}, "questions": {"value": "1. What are the definitions of stationary and non-stationary environments? \n\n2. Does kSW-PPO converge? What are the bounds of the final returns compared to the average return or the discounted return? What convergent behaviour can we conclude from the loss bounds?\n\n3. The paper reports the number of episodes. How many training steps does it correspond to?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gbzjgvT2t9", "forum": "9y2qUK0zus", "replyto": "9y2qUK0zus", "signatures": ["ICLR.cc/2026/Conference/Submission16629/Reviewer_ASYF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16629/Reviewer_ASYF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964165149, "cdate": 1761964165149, "tmdate": 1762926696417, "mdate": 1762926696417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper notes that discounted objectives are better suited for non-stationary settings, while averag- reward objectives are better for stationary ones. They unify the two objectives via the \"k-sliding window return\", which aims to maximize the sum of rewards over a fixed horizon into the future. They provide analysis around how this objective compares with discounted and average-reward objectives. They then extend PPO to use this objective (kSW-PPO), and evaluate their extension against discounted and average-reward versions of PPO empirically."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The contextualization in stationary and non-stationary settings is novel and one that isn't acknowledged often when comparing discounted vs. average-reward criteria.\n\n* The analyses, as far as I checked, appear correct. The result in Section 4.3 where—in a stationary setting—the $k$-sliding-window leads to the same optimal policy as the average-reward criterion under the objective $\\mathbb{E}_{s\\sim d^\\pi}[V_k^\\pi(s)]$, was particularly interesting, and seems to mirror a comparable result for discounted objectives (Sutton & Barto, 2018; Naik et al., 2019).\n\n### References\n\n* Sutton, R. S., Barto, A. G. (2018). Reinforcement learning: an introduction. \n* Naik, A., Shariff, R., Yasui, N., Yao, H., Sutton, R. S. (2019). Discounted reinforcement learning is not an optimization problem."}, "weaknesses": {"value": "* The $k$-sliding-window return is not novel. In the RL space, De Asis et al. (2020) advocated for using an identical objective in non-finite-horizon problems, but motivated by how the objective leads to stable temporal difference updates. The idea goes farther back in the control theory literature—see receding horizon control.\n\n* The analysis between the $k$-sliding-window and the discounted objective (Section 4.2) mirrors prior bias-variance analyses in the literature, as many such analyses would assume a finite-horizon to simplify things. The bias-equivalence between discounting and the sliding window emphasizes the commonly-acknowledged effective horizon interpretation of discounting, in terms of the expected sequence length of a stochastic process. This relationship highlights that $\\gamma$ plays a very similar role as $k$, which kind of clashes with the claim that the $k$-sliding-window unifies discounted and average-reward objectives. That is, discounting already provides a way to interpolate toward the average-reward extreme, where crossing the Blackwell-optimal discount factor will lead to the same optimal policy as the average-reward criterion. Of note, $k$ interpolates horizontally while $\\gamma$ interpolates vertically. This hard, horizontal cut-off prevents a guaranteed \"Blackwell-optimal\" $k$ where every $k$ beyond that would lead to the average-reward optimal policy. Instead, depending on the MDP, it can converge to a periodic loop where the optimal policy depends on $k$ modulo this period.\n\n* Only 5 seeds were used in the empirical evaluation, which has been repeatedly shown to not be enough to make a proper statistical comparison for the claims being made (e.g., Henderson et al., 2017; Colas et al., 2018; Patterson et al, 2023; Patterson et al, 2024). Given the high variability of the results, it's unclear whether any conclusions drawn are within statistical significance. Of note, the shaded regions represent the standard deviation which is not a measure of confidence.\n\n### References\n\n* De Asis, K., Chan, A., Pitis, S., Sutton, R. S., Graves, D. (2020). Fixed-horizon temporal difference methods for stable reinforcement learning.\n* Henderson, P., Islam, R., Bachman, P. Pineau, J., Precup, D., Meger, D. (2018). Deep reinforcement learning that matters.\n* Colas, C., Sigaud, O., Oudeyer, P. (2018). How many random seeds? Statistical power analysis in deep reinforcement learning experiments.\n* Patterson, A., Neumann, S., White, M., White, A. (2023). Empirical Design in Reinforcement Learning.\n* Patterson, A., Neumann, S., Kumaraswamy, R., White, M., White, A. (2024). The Cross-environment Hyperparameter Setting Benchmark for Reinforcement Learning"}, "questions": {"value": "* The paper repeatedly mentions the \"average return\" objective, but I interpreted this to mean average reward given the context (e.g., the value definitions in 3.2 use differential returns, etc.). Can the authors clarify if this interpretation is correct? If so, I think it should definitely be revised as \"average return\" isn't well-situated in the literature.\n\n* When mentioning how discounted objectives are better suited for non-stationary settings, what is this grounded in? On the theoretical side, the analyses of discounted algorithms often still rely on stationary assumptions for convergence. In a non-stationary problem, one could consider a hypothetical setup where an agent has access to the momentary optimal policies for both discounted and average-reward objectives—the average-reward optimal policy seems like it would lead to more total reward here, that the benefit of discounted objectives in such a setting may lie somewhere in between (perhaps on the algorithmic or bias-variance front).\n\n* Regarding Section 4.3, can the authors comment on whether the same result can be achieved when comparing the same performance metric but with discounted values in place of the k-sliding window values? e.g., Section 10.4 of Sutton & Barto (2018) seems to arrive at this conclusion for the discounted version of the chosen performance metric.\n\n* Can the authors comment on the statistical significance of the results, and whether 5 seeds are enough for the conclusions drawn?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Iajj1gMBDE", "forum": "9y2qUK0zus", "replyto": "9y2qUK0zus", "signatures": ["ICLR.cc/2026/Conference/Submission16629/Reviewer_JvrK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16629/Reviewer_JvrK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972020590, "cdate": 1761972020590, "tmdate": 1762926696012, "mdate": 1762926696012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel objective for on-policy reinforcement learning, k-sliding-window return, which provides a trade-off between average- and discounted-reward returns. The authors further present a theoretical framework to validate and analyse its relationship with both average and discounted returns. Experimental results demonstrate that the proposed approach outperforms the selected baseline methods in both stationary and non-stationary environments."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on identifying a trade-off between average and discounted returns. To this end, a simple k-window approach is proposed and examined theoretically. Overall, the paper is mostly written and well organised. To the best of my knowledge, the proposed approach is novel."}, "weaknesses": {"value": "I completely understand that the primary aim of this paper is to provide a theoretical framework. However, it currently lacks evidence demonstrating how the proposed approach could have a broader impact on the field.\n\nFirst, the underlying motivation is only vaguely justified, relying on a single illustrative example (see Figure 1). Discounted approach is widely used in the literature. As discounted RL generally performs well in stationary environments, a deeper analysis would be valuable to clarify the necessity of a sliding-window approach. For instance, by discussing issues like bias.\n\nSecond, one would expect the proposed method to improve performance in non-stationary environments. However, the choice of baselines is rather limited, and the approach appears to perform better only in stationary settings, which calls the claimed trade-off into question (see Figure 2).\n\nThird, the paper overlooks several relevant related works. Including comparisons with stronger and more representative baselines would considerably strengthen the paper’s contribution and positioning. For instance, discounting mismatch (https://www.ifaamas.org/Proceedings/aamas2022/pdfs/p1491.pdf), average reward adjusted discounted RL (https://arxiv.org/abs/2004.00857), and Trust Region Methods (https://arxiv.org/abs/2004.00857). \n\nFourth, my major concern is that the authors claim in the introduction that the environment cannot be known in advance. However, the optimal choice of the hyperparameter k appears to be environment-dependent (see Section 5.3). Specifically, k = 50 performs better in stationary environments, whereas k = 10 yields superior results in non-stationary settings. This observation seems to contradict the stated contributions of the proposed method."}, "questions": {"value": "In addition to the points outlined above, I would like to raise a few further questions for clarification:\n\n1) In Section 4.1, is the proposed method implemented using a sliding or an overlapping window? A sliding window may risk capturing only a limited time horizon. How do the authors ensure that the approach retains a sufficiently long-horizon perspective? Could an adaptive-window mechanism be employed to mitigate this limitation?\n\n2) While PPO is one of the most widely adopted algorithms in the literature, could the proposed method be extended to other approaches, such as actor–critic frameworks? Is the method inherently specific to PPO, or could it be incorporated into alternative algorithms in a more plug-and-play fashion?\n\nOverall, I would like to acknowledge the authors’ considerable effort in developing and presenting a solid theoretical framework. However, Sections 4.2 and 4.3 could be moved to the appendix. Instead, the authors could provide more substantial empirical evidence to demonstrate the broader and more universal benefits of the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cRkpkT2k8O", "forum": "9y2qUK0zus", "replyto": "9y2qUK0zus", "signatures": ["ICLR.cc/2026/Conference/Submission16629/Reviewer_A5Ez"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16629/Reviewer_A5Ez"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111918026, "cdate": 1762111918026, "tmdate": 1762926695489, "mdate": 1762926695489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}