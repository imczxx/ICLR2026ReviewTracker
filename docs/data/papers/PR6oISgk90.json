{"id": "PR6oISgk90", "number": 17548, "cdate": 1758277411511, "mdate": 1759897167948, "content": {"title": "Reinforced Preference Optimization for Recommendation", "abstract": "Recent breakthroughs in large language models (LLMs) have fundamentally shifted recommender systems from discriminative to generative paradigms, where user behavior modeling is achieved by generating target items conditioned on historical interactions.\nYet current generative recommenders still suffer from two core limitations: the lack of high-quality negative modeling and the reliance on implicit rewards. \nReinforcement learning with verifiable rewards (RLVR) offers a natural solution by enabling on-policy sampling of harder negatives and grounding optimization in explicit reward signals. \nHowever, applying RLVR to generative recommenders remains non-trivial. \nIts unique generation space often leads to invalid or repetitive items that undermine sampling efficiency, and ranking supervision is sparse since most items receive identical zero rewards.\nTo address these challenges, we propose \\textbf{Reinforced Preference Optimization for Recommendation} (\\textbf{ReRe}), a reinforcement-based paradigm tailored to LLM-based recommenders, an important direction in generative recommendation. \nReRe incorporates constrained beam search to improve sampling efficiency and diversify hard negatives, while augmenting rule-based accuracy rewards with auxiliary ranking rewards for finer-grained supervision.\nExtensive experiments on three real-world datasets demonstrate that ReRe consistently outperforms both traditional and LLM-based recommenders in ranking performance.\nFurther analysis shows that ReRe not only enhances performance across both base and SFT-initialized models but also generalizes robustly across different backbone families and scales.\nBeyond empirical gains, we systematically investigate the design space of RLVR in recommendation across generation, sampling strategy, reward modeling, and optimization algorithm, offering insights for future research.\nOur codes are available at \\url{https://anonymous.4open.science/r/ReRe-E1B0}.", "tldr": "Our paper proposes ReRe, a novel reinforcement fine-tuning paradigm tailored for LLM-based recommenders by adapting the sampling strategy and the reward.", "keywords": ["Recommender System", "Large Language Model", "Reinfocement Learning with Verifiable Reward"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4d3026934d22352334abf13e17a29c4a2ff35791.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Reinforced Preference Optimization (RPO), a reinforcement learning–based extension of Direct Preference Optimization (DPO) for recommendation tasks. Unlike standard DPO, which aligns models to short-term revealed preferences, RPO introduces a utility critic that estimates long-term user satisfaction, combining it with reward modeling and policy updates. The framework alternates between preference optimization (based on pairwise comparisons) and utility reinforcement (based on delayed feedback)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The overall idea of integrating a reinforcement objective into DPO is reasonable and grounded in established RL principles. However, the derivation of the “utility critic” and the way it interacts with the preference policy is somewhat heuristic. The connection to standard RL formulations (e.g., Q-learning or advantage functions) is not rigorously developed."}, "weaknesses": {"value": "The paper claims that reinforcement learning with verifiable rewards (RLVR) “naturally” addresses implicit reward issues in LLM-based recommenders, but this claim is not rigorously justified.\n\nI am wondering what is verifiable reward? what will be the difference between verifiable reward and traditional reward in DL/RL?\n\nThe core of ReRe—beam search for sampling and ranking-based auxiliary rewards—is an extension of existing DPO and GRPO concepts. There is no much novel optimization algorithm or unique reward modeling principle. My main concern is that the proposed method primarily combines existing ideas (beam search, ranking loss, constrained decoding) with limited algorithmic innovation."}, "questions": {"value": "Please refer to weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N9i6WEvdYa", "forum": "PR6oISgk90", "replyto": "PR6oISgk90", "signatures": ["ICLR.cc/2026/Conference/Submission17548/Reviewer_yVsc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17548/Reviewer_yVsc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760575962885, "cdate": 1760575962885, "tmdate": 1762927420144, "mdate": 1762927420144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ReRe to fix two flaws of LLM-based generative recommenders: poor high-quality negative modeling and reliance on implicit rewards. ReRe uses constrained beam search to improve sampling efficiency/diversify hard negatives and combines rule-based with ranking rewards for finer supervision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. ReRe effectively addresses the two key flaws of LLM-based generative recommenders (insufficient high-quality negative modeling and reliance on implicit rewards) by integrating constrained beam search (for improving sampling efficiency and diversifying hard negatives) and a combined reward (rule-based accuracy + auxiliary ranking rewards), directly tackling the unique generation space and sparse supervision challenges of RLVR adaptation .\n\n2. The study uses three real-world datasets (Amazon Toys, Amazon Industrial, Yelp) and compares ReRe with diverse baselines.\n\n3. ReRe maintains robust performance across different backbone models (Qwen2.5-1.5B, Gemma-2B, Qwen2.5-7B) and initialization methods (Base, SFT)."}, "weaknesses": {"value": "1. There is a lack of training-time efficiency comparisons with other generative recommendation methods (e.g., those that do not use reinforcement learning) as well as with traditional methods.\n\n2. The dataset information in Table 5 is not clearly described, and the experiments rely exclusively on relatively small-scale datasets."}, "questions": {"value": "1. In Table 5, do the numbers for “Tran” refer to the number of interactions or the number of users?\n\n2. Can this method be combined with generative recommendation approaches (e.g., TIGER)? A semantic ID–based generative paradigm appears more practically viable, whereas relying on text prompts may constrain inference efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8BkDUOd5pu", "forum": "PR6oISgk90", "replyto": "PR6oISgk90", "signatures": ["ICLR.cc/2026/Conference/Submission17548/Reviewer_Sk62"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17548/Reviewer_Sk62"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576151162, "cdate": 1761576151162, "tmdate": 1762927419743, "mdate": 1762927419743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors have addressed reinforcement learning in generative recommenders. The authors found that existing works often rely on implicit rewards and ignore high-quality negative modeling. To address these two problems, this paper proposes to integrate the RLVR into the post-training of LLM-based recommenders. For adaptation, this paper proposed a constrained beam search and an augmenting rule-based accuracy reward. The extensive experiments have validated the effectiveness of the proposed method."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ S1. This paper is well-organized and -written, making it easy to follow.\n+ S2. This paper is well-motivated. \n+ S2. Extensive experiments have been conducted.\n+ S3. The code is released, making it easy to reproduce."}, "weaknesses": {"value": "- W1. Some up-to-date papers were ignored by this paper. For example, LatentR3[1] also adopted the GRPO algorithm. What's the difference between ReRe and previous works?\n- W2. It is better to further investigate the generality of the proposed ReRe. This paper has investigated the LLM-based recommender with the input of item titles, but how about the one with item ID, such as E4SRec or LLaRA?\n\n\n\n[1]. Zhang, Yang, et al. \"Reinforced Latent Reasoning for LLM-based Recommendation.\" *arXiv preprint [arXiv:2505.19092](https://arxiv.org/abs/2505.19092)* (2025).\n\n\n\n[2]. Li, Xinhang, et al. \"E4srec: An elegant effective efficient extensible solution of large language models for sequential recommendation.\" *arXiv preprint [arXiv:2312.02443](https://arxiv.org/abs/2312.02443)* (2023).\n\n\n\n[3]. Liao, Jiayi, et al. \"Llara: Large language-recommendation assistant.\" *Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval*. 2024."}, "questions": {"value": "All my questions have been included in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x2LSqyQQHs", "forum": "PR6oISgk90", "replyto": "PR6oISgk90", "signatures": ["ICLR.cc/2026/Conference/Submission17548/Reviewer_u1B8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17548/Reviewer_u1B8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797828312, "cdate": 1761797828312, "tmdate": 1762927419201, "mdate": 1762927419201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses two major limitations of Reinforcement Learning with Verifiable Rewards (RLVR) in generative recommendation models. First, the unique generative space often produces invalid or duplicate items, reducing sampling efficiency. Second, since most items receive identical zero rewards, the ranking supervision signals become sparse. To overcome these issues, the authors propose Reinforced Preference Optimization for Recommendation (ReRe). Specifically, ReRe introduces a constrained beam search mechanism to improve sampling efficiency and increase the diversity of hard negative samples. In addition, it supplements rule-based accuracy rewards with auxiliary ranking rewards, enabling finer-grained supervision."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.ReRe effectively addresses the challenge of hard negative sampling by introducing constrained beam search.\n\n2.The use of ranking rewards alleviates the limitations of binary rule-based supervision.\n\n3.The experimental results demonstrate solid performance and validate the method’s effectiveness.\n\n4.The overall structure and logic of the paper are clear and well-organized."}, "weaknesses": {"value": "1.For LLM-based recommender systems, prompt design is a crucial component, yet the paper does not discuss it.\n\n2.Figure 2 fails to clearly illustrate ReRe’s contribution to negative sample sampling; further clarification or revision is needed.\n\n3.Although constrained beam search mitigates the generation of invalid or duplicate items, ReRe may still be biased toward popular items due to the long-tail distribution, potentially overlooking less popular ones."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IxUL2IbQL5", "forum": "PR6oISgk90", "replyto": "PR6oISgk90", "signatures": ["ICLR.cc/2026/Conference/Submission17548/Reviewer_XSob"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17548/Reviewer_XSob"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825915641, "cdate": 1761825915641, "tmdate": 1762927418686, "mdate": 1762927418686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}