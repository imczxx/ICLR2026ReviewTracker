{"id": "3iw5t2W41F", "number": 17069, "cdate": 1758271813247, "mdate": 1759897200324, "content": {"title": "Remaining-data-free Machine Unlearning by Suppressing Sample Contribution", "abstract": "Machine unlearning (MU) aims to remove the influence of specific training samples from a well-trained model, a task of growing importance due to the ``right to be forgotten.” The unlearned model should approach the retrained model, where forgetting data do not contribute to the training process. Therefore, unlearning should withdraw their contribution from the pre-trained model. However, quantifying and disentangling sample's contribution to overall learning process is highly challenging, leading most existing MU approaches to adopt other heuristic strategies such as random labeling or knowledge distillation. These operations inevitably degrade model utility, requiring additional maintenance with remaining data. To advance MU towards better utility and efficiency for practical deployment, we seek to approximate sample contribution with only the pre-trained model. We theoretically and empirically reveal that sample's contribution during training manifests in the learned model's increased sensitivity to it. In light of this, we propose MU-Mis (Machine Unlearning by Minimizing input sensitivity), which directly suppresses the contribution of  forgetting data. This  straightforward suppression enables MU-Mis to successfully unlearn without degrading model utility on the remaining data, thereby eliminating the need for access to the remaining data. To the best of our knowledge, this is the first time that a remaining-data-free method can outperform state-of-the-art (SOTA) unlearning methods that utilize the remaining data.", "tldr": "We develop an effective and efficient machine unlearning method that could unlearn without utilizing the remaining data to compensate model utility.", "keywords": ["Machine Unlearning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/26daf7a97d8b8bb7cb3dbcf89dca2c63d832701d.pdf", "supplementary_material": "/attachment/1aaf1f475a808976b4229fd7d3331397c874ee46.zip"}, "replies": [{"content": {"summary": {"value": "Overall, this paper focuses on machine unlearning without access to retain set. This paper essentially relies on an observation that, as each sample is used during training, the learned model increases in sensitivity towards it. Specifically, during training of a sample, the difference of gradients between the relevant and irrelevant classes increases. This observation is used to design a loss to suppress the sample contributions, thus achieving unlearning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important area of machine unlearning which does not use any data from the retain set.\n\nThe paper seems sound in the theoretical areas.\n\nExperimental results in Table 1 and Table 2 look convincing."}, "weaknesses": {"value": "I think the intuition of the method is not well expressed. Although the math and equations are there, the intuition is not clear, in the sense that, it should be able to be expressed in a few lines at most. However, the paper does not express these simple concepts well.\n\nIn terms of the proposed method, I believe the idea is quite simple. Basically, the sensitivity of each forget sample is computed, with respect to the relevant class and also irrelevant classes. Then, the loss is designed to suppress this difference, thus making the model learn to forget, similar to how a unlearned model would be. However, these concepts have not been explained clearly in the paper.\n\n\nImportantly, the full related work section has been placed in the Supplementary. I do not feel good about this, since I do not think it is fair for most other works that use space to discuss it in the main paper. In some sense, since reviewers are not obligated to read the supplementary, it can be said to some extent that this paper does not have a proper related work section. All in all, since it is a main and important component of the paper, there should be substantial discussion on related work in the main paper. This is a huge issue for me.\n\nI also feel uncomfortable that the implementation details section is fully in the supplementary.\n\nSection 3.2 which explains the input sensitivity is too long in my opinion. Also, the derivations and math look very similar to influence functions (some example papers shown below). In this long section 3.2, I am unable to grasp the novelty and contributions of these theoretical analyses, how they are different from previous insights, and why they are important.\n\nEfficient Machine Unlearning via Influence Approximation. arXiv 2025\n\nReliable Active Learning via Influence Functions. TMLR 2023\n\nInfluence Selection for Active Learning. ICCV 2021\n\n\nThere is a citation error for two references:\n\nTowards Unbounded Machine Unlearning is a NeurIPS 2023 paper, not 2024.\n\nIs retain set all you need in machine unlearning? Restoring performance of unlearned models with out-of-distribution images is an ECCV 2024 paper, not 2025\n\nThe authors can consider reporting results and/or comparing with more recent works such as:\n\nLearning to Unlearn for Robust Machine Unlearning. ECCV 2024\n\nAdversarial Machine Unlearning. ICLR 2025\n\nDecoupled Distillation to Erase: A General Unlearning Method for Any Class-centric Tasks. CVPR 2025\n\nLoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty. CVPR 2025\n\nMUNBa: Machine Unlearning via Nash Bargaining. ICCV 2025\n\n\n\n\nMinor errors that do not affect the score: Line 105: knowledge -> knowledge"}, "questions": {"value": "Please refer to Weaknesses for most questions.\n\nHow would the model be applied in cases without classification tasks? E.g., forgetting a single name or data entry.\n\nThe method aims to minimize the sensitivity difference between relevant and irrelevant classes. Just curious, what would happen if the gradients for the irrelevant classes are fixed, i.e., without backpropagating gradients, and only the gradient for the relevant class is used? I guess, in some sense, it would tell us if the loss is mainly working on the relevant class or the irrelevant classes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dACspHFpCp", "forum": "3iw5t2W41F", "replyto": "3iw5t2W41F", "signatures": ["ICLR.cc/2026/Conference/Submission17069/Reviewer_6Bdx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17069/Reviewer_6Bdx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761583122102, "cdate": 1761583122102, "tmdate": 1762927079117, "mdate": 1762927079117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the issue of improving unlearning methods using a new per-example signal and developing a method that does not require access to remain data (RD), only to the forget set (FS).\nSpecifically, it proposes MU-Mis, and revolves around a specific per-example signal, namely that of the sensitivity difference between the example’s target-class logit and non-target logits. MU-Mis for each FS example shrinks this gap.\nThe method optimizes a fairly simple objective over only FS, ie no RD access). \nThe paper presents extensive experiments with CIFAR-100, Tiny-ImageNet, and CIFAR-20 with ResNet-18 and ViT models, using a comprehensive set of metrics. The central claim is that MU-Mis performs on par or better than MU methods that access RD, while being faster."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Simple, nice idea revolving around the notion of per-example sensitivity of FS examples, with a plan to efficiently compute it.\n\n2. No RD access required. \n\n3. Comprehensive evaluation and results/discussion.\n\n4. Competitive performance vs 'older' baselines; often comparable to SalUn, with large speedups in some settings."}, "weaknesses": {"value": "1. Incorrect/unsupported claims with respect to achieving same or even better performance than MU methods accessing RD.\n\nThis claim is made using rather older methods. MU-Mis is shown to be comparable to Salun (which is typically either the best or 2nd best performer to MU-Mis.\n\nThe authors appear to be unaware of the paper by Zhao et al (NeurIPS 24 - https://arxiv.org/abs/2406.01257) which actually is shown to significantly outperform Salun (on datasets and models also tested in this paper). Specifically, Zhao et al show that their method, coined RUM, can encapsulate Salun (and several other MU methods) and significantly improve their performance. Hence,  RUM-Salun should be expected to also significantly outperform MU-Mis, no?. The authors should address this concern and test whether this expectation is valid and perhaps revisit their claims if so, At any rate, that method appears to be a much stronger baseline than Salun and should be included here.\n\nInterestingly, RUM appears to utiize a signal that is very similar to the sensitivity one used in this paper, namely the per-example memorization of FS examples (which the authors acknowledge to be akin to their sensitivity signal in this paper). So the link between the two approaches is very interesting and should be discussed. \n\nHowever, Zhao et al go further, utilizing different MU methods for examples with different memorization levels. And this approach (let's call it full RUM) appears to significantly outperform even RUM-Salun... So, again, the claims should be revisited in light of these methods. And full RUM should be included as a baseline/competing algorithm.\n\nAnd RUM appears to be able to use known memorization proxies, which are very efficient to compute - see the companion RUM paper (https://arxiv.org/html/2410.16516v2). \n\n2. The MIA method used in the paper may be rather simplistic, and related discussions on MIA results do not really help MU evaluation.\n\nThe paper by Kurmanji et al (cited in the paper) actually put forth a LiRA-like strong MIA for MU. So (i) this should be acknowledged and justified and (ii) this or other similar strong-grade MIA attacks should be used. Or maybe you should use the MIA also used in the RUM papers for consistent comparisons?\n\n3. On a related issue, the claim that \"lower MIA score is better\" seems counter-intuitive.\n\nAs the authors clearly state, performance should be viewed in relation to Retrain. So MIA scores of an MU method should be 'normalized' w.r.t. those of Retrain, no? The notion of MIA-gap (e.g., MIA_Retrain vs MIA_MU-Mis) in the RUM and other papers has been used for this reason. This should better be used here as well (with an appropriate MIA).\n\n4. The paper may/will benefit by incorporating more advanced baselines, in addition to RUM-Salun and full RUM. A highly interesting set of MU methods were proposed for the NeurIPS MU competition - the paper https://arxiv.org/abs/2406.09073 presents a rich set of such methods which notably outperformed well known published algorithms. Other MU methods have emerged more recenlty as well."}, "questions": {"value": "Please address the above weaknesses.\nEspecially the ones wrt RUM-Salun and full RUM, and wrt the MIA-related issues.\n\nOverall, the idea is clever and not requiring RD access is highly desirable. \nSo, I remain open to improving my score if, for example, it is shown that the proposed method continues to outperform SOTA RD-based MU methods and if results for better MIA scores show excellent performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rYKQpApC95", "forum": "3iw5t2W41F", "replyto": "3iw5t2W41F", "signatures": ["ICLR.cc/2026/Conference/Submission17069/Reviewer_pcTz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17069/Reviewer_pcTz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776732776, "cdate": 1761776732776, "tmdate": 1762927078783, "mdate": 1762927078783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MU-Mis, a novel remaining-data-free machine unlearning (MU) method based on the insight that a sample’s contribution to training is reflected in the model’s input sensitivity to that sample. The authors derive a theoretical connection between sample contribution $\\frac{\\partial A}{\\partial x_i}$ and the model’s input Jacobian $\\frac{\\partial f}{\\partial x}, and empirically validate that learned models exhibit higher input sensitivity to their training data. MU-Mis performs unlearning by minimizing the sensitivity gap between target and non-target logits on the forgetting samples, thus “withdrawing” their contribution without touching the remaining data. Extensive experiments show that MU-Mis achieves utility and privacy on par with or exceeding remaining-data-dependent methods, while being more efficient."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**S1**. The ability to unlearn without having access to the remaining data (fully or partially) is important and challenging, which this paper seems to be tackling. \n\n**S1**.  The derivation linking training sample contribution to the model’s input sensitivity while not very rigorous, but it makes it very intuitive.\n\n**S2**. The proposed method is computationally light. It requires only gradient computations on forgetting samples and uses simple stopping criteria."}, "weaknesses": {"value": "**W1**. MU-Mis operates effectively for sample- or class-level forgetting but doesn’t directly extend to feature or attribute unlearning, or probably fine-grained cases where samples partially overlap. In that sense, the model is not domain general. \n\n**W2**.  The method randomly selects a single irrelevant class $c'$ for the loss calculation in each step. An ablation study or justification for this random sampling (versus a deterministic average) can be clarifying.\n\n**W3**. The stopping guideline relies on a threshold ratio $\\delta$, which is a new hyperparameter. Table A1 shows that the optimal $\\delta$ varies significantly across different tasks (ranging from 0.68 to 3.00). This overstates the claim of \"effortless\" hyper-tuning. \n\n**W4**. Why RTE for pretrain and retrain is omitted overall? They can serve as good reference points. \n\n**W5**. The proposed method is built on the input sensitivity of the model. In practice, training deep models usually involves some techniques to reduce such sensitivities. For example, adversarial training, augmentation, or different types of regularizations. It’s not clear how the model would perform in the presence of these practical techniques."}, "questions": {"value": "**Q1**. How robust is the method to the choice of sensitivity metric (e.g., using $L_1$ vs Frobenius norm of $\\nabla_xf$)?\n\n**Q2**. I wonder if MU-Mis could inadvertently alter representations of semantically similar retained samples? \n\n**Q3** Do you have any results showing larger forget-set sizes? E.g. > 30%?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JSn1o3mKY8", "forum": "3iw5t2W41F", "replyto": "3iw5t2W41F", "signatures": ["ICLR.cc/2026/Conference/Submission17069/Reviewer_wSLf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17069/Reviewer_wSLf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928828571, "cdate": 1761928828571, "tmdate": 1762927078579, "mdate": 1762927078579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a remaining-data-free unlearning method that treats a sample’s training \"contribution\" as being reflected by the model’s input sensitivity to that sample. It empirically observes that training enlarges the gap between the target-logit input gradient and irrelevant-logit gradients for seen data, and shows that a retrained-without-forget set model shrinks this gap. The method then unlearns by minimizing this sensitivity-gap on forget samples with a simple stopping rule tied to the recovery of irrelevant-class sensitivity. Experiments across several datasets/architectures claim utility comparable to strong remaining-data-dependent baselines, plus efficiency and resilience under sequential unlearning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a remaining-data-free unlearning loss that minimizes the gap between target-logit and irrelevant-logit input sensitivities, along with a concrete stopping rule. This is simple and easy to implement, and the method itself uses only the forget set.\n\n2. The method is evaluated on multiple image-classification settings (full-class, sub-class, random-subset; ResNet-18, ViT), and the proposed method \"MU-Mis\" matches or beats SOTA that does use remaining data in several cases (e.g., CIFAR-100) while being efficient, especially on ViT."}, "weaknesses": {"value": "1. The theoretical analysis in Section 3.2, which aims to formally connect a sample's \"contribution\" to its \"input sensitivity\", is weak. The derivation relies on several convenient \"simplifications\" and \"omissions\". The central claim that the \"Residual Term\" is negligible is not rigorously proven and is instead justified by referencing a simple MLP example in the appendix. The theory feels more like a post-hoc rationalization for a empirical finding rather than a solid derivation that leads to the method.\n\n2. The claim \"first remaining-data-free method to outperform SOTA remaining-data-dependent\" of the paper reads a bit too strong. In several tables the advantages are small or mixed (e.g., CIFAR-100/Tiny-ImageNet where SSD/BT are very close, and the performance in random-subset task is a bit weak). The novelty/significance claim could be scoped more carefully."}, "questions": {"value": "1. Do different samples (e.g. highly memorized vs. typical ones) exhibit different sensitivity signatures? Or if the method is uniformly effective at unlearning different samples?\n\n2. Could you provide more details of the MIA (e.g. attack family, calibration, shadow data choices, etc.) you used in the paper? The current MIA details are brief and privacy claims are sensitive to these choices."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KSuLS8xcNm", "forum": "3iw5t2W41F", "replyto": "3iw5t2W41F", "signatures": ["ICLR.cc/2026/Conference/Submission17069/Reviewer_oXDL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17069/Reviewer_oXDL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955833942, "cdate": 1761955833942, "tmdate": 1762927078367, "mdate": 1762927078367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}