{"id": "6R42MRRs50", "number": 4742, "cdate": 1757756286140, "mdate": 1759898016792, "content": {"title": "Angel or Demon: Investigating the Plasticity-Enhanced Strategies' Impact on Backdoor Threats in Deep Reinforcement Learning", "abstract": "Deep Reinforcement Learning (DRL) faces significant threats from backdoor attacks, as indicated by numerous studies.\nHowever, these studies are conducted under idealized scenarios, whereas in practical settings DRL agents typically incorporate intervention strategies to maintain the policy's plasticity.\nSuch discrepancies may lead to misperceptions regarding the severity and nature of DRL backdoor attacks.\nTo bridge this gap, we investigate three research questions: \n(1) How do interventions impact backdoor attacks in DRL? \n(2) What are the underlying causes of these impacts? \n(3) What novel effects emerge when interventions are combined?\nTo answer these questions, we empirically study 10,998 cases covering representative interventions and attack scenarios.\nThe results reveal that most interventions, such as *Weight Clipping* and *Layer Normalization*, mitigate backdoor attacks due to two main mechanisms: disrupting fragile backdoor pathways and expanding the agent's representation space.\nInterestingly, *SAM* exacerbates the backdoor threat (e.g., leading to a 99.51\\% relative increase in attack effectiveness in robotic tasks) as it flattens the loss landscape, which reduces the pathway competition between the backdoor and benign tasks.\nNotably, combining *SAM* with other interventions further amplifies this effect.\nDrawing on these findings, we highlight two novel insights into robust backdoor injection and sharpness-based detection, with the aim of inspiring future research.", "tldr": "", "keywords": ["deep reinforcement learning", "backdoor attacks", "plasticity"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b57a6130d29eec5a4f1c3c4e579039dc8eeeec02.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper performs a study on the impact of standard Machine Learning interventions on the effectiveness of backdoor attacks in deep reinforcement learning. Specifically, they measure changes in attack success rate and benign episodic return metrics in DRL agents targeted by backdoor attacks before and after interventions like Weight Clipping, Layer Normalization, or Weight Decay are applied. They find that these interventions generally do not impact the performance of backdoor attacks like TrojDRL, UniDoor, and BadRL when the agent is poisoned during its initial training (TM1), but they do have non-trivial impact when agents are poisoned in a post-training regime (TM2).\n\nThe motivation of this is to study whether backdoor attacks can be prevented or made more easily detectable with the implementation of standard ML interventions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* I generally really appreciate papers like this that take a scientific approach towards understanding more subtle or ignored aspects of AI security. I think the authors' chosen angle of studying standard ML interventions is reasonable and fairly well motivated. \n\n* Section 5, in particular Figure 5c, is interesting and scientific, though it is hard to interpret as I will discuss later. \n\n* The paper's experimental results are extensive with respect to their chosen attacks, threat models, and interventions\n\n* The proof in Appendix E, despite its assumptions, is appreciated."}, "weaknesses": {"value": "### Misguided Conclusions\n\n1. I think this paper is misguided in trying to find a striking result in the data rather than analyzing and appreciating the results for what they are. Specifically, based upon the experimental results, I disagree with the author's claim that these interventions \"mitigate backdoor attacks\" [abstract]. Figure 3 along with the numerical results in the appendix indicate to me a very minimal impact of these interventions on the effectiveness of backdoor attacks. Instead, given that both the agent's ASR and BTP scores decreased across nearly all interventions, I think it is more likely that the interventions simply resulted in a universal degradation of performance, which included a decrease in performance in triggered states. Therefore, I don't think these interventions isolate backdoor attacks and decrease their effectiveness. I think practitioners may also avoid implementing these interventions if they truly result in decreased performance.\n\n2. Evaluating the impact of these interventions in the TM2 setting, where the adversary post-trains an existing model to inject the backdoor, seems strange to me. My understanding is that these interventions must be implemented at training time, meaning the adversary is the one using, for instance, weight clipping. Am I correct in this assessment? If this is the case, and given that the interventions decrease BTP performance, I find it unlikely that an adversary would willingly use these interventions. \n\n### Incomplete Evaluation\n\nTo be clear, as I mentioned in my Strengths section, the paper's results are very extensive within the scope of the 3 attacks and 9 environments chosen by the authors. Studying over 10,000 combinations of attacks, environments, and threat models is impressive and appreciated. That being said, I think the authors missed an opportunity in (1.) only evaluating attacks using static reward poisoning (TrojDRL, BadRL, and UniDoor all use the same fundamental reward poisoning strategy), and (2.) only exploring continuous action space domains. \n\n1. I think these results would be more compelling if they showed promising mitigations against more state of the art backdoor attack strategies like SleeperNets, which the authors already cite. I do have sympathy for the authors here if they began or completed their evaluations before SleeperNets was published, as their experimental results are surely time consuming, but their results are incomplete without analyzing it in my opinion. Furthermore, since SleeperNets has not been evaluated in continuous action space domains before, to the best of my knowledge, this could be a small contribution on its own.\n\n2. In regards to environments, it is my opinion that the paper would benefit from further evaluations on discrete action space domains, like Atari, in lieu of evaluations on simpler, classic control tasks like cart pole. That being said, it a reasonable restriction given the scope of the results. \n\n### Presentation of Results\n\n1. In general it seems there is a disconnect between the way results are explained in the text versus how they appear in the plots and figures. For instance, in the text around figure 5 a 39.31% change in weight magnitude \"performance ranges\" is claimed, however in the plot there appears to be little difference between the conventional training versus backdoor training results. To me this misunderstanding stems from the fact that the text is seemingly assessing variance in results, noting that backdoor models result in higher variance, while readers looking at the plots will be primarily concerned with mean values. Perhaps the authors can find more clearly insightful results if they analyze some results on a per-case basis (e.g. noting a particular environment or attack with a large discrepancy), rather than looking at aggregates. This may give more insight into particular cases where these interventions are impactful, rather than seeing the very small changes observed in aggregate. \n\n2. Continuing with section 5.2, figure 6 is very confusing and hard to interpret in my judgement. I think some of the confusion stems from the \"rank\" metric being used and its conflict with the term \"rank\" used in linear algebra and throughout the paper. Additionally, in my mind, a lower rank usually indicates higher impact (e.g. someone who ranks #1 in a race is the winner), while here a rank of 8 seems to be the highest score. Perhaps a term like \"relative impact\" would be more clear. \n\n### Minor Weaknesses\n\n* I don't think the threshold $\\epsilon$ is ever given a value.\n\n* Many figures are very small and hard to read. I think sections like the Combination Analysis can be moved to the appendix to create more space for larger figures and more analysis.\n\n* The legend in figure 4 clips over some of the results\n\n### Conclusion\n\nOverall I think the results of this paper are too inconclusive. I do appreciate the authors' efforts, and I do believe that such evaluations are important, but I'm unsure if the claimed results are significant enough in their mitigation of backdoor attacks against DRL to warrant publication in a major ML venue as a finished product. I believe there are likely other interesting conclusions which may be drawn from the data, but the current paper does not bring these to light in its current form."}, "questions": {"value": "* What value of $\\epsilon$ is used in this paper's evaluation?\n\n* When evaluating TM2 are the interventions performed during the adversary's post-training phase?\n\n* When evaluating BTP with interventions, are the authors comparing against a \"Conventional Training\" agent trained with or without interventions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kDsnZp8eHl", "forum": "6R42MRRs50", "replyto": "6R42MRRs50", "signatures": ["ICLR.cc/2026/Conference/Submission4742/Reviewer_Y4ax"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4742/Reviewer_Y4ax"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675720393, "cdate": 1761675720393, "tmdate": 1762917548732, "mdate": 1762917548732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies backdoor attacks against Deep Reinforcement Learning (DRL). In real-world deployments, DRL agents often adopt intervention strategies (e.g., normalization, weight regularization, sharpness control) to improve stability and mitigate plasticity loss. However, no prior work has examined how these interventions themselves influence the effectiveness of backdoor attacks. The goal of this paper is therefore to investigate how and why intervention strategies affect the behavior of DRL backdoor attacks.\n\nTo this end, the authors conduct over ten thousand experiments under two threat models (training-time and post-training injection), eight types of interventions, and three representative backdoor attack methods. This paper first evaluates how different interventions impact the attack success rate (ASR) and benign task performance (BTP), showing that some interventions mitigate attacks while others not. Then, they analyze the underlying causes through four pathological indicators (weight magnitude, effective rank, loss-landscape sharpness, dormant neurons), and condlude that interventions influence the model’s adaptability and stability. Finally, they explores how combinations of interventions affect backdoor."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important and previously unexplored question: how intervention strategies affect backdoor attacks. This perspective is novel because most prior works focus on improving DRL performance or robustness, while overlooking the potential security side effects of such interventions.\n\n\n2. The paper conducts a large number of experiments covering diverse DRL tasks, attack settings, and intervention strategies. The experimental scale and breadth are sufficient to provide a solid empirical foundation and can serve as a valuable reference for future work in this area. Although certain concerns about the experimental depth will be mentioned later, the comprehensiveness of the evaluation remains one of the paper’s clear strengths."}, "weaknesses": {"value": "1. Although the paper provides extensive experimental results, many of the conclusions are not sufficiently verified through follow-up analysis. For example, in Section 5.2 the authors claim that SAM strengthens backdoor attacks because it makes the loss landscape smoother. However, they do not perform additional experiments (for instance, by making the loss landscape sharper to see if the attack weakens), which would be necessary to clearly validate this explanation. As a result, although most of the conclusions appear reasonable, they are not yet convincing enough to be fully trusted due to the lack of sufficient validation.\n\n2. The paper reports many findings across different interventions and metrics, but these results are presented in a fragmented way without forming a coherent main message. Although the paper highlights two key insights in the conclusion: (1) Robust Backdoor Injection (2) Sharpness-Based Detection. While both are interesting and practically useful observations, they seems to application-level insights rather than general principles. As a result, the reader may find it difficult to identify the key takeaway."}, "questions": {"value": "1. What is the key conclusion or principle that readers should take away? (explaination see Weakness #2)\n\n2. Some of the conclusions in the paper seem well supported by evidence, while others appear more preliminary.\nCould the authors clarify which findings they consider most reliable and which ones are more exploratory or require further validation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PzHP5O9oqR", "forum": "6R42MRRs50", "replyto": "6R42MRRs50", "signatures": ["ICLR.cc/2026/Conference/Submission4742/Reviewer_q1rd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4742/Reviewer_q1rd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891640618, "cdate": 1761891640618, "tmdate": 1762917548239, "mdate": 1762917548239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tries to understand the intersection of two fields of reinforcement learning study: techniques used to maintain plasticity to allow for long term reinforcement learning, and attacks designed to insert backdoor behavior in response to certain observational triggers. They seek to understand how the application of different plasticity techniques impacts the feasibility of successfully executing backdoor attacks, and find that most common techniques make backdoor training harder, while only one (SAM) improves attack success rate. The authors do this across two attack settings: one where an attack is inserted during training, and another where it is inserted during post-training. They then go on to try to understand commonalities between techniques that make backdoor creation harder, and identify that multiple techniques have the effect of increasing the rank of weight matrices. They believe that this effective \"promotion\" of smaller-magnitude directions helps drown out the salience of the trigger used in the backdoor."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- I appreciated the systematic testing of multiple interventions over multiple backdoor threat models \n- The discussion section gave valuable insights into why we might expect the interventions studied to have the effects they do on backdoor injection"}, "weaknesses": {"value": "- Plots and figures were tiny relative to the scale of text, making it hard to see and understand them \n- Writing quality was generally poor and made the paper hard to understand \n- The use of shortened acronyms (e.g. TM1 and TM2 for pretraining and posttraining injection) made the paper hard to parse - I had to keep reminding myself which is which. The paper would have been much easier to read if each acronym were replaced with a (shortened) reference to what it was actually referring to"}, "questions": {"value": "- How is it possible for SAM to induce a greater than 100% Success Attack Rate? This was quite confusing, and made me wonder if there was a typo"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MmgYdRYiKT", "forum": "6R42MRRs50", "replyto": "6R42MRRs50", "signatures": ["ICLR.cc/2026/Conference/Submission4742/Reviewer_Gy5W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4742/Reviewer_Gy5W"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953030523, "cdate": 1761953030523, "tmdate": 1762917547753, "mdate": 1762917547753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on backdoor attacks and deep reinforcement learning plasticity, and  shows that interventions, such as Weight Clipping and Layer Normalization, mitigate backdoor attacks by expanding the agent’s representation space."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Please see below."}, "weaknesses": {"value": "It is already well-known that weight clipping and layer normalization mitigates backdoor attacks [1,2,3]. These concepts have been well studied. None of these prior work on demonstrating the exact same results were discussed, cited or mentioned even in the background. These issues are foundational to deep neural networks and well-studied. There is nothing specific to reinforcement learning that makes the results even remotely surprising. \n\nI also find it quite concerning that the paper did not read and review prior work and try to place their claims and contributions according to that. There is a long list of citations, i.e. 5 pages, yet this long list somehow excludes the papers that are showing quite similar issues. \n\nOnly one single deep reinforcement learning algorithm is tested. This is not enough evidence to convey general claims about deep reinforcement learning.\n\nMost of the experimental section is based on quite simple tasks such as CartPole, Acrobot,  MountainCar, Lunar Lander with discrete action set and low dimensional states. These are not the environments where a publication can make general claims about deep reinforcement learning. The paper studies concepts that are basic to deep reinforcement learning. Yet, the experimental setting does not reflect a setting that can verify the claims made in the paper regarding deep RL.\n\nFurthermore, looking at the results, TM1 is always within the standard deviation across interventions. Again this significantly limits the claims of the paper.\n\nI also disagree with the phrase “DRL agents typically incorporate intervention strategies,\nwhich mitigates the plasticity loss problem”. This is just a new research area, it has not much to do with what happens in practice.\n\n\n\nEssentially, only 3 backdoor attacks have been used [4,5,6]. But the results are reported to inflate and make the impression that a comprehensive study has been conducted. I find this kind of presentation quite misleading.\n\nThere are many more backdoor attacks in deep reinforcement learning. I am not necessarily going to cite all of them here. But I just find it misleading to constantly phrase working on 10s of thousands cases while essentially working on simply 3 backdoor attacks.\n\nGiven all these limitations, this paper might be more appropriate for a venue that will appreciate slightly more known and expected results. \n\n\n[1] Can You Really Backdoor Federated Learning?, NeurIPS 2019.\n\n[2] A Gradient Control Method for Backdoor Attacks on Parameter-Efficient Tuning, ACL 2023.\n\n[3] Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization, ICML 2024.\n\n[4] BadRL: Sparse Targeted Backdoor Attack against Reinforcement Learning, AAAI 2024.\n\n[5] UNIDOOR: A Universal Framework for Action-Level Backdoor Attacks in Deep Reinforcement Learning, Arxiv 2025. \n\n[6] TrojDRL: Evaluation of Backdoor Attacks on Deep Reinforcement Learning, ACM/IEEE Design Automation Conference (DAC) 2020."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gETzV1PQlF", "forum": "6R42MRRs50", "replyto": "6R42MRRs50", "signatures": ["ICLR.cc/2026/Conference/Submission4742/Reviewer_bCoi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4742/Reviewer_bCoi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020765311, "cdate": 1762020765311, "tmdate": 1762917547247, "mdate": 1762917547247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}