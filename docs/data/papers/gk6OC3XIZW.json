{"id": "gk6OC3XIZW", "number": 15475, "cdate": 1758251721650, "mdate": 1759897304459, "content": {"title": "Contamination Detection for VLMs Using Multi‑Modal Semantic Perturbations", "abstract": "Recent advances in Vision–Language Models (VLMs) have achieved state-of-the-art performance on numerous benchmark tasks. However, the use of internet-scale, often proprietary, pretraining corpora raises a critical concern for both practitioners and users: inflated performance due to \\emph{test-set leakage}. While prior work has proposed mitigation strategies such as decontamination of pretraining data and benchmark redesign for LLMs, the complementary direction of developing detection methods for \\emph{contaminated VLMs} remains underexplored. To address this gap, we deliberately contaminate open-source VLMs on popular benchmarks and show that existing detection approaches either fail outright or exhibit inconsistent behavior. We then propose a novel simple yet effective detection method based on \\textit{multi-modal semantic perturbation}, demonstrating that contaminated models fail to generalize under controlled perturbations. Finally, we validate our approach across multiple contamination strategies, confirming its robustness and effectiveness. The code and perturbed dataset will be released publicly.", "tldr": "We devise a novel contamination detection method for vision language models.", "keywords": ["Vision Language Models", "Data Contamination"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3307ec203897cafdb57ebb242f1b95ea5b56b13c.pdf", "supplementary_material": "/attachment/30b36ff9eb4b19bb0786d177a988db5611ddede2.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles the underexplored problem of detecting test-set contamination in Vision–Language Models (VLMs) — as opposed to merely avoiding it via decontamination. Instead of text‐only perturbations used for LLMs, the authors propose a multi-modal semantic perturbation pipeline that manipulates the visual scene while preserving composition, thus minimally shifting input semantics yet changing the ground-truth answer. They contaminate LLaVA and Qwen2-VL under controlled fine-tuning regimes and show that existing detection baselines (e.g., shared likelihood, guided prompting, circular eval, choice confusion) fail to reliably track contamination. Their method exhibits high practicality (black-box only), reliability across fine-tuning types, and consistent monotonic signal w.r.t. contamination degree. They further validate on natural counterfactuals (NaturalBench), larger models (LLaVA-13B), and even simulated pretraining leakage, demonstrating generality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper poses a non-generic, well-defined, and deep problem — not merely “robustness” or “performance drops,” but specifically how to detect contamination without assuming prior access to clean references or to the training corpus. The authors not only identify but formalize the essential requirements (practicality, reliability, consistency) and then prove why existing methods violate them, rather than merely benchmarking blindly. The methodology is elegant but grounded — the perturbation pipeline is semantically meaningful, not superficial image corruption — and their experimental design is unusually thorough and fair (multiple contamination strategies, ablations, real counterfactuals, automated filtering, alternative captioners)."}, "weaknesses": {"value": "The approach implicitly depends on the availability of strong controlled semantic editors (Flux + GPT-4o + ControlNet); although ablations with Molmo and automated filtering are shown, the method’s feasibility still assumes future generative tools remain capable and unbiased.\n\nThe evaluation domain is limited to visual-grounded multiple-choice VQA benchmarks (e.g., RealWorldQA, MMStar); it is argued that free-form QA is possible, but no concrete evidence is provided. \n\nWhile the method is claimed fully black-box, it still rests on the assumption that perturbed samples are truly non-harder than originals: a subtle but critical assumption, mainly supported indirectly via model behavior rather than formal difficulty guarantees."}, "questions": {"value": "Your framework assumes that the perturbed version is of comparable or lower difficulty than the original, but this is only inferred indirectly via clean model behavior. How do you enforce or guarantee this assumption beyond empirical observation? Could there exist cases where the perturbation unintentionally increases difficulty and generates false positives?\n\nYour method relies on having access to a generative model strong enough to produce controlled, faithful semantic perturbations. In lower-resource or restricted deployment regimes, do you still consider your method “practical” (Req. 1)? What is your definition of practicality beyond using “a black box”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SkvEVkcmLQ", "forum": "gk6OC3XIZW", "replyto": "gk6OC3XIZW", "signatures": ["ICLR.cc/2026/Conference/Submission15475/Reviewer_6vCV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15475/Reviewer_6vCV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786203146, "cdate": 1761786203146, "tmdate": 1762925767267, "mdate": 1762925767267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces multi-modal semantic perturbation as a detection framework for identifying data contamination in VLMs. The method detects contamination by measuring the performance degradation of a model on perturbed samples compared to the original. Experiments show the proposed detection setting is reliable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-motivated problem definition.\n2. Extensive comparisons across contamination settings (fine-tuning, LoRA, pretraining) and baselines demonstrate consistent detection performance.\n3. The detection approach is straightforward by comparing performance on the original vs. perturbed input."}, "weaknesses": {"value": "Perturbation generation requires LLM and diffusion inference per sample, implying high computational cost. This method could be a bit difficult to generalize due to scalability and efficiency constraints."}, "questions": {"value": "See weakness. How computationally expensive is the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yFZvXkeFBE", "forum": "gk6OC3XIZW", "replyto": "gk6OC3XIZW", "signatures": ["ICLR.cc/2026/Conference/Submission15475/Reviewer_Duhn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15475/Reviewer_Duhn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874224167, "cdate": 1761874224167, "tmdate": 1762925766876, "mdate": 1762925766876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies test-set leakage detection for VLMs. The authors deliberately “contaminate” open-source models like Qwen2-VL by fine-tuning them on benchmark data and show that prior text-based contamination detection methods fail. They then propose a method called multi-modal semantic perturbation, which uses GPT-4o to rewrite captions and Flux + ControlNet to generate visually altered images that subtly change the correct answer. If a model performs well on the original but fails on the perturbed version, it is flagged as potentially contaminated."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets data leakage in multimodal models, which is an important issue.\n\n2. It provides systematic experiments across multiple contamination types and model sizes, with quantitative comparisons to several baselines.\n\n3. The overall framework is simple and easy to understand, and the perturbed benchmark will likely be useful to the community."}, "weaknesses": {"value": "**1. Inaccurate and heavy generation pipeline with high manual cost.**\n\nI find the proposed “detection pipeline” is simple, unnecessarily complex, and fragile for what it aims to do. It requires GPT-4o to generate dense captions, a Flus plus ControlNet for Canny-guided editing (a very crude method for composition-preserved image generation), and finally human filtering to discard failed generations. \n\nIn addition, the method needs manual filtering to remove a large portion of the data. In particular, RealWorldQA is reduced from 765 to 440 samples, and MMStar from 1500 to 495, over 1/3 is filtered.\n\nTo me, this already contradicts their claim of practicality: a method that needs strong proprietary models and heavy human effort cannot serve as a scalable detector. The “clean vs. contaminated” separation is only visible after extensive manual curation.\n\n**2. The detection signal is confounded with robustness, not contamination.**\n\nThe method assumes that if a model fails on perturbed images, it flags contamination. But the paper itself shows that many perturbations accidentally change task difficulty. In Figure 3, the perturbation even enlarges the visual cue (a speed limit sign), making the new image easier.\n\nIn other cases (Figure 4), the perturbed image drifts too far from the original, so even a clean model can fail while a contaminated one may still succeed. In my view, the metric mainly captures distribution shift sensitivity rather than true memorization, so the signal is not clean. Essentially, this is caused by the instability of the perturbation method being constructed.\n\n**3. Unrealistic and self-serving contamination setup.**\n\nThe contamination experiment is extremely idealized: directly fine-tune the model on the full test set for one to three epochs. This guarantees that the model has seen every evaluation item. Real leakage in practice is much subtler: partial overlap, web-scale data, paraphrased variants, and it’s unclear whether the proposed metric would still detect that. \n\nTherefore, while the results look strong, the paper proves only that the method works under a trivially detectable contamination pattern. It’s closer to a sanity check than to a general detection method.\n\n**4. Limited applicability and reliance on strong visual grounding.**\n\nThe method only works for benchmarks where the image strictly determines the answer. As they note themselves, “if a question can be answered without visual input, perturbing the image is meaningless”. This excludes open-ended captioning, grounded reasoning, or OCR-heavy tasks. So the claimed “general framework for contamination detection” is actually limited to multiple-choice VQA tasks with strong visual dependency."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GQJ7jDKX1D", "forum": "gk6OC3XIZW", "replyto": "gk6OC3XIZW", "signatures": ["ICLR.cc/2026/Conference/Submission15475/Reviewer_55ak"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15475/Reviewer_55ak"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894064661, "cdate": 1761894064661, "tmdate": 1762925766492, "mdate": 1762925766492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel detection framework based on multi-modal semantic perturbations, which involves generating new test samples by subtly altering an image’s content (while preserving overall semantics) so that a model which merely memorized the original image-text pair will fail on the perturbed input. Experiments show that existing detection methods often fail or give inconsistent results on VLMs, whereas the proposed perturbation-based approach consistently flags contaminated models across diverse fine-tuning settings and degrees of contamination, satisfying key requirements of reliability, practicality, and consistency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Proposes an original and practical contamination detection method tailored for VLMs using multi-modal semantic perturbations.\n\nDemonstrates strong technical quality through extensive and controlled experiments across diverse settings.\n\nClear presentation with significant implications for reliable and fair evaluation of vision-language models."}, "weaknesses": {"value": "While the proposed method is practically useful, it primarily consists of integrating existing tools—LLMs for captioning and diffusion models for image editing—into a contamination detection pipeline. As such, the technical novelty is relatively limited. The idea of testing generalization via perturbed inputs is well-established, and the paper applies this concept to the multi-modal setting without introducing fundamentally new algorithms or theoretical insights.\n\nThe description of the core methodology in Section 4 is relatively high-level and omits several important implementation and design details. For example, while the use of GPT-4o and ControlNet is outlined, it is unclear how semantic alignment between the new answer and generated image is ensured, or how failure cases are handled systematically beyond manual filtering.\n\nThe core contamination detection criterion—declaring a model contaminated if it answers perturbed samples incorrectly while answering original ones correctly—is intuitive but lacks technical precision. The paper does not clarify whether this evaluation is done at the sample level, across aggregate performance metrics, or via some probabilistic threshold.\n\nSince the proposed method relies on detecting failures under semantic perturbations, it would be informative to compare against standard OOD generalization or robustness baselines. This would help disentangle contamination from general lack of robustness."}, "questions": {"value": "same as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pilZL0yLdy", "forum": "gk6OC3XIZW", "replyto": "gk6OC3XIZW", "signatures": ["ICLR.cc/2026/Conference/Submission15475/Reviewer_T6b8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15475/Reviewer_T6b8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144244237, "cdate": 1762144244237, "tmdate": 1762925765698, "mdate": 1762925765698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}