{"id": "EB2Qgp5Vb0", "number": 21538, "cdate": 1758318670446, "mdate": 1759896916944, "content": {"title": "Deep Learning with Learnable Product-Structured Activations", "abstract": "Modern neural architectures are fundamentally constrained by their reliance on fixed activation functions, limiting their ability to adapt representations to task-specific structure and efficiently capture high-order interactions. We introduce deep low-rank separated neural networks (LRNNs), a novel architecture generalizing MLPs that achieves enhanced expressivity by learning adaptive, factorized activation functions.  LRNNs generalize the core principles underpinning continuous low-rank function decomposition to the setting of deep learning, constructing complex, high-dimensional neuron activations through a multiplicative composition of simpler, learnable univariate transformations. This product structure inherently captures multiplicative interactions and allows each LRNN neuron to learn highly flexible, data-dependent activation functions. We provide a detailed theoretical analysis that establishes the universal approximation property of LRNNs and reveals why they are capable of excellent empirical performance. Specifically, we show that LRNNs can mitigate the curse of dimensionality for functions with low-rank structure. Moreover, the learnable product-structured activations enable LRNNs to adaptively control their spectral bias, crucial for signal representation tasks. These theoretical insights are validated through extensive experiments where LRNNs achieve state-of-the-art performance across diverse domains including image and audio representation, numerical solution of PDEs, sparse-view CT reconstruction, and supervised learning tasks. Our results demonstrate that LRNNs provide a powerful and versatile building block with a distinct inductive bias for learning compact yet expressive representations.", "tldr": "a new deep learning architecture with learnable product-structured activations", "keywords": ["deep learning architecture", "implicit neural representation", "low-rank tensor decomposition", "partial differential equations"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec4d9b07384412228e44f32e8c557cc69aaf3e4d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a novel architecture, low-rank separated neural networks (LRNNs), that can be understood as MLPs with learnable activations taking a particular product-factorized form. The authors introduce their architecture, compare it to an MLP, and prove a universal approximation theorem as well as theoretical results studying their architecture's parameter efficiency and inductive bias. They then carry out a thorough empirical study of their architecture, showing that it outperforms prior work in terms of parameter efficiency, representation quality, and convergence rate when used as an implicit neural representation, exhibits good parameter efficiency for solving low-dimensional Poisson equations, and achieves better reconstruction quality on audio representation and CT scan reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written, and I was able to follow the main ideas without much trouble. The authors generally do a good job of explaining the significance of their theoretical results. However, I have some questions about the assumptions for Theorem 2, which I detail below.\n- I appreciate that the authors have included theoretical analysis for their architecture's parameter efficiency in addition to the standard universal approximation results. I have some questions for Theorem 2, which I detail below.\n- The empirical results are strong across the board, though as I note below, LRNN's particularly strong performance in image representation might be overkill for real-world tasks."}, "weaknesses": {"value": "- I'd appreciate the authors providing some more intuition for the assumptions of Theorem 2, since this is a key theoretical result motivating their new architecture. I include some specific questions below.\n- The time-to-solution results in Table 1 are a valuable addition to the other data on LRNN-SPIDER's image representation performance. One could argue that the very high PSNRs (>100 dB) reported for LRNN are overkill for real-world tasks, and that the baseline methods reach a threshold of sufficient quality faster than LRNNs. For instance, I cannot discern any visual difference between the SPIDER reconstruction (35.3 dB) and the LRNN reconstruction (107.9 dB) in Figure 3, nor can I discern a visual difference between any of the non-ReLU baselines for the retina image in Figure 10."}, "questions": {"value": "How should we interpret the assumptions for Theorem 2? In particular: \n\n-What are some examples of functions \"whose ANOVA decomposition is dominated by terms involving at most $m ≪ d$ variables\" and functions that violate this assumption? \n- Does the $m_a ≪ d$ condition in Assumption 3 of the proof (lines 1026-1031) correspond to a particular big-O or little-o bound on the interaction order $m_a$?\n- How restrictive is Assumption 4? It seems that it must be fairly restrictive, because if $m_a = d$, then one of the terms in the functional ANOVA decomposition involves all $d$ variables and the restrictions on $f$ come entirely from Assumption 4, but the complexity of the LRNN approximation still grows only polynomially with $d$ according to Theorem 2. So Assumption 4 should be pretty strong to mitigate the curse of dimensionality in this way."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9A8wGinjbY", "forum": "EB2Qgp5Vb0", "replyto": "EB2Qgp5Vb0", "signatures": ["ICLR.cc/2026/Conference/Submission21538/Reviewer_yobX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21538/Reviewer_yobX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666315759, "cdate": 1761666315759, "tmdate": 1762941826941, "mdate": 1762941826941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel approach to expressing a neural representation, primarily utilizing MLPs, by introducing a low-rank formulation and incorporating activation functions as learnable univariate functions. The paper presents preliminary results on the tasks of signal representation and inverse problems, including sparse-CT and super-resolution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The premise of decomposing each layer into a low-rank layer is interesting and the preliminary results indicate its promising ability for signal representation and inverse problems. \n\n- The paper presents a careful explanation of how LRNNs are implemented, along with added derivations to ensure convergence and stability.  \n\n- The paper presents promising initial results, highlighting the impressive ability of LRNNs for signal representation and PDEs."}, "weaknesses": {"value": "Lacks rigorous evaluation: The paper doesn’t show results on image representation or audio representation across a wide variety of samples. The ability of LRNNs to perform across a wide variety of data, not just a few select images, is crucial to understanding the value of this contribution.  Please display results on a large number of examples (e.g., 1000) such as Celeb-A, FFHQ, or ImageNet. Previous papers on signal representations (Functa[1,2], TransINR[3], STRAINER[4]) have shown a rigorous assessment of INRs on the task of image fitting. Furthermore, for tasks such as super-resolution and CT reconstruction, only one image/signal is used, which does not accurately convey the general performance of LRNNs.\n\nInsufficient analysis of LRNN: The paper does not address why a deep low-rank network is able to correctly capture high-quality image or audio-level details. Typically, low-rank approximations work when a distribution can be easily marginalized into “r” independent vectors. Given that LRNNs use a small “r”,  this seems rather counterintuitive as to why it would work. Any qualitative or formal insights would greatly benefit the paper.\n\n[1]From data to functa: Your data point is a function and you can treat it like one, Dupont et.al\n[2]Spatial Functa: Scaling Functa to ImageNet Classification and Generation, Bauer et.al.\n[3]Transformers as Meta-Learners for Implicit Neural Representations, Chen et.al\n[4]Learning Transferable Features for Implicit Neural Representations, Vyas et.al."}, "questions": {"value": "1. For tasks such as 3D scene reconstruction (NERFs), where signal representations are most widely used, have the authors tried/observed any significant performance improvements (faster convergence?) \n\n2. L78: The Authors claim that a low-rank assumption would enhance expressivity. However, that relies on a few key assumptions: for the data to have a low rank of “r”, it would mean that it can be marginalized into r independent variables/distributions. Only in such a case could the LRNN’s design “enhance expressivity.” Is this true in the case of natural images or audio’s frequency spectra? \n\n3. Compared to PCA with “r” components, what benefit does a 2-layer LRNN yield, besides it being nonlinear?\n\n4. The results for the cameraman image appear to be inconsistent with those in Table 7 and Figure 3 (PSNR plot). In Table 7, it is reported that SPDR reaches 30 dB in 2 seconds, before LRNNs. However, the plot shown in Figure 3 doesn’t concur with this finding. It seems that both the figure and the table correspond to the same experiment. Please explain the discrepancy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OzCNDltiJM", "forum": "EB2Qgp5Vb0", "replyto": "EB2Qgp5Vb0", "signatures": ["ICLR.cc/2026/Conference/Submission21538/Reviewer_XNN7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21538/Reviewer_XNN7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966358756, "cdate": 1761966358756, "tmdate": 1762941826675, "mdate": 1762941826675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Deep Low-Rank Separated Neural Networks (LRNNs) — a new neural network architecture that generalizes multilayer perceptrons (MLPs) by endowing each neuron with a learnable, product-structured activation function. Instead of relying on fixed non-linearities (e.g., ReLU, Tanh, Sine), LRNNs construct activations as multiplicative compositions of learnable univariate functions. This design captures multiplicative and higher-order interactions efficiently, inspired by low-rank separated representations and tensor decomposition theory. The authors provide theoretical guarantees, including universal approximation, variance-controlled initialization, and curse of dimensionality mitigation for low-rank structured functions. Empirical results are demonstrated on images, audio, PDE solving, sparse-view CT reconstruction. Overall, LRNNs offer a unified, theoretically grounded architecture that merges ideas from tensor decompositions, implicit neural representations (INRs), and adaptive activations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is very well-written and motivated.\n2. Related work section is well rounded.\n3. The paper is easy to follow.\n4. The idea of learnable product-structured activations is novel unifying classical low-rank function decomposition with modern deep learning architectures.\n5. The theoretical results are are well motivated and rigorously derived.\n6. Experiments span multiple domains such as image and audio INRs, PDE solving, and CT reconstruction — demonstrating the versatility of LRNNs.\n7. LayerNorm regularization, parameter sharing across layers, and use of small MLPs for component functions are well justified.\n8. I believe this can open up new architectural choices for many domain including scientific machine learning."}, "weaknesses": {"value": "1. Explicit computation requirement comparison is missing for the numerical experiments, these would make the proposed architecture much more favourable.\n2. The paper acknowledges that LRNNs require higher memory due to product-term storage. More quantitative profiling (time per iteration, memory vs MLPs) would help assess practicality for large-scale settings.\n3. The plot(s) can be made more clearly legible.\n4. Some ablations regarding the architecture choices should be discussed in the main paper."}, "questions": {"value": "1. Can author(s) discuss how the effective separation rank evolve during training? Does over-parameterizing  r lead to redundancy or implicit sparsity (similar to low-rank dropout)?\n2. The variance-controlled initialization ensures bounded gradients theoretically. In practice, do we observe gradient explosion/vanishing for deeper networks?\n3. Can there be a discussion around the interpretability of LRNNs?\n4. Could the product-structured activation principle be incorporated into attention or convolutional blocks to yield hybrid LRNN-transformer models?\n5. Given the increased per-layer product cost, can the author(s) comment on the potential use of mixed-precision or kernel-fusion optimizations to reduce compute overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6JaNQqoYOH", "forum": "EB2Qgp5Vb0", "replyto": "EB2Qgp5Vb0", "signatures": ["ICLR.cc/2026/Conference/Submission21538/Reviewer_VXpv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21538/Reviewer_VXpv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762035841835, "cdate": 1762035841835, "tmdate": 1762941826438, "mdate": 1762941826438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method for learning more expressive activation functions as products of small neural networks themselves.  Theoretical results are presented on the universal expressivity of this parameterization and its ability to efficiently represent functions which have low rank structure to exploit.  Principled initialization schemes are proposed and theoretically justified.  Experiments on function reconstruction tasks show a significant improvement compared to state of the art architectures, while experiments with standard classification tasks show a marginal improvement to test accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is very well written with a clear exposition and convincing experimental results.  The theoretical results were also presented in a clear and complete manner.  I particularly appreciated the commentary and theoretical results on parameter complexity, this is often omitted during discussions of universal approximation.  This work gives a significant improvement to architectures for signal reconstruction specifically and should be accepted."}, "weaknesses": {"value": "Some minor comments for improving the development:\n\nThe commentary on parameter sharing at the end of Section 3.2 was of immediate interest during a first read but is minimal in the main text and pushed to the appendix.  An extra comment here and at least a reference to where this appears in the appendix would make for a smoother read.\n\nThere is a single comment on memory considerations in the concluding remarks, but having more commentary on this would be very useful for practitioners interested in using this method."}, "questions": {"value": "1. What are the architecture size/shape parameters (depth/width) of the small MLPs used for the g's throughout?  \n\n2. How sensitive is the performance to these architecture size choices for the g's?  As I understand the scaling law experiments increase the parameter count with the width of the small MLPs, is there sensitivity to the number of layers or rank as well?\n\n3. Is there a typo in eq (2) and where this appears in the appendix as well?  An extra comma on the right-hand side."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OOpBGx8yo3", "forum": "EB2Qgp5Vb0", "replyto": "EB2Qgp5Vb0", "signatures": ["ICLR.cc/2026/Conference/Submission21538/Reviewer_XhCn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21538/Reviewer_XhCn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112374144, "cdate": 1762112374144, "tmdate": 1762941826127, "mdate": 1762941826127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}