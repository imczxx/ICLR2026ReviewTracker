{"id": "2RWf359T0p", "number": 10602, "cdate": 1758177081067, "mdate": 1759897641292, "content": {"title": "Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning", "abstract": "It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to diversity collapse, where outputs lack variety. This phenomenon has been linked to a sharpening effect induced by RL. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits sharpening. Building directly on this analysis, we introduce a principled method—differential smoothing—that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 (correctness) and Pass@k (diversity), with up to 6.7\\% improvements on AIME24 dataset.", "tldr": "We propose a novel method to improve RL fine-tuning of LLM", "keywords": ["LLM Reasoning", "RL", "Diversity"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b66a35ab7dc3247abc406b2c47a9a903accd67ac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors tackle the problem of diversity collapse when finetuning LMs with reinforcement learning. The phenomenon is referred to as the sharpening effect and the authors show theoretically that this effect arises during RL finetuning of LMs.\n\nTo mitigate the sharpening effect in RL the authors propose a modification of the reward function—differential smoothing---to down-weight correct samples and upweight incorrect samples by the likelihood under the base model. The authors prove that this mitigates the sharpening effect in vanilla RL. In practice, the authors propose differential-smoothing GRPO or DS-GRPO which modifies the GRPO advantages with the corresponding likelihoods depending on the trajectory being correct or incorrect.\n\nOn Countdown and Math500 datasets the authors show empirically that this both improves pass@1—overall correctness—and pass@k—diversity. The authors also provide an explanation regarding how entropy penalties and entropy bonuses have both been shown to improve performance on distinct datasets by showing a relationship with the number of unique correct solutions; entropy bonuses work well with datasets which have a high number of unique correct solutions.  \n\nAdditionally in the vanilla RL setting the authors show theoretically that DS-GRPO surpasses both vanilla GRPO and direct policy entropy maximization."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* Strong theoretical underpinning of the DS-GRPO method put forward.\n* Two theoretical analyses:\n    * Demonstrating the sharpening effect in RL\n    * Showing that DS-GRPO surpass vanilla GRPO and direct entropy maximization in terms of correctness and trajectory diversity/ \n* Very interesting analysis showing that entropy penalties benefit the MATH500 dataset while an entropy bonus helps with the Countdown dataset. There is a correlation between the uplift in performance using an entropy bonus with the solution diversity of correct answers. So problems with a high solution diversity will benefit from an entropy bonus and problems with low solution diversity will not."}, "weaknesses": {"value": "There are a few of weaknesses with the empirical presentation:\n* Figure 3 in the main text has pass@k curves which are cherry picked from Figure A2 in the appendix. A more thorough presentation of the results would be to plot the % uplift in the performance using DS-GRPO over GRPO and average over model families for each dataset. This would give a notion of uncertainty as well and if the effect of DS-GRPO is statistically significant versus vanilla GRPO.\n* In lines 387 to 390 the authors note that an entropy bonus improves performance over vanilla GRPO on one datasets but an entropy penalty then improves performance on a different dataset and that these trends are differing, however the difference between these two approaches is only very obvious on Math500 not on Countdown, Figure 4 (right). I think to properly draw this conclusion the authors should repeat the experiments with multiple seeds to make sure that this effect is not simply due to experimental noise, obviously I understand resource constraints though."}, "questions": {"value": "Minor: small question regarding the theoretical analysis in Section 6. The use of $\\sigma$ is usually used to define a standard deviation, but it is being used to define a variance with $\\sigma_{DS}$?\n\nHas this entropy collapse/sharpening effect been observed in vanilla RL prior to LMs? Or is this an LM specific phenomenon when using RL finetuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v2e77ramX6", "forum": "2RWf359T0p", "replyto": "2RWf359T0p", "signatures": ["ICLR.cc/2026/Conference/Submission10602/Reviewer_jhoN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10602/Reviewer_jhoN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865160479, "cdate": 1761865160479, "tmdate": 1762921868602, "mdate": 1762921868602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a formal framework for the sharpening effect observed after reinforcement learning (RL) on Large Language Models (LLM) that leads to increased correctness but also reduced output diversity. Based on that, the authors introduce a new method called \"differential smoothing\" which aims to improve both, correctness and diversity. Further the authors formally prove that differential smoothing improves diversity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Rigorous theoretical framework: The paper provides a formal and intuitive theoretical analysis of the sharpening effect \n- The method is tested on multiple reasoning benchmarks (MATH500, AIME24/25, OlympiadBench, AMC23, Countdown) and several model families (Qwen2.5, Qwen3, Ministral-8B)\n- The paper introduces an improved entropy-based method as comparison"}, "weaknesses": {"value": "- The paper would benefit from a comparison of DS-GRPO with more recent RL methods such as CISPO[1]. This would clarify how the proposed approach performs relative to the current state of the art.\n- The paper reports absolute diversity measures, but typically diversity collapse is assessed relative to the base model. Showing how diversity changes compared to the pre-training baseline (e.g. [1], Figure 12) would provide stronger evidence that DS-GRPO truly mitigates entropy collapse.\n- Since the proposed method introduces a new form of entropy control, it would be valuable to compare it directly with other entropy-based methods to better isolate the specific contribution of the “differential” mechanism.\n- All experiments are conducted on smaller models (<8B), which are known to behave differently under GRPO training. While this limitation is understandable given academic resource constraints, it would be interesting if the observed improvements hold for larger-scale models (e.g., 32B+), where RL dynamics may be different.\n\n1: MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention  \n2: A Sober Look at Progress in Language Model Reasoning"}, "questions": {"value": "On several benchmarks, the improvements of DS-GRPO over vanilla GRPO appear relatively small which raises the question if the suggested mechanism is actually the main reason for diversity collapse in RL fine-tuning. Maybe the authors could elaborate a bit more. Is the diversity after DS-GRPO training almost the same as for the base model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JROdKK8kVW", "forum": "2RWf359T0p", "replyto": "2RWf359T0p", "signatures": ["ICLR.cc/2026/Conference/Submission10602/Reviewer_17fN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10602/Reviewer_17fN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941455527, "cdate": 1761941455527, "tmdate": 1762921867992, "mdate": 1762921867992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "the paper addresses the reasoning sharpening problem in RL fine-tuning of LLMs. RL fine-tuning often lets models over-optimize and hence lose diversity. The paper proposes a differential smoothing mechanism (DS-GRPO algorithm) to counteract this issue. The papers shows that this change can help keep the expected reward unbiased while lowering variance and preventing entropy collapse."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The sharpening and lack of diversity is an important problem studied\n- The paper provides a theoretically justified method for addressing the sharpening issue of RL fine-tuning, and the solution is simple to adapt for existing RL algorithms\n- The paper provides solid empirical evidence across various reasoning benchmarks"}, "weaknesses": {"value": "I'm not an expert on the reasoning datasets, but it seems that the pass rate changes from the original GRPO and the proposed DS-GRPO are consistent but also modest."}, "questions": {"value": "Is the improvement additive to entropy regularization if we combine both, or are the effects redundant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QM0A5RREe9", "forum": "2RWf359T0p", "replyto": "2RWf359T0p", "signatures": ["ICLR.cc/2026/Conference/Submission10602/Reviewer_fC3U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10602/Reviewer_fC3U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946768320, "cdate": 1761946768320, "tmdate": 1762921867517, "mdate": 1762921867517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes existing methods for combating the loss of large language model (LLM) generation diversity after reinforcement learning (RL)-based post-training. The authors observe that previous methods which maximize generation entropy to improve diversity cause a drop in accuracy, while methods which do the opposite improve the accuracy of the model but harms response diversity. These effects are explained in the paper through theorems that show a \"sharpening effect\" inherent in traditional RL methods in which correct responses are both more likely to be reinforced, as well as generate greater model updates when they are sampled. The authors then propose their method, DS-GRPO, a modification to the existing GRPO framework which promotes diversity in correct responses while boosting the training signal for negative responses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a unifying theoretical framework for existing methods which add an entropy-modifying term to the RL objective\n2. The empirical results are strong, showing consistent improvement against previous methods\n3. Theorems clearly demonstrate the gap that previous methods did not address"}, "weaknesses": {"value": "1. In (Section 3.1, line 155 to 161), the paper presents a reward function that is the indicator function for sampled positive examples from $\\pi_\\text{base}$. It is unclear how applicable this reward function is in practice. Learned reward functions (e.g. from preference data as in RLHF [1]) do not match this framework. When using verifiable rewards as in GRPO [2], the verifier is run on the model outputs in each iteration rather than \"sampled from the base policy, $\\pi_\\text{base}$\" (Section 3.1, line 159)\n\nNitpicks:\n2. In (Abstract, line 21), the sentence \"...existing heuristics...\" should specify comparison to \"Vanilla RL and RL with direct entropy maximization\" (Section 6, line 439)\n3. In (Equation 1, line 150), I did not immediately recognize $\\pi^*_\\text{van}$ as \"the vanilla policy\". I recommend using the full word instead.\n\nReferences:\n[1] Ouyang, L. et al. (2022). Training language models to follow instructions with human feedback. https://arxiv.org/abs/2203.02155 \n[2] Shao, Z., Wang et al. (2024). DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. https://arxiv.org/abs/2402.03300"}, "questions": {"value": "1. How are $\\gamma_p$ and $\\gamma_n$ chosen in the experiments? Can they be optimized separately?\n2. How sensitive is DS-GRPO to changes in these hyperparameters?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AKfX9dWI5H", "forum": "2RWf359T0p", "replyto": "2RWf359T0p", "signatures": ["ICLR.cc/2026/Conference/Submission10602/Reviewer_KQde"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10602/Reviewer_KQde"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762559546465, "cdate": 1762559546465, "tmdate": 1762921867214, "mdate": 1762921867214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}