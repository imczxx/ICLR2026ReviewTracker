{"id": "DJ6AR99XFA", "number": 15162, "cdate": 1758248466185, "mdate": 1763602220605, "content": {"title": "Representational Alignment between Deep Neural Networks and Human Brain in Speech Processing under Audiovisual Noise", "abstract": "Speech recognition in the human brain is an incremental process that begins with acoustic processing and advances to linguistic processing. While recent studies have revealed that the hierarchy of deep neural networks (DNNs) correlates with the ascending auditory pathway, the exact nature of this DNN-brain alignment remains underexplored. In this study, we investigate how DNN representations align with the brain's acoustic-to-linguistic processing. Specifically, we employed neural encoding models to simulate neural responses to acoustic (i.e., speech and noise envelope) and linguistic features (i.e., word onset and surprisal). By applying representational similarity analysis (RSA), we quantified the similarity between these neural responses and the DNN embeddings generated by a pre-trained automatic speech recognition (ASR) model, both before and after fine-tuning on audiovisual noisy data. Our results demonstrate significant DNN-brain alignment:  embeddings from shallow layers exhibit higher similarity to neural responses associated with acoustic features, while those from deeper layers align more closely with neural responses related to linguistic features. Importantly, the fine-tuning process enhances this alignment by improving noise processing in shallower layers and refining linguistic representations in deeper layers. These results suggest that fine-tuned DNN models can naturally develop human-like processing patterns in noisy environments, highlighting a functional alignment between the human brain and DNNs in speech representation.", "tldr": "", "keywords": ["Automatic Speech Recognition", "EEG", "Information Representation"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/eb04c890c59b00a9bdb5629092e0f9b6b42c4678.pdf", "supplementary_material": "/attachment/0dd08ef6dec37e5c146de82bf1f2e93ee8ff77a7.zip"}, "replies": [{"content": {"summary": {"value": "This work is part of a broader research effort to explore the alignment between representations from deep neural network models (DNNs) and human brain activity during speech comprehension. Specifically, the authors examine the hierarchy of language processing from early auditory pathways to higher-level language cognition, in both a DNN (AVHuBERT model) and human brain recordings (EEG) while participants listen to speech under naturalistic audiovisual conditions. The evaluation focuses on building neural encoding by linearly mapping interpretable four features (speech-envelope, noise-envelope, word onset and word surprisal) to EEG on a per channel, per participant basis, and compute the representational dissimilarity matrices (RDMs) between actual and predicted neural responses. Also, the authors use RDMs to assess the similarity between DNN embeddings of hidden layers and neural responses for pretrained, and fine-tuned speech recognition models.\n\n**Contributions:**\n\n* *Introduction to novel stimulus condition setup:* This study presents a novel setup for audiovisual congruent and incongruent conditions while participants listen to speech.\n* *Introduction to new EEG dataset:* This study presents a new naturalistic EEG dataset in which participants listened to speech clips with real-world noise while simultaneously viewing the noise-related background videos.\n* *Comprehensive evaluation:* The study presents an extensive assessment of both pretrained and finetuned speech models and evaluates their hierarchy of information processing. Further, the fine-tuned models are created with an audio-only model using only audio inputs, and another for the audiovisual model using both audio and visual inputs. Using these settings, the authors measure the alignment between DNN embeddings and EEG responses across layers.\n\n**Technical summary:**\nThis is primarily an empirical study, and its methodology involves the following components:\n* *Data collection:* The authors collected a new EEG dataset, with participants simultaneously viewing noise-related videos during speech-in-noise.\n* *mTRF neural encoding model:* The four interpretable features (speech-envelope, noise-envelope, word onset and word surprisal) are used in an encoding model to predict EEG response per channel and per participant. The temporal alignment between speech stimuli and EEG responses involves delaying of signal (i.e., temporal lags from -200 ms to 800 ms), where the 48 secs speech stimulus is sampled at 25 Hz (1200 samples x  4 features x 26 [lags]) as input, while 1200 samples x 64 [EEG channels]) as the target. A simple regression that maps the stimulus representations to brain activity.\n\n**Experimental design/evaluation:**\nThe pretrained models are compared to their fine-tuned models under audio-only tuned and audiovisual tuned conditions:\n* *Representation similarity analysis:* This analysis evaluates the similarity between DNN embeddings across layers and EEG response under four conditions: (i) pretrained model, Audiovisual noise congruent, (i) pretrained model, Audiovisual noise incongruent, (i) fine-tuned model, Audiovisual noise congruent, (i) finetuned model, Audiovisual noise incongruent. This analysis provides a layer-wise alignment profile and tests how fine-tuning and audiovisual congruency modulate human-like information processing across layers (early auditory to higher-level language).\n* *Neural encoding performance:* neural encoding models are evaluated based on similarity between DNN embeddings and predicted neural responses for acoustic (speech- and noise-envelope) and linguistic features (word onset and word surprisal). Here, the predicted neural responses are obtained from neural encoding models. This analysis provides whether the predicted responses from the neural encoding model could preserve the representational geometry of the actual neural data.\n\n**Main findings:**\nAccording to the authors’ interpretation, the main findings are as follows:\n* Speech processing with audiovisual-congruent noise is significant for most of the EEG channels across layers in pretrained and fine-tuned models, whereas the audiovisual-incongruent condition is not.\n* Feature-based DNN-brain alignment revealed that acoustic features show higher similarity in shallow layers, while linguistic features show higher similarity in deeper layers.\n* For both pretrained and finetuned models, acoustic features show decreasing trend, while linguistic features show an increasing trend towards deep layers.\n* The Comparison between audio-only and audiovisual tuned models reveal that the increasing trend of noise envelope shifts to layers 1-4 for audio-only and 1-5 for audiovisual, while decreasing in deeper layers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I found this work to have the following strengths:\n* *Clarity:* The stimulus conditions involving speech clips with audiovisual congruent and incongruent setups are well explained, with a clear example. The EEG dataset collection, the DNN architectures, and their feature extraction process is easy to follow, with a clear separation between pretrained and fine-tuned models. Later, the neural encoding model using mTRF estimation, and the results across three subsections are clear.\n* *Originality:* The idea of using speech clips in audiovisual noise conditions while participants listen to speech, while simple, is quite novel conceptually. Since, humans process speech comprehension naturally under these natural conditions, this paper examines the DNN-brain alignment across these conditions in both pretrained and finetuned settings and provides a generalization of human-like hierarchy information processing in both systems.\n* *Significance:* This work is significant in that it contributes to a better understanding of the parallels between speech language models and neuroscientific findings from speech comprehension in the human brain. It helps to understand how the layers of speech models process information and how the representation changes across layers while tuned with audio-only and audiovisual information. Overall, the new EEG dataset offers more naturalistic conditions and maintains prior findings, such as acoustic information processing in shallow layers and linguistic information in deeper layers."}, "weaknesses": {"value": "From my perspective, the primary weaknesses of this study arise from the lack of literature works, selection of evaluation metrics, and findings from single DNN model:\n\n* *The selection of RSA as an evaluation metric is unclear*. Prior studies report that distributional similarity measures like RSA which compare the statistical properties of neural and model representations but may not capture fine-grained functional correspondences [Marques et al., 2021]. Therefore, authors should also consider metrics like PCC and R2 score as these are commonly used in neural encoding models. \n* *Limitation of model scope:* The most significant limitation is the considering of only one DNN model, which the authors acknowledge in the limitations section. However, this limitation blocks generalizability of the findings. For instance, the authors considered the AV-HuBERT model and fine-tuned it with audio-only and audiovisual settings. A strong empirical evaluation include additional models such as AST, Wav2Vec2.0 and Whisper: AST is audio spectrogram Transformer model; Wav2Vec2.0 is a self-supervised speech model; while Whisper is an encoder-decoder model in which encoder process speech while decoder performs automated speech recognition. Such comparisons would test whether these models also have similar findings as AV-HuBERT or are different in information processing of acoustic and linguistic features across layers. Further, prior studies [Miller et al. 2022] and [Vidya et al. 2022] reported that shallow layers process acoustic and phoneme-level information while deeper layers process more linguistic information. Also, authors should consider fine-tuned ASR models and evaluate their findings.\n* *Fixed regularisation parameter:* The results are reported using an optimal regularisation parameter λ=0.01. In general, for a voxelwise encoding model, we learn one regularisation parameter for each voxel. However, for the mTRF model, the authors use a single regularisation parameter across 64 channels. The authors did not discuss any reasons for this selection.\n* *Lack of related work:* One of the major limitations of the current study is the lack of related work where prior study explores the alignment between DNN models and brain alignment, considering the hierarchy of information processing. For eg: the introduction sentence of speech comprehension cites Hickock and Poeppel but missing popular speech comprehension work (Huth et al. 2016). Several other interpretability works, such as Oota et al. 2024, investigated the type of information present in language models that truly predict brain activity by controlling low-level features across layers of representation, and find that speech models prediction in auditory cortex is beyond low-level features while prediction in late language regions is largely due to low-level features, indicating lack of semantics. Also, Oota et. al. 2023 investigated pretrained and task-specific fine-tuned models to see how task-specific models show alignment across layers in language regions. I recommend authors to include following works:\n\n[Huth et al. 2016] Natural speech reveals the semantic maps that tile human cerebral cortex, Nature-2016\n\n[Oota et al. 2023] Speech Taskonomy: Which Speech Tasks are the most Predictive of fMRI Brain Activity? Interspeech-2023\n\n[Oota et al. 2024] Speech language models lack important brain-relevant semantics, ACL-2024\n\n[Wu et al. 2025] Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment\n\n[Rupp et al. 2025] A hierarchy of processing complexity and timescales for natural sounds in the human auditory cortex, PNAS-2025\n\nFor a complete and detailed account of both major and minor issues, please refer to the “Questions” section.\nI would like to thank the authors for the interesting naturalistic speech listening setup invested in this work. However, there are several points that I believe require further attention/work. I have divided these into major issues, which should be prioritized, and minor ones, which should be addressed for a strong version of current work."}, "questions": {"value": "**Major Comments**\n\n* *Impact of PCA on the RSA metric:* The authors perform PCA on stimulus features to reduce dimension from 768 to 205 and use the reduced dimension in measuring RSA between DNN embeddings and Neural Response. As I mentioned earlier in weaknesses, RSA focuses more on comparing statistical properties and may miss fine-grained functional correspondences; it remains unclear whether the findings remain similar while considering higher dimensions. I strongly encourage the authors to measure the similarity before applying PCA to verify whether the findings remain the same. In addition, since predictive metrics are common in neural encoding, I suggest authors perform similar analysis with either PCC or R2 metric, and please provide conclusions drawn from both RSA and either PCC or R2.\n* *DNN model validation is missing:* The authors empirically evaluate the pretrained, and fine-tuned conditions under audio-only and audio-visual conditions and observe the alignment patterns with the brain across layers. However, the authors should perform similar analysis on several audio, audio-visual datasets to verify whether there is a task performance effect across layers. This is an important evaluation to confirm what fine tuning really adds into the model.\n* *Clarity in Figure2:* Figure 2 reports only four conditions . However, the fine tuning was performed under two conditions (audio-only and audiovisual), there should be a total six topographic representational similarity maps. I recommend authors to include all six settings and clearly specify the fine-tuning settings for clarity.\n* *Neural encoding with stimulus features:* The neural encoding experiment in this paper considers four interpretable features. I strongly recommend authors to consider stimulus features in the neural encoding experiment, as this approach helps if the features contain dominant acoustic information, early layers should show higher similarity; if linguistic information is dominant, deeper layers should show higher similarity with the brain. If time permits, I suggest authors perform this experiment, and referencing them in the main manuscript. This is because prior speech-based brain encoding studies that all use full feature representations from DNN used in neural encoding models.\n\n**Minor Comments/Typos**\nWhile addressing the following points may not be critical to the paper’s core contributions, doing so would enhance the overall quality. \n* Line 48: I would recommend authors to clearly mention auditory cortex instead of primary cortex. Please follow uniformity as the primary cortex is used only here. Otherwise make it clear primary vs. non-primary cortex and use the terms throughout the paper.\n* Line 148: Consider relocating the entire Training Protocol section to Appendix G to conserve space. This section does not provide method-specific insights central to the main narrative and could be referenced as needed.\n* Line 52: Please clarify deeper layers are better predicted by phonetic features. Here, phonetic features are low-level phonemes or higher-level language features?\n* Line 248: Explain ε(t) the noise term in the equation on line 246.\nClarify in Figure captions 2 and 3 that the numbers reported above on each Topomap denotes layer’s RSA value.\n* Line 126: matrixs -> matrices\n* Line 138: resulting 32 speech-in-noise -> resulting in 32 speech-in-noise\n* Line 231:  1 to10 Hz ->  1 to 10 Hz\n* Clarify: Line 248 says time lags of -200 to 800 ms while Line 260 says 0 to 800ms. Please clarify why these ranges are different?\n* Figure 2C: Layer 12 RSA value is -0.74 while rest all are of three decimals. Please verify it once.\n* Line 103: “Speech audios” is not a correct term. If we say audio, it includes non-speech and speech. Please use either speech or audio, but not both. Correct at several other places in the paper.\n* Line 251: Pearson’s correlation -> Pearson correlation\n* Uniformity is missing: feed forward vs. feed-forward, audiovisual vs. audio-visual, pretrained vs. pre-trained\n* Line 423: LME is not defined or described anywhere in the main paper. Linear Mixed Effects only defined in Appendix.\n* Line 245: EEG responses was excluded -> EEG responses were excluded"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "E6vBnwy7CF", "forum": "DJ6AR99XFA", "replyto": "DJ6AR99XFA", "signatures": ["ICLR.cc/2026/Conference/Submission15162/Reviewer_Q6SF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15162/Reviewer_Q6SF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761139994954, "cdate": 1761139994954, "tmdate": 1762925474531, "mdate": 1762925474531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "d1xoupBq9m", "forum": "DJ6AR99XFA", "replyto": "DJ6AR99XFA", "signatures": ["ICLR.cc/2026/Conference/Submission15162/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15162/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763602219810, "cdate": 1763602219810, "tmdate": 1763602219810, "mdate": 1763602219810, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper study a novel audio-visual EEG dataset, composed of 32 audio-visual stimuli, with 16 congruent audio-visual noise and 16 incongruent audio-visual noise. On top of these audio-visual background could be heard speech, and the 20 subjects had to answer questions on the speech or visual contents.\nThe paper introduces a double-step approach to model brain responses.\nFirst, the response of each EEG electrode is predicted/denoised by fitting mTRF encoding model from basic auditory and linguistic features.\nSecond, the pattern of shared temporal response across stimuli of these predictions is compared to the pattern of shared temporal response across stimuli of the model (25Hz) for each electrode and model layers independently.\n\nThe main contribution of the paper is perhaps it's study of the representation of conflicting audio-visual informations, with a different encoding scores from DNN models between the congruent and incongruent case."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "One of the paper main strength is to study a multimodal scenario that is in fact very common in daily life: congruent or incongruent audio-visual noise. \nStudying how the brain reacts to these multi-modal inputs could help to shape our understanding of multi-modal integration in the brain.\nNotably, it could help reveal how the brain segregate and combine different stream of informations, which is probably done through task-dependent selecting mechanisms.\nThe paper consequently present an interesting experimental paradigms, which is then dissected by modeling the coding of linguistic features.\nOn the results, the paper hints to several remarkable and unexpected results.\n- First, audio fine-tuned model present a worse code of acoustic features than pretrained models. This question the efficacy of the audio-only model in preserving acoustic informations. In contrast the audio-visual models kept a good encoding of acoustic features while \n- The paper stress large difference in the encoding of \"basic features\": speech enveloppe, noise enveloppe, word onset and word surprisal in fine-tuned versus pretrained models.\n\nThe paper present overall a large amount of work including:\n- dataset recording\n- dataset preparation (use of forced alignment through MFA)\n-  exploration of novel multi-modal LLM for brain encoding.\nRigorous methods\n- careful statistical estimation of the robustness of the finding, although these findings interpretation could be disputed (see weaknesses)\n- cross-validations (although not fully nested)"}, "weaknesses": {"value": "Major remarks:\nDespite the previous two interesting results, the author center their claim on their simulation approach. Unfortunately this approach seems more advocated by a lack of ability to record large novel dataset rather than a true need for such methodological development.\nIf this approach has been designed to compensate the noisiness of EEG recordings, the author should state it explicitly and motivate their use of EEG compared to other recording techniques.\n\nMore precisely, this approach cast doubts on how informative the results are:\n- The author suggest that one of the main strength of their approach is to:\n\"enables us to generate simulated neural responses for new noise scenarios without the need to re-collect neural activity from the\nhuman brain. Therefore, our framework can be extended to evaluate DNN-brain alignment in more realistic and diverse noisy environments.\" \nI am not convinced that the author study true neural representations, as they mostly compare two model predictions.\nThe author present no evidences that studying simulated neural representations is a viable alternative to performing novel recording of brain data. To collect such evidences, the author should demonstrate that their approach is able to predict results obtained from the separate analysis of true neural recording, without these results being a direct consequence of using a model with a hierarchy of computations.\n\n- The encoding of linguistic features in speech-processing neural network has been studied in the past (for example, Pasad et al. 2022).\nSimilarly, the encoding of linguistic features in the brain has also been studied in the past (for example Li et al, Nature Neuroscience 2023),  although not necessarily in the audio-visual condition and multimodal models\nRelative to these published findings, the paper seems to deliver no novel insights gained from multi-modal models, but I could be partially wrong in this assessment.\n\n- Overall, because of their simulation procedure, the paper makes conclusions on model behavior which could be potentially unrelated to the recorded neural data. Furthermore, these conclusions are focused on a hierarchical integration of acoustic to linguistic features which have been reported previously. In turn it fails to make strong conclusions on audio-visual integration which has not been extensively studied.\nFor example their conclusions reads \"Our findings suggest that speech representations learned in transformer-based DNNs\nnaturally evolve toward the human-like processing patterns, providing credible evidence for the interpretability of DNNs in automatic speech recognition tasks.\", which is devote of the audio-visual paradigm, of main interest here. Hierarchical speech processing through the lenses of DNNs have been considerably explored, with cleaner recording apparatus (MEG, fMRI and intracranial recording).\n\nMinor remark:\n- The paper makes claim about the representational geometry, while it compares time-courses across stimuli. The neural code for linguistic inputs is usually defined to be a population pattern, with the population responses potentially evolving in time. \nIt is not clear why the author decided to study time-courses across stimuli, but this could be due to the difference in EEG recording versus intracranial, fMRI or MEG recording with which the present reviewer is more used to."}, "questions": {"value": "- It was not exactly clear how the RDMs were computed. My interpretation was that for each electrode the temporal signal of two stimuli were correlated, but for the neural-network it was unclear if this correlation was done for each unit of each layer as well.\nUsually, \"representations\" geometry is done across unit responses at the level of the whole brain or of a ROI. \nWould it be possible for the author to make this more explicit?\n\n- Would it be possible for the author to recenter their writing and explanations on the dataset being studied rather than the model representations?\n- What have they understood on audio-visual processing from this exploration?\n- How do their brain recording and model representations differ in this audio-visual processing? Notably, what do the author make of the lower RDM correlation in the incongruent vs congruent condition of figure 2?  \n\n- Why was it necessary for the author to use the simulation approach?\n- How can the author be certain that their simulation faithfully reproduces neural responses (no r or R2 scores reported!)\n\n- In their TRF regression, an optimal lambda hyperparameter seems to be used. Normally these regressions use a nested-cross validation approach, with the regularization parameter potentially varying across outer cross-validation folds. Why was it impossible to follow such nested-cross validation in this scenario?\n\nMinor question:\n- Why do the author resort to PCA of the model embeddings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W3ht8yHFmU", "forum": "DJ6AR99XFA", "replyto": "DJ6AR99XFA", "signatures": ["ICLR.cc/2026/Conference/Submission15162/Reviewer_9sfq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15162/Reviewer_9sfq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920940515, "cdate": 1761920940515, "tmdate": 1762925473991, "mdate": 1762925473991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper authros explores aligment between DNN representations and human brain activity during speech processing unver audiovisual noise. Using transformer based ASR model (AV-HuBERT) and EEG recordings, the authors correlate intermediate DNN resperesentation with neural encoding responses associated with both acoustic (speech + noise envelopes) and linguistic (word onsets, word suprisal) speech features. They also investigates how the DNN-brain aligments changes before and afer fine-tuning the DNN on noisey, multimodal data.  The observe that shallow layes of the DNN align with acustic brain responses, while deep layers correspnd to lingustic responses. Fine-tuning further modulates these aligmnets, especially with multi-modal input."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(1) The work is well motivated to better understand representational parallels betwwen artifical and biological systems.\n\n(2) The combination of neural encoding models (for generating feature-specific simulated EEG responses) and representational similarity analysis (RSA) enables the study to probe alignment across specific levels of processing in both the DNN and the brain.\n\n(3) Comprehensive empirical evaluation is provided, including both pre-trained and fine-tuned DNNs and comparisons between audio-only and audio-visual models, strengthening the reliability of the main claims."}, "weaknesses": {"value": "(1) No side-by-side comparison is provided between other competitive ASR architectures or alternative transformer variants.\n\n(2) While these can be standards , but i think since only two acoustic (speech envelope, noise envelope) and two linguistic (word onset, word surprisal) features are analyzed there is a risk the results reflect the peculiarities of these specific features, rather than fundamental DNN-brain mapping\n \n(3) The claim that fine-tuning naturally evolves DNNs toward human-like processing patterns may be somewhat overstated. For example, reductions in acoustic-feature alignment post-fine-tuning (Section 3.4, Fig. 4B, C) can also indicate loss of generalized auditory processing in favor of task-specific cues, which is not clearly discussed or  examined in the experiments.\n\n(4) There can be potential overfitting in simulated EEG. Regularization and strategies for avoiding overfitting are not fully detailed. The more detailed ablation, or alternative validation (eg.  leave-one-subject-out) ) could increase the condidence in score.\n\n(5) There is limited documentation on a few critical technical choices, such as the exact PCA implementation (number of components, variance explained per-layer, potential for information leakage), and whether equal-length segments are enforced after alignment/truncation. This is particularly relevant because EEG and DNN embeddings might not have matched temporal resolutions or coverage post-processing (see Section 2.5 and related)."}, "questions": {"value": "(1) Is there any justucatuin for the choice of acoustic and linguistic features ? I wonder if features (such as phonemes, syllables, or syntactic even annotaitons) can yield substantially different aligment maps ?\n\n(2) How robust is the EEG–DNN representational alignment when applying alternative dimensionality reduction schemes beyond PCA, or when matching time windows in different ways?\n\n(3) The fine-tuning process appears to decrease representational similarity for some acoustic features. To what extent does this tradeoff reflect task optimization, and how do the authors reconcile this with the claim of \"natural\" emergence of brain-like processing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "d1lYAI2C1S", "forum": "DJ6AR99XFA", "replyto": "DJ6AR99XFA", "signatures": ["ICLR.cc/2026/Conference/Submission15162/Reviewer_LRek"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15162/Reviewer_LRek"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923497232, "cdate": 1761923497232, "tmdate": 1762925473536, "mdate": 1762925473536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates representational alignment between deep neural networks (DNNs) and human brain activity during speech processing under audiovisual noise. The authors employ neural encoding models (mTRF) to simulate brain responses to acoustic features (speech/noise envelope) and linguistic features (word onset/surprisal), then apply representational similarity analysis (RSA) to compare these simulated responses with embeddings from AV-HuBERT (a transformer-based ASR model). Using EEG data from 20 participants listening to speech with audiovisual noise, they demonstrate that shallow DNN layers align with acoustic processing while deeper layers align with linguistic processing. Importantly, fine-tuning on audiovisual noisy data enhances this alignment by improving noise representation in shallow layers and refining linguistic representations in deep layers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Its primary strength lies in its methodology, which combines neural encoding models (mTRF) with Representational Similarity Analysis (RSA) to enable a precise, feature-level analysis beyond simple comparisons of raw signals to embeddings. The study is particularly well-designed due to its systematic distinction between acoustic features (envelopes) and linguistic features (word onset/surprisal), targeting established neural correlates of sensory (N1-P2) and higher-level (N400-like) processing, respectively. The research utilizes robust experimental controls, including comparisons of pre-trained versus fine-tuned models, audio-only versus audio-visual fine-tuning, and congruent versus incongruent conditions across 20 participants. Empirically, the paper convincingly demonstrates a DNN representation align with brain’s acoustic-to-linguistic processing with shallow layers (1-6) showing high similarity to acoustic features that decline with depth, while deep layers (7-11) exhibit increasing similarity to linguistic features. Finally, the modulation analysis provides a genuine insight into adaptive computational strategies by showing that audio-visual fine-tuning shifts the acoustic-linguistic transition layer and enhances noise processing, suggesting a specific mechanism for multimodal integration in noisy environments."}, "weaknesses": {"value": "- Significant gaps in Related Works literature review\n    - While the first contribution regarding methodological contribution could mislead that combination of using **neural encoding models and Representational Similarity Analysis (RSA)** is completely new, (Tuckute et al., 2023) have combined neural encoding models and RSA. While authors have cited (Tuckute et al., 2023) it wasn’t cited in a context of methodology.\n    - Hierarchical correspondence between acoustic and semantic processing has been previously demonstrated in both high-temporal-resolution data, such as the findings reported by (Goldstein et al., 2025) using ECoG, and within the fMRI domain through studies of hierarchical auditory processing.\n        - (Goldstein et al., 2025) develops **encoding models** to map low-level **acoustic features**, mid-level speech features, and contextual word embeddings onto brain activity during real-life conversation, demonstrating a unified framework for these feature types.\n        - (Tezcan et al., 2023), which authors also cited, uses **encoding models** (specifically Temporal Response Functions, or TRFs, a type of encoding model) to investigate how **acoustic features** and **phoneme-level linguistic features** contribute to brain responses during speech comprehension.\n        - While different modality(fMRI), (Heer et al., 2017, https://www.jneurosci.org/content/37/27/6539) showed **hierarchical processing** of **acoustic (spectral) & linguistic (articulation) features** through **voxelwise encoding models** on **natural speech comprehension** in **fMRI**.\n- Justification for Methodology is Needed\n    - This paper aims to examine the similarity between how humans adapt to noise in noisy environments and the process in encoding models. However, there's insufficient explanation as to whether the encoding model + RSA approach is absolutely necessary to examine this. Wouldn’t the traditional encoding model approach be sufficient?\n- Clarifying Augmentation vs. Adaptive Alignment;\n    - Clarification is needed on whether adding noise is just from a model augmentation perspective, or model actually became better at handling noise. It seems necessary to check whether similar patterns of results appear after audio augmentation with other methods. Audio flip, for example, that humans don’t naturally perceive, would be a good comparison to validate if the model actually adapted to align with human brain representation of recognition on noisy environment.\n- Scope of Linguistic Features :\n    - The expression 'linguistic feature' might be too risky. Linguistic feature can include far more features other than word onset, word surprisal; such as phonemes, syntactic structure.  To convincingly demonstrate that the deep layers of a speech model like AV-HuBERT capture broad linguistic features, it would be beneficial to include a comparative RDM analysis using established, high-dimensional **pretrained language model (LLM) embeddings** (e.g., from GPT or BERT). This would confirm whether the observed alignment truly extends to abstract, multimodal linguistic knowledge."}, "questions": {"value": "- The authors should strengthen introduction to include more relevant papers and clearly denote what has been done and what novelties are added. The idea of mimicking the process by which humans adapt to noise in noisy situations by giving noise to the model and finetuning it to better predict human brain activity seems like an interesting idea, but it doesn't seem to be well highlighted.\n- The importance of using RSA in combination with encoding model doesn’t seem to be clearly demonstrated. The authors should explain what limitations arise when using a general encoding model alone (i.e., merely predicting brain response from DNN intermediate representations after exposure to AV noise stimuli).\n- Fine-tuning hyperparameters (learning rate, batch size, number of epochs) are not provided. It could be provided in appendix.\n- Regarding experimental details, it doesn't seem to be clearly written whether what humans heard and the noise given to the model are the same. The authors should clarify this point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UitDdj378u", "forum": "DJ6AR99XFA", "replyto": "DJ6AR99XFA", "signatures": ["ICLR.cc/2026/Conference/Submission15162/Reviewer_H2zJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15162/Reviewer_H2zJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982449894, "cdate": 1761982449894, "tmdate": 1762925473150, "mdate": 1762925473150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}