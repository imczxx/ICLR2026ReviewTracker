{"id": "2WZHjN43gB", "number": 9470, "cdate": 1758123570818, "mdate": 1759897719285, "content": {"title": "Intra-Request Branch Orchestration for Efficient LLM Reasoning", "abstract": "LLMs increasingly rely on inference-time reasoning algorithms such as chain-of-thought and multi-branch reasoning to improve accuracy on complex tasks. These methods, however, significantly increase token usage (cost) and per-request latency.\nPrior work has primarily focused on reducing token usage, often at the expense of accuracy, while overlooking other latency factors.\n\nWe present DUCHESS, an LLM serving system that reduces computational cost and latency without sacrificing accuracy through intra-request branch orchestration guided by predictions.\nWithin each request, DUCHESS predicts branch correctness with\na lightweight linear probing model over LLM layer activations. The orchestration policy uses these predictions to decide whether to terminate a branch early, duplicate an existing branch, or continue exploring a branch.\nWhen handling multiple requests, DUCHESS can further reduce latency by prioritizing easier reasoning tasks, when request complexity can be estimated from the prompt.\n\nExperiments on three reasoning benchmarks show that DUCHESS consistently improves the token‚Äìaccuracy Pareto frontier, reducing token usage by 42‚Äì63\\% at matched accuracy compared to self-consistency. For request serving with vLLM, \\name reduces mean, median, and tail latencies by 57-81\\%, 58-85\\%, and 52-84\\% with First-Come-First-Served (FCFS) scheduling across three datasets, compared to self-consistency. At higher request rates, scheduling jobs by increasing predicted difficulty reduces latency further over FCFS.", "tldr": "We reduce the computational cost and latency of reasoning through intra-request orchestration of reasoning branches.", "keywords": ["Efficient LLM Reasoning", "Systems for LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e291ebefa99792390216728fb013519629ae9c5d.pdf", "supplementary_material": "/attachment/ad1f128a8b523ad1fc1d45de2ef58a21fbf5ddd3.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces an innovative system called DUCHESS (Intra-request branch orchestration for efficient LLM reasoning). It proposes to make Large Language Models (LLMs) faster and cheaper to run, especially when they are solving complex problems using multi-branch reasoning methods (like Self-Consistency or Chain-of-Thought). While these reasoning algorithms help LLMs achieve higher accuracy on difficult tasks, they typically increase the number of output tokens generated, leading to higher computational costs and longer waiting times (latency) for users. The proposed system DUCHESS tackles this challenge by intelligently managing these reasoning paths, or \"branches,\" to maintain accuracy while drastically reducing cost and latency.\nThe work is primarily based on the following two mechanisms:\n1. Intelligent Intra-Request Branch Orchestration (Stopping, Forking, Continuing).\n2. Optional Inter-Request Complexity-Aware Scheduling.\n\n*Trying to make the review bullet points for the clarity and easy to respond."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper provides strong evidence that the fundamental approach of DUCHESS is sound and highly effective, leading to significant, quantifiable improvements over existing baselines:\n1. Dominance on the Cost‚ÄìAccuracy Pareto Frontier.\n2. Efficacy of Lightweight Prediction and Minimal Overhead.\n3. Substantial Latency and Straggler Reduction.\n4. Optional Complexity-Aware Scheduling.\n5. Novel Intra-Request Branch Orchestration Policy."}, "weaknesses": {"value": "The paper is transparent in listing several limitations, particularly regarding the aggregation of results and the scope of implementation:\n1. Dependencies in Answer Aggregation.\n2. Limited Scope of Complexity-Aware Scheduling.\n3. Predictor Layer Constraints.\n4. Low Complexity Predictor Accuracy.\n5. Hyperparameter Tuning Range.\n6. Scheduling Overhead."}, "questions": {"value": "A couple of questions here that the paper clearly covers as limitations-\n1. About the Enhanced Answer Aggregation and Termination Strategy: I would wish for request termination strategies that explicitly account for dependencies among collected answers.\n2. Broader Applicability and Optimization of Inter-Request Scheduling: \n a. Extension of complexity-aware scheduling to datasets that lack inherent difficulty labels. \n b. Improvement in the accuracy of the request complexity predictor.\n c. Profiling of alternative LLM layers.\n d. Mitigation of queueing delays caused by prefilling.\n3. More Extensive Predictor Tuning and Exploration."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UisVZZjmgQ", "forum": "2WZHjN43gB", "replyto": "2WZHjN43gB", "signatures": ["ICLR.cc/2026/Conference/Submission9470/Reviewer_XiJK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9470/Reviewer_XiJK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920373729, "cdate": 1761920373729, "tmdate": 1762921059074, "mdate": 1762921059074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DUCHESS, an LLM serving system that aims to reduce computational cost and latency in multi-branch reasoning setups. The system incorporates a linear probing model that leverages the transformer model's activations to predict correctness of a reasoning branch. These predictions are utilized by the system to perform branch level operations including 1) early stopping, 2) additional branching, and 3) continued generation. On datasets like GSM8K, MATH, and MMLU, the work demonstrates token length reductions of 42%-63%, and 57%-81% reduction in mean latency in concurrent request serving scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The core intuition and methodology are well-grounded in observations from prior research work about:\n    - Informativeness of the middle layers of transformer models.\n    - Linear probing techniques for extracting signals from prior layers.\n    - However, concerns exist regarding hyperparameter selection and generalizability (especially with regards to model architecture families).\n- Overall, the paper is well structured with informative diagrams/tables, along with system workflow explanation.The writing is mostly succinct with minimal filler.\n- The work showcases results with deployment considerations in mind. Specifically, the throughput numbers (queries per minute) in figure 4, showcase promising results for production environments. \n- Additional component overhead (branch correctness prediction/answer extraction), in terms of latency, provide a confident argument in favor of the added complexity."}, "weaknesses": {"value": "- The connection between the two main contributions, i.e., 1) intra-request orchestration and 2) inter-request scheduling, feels disconnected. \n- Explanations about the choice of hyperparameter selection is incomplete at times. For example: It is mentioned in line 211 that probing for branch correctness is performed at intervals of $i=16$ tokens, while in line 215, it is mentioned that a suitable range is $16 \\leq i \\leq 80$. \n- The originality seems somewhat incremental: \n    - The core idea of early termination based on hidden states has been explored (e.g., Zhang et al., 2025; Afzal et al., 2025). \n    - The probing technique, although claimed to be sufficiently advanced compared to prior works (see lines 127-135), does not seem entirely novel. \n    - The complexity-aware scheduling component feels like an add-on rather than a core contribution, and is only evaluated on one dataset (see figure 5).\n- The experiments are performed with just one model, i.e., DeepSeek-R1-Distill-Llama-8B. It is unclear if the methods proposed generalize across different models and if the technique applies to non-reasoning models \n- In lines 204-207, it is said that, based on the validation performance of the model on GSM8K, the layer 14 was chosen for correctness prediction. Selection of the prediction layer based on empirical results on a single dataset/model combination raises concerns about applicability in real-world deployment scenarios. \n- It is unclear whether the methods generalize to tasks involving longer reasoning traces (GPQA-Diamond, for example), as opposed to the presently evaluated datasets (GSM8K, MATH, MMLU), which require relatively lower generation lengths.\n- Hyperparameter choices such as the probing interval ($i=16$), or the branch termination threshold($\\tau$) would benefit from further analysis and/or advice about selection criteria in deployment scenarios where single-dataset validation data might not suffice.\n- In lines 148-150, it is mentioned that the termination of entire request, once a \"sufficient\" number of answers is collected, prevents stragglers from delaying completion. The claim would benefit from further explanation of the so-called \"straggler\" branches and how the method prevents them. The main concern here is whether the remaining long-running branches pre-empted by the method, are actually wasteful in practice, or they meaningfully contribute towards alternate reasoning path expansion.\n- The work motivates the proposed method from the perspective of improving multi-branch reasoning. With this regard, comparison with existing techniques which already attempt to tackle this problem, such as ESC, RASC, Adaptive Consistency would better support the claims.\n\nThe work's significance is primarily in its engineering integration and empirical validation rather than algorithmic breakthrough."}, "questions": {"value": "- Q1. Regarding linear probing models for branch correctness prediction:\n(Refer Appendix A.1, lines 710-711) Why are the hidden layer sizes of the MLP different for MMLU vs. GSM8K/MATH datasets?\n- Q2. Regarding Experimental Settings:Why is the max token length per branch is capped at 4,096 (see lines 304-305) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kegnEAtPI7", "forum": "2WZHjN43gB", "replyto": "2WZHjN43gB", "signatures": ["ICLR.cc/2026/Conference/Submission9470/Reviewer_GqzD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9470/Reviewer_GqzD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976402214, "cdate": 1761976402214, "tmdate": 1762921058640, "mdate": 1762921058640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DUCHESS, a serving‚Äëtime controller for multi‚Äëbranch chain‚Äëof‚Äëthought (CoT) inference. A lightweight MLP probe on intermediate activations predicts per‚Äëbranch correctness, enabling three actions at decode time: early‚Äëterminate, selectively fork promising branches (reusing KV cache), or continue. A request terminates when a consensus criterion is met. In multi‚Äërequest settings, an optional scheduler prioritizes prompts predicted to be easier. Probes are trained using periodic CoT probes (interval i) with mid‚Äëlayers performing best; key results report 42‚Äì63% token reductions at iso‚Äëaccuracy and large mean/tail latency drops under vLLM, further improved by difficulty‚Äëaware scheduling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Beyond prior early-stopping work in reasoning (e.g., DeepConf), this paper treats early stop as an end-to-end serving problem with a hardware/system-aware design: a lightweight MLP activation probe that runs co-resident with the model to avoid activation transfer, KV-cache-aware selective branching, and request-level consensus with optional SJF-style difficulty scheduling. This coupling yields consistent iso-accuracy token savings (‚âà42‚Äì63%) and sizable wall-clock gains‚Äîincluding mean/P95 latency and TTFT‚Äîon vLLM under load.\n\nConsistent token and latency savings at matched accuracy; helpful layer‚Äëwise probe analysis (mid‚Äëlayer peak)."}, "weaknesses": {"value": "- Probe seems trained per dataset/model; multi-task/domain robustness is not stress‚Äëtested.\n- Comparisons to incremental/entropy‚Äëaware branching (e.g., ESC/RASC) and similarity‚Äëpruning (Slim‚ÄëSC) would better calibrate gains.\n- The paper reports per‚Äëcomponent costs, but explicit wall‚Äëclock accounting under large batches/multi‚ÄëGPU and periodic probing vs. the achieved savings would help.\n- Difficulty‚Äëaware prioritization may starve \"hard\" requests at load; fairness/quality safeguards are not fully analyzed."}, "questions": {"value": "- Sensitivity to ùëñ (probing interval), correctness threshold ùúè, and the consecutive‚Äëround parameter S (for early termination). Any auto‚Äëtuning procedure?\n- Does a probe trained on one backbone/size transfer to others, or is per‚Äëmodel retraining required?\n- Ablate early termination vs. branch‚Äëlevel pruning: what fraction of latency/tokens each contributes?\n- Under difficulty‚Äëaware scheduling, how are starvation or fairness handled when many \"hard\" jobs arrive?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "avtNr9w1Gi", "forum": "2WZHjN43gB", "replyto": "2WZHjN43gB", "signatures": ["ICLR.cc/2026/Conference/Submission9470/Reviewer_cMbD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9470/Reviewer_cMbD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993286207, "cdate": 1761993286207, "tmdate": 1762921058359, "mdate": 1762921058359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors study the problem of LLM serving system. In particular, they propose an LLM serving system which they call DUCHESS that orchestrates multi-branch reasoning within a single request using predictions from a lightweight probe over LLM activations. DUCHESS takes one of three branch-level actions: \n\n- (1) Early termination with CoT probing to elicit a short final answer\n- (2) Selective branch-out (forking promising branches while reusing KV cache)\n- (3) Continuation (keep generating tokens for branches that are still uncertain, giving them additional steps to potentially reach a correct answer)\n\nBeyond intra-request control, the system optionally adds complexity-aware scheduling across a request pool, which using prefill activations of the first decoded token to predict difficulty, the scheduler serves easier prompts first to reduce mean latency.\n\nExperiments on GSM8K, MMLU, and MATH with DeepSeek-R1-Distill-Llama-8B and vLLM, show effectiveness under both cost‚Äìaccuracy trade-offs (tokens vs. accuracy) and serving latency.\n\nAdmittedly, the area of this research is not my primary research area. I apologise that I can only share some general comments on this paper."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "``S1``: The empirical gains on both cost‚Äìaccuracy and serving latency look good, with clear and reproducible setups. Particularly, \n\n- ``reducing token usages by 42%-63% at matched\naccuracy compared to Self-consistency``, \n- ``reducing mean, P50, and\nP95 latencies by 57-81%, 58-85%, and 52-84% on three datasets``, and \n- ``reducing mean latency by up to 29.7% and 34.7%\ncompared to FCFS``.\n\n``S2``: Selective branch-out with KV reuse is a practical systems optimisation that preserves parallelism and reduces stragglers."}, "weaknesses": {"value": "I apologise that I‚Äôm not very familiar with LLM serving systems. I will carefully refer to the comments of other fellow reviewers, who are more confident than me, for my final judgment. Some general weaknesses that I can notice are as follows.\n\n``W1``: The authors currently focus on math plus general knowledge (GSM8K, MMLU, MATH). A broader commonsense and graduate-level domain generalization is in fact, not tested. The inclusion of SuperGPQA and CommonsenseQA would help strengthen the contributions of the proposed LLM serving system DUCHESS.\n\n``W2``: Request termination and voting don't actually account for dependencies introduced by branch duplication. I have noticed that the authors mention this as a limitation at the end of the paper. This opens the door to weighted voting or de-duplication."}, "questions": {"value": "Apart from those aforementioned in the weaknesses section:\n\n``Q1``: I wonder would selective branch-out reduce reasoning diversity?\n\n``Q2``: The authors report 16.4% and 7.8% mistaken early termination on MMLU/MATH. It would be great if the authors could elaborate the possibility of a detecting and recovering mechanism based on this finding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "TBwUS1bDbO", "forum": "2WZHjN43gB", "replyto": "2WZHjN43gB", "signatures": ["ICLR.cc/2026/Conference/Submission9470/Reviewer_kRNj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9470/Reviewer_kRNj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071572714, "cdate": 1762071572714, "tmdate": 1762921058000, "mdate": 1762921058000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}