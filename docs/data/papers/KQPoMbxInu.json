{"id": "KQPoMbxInu", "number": 21998, "cdate": 1758324619872, "mdate": 1763688493992, "content": {"title": "Point-Focused Attention Meets Context-Scan State Space: Robust Biological Visual Perception for Point Cloud Representation", "abstract": "Synergistically capturing intricate local structures and global contextual dependencies has become a critical challenge in point cloud representation learning. To address this, we introduce PointLearner, a point cloud representation learning network that closely aligns with biological vision which employs an active, foveation-inspired processing strategy, thus enabling local geometric modeling and long-range dependency interactions simultaneously. Specifically, we first design a point-focused attention, which simulates foveal vision at the visual focus through a competitive normalized attention mechanism between local neighbors and spatially downsampled features. The spatially downsampled features are extracted by a pooling method based on learnable inducing points, which can flexibly adapt to the non-uniform distribution of point clouds as the number of inducing points is controlled and they interact directly with point clouds. Second, we propose a context-scan state space that mimics eye's saccade inference, which infers the overall semantic structure and spatial content in the scene through  a scan path guided by the Hilbert curve for the bidirectional S6. With this focus-then-context biomimetic design, PointLearner demonstrates remarkable robustness and achieves state-of-the-art performance across multiple point cloud tasks.", "tldr": "A point cloud representation learning network inspired by biological vision mechanisms.", "keywords": ["Point cloud learning", "Attention mechanism", "State space model", "Biomimetic vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/efcd08a2c26a1e58a69230cdca2d8c4edc0003af.pdf", "supplementary_material": "/attachment/f8b4010d8ada154030aa9bb4f30718d6a7ef2cdd.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes PointLearner, a biomimetic point cloud representation learning network that aims to simulate the \"focal-context\" perception mechanism of biological visual systems to collaboratively capture both the local fine structure and global context dependencies of point clouds. Specifically, Point-focused attention models local fine details and global semantic dependencies through a local neighbor branch and a spatial downsampling branch. Context-scan state space uses a Hilbert curve to serialize the point cloud and integrates a bidirectional S6 model to dynamically scan and integrate the global semantic structure."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The dual-path design of local neighbor branch and spatial downsampling branch effectively balances the contribution of local structure and global semantics.\n2. To address the non-uniform distribution of point clouds, the paper introduces a learnable inducing points pooling method. By enabling attention interaction between inducing points and the original point cloud, this method flexibly extracts global semantics and shows more stable performance at higher sampling rates compared to the farthest point sampling method.\n3. The model demonstrates promising performance improvements over existing methods on four mainstream point cloud datasets (ModelNet40, ShapeNet, S3DIS, ScanObjectNN)."}, "weaknesses": {"value": "1. The paper does not provide a comparison of the computational cost of the proposed model with previous methods. Each encoder block not only incorporates a two-branch architecture but also adds a state space model for global feature modeling. This may lead to a significant increase in computational overhead, which requires further clarification.\n2. The spatial downsampling branch of the Point-focused attention is capable of capturing global features, so the motivation for introducing the state space model for additional global feature modeling is not well justified. While the authors mention simulating eye jump inference, this design still seems redundant and lacks sufficient novelty, as it closely resembles the design of PointMamba.\n3. The details of model training are not sufficiently clear. It is recommended to include a more detailed description of the model architecture across different datasets in the form of tables to help readers better understand the model’s structure and implementation.\n4. The experimental comparison is flawed, as it does not distinguish self-supervised models, which could lead to unfair comparisons. For example, pre-trained models on ShapeNet, such as PointGST, may not directly apply to segmentation tasks in indoor scenes. To make the comparison fairer on the S3DIS dataset, it would be more appropriate to include self-supervised methods pre-trained on indoor scene data, such as Sonata."}, "questions": {"value": "1. The serialization strategy in this paper converts 3D data into a 1D sequence, which may disrupt the adjacency relationships between points. Mamba3D argues that serialization is unnecessary, and StructMamba3D also forgoes serialization in favor of spatial state modeling to preserve adjacency relationships. Why does this paper still choose to adopt the Hilbert curve serialization strategy? Are there sufficient experimental results to demonstrate its advantage in terms of model performance?\n2. Can the proposed method be applied in the context of self-supervised learning? Leveraging large amounts of unlabeled data to enhance feature modeling capabilities is an important direction. Increasing the dataset size can also help transformer models with large receptive fields to achieve good local or structural modeling. Investigating the impact of self-supervised pre-training on model performance and validating the model’s scalability would further enhance its applicability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B1r7kvCmJ1", "forum": "KQPoMbxInu", "replyto": "KQPoMbxInu", "signatures": ["ICLR.cc/2026/Conference/Submission21998/Reviewer_Qkeu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21998/Reviewer_Qkeu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484205701, "cdate": 1761484205701, "tmdate": 1762942012827, "mdate": 1762942012827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PointLearner, a point cloud representation learning network that is inspired by biological vision mechanisms.\nThe method introduces two main components: (1) a point-focused attention module that mimics foveal vision by combining fine-grained local neighbor attention with coarse-grained global semantics from a novel induced point pooling strategy, and (2) a context-scan state space module that serializes point clouds using a Hilbert curve and applies bidirectional S6 to simulate saccadic eye movements for global scene inference. \nThe authors demonstrate state-of-the-art performance on ModelNet40, ShapeNet, S3DIS, and ScanObjectNN datasets, with particular robustness to strong noise and varying point densities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The analogy to foveal vision and saccadic eye movements is well-motivated, intuitively aligning with the challenges of point cloud learning, including local detail preservation and global context integration. \n\n2. The work effectively combines the strengths of attention (local geometry sensitivity) and state space models (long-range dependency modeling) within a biologically plausible framework, demonstrating the potential of such hybrid paradigms."}, "weaknesses": {"value": "1. The biological analogy is intuitive but somewhat heuristic. \nA more rigorous theoretical or empirical analysis of how closely the modules mimic biological vision would strengthen the motivation and be helpful for understanding.\n\n2. Visualizing heat maps could be useful for better understanding the attention responses and the advantage of the proposed method."}, "questions": {"value": "What are the specific advantages of using the Hilbert curve over the Morton curve or learned serialization strategies in terms of spatial locality preservation, continuity, and computational efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iN0e1Je14m", "forum": "KQPoMbxInu", "replyto": "KQPoMbxInu", "signatures": ["ICLR.cc/2026/Conference/Submission21998/Reviewer_i3Jg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21998/Reviewer_i3Jg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652560970, "cdate": 1761652560970, "tmdate": 1762942012615, "mdate": 1762942012615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PointLearner, a biologically inspired point cloud learning framework that integrates local geometric modeling and global contextual reasoning. The key idea is to mimic human visual perception, which combines a point-focused attention mechanism  with a context-scan state space. Experiments on several benchmarks show state-of-the-art performance and strong robustness to noise and varying point densities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. A novel biomimetic perspective. The motivation of this paper is very interesting, which is derived from biological visual analogies, effectively balancing local attention and the understanding of global features.\n\n2. Robust architectural design. The proposed point-focused attention module and state space module are well integrated and can effectively handle noise and sparse sampling, demonstrating good generalization ability.\n\n3. Clear structure and writing. This paper is logically clear and easy to follow."}, "weaknesses": {"value": "Lack of domain-specific motivation. Although the biological vision analogy is creative, the paper does not clearly explain why this mechanism is particularly suitable for point clouds. The proposed “foveation + saccade” process seems domain-agnostic and could also be applied to images or videos. Without a clear justification, the choice of point cloud data feels somewhat arbitrary."}, "questions": {"value": "Why did the authors choose to apply this biomimetic architecture specifically to point clouds rather than to 2D images or videos, where visual perception analogies might seem even more natural?\n\nHave the authors considered testing or conceptually discussing how this framework would perform on other vision data (e.g., images or video) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N4u4zced4m", "forum": "KQPoMbxInu", "replyto": "KQPoMbxInu", "signatures": ["ICLR.cc/2026/Conference/Submission21998/Reviewer_5iH6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21998/Reviewer_5iH6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901988570, "cdate": 1761901988570, "tmdate": 1762942012366, "mdate": 1762942012366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PointLearner, a point cloud representation learning network that closely aligns with biological vision by employing an active, foveation-inspired processing strategy, thus enabling simultaneous local geometric modeling and long-range dependency interactions."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The motivation of explaining the network design from a biological vision perspective is interesting;\n2.The argument in the Introduction regarding the limitations of local attention is correct;\n3.The goal of \"synergistically capturing local fine-grained structures and global contextual dependencies\" is valid;\n4.The experimental results demonstrate the effectiveness of the proposed network architecture;\n5.The paper is well-written and easy to understand."}, "weaknesses": {"value": "1.Although the motivation and starting point of the paper are sound, the core ideas of the proposed solution lack novelty;\n2.The Fine-grained Perception module, which updates the center token using k neighboring tokens, is very common apart from the novel use of attention;\n3.The Coarse-grained Awareness module is essentially a standard approach that uses learnable queries with cross-attention to aggregate features;\n4.The Competitive Normalized Fusion is a commonly adopted strategy when a network has multiple parallel branches;\n5.Most of the datasets used are pure point cloud data. To better align with the biological vision motivation, it is recommended to include more datasets with RGB information."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FrjnNw5zE3", "forum": "KQPoMbxInu", "replyto": "KQPoMbxInu", "signatures": ["ICLR.cc/2026/Conference/Submission21998/Reviewer_9P6s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21998/Reviewer_9P6s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953147748, "cdate": 1761953147748, "tmdate": 1762942011999, "mdate": 1762942011999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "A summary of the major changes made in the revised manuscript"}, "comment": {"value": "First of all, we would like to thank the Program Chairs, Senior Area Chairs, Area Chairs, and all the anonymous reviewers for their precious time and thoughtful review of this manuscript. The raised comments and suggestions are extremely valuable and constructive, which are very helpful for improving the quality of the manuscript. In this revision, we have considered and addressed all the comments with our utmost carefulness. In responding to the reviewers’ invaluable comments, significant changes have been made to the original manuscript.\n\n**A summary of the major changes made in the revised manuscript is given in the following.**\n\n- We have added attention heatmaps for PointLearner in Appendix E.1 to better understand the attention response and the advantage of the proposed method.\n\n- We have further discussed in Appendix C.1 the advantages of using the Hilbert curve over the Z-Order curve and learnable serialization strategies in terms of spatial locality preservation, continuity, and computational efficiency.\n\n- We have compared PointLearner with several previous state-of-the-art methods in Tab. 5 to further analyze its computational overhead, with the params, latency, and memory footprint in a single inference being used as evaluation metrics.\n\n- We have listed detailed network architectures and training settings across different datasets in a tabular form in Appendix B.2 to better understand the model's structure and implementation. \n\n- We have fully discussed self-supervised pretraining methods for hybrid architectures as a future research direction in Appendix F."}}, "id": "evkGPWjmQb", "forum": "KQPoMbxInu", "replyto": "KQPoMbxInu", "signatures": ["ICLR.cc/2026/Conference/Submission21998/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21998/Authors"], "number": 9, "invitations": ["ICLR.cc/2026/Conference/Submission21998/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763657499011, "cdate": 1763657499011, "tmdate": 1763657499011, "mdate": 1763657499011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}