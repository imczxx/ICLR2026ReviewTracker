{"id": "awBRdqA09x", "number": 24051, "cdate": 1758352183230, "mdate": 1763583773238, "content": {"title": "Aligning at the Source: Steering Corrective to the Origins of Harmfulness in LLMs", "abstract": "Persistent vulnerabilities in safety alignment hinder the deployment of large language models (LLMs). Existing methods remain susceptible to jailbreak attacks, suggesting a fundamental, unaddressed flaw in current safety paradigms. In this work, we diagnose the root cause of this fragility, identifying a systemic issue we term Depth-wise Alignment Discrepancy. We find a fundamental misalignment: harmful vectors—latent representations predisposed to unsafe content—predominantly originate in the model's lower layers, yet conventional alignment training concentrates its corrective gradients disproportionately on the top-most layers. This creates a brittle, \"end-of-pipe\" defense that is easily bypassed. To address this discrepancy, we propose SAGA, a framework that achieves robust safety through two synergistic innovations. First, it leverages high-entropy Chain-of-Thought (CoT) augmented data to provide the deep semantic signals necessary to reach the source of harmfulness. Second, it introduces a novel Synergistic Gradient Scaling (SGS) mechanism to explicitly reshape the gradient flow, ensuring these corrective signals are precisely applied to the identified vulnerable layers. Extensive experiments on five LLMs against six distinct jailbreak attacks demonstrate SAGA's superiority, reducing attack success rates (ASR) by 21\\%–63\\% compared to state-of-the-art baselines. Our method preserves downstream task accuracy while introducing minimal computational overhead (<3\\%).", "tldr": "", "keywords": ["Safety Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/19efa72b8e720327943832dcaa79b5ecf38eac88.pdf", "supplementary_material": "/attachment/a4dd8187b12c11fafbdfb3ea937bad3cc995e628.zip"}, "replies": [{"content": {"summary": {"value": "This paper reveals the Depth-wise Alignment Discrepancy, where harmful vectors predominantly originate from the model’s lower layers. Based on this observation, the authors propose a Synergistic Gradient Scaling (SGS) mechanism to explicitly reshape the gradient flow, and verify the effectiveness of their method across various datasets and attack baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation and method are clear, and the visualization is helpful.\n\n2. The authors provide sufficient evidence to support the effectiveness of their proposed method."}, "weaknesses": {"value": "1. The claim in Figure 2b that the harmful vectors exhibit a positive correlation with the applied gradient magnitude across layers is not clearly supported. The observed trends are not strictly consistent in numerical terms (i.e., there is no complete one-to-one correspondence between peaks and troughs), but only similar in general form. Moreover, the analysis lacks a direct logical explanation, making the conclusion appear somewhat forced.\n\n2. While the authors make considerable efforts to isolate the influence of subsequent layers, the ablation study remains insufficiently convincing. For instance, in Figure 3, it is expected that aligning more layers would naturally lead to a lower ASR. To more clearly demonstrate the relative effectiveness of the top versus lower layers, it would be necessary to conduct a similar experiment but with an opposite target, which is performing adversarial (malicious) training on different layers of a well-aligned model. If fine-tuning only a few top layers does not significantly increase the ASR, it would provide stronger empirical support for the authors’ claim.\n\n3. The authors claim a monotonic trend across different layers; however, this trend only holds from the lower-middle to the top layers, and the behaviour of the lowest layers is not adequately explained.\n\n4. As a gradient-based optimization method, GCG should be demonstrated to achieve (or approach) 100% ASR with a suitable attack budget in a white-box setting against a fine-tuned LLM (without filtering, such as PPL).\n\n5. It is unclear why LLaMA-3 and DeepSeek-R1 are used to demonstrate the phenomenon in Figure 1 but do not appear in the experimental section.\n\n6. The authors introduce numerous terms and methods without providing brief explanations in the main paper. For instance, the concept of token clusters in Figure 1 is not clarified, which makes the paper difficult to follow.\n\n7. The paper has not been thoroughly proofread. It is missing references in Appendix D and the image for Figure 9. In addition, the item presented as Table 8 should be referenced as Figure 8."}, "questions": {"value": "Refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CXQT3EJYPV", "forum": "awBRdqA09x", "replyto": "awBRdqA09x", "signatures": ["ICLR.cc/2026/Conference/Submission24051/Reviewer_MYjd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24051/Reviewer_MYjd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760692763660, "cdate": 1760692763660, "tmdate": 1762942913309, "mdate": 1762942913309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reveals the Depth-wise Alignment Discrepancy, where harmful vectors predominantly originate from the model’s lower layers. Based on this observation, the authors propose a Synergistic Gradient Scaling (SGS) mechanism to explicitly reshape the gradient flow, and verify the effectiveness of their method across various datasets and attack baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation and method are clear, and the visualization is helpful.\n\n2. The authors provide sufficient evidence to support the effectiveness of their proposed method."}, "weaknesses": {"value": "1. The claim in Figure 2b that the harmful vectors exhibit a positive correlation with the applied gradient magnitude across layers is not clearly supported. The observed trends are not strictly consistent in numerical terms (i.e., there is no complete one-to-one correspondence between peaks and troughs), but only similar in general form. Moreover, the analysis lacks a direct logical explanation, making the conclusion appear somewhat forced.\n\n2. While the authors make considerable efforts to isolate the influence of subsequent layers, the ablation study remains insufficiently convincing. For instance, in Figure 3, it is expected that aligning more layers would naturally lead to a lower ASR. To more clearly demonstrate the relative effectiveness of the top versus lower layers, it would be necessary to conduct a similar experiment but with an opposite target, which is performing adversarial (malicious) training on different layers of a well-aligned model. If fine-tuning only a few top layers does not significantly increase the ASR, it would provide stronger empirical support for the authors’ claim.\n\n3. The authors claim a monotonic trend across different layers; however, this trend only holds from the lower-middle to the top layers, and the behaviour of the lowest layers is not adequately explained.\n\n4. As a gradient-based optimization method, GCG should be demonstrated to achieve (or approach) 100% ASR with a suitable attack budget in a white-box setting against a fine-tuned LLM (without filtering, such as PPL).\n\n5. It is unclear why LLaMA-3 and DeepSeek-R1 are used to demonstrate the phenomenon in Figure 1 but do not appear in the experimental section.\n\n6. The authors introduce numerous terms and methods without providing brief explanations in the main paper. For instance, the concept of token clusters in Figure 1 is not clarified, which makes the paper difficult to follow.\n\n7. The paper has not been thoroughly proofread. It is missing references in Appendix D and the image for Figure 9. In addition, the item presented as Table 8 should be referenced as Figure 8."}, "questions": {"value": "Refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CXQT3EJYPV", "forum": "awBRdqA09x", "replyto": "awBRdqA09x", "signatures": ["ICLR.cc/2026/Conference/Submission24051/Reviewer_MYjd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24051/Reviewer_MYjd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760692763660, "cdate": 1760692763660, "tmdate": 1763603796604, "mdate": 1763603796604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper diagnoses a depth-wise alignment discrepancy: harmful internal representations that predispose LLMs to unsafe outputs arise mostly in lower/middle layers, while standard alignment gradients concentrate near the top, yielding a brittle “end-of-pipe” defense. To address this, the authors propose SAGA."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Identifies a layer-wise misallocation of alignment gradients relative to harmfulness genesis.\n- Proposes SAGA, uniting data-centric and optimization-centric strategies to align gradient flow with harmfulness sources.\n- Demonstrates improved robustness against diverse jailbreaks and after finetuning, with minimal overhead."}, "weaknesses": {"value": "- In L384, the authors mention “use llama-guard, and the manually review to judge.” Did you provide any results on human agreement? Reporting machine-human agreement would make the evaluation more convincing.\n\n- When evaluating SAGA, beyond ACC on a few benchmarks, did you examine changes in helpfulness or refusal rates on benign but sensitive prompts (i.e., potential over-refusal)?\n\n- There are also a few minor presentation errors. It is recommended that the authors carefully proofread the entire paper. For example, Table 2 repeats the ‘%’ symbol, and Figure 2 contains a typo (“visualiztioin”)."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6WcQUV1gwI", "forum": "awBRdqA09x", "replyto": "awBRdqA09x", "signatures": ["ICLR.cc/2026/Conference/Submission24051/Reviewer_Mmue"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24051/Reviewer_Mmue"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971595771, "cdate": 1761971595771, "tmdate": 1762942912914, "mdate": 1762942912914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper diagnoses the \"persistent fragility\" of LLM safety alignment. It identifies a core problem: \"Depth-wise Alignment Discrepancy\" (DAD), where harmful representations (termed \"harmful vectors\") predominantly originate in the lower-to-mid layers of the model (e.g., layers 10-20). However, conventional alignment training (like RLHF) concentrates its corrective gradients disproportionately on the top-most layers (e.g., 28-31). This mismatch creates a \"brittle, end-of-pipe\" defense.\n\nTo fix this, the paper proposes SAGA (Source-guided Alignment), a framework with two synergistic components:\n\n- High-Entropy CoT Data: It uses safety-related CoT data with high information entropy. The paper argues this complex reasoning data provides the \"deep semantic signals\" necessary to induce gradients that naturally penetrate the model's lower layers.\n\n- Synergistic Gradient Scaling (SGS): This is a novel optimization mechanism that dynamically rescales the layer-wise gradients ( alpha_l(t) ) to match a target \"Harmful Vector Distribution\" ( H ). This forces the optimizer to apply corrections at the source of the harmful vectors, not just at the output.\n\nExperiments on five LLMs show SAGA significantly reduces ASR (by 21%-63%) over baselines while preserving downstream task accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Excellent Mechanistic Diagnosis: The paper's primary strength is its clear, compelling, and empirically-backed diagnosis of the \"Depth-wise Alignment Discrepancy\", which provides a strong \"why\" for persistent alignment fragility.   \n\nNovelty: The Synergistic Gradient Scaling (SGS) mechanism is a novel and more nuanced optimization strategy than related methods that propose simply freezing layers (like SPPFT ).  The SAGA framework is well-designed. The high-entropy CoT data provides the necessary deep gradient signal , and the SGS mechanism precisely aims that signal. This synergy is a highly principled approach.  \n\nStrong Results: The framework demonstrates a significant reduction in ASR (21%-63%)  and, crucially, preserves downstream task accuracy (Table 7) , avoiding the common \"alignment tax.\""}, "weaknesses": {"value": "Novelty of Problem Diagnosis: The paper should be more precise in its \"Related Work\"  section. The problem of \"shallow safety alignment\"  and \"safety layers\"  in the middle of the model has been identified in very recent (ICLR 2025) work. The paper's novelty is its deep analysis (DAD) and its solution (SAGA), not the initial discovery of the phenomenon, which should be more clearly acknowledged.   \n\nMissing Critical Prior Art Comparison: The paper fails to cite or compare against the most relevant prior solution to this problem: SPPFT (Safely Partial-Parameter Fine-Tuning) by Li et al. (ICLR 2025). SPPFT also identifies \"safety layers\"  and proposes to \"fix the gradient\" (i.e., freeze them). SAGA's SGS  is a more advanced scaling operation, and a direct comparison is essential."}, "questions": {"value": "1. The authors should address the relationship to \"Safely Partial-Parameter Fine-Tuning (SPPFT)\".\n\n1. The link between \"high-entropy CoT\" and \"deeper gradients\" is a key part of the narrative. Can the authors add a plot (e.g., in Figure 4) showing the gradient distribution (similar to Fig 2a) for the Original Dataset vs. the High Information Entropy dataset? This would visually confirm this data-centric claim and close the loop on the argument."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hbLo0JVl1a", "forum": "awBRdqA09x", "replyto": "awBRdqA09x", "signatures": ["ICLR.cc/2026/Conference/Submission24051/Reviewer_d74F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24051/Reviewer_d74F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762239338214, "cdate": 1762239338214, "tmdate": 1762942912649, "mdate": 1762942912649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}