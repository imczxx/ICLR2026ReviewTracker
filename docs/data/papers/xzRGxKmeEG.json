{"id": "xzRGxKmeEG", "number": 15213, "cdate": 1758249013126, "mdate": 1763762592824, "content": {"title": "Revisiting Multi-Agent Debate as Test-Time Scaling: When Does Multi-Agent Help?", "abstract": "The remarkable growth in large language model (LLM) capabilities has spurred exploration into multi-agent systems, with debate frameworks emerging as a promising avenue for enhanced problem-solving. These multi-agent debate (MAD) approaches, where agents collaboratively present, critique, and refine arguments, potentially offer improved reasoning, robustness, and diverse perspectives over monolithic models. Despite prior studies leveraging MAD, a systematic understanding of its effectiveness compared to single-agent methods, particularly under varying conditions, remains elusive. This paper seeks to fill this gap by conceptualizing MAD as a test-time computational scaling technique, distinguished by collaborative refinement and diverse exploration capabilities. We conduct a comprehensive empirical investigation comparing MAD with strong self-agent test-time scaling baselines on solution-finding tasks (e.g., mathematical reasoning) and response-judging tasks (e.g., safety). Our study systematically examines the influence of task type, task difficulty, and agent diversity on MAD’s performance. Our key findings reveal that, for solution-finding tasks, MAD offers only limited advantages over self-agent scaling—even with diverse agents—although its effectiveness increases slightly as problem difficulty rises. Conversely, for response-judging tasks, especially on safety-reasoning tasks, MAD’s collaborative refinement generally strengthens defense and judgment as more agents are added. Moreover, incorporating diverse agent configurations yields a more pronounced reduction in attack success, indicating that agent diversity is crucial for response-judging tasks, unlike in solution-finding tasks. We believe our findings provide critical guidance for the future development of more effective and strategically deployed MAD systems.", "tldr": "We analyze multi-agent debate (MAD) as a test-time scaling method, revealing when it helps or harms compared to self-agent approaches in mathematical reasoning and safety tasks.", "keywords": ["multi-agent debate", "MAD", "large language models", "test-time scaling", "reasoning", "safety"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2246676de31dd8998beea5e0dba613bfa8d69158.pdf", "supplementary_material": "/attachment/17866f64f9ba5190f190139aca92b2bf2732d697.zip"}, "replies": [{"content": {"summary": {"value": "This paper reframes multi-agent debate (MAD) as a test-time scaling strategy. The authors evaluate homogeneous and heterogeneous MAD on mathematical reasoning and safety. For math, MAD rarely beats strong self-consistency, with slight gains only on the hardest problems; for safety, adding agents and diversity generally lowers attack success via collaborative refinement, with benefits most pronounced under heterogeneous configurations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear conceptual positioning of MAD as test-time scaling with two distinct features: collaborative refinement and diverse exploration.\n\n\n- Systematic, controlled comparisons against strong self-agent baselines across tasks, difficulties, model sizes, and diversity settings, with matched generation budgets.\n- Provide actionable insights for math and safety tasks."}, "weaknesses": {"value": "The primary concern is the generalizability of the conclusions, both within and across domains. Although the paper reports task-specific findings, there is no clear trend across tasks, and the divergent conclusions for math vs. safety make it difficult to infer whether MAD will help in other settings. Moreover, evaluating only a vanilla MAD configuration does not preclude gains from alternative debate designs. Prior work, e.g., Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate, has reported improvements on math, and other evaluations (e.g., Should We Be Going MAD? A Look at Multi-Agent Debate Strategies for LLMs) are cited but not compared in depth. Overall, the lack of thorough cross-study comparison and limited protocol coverage raises questions about both external validity and novelty.\n\n\n- Primary focus on math reasoning and safety; it’s unclear how well findings generalize to other domains (planning, coding, long-form generation) or to alternative debate protocols beyond the vanilla setup.\n- Safety evaluation relies on a closed-source judge (gpt-4o-mini), which can introduce evaluation bias; robustness to the choice of judge is not established.\n- The framing is largely confined to multi-agent debate; there’s limited comparison against other multi-agent collaboration paradigms, making it unclear whether conclusions transfer beyond debate. Is it limited only to a multi-agent debate framework? Can it be generated for other Multi-agent collaboration systems?"}, "questions": {"value": "- Even within math and safety, how robust are the conclusions across subdomains and difficulty regimes?\n- Where do MAD’s gains primarily come from—multi-agent diversity, the debate/refinement phase, or simply increased sampling? Can you decompose the contribution of each?\n\n\n\n\n- Do you have guidance or a heuristic for deciding when to use MAD on a new task?\n-  Why use majority voting over two sampled generations, and how are disagreements/ties resolved?\n- How should practitioners choose the number of agents? Any compute-aware heuristic or scaling law for picking an effective agent count?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y6vrhXfCmF", "forum": "xzRGxKmeEG", "replyto": "xzRGxKmeEG", "signatures": ["ICLR.cc/2026/Conference/Submission15213/Reviewer_yLiU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15213/Reviewer_yLiU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773941077, "cdate": 1761773941077, "tmdate": 1762925514133, "mdate": 1762925514133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for their time and valuable feedback. We appreciate that the reviewers recognized our work as empirically thorough, rigorous, and well-written. We are also grateful for the acknowledgement of our analysis regarding the internal dynamics that explain the **conditional effectiveness** of Multi-Agent Debate (MAD).\n\nIn this rebuttal, we have focused on clarifying our **novelty** and contributions, justifying our **experimental design**, and providing **additional experiments** to strengthen our claims. Below, we outline the specific updates made to the manuscript, followed by responses to the common concerns raised.\n\n---\n\n### **Summary of Updates in the Revised PDF**\n\nWe have revised the manuscript to address the reviewers' comments. All updated sections in the revised PDF are marked in ${\\color{blue}\\text{blue}}$.\n\n* **Clarifying Novelty & Contributions:**\n    * We have explicitly included a comparison with [1] and other relevant works to emphasize our unique contributions. This discussion is added to the **Related Work** section (${\\color{blue}\\text{Appendix A, Page 15}}$).\n    * We revised the introduction to clearly frame our primary goal: Defining the unique features of MAD as a form of test-time scaling and identifying the factors that determine its conditional effectiveness (${\\color{blue}\\text{Section 1, Page 1}}$). We move beyond simply asking \"is MAD good?\" to answering \"when and why is it effective?\", with a specific focus on task types.\n\n* **Expansion of Task Families:**\n    * To emphasize our novelty and address concerns about evaluation on only two tasks, we formally distinguish between **Solution-Finding Tasks** and **Response-Judgment Tasks**. This distinction is clarified in the Introduction (${\\color{blue}\\text{Section1, Page 2}}$) and Experimental Setting (${\\color{blue}\\text{Section 3, Page 4}}$).\n    * We have added results for **two new benchmarks** to strengthen our analysis:\n        1.  **GPQA-Diamond** (representing Solution-Finding).\n        2.  **RewardBench2** (representing Response-Judgment).\n        * These results and the accompanying analysis are included in ${\\color{blue}\\text{Appendix G, Page 24)}}$.\n\n* **Judge Reliability Experiments:**\n    * To ensure the robustness of our evaluation, we conducted additional experiments on safety tasks using a stronger judge model (*Gemini-2.5-Pro*). The results demonstrate the reliability of our evaluation framework and are available in ${\\color{blue}\\text{Appendix H.5, Page 29}}$.\n\n* **Writing & Clarification:**\n    * We have added necessary details regarding our evaluation protocols (${\\color{blue}\\text{Section 3, Page 4}}$).\n    * We have unified our analysis into a cohesive conclusion (${\\color{blue}\\text{Section 6, Page 10}}$).\n    * We have corrected all typos and grammatical errors pointed out by the reviewers."}}, "id": "RecYCeZZ2k", "forum": "xzRGxKmeEG", "replyto": "xzRGxKmeEG", "signatures": ["ICLR.cc/2026/Conference/Submission15213/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15213/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15213/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724596225, "cdate": 1763724596225, "tmdate": 1763724596225, "mdate": 1763724596225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "### **Responses to Common Concerns**\n\n#### **W1: Novelty and Distinction from Prior Work**\n*Common Question: How does this work differ from previous studies like [1], which also compared MAD against single-agent baselines?*\n\n**Response:**\nWhile both our work and [1] perform systematic comparisons, our framework and core findings diverge significantly. Our objective is not to issue a binary verdict on whether MAD is \"better,\" but to uncover **\"when, why, and how\"** MAD provides value. This is more practical and provides better insights, as it guides the user on  *which tasks to apply MAD to*, rather than *simply whether to apply MAD or no*t. We also include in-depth analysis explaining why for each question. Specifically, our contributions are as follows:\n\n1.  **Test-Time Scaling Perspective:** We are the first to rigorously define MAD through the lens of **test-time scaling**. We isolate whether performance gains stem from the specific interactions inherent in MAD—diverse exploration and collaborative refinement—or simply from increased compute.\n2.  **Conditional Effectiveness:** Our insights highlight that the **nature of the task** is the primary determinant of success.\n    * For **Solution-Finding** tasks (e.g., general reasoning), we find that MAD offers very marginal gains, even with heterogeneous agents.\n    * For **Response-Judgment** tasks, MAD yields significant benefits, provided agent diversity is maintained.\n3.  **Nuanced Conclusion:** Unlike previous works that often conclude either \"MAD is generally good\" or \"Heterogeneous MAD is good while Homogeneous is bad,\" our work provides a granular framework showing that effectiveness is highly conditional on the task type, further supported by our analysis across varying diversity levels and model scales.\n\n\nWe have revised the Introduction (${\\color{blue}\\text{Page 1}}$) and Related Work (${\\color{blue}\\text{Appendix A, Page 15}}$) to explicitly articulate these distinctions.\n\n#### **W2: Limited Task Scope**\n*Common Question: The initial draft focused primarily on mathematical reasoning and safety. Does the framework hold for other domains?*\n\n**Response:**\nWe agree that broader task coverage strengthens our claims. To address this, we have expanded our experimental suite to cover a wider spectrum of task families.\n\n1.  **New Benchmarks:** We added **GPQA-Diamond** to represent complex *Solution-Finding* tasks and **RewardBench2** to represent *Response-Judgment* tasks.\n2.  **Consistent Findings:** Our results on these new tasks align perfectly with our initial hypothesis:\n    * On **GPQA (Solution-Finding)**, MAD does not bring clear gains compared to single-agent scaling.\n    * On **RewardBench (Response-Judgment)**, MAD is highly effective.\n3.  **Implication:** This confirms that MAD is most effective in subjective domains or tasks with limited search spaces (judging), rather than in open-ended solution spaces with gold-standard answers.\n\nThe detailed results and analysis for these additional tasks are provided in ${\\color{blue}\\text{Appendix G (Page 24)}}$.\n\n---\n\n**Detailed Responses**\nIn addition to this general response, we have provided detailed point-by-point responses to specific questions within the **Official Comment** for each reviewer.\n\nThank you again for your constructive feedback. We believe these updates significantly strengthen the manuscript. We are happy to engage in further discussion if there are any remaining questions.\n\n**References:**\n\n[1] *Stop Overvaluing Multi-Agent Debate -- We Must Rethink Evaluation and Embrace Model Heterogeneity*"}, "title": {"value": "General Response 2/2 (Novelty, Experiments, and Clarification)"}}, "id": "RecYCeZZ2k", "forum": "xzRGxKmeEG", "replyto": "xzRGxKmeEG", "signatures": ["ICLR.cc/2026/Conference/Submission15213/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15213/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15213/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724596225, "cdate": 1763724596225, "tmdate": 1763758708363, "mdate": 1763758708363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript reframes Multi‑Agent Debate (MAD) as a test‑time scaling mechanism that combines diverse parallel exploration with sequential collaborative refinement. Under matched generation budgets, the study compares MAD to strong self‑agent baselines: Self‑Consistency (SC) and Self‑Refinement (SR), across mathematical reasoning and safety tasks. The main findings are: (i) in math, SC typically outperforms or matches MAD, with only slight gains for MAD on very difficult problems where collaborative refinement can validate a rare correct trajectory; (ii) in safety, SR can amplify attack success rates (ASR) for smaller models, whereas MAD with more agents and heterogeneous configurations tends to reduce ASR and sometimes matches/surpasses the safest single agent as rounds increase."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper conceptualizes Multi-Agent Debate (MAD) as a test-time scaling strategy, clearly analyzing its effectiveness along two distinct dimensions: parallel scaling to enhance generative diversity and sequential refinement for iterative critique and revision. This approach effectively elucidates MAD’s relative advantages over single-agent baselines.\n\n2. The authors systematically evaluate and compare multi-agent and single-agent approaches across different tasks, incorporating diverse agent configurations through variations in models and prompts. This comprehensive analysis provides nuanced insights into how heterogeneity influences multi-agent outcomes.\n\n3. The employment of detailed diagnostics (e.g., Best-of-Correctness [BoC], Best-on-Follow [BoF]) and the thorough analysis of answer transitions from initial to final states further clarify the internal dynamics underpinning the effectiveness of multi-agent strategies."}, "weaknesses": {"value": "1.The study categorizes tasks into two broad regimes—those requiring extensive exploration of large search spaces and those necessitating consensus among limited options. However, for each category, the authors select only one specific type of task (mathematics and safety, respectively), which limits the representativeness and generalizability of the conclusions.\n\n2.Prior research ([1]) appears to have explored similar comparative analyses between MAD and Self-Consistency (SC), including investigations into heterogeneity-driven MAD approaches, yet no citation or discussion of this work is provided in the manuscript.\n\n3.The reliance solely on GPT-4o-mini for evaluating the Attack Success Rate (ASR) in safety tasks raises concerns regarding evaluator credibility. Given that GPT-4o-mini itself may face limitations in handling complex safety evaluations, incorporating human validation or cross-validation with more robust evaluators would significantly enhance confidence in these results.\n\n4. While alignment across methods is established based on the number of generated responses, MAD methods incur additional overhead in context tokens due to cross-agent interactions and extended prompts. This aspect is not quantified, limiting the fairness and practical interpretability of the computational cost comparisons.\n\n5. Figure 7 inaccurately labels metrics as \"accuracy\" rather than ASR, creating confusion and reducing clarity. Consistency across the manuscript's terminology and visual presentations should be ensured.\n\n[1] Zhang, Hangfan, et al. \"Stop Overvaluing Multi-Agent Debate—We Must Rethink Evaluation and Embrace Model Heterogeneity.\" arXiv preprint arXiv:2502.08788 (2025)."}, "questions": {"value": "1.Could the authors validate their proposed two-regime task classification with additional datasets (e.g., code generation, knowledge-intensive QA, or tool-assisted planning) to better substantiate the generalizability of their conclusions?\n\n2.Can the authors provide evidence regarding the reliability and accuracy of GPT-4o-mini as the evaluator for safety-related tasks? For instance, could human evaluators or comparisons with stronger models be included, and consistency or inter-rater agreement explicitly reported?\n\n3.Could the authors provide a detailed comparison of token usage and computational resources (e.g., prompt tokens, context tokens per round and agent, total token counts) to facilitate a fairer and more practical assessment of MAD relative to single-agent baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IdRCYAhyMT", "forum": "xzRGxKmeEG", "replyto": "xzRGxKmeEG", "signatures": ["ICLR.cc/2026/Conference/Submission15213/Reviewer_2Jxw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15213/Reviewer_2Jxw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877874220, "cdate": 1761877874220, "tmdate": 1762925513736, "mdate": 1762925513736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reframes multi-agent debate (MAD) as a form of test-time scaling that mixes parallel exploration and sequential collaborative refinement, and asks when (and for what tasks) MAD actually helps over strong single-agent baselines such as self-consistency (SC) and self-refinement (SR). Through controlled comparisons on mathematical reasoning (MATH500, AIME 2024/2025) and safety evaluations (Anthropic Harmful, MultiJail), the authors find that homogeneous MAD rarely beats parallel SC for math; gains appear only on the hardest problems where collaborative refinement can surface and verify a rare correct trajectory. Adding diversity via personas/prompting yields small additional math gains, while mixing different model families tends to regress toward a harmonic-mean–like performance and may destabilize consensus, limiting benefits versus simply sampling more from the strongest model. In contrast, for safety tasks, collaborative refinement with more (and larger) agents consistently reduces attack success rates, and heterogeneous configurations (different prompts or model families) are especially effective. The study’s takeaways are practical: prefer SC for math unless problems are extremely difficult (where MAD can verify a rare correct path), and use heterogeneous MAD for safety to counter bias propagation and improve robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* This paper focuses on how and when to choose appropriate design decision for MAD systems, which is a critical challenge in the application of MAD.\n* The paper is well written with highlighted takeaways, offering practical guidance to MAD design."}, "weaknesses": {"value": "* The idea of agent diversity, and the observation that MAD sometimes fails single agent methods have been well acknowledged in previous research [1][2][3]\n* While with comprehensive evaluation and practical design recommendations, this paper did not offer a unified solution to this problem, limiting the technical contribution.\n* The takeaways are bounded by task types, difficulty, which is sensitive to the selection of base models, and may not be generalizable to stronger models or stronger reasoning models, especialy given that MADs are naturally sensitive to prompts. It is not promising that these takeaways can benefit further MAD system design.\n\n\n[1] Reconcile: Round-table conference improves reasoning via consensus among diverse llms\n[2] Stop Overvaluing Multi-Agent Debate -- We Must Rethink Evaluation and Embrace Model Heterogeneity\n[3] Should we be going MAD? A Look at Multi-Agent Debate Strategies for LLMs"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yR9xgLqJdd", "forum": "xzRGxKmeEG", "replyto": "xzRGxKmeEG", "signatures": ["ICLR.cc/2026/Conference/Submission15213/Reviewer_7HNX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15213/Reviewer_7HNX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939607952, "cdate": 1761939607952, "tmdate": 1762925513300, "mdate": 1762925513300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically evaluates Multi-Agent Debate (MAD) against strong baselines (Self-Consistency and Self-Refinement) on mathematical reasoning and safety tasks. It finds MAD's effectiveness is highly conditional: it offers limited benefits in math, except for very difficult problems when using weaker models , and it increases vulnerability in safety tasks unless heterogeneous agents are employed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "This paper is empirically thorough, systematically testing MAD performance across a range of tasks, difficulty levels, and model scales, demonstrating that MAD is not a universally superior method but is beneficial only under specific, well-defined conditions."}, "weaknesses": {"value": "The primary weakness is a severe lack of novelty. The paper's core findings are nearly identical to those in a recent, highly related publication If multi-agent debate is the answer, what is the question? (arXiv:2502.08788). Both perform a systematic evaluation of MAD and arrive at the same two conclusions:\n\n1.MAD is Overvalued: Both papers find that MAD generally fails to outperform strong single-agent baselines like Self-Consistency (SC).\n\n2.Heterogeneity is the Solution: Both papers identify model heterogeneity (using diverse agents) as the key solution.\n\nThe prior work already established heterogeneity as the 'universal antidote'. This paper's primary conclusions are therefore a re-discovery, which significantly limits its contribution. The authors must address this major overlap."}, "questions": {"value": "Please respond to the above weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uZ6CYMe7o7", "forum": "xzRGxKmeEG", "replyto": "xzRGxKmeEG", "signatures": ["ICLR.cc/2026/Conference/Submission15213/Reviewer_LsAr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15213/Reviewer_LsAr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15213/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012505903, "cdate": 1762012505903, "tmdate": 1762925512886, "mdate": 1762925512886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response 1/2 (PDF Updates)"}, "comment": {"value": "We thank the reviewers for their time and valuable feedback. We appreciate that the reviewers recognized our work as empirically thorough, rigorous, and well-written. We are also grateful for the acknowledgement of our analysis regarding the internal dynamics that explain the **conditional effectiveness** of Multi-Agent Debate (MAD).\n\nIn this rebuttal, we have focused on clarifying our **novelty** and contributions, justifying our **experimental design**, and providing **additional experiments** to strengthen our claims. Below, we outline the specific updates made to the manuscript, followed by responses to the common concerns raised.\n\n---\n\n### **Summary of Updates in the Revised PDF**\n\nWe have revised the manuscript to address the reviewers' comments. All updated sections in the revised PDF are marked in ${\\color{blue}\\text{blue}}$.\n\n* **Clarifying Novelty & Contributions:**\n    * We have explicitly included a comparison with [1] and other relevant works to emphasize our unique contributions. This discussion is added to the **Related Work** section (${\\color{blue}\\text{Appendix A, Page 15}}$).\n    * We revised the introduction to clearly frame our primary goal: Defining the unique features of MAD as a form of test-time scaling and identifying the factors that determine its conditional effectiveness (${\\color{blue}\\text{Section 1, Page 1}}$). We move beyond simply asking \"is MAD good?\" to answering \"when and why is it effective?\", with a specific focus on task types.\n\n* **Expansion of Task Families:**\n    * To emphasize our novelty and address concerns about evaluation on only two tasks, we formally distinguish between **Solution-Finding Tasks** and **Response-Judgment Tasks**. This distinction is clarified in the Introduction (${\\color{blue}\\text{Section1, Page 2}}$) and Experimental Setting (${\\color{blue}\\text{Section 3, Page 4}}$).\n    * We have added results for **two new benchmarks** to strengthen our analysis:\n        1.  **GPQA-Diamond** (representing Solution-Finding).\n        2.  **RewardBench2** (representing Response-Judgment).\n        * These results and the accompanying analysis are included in ${\\color{blue}\\text{Appendix G, Page 24)}}$.\n\n* **Judge Reliability Experiments:**\n    * To ensure the robustness of our evaluation, we conducted additional experiments on safety tasks using a stronger judge model (*Gemini-2.5-Pro*). The results demonstrate the reliability of our evaluation framework and are available in ${\\color{blue}\\text{Appendix H.5, Page 29}}$.\n\n* **Writing & Clarification:**\n    * We have added necessary details regarding our evaluation protocols (${\\color{blue}\\text{Section 3, Page 4}}$).\n    * We have unified our analysis into a cohesive conclusion (${\\color{blue}\\text{Section 6, Page 10}}$).\n    * We have corrected all typos and grammatical errors pointed out by the reviewers."}}, "id": "YN7APZG8sv", "forum": "xzRGxKmeEG", "replyto": "xzRGxKmeEG", "signatures": ["ICLR.cc/2026/Conference/Submission15213/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15213/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission15213/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763758440481, "cdate": 1763758440481, "tmdate": 1763758710669, "mdate": 1763758710669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}