{"id": "wU4Tjlzg3h", "number": 2346, "cdate": 1757061669849, "mdate": 1763714842140, "content": {"title": "MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems", "abstract": "Scaling up data, parameters, and test-time computation has been the mainstream methods to improve LLM systems (LLMsys), but their upper bounds are almost reached due to the gradual depletion of high-quality data and marginal gains obtained from larger computational resource consumption. \nInspired by the abilities of human and traditional AI systems in learning from practice, constructing memory and continual learning frameworks for LLMsys has become an important and popular research direction in recent literature.\nYet, existing benchmarks for LLM memory often focus on evaluating the system on homogeneous reading comprehension tasks with long-form inputs rather than testing their abilities to learn from accumulated user feedback in service time.   \nTherefore, we propose a user feedback simulation framework and a comprehensive benchmark covering multiple domains, languages, and types of tasks to evaluate the continual learning abilities of LLMsys.\nExperiments show that the effectiveness and efficiency of state-of-the-art baselines are far from satisfying, and we hope this benchmark could pave the way for future studies on LLM memory and optimization algorithms.", "tldr": "", "keywords": ["Memory", "Continual Learning", "Large Language Model"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/41eb59fdc710fde9b13a136728c3a06d695ab3e3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a memory benchmark where prior user-system interactions result in feedback that is expected to be incorporated in new interactions during testing. Using a collection of 11 publicly available datasets, a benchmark is constructed by simulated user-agent interactions where users give 'feedback' on the agent responses. Feedback types can be explicit (eg: verbose feedback, up/down vote) or implicit (eg: close a session). Memory systems are scored based on extent to which memory systems are able to exploit historical feedback-aware conversations, to improve baseline performance. Metrics are task dependent and the paper also tracks how execution time goes up when the memory modules are used."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Use of multiple tasks, publicly available sources\n- Interesting setup of studying learning from feedback using simulators."}, "weaknesses": {"value": "- A study of the *nature* of feedback that was auto generated appears to be missing. It has only been studied indirectly by measure task performance when memory systems use this feedback. \n- The paper appears to miss recent memory benchmarks in the discussion such as : https://aclanthology.org/2025.findings-acl.989.pdf and other such related work.\n- The beginning of the paper setup various *types* of memories - procedural, declarative, episodic etc but it does not present an analysis based on this classification. (Lines 125-135)"}, "questions": {"value": "As noted in the main review the benchmark relies on simulated feedback but there hasn't been any formal human evaluation of feedback quality.  Isn't there a potential risk of for task-specific information leaking into the simulated user responses? And thus, depending on the nature of the memory system being tested -- some design choices could exploit this leakage better than other systems (eg: insert in prompt vs BM25)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z5AXttaFY3", "forum": "wU4Tjlzg3h", "replyto": "wU4Tjlzg3h", "signatures": ["ICLR.cc/2026/Conference/Submission2346/Reviewer_dhtk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2346/Reviewer_dhtk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601639016, "cdate": 1761601639016, "tmdate": 1762916201351, "mdate": 1762916201351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MemoryBench, a benchmark and simulation framework to evaluate memory and continual‑learning abilities of LLM systems (LLMsys) across declarative vs. procedural memory and explicit vs. implicit feedback. It spans 11 datasets, multiple domains (open, legal, academic) and task formats (LiSo/SiLo/LiLo/SiSo), and provides an LLM‑as‑user simulator to generate verbose/action feedback plus a performance monitor that consolidates heterogeneous metrics via an LLM‑as‑judge."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear problem framing & useful taxonomy. The paper distinguishes declarative vs. procedural memory and explicit vs. implicit feedback, and shows where existing benchmarks fall short (Table 1), motivating a realistic continual‑learning setting. \n\n2. Breadth and heterogeneity. Coverage across domains, languages (en/zh), and task formats with varied input/output lengths (Table 2, ) makes overfitting to one narrow task less likely and increases ecological validity. \n\n3. End‑to‑end evaluation pipeline. The Fig. 1 framework specify data preparation, feedback simulation, metric integration, and both on‑ and off‑policy protocols—good engineering depth and transparency. \n\n4. Cost/latency reporting. Time profiles per sample (Fig. 3; Table 14) and case studies of pathological slowdowns (Mem0; Fig. 10) are valuable for practitioners."}, "weaknesses": {"value": "1. Validity of simulated feedback and evaluation loop. The core claims rely heavily on LLM‑as‑user for qualitative feedback and LLM‑as‑judge for metric consolidation (e.g., JuDGE/IdeaBench/SciTechNews; A.1.2). There is no human study to calibrate either the fidelity of simulated user behavior or the consistency of the LLM‑judge vs. humans, especially across languages (zh/en) and specialized domains (law). The paper shows within‑system gains with feedback (Table 11), but that primarily measures self‑consistency under the same evaluation mechanism. Without a small human validation or cross‑judge agreement, the external validity of conclusions is uncertain.\n\n2. Fairness of baseline configuration and ablations. Several outcomes could be sensitive to embedding model choices and retrieval budgets. For instance, the paper mixes Qwen3‑Embedding‑0.6B for some methods and all‑MiniLM‑L6‑v2 for others (A.3.2), and truncates long contexts for some systems (sec3.1). It is unclear whether memory systems were re‑tuned for the heterogeneous tasks (SiLo/LiLo vs. Locomo‑style LiSo). Without embedding parity, retrieval‑count parity, and tuning ablations, the “RAG ≥ memory” takeaway may partially reflect configuration rather than method.\n\n3. Statistical reporting. Aggregate results are normalized (min‑max or z‑score) over the tested models in each dataset (sec3.2), which makes absolute gains hard to interpret and can shift with the set of systems included. There are no confidence intervals/variance across seeds or tests of significance\n\n4. Prompt alterations for DialSim. The paper replaces the official prompt’s full history with retrieval‑based access (A.1.1). This improves comparability but may also penalize models originally evaluated under full‑context access; the impact of this design choice is not ablated."}, "questions": {"value": "1. Human validation: Can you provide a small human study (e.g., 100–200 cases across en/zh) to calibrate (a) LLM‑as‑user satisfaction scores and (b) LLM‑as‑judge merged scores against human judgments? Even inter‑judge agreement with another strong LLM would help.\n2. Embedding & retrieval parity: How do results change if all systems (RAG and memory) use identical embedding models and identical retrieval counts/limits? Please include an ablation where A‑Mem, Mem0, and MemoryOS use Qwen3‑Embedding‑0.6B (or vice versa) and fixed top‑k.\n3. Normalization choice: Since min‑max depends on which systems are included, could you also report absolute task metrics and effect sizes with CIs?\n\n**suggestion**\n- Add robustness checks: multiple backbones (you provide Qwen3‑8B/32B; Table 10), plus at least one non‑Qwen backbone to rule out model‑family confounds.\n\n- Report variance over 3+ random seeds and include bootstrap CIs for LLM‑judge scores to quantify measurement noise. (§3.2)\nUnify embeddings & rerankers across systems and add an Embedding‑parity ablation.\n\n- Ablate simulator knobs (turns=1/2/3; persona expertise) and report how these alter gains in Table 11"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eMXq2LhGuN", "forum": "wU4Tjlzg3h", "replyto": "wU4Tjlzg3h", "signatures": ["ICLR.cc/2026/Conference/Submission2346/Reviewer_6W2s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2346/Reviewer_6W2s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749520060, "cdate": 1761749520060, "tmdate": 1762916201133, "mdate": 1762916201133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an extensive benchmark for evaluating memory and learning capabilities of language model systems. For evaluating memory, the authors divide memory into two big categories: declarative memory (for factual knowledge such as wikipedia or personal profiles) and procedural memory (for non-factual rules related to the execution of tasks, such as user feedback and behavior logs). Declarative memory is well-studied in prior works, yet procedure memory is not. To this end, the authors propose a taxonomy of procedure memory into two types: explicit feedback and implicit feedback. The main novelty and contribution of the new benchmark, lies in how it creates realistic and diverse user-feedback data. To this end, the authors implement the user simulator with a LLM-as-user paradigm combined with a two-state programmable action simulator, i.e., use LLM to simulate the responses and then use programmable simulator to generate diverse ”like”, “”dislike or other actions based on prior studies of user behaviors on LLM service. \n\nThe authors then evaluate the existing memory systems for solving the end tasks in on-policy and off-policy settings, given access to the created synthetic dataset partitioned into different domains (such as Open-DOmian, Legal, Academic) and task formats (such as LiSo, SiLo, LiLo and SiSo). Experiments reveal that the created user feedback are beneficial for solving the task (compared to LLM without memory), but vanilla RAG often outperform complex memory-based LLM systems such as A-Mem, Mem0 or MemoryOS for the new long-input long-output task proposed in this paper. (By contrast, prior works mainly reported good performance of memory systems on long-input short-output reading comprehension tasks). This shows limitation of existing memory-based LLM systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The main contribution is the construction of a set of realistic and diverse user-feedback data, that allows evaluating the model’s ability of utilize procedural memory in solving a task. \n2. The state-of-the-art memory systems as well as vanilla RAG system have limited performance (70%) on the proposed benchmark, which makes the benchmark potentially useful for improving memory systems in the future."}, "weaknesses": {"value": "The main concern is that the reasons of the limited performance are not fully discussed: in section 3.3, the authors attributed the reason to sensitivity to noisy memory by memory systems, as well as to the unique long-input long-output task nature in the proposed benchmark (compared to the long-input short-output nature in prior works). Yet there is no empirical evidence, e.g., what passages are retrieved by the model to solve each test query by different method? Also, how should we interpret the scale of performance, in the sense that how high can human performance reach? Such examples and natural performance bounds are necessary to understand whether the task is solvable and whether the evaluation method is sound.\n\nFinally, the authors did not include backgrounds about how the advanced memory systems work. Including them would make the paper significantly more self-contained, as well as help understanding their limitations."}, "questions": {"value": "1. Is there a breakdown of the SOTA models' performance on the proposed benchmark by the types of feedback, which are outlined in Table 1? Is there an option to evaluate with a specific type of feedback only, e.g., verbose or action or implicit feedback only? It would make the benchmark more fine-grained and useful.\n\n2. Does the model choice of the LLM-as-User-Simulator affect the performance? In this paper, Qwen-3-32B is used as the user simulator. Will this choice lead to any biased evaluation result favoring LLM systems using Qwen as their backbones?\n\n1. \"LR\" is used throughout the paper without being spelt in full once. \n2. L31: \", which\" -> \"This\"\n3. L38: \"improve with\" -> \"improve\"\n4. L39: \"remains\" -> \"remain\"\n5. L52: \"in future\" -> \"in the future\"\n6. L58: \"isn't a\" -> \"is no\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p4T2igTkc4", "forum": "wU4Tjlzg3h", "replyto": "wU4Tjlzg3h", "signatures": ["ICLR.cc/2026/Conference/Submission2346/Reviewer_NcyV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2346/Reviewer_NcyV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952430699, "cdate": 1761952430699, "tmdate": 1762916200958, "mdate": 1762916200958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a benchmark (MemoryBench) and a user-feedback simulation framework to evaluate the memory and continual learning abilities of existing LLMsys. Through experiments, the authors show that existing memory systems and vanilla RAG methods are not good at utilizing procedural knowledge (e.g., user feedbacks and behavior logs)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation for considering user feedback is intuitive and clearly introduced.\n- Compared with previous works, MemoryBench focuses on more fine-grained categories and introduces user feedback (which is not well explored previously) during evaluation.\n- Different task types (long/short inputs/outputs) from different datasets or benchmarks are considered in MemoryBench.\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "- There are two main concerns about the LLM-as-user-Simulator framework:\n  - 1) How closely does the framework reflect (real) human behaviour?\n  - 2) User feedback from humans often contains noise (e.g., incorrect likes/dislikes and situations in which users insist on their own mistakes during conversations, requesting the model to concede). To what extent can the current LLMsys handle such noise?\n- Many new concepts/definitions are introduced in Section 2.1. Some examples or detailed explanations (e.g., procedural memory, #Case in Table 1) can be further discussed. A clear diagram may improve the readability.\n- In Section 3.3 (Comparison of different LLM-based LLMsys), the authors find that the RAG methods outperform the existing state-of-the-art memory-based LLMsys. Some analyses are conducted, however, some key evidence/experiments or further discussions are missing: \n  - 1) Line 417: Why do memory-based LLMsys outperform RAG methods on LiSo tasks? \n  - 2) What does \"Existing [...] do not differentiate [...]\" (Line 419) mean? If these methods were able to differentiate historical logs from the current task context, would that lead to better performance?\n- In Figure 3, the authors separate memory time from prediction time. For RAG methods, are retrieval and vector database construction considered part of memory time or prediction time?\nMinor:\n- \"Data Provider\" (in Figure 1) $\\rightarrow$ \"Task Provider\""}, "questions": {"value": "Please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F8nZvEgzNn", "forum": "wU4Tjlzg3h", "replyto": "wU4Tjlzg3h", "signatures": ["ICLR.cc/2026/Conference/Submission2346/Reviewer_yWTm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2346/Reviewer_yWTm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001344026, "cdate": 1762001344026, "tmdate": 1762916200640, "mdate": 1762916200640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}