{"id": "nPcrFDHL2i", "number": 3585, "cdate": 1757482309548, "mdate": 1762923466584, "content": {"title": "WonderZoom:  Multi-Scale 3D World Generation", "abstract": "We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to ``zoom into'' a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/.", "tldr": "WonderZoom generates multi-scale 3D worlds from a single image, enabling users to zoom in and synthesize new fine-scale 3D content on demand.", "keywords": ["Multi-scale generation", "3D world generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/b4aaa816d901a9c49f8756bb61582a2f8fea7165.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "WonderZoom is a framework for generating multi-scale 3D scenes from a single image. The method introduces scale-adaptive Gaussian surfels, which support dynamic updates and real-time rendering. It further proposes a progressive detail synthesizer consisting of three components: new-scale image generation, depth registration, and auxiliary view generation. Experimental results demonstrate that WonderZoom outperforms existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel problem setting. The task of multi-scale 3D scene generation is both original and practical. Previous methods typically focus on single-scale generation, limiting their ability to support continuous exploration.\n\n2. The proposed scale-adaptive Gaussian surfels and progressive detail synthesizer are well-motivated and empirically effective."}, "weaknesses": {"value": "1. The experiments are conducted on only six scenes, which is insufficient to demonstrate robustness or generalization. It is unclear how representative these examples are, and whether they might be cherry-picked. A broader and more systematic evaluation would strengthen the paper.\n\n2. A user study or at least a user-oriented evaluation would help substantiate claims about interactive quality.\n\n3. The paper does not provide any failure cases or discussions of limitations."}, "questions": {"value": "1. How robust is the overall pipeline, given that it relies on multiple base models? Under what conditions does it fail?\n\n2. Can the next scale be generated via camera movements other than zoom-in (e.g., panning left/right or upward/downward)? All examples shown seem to rely solely on zoom-in motion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "StUUL0bcMd", "forum": "nPcrFDHL2i", "replyto": "nPcrFDHL2i", "signatures": ["ICLR.cc/2026/Conference/Submission3585/Reviewer_KTZf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3585/Reviewer_KTZf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761463686887, "cdate": 1761463686887, "tmdate": 1762916846291, "mdate": 1762916846291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thanks for all the constructive comments."}}, "id": "7jugea9TuF", "forum": "nPcrFDHL2i", "replyto": "nPcrFDHL2i", "signatures": ["ICLR.cc/2026/Conference/Submission3585/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3585/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762923465695, "cdate": 1762923465695, "tmdate": 1762923465695, "mdate": 1762923465695, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents WonderZoom, a novel framework designed for multi-scale 3D world generation from a single image. Unlike existing 3D scene generation systems—such as WonderWorld, HunyuanWorld, and SceneScape—which typically produce content at a fixed scale like landscapes, rooms, or objects, WonderZoom enables the creation of consistent 3D content across multiple spatial scales.\n\nTo address this challenge, the framework introduces two key technical contributions: Scale-Adaptive Gaussian Surfels, a hierarchical 3D representation that can be dynamically updated to incorporate fine-scale details without re-optimizing existing geometry, and the Progressive Detail Synthesizer, an autoregressive module that incrementally generates fine-scale 3D structures guided by user prompts and camera zoom-ins while preserving both geometric and semantic consistency.\n\nWith WonderZoom, users can interactively zoom into regions of a generated 3D scene to create new structures—including microscopic details—that were not present at broader scales. Experimental results demonstrate that WonderZoom achieves superior performance compared to leading 3D and video generation methods in terms of prompt alignment, rendering quality, and visual aesthetics."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Novel Problem Formulation**\n    This work presents a significant paradigm shift by being the first to explicitly tackle **multi-scale 3D generation**. It moves beyond the established challenges of reconstruction or single-scale generation, pioneering a new direction for interactive and progressive 3D synthesis. This is a highly compelling and timely contribution.\n\n2.  **Innovative Technical Representation**\n    The introduction of **scale-adaptive Gaussian surfels** is an original and well-motivated representation. By ingeniously extending Gaussian surfels to be scale-aware with dynamic update mechanisms, it effectively bridges the domains of high-fidelity rendering and generative modeling.\n\n3.  **Comprehensive Empirical Validation**\n    The experimental section is thorough and demonstrates that WonderZoom consistently outperforms existing baseline methods in the challenging task of multi-scale 3D world generation. The results robustly validate the effectiveness of the proposed approach.\n\n4.  **Exemplary Clarity and Structure**\n    The paper is exceptionally well-organized and clearly written. The logical flow and structured presentation make the complex technical contributions easy to understand and follow.\n\n5.  **Strong Potential Impact**\n    WonderZoom holds considerable promise for advancing the frontier of 3D content creation. Its capability for generation at arbitrary scales has the potential to transform workflows in fields such as 3D design, virtual reality world-building, and scientific visualization."}, "weaknesses": {"value": "1.  **Multi-Scale Consistency in Texture and Content**\n    The rendered examples reveal challenges in maintaining consistent detail and texture across different scales. Spatial transitions often appear uneven when content from various scales is composited. For instance:\n    *   In the **beach scene (Scale 1)**, the central region is rendered with significantly finer detail than the periphery, creating a noticeable and artificial boundary.\n    *   In the **yellow bird example (Scale 1)**, the textures of the background roof eaves are visibly misaligned.\n    Such inconsistencies ultimately undermine the realism and aesthetic coherence of the final output.\n\n2.  **Limitations in Quantitative Evaluation**\n    While the visual comparisons are comprehensive, the quantitative evaluation feels limited. For example, the analysis lacks user study results to statistically validate the perceptual quality of the outputs.\n    \n    Furthermore, the paper proposes that **Proposition 1 (Seamless Scale Transition)** facilitates smoother transitions, but this claim is not yet substantiated by experimental data. Providing metrics or a comparative analysis to demonstrate this improvement would strengthen the argument significantly."}, "questions": {"value": "1.  **Error Accumulation in the Two-Stage Process**\n    Your approach employs a two-stage method for generating finer-scale images: first applying super-resolution, followed by image editing. Could this sequential process lead to an accumulation of errors, ultimately causing inconsistencies in style and texture between the newly generated output and the existing scene?\n\n2.  **Consistency in Overlapping Regions**\n    During the iterative process of zooming into and out of different regions, if these areas overlap, what is the model's behavior? Will it strive to preserve the details generated in a previous pass, or will it overwrite them, potentially leading to temporal inconsistencies in the overlapping sections?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "78J17xQ2r2", "forum": "nPcrFDHL2i", "replyto": "nPcrFDHL2i", "signatures": ["ICLR.cc/2026/Conference/Submission3585/Reviewer_SZuN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3585/Reviewer_SZuN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568738945, "cdate": 1761568738945, "tmdate": 1762916845729, "mdate": 1762916845729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents WonderZoom, a framework for generating multi-scale 3D worlds from a single image. It allows users to zoom into any region and synthesize new, coherent fine-scale content. The method introduces scale-adaptive Gaussian surfels for dynamic, real-time 3D representation and a progressive detail synthesizer that adds finer geometry and appearance guided by user input. Experiments demonstrate improved visual quality, consistency, and prompt alignment compared to existing 3D and video generation methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles an important and underexplored problem of generating coherent 3D worlds across multiple spatial scales, demonstrating clear novelty compared to existing single-scale approaches.\n2. The proposed scale-adaptive Gaussian surfels provide an efficient representation that enables dynamic updates and real-time rendering without re-optimization.\n3. Quantitative experiments show consistent improvements over several strong baselines in prompt alignment and perceptual quality, supporting the method's effectiveness."}, "weaknesses": {"value": "1. The experimental dataset is limited in diversity and scale, making it uncertain whether the method generalizes to more complex or realistic scenes.\n2. The paper could be strengthened by including an analysis of potential failure cases that may arise during repeated zoom-in generations, such as semantic drift or geometric inconsistency across scales, to better understand the model's robustness.\n3. While the paper includes ablation studies on several key components, the contribution of each stage within the progressive detail synthesizer (e.g., super-resolution, editing, and depth registration) is not analyzed separately. A more detailed breakdown could help clarify how these stages jointly affect cross-scale coherence and visual quality."}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I79SuxaJCI", "forum": "nPcrFDHL2i", "replyto": "nPcrFDHL2i", "signatures": ["ICLR.cc/2026/Conference/Submission3585/Reviewer_PcQR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3585/Reviewer_PcQR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761785204521, "cdate": 1761785204521, "tmdate": 1762916845404, "mdate": 1762916845404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "WonderZoom achieves multi-scale 3d world generation which let the user to interactively zoom into one region of the scene, and explore contents. It proposes a scale-adaptive Gaussian surfels representation, which will selectively choose Gaussian surfels for rendering based on the current scale. Besides, for generative super resolution purpose, they propose a progressive refiner that add new zoomed-in details. And they perform test-time optimization over depth prior to ensure depth consistency across scales. Finally, VLM is used to support multi-view synthesis after getting into one zoom level. For experiments, the authors carefully compare their framework with several video-based generator (single-scaled), and showcases their quality is better. And they ablate their three design choices in terms of depth registration, scale-based selective Gaussian Surfel rendering, and multi-view auxiliary view with video prior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It is the first work that can generates extreme-scale 3d exploration. \n\n2. Generic and modular framework to support multi-scale scene generation, which is not restricted to any prior to use."}, "weaknesses": {"value": "1. Lack of clear practical use cases.\nWhile the proposed techniques are technically sound, their real-world utility remains unclear. The idea of generative, multi-modal content at extreme zooming scales (e.g., zooming into a flower to reveal a bug) appears visually interesting but lacks an evident practical motivation or application scenario.\n\n2. Inconsistency across scales.\nThe generated content exhibits noticeable discontinuities between scales. As shown in the qualitative results, fine-scale details are not preserved when zooming out. The use of scale-dependent Gaussian surfels, though computationally efficient, undermines 3D structural consistency across scales.\n\n3. Limited and unnatural interaction design.\nThe system does not support seamless, free-form interaction within a unified 3D scene. Users must explicitly select a scale level, after which the model hallucinates new content at that scale. This results in fragmented and unrealistic interactions—far from how people naturally perceive or explore multi-scale scenes.\n\n4. Lack of architectural coherence.\nThe framework integrates multiple independent components without a strong unifying principle. This design introduces redundancy and heavy reliance on various priors. For instance, auxiliary view synthesis is treated as a separate generation process rather than being jointly optimized with depth prediction across scales.\n\n5. Weak and incomplete baseline comparisons.\nThe paper compares against only trivial or loosely related baselines. Even though there are few existing 3D multi-scale generation methods, several strong 2D multi-scale models (e.g., EarthGen) could serve as relevant baselines. Incorporating simple depth priors into such 2D frameworks would likely yield more competitive multi-scale results."}, "questions": {"value": "1. What would happen if we use all Gaussian surfels for rendering? Will there be artifacts due to content inconsistency across scales? \n\n2. Why is it one interesting task for the community to explore? what is the implication of such generative super resolution task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VH1kVEvcg6", "forum": "nPcrFDHL2i", "replyto": "nPcrFDHL2i", "signatures": ["ICLR.cc/2026/Conference/Submission3585/Reviewer_qch5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3585/Reviewer_qch5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3585/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806537313, "cdate": 1761806537313, "tmdate": 1762916844935, "mdate": 1762916844935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}