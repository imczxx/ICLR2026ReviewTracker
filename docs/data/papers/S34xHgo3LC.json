{"id": "S34xHgo3LC", "number": 24583, "cdate": 1758358149097, "mdate": 1759896759641, "content": {"title": "Temporal  Difference Learning for Diffusion Models", "abstract": "Diffusion models are typically trained with reconstruction losses at single, isolated time steps, which does not enforce consistency between predictions along the denoising trajectory. This lack of cross-time consistency can degrade performance, especially for few-step samplers. We introduce a temporal difference (TD) objective that penalizes inconsistency of the model’s multi-step progress along the denoising path. By reformulating the diffusion process as a Markov reward process and casting the denoising task as a policy evaluation problem in reinforcement learning, we derive a unified TD approach that applies to both discrete- and continuous-time diffusion formulations. We further propose a principled sample-based reweighting method that stabilizes training. \n\nEmpirically, we show that adding our TD objective can significantly improve sample efficiency and enhance generative model quality, as measured by FID. In particular, TD exhibits stronger advantages when the number of sampling steps is small, highlighting its practical utility under low-computation-budget scenarios. We provide extensive ablation studies to justify our design choices, including loss reweighting, regularization weight, and one-step distance. Overall, our TD approach can be a general drop-in that enforces cross-time consistency and improves fixed-NFE generation quality, with potential utility across a wide range of diffusion generative models.", "tldr": "", "keywords": ["Diffusion Models", "Temporal-difference learning", "Generative models", "Reinforcement learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0fa655713a23976768ce21f9acaace87e8c8abed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a *Temporal Difference (TD)* learning objective for diffusion models. The core idea is that standard diffusion training optimized at isolated time steps does not enforce cross-time consistency, which harms performance for few-step samplers. \nThe authors recast the denoising trajectory as a *Markov Reward Process (MRP)* and treat training as a policy-evaluation problem, deriving a unified TD objective applicable to both discrete-time and continuous-time diffusion families. \nTo stabilize optimization across heterogeneous time pairs, they introduce a principled sample-based reweighting scheme $(w_{TD})$. \nThe TD objective is used as a drop-in regularizer combined with the standard EDM reconstruction loss. \nEmpirical results on CIFAR-10 (32$\\times$32) show consistent FID improvements in few-step (12 -18 step) regimes; gains in standard settings (e.g., large UNet, 18 steps) are present but marginal."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper offers an interesting connection between reinforcement learning (TD / policy evaluation) and diffusion-model training, applying RL tools to enforce  internal  temporal consistency rather than for external reward optimization.  \n\n 2. A two-time posterior-mean formulation is proposed, applicable across the DDPM, DDIM, VP, VE, and EDM families, and used to introduce the TD loss. \n\n3. The sample-pair reweighting $w_{TD}$ is introduced to address the differing scales introduced by pairing arbitrary time indices; ablations show its effect on stability and performance."}, "weaknesses": {"value": "1. All experiments are conducted on unconditional CIFAR-10 (32$\\times$32), and there is currently no evidence to suggest that the method scales to high-resolution images, conditional generation (e.g., class-conditional or text-conditioned tasks), or other modalities.  \n\n2. The paper only compares to baseline EDM and lacks comparison with other training-time methods (Consistency Models, Progressive Distillation), both conceptually and experimentally.  Additionally, the low-NFE gains are demonstrated only with the Heun sampler. \n\n3.  The paper shows mixed results: (1) For the large model at 18 steps, the improvement is from 2.3893 to 2.3768; (2) For the small model at 12 steps, the improvement is from 10.58 to 10.22.  Although these improvements are the paper's main claim, the computational overhead (training time, memory) is not quantified."}, "questions": {"value": "1. Equation (16) supports arbitrary $k$, but why was fix k=1 in all experiments, while the ablation study  for $k \\in \\{2,3,5\\}$ is missing?   Is $k=1$ optimal in practice?\n\n\n2. Does the proposed method have unique practical advantages and contributions in optimizing information flow, compared to the method in [1]? \n\n\n3. What is the relationship and specific difference between the proposed *Temporal Difference* and the *Temporal Dynamics*    discussed in [2] and [3]?  \n\n[1] Li, S., et al., EVODiff: Entropy-aware Variance Optimized Diffusion Inference, NeurIPS 2025. \n\n[2]. Floros, A., et al., Tracing the Roots: Leveraging Temporal Dynamics in Diffusion Trajectories for Origin Attribution, 2024.\n\n[3]. Guo, X., et al., Dynamical Diffusion: Learning Temporal Dynamics with Diffusion Models, ICLR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PO2iXN8hAE", "forum": "S34xHgo3LC", "replyto": "S34xHgo3LC", "signatures": ["ICLR.cc/2026/Conference/Submission24583/Reviewer_Zhiz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24583/Reviewer_Zhiz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760854063337, "cdate": 1760854063337, "tmdate": 1762943127610, "mdate": 1762943127610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a TD learning framework for diffusion model. Instead of the existing method that matches single time-step function evaluation against the ground truth data, the paper proposes to enforce the consistency between two time steps. Further, they propose a reweighting schedule for multiple time-step input. The authors compare the result of TD-assisted training against standard EDM, showing the gain in small NFEs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. It is not hard to understand the idea. The authors clearly state the contribution of the paper."}, "weaknesses": {"value": "1. The biggest concern of this paper is the significance of the approach: consistency model [1] came out in 2024, which distills the EDM with 1 or 2 time steps. While the proposed method claims the gain in some moderate NFEs, it is still larger than the recent generative model's performance. For my personal view, it is hard to claim the significance of the approach compared to the recently proposed method. Furthermore, the TD framework also reminds me of the consistency distillation, which has been showcased in previous work [1].\n\n2. The gain of the proposed method is also marginal. The proposed method fails to greatly improve EDM. \n\n3. The limited experiment on the CIFAR-10 is also a concern. Nowadays, the generation benchmark has shifted towards more complex distributions like ImageNet. I wonder if the gain of the TD framework is still relevant on that benchmark. The same applies to the architecture, whether the proposed method exihibits gains on the diffusion transformer architecture, too. \n\n4. I wonder the time efficiency of the proposed method, since the evaluation of the loss function requires multiple function calls.\n\n\n\n***References***\n\n[1] Consistency Models, ICML 2024."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3jbS7KsYBK", "forum": "S34xHgo3LC", "replyto": "S34xHgo3LC", "signatures": ["ICLR.cc/2026/Conference/Submission24583/Reviewer_utRC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24583/Reviewer_utRC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761390826549, "cdate": 1761390826549, "tmdate": 1762943127349, "mdate": 1762943127349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a temporal difference (TD) learning objective to address the lack of cross-time consistency in standard diffusion model training, which typically relies on single-step reconstruction losses. The authors derive a unified TD loss that enforces multi-step consistency along the trajectory and can be applied to both discrete and continuous-time diffusion formulations. Empirical results on CIFAR-10 demonstrate that combining this TD objective with standard training improves sample quality, particularly under low-compute sampling conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is original. While recent works have framed diffusion sampling as an RL problem (e.g., DDPO, DPOK) to optimize black-box rewards, this paper takes a distinct path that it reformulates the diffusion training itself as a ​policy evaluation​ problem within a Markov Reward Process (MRP). The key insight is to define a \"reward\" as the drift of the true posterior mean and a \"value function\" as the cumulative drift, leading to a Temporal Difference (TD) loss that directly penalizes inconsistencies in the model's denoising trajectory. \n- As far as I checked, this paper is technically sound. The authors carefully derive it starting with a discrete-time MRP formulation and then elegantly extending it to continuous time. A particularly strong aspect is the derivation of a ​principled, sample-based reweighting scheme​ (w_TD) to stabilize training.\n- The empirical evaluation, while limited to CIFAR-10, is rigorous. It includes ablations on key hyperparameters (mixing coefficient, stride, weighting), and the results consistently support the claims. The fact that the method provides a clear advantage in the low-NFE regime strengthens the validity of the approach.\n- The paper is generally well-written and clear, especially given the technical complexity of bridging these two domains. The use of a ​unified notation​ (Table 1) for different diffusion families is a major aid to understanding and helps demystify the differences between model types. Algorithm 1 provides a concrete summary of the training procedure."}, "weaknesses": {"value": "- This paper's central claim is that the TD approach is a \"general drop-in\" for a \"wide range of diffusion generative models.\" However, the experimental validation is confined exclusively to ​unconditional image generation on CIFAR-10 (32x32)​​ using a single architecture (SongUNet) and a single sampler family (EDM's probability-flow ODE).\n  - CIFAR-10 is a small, well-controlled dataset that is not representative of the complex, high-resolution data where diffusion models are most prominently used and where temporal consistency issues might be more pronounced. The failure to demonstrate the method on even a standard benchmark like ImageNet 64x64 or a conditional task (e.g., text-to-image) significantly weakens the claim of generality.\n- This paper mentions the \"additional constant-factor computational and memory overhead\" as a limitation but provides no data to quantify this cost. For a method aiming to improve efficiency, this overhead is a critical practical factor. \n  - We cannot evaluate the practical utility of the method without understanding its cost. How much does the target network and the additional forward/backward passes impact compute/memory usage? This lack of information makes it impossible to conduct a fair cost-benefit analysis.\n- The MRP formulation, while elegant, is presented more as an analogy than a deep theoretical connection. The \"reward\" is defined as a vector difference of posterior means, and the \"value function\" is the cumulative drift. This differs significantly from standard RL, where rewards are scalar and value functions represent expected cumulative reward."}, "questions": {"value": "How does your TD approach conceptually and empirically relate to Consistency Models (CM) [Song et al., 2023]? Both methods aim to improve temporal consistency, but CMs enforce a strict consistency function. Could you discuss the trade-offs? For instance, does your method offer advantages in sample quality or training stability compared to CM distillation, especially in the very low NFE regime (e.g., NFE < 10)?\n\nCould you provide concrete numbers for the training time and memory usage increase (%) when adding the TD objective compared to the baseline EDM, for both the small and large model configurations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rQ2Mn13Uir", "forum": "S34xHgo3LC", "replyto": "S34xHgo3LC", "signatures": ["ICLR.cc/2026/Conference/Submission24583/Reviewer_8bya"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24583/Reviewer_8bya"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818109916, "cdate": 1761818109916, "tmdate": 1762943127151, "mdate": 1762943127151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a TD-learning based method for training diffusion models. Numerical experiments are conducted to demonstrate the effectiveness of the proposed method. Ablation studies are also included."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces TD-learning method for training the score network.\n2. The authors conducted numerical experiments for their methods."}, "weaknesses": {"value": "I have several major concerns:\n1. The problem studied in this paper is not clearly stated. The authors cited RL-based approaches for post-training/fine-tuning of diffusion models while they compare their approach with EDM in numerical study. This makes me confused the goal of this work.\n2. Following 1), if the goal is fine-tuning, the authors should include comparison to other methods such as DPOK. If the goal is pre-training of score networks, I am very concerned the computational cost of adding a RL block in this stage. At least from experiments, the performance gap compared to EDM is not that significant.\n3. The interpretation of (14) is not that convincing to me. One can rewrite (14) as $ \\delta_t  = (\\mu_{t-1}^{true} - \\mu_{\\theta, t-1}) - (\\mu_{t-2}^{true} - \\mu_{\\theta', t-2}) $. In this way, forcing $ \\delta_t $ small is forcing all the score matching error small *uniformly* over all time steps. I am not sure if this is possible theoretically and if this is indeed necessary. I suggest the authors carefully elaborate this point."}, "questions": {"value": "One minor question:\n1. Why do you still add EDM loss to the final loss function? Could you explain how much could you gain with and without this term?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FPxXq8N2jG", "forum": "S34xHgo3LC", "replyto": "S34xHgo3LC", "signatures": ["ICLR.cc/2026/Conference/Submission24583/Reviewer_hRXk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24583/Reviewer_hRXk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762207030348, "cdate": 1762207030348, "tmdate": 1762943126945, "mdate": 1762943126945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}