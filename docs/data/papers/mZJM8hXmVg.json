{"id": "mZJM8hXmVg", "number": 20215, "cdate": 1758303754964, "mdate": 1759896990128, "content": {"title": "Advancing Multimodal Fusion on Heterogeneous Data with Physics-inspired Attention", "abstract": "Multimodal fusion learning paradigm has shown great potential in various fields such as Medicine, Science, Engineering, etc. as it offers a framework to jointly learn from heterogeneous data sources. Such learning paradigm is extremely desirable in medical domain, where AI models are expected to perform various healthcare tasks by learning from disparate data modalities such as imaging, clinical records, and omics. Existing multimodal fusion strategies face several major challenges. First, they struggle to capture complex cross-modal interactions effectively. Second, they are often specialized to specific modalities, i.e., either imaging, omics, etc. hence hindering their adaptability and generalizability to other modalities. Finally, they incur high computational costs, restricting their applicability in resource-constrained healthcare AI. To address these challenges, we propose a novel multimodal learning framework – Efficient Hybrid fusion Physics-informed Attention Learning Network (EHPAL-Net), which is a lightweight and scalable framework that integrates various modalities through novel Efficient Hybrid Fusion (EHF) layers. Each EHF layer initially captures rich modality-specific multi-scale spatial information, followed by a Physics-informed Cross-modal Fusion Attention module to model fine-grained, structure-preserving cross-modal interactions, thereby learning robust complementary shared representations. Furthermore, EHF layers are sequentially learned for each modality making them adaptable and generalizable. Extensive evaluations on 15 public datasets show that EHPAL-Net outperforms leading multimodal fusion methods, boosting performance by up to 3.97% and lowering computational costs by up to 87.8%, ensuring more effective and reliable predictions.", "tldr": "", "keywords": ["Medical Imaging", "Multimodal Fusion", "Attention", "Deep Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa3d1380d7b5b9de049a948bef39782e6c22727d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a physics-inspired attention learning network for the multimodal fusion learning (MFL) task. Specifically, the authors targeted at multimodal tasks in medical context, considering the challenge that different modalities do not share semantics. The key design is their efficient hybrid fusion (EHF) layer, where a physics-inspired attention module is used to capture cross-modality interactions. Different from existing early / late fusion methods, the authors proposed a sequential structure where the input modalities are integrated one by one. The overall architecture is a little complicated, but unique and novel. The authors claimed they solved three main challenges - performance, efficiency, and generalization. But the reviewer do has some questions on some aspects."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed EHF layer, especially the Physics-inspired Cross-modal Fusion Attention (PCMFA) module, is both novel and effective. By jointly optimizing cross-modal interactions in hyperbolic and quantum spaces, the network captures rich structural relationships across different modalities. This provides an effective way in combing the advantages of hyperbolic neural networks and quantum neural networks.\n2.  The performance of the proposed model is supported by the comparison with state-of-the-art single-modal and multi-modal learning methods, and the generalization of the model is demonstrated by datasets from various sources and modalities.\n3. The effectiveness of the key components are supported by both theoretical analysis and ablation studies."}, "weaknesses": {"value": "1. Concerns on performance comparison. In Table 2 we can see that existing SOTA methods already achieved high performance (with ACC over 98% and AUC over 99%) on many datasets. The concern is whether using those datasets could strongly support the author's claim that the \"performance\" challenge is solved. More results and datasets like \"BRCA\" and \"ICD9\" would be more persuasive.\n2. Concerns on efficiency. The authors used two metrics, number of parameters (#P) and floating-point operations (#F) to show and compare the models' efficiency. However, the efficiency of the proposed method is only significant when using Shuffle-Net as backbone. Since most of the compared methods are using ResNet as their backbones, I am concerned whether this would be a fair comparison. Considering the fact that the proposed EHPAL-Net mainly achieved highest performance with a ResNet50 backbone, this issue is even more significant."}, "questions": {"value": "1. It is a little unclear how the experiments are designed and evaluated. The focus of this paper is multimodality fusion, which works on combining data from different modalities for the final task. But the results are reported for per dataset, which may cause confusion for readers.\n2. Considering the efficiency, it would be better if a fairer comparison could be provided. And it is also recommended to provide more metrics, e.g., running time and memory consumption.\n3. I can see that the authors spent a lot of efforts on both model development and the experiments. From the perspective of both a reader and a reviewer, there are too many contents included here, especially as a conference paper. For example, since there are three main components and each of them are constructed by even deeper hierarchies, the readers will need to check different appendices frequently on how each module works and the related ablation studies. The contents of this work seem like a better fit for top-tier journals."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0B7lelYUrG", "forum": "mZJM8hXmVg", "replyto": "mZJM8hXmVg", "signatures": ["ICLR.cc/2026/Conference/Submission20215/Reviewer_jkx9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20215/Reviewer_jkx9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861388213, "cdate": 1761861388213, "tmdate": 1762933714475, "mdate": 1762933714475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EHPAL-Net, a multimodal fusion framework designed to improve the efficiency and generalizability of multimodal learning, particularly for medical applications. The method introduces EHF layers that first capture modality-specific multi-scale features and then apply a physics-informed cross-modal attention to model fine-grained, structure-preserving interactions across modalities. Experiments on 15 heterogeneous medical datasets demonstrate improved performance and reduced computational cost compared to existing fusion methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The experimental evaluation is extensive, covering 15 heterogeneous medical datasets, which provides strong empirical evidence of robustness.\n\n- The proposed method is conceptually novel, combining hybrid fusion and physics-informed attention in a lightweight, scalable architecture."}, "weaknesses": {"value": "- The claimed challenge that existing methods are “specialized to specific modalities” lacks solid justification. Many recent multimodal frameworks are modality-adaptable and can generalize by substituting modality-specific encoders.\n\n- The scope and positioning of the paper are ambiguous: although the title and introduction suggest a general-domain multimodal framework, all experiments are performed solely on medical datasets. The three challenges identified (cross-modal interaction, modality specialization, and computational cost) are generic and not uniquely tied to healthcare AI. So is the method design. The paper would benefit from either (i) reframing the motivation specifically around clinical multimodal learning, or (ii) extending experiments to both clinical and general-domain multimodal datasets to justify broader claims.\n\n- The methodology lacks clear conceptual grounding. The design choices are not well-connected to the stated challenges, and lack strong problem-driven justification.\n\n- The related work section lacks discussion of more recent and relevant approaches in clinical multimodal fusion, particularly those that explicitly model the interplay between modality-specific and modality-shared representations, which could provide valuable conceptual and empirical comparisons."}, "questions": {"value": "Minor comment: \n- The purpose of the citation of paper in line 456, page 9 is unclear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "laQGUBhiy1", "forum": "mZJM8hXmVg", "replyto": "mZJM8hXmVg", "signatures": ["ICLR.cc/2026/Conference/Submission20215/Reviewer_uRBh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20215/Reviewer_uRBh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984864875, "cdate": 1761984864875, "tmdate": 1762933713936, "mdate": 1762933713936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Efficient Hybrid-fusion Physics-inspired Attention Learning Network (EHPAL-Net), a lightweight and scalable multimodal fusion framework designed for heterogeneous biomedical data, such as imaging, multi-omics, and EHR. Its Efficient Hybrid Fusion (EHF) layer sequentially integrates modalities through: 1) Efficient Multimodal Residual Convolution (EMRC) for multi-scale spatial representations, 2) Physics-inspired Cross-Modal Fusion Attention (PCMFA) combining hyperbolic and quantum-inspired attention to model complex cross-modal interactions, and 3) Shared Information Refinement (SIR) for representational diversity. A large number of heterogeneous medical datasets are used to validate the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The problems to be addressed, the performance, generalization, and efficiency of multimodal fusion learning, are significant.\n- The integration of the hyperbolic dual-geometry attention has not been seen in the field of multimodal learning, and it looks novel to me.\n- The reported reduction in the number of model parameters (98.3%) and FLOPs (97.6) is impressive."}, "weaknesses": {"value": "- The presentation of the paper could be improved. As a researcher working on multimodal learning, I find it difficult to follow the logic of the paper, which looks to me quite diffuse and redundant. Key ideas such as \"physics-inspired attention\", \"dual-geometry interaction\",  and \"hierarchical structure preservation\" are mentioned repeatedly, but I could not find their precise definitions or theoretical justifications. The narrative often cycles through the same claims without further clarification and justification of the mechanisms enabling the desired effects.\n- The proposed framework claims to be \"physics-inspired\", but the connection to physical principles appears largely metaphorical rather than mechanistic. The framework design and choice of the submodules are not evidently grounded in physical modeling.\n- I do not understand the exact meaning of \"modality\" in this paper. In lines 450, for example, it seems to imply that HAM10000, SIPaKMeD, and PathMNIST are treated as three modalities, each with its distinct data sources and label sets. The source codes provided by the authors seem to confirm my guess. The usual setting of multimodal fusion means that one sample has data of different modalities, e.g., a patient has a CT scan image and a pathological image, where information from these modalities is combined to make a prediction for the sample concerned. I think the exact definition of \"modality\" in this paper should be explicitly defined, and what the modalities are in each dataset should be clearly detailed."}, "questions": {"value": "- Please see the weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5JXy3DkI6z", "forum": "mZJM8hXmVg", "replyto": "mZJM8hXmVg", "signatures": ["ICLR.cc/2026/Conference/Submission20215/Reviewer_7JKi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20215/Reviewer_7JKi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989818741, "cdate": 1761989818741, "tmdate": 1762933713645, "mdate": 1762933713645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}