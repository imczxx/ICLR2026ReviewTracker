{"id": "GMP1S4R6Ke", "number": 4989, "cdate": 1757828679053, "mdate": 1759898001429, "content": {"title": "LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing", "abstract": "Recent attempts to combine low-rank adaptation (LoRA) with mixture-of-experts (MoE) for multi-task adaptation of Large Language Models (LLMs) often replace whole attention/FFN layers with switch experts or append parallel expert branches, undermining parameter efficiency and limiting task specialization.  We introduce **LoRA-Mixer**, a modular MoE framework that routes task-specific LoRA experts into the core projection matrices of the attention module (input/output linear layers), rather than primarily targeting FFN blocks. The design delivers fine-grained token-level specialization by fully exploiting the attention mechanism, while remaining drop-in compatible with Transformers and state-space models (SSMs) as the linear projection layers are ubiquitous. To train robust routers from limited data while promoting stable, selective decisions and high expert reuse, **LoRA-Mixer** employs an adaptive **Routing Specialization Loss (RSL)** that jointly enforces global load balance and input-aware specialization via an entropy-shaping objective. The framework supports two regimes: (i) joint optimization of adapters and router with a differentiable hard–soft top-k routing scheme, and (ii) plug-and-play routing over frozen, pre-trained LoRA modules sourced from public repositories.  Across 15 benchmarks—including MedQA, GSM8K, HumanEval, and GLUE—RSL-optimized LoRA-Mixer outperforms state-of-the-art routing and LoRA-MoE baselines while using 48% of their trainable parameters, with gains of +3.79%, +2.90%, and +3.95% on GSM8K, CoLA, and ARC-C, respectively. Cross-model transfer and adapter reuse experiments further demonstrate the approach’s versatility and data efficiency.", "tldr": "", "keywords": ["MoE Optimization", "LLM", "Low resource adaptation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/52cb0ffe807e5ba4a52895d4e8b0c9c97e375d2e.pdf", "supplementary_material": "/attachment/1f67db1750b96b939e6f7cde28855d364e6d6de5.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces LoRA-Mixer, a modular mixture-of-experts (MoE) framework for combining multiple LoRA adapters through serial attention routing at the projection layers of Transformers or state-space models (SSMs). Instead of replacing full attention or FFN blocks, LoRA-Mixer injects multiple LoRA “experts” directly into the linear projections and learns a lightweight router optimized by a novel Routing Specialization Balance Loss (RSL). RSL integrates entropy-based specialization and global load balancing, encouraging both expert diversity and stability. The authors claim that this approach achieves fine-grained token-level routing with substantially fewer trainable parameters. Experiments show consistent gains over LoRA-MoE baselines such as MixLoRA, MoLE, and LoRAHub while using only ~48 % of their parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) the paper introduces a simple yet effective way to route LoRA experts through attention projections, improving modularity without large architectural changes.\n\n(2) The proposed RSL loss is a reasonable refinement that balances specialization and load, supported by clear theoretical grounding.\n\n(3) Experiments across multiple models and tasks show consistent, reproducible gains with good parameter efficiency."}, "weaknesses": {"value": "(1) The efficiency claim relies mainly on the “48% trainable parameters” figure, without corresponding latency, throughput, FLOPs, or memory comparisons under matched hardware conditions.\n\n(2) The paper does not clarify whether routers are defined per-projection (Q/K/V/O), shared per layer, or applied only to selected layers, which affects both computational cost and specialization behavior.\n\n(3) The influence of RSL coefficients (λ, α), preservation regularizer β, and Top-K settings is not systematically reported; the cited appendices lack quantitative curves or trend analyses.\n\n(4) Figures primarily show average expert load across tasks rather than per-token entropy or variance, leaving the “input-aware routing” claim insufficiently supported.\n\n(5) Transfer from Mistral-7B to LLaMA3-8B improves GSM8K but slightly degrades ARC-E, indicating partial rather than robust transferability.\n\n(6) While several routing-loss baselines are included, recent MoE-LoRA variants from 2024–2025 (e.g., DynMoLE, MORAL) under matched protocols are missing, limiting external comparison."}, "questions": {"value": "(1) Could the authors provide runtime measurements (training/inference latency, FLOPs, or GPU memory) under matched hardware?\n\n(2) Please clarify whether the router operates per-projection (Q/K/V/O), shared per layer, or selectively on certain layers. \n\n(3) Could the authors include quantitative ablations or trend plots showing the sensitivity of RSL coefficients (λ, α), preservation regularizer β, and Top-K to model performance? \n\n(4) Would it be possible to visualize token-level routing entropy or variance to directly support the claim of input-aware specialization?\n\n(5) In Table 5, performance improves on GSM8K but drops slightly on ARC-E when transferring from Mistral-7B to LLaMA3-8B. Could the authors analyze why transferability differs across tasks or layers?\n\n(6) Would the authors consider adding results for recent MoE-LoRA variants (e.g., DynMoLE 2025, MORAL 2024) under identical data and training setups to strengthen external comparisons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0QyO0Mg8BD", "forum": "GMP1S4R6Ke", "replyto": "GMP1S4R6Ke", "signatures": ["ICLR.cc/2026/Conference/Submission4989/Reviewer_EiuV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4989/Reviewer_EiuV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760991772656, "cdate": 1760991772656, "tmdate": 1762917811907, "mdate": 1762917811907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LoRA-Mixer, a modular mixture-of-experts (MoE) framework that coordinates multiple LoRA adapters within the linear projection layers of attention or state-space models.\nIt proposes a new Routing Specialization Balance Loss (RSL) to balance expert load while promoting input-aware specialization through entropy regularization.\nThe method supports plug-and-play reuse of pre-trained LoRA modules and demonstrates strong performance and data efficiency across 15 benchmarks such as GLUE, GSM8K, and HumanEval.\nResults show LoRA-Mixer achieves higher accuracy with 48% fewer trainable parameters compared to existing LoRA-MoE baselines and transfers well across architectures (e.g., from Mistral-7B to LLaMA3-8B)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Novel modular design: Routes LoRA experts directly through projection layers, improving attention specialization and maintaining compatibility with both Transformers and SSMs.\n\n+ Effective routing optimization: The proposed RSL loss theoretically and empirically stabilizes training while enhancing expert selectivity and data efficiency.\n\n+ Comprehensive evaluation: Strong empirical gains on diverse benchmarks and clear demonstrations of cross-model transferability and low-data robustness."}, "weaknesses": {"value": "1. **Layer placement strategy is unclear and may be sub-optimal.**\nLoRA-Mixer is described as being “applied to the linear projection layers” and claimed to be architecture-agnostic, but there is no systematic ablation to determine which layers benefit most or whether per-layer routing depth should vary. The authors choose to integrate the router into the attention projection layers (Q/K/V/O) rather than the FFN projections, yet this design choice is not well justified.\nWhile the motivation seems to be leveraging attention’s semantic sensitivity and avoiding interference with task-specific FFN adaptation, most prior LoRA-MoE and MixLoRA works inject modular experts into the FFN for scalability and expressive power. A comparative analysis of applying LoRA-Mixer in attention vs. FFN projection layers—under equal compute and parameter budgets—would help validate whether the current design is optimal. Moreover, the paper itself notes that using a uniform Top-K across layers may introduce redundancy, suggesting this design space remains underexplored.\n\n2. **Lack of evidence for scalability to larger base models.**\nAll experiments are conducted on small-to-medium-scale LLMs such as Mistral-7B, LLaMA3-8B, and Falcon-Mamba-7B, with limited discussion of scalability to models exceeding 30B parameters. It remains unclear whether LoRA-Mixer maintains routing stability, expert load balance, and training efficiency at larger scales.\nWhen scaling up, the authors are encouraged to provide empirical results or analysis on larger LLMs, examining aspects such as training/inference latency, memory overhead, convergence speed, and RSL behavior. Demonstrating consistent benefits in larger architectures would significantly strengthen the paper’s generality and practical value.\n\n3. **Missing complexity and parameter-efficiency comparison with LoRA, MixLoRA, and MoLE.**\nWhile performance comparisons are extensive, the paper lacks a quantitative analysis of trainable parameters, FLOPs, memory usage, and inference latency across methods such as LoRA, MixLoRA, and MoLE. Without aligning these metrics, claims of data or parameter efficiency cannot be fairly assessed.\nIt would be valuable to include a table summarizing, for each baseline, the number of trainable parameters, computational cost (FLOPs), and peak memory footprint, as well as an efficiency–accuracy trade-off curve under comparable resource budgets. This would make the results more interpretable and confirm whether LoRA-Mixer’s gains arise from better design rather than higher capacity or computation."}, "questions": {"value": "**Question on benchmark selection (no extra experiments required)**\n\nCould the authors clarify the rationale for the main-table task set (Medical, CoLA, SST-2, GSM8K, ARC-E, ARC-C, HumanEval)? In practice, we’ve observed high variance on some GLUE-style tasks (e.g., CoLA, SST-2, RTE), which can blur comparative conclusions. It would help to include a short analysis addressing:\n+ Why these tasks? Please explain how they map to the paper’s goals\n+ Variance and reliability. Do these tasks provide stable rankings under multiple seeds? If possible, comment on typical standard deviations or confidence intervals you observe (no new runs needed—prior experience/known stats are fine).\n+ Difficulty/calibration. Why not include more complex and widely used suites like MMLU, math benchmarks, or additional code benchmarks? If excluded, a brief justification (e.g., cost, mismatch with claimed capability, or lack of compatible LoRA experts) would make the scope clearer.\n\nIf the above concerns can be reasonably addressed or clarified in a revision, I would be inclined to raise my overall score"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z0zYuENumH", "forum": "GMP1S4R6Ke", "replyto": "GMP1S4R6Ke", "signatures": ["ICLR.cc/2026/Conference/Submission4989/Reviewer_A4gH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4989/Reviewer_A4gH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848493122, "cdate": 1761848493122, "tmdate": 1762917810756, "mdate": 1762917810756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To enhance the capability of expert specialization in Mixture-of-Experts (MoE) architecture models, a method called LoRA-Mixer is proposed. LoRA-Mixer incorporates multiple LoRA modules as experts into the linear projection layer, enabling adaptation to models with either Transformer or SSMs architectures. Additionally, to balance global load balancing in routing and expert specialization, an RSL loss is introduced. This loss adds a token-level entropy term (at the token level) to the existing auxiliary term for global load balancing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. By treating multiple LoRA modules as experts and integrating them into the linear projection layer, the method can adapt to models with either Transformer or SSMs (State Space Models) architectures.\n2. The RSL loss is proposed to balance global load balancing in routing and expert specialization.\n3. Experiments conducted on models with different frameworks have validated the effectiveness of LoRA-Mixer."}, "weaknesses": {"value": "1. The authors added LoRA-Mixer to both the input projection layer and the output projection layer, but they did not further analyze the respective contributions of these two parts to the experimental results in their experiments.\n2. The authors haven't provided the code, so it's impossible to determine whether the results can be reproduced."}, "questions": {"value": "Can the authors provide further ablation experiments on adding LoRA-Mixer to the input projection layer and the output projection layer respectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qrvM67Yr7t", "forum": "GMP1S4R6Ke", "replyto": "GMP1S4R6Ke", "signatures": ["ICLR.cc/2026/Conference/Submission4989/Reviewer_RivB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4989/Reviewer_RivB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4989/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762239700353, "cdate": 1762239700353, "tmdate": 1762917809647, "mdate": 1762917809647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}