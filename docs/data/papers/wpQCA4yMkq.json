{"id": "wpQCA4yMkq", "number": 3583, "cdate": 1757481971947, "mdate": 1763051986653, "content": {"title": "JenBridge: Adaptive Long-Form Video Soundtracking across Scene Transition", "abstract": "We address the challenge of generating high-fidelity, long-form soundtracks that remain coherent across scene transitions. Existing AI music systems are mainly designed for short, isolated clips and lack mechanisms to ensure narrative continuity. We present \\texttt{JenBridge}, a modular and interpretable framework for adaptive long-form video soundtracking that ensures both high-fidelity audio generation and transition naturalness. The core architecture is a Transformer-based generative model trained with a flow-matching objective, following a two-stage paradigm: pretraining on large-scale text–audio corpora to establish robust musical priors, then adapting to the video domain with dual text–visual conditioning for precise cross-modal alignment. Crucially, to achieve long-form coherence across diverse scene changes, \\texttt{JenBridge} incorporates a novel adaptive transition mechanism. This system features a versatile toolkit of transition styles, including a generative transition method, and uniquely employs a Large Language Model (LLM) Agent that acts as a director to select the most appropriate transition for each narrative shift intelligently. To rigorously assess this task, we propose the LVS Benchmark, a new benchmark that includes a curated dataset and novel evaluation metrics focusing on holistic and transition-aware assessment. Extensive experiments on the proposed benchmark demonstrate that \\texttt{JenBridge} significantly outperforms existing methods in both objective and subjective metrics, particularly in terms of transition naturalness and overall narrative coherence. JenBridge represents a significant step towards fully automated, professional-quality video soundtracking. The codes and benchmark will be made publicly available.", "tldr": "A video-to-music diffusion-based framework to generate arbitrary-length high-fidelity music waveforms with smooth transition.", "keywords": ["video-to-music generation; video soundtracking; music generation; diffusion model; generative models; transition"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6f3b80b651b61790240d82df9e0432ec3f286512.pdf", "supplementary_material": "/attachment/41b97d45353a9809b5277903e87aabe45e2c26d4.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents JenBridge, a framework for generating long-form video soundtracks. It segments video, generates music per segment using a video-aware MMDIT, and introduces an adaptive transition mechanism. This mechanism uses an LLM Agent to select the optimal transition style to connect segments across scene changes. It introduce a comprehensive benchmark with rich annotations and a holistic evaluation protocol for long-form video soundtracking."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work tackles video-to-music temporal alignment from a novel perspective: segmenting the video, generating music per scene, and then adaptively transitioning between these segments to form a complete soundtrack .\n2. The proposed JenBridge framework offers a modular and interpretable approach to the complex task of long-form video soundtracking.\n3. The paper introduces the LVS Benchmark, a comprehensive resource featuring rich annotations and a holistic evaluation protocol specifically designed for long-form video soundtracking ."}, "weaknesses": {"value": "1. The supplementary materials appear incomplete, lacking full comparative results for all examples and providing insufficient evidence to substantiate claims of coherence and fidelity across diverse transitions (only 3 examples).\n2. Applying transitions only at segment boundaries may disrupt overall musical integrity, with examples showing potentially unnatural abrupt shifts.\n3. Comparisons with recent SOTA methods like Diff-BGM [4], MuMu-LLaMA [1], GVMGen [2], and CCCG [3] are missing.\n4. Evaluation is confined to the proposed LVS benchmark, lacking validation on other public datasets (e.g., BGM909 [4], SymMV [5]).\n5. Key quantitative metrics such as FAD and KL divergence are absent from the objective evaluation.\n\n[1]  MuMu-LLaMA: Multi-modal Music Understanding and Generation via Large Language Models\n\n[2] GVMGen: A General Video-to-Music Generation Model with Hierarchical Attentions\n\n[3] Customized Condition Controllable Generation for Video Soundtrack\n\n[4] Diff-BGM: A Diffusion Model for Video Background Music Generation\n\n[5] Video Background Music Generation: Dataset, Method and Evaluation"}, "questions": {"value": "1. Considering the variability inherent in generative music models (dependence on random seeds), how does the segment-wise generation and stitching methodology maintain global musical coherence across potentially disparate, independently generated segments, not just ensuring smooth local transitions?\n2. Who performed the manual selection for the LVS benchmark curation ? Did this involve professionals with audio-visual production expertise?\n3. Are all conditioning inputs (sequence text, global text, visual features) necessary ? Could ablations demonstrate their individual contributions?\n4. Why were some metrics omitted from the ablation study results in Table 2 ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "C4WNoJVvJt", "forum": "wpQCA4yMkq", "replyto": "wpQCA4yMkq", "signatures": ["ICLR.cc/2026/Conference/Submission3583/Reviewer_ikiE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3583/Reviewer_ikiE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761312847030, "cdate": 1761312847030, "tmdate": 1762916844068, "mdate": 1762916844068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "ScHzIKfaPb", "forum": "wpQCA4yMkq", "replyto": "wpQCA4yMkq", "signatures": ["ICLR.cc/2026/Conference/Submission3583/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3583/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763051985453, "cdate": 1763051985453, "tmdate": 1763051985453, "mdate": 1763051985453, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents JenBridge, a framework for adaptive audio transitions between video-clips that enables high-quality long-form video soundtrack generation. JenBridge consists of a video segmentation module, a foundational text-to-music model trained on a private dataset, and an adaptive transition module orchestrated by a LLM agent that considers both visual captions and musical prompts. Generative transition technique leverages ControlNet-based inpainting model, interpolating between adjacent clips using slerp on text embeddings and latent representations. Additionally, the authors propose LVS Benchmark, constructed from videos filtered from the MMTrail dataset, accompanied by scene boundary annotations and visual description generated by captioning model. JenBridge showed state-of-the-art generated soundtrack quality on both objective and subjective evaluations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. JenBridge can generate long-form soundtrack for videos of arbitrary length using its adaptive transition module.\n\n2. JenBridge outperforms baseline models in long-from soundtrack generation on both video-music alignment metric and musical aesthetics metric.\n\n3. The authors successfully fine-tuned LLM/VLMs for V2M task and integrated them into JenBridge."}, "weaknesses": {"value": "1. In the introduction, the authors point out that previous V2M pipelines were typically unmodifiable, giving users less creative control on intermediate steps or the final output. They then state JenBridge tries to address this limitation. However, it is still unclear in what sense the users can creatively modify or control the intermediate phase within the JenBridge framework.\n\n2. In the abstract, the authors state that LVS Benchmark contains novel metrics, but all the objective metrics in the benchmark were preexisting ones. (ImageBind Score, Meta Audiobox Aesthetics)\n\n3. The Transition Naturalness metric in user study for CMT_s is 1.6 and VidMuse_s is 1.8. In ablation studies, the transition metric for JenBridge with only Abrupt Cut is 2.8. Why do the authors think JenBridge without adaptive transition outperforms CMT_s and VidMuse_s, even thought they all use same transition methods? In my opinion, if Transition Naturalness is a metric for evaluating the transition smoothness and appropriateness as explained by the authors, evaluated results for these models should not differ very much.\n\n4. An ablation study of dropping both Adaptive Transition and Visual Conditioning would better demonstrate the effectiveness of core contributions of JenBridge, since the private text-to-music model alone already seems to improve performance a lot."}, "questions": {"value": "Many details are missing in the paper, hindering full understanding of the proposed approach and its experiments. For examples, the following questions should be elaborated:\n\n1. What are the ratios of selected transition types?\n2. Why did the authors interpolate between latent representations L_{k, i} and L_{k+1, N-i+1}, not L_{k, i} and L_{k+1, i} or other choice of pairs?\n3. How many videos were evaluated by participants in the user study?\n4. How did the metrics omitted in Table 2 compared to those in Table 1 change in the ablation study?\n5. Are the durations of transitions fixed? What are the durations for each transition?\n6. It would be nice to provide a demo of generated results by JenBridge, compared on other baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D0yzsg0QVS", "forum": "wpQCA4yMkq", "replyto": "wpQCA4yMkq", "signatures": ["ICLR.cc/2026/Conference/Submission3583/Reviewer_Z7vX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3583/Reviewer_Z7vX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477671418, "cdate": 1761477671418, "tmdate": 1762916843685, "mdate": 1762916843685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of generating coherent, long-form musical soundtracks for videos with dynamic scene changes. The authors argue that existing video-to-music models are primarily designed for short, isolated clips and lack mechanisms for ensuring narrative continuity across transitions.\n\nTo solve this, they propose JenBridge, a modular framework that operates in a three-stage process. First, it segments a long video into a sequence of semantically coherent clips. Second, a video-aware diffusion model generates a distinct piece of music for each individual clip. The core contribution lies in the third stage: an adaptive transition mechanism. This mechanism employs a LLM Agent, acting as a \"director,\" to intelligently select the most appropriate transition style from a versatile toolkit to connect the music segments.\n\nTo facilitate evaluation, the paper also introduces the Long-form Video Soundtrack (LVS) Benchmark, a benchmark specifically designed for this task. The authors claim that on this benchmark, JenBridge significantly outperforms existing methods in both objective and subjective metrics, particularly in transition naturalness and overall narrative coherence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "*   **Well-Motivated Problem Formulation**: The paper is motivated by a significant and practical challenge in video-to-music generation: creating coherent, long-form soundtracks that adapt across dynamic scene transitions. This provides a clear and relevant goal for the work.\n\n*   **A New Approach for Music Transition**: The paper introduces an adaptive transition mechanism directed by an LLM Agent. The concept of using an LLM to select from a toolkit of transition styles based on narrative context is a distinct contribution. This method introduces a layer of semantic reasoning to the process of stitching music segments.\n\n*   **Modular and Interpretable Architecture**: The JenBridge framework is designed in a modular fashion (segmentation, generation, transition). This architectural choice makes the system interpretable and allows for potential control over intermediate steps, which aligns with the stated goal of creating a practical tool."}, "weaknesses": {"value": "While the motivation is interesting, the paper's contribution is undermined by severe weaknesses in its evaluation, methodology, and overall academic rigor.\n\n1.  **Insufficient Experimental Evaluation**\n\n    The paper's performance claims are unsubstantiated due to an incomplete experimental setup.\n\n    *   **Missing Standard Metrics:** The evaluation omits crucial, community-standard audio quality metrics like Fréchet Audio Distance (FAD). This absence makes a direct and objective comparison of the model's audio fidelity against prior work difficult and leaves the assessment of quality incomplete..\n    *   **Limited Generalization on Established Benchmarks:** The method is not evaluated on existing long-form video-to-music benchmarks (e.g., **SymMV**, **V2M-bench**), raising questions about its generalization capabilities. Evaluating solely on LVS prevents a clear understanding of how the model performs relative to the state of the art in established settings.\n    *   **Confounded Baseline Comparison:** The comparison is uninformative as it conflates the contribution of the proposed transition logic with the underlying generator's performance. A proper ablation would apply the same transition logic to baselines to disentangle these effects.\n\n2.  **Weaknesses in Methodological Justification**\n\n    The paper primarily describes a system's architecture without providing sufficient scientific justification or empirical validation for its core design choices.\n\n    *   **Unmotivated Architectural Complexity:** The choice of a complex, two-stage VLM-LLM pipeline over a more direct, single end-to-end model (e.g., Qwen-VL) is not adequately justified. The architecture appears ad-hoc and is not supported by ablation studies that demonstrate the superiority of this specific, more complex configuration over simpler alternatives.\n    *   **Outdated Components:** The use of a 2022 EnCodec model is a significant limitation, as modern, higher-fidelity codecs could substantially alter the results.\n    *   **Unfulfilled Premise:** The paper claims to address the \"unmodifiable\" nature of prior models but fails to demonstrate how its complex pipeline offers any practical improvement in user controllability.\n\n3.  **Poorly Motivated Benchmark**\n\n    The necessity of the new LVS benchmark is not established. The authors do not differentiate it from existing benchmarks like **SymMV** or explain what unique scientific questions it addresses.\n\n4.  **Lack of Academic Rigor**\n\n    *   **Citation Errors:** The paper contains frequent and noticeable citation errors.\n    *   **Report-Style, Not Research:** The overall presentation lacks the analytical depth, rigorous ablations, and generalizable insights expected of a research paper, reading instead like a system description."}, "questions": {"value": "1.  **On Evaluation Rigor**: The evaluation lacks standard audio quality metrics (like FAD/FD) and is not benchmarked on any existing long-form video datasets (like SymMV or V2M-bench). Can you provide these results to properly situate your work and validate its performance?\n\n2.  **On the LVS Benchmark's Contribution**: Could you clarify the necessity of the LVS benchmark over existing long-form benchmarks? What unique aspects does it evaluate that prior benchmarks do not?\n\n3.  **On Architectural Justification**: What is the motivation for using a complex VLM-to-LLM pipeline instead of a single, more direct vision-language model (e.g., Qwen-VL)? Can you provide an ablation or rationale to support this specific design choice?\n\n4.  **On Isolating the Transition Module's Contribution**: To provide a fairer assessment and isolate your core contribution, could you apply your proposed LLM-directed transition mechanism to the outputs of a strong baseline generator like AudioX and report the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZafMzJUglA", "forum": "wpQCA4yMkq", "replyto": "wpQCA4yMkq", "signatures": ["ICLR.cc/2026/Conference/Submission3583/Reviewer_KL2U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3583/Reviewer_KL2U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857344481, "cdate": 1761857344481, "tmdate": 1762916843096, "mdate": 1762916843096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a video sound-tracking generation pipeline for long videos across scene transitions. The music generation model has a two-stage design: a pretrained text-to-music generation model, and a visual-to-music prompt translator based on a Qwen3 LLM to translate a video caption to a music prompt. It also categorizes transition types into four categories and either uses a fixed audio transition or a controlNet model to add the music clip during the transition.  The transition type is determined by a LLM agent based on Qwen3."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe proposed two-stage design is novel. The translator design can convert a powerful text-to-music backbone into a video-to-music generator.\n-\tExperimental results are strong. The proposed method outperforms existing methods AudioX and VidMuse on both subjective metrics and user preference.\n-\tThe ablation study is comprehensive."}, "weaknesses": {"value": "-\tOne drawback of the two-stage method is that it loses some critical information for background music generation. For example, the motion of objects is not included as cues. The model cannot generate music piece that matches the movement patterns of the person/objects in the video. Authors should discuss more on this potential issue.\n-\tThe design of transition types is kind of ad hoc. It is more preferred to have a unified model to predict the transition types and produce the music at the same time."}, "questions": {"value": "na"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PqXXbouqoZ", "forum": "wpQCA4yMkq", "replyto": "wpQCA4yMkq", "signatures": ["ICLR.cc/2026/Conference/Submission3583/Reviewer_Jcim"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3583/Reviewer_Jcim"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3583/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930856206, "cdate": 1761930856206, "tmdate": 1762916842795, "mdate": 1762916842795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}