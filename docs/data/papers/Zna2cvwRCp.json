{"id": "Zna2cvwRCp", "number": 7564, "cdate": 1758027681927, "mdate": 1759897846048, "content": {"title": "Fidel-TS: A High-Fidelity Benchmark for Multimodal Time Series Forecasting", "abstract": "The evaluation of time series forecasting models is hindered by a critical lack of high-quality benchmarks, leading to a potential illusion of progress. Existing datasets suffer from issues ranging from pre-training data contamination in the age of LLMs to the causal and description leakage prevalent in early multimodal designs. To address this, we formalize the core principles of **high-fidelity benchmarking**, focusing on data sourcing integrity, strict causal soundness, and structural clarity. We introduce **Fidel-TS**, a new large-scale benchmark built from the ground up on these principles by sourcing data from live APIs. Our extensive experiments validate this approach by exposing the critical biases and design limitations of prior benchmarks. Furthermore, we conclusively demonstrate that the causal relevance of textual information is the key factor in unlocking genuine performance gains in multimodal forecasting.", "tldr": "", "keywords": ["Multi-modal Time Series Forecasting", "Multi-modal Datasets", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/99e0e5f5e45976aebeea79d1d41b353acef379f7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Fidele-TS, a new high-fidelity benchmark for multimodal time series forecasting. It addresses critical shortcomings in existing benchmarks, which often suffer from pre-training data contamination, causal and description leakage, and outdated or small-scale datasets that create an illusion of progress. The authors formalize key principles of high-fidelity benchmarking: (1) data sourcing integrity using live, authentication-protected APIs to ensure freshness and prevent contamination; (2) strict causal soundness by incorporating only exogenous textual information such as weather forecasts; and (3) structural clarity through clear separation of forecasting subjects and data channels. Built upon these principles, FIDEL-TS provides millions of high-frequency, leak-free data points with aligned textual information. Extensive experiments show that prior benchmarks overestimated model capabilities, and that genuine multimodal performance gains depend on the causal relevance of textual data. This benchmark establishes a robust and causally sound foundation for evaluating modern forecasting models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a crucial gap in time-series forecasting by addressing benchmark contamination and data leakage, providing a strong conceptual and empirical foundation for fair evaluation.\n2. FIDEL-TS is built on clear, rigorous principles—data integrity, causal soundness, and structural clarity—ensuring high realism and reproducibility.\n3. Extensive experiments across diverse models and datasets convincingly demonstrate the benchmark’s validity and reveal hidden biases in prior evaluations."}, "weaknesses": {"value": "1. The proposed benchmark heavily depends on live API data sources, which may challenge long-term reproducibility and accessibility.\n2. The causal soundness principle, though central, lacks a formal quantitative validation or theoretical grounding beyond qualitative reasoning.\n3. Experimental comparisons focus mainly on benchmark-driven insights, with limited exploration of computational efficiency, sensitivity analyses, or detailed ablation studies that could further strengthen methodological rigor."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ftlyVPPIxM", "forum": "Zna2cvwRCp", "replyto": "Zna2cvwRCp", "signatures": ["ICLR.cc/2026/Conference/Submission7564/Reviewer_Jj4v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7564/Reviewer_Jj4v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760925290899, "cdate": 1760925290899, "tmdate": 1762919655964, "mdate": 1762919655964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new benchmark for multimodal forecasting, where the authors claim to address the issues of “data sourcing integrity”, “causal soundness” and “structural clarity” with prior benchmarks. Their proposed benchmark, Fidel-TS sources target time series from live APIs. They test a variety of models (unimodal models, multimodal models, LLMs) on this benchmark."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The design of the benchmark is well thought-out. I like how the authors differentiate between the target time series and the context time series.\n- The idea behind using real-world sources for both the time series and textual data seems interesting, and the authors present useful ideas in the paper to build real-world benchmarks for the multimodal forecasting task."}, "weaknesses": {"value": "- **Unable to understand the choice behind weather being the primary textual modality**: The authors state that they focus on weather as the primary textual modality. How do the authors know that the weather affects the variables in an observable way? i.e. weather may affect traffic speed, photovoltaics etc. but is the effect visually visible in the time series and not confounded with other variables that might make the effect unobservable in the time series? We need to remember we are dealing with open systems with unobserved confounders etc. so we need to take all this into account when building such a benchmark. \nI am aware this is a first step towards that, but the authors should verify\n(1) Context is necessary to predict the target time series (See CiK [1] for more details on why this is important). If this is not verified, then it brings into question the quality of the benchmark.\n(2) Verify whether changes in weather are correlated with changes in the target time series, and that the correlations are visible.\n(3) If possible, make a subset of windows in the target time series which require the context more than the other windows. Surely, not all the windows in the target time series would require the context for prediction. Marking such windows would allow one to evaluate models separately on both subsets of windows. \nImportantly, I believe that this verification should not be done with a model that they train on the data (which will be trained to use the text signals) but manually with humans or an alternate independent mechanism.\nI’ve re-read the paper several times and I do not think either of the first two points have been ensured.\n\n- **Figure 2 is confusing and completely different from what the authors discuss in the main text**: the authors talk about a dynamic event database in figure 2, and how there is an “Event list” etc. but there is no mention of any of these terms in the main text. There is a huge discrepancy between the figure and the text.\n\n- **Extremely poor presentation**: It was extremely difficult for me to read the paper. There is excessive unnecessary text that severely interrupt the flow of the text. There are too many instances of these for me to point out. There are also too many passages in the text that seem to be heavily LLM-written.\n(Note that I’ve compared this paper to other papers in my batch, and it was the one with the least understandable presentation)\n\n- **Unable to understand the intuition behind Table 2**: Why does the table solely show the performance of unimodal models, on a benchmark meant for multimodal forecasting? What is the purpose of these results? If it is to purely benchmark the models, unimodal results should only be used for comparison with multimodal results, as by themselves the unimodal results do not have any value.\n\n- **Unable to reason why testing LLMs zero-shot on this setup makes sense**: It does not make sense for me to test LLMs zero-shot on this benchmark. Clearly, there are two different kinds of benchmark for multimodal forecasting according to [2]: training-based (Time-MMD [3], ChatTime [5]) where models require training to understand the nuances of the data in the benchmark, and zero-shot (CiK [1]) where context-aided forecasting only requires the model to apply a clear scenario on a forecast (as explored also in [4]) This benchmark clearly falls under training-based, meaning that models should be trained to use the correlations between the context and the time series. It doesn't make sense to directly test LLMs on such data that requires training to be used.\n\n- **Insufficient evaluation of LLMs in the zero-shot setup**: If I am correct, the authors only evaluate LLMs with a prompting methodology similar to Direct Prompting as presented in CiK [1]. However this is not the only prompting methodology to use LLMs for forecasting; there also exists LLMP / LLMTime (CiK evaluates the LLMP methodology) and it has been shown in CiK that the strategy clearly makes a difference. The authors must evaluate these strategies as well, at least with a subset of LLMs.\n\n- **Unfair comparison of LLMs with trained multimodal models**: Foregoing the last point, comparing LLMs zero-shot and claiming that they are worse than the multimodal models is not fair; when the other multimodal models are trained on a subset of the data. Please either reword the claims or mention the discrepancy in the setup.\n\n- **Unable to understand how the benchmark is different from the others**: It is still unclear how the benchmark is different from the other benchmarks. I’d prefer to look at a table which clearly shows how this benchmark differs. \n\n- **Confusing experiment results**: The authors show that the FITS model gives worse results when weather is incorporated; however in the benchmark the authors do not mark any of such cases and this is purely an empirical results. The authors do not also claim that benchmark has such “bad actors” in the context, and instead if I’m right, they only claim that context is relevant and the benchmark is useful to test multimodal forecasting capabilities. \n   - These results make me re-think the quality of the entire paper, as such instances may have been observed in all models, but with the conclusion that the model cannot use context.\n   - **Suggestion**: Either mark or remove the bad actor variables so we can appropriately evaluate models without them\n\n- **Limited Analysis of the results**: Adding to the above point, the authors present a very limited analysis of results:  no examples are provided of the forecasts of LLMs with and without context, demonstrating how the context meaningfully changes the forecasts.\n\n- **No discussion on how the different models compare in terms of cost and parameter count**: An issue that the CiK paper [1] highlighted is the high cost of LLMs. That is not discussed here. I’m not sure how comparable the different LLMs and multimodal models are, in terms of parameter count.\n\n## Minor\n\n- **Spelling mistakes (minor)**: The authors consistently use “FIATS” in Section 4.4 - I think they are referring to the FITS model but this should be corrected. This is repeated again and again; I don’t think this is professional on the authors’ part to not proofread their paper for such mistakes.\n\n\n- **Unnecessary claims on implementation**: “To cater to the varied nature of modern models, our framework not only provides standard PyTorch (Paszke et al., 2019) interface, but also integrates HuggingFace Transformers (Wolf et al., 2020) for foundation models, and leverages PyTorch Lightning (Falcon et al., 2019) to accelerate the training of complex multimodal models. Recognizing the unique requirements of LLMs, it also supports both local deployment via vLLM (Kwon et al., 2023) and remote API calls through a simple socket”\nI do not see any of this as novelty, and worth mentioning in the main text at all.\n\t\tThe authors only propose a time series x text benchmark. All tested models are from other papers, which are already based on PyTorch/HuggingFace etc.\n\n\n### Summary Note: \nThe only reason for me giving a Score of 2 is I see some value in the benchmark that the authors are attempting to build. However if they are addressing the mentioned issues, they should address it properly in a principled way, and further evaluate models the correct way, and make the right claims. The presentation is completely off (thereby a score of 1 from me), as per the paper the approach is not sound at all (thereby a score of 1 again)."}, "questions": {"value": "- **“Strict Causal Soundness”**: The authors state “Strict Causal Soundness: which incorporates only verifiably exogenous textual information, such as weather forecasts and scheduled maintenance, to prevent causal and description leakage”\n    - However, I feel the term “Strict Causal Soundness” doesn’t make sense for what the authors describe. Causal soundness would ideally refer to how the variables in the context are causal parents of the target time series variables, which the authors do not verify. I would rather the authors term this “External Variables” or something like that to not mislead the readers and make the wrong claims.\n- **“Causal leakage”**: The same with “Causal leakage” which the authors term as “retrieved documents contain future information unavailable at prediction time” which is just temporal leakage. Any claims of causality should be carefully made. I suggest the authors reword this.\n- **“Ambiguous variable structure”**: The authors discuss an “ambiguous variable structure” - I don’t think this is a problem at all, as the referred unimodal benchmarks are only meant for univariate forecasting, meaning there is no variable structure. Therefore this is only an issue when it comes to covariate-informed forecasting or multimodal forecasting. But the authors say that “it perpetuates the ambiguous variable structure of classic unimodal benchmarks” which doesn’t make sense.\nQuestion on wording: “To ensure our evaluation mirrors the practical scenario of forecasting, we extend our high-fidelity principle from the benchmark to the experimental setup itself.”\nI’m not sure what the authors mean here. Can you clarify?\n\nTo note, all my questions mostly point to the poor presentation of the work. I suggest the authors carefully understand this and ensure that their contributions are clearly communicated to readers.\n\n## REFERENCES\n\n[1] Williams, Andrew Robert, Arjun Ashok, Étienne Marcotte, Valentina Zantedeschi, Jithendaraa Subramanian, Roland Riachi, James Requeima et al. \"Context is key: A benchmark for forecasting with essential textual information.\" ICML 2025.\n\n[2] Zhang, Xiyuan, Boran Han, Haoyang Fang, Abdul Fatir Ansari, Shuai Zhang, Danielle C. Maddix, Cuixiong Hu et al. \"Does Multimodality Lead to Better Time Series Forecasting?.\" arXiv preprint arXiv:2506.21611 (2025).\n\n[3] Liu, Haoxin, Shangqing Xu, Zhiyuan Zhao, Lingkai Kong, Harshavardhan Prabhakar Kamarthi, Aditya Sasanur, Megha Sharma et al. \"Time-mmd: Multi-domain multimodal dataset for time series analysis.\" NeurIPS 2024.\n\n[4] Ashok, Arjun, Andrew Robert Williams, Vincent Zhihao Zheng, Irina Rish, Nicolas Chapados, Étienne Marcotte, Valentina Zantedeschi, and Alexandre Drouin. \"Beyond Na\\\" ive Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs.\" arXiv preprint arXiv:2508.09904 (2025).\n\n[5] Wang, Chengsen, Qi Qi, Jingyu Wang, Haifeng Sun, Zirui Zhuang, Jinming Wu, Lei Zhang, and Jianxin Liao. \"Chattime: A unified multimodal time series foundation model bridging numerical and textual data.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 39, no. 12, pp. 12694-12702. 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rMEI5fAIv9", "forum": "Zna2cvwRCp", "replyto": "Zna2cvwRCp", "signatures": ["ICLR.cc/2026/Conference/Submission7564/Reviewer_1Gia"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7564/Reviewer_1Gia"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854197352, "cdate": 1761854197352, "tmdate": 1762919655413, "mdate": 1762919655413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new benchmark for text+time-series multimodal forecasting which overcomes existing time-series datasets. Prior benchmarks suffer from pre-training data contamination, causal and description leakage. Fidel-TS is designed on: data sourcing integrity (using live, authentication-protected APIs), strict causal soundness (including only exogenous, verifiable textual data to avoid leakage), and clear separation of “subjects” and “channels” to assess model generalization. The benchmark includes multiple high-frequency datasets with aligned textual data such as weather forecasts and control events. Experiments using various unimodal, multimodal, and large language models (LLMs) show that previous benchmarks inflated model performance due to leakage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Well motivated benchmark design: Introduces clear, principled criteria for dataset integrity, causal soundness, and structure.\n2. Use of live API streams prevents contamination and ensures temporal realism.\n3. Evaluates a wide range of models and exposes limitations in prior benchmarks.\n4. Provides a scalable reproducible framework allowing future integration of new modalities and data sources."}, "weaknesses": {"value": "1. Focuses mainly on weather and control events. Can you add more complex domains like news or economics?\n2. Evaluation scope: While comprehensive, some comparisons (EG: non-live alternative data sources) could be further expanded.\n3. Evaluation on long context inputs long horizon tasks. Critical vs simple data domains; adapting to sudden distributional shifts in temporal domains etc."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UiZOz4EuuV", "forum": "Zna2cvwRCp", "replyto": "Zna2cvwRCp", "signatures": ["ICLR.cc/2026/Conference/Submission7564/Reviewer_xbyV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7564/Reviewer_xbyV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762120215457, "cdate": 1762120215457, "tmdate": 1762919654986, "mdate": 1762919654986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to overcome benchmarking limitations in the field by proposing a high-quality benchmark called Fidel-TS. It is a large scale benchmark containing 6 datasets (500k - 67million points) with high-frequency, continuously updated data, that is authentication protected. This gets around the pretraining data leakage problem -- where one is unsure whether an LLM was pretrained on the evaluation data. The benchmark also allows for forecasting over long prediction horizon and test sets to evaluate generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Continuously updated data streams from authentication protected APIs is a solid approach to protect against data leakage into LLM pretraining data. This is a unique aspect of this work\n\n- One of the few benchmark papers in the field that protects against direct dataset memorization while still providing large scale benchmarking/evaluation (500k - 67 million points depending on the domain, table 1)\n\n- The framework is designed to be extensible to other domains (asuch as ecnomic indicators or social media trends, as the authors state) and enables other researchers to add to this benchmark, which would benefit the community.\n\n- Allows testing for generalization to new subjects."}, "weaknesses": {"value": "- Authors mention that the primary source of exogenous data is weather data (scheduled maintenance is another). This limits the generality of the benchmark and is a bit too specific.\n\n- In real world scenarios, many events occur that might affect forecasting. This information may be ingested by the model via searching news articles or other media, that might sometimes be informative and other times not. In extreme cases, the background/context might even be conflicting. How can we benchmark models in such scenarios. These questions are not addressed in the work and seem to be out of scope of the paper. In my opinion, this is a limitation of the work."}, "questions": {"value": "- Can the authors explain the text on page 5: \"Second, to address the unique computational challenges\nof evaluating LLMs, we curate smaller subsets (denoted by the -mini suffix) constructed via\nimportance sampling. By selecting samples from the full test set where unimodal models outperform\nmultimodal ones and vice versa, we ensure both computational efficiency and fairness.\" The last sentence is unclear. Also, more details on the importance sampling would help. \n\n- The pass rate is reported in tables but the text does not define what it means for a model to \"pass\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6MH09BThpB", "forum": "Zna2cvwRCp", "replyto": "Zna2cvwRCp", "signatures": ["ICLR.cc/2026/Conference/Submission7564/Reviewer_RYJU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7564/Reviewer_RYJU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142839441, "cdate": 1762142839441, "tmdate": 1762919654416, "mdate": 1762919654416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Our primary motivation and the choice of weather."}, "comment": {"value": "We sincerely thank the reviewers for their thoughtful feedback and for recognizing the core strengths of our work. We find that a common question is whether it is possible to expand more data sources as text input for our benchmark, such as economy, news, etc. We believe this is a good opportunity to **clarify the primary motivation behind our design choices for Fidel-TS**.\n\nAs we demonstrated in our paper (Section 2, lines 72-121), the core intention of our work is to first address the foundational problems we have observed in existing benchmarks, which create a significant disconnect from real-world forecasting scenarios. These problems fall into two main categories:\n* **Time Series Data Integrity**: As we mentioned in our paper (Section 2.1, lines 74-92), existing benchmarks are often outdated, small-scale, vague in structure and use low-frequency data. This hinders robust model evaluation.\n* **Multimodal Data Fidelity**: This is a more subtle but equally critical issue. While the first multimodal benchmark commendably tried to include diverse text types like news and economic reports, this approach inadvertently introduced severe data quality issues. As we detail in our paper (Lines 041-043), these include pre-training contamination, temporal leakage (accessing information published after the event), and description leakage (text explicitly stating the ground-truth).\n\nFor example, a benchmark that retrospectively scrapes news articles analyzing a stock market dip to \"predict\" that same dip is creating a test scenario that is impossible in the real world. A model succeeding on such a task is not demonstrating forecasting ability but rather an ability to extract answers from post-hoc descriptions. Evaluating models in such unrealistic settings offers limited value for real-world applications.\n\n**Establishing high fidelity is a necessary prerequisite for evaluating complex multimodal forecasting, not an alternative to it. Therefore, our immediate priority was to establish a benchmark with strict real-world fidelity**, rather than pursuing textual breadth at the cost of fidelity. This led us to select weather report as our primary textual modality. As a near-universal factor, weather reports are easily accessible in the real world, and weather forecasts are quite mature. Weather can affect changes in real-world systems such as power and traffic flow, but the changes in weather are independent of these systems and will not be affected by them. Also, the weather report can be known in advance and will not directly describe how the variables to be predicted will change below. This provides a general and representative task for evaluating multimodal models close to the real world.\n\nWe also recognized the need to move beyond a single source of textual data. Although common sense suggests that weather is closely linked to most real-world systems, the effectiveness of multimodal forecasting relies more on text that is highly relevant to the system. Such text tends to reflect system changes more accurately than weather-related information. We took effort to incorporated internal control events in the BEAR dataset. These events share the same crucial properties as weather forecasts, showcasing a different but equally valid type of multimodal input.\n\nWe wholeheartedly agree with the assessment that incorporating broader and more complex data sources is the crucial next step. This is precisely the path forward we envision and have explicitly outlined in our **Limitations and Future Work** section (Lines 478-485). As we state in our paper, this will require \"novel methodologies that can filter and align these noisier data streams while upholding the principles of high fidelity\" (Lines 482-484). Our extensible framework is designed to facilitate exactly this line of future research.\n\nWe hope this response clarifies that our deliberate focus on textual data quality is a necessary foundational step towards a richer, more complex benchmarking scenarios while following the principle of real-world fidelity."}}, "id": "aVO97cs5Y3", "forum": "Zna2cvwRCp", "replyto": "Zna2cvwRCp", "signatures": ["ICLR.cc/2026/Conference/Submission7564/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7564/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission7564/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724327119, "cdate": 1763724327119, "tmdate": 1763724327119, "mdate": 1763724327119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}