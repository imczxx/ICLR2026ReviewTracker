{"id": "lS5s3TorBW", "number": 6722, "cdate": 1757993452602, "mdate": 1759897898696, "content": {"title": "A Sign-aware Graph Transformer with Prototypical Objectives for Signed Link Prediction", "abstract": "Signed link prediction in bipartite graphs is a fundamental task with wide-ranging applications, yet it poses significant challenges. Current Graph Neural Networks are inherently local due to their message-passing nature, preventing them from capturing the long-range dependencies crucial for accurate prediction. Furthermore, they often fail to model complex real-world data distributions characterized by severe class imbalance and rich intra-class multimodality. To overcome these limitations, we propose the Hierarchical Prototypical Contrastive Sign-aware Graph Transformer (HPC-SGT). At its core, our framework features a Sign-aware Graph Transformer that operates on the line graph dual, leveraging novel spectral and motif-based inductive priors to learn structurally-aware global representations. This expressive encoder is optimized via a hierarchical prototypical objective, which learns a geometrically structured embedding space. It couples a class-balanced contrastive loss to robustly handle data imbalance with clustering and separation regularizers to explicitly model multi-modal class structures. The framework is unified by a cross-view consistency mechanism that grounds the learned semantic representations in the graph's foundational topology, bridging the structure-semantics gap. Extensive experiments on challenging benchmarks show that HPC-SGT significantly outperforms a wide range of state-of-the-art methods. Ablation studies further validate the contribution of each component, establishing HPC-SGT as a new, powerful, and principled solution for signed link prediction. Our code is available in the supplementary materials.", "tldr": "", "keywords": ["Signed Link Prediction", "Graph Transformer", "Contrastive Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/12742165fc213c428cc7136dbd8cf585106ecc23.pdf", "supplementary_material": "/attachment/0d966b4a717f617357b8b49d6d23f304d1338509.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces the Hierarchical Prototypical Contrastive Sign-aware Graph Transformer (HPC-SGT), designed to address long-range dependencies and class imbalance in signed graphs. The proposed approach converts a bipartite signed graph into a line graph, thereby reformulating the link prediction problem as a node classification task. A transformer-based architecture is then employed, leveraging two embeddings to enhance structural representation learning. Extensive experiments across multiple datasets demonstrate the effectiveness and robustness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addressing the problem of predicting signed edges is particularly meaningful, as it is closely connected to numerous real-world applications such as social network analysis, trust prediction, and recommendation systems, making this line of research both relevant and impactful.\n- The transformation of a signed bipartite graph into a line graph, thereby converting the link prediction task into a node classification task, presents an interesting and effective design choice.\n- The method demonstrates consistent performance across multiple datasets, supporting the validity and general applicability of the proposed approach.\n- The paper includes a comprehensive ablation study, which clearly demonstrates the contribution of each component in the proposed model."}, "weaknesses": {"value": "- The paper’s motivation could be further strengthened. In particular, the rationale for the existence of severe class imbalance and rich intra-class multimodality is not fully established. Additionally, the effectiveness of the proposed method in handling intra-class multimodal structures is not empirically validated.\n- A more thorough discussion of the transformation from a signed bipartite graph to a line graph could enhance the perceived novelty of the approach. For instance, addressing the possibility of non-isomorphic graphs producing identical line graphs or analyzing the computational complexity of this transformation would provide deeper insight.\n- The paper lacks empirical evidence demonstrating that the proposed method more effectively captures long-range dependencies compared to existing baselines, which limits the strength of the contribution."}, "questions": {"value": "1. Could the authors compare the performance of GNN-based baselines applied to the transformed line graph in order to demonstrate the necessity of the transformer architecture?\n2. In the extension of Figure 4, if the loss function related to class imbalance is ablated, how does the performance vary?\n3. Does the number of distinct motif types ($N_p$) affect the performance? \n4. Is $ss^T$ in equation 3 scalable? What is the time complexity for this computation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "694NIBXw7C", "forum": "lS5s3TorBW", "replyto": "lS5s3TorBW", "signatures": ["ICLR.cc/2026/Conference/Submission6722/Reviewer_bDpP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6722/Reviewer_bDpP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807385913, "cdate": 1761807385913, "tmdate": 1762919011771, "mdate": 1762919011771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HPC-SGT, a novel framework for signed link prediction in bipartite graphs. The core challenge addressed is that traditional Graph Neural Networks (GNNs) are inherently local and struggle to capture the long-range dependencies crucial for this task, while also failing to model complex real-world data distributions characterized by class imbalance and multi-modal structures within classes.\n\nThe contributions of HPC-SGT are threefold. First, it introduces a Sign-aware Graph Transformer (SGT) that operates on the line graph dual of the original bipartite graph, effectively reframing link prediction as a node classification problem. This SGT is empowered with novel graph-native inductive priors—Relational Spectral Encoding (RSE) and Topological Motif Encoding (TME)—which are injected into the self-attention mechanism to capture global structural principles from balance theory and local higher-order connectivity patterns. Second, the model is optimized via a hierarchical prototypical learning objective designed to handle class imbalance through a balanced discriminative loss and to model intra-class multimodality using clustering and separation regularizers. Third, a cross-view consistency mechanism ensures the learned semantic representations remain grounded in the graph's foundational topology, bridging the structure-semantics gap.\n\nExtensive experiments on multiple benchmarks demonstrate that HPC-SGT significantly outperforms a wide range of state-of-the-art methods. Ablation studies confirm the importance of each component, establishing HPC-SGT as a powerful and principled solution that successfully overcomes the limitations of locality in GNNs and the challenges of complex data distributions in signed link prediction."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper demonstrates high originality by synergistically combining several advanced but previously distinct concepts into a unified framework. It innovatively merges the global receptive field of Transformers with the structural specificity of graphs by operating on the line graph, a paradigm shift from node-centric to link-centric modeling. Furthermore, it uniquely injects graph-specific inductive biases (spectral balance, motifs) directly into the attention mechanism of a Transformer, moving beyond simple positional encodings to create a truly structure-aware architecture.\n2. The proposed hierarchical prototypical objective is a novel contribution in itself. Instead of treating class imbalance and multimodality as separate issues, it addresses them simultaneously within a single, probability-based framework. The idea of using multiple prototypes per class to capture intra-class variance and a geometric separation regularizer is a creative and principled approach to structuring the embedding space for complex, real-world data distributions.\n3. The paper exhibits exceptional quality in its experimental design. The evaluation is conducted on multiple large-scale benchmarks, and the model is compared against a wide array of fourteen baselines spanning different methodological families (unsigned embeddings, early signed methods, GNNs, and Transformers). This thorough comparison provides strong, convincing evidence for the claimed superiority of HPC-SGT. The use of multiple standard metrics further strengthens the validity of the results.\n4. The clarity extends to the technical specifics. Key concepts like the line graph transformation, the formulation of the RSE and TME priors, and the mathematical details of the prototypical objective are explained with precision. The use of clear mathematical notation and formulae allows a technically proficient reader to understand the implementation details without ambiguity."}, "weaknesses": {"value": "1. The paper rightly acknowledges the computational cost of the Transformer architecture as a limitation, but the empirical analysis of scalability could be more thoroughly explored. While runtime comparisons are provided in Table 5, they are relatively high-level and do not deeply investigate how the model performs on graphs of an order of magnitude larger than the current benchmarks. To strengthen the practical impact, the authors could include a scalability analysis on a truly massive graph (e.g., with billions of edges) or provide a more detailed breakdown of memory usage and training time per component. A constructive step would be to implement and evaluate a specific sparse attention mechanism, as mentioned in the future work, and report its performance trade-offs in the main text or appendix. This would make the framework more accessible for real-world applications where efficiency is critical.\n\n2. The hierarchical prototypical objective is a novel contribution, but the paper does not fully explore the interpretability of the learned prototypes. Understanding what semantic or structural patterns each prototype captures could provide valuable insights into the model's decision-making process and enhance trustworthiness. For instance, the authors could analyze whether prototypes correspond to specific user behaviors (e.g., \"critical reviewers\" or \"enthusiastic fans\") or graph motifs. A simple yet actionable improvement would be to include a qualitative analysis in the appendix, such as visualizing the nodes or links closest to each prototype or discussing the characteristics of high-assignment examples. This would bridge the gap between the geometric formulation of the objective and its real-world meaning, adding a layer of explainability to the model's predictions.\n\n3. The framework is evaluated on static graph snapshots, but many real-world signed networks (e.g., e-commerce or social platforms) are dynamic, with edges and signs changing over time. The paper's focus is on static prediction, which is standard, but a discussion on how HPC-SGT might adapt to temporal settings would broaden its significance. As a light weakness, the authors could enhance the work by briefly outlining the challenges and potential extensions for dynamic graphs in the conclusion or future work section. For example, they could suggest how the cross-view consistency mechanism might be regularized over time or how the prototypes could be made incremental. This would position HPC-SGT as a foundation for future research in streaming graph learning, making it more forward-looking."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EIsuVNpMMs", "forum": "lS5s3TorBW", "replyto": "lS5s3TorBW", "signatures": ["ICLR.cc/2026/Conference/Submission6722/Reviewer_YMwL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6722/Reviewer_YMwL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810934493, "cdate": 1761810934493, "tmdate": 1762919011361, "mdate": 1762919011361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HPC-SGT, a novel Graph Transformer framework for signed link prediction in bipartite graphs. The model operates on the line graph dual , which allows it to overcome the locality constraints of traditional GNNs and capture global dependencies. \nThe introduction of  RSE and TME priors, which inject crucial structural and balance-theory information directly into the Transformer's attention mechanism, making it topology-aware."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed methodology appears to be well-designed for the task of signed link prediction.\n2. The framework's effectiveness is validated through extensive experiments, where it achieves state-of-the-art performance on multiple benchmarks.\n3. Mathematical notation is clearly defined and used consistently throughout the manuscript."}, "weaknesses": {"value": "1. The paper's title addresses general \"Signed Link Prediction.\" However, the entire methodology, from the problem definition to the experimental setup, is exclusively focused on \"Signed Bipartite Graphs\". This constitutes a significant mismatch, as the title suggests a broader applicability than the work actually provides.\n2. The paper claims to address \"severe class imbalance\" as a key challenge. However, the proposed solution (Equation 12), is a standard Weighted Cross-Entropy loss, which applies a class-balancing weight $\\alpha_{y_{i}}$. This is a well-known, common technique and lacks the novelty required to be considered a core contribution of the framework.\n3. Since the weighted loss ($\\alpha_{y_{i}}$) is a standard technique, a fair comparison would require applying this same weighted loss to the baseline models (e.g., LightGCL, SIGformer). The paper fails to do this, making it impossible to determine if the superior F1-scores stem from the novel SGT architecture or simply from this standard weighting trick that any baseline could have used.\n4. The main comparison (Table 1) uses datasets that are not complete balanced. The only \"severely\" imbalanced dataset (Bonanza) is relegated to a supplementary experiment (Section 5.5) and is not compared against the main SOTA baselines from Table 1. This disconnects the paper's motivation from its primary experimental validation.\n5. The framework relies on a Transformer architecture operating on the line graph dual. This approach is computationally expensive. The line graph's size scales with the number of edges in the original graph, and the Transformer's self-attention mechanism has quadratic complexity. The paper's own results show performance gains over the next-best Transformer baselines that are arguably marginal"}, "questions": {"value": "See the above box."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jrbUSxctXi", "forum": "lS5s3TorBW", "replyto": "lS5s3TorBW", "signatures": ["ICLR.cc/2026/Conference/Submission6722/Reviewer_Ed4P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6722/Reviewer_Ed4P"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6722/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762451890530, "cdate": 1762451890530, "tmdate": 1762919010654, "mdate": 1762919010654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}