{"id": "xhrN80hmJ9", "number": 13828, "cdate": 1758223296668, "mdate": 1759897409903, "content": {"title": "Learning to Reason via Mixture-of-Thought for Logical Reasoning", "abstract": "Human beings naturally utilize multiple reasoning modalities to learn and solve logical problems, i.e., different representational formats such as natural language, code, and symbolic logic. In contrast, most existing LLM-based approaches operate with a single reasoning modality during training, typically natural language. Although some methods explored modality selection or augmentation at inference time, the training process remains modality-blind, limiting synergy among modalities. To fill in this gap, we propose Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three complementary modalities: natural language, code, and a newly introduced symbolic modality, truth-table, which systematically enumerates logical cases and partially mitigates key failure modes in natural language reasoning. MoT adopts a two-phase design: (1) **self-evolving MoT training**, which jointly learns from filtered, self-generated rationales across modalities; and (2) **MoT inference**, which fully leverages the synergy of three modalities to produce better predictions. Experiments on logical reasoning benchmarks including FOLIO and ProofWriter demonstrate that our MoT framework consistently and significantly outperforms strong LLM baselines with single-modality chain-of-thought approaches,\nachieving up to **+11.7pp** average accuracy gain.\nFurther analyses show that our MoT framework benefits both training and inference stages; that it is particularly effective on harder logical reasoning problems; and that different modalities contribute complementary strengths, with truth-table reasoning helping to overcome key bottlenecks in natural language inference.", "tldr": "", "keywords": ["Logical Reasoning", "Self-evolving Training", "Large Language Models", "Parallel Scaling", "Test time scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e0f5977bd05462ed16d9b419bf020a320009e1e6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an approach to improve the logical-reasoning capabilities of LLMs by incorporating different reasoning modalities into the decision process. Study aims treat natural language, code and truth table as different reasoning interfaces and apply majority voting to determine model's output to a logical reasoning problem after training the model on multiple reasoning modalities. Performance of the proposed approach has been evaluated on FOLIO, ProofWriter and ProverQA benchmarks, compared with Gemma-2-2B/9B and Qwen-2.5-7B-Instruct."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The study harvests different reasoning interfaces and connects NL reasoning with symbolic reasoning in a single framework.\n- Clear performance superiority compared to Qwen-2.5-7B-Instruct, a model capable of reasoning.\n- Proposed approach does not require any external teachers.\n- Uses interpretable reasoning interfaces, provides space for interpretability studies."}, "weaknesses": {"value": "- Gemma-2-2B/9B models are inherently not proficient in reasoning tasks, their pre-training lacks exposure to used reasoning interfaces (code and truth table).\n- Evaluation domain is restricted to logical reasoning which is not the most suitable domain for harvesting different multimodal reasoning interfaces, specifically code.\n- Although it is symbolic, truth-table's capability and suitability as a reasoning interface is not convincing.\n- Majority voting is a shallow approach for combining the outcomes of reasoning interfaces. It is not suitable for different reasoning problem types."}, "questions": {"value": "- Include comparisons with models better at reasoning.\n- Additional experiments on the efficiency of truth-table as a reasoning interface would proof its contribution.\n- Evaluation with more comprehensive reasoning benchmarks (e.g. math reasoning) would highlight this approach's capabilities and limitations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0TIodNaAdy", "forum": "xhrN80hmJ9", "replyto": "xhrN80hmJ9", "signatures": ["ICLR.cc/2026/Conference/Submission13828/Reviewer_Fz8C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13828/Reviewer_Fz8C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854584154, "cdate": 1761854584154, "tmdate": 1762924352278, "mdate": 1762924352278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to fine-tune LLMs to reason in three different “thought modalities” for solving logical reasoning problems. They train in a self-evolving manner where they generate thoughts and use thoughts which resulted in correct answers as the data for the next iteration. To perform inference, the model responds in each of the three modalities and then takes the majority answer as the final answer. They show that this approach improves in accuracy over using any one of the thought modalities as well as helps on the hardest of reasoning problems."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The “thought modality” fine-tuning appears to be novel. The exploration of fine-tuning on different “thought modalities” such as coding and CoT has been extensively studied in terms of post-training on different datasets (coding problems, math word problems, etc.), but I am not aware of work which fine tunes on different solution methods for the same dataset.\n* The paper is well written and easy to follow.\n* The problem of improving reasoning performance by increasing “thought” diversity is important and this approach to fine-tuning on different solution strategies can be impactful."}, "weaknesses": {"value": "* Experiments are only performed on two datasets and results are missing standard deviations.\n* The natural language and code reasoning modalities seem highly general, but the truth table modality seems specific to the two benchmarks used in this paper. If using MoT on a different reasoning dataset, this could potentially harm performance.\n* The experiments are a little hard to evaluate due to the confounder of the number of samples which different methods use and the different models used. For instance in Table 2, the last row MoT method takes the majority answer from 3 samples, but I assume that the line above that (MoT Single-Thought) only uses one sample. Also, Logic-LM is only evaluated with GPT-4."}, "questions": {"value": "1. Does MoT Single-Thought use majority voting from three samples? If not, I think this is an important baseline as well.\n2. How should one come up with the relevant reasoning modalities? Are there a set of general reasoning modalities that could work for any reasoning problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MKPQOpi6rg", "forum": "xhrN80hmJ9", "replyto": "xhrN80hmJ9", "signatures": ["ICLR.cc/2026/Conference/Submission13828/Reviewer_yjSj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13828/Reviewer_yjSj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946246908, "cdate": 1761946246908, "tmdate": 1762924351911, "mdate": 1762924351911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework called Mixture-of-Thought (MoT), which aims to enable an LLM to jointly learn and utilize three complementary reasoning modalities (natural language, code, and a novel truth-table modality) within a unified model via a self-evolving training process. This method significantly outperforms single-modality baselines on standard logical reasoning benchmarks (FOLIO, ProofWriter), particularly by introducing the truth-table modality to address key bottlenecks in natural language reasoning, such as \"missing branches\" and \"invalid converse\"."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's motivation is clear and well-supported. The fusion of three modalities is experimentally shown to effectively enhance model performance. Notably, the design of the truth-table modality is driven by a thorough error analysis shown in Figure 1c, which pointedly addresses key bottlenecks of NL CoT.\n\n- The paper proposes a self-evolving training method that enables a single LLM to jointly learn and synergistically utilize multiple reasoning modalities, addressing the \"modality-blind\" problem of existing methods during the training phase."}, "weaknesses": {"value": "- The generalization ability of the truth-table (TT) modality is questionable. This modality is currently highly customized for logical reasoning and has only been validated on two logical reasoning benchmarks. I suggest the authors discuss or explore the framework's potential application in other reasoning domains, such as mathematical or commonsense reasoning.\n\n- Experiments in Table 3, Figure 3b, Figure 4, and Figure 7 show that the code modality almost always performs worse when used alone. However, Table 3 also shows that including it in joint training still yields performance gains. It will be better to provide supplementary qualitative or quantitative analysis on: (i) What are the specific advantages of the code modality? (ii) What specific problems or sample types does it solve that the other two modalities cannot?\n\n- It can benefit from further analyzing the application scenarios for each modality. If it's possible to identify which problems are best suited for a specific modality, perhaps a dynamic routing strategy could be employed at inference time instead of performing three separate inferences for all problems (as in MoT-Inference), thereby optimizing the high inference cost.\n\n- Please provide a more detailed explanation of the \"reason to prune\" step within the Truth-Table CoT, either in the main paper or the appendix. The current description, \"eliminates rows that violate any premise through reasoning via LLMs\", is too abstract and lacks sufficient implementation details for reproducibility.\n\n- Including a post-MoT-training error analysis is necessary, similar to the pre-training analysis in Figure 1c. This would help verify whether MoT training not only improves ensemble performance but also enhances the NL modality itself, for example, by reducing the frequency of \"missing branch\" and \"invalid converse\" errors.\n\nSome recent relevant works should be included to discuss or compare with the proposed method: \n\n[1] Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n\n[2] Cumulative Reasoning with Large Language Models\n\n[3] DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oNcsOzPgo3", "forum": "xhrN80hmJ9", "replyto": "xhrN80hmJ9", "signatures": ["ICLR.cc/2026/Conference/Submission13828/Reviewer_1P9u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13828/Reviewer_1P9u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982523242, "cdate": 1761982523242, "tmdate": 1762924351297, "mdate": 1762924351297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Mixture-of-Thought, a framework that enables large language models to reason using three complementary modalities ( i.e., natural language, code, and a symbolic truth-table format ). Unlike prior methods that train with a single modality and only vary modalities at inference, MoT trains and infers across modalities to exploit their synergy. A self-evolving training phase jointly learns from filtered, self-generated rationales in all three formats, while the inference phase integrates them to produce stronger predictions. The truth-table modality systematically enumerates logical cases, helping mitigate common failure modes of natural-language reasoning. Experiments on logical reasoning benchmarks such as FOLIO and ProofWriter show MoT significantly outperforms strong single-modality chain-of-thought baselines, with average gains up to 11.7 percentage points."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The proposed Self-Evolving MoT Training method is reasonable and effective.\n3. Extensive experiments and ablations were conducted on various models and showed strong performance."}, "weaknesses": {"value": "1. The proposed method appears applicable only to specific logical reasoning tasks, and its generalization to broader reasoning tasks or other domains remains uncertain."}, "questions": {"value": "1. To better highlight the impact of MoT inference, I suggest also reporting the Self-Consistency with three votes metric for the MoT with Single-Thought inference results in Table 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vCJgJvXNeZ", "forum": "xhrN80hmJ9", "replyto": "xhrN80hmJ9", "signatures": ["ICLR.cc/2026/Conference/Submission13828/Reviewer_6rwT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13828/Reviewer_6rwT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13828/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762064272491, "cdate": 1762064272491, "tmdate": 1762924350779, "mdate": 1762924350779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}