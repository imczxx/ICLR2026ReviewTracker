{"id": "VSDV0SWwOC", "number": 24777, "cdate": 1758360264344, "mdate": 1759896749440, "content": {"title": "LS-Merge: Merging Language Models in Latent Space", "abstract": "Model merging in weight space is an efficient way to reuse pretrained models, but existing methods typically assume matching architectures or sizes, making heterogeneous merges brittle or infeasible. We address this limitation by encoding model weights into a smooth latent space, enabling cross-architecture operations, and performing the merge in the latent space before decoding back to weights. This approach faces two major challenges. First, LLMs contain billions of parameters, which makes latent encoding computationally demanding. Second, using high compression ratios often hinders the encoder’s ability to generalize to unseen weights. We tackle these issues with a transformer-based variational autoencoder (VAE) trained in a two-stage compression curriculum with structured layer-aware chunking: the model first learns a high-capacity latent representation and then distills to a compact code, improving both stability and out-of-distribution generalization. To align heterogeneous models, we introduce a dimensionality-matching projection that allows interpolation between models of different sizes. Empirically, latent-space interpolation is consistently more robust than direct weight-space averaging and yields stronger downstream performance when merging models of different sizes. Together, these components provide a scalable, architecture-agnostic recipe for model merging.", "tldr": "Merging Language Models in Latent Space", "keywords": ["LS-Merge", "LLM merging", "latent space", "weight space learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d6c926c9a0e59e561ac2d47db8f24abaeccf458.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper propose a novel method to replace fragile weight-space merging with LSMerge: a latent space merging technique. It is an encode–align–decode pipeline that operates on a latent space of model weights learned by a transformer VAE. Specifically: the method \n(1) encodes parameters into latents via layer-aware chunking, \n(2) aligns heterogeneous models’ latent distributions (depth/width mismatches) using Optimal Transport (OT), and \n(3) decodes an interpolated latent back to weights. \nEvaluations demonstrates performance gain in (i) self-merging of a single model’s latents, (ii) LoRA expert fusion vs. model-soup/SLERP baselines, and (iii) heterogeneous merges (within-family size changes and cross-family)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Principled formulation: The core idea of encoding model weight into a shared latent manifold and aligning distributions via OT is right on target to resolve the pain point of current merging methods in shape matching requirements. This paper gives a more grounded understanding of how to merge models by aligning the model's representation geometry.\n\n2. Comprehensive experiment setting: The same recipe can be used for many use cases, including self-merging, LoRA-expert fusion, and cross-family merging. This shows that this method is a more fundamental framework rather than a \"trick\"."}, "weaknesses": {"value": "1. Lack of analysis on reconstruction error: The main component of the merging system is a VAE which introduces lossy compression during reconstruction, yet there isn't an analysis of how that bottleneck correlates with downstream task accuracy. This matters as it could confound where the gains are from. I'm curious if the author can ablate on compression level vs. KL-weights. \n\n2. Lack of understanding of successful merging condition: The experiments are diverse in terms of merging various types of models (self, LoRA etc.) but it is unclear what is the boundary condition of successful latent space merging? I'm curious to see if the authors have found any failure case and if so, have done any analysis of what's the key difference between merges that fails/succeeds."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wjEA6XtKad", "forum": "VSDV0SWwOC", "replyto": "VSDV0SWwOC", "signatures": ["ICLR.cc/2026/Conference/Submission24777/Reviewer_mkDJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24777/Reviewer_mkDJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865647952, "cdate": 1761865647952, "tmdate": 1762943193487, "mdate": 1762943193487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LS-Merge is a method that uses a Transformer-based VAE to compress chunks of weight matrices from various LLMs, enabling heterogenous merges where models have different architectures. THhe VAE encodes weight chunks into lower dimensional representations, aligns them in latent space, interpolates them, and then decodes them back into model space. This method can improve performance of single models, improve merges of multiple LoRA adapters, and perform competitively with direct weight space merging methods."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. By applying the LS-Merge VAE on pre-trained Gemma models, the output model can improve on benchmarks like GSM8K and MMLU without additional gradient steps on the model weights, just on encoding and decoding the weights with the pretrained VAE. This represents a unique and nice result of improving singular model.  \n2. The proposed method of using weights as data to learn via a VAE is novel, interesting, and a potential direction to expand upon in future work, especially given interesting results like point 1. Also, it appears that only a few models worth (2 Gemma models) is sufficient to train on to achieve useful results. \n3. This method can achieve heterogenous merging, mostly on intra-family based merges."}, "weaknesses": {"value": "1. While the results are impressive, the description of the method is not entirely clear at times and detracts from the contribution of the paper. For example, it is not clear how heterogenous rescaling occurs according to the description, and it is not when the OT based alignment occurs in the latent space training. The contribution of the paper seems solid, but its presentation seems rushed and the paper does not seem reproducible or easily understandable in its current state. Another example is the statement that the evaluation of cross family evaluation is performed using lm-eval due to issues with the llama model when using previous evaluation code. What does this mean here? \n2. Despite good results, this work lacks some analysis of what is learned by the VAE model, as well as some ablations of key choices. It does not seem clear why this method works, or what about it makes it work. What is the latent size used in this work? And what is the chunked size? And how were these values set? \n\nThis paper is quite interesting but unfortunately its presentation and polish is very lacking, which brings into question the correct execution of this work. I think this paper could be impactful and a nice contribution, but in its current form I cannot recommend accepting it as it seems only partially finished."}, "questions": {"value": "1. In section 3.1, is the v_proj included in this analysis? It is missing from the description of the moment analysis of the key LLM weights. \n2. What exactly is a layer matrix in section 3.1 line 150 and section 3.2 line 204? Is it a single weight like q_proj or up_proj or is it the entire Transformer layer?\n3. Is the embedding from line 206 part of the transformer encoder? Or is it separate?\n4. What is the exact operation for the rescaling procedure described for heterogenous mapping? It is not clear how the value r is used in this mapping. \n5. What is the model used as base in Table 5? And which model is weighted 0.1 in the mixture?\n\nTypos\n1. Line 192, porjection\n2. Line 139 up_porj, down_porj\n3. Line 233 artihmetic \n4. What is the \"fixed of 2\" mean on line 321?\n5. Line 796 is missing a reference to a figure in Latex. \n6. Line 403 familly\n7. Line 404 \"is perform\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ITBKguzztn", "forum": "VSDV0SWwOC", "replyto": "VSDV0SWwOC", "signatures": ["ICLR.cc/2026/Conference/Submission24777/Reviewer_593M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24777/Reviewer_593M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958775912, "cdate": 1761958775912, "tmdate": 1762943193275, "mdate": 1762943193275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LS-Merge is a method that uses a Transformer-based VAE to compress chunks of weight matrices from various LLMs, enabling heterogenous merges where models have different architectures. THhe VAE encodes weight chunks into lower dimensional representations, aligns them in latent space, interpolates them, and then decodes them back into model space. This method can improve performance of single models, improve merges of multiple LoRA adapters, and perform competitively with direct weight space merging methods."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. By applying the LS-Merge VAE on pre-trained Gemma models, the output model can improve on benchmarks like GSM8K and MMLU without additional gradient steps on the model weights, just on encoding and decoding the weights with the pretrained VAE. This represents a unique and nice result of improving singular model.  \n2. The proposed method of using weights as data to learn via a VAE is novel, interesting, and a potential direction to expand upon in future work, especially given interesting results like point 1. Also, it appears that only a few models worth (2 Gemma models) is sufficient to train on to achieve useful results. \n3. This method can achieve heterogenous merging, mostly on intra-family based merges."}, "weaknesses": {"value": "1. While the results are impressive, the description of the method is not entirely clear at times and detracts from the contribution of the paper. For example, it is not clear how heterogenous rescaling occurs according to the description, and it is not clear when the OT based alignment occurs in the latent space training. The contribution of the paper seems solid, but its presentation seems rushed and the paper does not seem reproducible or easily understandable in its current state. Another example is the statement that the evaluation of cross family evaluation is performed using lm-eval due to issues with the llama model when using previous evaluation code. What does this mean here? \n2. Despite good results, this work lacks some analysis of what is learned by the VAE model, as well as some ablations of key choices. It does not seem clear why this method works, or what about it makes it work. What is the latent size used in this work? And what is the chunked size? And how were these values set? \n\nThis paper is quite interesting but unfortunately its presentation and polish is very lacking, which brings into question the correct execution of this work. I think this paper could be impactful and a nice contribution, but in its current form I cannot recommend accepting it as it seems only partially finished."}, "questions": {"value": "1. In section 3.1, is the v_proj included in this analysis? It is missing from the description of the moment analysis of the key LLM weights. \n2. What exactly is a layer matrix in section 3.1 line 150 and section 3.2 line 204? Is it a single weight like q_proj or up_proj or is it the entire Transformer layer?\n3. Is the embedding from line 206 part of the transformer encoder? Or is it separate?\n4. What is the exact operation for the rescaling procedure described for heterogenous mapping? It is not clear how the value r is used in this mapping. \n5. What is the model used as base in Table 5? And which model is weighted 0.1 in the mixture?\n\nTypos\n1. Line 192, porjection\n2. Line 139 up_porj, down_porj\n3. Line 233 artihmetic \n4. What is the \"fixed of 2\" mean on line 321?\n5. Line 796 is missing a reference to a figure in Latex. \n6. Line 403 familly\n7. Line 404 \"is perform\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ITBKguzztn", "forum": "VSDV0SWwOC", "replyto": "VSDV0SWwOC", "signatures": ["ICLR.cc/2026/Conference/Submission24777/Reviewer_593M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24777/Reviewer_593M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958775912, "cdate": 1761958775912, "tmdate": 1763484071727, "mdate": 1763484071727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel method for model merging where the size of the models do not need to match. In order to do so, the authors use variational auto-encoders, optimal transport, linear interpolation,  and projections down to a merged weight space. The results beat strong baselines (including very recently published methods) on a wide variety of tasks. I’ll keep this reviews short because, overall, I would be very happy to see this published in the conference."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Though the paper is dense and covers a lot of complex topics, it is well-written and easy to follow. In addition, things are presented clearly, such as the caption of Figure 1.\n\nThere are a lot of experimental results with very strong baselines that they beat.\n\nIt is an interesting idea intellectually and I would have liked to have seen the paper published even if the results had not beaten the baselines – but they did."}, "weaknesses": {"value": "Perhaps I missed it, but I think a bit more background on OT could be useful for the presentation to the reader. The paragraph at the end of section 3 could be expanded a bit more. I ended up needing to look at one of the cited papers. However, most of the other parts of the paper explained complex topics very well."}, "questions": {"value": "The method seems like it could be a bit computationally expensive - and this is mentioned in the limitations. How expensive is it exactly? I don't have a particular way I'd like to see this question answered, but whatever makes sense. For instance, maybe time on a GPU? What sort of GPU (which is definitely dependent on the models)? Maybe percent of compute needed compared to doing something from scratch? Or comparison to another method (i.e. AIM)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R6GCDlznrs", "forum": "VSDV0SWwOC", "replyto": "VSDV0SWwOC", "signatures": ["ICLR.cc/2026/Conference/Submission24777/Reviewer_XrKv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24777/Reviewer_XrKv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762268651111, "cdate": 1762268651111, "tmdate": 1762943193076, "mdate": 1762943193076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}