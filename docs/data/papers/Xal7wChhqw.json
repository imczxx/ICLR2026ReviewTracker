{"id": "Xal7wChhqw", "number": 6501, "cdate": 1757987221820, "mdate": 1759897910855, "content": {"title": "Understanding Robustness Against Gradient Inversion Attacks: A Flat Minima Perspective", "abstract": "Gradient Inversion Attacks (GIAs), which aim to reconstruct input data from its gradients, pose substantial risks of data leakage and challenges of data privacy in distributed learning systems, e.g., federated learning (FL).\nNevertheless, existing defenses against GIA are mostly ad-hoc by relying on gradient modifications without a principle of when gradients are vulnerable to GIA and how we can fundamentally suppress the possibility of data leakage.\nWe interpret GIA with the mutual information between the gradients $G$ and their data $X$, i.e., $I(X;G)$, which is revealed to be upper-bounded by the Hessian of loss. \nBased on the findings, we rethink the robustness against GIA for a flat minima searching-based FL algorithm, where it inherently suppresses Hessian values, thus minimizing $I(X;G)$.\nWe extensively demonstrate that the gradients computed by searching flatter minima in the FL scenario achieve a substantial improvement in robustness against GIAs.\nOur work sheds light on novel benefits of flat minima searching, not only promoting better generalization but also hardening privacy in FL systems.", "tldr": "", "keywords": ["Federated Learning", "Gradient Inversion Attack", "Privacy", "Robustness"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6f29c217b27c0164538417e638a1d60b385f42c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the privacy risks posed by GIAs in federated learning scenarios. The authors argue that most existing defenses rely on heuristic gradient modifications and lack a principled understanding of when gradients are vulnerable. To address this gap, they reinterpret GIAs through the lens of mutual information I(X;G)between gradients and data, proving that it is upper-bounded by the loss Hessian. Building on this theory, they analyze flat-minima-search-based FL algorithms, showing that flatter minima inherently suppress Hessian values, thereby reducing information leakage. Extensive experiments demonstrate significant robustness improvements against GIAs, highlighting flat minima’s dual benefits for both generalization and privacy preservation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper provides a principled and interpretable understanding of GIAs by introducing a mutual information perspective and explicitly linking gradient vulnerability to the Hessian of the loss function. This connection provides a solid theoretical foundation for analyzing gradient leakage, moving beyond previous ad-hoc or heuristic defenses.\n2.\tThe authors propose a new viewpoint by linking flat minima searching to privacy protection. They argue that optimization toward flatter minima inherently smooths the loss landscape, thereby suppressing the mutual information between data and gradients. This insight reveals that the pursuit of flat minima not only benefits generalization but also acts as an intrinsic defense mechanism against data leakage.\n3.\tThe paper rephrases the convergence process of flat-minima-search algorithms such as FedSAM and provides formal robustness guarantees against GIAs. This theoretical assurance strengthens the credibility of the proposed defense mechanism and bridges the gap between convergence behavior and privacy robustness in FL."}, "weaknesses": {"value": "1. The authors claim that existing approaches that perturb training data or gradients “lack a principled way of knowing when gradients become vulnerable to GIA.” However, it remains unclear whether the effectiveness of these perturbation-based defenses can be interpreted through the proposed mutual information framework between G and X. For example, in the case of differential privacy (DP), under a fixed privacy budget, would I(G+\\delta;X) still remain within the theoretical upper bound derived in this paper? How does I(G+\\delta;X) compare to I(G;X) is it theoretically larger or smaller? The authors do not discuss these aspects, which makes the paper’s motivation somewhat insufficient.\n\n2. In the experimental section, the authors do not present the impact of flat-minima-search optimization methods on the global model’s accuracy and convergence speed. Intuitively, constraining the upper bound of I(G;X) may hinder the model’s ability to sufficiently learn informative features of inputs from gradients, potentially leading to slower convergence and reduced accuracy. Moreover, the trade-off among model accuracy, convergence efficiency, and privacy protection is an important issue in this research area. The authors should provide a clear discussion on how the flat-minima optimization influences these factors.\n\n3. The authors’ experimental setup for federated learning is impractical. In realistic FL scenarios, client data are typically non-IID because each user holds private datasets with diverse distributions; evaluating only under IID setting therefore risks overestimating robustness and generality. Moreover, the datasets used in the paper appear limited in scale and diversity. The authors should extend experiments to larger and more diverse benchmarks (for example, CIFAR-100 and CelebA) to show that the proposed Hessian-based insights and flat-minima defenses scale beyond small datasets.\n\n4. The authors should provide complete descriptions for the x-axis and y-axis in the line charts presenting the experimental results, especially for Figs. 1, 3, and 4, ensuring that the format is consistent with Fig. 2. Additionally, the labels on the right side of Figs. 5 and 6 are in white, which should be changed to black for better visibility."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BoBBb7xAy4", "forum": "Xal7wChhqw", "replyto": "Xal7wChhqw", "signatures": ["ICLR.cc/2026/Conference/Submission6501/Reviewer_orDq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6501/Reviewer_orDq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791740768, "cdate": 1761791740768, "tmdate": 1762918870740, "mdate": 1762918870740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies an effective way to mitigate gradient inversion attacks, proposes to use a flat minima searching-based algorithm and analyzes from the perspective of mutual information. The paper provides both theoretical analysis and numerical results."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The research topic is quite interesting and useful in federated learning, which aims at performing privacy-preserved distributed machine learning."}, "weaknesses": {"value": "* The paper is hard to read starting from Section 3. The most obvious weakness is that no algorithm pseudocode is provided, even in the appendix. Without pseudocode, the reviewer cannot evaluate the correctness of the algorithm or the analysis.\n* It seems that the paper shares a lot of common ground with, both for analysis and problem formulation, Qu et al. (2022a). Then, the question becomes what differentiates this paper from Qu et al. (2022a), other than that this paper deals with a multi-agent system?\n* It is also interesting that the main convergence results in lines 215 and 220 are not stated as a theorem(s). Still, it remains unclear what FedSAM is. It is also a bit strange that the authors choose to keep all the assumptions in the appendix, which render many constants in the theorem itself unstated. For example, Lipschitz constant $L$,\n* Due to the above reasons, I believe the current paper in its current form is far from ready for review."}, "questions": {"value": "* Can the authors provide algorithm pseudocode?\n* Can the authors unbiasedly state their contributions as compared to Qu et al. (2022a)?\n* Can the authors organize their theoretical results with a formal statement?\n* Can the authors thoroughly reexamine their paper to make sure the terms are used after they are properly stated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jvq53nnfpS", "forum": "Xal7wChhqw", "replyto": "Xal7wChhqw", "signatures": ["ICLR.cc/2026/Conference/Submission6501/Reviewer_V9LN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6501/Reviewer_V9LN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795050291, "cdate": 1761795050291, "tmdate": 1762918870152, "mdate": 1762918870152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigated the problem of gradient inversion attacks in distributed learning, aiming to understand under what circumstances the gradient is likely to leak the original data from an information theory perspective. Gradient Inversion Attacks are interpreted with mutual information between the gradients and the data, which is revealed to be upper-bounded by the Hessian of loss and connects to the recently proposed sharpness-aware minimization (SAM) approaches. It reveals from both theoretical and empirical aspects that SAM improves robustness against gradient inversion attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Interpreting gradient inversion attacks with the mutual information between gradients and the data makes sense, and revealing its connection with SAM is interesting.\n2. The arguments are validated from both theoretical and experimental perspectives."}, "weaknesses": {"value": "1. The experiments are conducted mainly on small networks, and the effectiveness on larger models is not verified.\n2. The major baseline in this paper is FedAvg, which has been shown to be vulnerable to gradient inversion attacks. Without comparison with existing defense mechanisms, it is hard to tell the effectiveness of the proposed method.\n3. The comparison with FedAvg may not necessarily be fair, considering that SAM-based approaches essentially require additional SGD steps, which makes gradient inversion attacks more difficult.\n4. The model performance in terms of test accuracy should be evaluated as well."}, "questions": {"value": "1. In Table 1, it can be observed that FedASAM performs slightly better than other SAM-based approaches. Could you please explain?\n2. Could the authors consider comparing the SAM-based approaches with existing defense mechanisms?\n3. Since additional SGD steps increase the difficulty of gradient inversion, it is not clear if worse reconstruction is due to more local steps (i.e., one gradient descent step and one gradient ascent step for SAM) or the flat minima search principle. Could the authors further clarify?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ev58aq2lqm", "forum": "Xal7wChhqw", "replyto": "Xal7wChhqw", "signatures": ["ICLR.cc/2026/Conference/Submission6501/Reviewer_sxsy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6501/Reviewer_sxsy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913203268, "cdate": 1761913203268, "tmdate": 1762918869738, "mdate": 1762918869738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies gradient inversion attacks in federated learning via mutual information. The authors link the mutual information to the Hessian matrix, suggesting that a flatter loss landscape can enhance privacy protection. They propose utilizing sharpness-aware optimization algorithms, such as SAM, as a defense mechanism. They conduct some experiments to validate this idea."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-organized and easy to follow.\n\n* A flatter loss landscape contributes to privacy preservation is intuitive\n\n* The experimental results generally support the authors' core idea regarding the correlation between flatness and defense against GIAs."}, "weaknesses": {"value": "* Studying data leakage in FL using information theory is not fresh [1]. The theoretical results appear to be direct applications or minor reformulations of existing theoretical results regarding the relationship between the Fisher Information, the Hessian, and information leakage. The technical contribution in this regard is limited.\n\n* This paper is not well-suited for Learning Theory track, as it does not delve deeply into core learning theory aspects. It would be a much better fit for Privacy or Adversarial Learning track.\n\n* The authors have missed many recent and crucial studies [1,2]. Some of the attack baselines used in the experiments are somewhat outdated. The paper also does not include defense methods. A more thorough survey is needed.\n\n* The effectiveness of using sharpness-aware optimization as a defense is questionable. Sharpness-aware algorithms often require a significant number of iterations to settle into a flat minimum. An adversary could launch the attack at early time points in the training process (e.g., during initialization or early transient phases) when the loss surface is still sharp.\n\n* The experiments rely on small models and datasets. Moreover, based on Table 1, the defense performance is insufficient. The PSNR values often remain above 20, which may still allow for recognizable image reconstruction and thus, significant privacy leakage.\n\n[1] Defending Against Data Reconstruction Attacks in Federated Learning: An Information Theory Approach\n\n[2] Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nh5T25Dgpw", "forum": "Xal7wChhqw", "replyto": "Xal7wChhqw", "signatures": ["ICLR.cc/2026/Conference/Submission6501/Reviewer_gH8Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6501/Reviewer_gH8Q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961671771, "cdate": 1761961671771, "tmdate": 1762918869378, "mdate": 1762918869378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}