{"id": "zB84FcNK2m", "number": 6409, "cdate": 1757981618338, "mdate": 1763459719848, "content": {"title": "ResLR: Residual-Low-Rank Surrogates for Stable and Fast Context Adaptive Computing in Large Language Models", "abstract": "Large Language Models (LLMs) achieve state-of-the-art results on diverse tasks, yet inference remains expensive because every token traverses the full Transformer stack. Recent context adaptive computing methods mitigate this cost by token-wise layer skipping, but their per-layer routing is volatile, leading to accuracy oscillations and an extended fine-tuning process. We trace this instability to two issues: (i) direct skips violate the model’s functional hierarchy, and (ii) per-layer routing fails to exploit the similarity of activations between neighboring layers. We therefore propose a unified acceleration framework addressing both problems. First, we introduce the Residual-Low-Rank (ResLR) surrogate, a lightweight bypass that distills the residual transformation between consecutive layers into a low-rank operator within a compact subspace, thus synthesizing the effect of the skipped layers and preserving hierarchy. Second, we devise Block-Wise Multi-Path Routing, which clusters neighboring layers into blocks and issues a single routing decision per block, explicitly leveraging activation similarity to stabilize computation and reduce gating overhead. The method integrates into standard LoRA fine-tuning without extra stages. Across question answering, mathematical reasoning, and commonsense inference benchmarks, it reduces FLOPs by 48%–52\\% and yields $\\sim$1.9$\\times$ wall-time speed-ups while outperforming static and dynamic baselines. With feature probing suggests a $\\sim$90% functional preservation, variance analysis shows 42.3% lower score standard deviation and 53.7\\% more stable routing than layer-skipping approaches, establishing ResLR and block-wise routing as a robust approach for practical, low-cost LLM inference.", "tldr": "ResLR, an adaptive computing framework using low-rank surrogates and block-wise routing. It preserves the model's functional hierarchy, cutting inference FLOPs by 48%-52% for a ~1.9× speedup, improves stability, and achieves SOTA task performance.", "keywords": ["Residual-Low-Rank", "context adaptive computing", "Block-Wise Multi-Path Routing", "dynamic inference", "self-distillation"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cd5e2c2fcb92d91b33f66b234f67c701645a046c.pdf", "supplementary_material": "/attachment/8e61abbda605af5d3ce8e05c79e26f0e7d4438e3.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses routing instability in token-wise layer-skipping methods for efficient LLM inference. The authors propose ResLR surrogates, which is a low-rank approximation of multi-layer residuals trained via self-distillation—combined with block-wise routing that makes unified decisions for groups of layers. Experiments on LLaMA2-7B/13B show 48-52% FLOPs reduction with improved stability (53.7% lower routing variance) and performance gains over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is well motivated. The paper identifies the routing instability issue with existing layer-skipping methods caused by violating functional hierarchy and per-layer routing volatility. The empirical evidence (Figure 2) effectively demonstrates this problem.\n\n2. The experimental results are sound, as table one clearly shows empirical gains."}, "weaknesses": {"value": "1. The self-distillation formulation Is Circular. In particular, equation defines the main loss term, where E_{i+1} is the \"teacher\" output. But the paper never clarifies if E_{i+1} is computed with or without the surrogate f participating in the forward pass. If the surrogate participates: The target E_{i+1} depends on f, creating a moving target problem. This invalidates the entire theoretical analysis, as lemma 1 assumes approximating a fixed residual \\delta E, but here \\delta E depends on the thing being trained. The bias-variance decomposition (Eq. 9) breaks down because \\phi* is no longer well-defined. If the surrogate is frozen: This should be stated explicitly, and you need ablations showing training stability with/without this constraint.The self-distillation literature addresses these circularity issues with EMA teachers or momentum updates. Your formulation ignores this entirely, making the theoretical contribution questionable.\n\n2. \"Preserving Functional Hierarchy\" Is not validated. The core claim is that ResLR preserves the model's functional hierarchy better than direct skipping (lines 79-91), but this is never directly tested. The evidence provided is indirect: higher mutual information just shows correlated decisions, not functional correctness; lower variance shows stability, not hierarchy preservation; better task performance has many possible explanations."}, "questions": {"value": "The paper claims all baselines use \"self-distillation joint training\" but D-LLM's original paper doesn't. Did you retrain everything under your protocol? If so, are the comparisons fair to methods designed for different training regimes?\n\nAlso: the paper only compares layer-skipping methods. Speculative decoding achieves 2-3× speedups with zero accuracy loss. How does ResLR compare? Claiming \"state-of-the-art\" without comparing these is misleading."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gpPQFc7zkE", "forum": "zB84FcNK2m", "replyto": "zB84FcNK2m", "signatures": ["ICLR.cc/2026/Conference/Submission6409/Reviewer_f4Zp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6409/Reviewer_f4Zp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760905256294, "cdate": 1760905256294, "tmdate": 1762918809674, "mdate": 1762918809674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses LLM inference costs through dynamic layer skipping. The authors identify routing instability in existing methods and propose ResLR with two components: (1) low-rank surrogates to approximate skipped layers, motivated by SVD analysis showing 90% information retention in 50 components, and (2) block-wise routing that groups adjacent layers for unified decisions. Results show 48-52% FLOPs reduction with 1.9× speedup and improved stability vs D-LLM."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated problem. Figure 2 quantifies routing instability with mutual information and variance metrics, providing clear motivation.\n\n2. Reasonable design choices. SVD analysis supports the low-rank assumption. Block-wise routing addresses both efficiency and stability in a straightforward way.\n\n3. Consistent improvements across 9 benchmarks with both FLOPs reduction and wall-time speedup."}, "weaknesses": {"value": "1. There are some reproducibility concerns, as several essential implementation details are not fully specified:\n- Specific rank r values and their selection criteria\n- Gating network architecture and training procedure\n- Loss balancing between distillation and task objectives\n- ResLR insertion strategy (which layers, all or selective?)\n\nWith these details clarified, it would be easier to reproduce the method.\n\n2. The paper lacks explanation for why inter-layer residuals have low-rank structure. More critically, the claim that ResLR \"preserves functional hierarchy\" is not rigorously justified. How does low-rank projection preserve hierarchy differently than direct skipping? Without probing experiments or formal analysis, this core claim remains unsubstantiated.\n\n3. Testing only 7B and 13B is insufficient for \"excellent scalability\" claims. No evaluation on:\n- Models larger than 13B (30B, 70B)\n- Long-context scenarios (8k+ tokens)\n- Different architectures beyond LLaMA\n\nThe generalization remains questionable."}, "questions": {"value": "1. What specific rank r do you use? How is it selected?The paper mentions r << d but never specifies actual values. Is it the same across all layers? This is fundamental for reproducibility.\n2. How exactly does ResLR preserve functional hierarchy? This is your core claim, but the mechanism is unclear. Can you provide probing experiments or formal analysis showing that low-rank projection preserves hierarchy better than direct skipping?\n3. Does the low-rank structure hold across different models and tasks?\n4. Why is block-wise routing necessary if ResLR already addresses the hierarchy problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tfuaUWyxgx", "forum": "zB84FcNK2m", "replyto": "zB84FcNK2m", "signatures": ["ICLR.cc/2026/Conference/Submission6409/Reviewer_XiPs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6409/Reviewer_XiPs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761647421507, "cdate": 1761647421507, "tmdate": 1762918809392, "mdate": 1762918809392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical bottleneck in deploying Large Language Models (LLMs): high inference cost due to every token passing through all layers of the Transformer stack. While recent work has explored layer skipping to reduce this cost, such methods often suffer from unstable routing decisions and degrade the model’s internal functional structure, leading to volatile accuracy and inefficient fine-tuning.\n\nTo solve this, the authors propose ResLR, a unified acceleration framework that improves both efficiency and stability in dynamic inference. It combines two key innovations:\n1。 ResLR surrogates that replaces skipped Transformer layers with a learned low-rank operator, preserving the model’s functional hierarchy.\n2. Block-Wise Multi-Path Routing: Instead of deciding skip decisions per layer, the model groups layers into blocks and makes a single routing decision per block, reducing overhead and stabilizing the computation.\n\nThis framework is plug-and-play with standard LoRA fine-tuning and demonstrates state-of-the-art trade-offs on multiple reasoning and language benchmarks, reducing FLOPs by over 50%, increasing inference speed by 1.9×, and improving output stability over previous dynamic approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies two causes of instability in prior layer-skipping methods—(1) disrupting the model’s learned depth hierarchy, and (2) noisy, per-layer routing without inter-layer coordination.\n2. The proposed ResLR surrogate is proposed to learn to approximate the combined residual transformation of skipped layers using a low-rank structure, trained via self-distillation, and achieve good empirical results.\n3. The paper provides a clear bias-variance decomposition for the surrogate’s approximation error and justifies its rank selection.\n4. The method can be integrated seamlessly into existing LoRA fine-tuning pipelines."}, "weaknesses": {"value": "1. Unlike standard LoRA or other low-rank fine-tuning methods, the proposed ResLR surrogate operates as an external bypass module rather than a low-rank correction to existing Transformer weights. As a result, the trained ResLR components cannot be merged back into the original model weights, making deployment more complex and limiting compatibility with existing model-serving optimizations that rely on weight merging (e.g., static inference graph export).\n2. Because the dynamic router determines which layers to execute per token, the method requires custom inference logic and is not directly compatible with existing static inference engines, possibly complicating real-world deployment.\n3. While tested on LLaMA2, the method’s performance and compatibility with other LLM families (e.g., Qwen3, Llama3.x) is not assessed."}, "questions": {"value": "While block-wise routing amortizes decision cost, inference still requires executing token-level gating logic. In high-throughput or multi-token batch settings, how significant is this routing overhead, especially as block size or model size increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jGjv4RbH88", "forum": "zB84FcNK2m", "replyto": "zB84FcNK2m", "signatures": ["ICLR.cc/2026/Conference/Submission6409/Reviewer_gncQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6409/Reviewer_gncQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925871959, "cdate": 1761925871959, "tmdate": 1762918808975, "mdate": 1762918808975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Thanks for the Reviews: Clarifications and Revision #1 Summary"}, "comment": {"value": "We thank the reviewers for their insightful and constructive feedback. This official comment aims to address the initial concerns, clarify misunderstandings, and summarize the revisions made to the manuscript. **Throughout the discussion period, we are committed to being fully responsive and open to addressing the concerns of both reviewers and readers.**\n\n**Main technical clarification**\n\n1.**No circular self-distillation** The distillation target is generated by the original Transformer block in a parallel branch. Routing uses hard Gumbel-Softmax sampling; distillation is applied only when the skip path is selected. This can be evident in our initial code submission, and details appear in our dedicated reply to Reviewer f4Zp.\n\n2.**Orthogonality to speculative decoding** We hold that Speculative methods accelerate decoding via proposal–verification; ResLR reduces per-step compute. The two are complementary. In our dedicated reply to Reviewer f4Zp, we discussed the orthogonality between our proposed dynamic inference method and various speculative decoding approaches, including the EAGLE series, Medusa, and generic draft model-based speculative decoding.\n\n3.**Deployment capabilities to existing inference frameworks** We argue that the surrogate model cannot be merged into the main branch. However, this architecture can be exported as a bi-graph and computed with batched inference through scatter and gather operations. In our separate response to Reviewer gncQ, we discuss the similarity of this structure to a Mixture-of-Experts (MoE) model with K=2, which ensures compatibility with existing inference frameworks. We acknowledge that our method does not incorporate additional hardware-aware optimizations and, therefore, have included a discussion of this as a direction for future work in the conclusion.\n\n**Revision #1 summary**\n\n- **New results**: Llama3-8B and Qwen3-8B experiments, appended after the main results table 1.\n\n- **Evidence for functional hierarchy preservation**: We have augmented the manuscript with additional experimental sections and an Appendix E.8. Furthermore, in the introduction, we have clarified the operational definition of \"functional hierarchy preservation\" to substantiate the central claim of our paper. Quantitative analysis of our experiments reveals that our approach preserves approximately 90% of the functionalities of the pre-trained LLM. For a more detailed discussion, please refer to our individual responses to Reviewer f4Zp or XiPs.\n\n- **Presentation fixes**: Clarifying definitions, standardizing notation, and elaborating on details.\n\n- **Enhanced reproducibility checklist**: We acknowledge that due to page limitations, the description in the main paper was concise, with most details deferred to the appendix, requiring frequent jumps\\&navigation. In the revised version, we have enhanced the \"reproducibility statement\" section by incorporating centralized hyperlinks to improve reader readability.\n\nThe revised sections are highlighted in yellow."}}, "id": "K5s9pkGxcO", "forum": "zB84FcNK2m", "replyto": "zB84FcNK2m", "signatures": ["ICLR.cc/2026/Conference/Submission6409/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6409/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6409/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763116620229, "cdate": 1763116620229, "tmdate": 1763117129572, "mdate": 1763117129572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}