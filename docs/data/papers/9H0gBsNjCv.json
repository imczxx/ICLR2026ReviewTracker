{"id": "9H0gBsNjCv", "number": 12539, "cdate": 1758208471093, "mdate": 1759897503072, "content": {"title": "ContextPRM: Leveraging Contextual Coherence for multi-domain Test-Time Scaling", "abstract": "Process reward models (PRMs) have demonstrated significant efficacy in enhancing the mathematical reasoning capabilities of large language models (LLMs) by leveraging test-time scaling (TTS). However, while most PRMs exhibit substantial gains in mathematical domains, the scarcity of domain-specific training data and knowledge-based learning patterns limits their generalization ability when faced with other domains. To address this limitation, we shift the learning objective from verifying domain-specific knowledge to modeling domain-agnostic logical flow. Centering on \\textit{contextual coherence} between chain-of-thought (CoT) steps, our approach is realized through a novel data annotation and training framework, which enhances the model's generalization capabilities across diverse domains.  For instance, our resulting model, \\textbf{ContextPRM}, achieves a notable 6.5\\% average accuracy improvement over the majority voting baseline via weighted majority voting across nine non-mathematical domains in MMLU-Pro, including law, history, and philosophy, significantly surpassing the 2.2\\% improvement from VersaPRM and 0.5\\% gains from other mathematics-focused PRMs, demonstrating consistent performance across both mathematical and non-mathematical domains.", "tldr": "We shift the training of Process Reward Models from verifying domain-specific correctness to modeling domain-agnostic contextual coherence, achieving state-of-the-art multi-domain generalization.", "keywords": ["Process Reward Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a359f1ba1167c914adf2b4eb5e202f4055f55fdf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the poor cross-domain generalization of Process Reward Models (PRMs), which typically excel in mathematics but fail in other domains. The authors posit this is because traditional PRMs learn domain-specific correctness rather than domain-agnostic logical flow . They propose ContextPRM, a novel framework that shifts the learning objective to modeling the contextual coherence between reasoning steps. This is achieved through a synergistic combination of a new data annotation standard that labels the logical validity of step-to-step transitions and a corresponding context-aware training methodology that forces the model to evaluate these transitions in isolation . Experiments on the MMLU-Pro benchmark show that ContextPRM achieves state-of-the-art performance, particularly in non-mathematical domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's primary strength lies in its novel and intuitive reformulation of the PRM learning objective. Shifting the focus from \"is this step factually correct?\" to the logical coherence is a novel approach to decouple reasoning ability from domain-specific knowledge, directly targeting the identified problem of cross-domain generalization.\n\nThis conceptual contribution is backed by robust empirical results. The model achieves substantial gains over strong baselines, including the prior SOTA (VersaPRM), in a wide array of non-mathematical domains . The claims are well-supported by extensive experiments.\n\nFurthermore, the ablation studies are comprehensive. The finding that neither the context-based training nor the context-based labeling alone provides significant benefit, but their combination yields a dramatic performance increase which provides strong evidence for the authors' core claim of a necessary synergistic effect . The single-domain training analysis, which shows that training on \"logic-intensive\" domains yields better generalization than \"knowledge-intensive\" domains, is a compelling piece of evidence supporting the method's mechanism ."}, "weaknesses": {"value": "The ablation study reveals the full ContextPRM performs 2.2% worse than the baseline in math, suggesting that the method's trade-off for generalization is a notable loss in a domain that relies on strict, verifiable correctness rather than just contextual flow.\n\nThe method's practicality is dependent on a new, complex, and potentially costly data annotation pipeline . The entire approach relies on re-labeling the training data using an LLM verifier (gpt-4o-mini). This introduces a dependency on the quality of the verifier and reduces the method's simplicity."}, "questions": {"value": "The context-aware training method intentionally restricts the model's input to adjacent step pairs $(S_{i-1}, S_i)$ to evaluate the \"logical transition\". This design seems to optimize for local logical coherence. However, a reasoning chain can fail because a step $S_i$ relies on a flawed premise established in a non-adjacent prior step (e.g., $S_{i-2}$), even if the local transition from $S_{i-1}$ to $S_i$ is perfectly logical.Does this input limitation mean ContextPRM is primarily constrained to evaluating local coherence? How does the model learn to distinguish a \"locally coherent but globally flawed\" step from a \"globally correct\" one, especially when its annotation strategy marks all steps after the first error as \"incorrect\"? Was a more detailed analysis conducted on the model's ability to trace errors back to their non-adjacent roots?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper does not appear to raise any ethical concerns. It does not involve human subjects, sensitive data, or potentially harmful applications."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v8C4xy2p1o", "forum": "9H0gBsNjCv", "replyto": "9H0gBsNjCv", "signatures": ["ICLR.cc/2026/Conference/Submission12539/Reviewer_iymL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12539/Reviewer_iymL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761494767986, "cdate": 1761494767986, "tmdate": 1762923401579, "mdate": 1762923401579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Standard Process Reward Models (PRMs) excel at mathematical reasoning but fail to generalize to non-mathematical domains due to a reliance on domain-specific knowledge. This paper introduces ContextPRM, a novel approach that addresses this limitation by shifting the learning objective from verifying isolated correctness to modeling domain-agnostic logical flow. Through a new data annotation standard and a context-aware training method focused on the logical coherence between reasoning steps, ContextPRM achieves state-of-the-art multi-domain generalization. It demonstrates a 6.5% average accuracy improvement over the majority voting baseline across nine non-mathematical MMLU-Pro domains, significantly surpassing the 2.2% gain from the previous best model, VersaPRM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies a significant limitation of current Process Reward Models (PRMs): while effective in mathematical domains, they fail to generalize to non-mathematical domains like law, history, or philosophy. \n\n1. Instead of adding more domain-specific knowledge, the paper proposes a novel shift in the learning objective: moving from verifying \"domain-specific correctness\" to modeling \"domain-agnostic logical flow\", which is an intuitive and reasonable idea.\n\n1. In these domains, ContextPRM improves average accuracy by 6.5% over the majority voting baseline, outperforming the previous SOTA (VersaPRM), which achieved a 2.2% gain."}, "weaknesses": {"value": "1. The model's strong cross-domain generalization comes at a cost. The ablation study reveals that the full ContextPRM model shows a 2.2% performance decrease in the math domain compared to the baseline, suggesting a trade-off was made to achieve broader applicability. \n\n1. The qualitative \"Failure Cases\" reveal that the method still fails when complex, specialized knowledge is required. In examples from law (tort liability) and psychology (Rancho Los Amigos Scale), both ContextPRM and the baseline model failed to identify the fundamental initial error. This suggests that focusing on \"logical flow\" cannot fully compensate for a lack of deep, domain-specific understanding.\n\n1. Overall, the contribution is marginal. To my understanding, this paper just extends the definition of \"bad\" steps during annotation, and introduces a simple concatenation of steps as a new \"contextualized representation\", which does not really tackle the challenge of PRMs."}, "questions": {"value": "1. Why does ContextPRM underperform VersaPRM in the math domain? Could you further explain it?\n\n1. In Section 3.1, what is the formatting function $\\mathcal{F}$, just simple concatenation? How is it defined? What is the definition of the new context $\\tilde{\\mathcal{S}}$ and the contextualized step $\\mathcal{P}$? How does the coherence label $c_i$ come? Could you further explain them?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iDFph6gb8V", "forum": "9H0gBsNjCv", "replyto": "9H0gBsNjCv", "signatures": ["ICLR.cc/2026/Conference/Submission12539/Reviewer_jNz1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12539/Reviewer_jNz1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951671712, "cdate": 1761951671712, "tmdate": 1762923401035, "mdate": 1762923401035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper suggests context-coherence-based process reward modeling. Unlike past works that aim to evaluate the isolated correctness of a specific step, this work aims to assess its logical connection with the previous step. In some sense, it sounds a bit like a NLI task, aiming to evaluate whether two connected segments are entailments or contradictions.\n\nThe paper creates a labeled dataset via gpt-4o-mini and trains Llama-3.1-8B-PRM800K on the dataset. The performance of the trained model is showcased by a labeled subset of MMLU-Pro. Based on the performance gains achieved when the model is used as a external verifier. \n\nThe trained model outperform baslines highlighting the effectivenss of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The paper proposes an NLI-like training schema for PRMs"}, "weaknesses": {"value": "(1) I might have missed something, but I am not yet persuaded that this is a big difference from what other PRMs have been doing. Other PRM models also receive the S-1 steps (saying step S is the one of interest). While they may have not been explicility trained to look at logical connection of previous steps with step S I think that would have been kept into account. Accordingly in Figure 3 we can see that the performance gains is more pronounced in non-math domains. Which I think is not surprising as the authors trained on additional non-math data.\n\n(2) I think the paper would have been more interesting if their was more focus on math. Math relies heavily on logical coherence of steps, which I think is a more suitable domain for the proposed method.\n\n(3) A qualitative study on whether the trained model effectively detects different types of errors, compared to correctness-only PRMs, might have helped persuading me about (1), that this model is different from past models \n\n(4) Not a major issue, but I would have preferred if the training have been done to different models as well. Is the PRM800k necessary for this to work? Does it work in smaller/bigger models?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2NqLK6Wa3d", "forum": "9H0gBsNjCv", "replyto": "9H0gBsNjCv", "signatures": ["ICLR.cc/2026/Conference/Submission12539/Reviewer_wDYj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12539/Reviewer_wDYj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12539/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058487268, "cdate": 1762058487268, "tmdate": 1762923399492, "mdate": 1762923399492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}