{"id": "nw0pePP5qd", "number": 10155, "cdate": 1758162333885, "mdate": 1763019184552, "content": {"title": "DP-C4: Eliminating Solution Bias in Differentially Private Optimization via Coupled Clipping with Adaptive Thresholds", "abstract": "Differentially private (DP) stochastic optimization algorithms are widely used in privacy-preserving deep learning, where per-sample gradient clipping and noise injection protect sensitive information. However, these operations limit existing DP methods to converge within a constant-radius neighborhood of the first-order stationary point, leading to solution bias and the well-known privacy-utility trade-off. To enhance model utility, we propose a novel framework called DP-C4, which is designed to be error-Consistently-decayed, Coupledly-clipped, solution-Calibrated, and Convergence-guaranteed; this is the first time such a method is proposed. Specifically, it incorporates a carefully designed coupled clipping strategy and adaptive clipping thresholds, ensuring that both clipping bias and noise variance asymptotically vanish, thereby correcting the DP-induced solution bias. Furthermore, we develop a memory-efficient variant that reduces storage complexity without compromising privacy guarantees. We prove that our method converges to the optimum in strongly convex case by properly constructing a Lyapunov function, and to a diminishing neighborhood of the first-order stationary point in nonconvex case. Our theoretical results are supported by numerical experiments.", "tldr": "", "keywords": ["Differentially private", "stochastic optimization algorithm", "privacy-utility trade-off"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/8d5ebc0fcf91e3e83b96c6ff2e394b0ca70cc837.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes DP-C4 framework that employs coupled clipping of gradient differences with adaptive thresholds. The key innovation is ensuring both clipping bias and noise variance asymptotically vanish, attempting convergence in strongly convex settings and to a diminishing neighborhood in nonconvex cases. The authors provide supportive theoretical convergence guarantees, privacy analysis and empirical validation across several tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It tackles the limitation of DP optimization the inherent solution bias caused by constant clipping and noise. This is a practically relevant problem that has received limited attention in prior works.\n2. The fixed-point analysis in Section 3.3 demonstrates why DP-C4 eliminates solution bias while existing methods do not. Also, the Lyapunov functions (Theorem-1) for analyzing DP algorithms is promising.\n3. It includes ablation studies examining the impact of key hyperparameters including the overall clipping threshold $C$, different anchor update routines (R1- R4), large batch sizes $|D'|$ and update probability $p$. These analysis provides insights into the robustness and sensitivity of  DP-C4 and  DP-C4+ across parameter choices."}, "weaknesses": {"value": "1. The core idea closely follows DP-SVRG by clipping gradient differences $\\nabla f_i(x^k) - \\nabla f_i(w^k)$. While the coupled clipping with adaptive thresholds is new, the fundamental structure is borrowed from variance reduction techniques. The paper should more clearly articulate what is genuinely novel beyond adaptive threshold selection.\n2. A thorough comparison table is missing, showing Big-O complexity bounds (iteration complexity, sample complexity, etc.) alongside recent SOTA methods (e.g., DP-Adam, DP-SCAFFOLD) and other variance-reduced DP methods. Without such a table, it is impossible to justify the theoretical improvements and the additional computational overhead. The paper claims memory efficiency for DP-C4$^+$, however, it lacks formal complexity analysis or empirical runtime comparisons.\n3. The convergence rates in Theorems 1-2 are not explicitly stated in standard $\\mathcal{O}(\\cdot)$ notation and compared against known rates for DP-SGD ($\\mathcal{O}(1/\\sqrt{T})$ for nonconvex and DP-SVRG. Therefore, it is unclear whether the asymptotic solution bias elimination comes at the cost of slower convergence or higher per-iteration complexity.\n4. The results section is not sufficiently strong to support the claims. Table-1 compares methods at fixed $C=1$, but doesn't control for the actual privacy budget ($\\varepsilon$ ) consumed. Different methods may use different $\\varepsilon$ to achieve these results, making the comparison unfair.\n5. No analysis of how performance varies with privacy budget ($\\varepsilon$). Also, there is limited discussion of how DP-C4 relates to recent adaptive methods beyond DiceSGD."}, "questions": {"value": "1. Could you clarify how to implement the gradual increase of $C_2$ while maintaining the convergence guarantee from Theorem-1? Given that the theorem requires $C_2 \\geq \\frac{\\tau}{e}+1$ and $\\min { ||\\nabla f(w^k)||, ||x^k - x^*||}> e$, what algorithm or strategy would you recommend for adapting $C_2$ during training to avoid excessive noise injection in early iterations?\n2. The paper claims DP-C4 achieves \"the same level of privacy protection while adding less noise'' compared to DP-SGD. Could you provide experiment results with privacy-utility curves at equal ($\\varepsilon$, $\\delta$) budgets with optimally tuned hyperparameters for each method?\n3. The choice of large batch size $|D'| = 4096$ is not justified clearly. How sensitive are results to batch size?\n4. The nonconvex convergence result in Equation (7) involves separate $\\lambda$ coefficients ($\\lambda_1^k$, $\\lambda_2^k$, $\\lambda_3^k$, $\\lambda_4^k$) with complex dependencies on $\\mathbb{P}^k$, $\\mathbb{P}_1^k$, $\\mathbb{P}_2^k$, which is difficult to interpret. Why are so many separate terms necessary? Could these $\\lambda$ coefficients be combined into fewer or more interpretable quantities? The claim that bias \"gradually vanishes'' appears to rely on taking $C_2 \\to \\infty$, it directly contradicts the noise minimization objective since larger clipping thresholds require more noise for privacy. How to set $C_1$ and $C_2$ to balance these competing objectives?\n5. The fixed-point analysis in Section 3.3 assumes convergence has occurred, but doesn't address the transient behavior. Does the fixed-point analysis provide any rates of convergence or characterization of the transient behavior before reaching the fixed point?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ebg2tqSSLo", "forum": "nw0pePP5qd", "replyto": "nw0pePP5qd", "signatures": ["ICLR.cc/2026/Conference/Submission10155/Reviewer_Y9J6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10155/Reviewer_Y9J6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888596136, "cdate": 1761888596136, "tmdate": 1762921524558, "mdate": 1762921524558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "hrDwooPlue", "forum": "nw0pePP5qd", "replyto": "nw0pePP5qd", "signatures": ["ICLR.cc/2026/Conference/Submission10155/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10155/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763019183558, "cdate": 1763019183558, "tmdate": 1763019183558, "mdate": 1763019183558, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript addresses the problem of mitigating both clipping bias and noise variance to eliminate solution bias in differentially private (DP) stochastic optimization algorithms. The authors propose a novel framework, DP-C4, to tackle the solution bias introduced by per-sample gradient clipping and noise injection used for protecting sensitive information. DP-C4 employs coupled per-sample gradient clipping with adaptively chosen thresholds to reduce such bias. The authors prove that the proposed method can simultaneously ensure the asymptotic vanishing of clipping bias and noise variance without compromising privacy protection, achieving the same privacy level as DP-SGD. They also provide a novel convergence analysis for both the strongly convex case (proved to converge to the optimum) and the nonconvex case (shown to converge to a diminishing neighborhood of a first-order stationary point). Moreover, to reduce the memory burden of DP-C4, they propose DP-C4+, an extended version that removes the need to store all gradients involved in the computation while still maintaining theoretical properties of DP-C4. Finally, they present empirical validations and ablation studies demonstrating that DP-C4 and DP-C4+ achieve better privacy-utility trade-offs than existing baselines across multiple datasets and tasks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This manuscript tries to address an emerged  yet crucial issue in DP optimization, the joint mitigation of clipping bias and noise variance, where most existing DP algorithms (e.g., DP-SGD, DP-SVRG) tend to sacrifice one for the other."}, "weaknesses": {"value": "The privacy analysis of this paper seems problematic. \n\nStep 11 in the main algorithm does not satisfy DP guarantees. The C_1 and C_2 have not been privately released. Also, it seems that the author claims to add O(1) noise to the gradient average instead of O(1/n) noise expected in standard DP-SGD. I am afraid that the SGD cannot tolerate such a noise which is even larger than the gradient signal."}, "questions": {"value": "Can the author explain or justify the noise parameter selection detailed in the paper with a formal proof?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g78qqfxaA1", "forum": "nw0pePP5qd", "replyto": "nw0pePP5qd", "signatures": ["ICLR.cc/2026/Conference/Submission10155/Reviewer_Rr8d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10155/Reviewer_Rr8d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919940196, "cdate": 1761919940196, "tmdate": 1762921524135, "mdate": 1762921524135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focus on debiasing the clipping bias in DP optimization. The paper proposes clipping the gradient, and difference of gradient at two points, and adopts an SVRG-like update to reduce the variance.\n\nThe paper provides the convergence and privacy analysis for the proposed method under standard assumptions. By constructing a Lyaunov  function, the paper shows that the proposed method converges to a neighborhood of the stationary point with clipping. \n\nNumerical results shows that the proposed method achieves a better performance compared with existing algorithms."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposed a novel algorithm that converges to the neighbourhood the the statioanry point under clipping and without assumming gradient distribution.\n\n2. The paper is clearly written with sufficient details explaining the algorithm and its theoretical properties."}, "weaknesses": {"value": "1. The privacy analysis does not seems to be correct. The clipping threshold $C_{1k}, C_{2k}$ are computed using private data, but they are not privatized by the algorithm. More specifically, the sensitivity of the clipped gradient in the analysis provided in the appendix is incorrect, because the change of a sample not only impact the clipped gradient $g_1, g_2$, but also impact $C_{1}, C_2$. However, the privacy analysis treats the clipping threshold as non-private information and directly use them to determine the size of the noise."}, "questions": {"value": "Please address the potential mistake in the privacy guarantee."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "g50DghXY8m", "forum": "nw0pePP5qd", "replyto": "nw0pePP5qd", "signatures": ["ICLR.cc/2026/Conference/Submission10155/Reviewer_cv9R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10155/Reviewer_cv9R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940635231, "cdate": 1761940635231, "tmdate": 1762921523622, "mdate": 1762921523622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "DP-C4 and DP-C4+ are introduced for differentially private (DP) optimization. The core idea is to use a variance-reduction method for gradient estimation to enhance utility in DP algorithms. The empirical results are promising and convergence guarantees are established. Experiments are carried out on SVM, image classification, and NLP tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The empirical results are promising and convergence guarantees are established. \n- Experiments are carried out on SVM, image classification, and NLP tasks."}, "weaknesses": {"value": "- Baselines are not the latest approaches, and the superiority on the performance is limited. No results on the memory usage for DP-C4+.\n- The assumptions on bounded gradients, L-smoothness, and (µ-)strong convexity mostly don't hold for deep structures."}, "questions": {"value": "- introducing more up-to-date baselines,\n- the backbone methods need to conider more modern architectures.\n- can he assumptiosn on bounded gradients, L-smoothness, and (µ-)strong convexity be relaxed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YYT3BHvHiV", "forum": "nw0pePP5qd", "replyto": "nw0pePP5qd", "signatures": ["ICLR.cc/2026/Conference/Submission10155/Reviewer_73q5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10155/Reviewer_73q5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10155/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762534375893, "cdate": 1762534375893, "tmdate": 1762921522859, "mdate": 1762921522859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}