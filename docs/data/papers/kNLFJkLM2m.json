{"id": "kNLFJkLM2m", "number": 6553, "cdate": 1757988690756, "mdate": 1759897908385, "content": {"title": "Visual Representation Alignment for Multimodal Large Language Models", "abstract": "Multimodal large language models (MLLMs) trained with visual instruction tuning have achieved strong performance across diverse tasks, yet they remain limited in vision-centric tasks such as object counting or spatial reasoning. We attribute this gap to the prevailing text-only supervision paradigm, which provides only indirect guidance for the visual pathway and often leads MLLMs to discard fine-grained visual details from the vision encoder during training. In this paper, we present VIsual Representation ALignment (VIRAL), a simple yet effective regularization strategy that aligns the internal visual representations of MLLMs with those of pre-trained vision foundation models (VFMs). By explicitly enforcing this alignment, VIRAL enables the model not only to retain critical visual details from its own vision encoder but also to complement additional visual knowledge from VFMs, thereby enhancing its ability to reason over complex visual inputs. Our experiments consistently demonstrate performance improvements across all tasks on widely adopted multimodal benchmarks, with gains reaching up to 17.3% and an average improvement of 9.4% over the baseline. Furthermore, we conduct comprehensive ablation studies to validate the key design choices underlying our framework. We believe this simple finding opens up an important direction for the effective integration of visual information in training MLLMs.", "tldr": "", "keywords": ["multimodal large language models", "vision language models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8e0c98701e5b77e55a74a5fea16fea55af2b4d20.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Visual Representation Alignment (VIRAL) to align the internal visual representations of MLLMs with vision foundation models. VIRAL can not only retain critical visual details but also complement additional visual knowledge from VFMs. Experiment results demonstrate VIRAL achieves performance improvement across multiple downstream benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work is well-motivated and employs DINOv2 to supervise the visual features of specific layers in LLaVA-1.5, achieving commendable results across multiple benchmarks."}, "weaknesses": {"value": "1. The number of test benchmarks is too limited to comprehensively evaluate the enhancements this method brings to VLM models.\n2. Introducing a new model for supervising visual features will increase computational costs. The absence of validation and evaluation on more state-of-the-art VLM platforms makes it difficult to assess the generalizability of this method."}, "questions": {"value": "1. The baselines compared in this work are somewhat outdated. Why are more recent models, such as LLaVA-Next and LLaVA-Onevision, not included in the comparison? \n2. This paper only conducts ablation on layer indices using DINOv2. However, it remains unclear whether the optimal layer indices would significantly differ when using different visual encoders."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JzgyRNnL8o", "forum": "kNLFJkLM2m", "replyto": "kNLFJkLM2m", "signatures": ["ICLR.cc/2026/Conference/Submission6553/Reviewer_j4xG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6553/Reviewer_j4xG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735482372, "cdate": 1761735482372, "tmdate": 1762918897282, "mdate": 1762918897282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VIRAL, a regularization strategy that aligns intermediate visual representations in MLLMs with features from pretrained visual foundation models. The approach mitigates the loss of fine-grained visual details caused by text-only supervision, thereby enhancing spatial reasoning and visual understanding."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The phenomenon studied is interesting and worth investigating. The paper provides the first systematic analysis of visual representation degradation under text-only supervision and proposes an elegant alignment strategy that effectively improves visual comprehension in MLLMs.\n2. The proposed method demonstrates strong and consistent improvements across multiple MLLM backbones and benchmarks, with minimal additional training cost.\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. While the proposed alignment strategy shows promising results, the paper does not include comparisons with recent multi-encoder MLLMs (e.g., Eagle [1]) that explicitly fuse complementary vision encoders for fine-grained perception.\n2. As many modern MLLMs employ tiled high-resolution inputs (e.g., LLaVA-NeXT) to enhance fine-grained visual reasoning, a quantitative study of VIRAL under such settings would help clarify its incremental benefit.\n\n[1] Eagle: Exploring the design space for multimodal llms with mixture of encoders, ICLR 2025"}, "questions": {"value": "It would be helpful to know whether the proposed method has been evaluated for its generalization to video-based multimodal tasks, or if the authors believe the framework could be readily extended to such settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JRA75DfdUL", "forum": "kNLFJkLM2m", "replyto": "kNLFJkLM2m", "signatures": ["ICLR.cc/2026/Conference/Submission6553/Reviewer_RSrQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6553/Reviewer_RSrQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904273425, "cdate": 1761904273425, "tmdate": 1762918896646, "mdate": 1762918896646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VIRAL (Visual Representation Alignment), a regularization strategy that aligns intermediate visual representations in MLLMs with features from pre-trained vision foundation models (VFMs). The authors argue that text-only supervision causes MLLMs to discard fine-grained visual details, and explicit alignment with VFM features can preserve this information, improving performance on vision-centric tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The experiments show consistent performance improvements, demonstrating solid empirical gains across multiple benchmarks.\n-  The observation that text-only supervision leads to visual information loss is interesting and the progression from identifying the problem in Sec. 4.1 to proposing solutions is logical."}, "weaknesses": {"value": "- The main concern is the limited novelty and incremental contribution. The core idea of aligning intermediate representations using cosine similarity is standard practice in knowledge distillation and representation learning, and using pre-trained vision models to supervise morden model training is not new, see [1].\n- Insufficient analysis of key design choices. For example, why does DINOv2 work better than other VFMs? The paper doesn't provide deep analysis beyond showing empirical results.\n\n[1] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think. ICLR 2025."}, "questions": {"value": "Why increased sensitivity to token permutation is desirable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0ymonuihQF", "forum": "kNLFJkLM2m", "replyto": "kNLFJkLM2m", "signatures": ["ICLR.cc/2026/Conference/Submission6553/Reviewer_shHW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6553/Reviewer_shHW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936037651, "cdate": 1761936037651, "tmdate": 1762918896204, "mdate": 1762918896204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VIRAL, a regularization strategy aligning the internal visual representations of MLLMs with pre-trained vision foundation models (VFMs). VIRAL achieves up to 17.3% and on average 9.4% improvement across benchmarks, with ablation studies confirming its effectiveness. The method offers a new direction for integrating fine-grained visual information into MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clearly targets the core issue of visual detail loss under text-only supervision and effectively enhances visual reasoning.\n2. Demonstrates strong generality across various encoders and models with consistent benchmark gains.\n3. Adds only ~3% training cost while accelerating convergence, surpassing baselines within 3K steps."}, "weaknesses": {"value": "1. Limited benchmark coverage, lacking broader validation and analysis of potential trade-offs with language ability.\n2. No systematic comparison with other integration methods such as DeepStack, making relative advantages unclear.\n3. Insufficient theoretical explanation of cross-modal alignment, with little analysis of feature-space consistency or gradient flow.\n4. Depends on spatial-grid projection modules, limiting adaptability to Resampler or Q-Former structures.\n5. May reduce pruning efficiency when combined with visual token compression techniques."}, "questions": {"value": "1. Could the evaluation be expanded to more datasets, and could the trade-offs with language performance be analyzed?\n2. Could comparisons with DeepStack be added to further clarify the advantages of this method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zGTe0xbmXG", "forum": "kNLFJkLM2m", "replyto": "kNLFJkLM2m", "signatures": ["ICLR.cc/2026/Conference/Submission6553/Reviewer_By5h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6553/Reviewer_By5h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6553/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097977187, "cdate": 1762097977187, "tmdate": 1762918895860, "mdate": 1762918895860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}