{"id": "9jQkmGunGo", "number": 3534, "cdate": 1757467516911, "mdate": 1759898082531, "content": {"title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity", "abstract": "Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling (VS), a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., \"Generate 5 jokes about coffee and their corresponding probabilities\"), which relieves the pressure to produce a single \"typical\" answer. Experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), social dialogue simulation, synthetic data generation, and open-ended QA, without sacrificing safety and factual accuracy. For instance, in creative writing, VS increases diversity by 1.6-2.1x compared to direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.", "tldr": "We identify typicality bias in human preferences as a key cause of LLM diversity drop in RLHF; grounded in the theoretical insights, we propose Verbalized Sampling, a simple yet effective prompting method that improves LLM diversity across tasks.", "keywords": ["LLM Diversity", "Mode Collapse", "Creative Writing", "Dialogue Simulation", "Human Preference Bias"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e82cd7b97ea0a0d32b9c20aa16f0d16b1bc02585.pdf", "supplementary_material": "/attachment/d802c870605996f81a7955559ba5fde118198dc3.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a simple method, called verbalized sampling, to improve LLM diveristy. It's simple—get a model to generate responses _with probabilities over the answers_, and then pick from that according to some metric. They show that this improves diveristy at a given quality level. The submitted version has some issues that I'd like to see resolved, but overall I think this could be a nice paper, if the issues are resolved."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Nice to have clearly defined definitions in Table 1, which I found to be very clear. Nice.\n- I found Figure 2 to be well presented and clear, and I was happy to see the diversity and quality score here.\n- Method is very simple, which I think is great, and its great to study the probability threshold and scaling.\n- I think diversity with fixed quality is important, especially for synthetic data scaling. Indeed, I found the setting in section 7 compelling and useful."}, "weaknesses": {"value": "- the analysis in section 3.1, using a linear reward model is interesting but I believe it may have a flaw. The probability under the base reference model does _not_ just capture typically, but will capture many features, such as correctness, etc. Therefore, i'd like to see additional analysis for 3.1. in particular, what % of time in preference data is the more typical response, using BOTH a response with higher log-likelihood under pretrained model AND a simple prompted model preferred. You could use a simply logistic regression feature approach, similar to the Sharma et al Sycophancy paper. You could do this for example on some other PM datasets too, which would be helpful. \n- Moreover, you make claims about what _humans_ prefer, but in fact, _many_ humans are involved in the RLHF process, and it's unclear that they will all think the same text is typical. E.g., compare US and British english speakers. And the analysis is about the reward model scores, _not_ what humans actually prefer. Therefore, you should do the analysis I suggest above to isolate these questions. There are thus two questions: (i) what do humans prefer; (ii) what do PMs prefer. Different questions, worth studying individually, and you should not conflate the results of each type of analysis.\n- there are some extra baselines I'd like to see:\n    - I'd like to see the effect of the method compared to a pretrained model directly. How much of the diversity is actually recovered?\n    - i'd like to see an input seeding example. e.g., \"pull a random document from this set of 100\" and then do the task. This is environmental randomness.\n    - it is exactly right to look at diveristy, quality parteo curves. I suggest making this the main figure, and including it in the main text for Fig 2, **across all types** of task.\n    - I'd like to see an additional quality check, which uses e.g., LLM as a judge and computes win-rates against a reference set, rather than just using rubrics.\n- The writing is technically imprecise at times. small notes, as examples of this are below. In general, I'd like to see the strength of claims toned down and precision improved.\n    - intro line 65: please crispy define typicality.\n    - intro line 95: why is it principled? please explain crisply in text.\n    - line 206 \"to recover the diversity level\". does this actually rcover it?\n- I want to understand why the verbalization of probability distribution is helpful here? Do you have intuition around this? Can you explain this in the text?"}, "questions": {"value": "also, for Table 2, do you have a human assessment of quality for VS-standard vs e.g., direct?\n\n(see weaknesses for most questoins)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Es3hLCIdHT", "forum": "9jQkmGunGo", "replyto": "9jQkmGunGo", "signatures": ["ICLR.cc/2026/Conference/Submission3534/Reviewer_Kajm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3534/Reviewer_Kajm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761343194805, "cdate": 1761343194805, "tmdate": 1762916800761, "mdate": 1762916800761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argued that one important cause of the mode collapse is the biased preference data. The authors investigated this “typicality bias” that once quantified is shown to be detrimental to model output diversity. \nThe authors propose a new prompt based method to bypass mode collapse, specifically by prompting models to list different response candidates in JSON while also listing their corresponding probability"}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Overall, a solid and refreshing investigation into typicality bias, which identifies a key potentially undesired property in LLM preference data, and I believe this has great potential to unlock more LLM diversity.\n- For each configuration of verbalized sampling, the experiment is thorough with good coverage of models, domains, varying levels of interactiveness, metrics, and human annotations."}, "weaknesses": {"value": "- The motivation of the paper and the method feel mismatched — reading the paper feels like reading two. Typicality bias was thoroughly investigated and validated, but instead of continuing investigating any de-typicalized reward pairs, the authors used an unconvincing (see next point) reward-irrelevant method instead. The connection of typicality bias and verbalized sampling is at best, as the extensive appendix tried to prove under strong assumption, orthogonal. Line 203 to 207 (roughly) is not a coherent explanation either.\n- Unsatisfactory baselines. The method used in the paper claims that requesting the distribution in the prompt is the key to improve observed diversity. However, strictly comparable list-based counterparts to \"VS-*\" methods probability request is subtly missing. To break it down Table 1:\n\n| VS (k>1)          | List Level (k>1)  | Instance Level (k==1) |\n|-------------|-------------|----------------|\n| `VS-Standard`  | `Sequence`  | `Direct`       |\n| `VS-CoT`        | **null** | `CoT`          |\n| `VS-Multi` | **null** | `Multi-Turn`*  |\n\n*`Multi-Turn` is an unfaithful and unacceptable misnomer. Unlike `VS-Multi` it only samples one more candidate per turn / call. So it is de facto instance level.\n\nThe missing comparable counterparts, which could be named `Sequence-CoT` and `Sequence-Multi`, are essential in evaluating the proposed method’s efficacy. Prior works and this one have shown that list does increase diversity over instance-level."}, "questions": {"value": "- Line 178 claims \"Having confirmed typicality bias\". This claim lacks a description and content of the domains and types of the data. No description other than the dataset name is mentioned, yet the authors did not specify any conditions where the conclusion \"human raters are biased towards responses more typical for the base model\" are based on. So what are the domains of HelpSteer? How generalized can your claim be? Description about Eq1 lacks such context.\n- For `α > 0 means that, holding the true utility fixed, higher typicality bias increases the reward`, it seems that some styles might just be universally preferred by users that the base learned to prefer already, which might drive up `α`. What could be a baseline `α` if not 0?\n- For Creative writing evals, why `text-embedding-3-small` and `Claude 3.7`? Some explanation can be helpful.\n- Double-check line 243. Not very readable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XV4OuLE49O", "forum": "9jQkmGunGo", "replyto": "9jQkmGunGo", "signatures": ["ICLR.cc/2026/Conference/Submission3534/Reviewer_oDEZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3534/Reviewer_oDEZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828665728, "cdate": 1761828665728, "tmdate": 1762916799076, "mdate": 1762916799076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "* The paper identifies **typicality bias** in human preference data as a key cause of **mode collapse** in RLHF-aligned large language models.\n* It shows theoretically and empirically that this bias sharpens the model’s distribution, reducing output diversity.\n* The authors propose **Verbalized Sampling (VS)**, a simple, inference-time prompting method that restores diversity without retraining, achieving higher variety while maintaining quality."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel framing**\n   The paper introduces a fresh and compelling perspective by identifying *data-level human preference bias* (typicality bias) as a root cause of mode collapse. This shifts the discussion from algorithmic issues in RLHF to psychological factors in human annotations, offering a new concept for understanding mode collapse.\n\n2. **Strong empirical evidence**\n   The paper validates the typicality-bias hypothesis on real preference datasets such as HELPSTEER, showing consistent and statistically significant results across multiple base models. This empirical grounding gives credibility to the theoretical claims.\n\n3. **Practical mitigation method**\n   The proposed *Verbalized Sampling (VS)* is simple, training-free, and effective. It improves output diversity through prompt-level control at inference time, without modifying model weights. The method is orthogonal to existing decoding strategies and easy to adopt in practice.\n\n4. **Strong and comprehensive empirical validation**\nThe experiments are comprehensive and convincing. The paper tests its hypothesis across multiple datasets, diverse task categories, and several subtasks."}, "weaknesses": {"value": "1. **Limited validation of the core hypothesis**\n    - The central assumption—that *human annotators prefer more typical responses*—is not directly validated.\n    - The authors infer this pattern indirectly through reward model correlations. A direct analysis of whether humans explicitly favor more typical responses would make the hypothesis stronger.\n2. **Inference cost and deployment feasibility**\n    - While *Verbalized Sampling (VS)* effectively increases diversity, it requires generating *k* responses and verbalizing probabilities, leading to roughly *k×* higher inference cost. This makes the approach difficult to apply in large-scale LLM deployments despite its conceptual simplicity.\n3. **Lack of qualitative analysis**\n    - The paper would benefit from more qualitative examples to illustrate how VS improves diversity without hurting quality. Current evaluations are largely quantitative, leaving uncertainty about how these improvements appear in actual outputs.\n    - In particular, there should be a qualitative comparison between generating responses individually (one-by-one) and generating multiple responses simultaneously (e.g., 5 at once). Although the paper reports quantitative gains, it remains unclear whether the perceived quality or linguistic characteristics of the responses differ between these settings. Providing human evaluations or illustrative examples would clarify whether VS changes not just diversity scores but the actual quality of the outputs.\n4. **Missing ablation studies**\n    - Figure 14 could also analyze how varying the number of generated candidates in a direct setting appears, and variants like VS-CoT or VS-Multi should also be compared in Figure 14. This would clarify which components of VS contribute most to the observed gains.\n5. **Fairness issue in VS-Multi comparison**\n- The comparison involving VS-Multi appears potentially unfair, as it effectively produces a larger number of responses (e.g., *5×k* candidates) than other methods. This difference may inherently boost diversity and performance, so comparisons should control for the total number of generated outputs across settings."}, "questions": {"value": "- Q1 What is the expected inference cost for each of the proposed prompt settings?\n\n- Q2 Considering cost, diversity, and quality, which approach appears to be the most feasible overall?\n\n- Q3 How was the distribution of model-generated probabilities?\nDid the distribution differ across different VS variants, and is there any observed relationship between those probabilities and the quality of generated outputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WwguAim5iT", "forum": "9jQkmGunGo", "replyto": "9jQkmGunGo", "signatures": ["ICLR.cc/2026/Conference/Submission3534/Reviewer_fHBG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3534/Reviewer_fHBG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3534/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997909769, "cdate": 1761997909769, "tmdate": 1762916796293, "mdate": 1762916796293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}