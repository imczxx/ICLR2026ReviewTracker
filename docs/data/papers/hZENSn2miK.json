{"id": "hZENSn2miK", "number": 9675, "cdate": 1758133908193, "mdate": 1759897705035, "content": {"title": "Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval", "abstract": "Dual‚Äëencoder retrievers depend on the principle that relevant documents should score higher than irrelevant ones for a given query. Yet the dominant Noise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss, optimizes a softened ranking surrogate that we rigorously prove is fundamentally oblivious to score separation quality and unrelated to AUC. This mismatch leads to poor calibration and suboptimal performance in downstream tasks like retrieval‚Äëaugmented generation (RAG). To address this fundamental limitation, we introduce the MW loss, a new training objective that maximizes the Mann‚ÄëWhitney U statistic, which is mathematically equivalent to the Area under the ROC Curve (AUC). MW loss encourages each positive-negative pair to be correctly ranked by minimizing binary cross entropy over score differences.  We provide theoretical guarantees that MW loss directly upper-bounds the AoC, better aligning optimization with retrieval goals. We further promote ROC curves and AUC as natural threshold‚Äëfree diagnostics for evaluating retriever calibration and ranking quality. Empirically, retrievers trained with MW loss consistently outperform contrastive counterparts in AUC and standard retrieval metrics. Our experiments show that MW loss is an empirically superior alternative to Contrastive Loss, yielding better-calibrated and more discriminative retrievers for high-stakes applications like RAG.", "tldr": "", "keywords": ["Dense retrieval", "AUC", "Mann-Whitney", "RAG"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c5403088d85332aa0c0506c65e1dcec5a95f57b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the limitations of the Contrastive Loss (CL) for training dense neural retrievers and proposes a alternative called Mann-Whitney (MW) loss. The MW loss is a simple yet principled objective that directly maximizes the Area Under the ROC Curve (AUC) by minimizing binary cross-entropy over pairwise score differences. Theoretical guarantees and empirical evaluations demonstrate MW loss‚Äôs effectiveness in improving calibration and retrieval performance compared to CL."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Novelty:** The paper addresses a critical issue in dense retriever training and proposes a theoretically sound and empirically effective solution. The focus on global score calibration and AUC maximization is well-justified and has the potential to improve the reliability and applicability of neural retrievers in various domains.\n\n**Evaluation:** The authors conduct extensive experiments on various datasets and models, demonstrating the effectiveness of MW loss in both in-distribution and out-of-distribution scenarios. The results consistently show that MW loss outperforms CL in terms of AUC and standard retrieval metrics.\n\n**Clarity:** The paper is well-written and organized, with clear explanations of the problem, proposed method, and experimental results."}, "weaknesses": {"value": "**Generalization:** Exploring the performance of MW loss on various models would strengthen the paper‚Äôs claims and provide a more comprehensive understanding of its generalizability.\n\n**Efficiency:** While the paper mentions the computational differences between MW loss and CL, a more detailed analysis of the computational cost and its impact on training efficiency would be beneficial.\n\n**Other Methods:** While the paper focuses on comparing MW loss with CL, a comparison with other recent approaches for improving retrieval performance, such as margin-based losses or data augmentation techniques, would provide more of MW loss‚Äôs advantages or limitations."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EFiqLhxecL", "forum": "hZENSn2miK", "replyto": "hZENSn2miK", "signatures": ["ICLR.cc/2026/Conference/Submission9675/Reviewer_DCGo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9675/Reviewer_DCGo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760783212604, "cdate": 1760783212604, "tmdate": 1762921192455, "mdate": 1762921192455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose MW loss, which directly maximizes AUC by minimizing BCE over score differences between positives and negatives sampled from different queries. They prove MW upper-bounds AoC, show the computation reuses the same embedding/similarity passes as contrastive while doing more pairwise comparisons in the loss, and report consistent gains in AUC with competitive or better MRR/nDCG in-domain and on BEIR. Convergence is slower but argued to reflect the harder objective."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. clear theoretical critique\n\n2 embeddings/similarities are unchanged vs. contrastive; only the aggregation does B(B‚àí1) pairwise comparisons in the loss, which should vectorize well.\n\n3. results show broad AUC gains and frequent MRR/nDCG improvements across MiniLM/XLM-R-Base/Large, with plots of per-dataset gains."}, "weaknesses": {"value": "1. The baseline design largely collapses to CL vs. MW; this isolates the loss but under-probes stronger retrievers or alternative AUC-aware objectives, leaving open whether MW is the best practical route to calibration and top-k retrieval. \n\n2. In-domain improvements on MRR/nDCG for small/base models are modest on average even as AUC rises, so the paper should clarify when AUC gains meaningfully translate to ranking quality. \n\n3. The compute section argues ‚Äúsame embedding/similarity cost,‚Äù but the extra B(B‚àí1) BCE terms can still be a real bottleneck; wall-clock, peak memory, and throughput should be reported for typical B, H, corpus sizes. Convergence is slower; this is acknowledged but not quantified against a fixed training budget. \n\n4. Sensitivity to 100/1k, as well as to full-corpus vs. ANN sampling, would strengthen external validity."}, "questions": {"value": "1. Eq. (2) and Lemma 2: clarify the role of temperature œÑ in ‚Ñì_BCE vs. the œÑ used in contrastive; report sensitivity curves of AUC/MRR to œÑ and whether œÑ‚Üí‚àû reduces to a hinge-like objective. \n\n2. the AUC protocol picks top-500 corpus negatives; add a sensitivity analysis for 100/500/1000 and a variant using random-plus-hard mixes to show robustness.\n\n3. How stable is the global threshold learned by MW across domains‚Äîcan a single score cut transfer from NLI-trained models to BEIR subsets without per-task tuning?\n\n4. Do AUC gains predict top-k gains? On which datasets do we see high ŒîAUC but flat ŒîMRR/nDCG, and why? Any evidence of MW optimizing separation where intra-query ordering is less affected? \n\n\n5. Any failure modes with multi-relevant queries or table-heavy evidence where global calibration helps less than within-query ranking? Please include targeted analyses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uaqbCC1rWl", "forum": "hZENSn2miK", "replyto": "hZENSn2miK", "signatures": ["ICLR.cc/2026/Conference/Submission9675/Reviewer_Cvfk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9675/Reviewer_Cvfk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761204898607, "cdate": 1761204898607, "tmdate": 1762921192120, "mdate": 1762921192120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "RAG pipelines typically use retrievers based on dual-encoder models. These retrievers are often trained using contrastive learning (e.g., with an InfoNCE loss). However, this type of training suffers from the fact that the resulting relevance scores are not directly comparable across queries ‚Äî a phenomenon long known in the Information Retrieval community ‚Äî which makes a simple, globally applied thresholding strategy unlikely to succeed.\n\nTo address this, the authors propose the MW loss, based on the Mann‚ÄìWhitney U statistic (specifically, a differentiable proxy of this statistic). This loss serves as an upper bound on the AoC (Area over the Curve), thereby directly aligning training with a globally calibrated retrieval objective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* The paper is well written, well structured, easy to read and understand.\n* The underlying claims are sound, at least within the (limited) context the authors are considering.\n* It indeed offers a new loss that ensures consistency of relevance across queries; this implies that complex query-dependent thresholding strategies are no longer needed."}, "weaknesses": {"value": "(1) The main weakness of the paper is that it does not truly optimize what matters (contrary to what its title suggests).\n\nIn Information Retrieval (IR) and, in particular, in Retrieval-Augmented Generation (RAG), what ultimately matters is ranking quality, rather than classification accuracy with respect to an artificially defined binary relevance label. In this context, the AUC metric is rarely used to evaluate the quality of a ranking method, for several well-known reasons:\n\n- It does not depend on the actual positions of items in the ranked list, effectively treating the problem as classification rather than ranking.\n\n- It assumes binary relevance, whereas in practice relevance is often graded or continuous.\n\n- It is pairwise rather than listwise, and it is well established that listwise approaches tend to be more effective.\n\n(2) The choice of baselines for comparison is also quite weak: only a single baseline is considered. Since this baseline is not trained using the same loss function, the comparison‚Äîat least for the AUC criterion‚Äîappears either unfair or trivial.\n\nSee the next section (Questions) for suggestions of additional baselines and ablation studies."}, "questions": {"value": "* How do you position your method with respect to the RankNet objective function, which is fairly standard in the (older) Information Retrieval (IR) literature and can be seen as a differentiable proxy of the AUC, albeit computed at the level of a single query?\n* Score normalization and calibration methods, typically applied as post-processing steps, are widely used in the IR community to make relevance scores more consistent and comparable across queries. How does your approach relate to, or compare with, these existing techniques?\n* Given that the choice of baselines is somewhat limited (only a single method is considered, and arguably not the most representative of current practice in the RAG community), it would have been valuable to include additional ablations or variants, such asvthe use of other differentiable surrogates of the indicator function 1{ùëß‚â§0} (e.g., hinge loss, etc.);\n* the influence of the temperature parameter;\n* a deeper analysis of the somewhat counterintuitive observation that the proposed method preserves or even improves within-query (fine-grained) ranking quality. This last point is not obvious, since AoC is not inherently a ranking loss but rather a classification-oriented objective. Ideally, it would have been interesting to explore multi-valued (non-binary) relevance scenarios to validate this claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rn4m4KPTL0", "forum": "hZENSn2miK", "replyto": "hZENSn2miK", "signatures": ["ICLR.cc/2026/Conference/Submission9675/Reviewer_8QJW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9675/Reviewer_8QJW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761479569405, "cdate": 1761479569405, "tmdate": 1762921191832, "mdate": 1762921191832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "It introduces a Mann‚ÄìWhitney (MW) loss and proves an upper bound on the Area-over-ROC, so minimizing MW encourages higher AUC"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It introduces a MW loss, showing that minimizing MW tightens the AoC bound and encourages higher AUC."}, "weaknesses": {"value": "While the paper positions MW loss as a principled AUC-aligned alternative to Contrastive Loss, it does not compare against other well-known pairwise and listwise ranking objectives (e.g., margin ranking loss, triplet loss, RankNet/LambdaRank...).\n\nSome of these losses optimize s(q, p‚Å∫) ‚àí s(q, p‚Åª) directly, which is structurally very close to the proposed MW loss. Without such comparisons, it is difficult to assess whether MW loss provides a fundamentally new advantage."}, "questions": {"value": "While AUC is theoretically meaningful for score calibration, retrieval benchmarks are primarily evaluated using top-k metrics (nDCG, MRR, Recall@k). The gains in these metrics are sometimes small or even mixed. It remains unclear whether optimizing AUC is the right objective for retrieval effectiveness in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pLWHhV3WYM", "forum": "hZENSn2miK", "replyto": "hZENSn2miK", "signatures": ["ICLR.cc/2026/Conference/Submission9675/Reviewer_NUjr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9675/Reviewer_NUjr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976525664, "cdate": 1761976525664, "tmdate": 1762921191508, "mdate": 1762921191508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}