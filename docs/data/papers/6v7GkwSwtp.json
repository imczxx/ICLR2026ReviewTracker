{"id": "6v7GkwSwtp", "number": 9107, "cdate": 1758111476256, "mdate": 1759897742953, "content": {"title": "CellForge: Agentic Design of Virtual Cell Models", "abstract": "Virtual cell modeling represents an emerging frontier at the intersection of artificial intelligence and biology, aiming to predict quantities such as responses to diverse perturbations quantitatively. However, autonomously building computational models for virtual cells is challenging due to the complexity of biological systems, the heterogeneity of data modalities, and the need for domain-specific expertise across multiple disciplines. Here, we introduce CELLFORGE, an agentic system that leverages a multi-agent framework that transforms presented biological datasets and research objectives directly into optimized computational models for virtual cells. More specifically, given only raw single-cell multi-omics data and task descriptions as input (e.g., for control and perturbed conditions and a directive to build a model of a new perturbation), CELLFORGE outputs both an optimized model architecture and executable code for training virtual cell models and inference. The framework integrates three core modules: Task Analysis for presented dataset characterization and relevant literature retrieval, Method Design, where specialized agents collaboratively develop optimized modeling strategies, and Experiment Execution for automated generation of code. The agents in the Design module are separated into experts with differing perspectives and a central moderator, and have to collaboratively exchange solutions until they achieve a reasonable consensus. We demonstrate CELLFORGE’s capabilities in single-cell perturbation prediction, using six diverse datasets that encompass gene knockouts, drug treatments, and cytokine stimulations across multiple modalities. CELLFORGE consistently outperforms task-specific state-of-the-art methods, achieving up to 40% reduction in prediction error and 20% improvement in correlation metrics. Overall, CELLFORGE demonstrates how iterative interaction between LLM agents with differing perspectives provides better solutions than directly addressing a modeling challenge", "tldr": "", "keywords": ["LLMs"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d238cc985be00363696c1278314d8b5a3b8096ae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces CellForge, an agentic framework for developing dataset-specific perturbation prediction models. The system employs a collaborative multi-agent design, in which multiple instances of a large language model (LLM) assume distinct expert roles to create a code of a perturbation prediction model. The proposed pipeline comprises three stages: an analysis report phase that summarizes the data and performs literature searches focused on perturbation prediction methods; a research-planning stage, where agents collaborate to design a new model architecture; and a code-generation step, during which the model is implemented and automatically optimized. The authors claim that this customized approach to model development benefits the perturbation prediction task and that these artificially generated architectures are able to contend with the state-of-the-art general models.\n\nThe paper includes a comprehensive benchmark that evaluates the resulting architectures. The evaluation includes analysis of how well they predict gene expression, how relevant are the predictions from biological perspective (amount of the recapitulated differentially expressed genes) and, finally, quality of the models research plans. Additionally, the paper is equipped with ablation studies of different components of the pipeline. Overall, the architectures do not consistently achieve the highest scores in the benchmark but the reported numbers show that they at least can be on par."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly written, well-structured, and easy to follow, with an extensive benchmark."}, "weaknesses": {"value": "In general, the premise that different perturbation datasets require distinct architectures is contentious. Most perturbational single-cell datasets are generated using comparable experimental platforms, and recent models for perturbation prediction — such as **State** — do not report substantial performance gaps across datasets. Moreover, I have investigated one of the reasoning traces in the model repository (`example_report.json` and `example_analysis.json`, as well as examples in the appendix), and I was not convinced that the discussion between the expert LLMs results in recommendations that are better than general preprocessing strategies (e.g., filtering of mitochondrial and lowly expressed genes). Many of the architecture suggestions do not make sense at all (round 2: *“Mixed precision training, VAE latent space 64, rotary position embeddings”*; or round 3: *“Multimodal fusion combining scGPT, Geneformer, GEARS, uncertainty quantification”*) — the LLM simply tries to combine everything. Thus, the notion that each dataset necessitates a custom-designed architecture appears insufficiently motivated, and the authors should have provided a stronger conceptual or empirical justification for this claim and, based on that, designed a framework that does more than just superficial analysis of a dataset.\n\nNevertheless, my main point of criticism concerns not the premise of dataset-specific architecture design, but rather the **quality of the generated code** and the **evaluation procedure** that follows. To assess code quality, I examined the repository example `model1.py` and found several critical errors in both the architecture and the way it is evaluated. Most importantly, the `HybridAttentionModel` **does not implement a perturbation-prediction objective and is not evaluated on it, which in principle disqualifies the reported results.** The training and evaluation loops below make this clear:\n\n```python\n# training function\ndef train_model(model, train_loader, optimizer, scheduler, device, aux_weight=0.1):\n    model.train()\n    for i, batch in enumerate(train_loader):\n        x, pert = batch\n        x, pert = x.to(device), pert.to(device)\n        output, pert_pred, vae_recon, vae_kl = model(x, pert, is_train=True)\n        mse_loss = F.mse_loss(output, x)            # <- this is essentially an identity loss on x\n        pert_loss = F.binary_cross_entropy_with_logits(pert_pred, pert)  # <- this is circular classification loss\n        ...\n\ndef evaluate_and_save_model(model, test_loader, device, save_path='method3_hybrid_best.pt'):\n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, pert = batch\n            output, _, _, _ = model(x, pert, False) # predicts x from x\n            all_predictions.append(output.cpu().numpy())\n            all_targets.append(x.cpu().numpy())     # compares to the same x\n```\n\nEssentially, instead of perturbation prediction, the task is replaced by identity learning. Thus, it is not surprising that the model achieves high scores, since it learns the identity mapping of the data and, when evaluated on test perturbational data, it takes as input the same data on which it is tested. The fact that the evaluation of the model relies on the faulty code generated by an LLM is a serious error that invalidates the results completely.\n\nAnd this is not the only issue with the code. These are a few of many problems just within model 1:\n\t1.\tThe neural module test_perturbation_head is defined but never trained. Its role is also unclear, as it predicts the perturbation type using information that already includes the same label, rendering the head circular and meaningless.\n\t2.\tThe model defines self.transformer, which is applied to data with a sequence dimensionality of 1, making the use of attention mechanisms nonsensical.\n\t3.\tThe dataloader performs an obligatory PCA transformation, meaning the model does not even operate in expression space.\n\nSimilar evaluation issues can be found in other models 2–5, which renders the reported performance completely invalid and highlights the need for a complete revision of both model design and evaluation protocol."}, "questions": {"value": "I believe that the paper would require a **major revision** that should include the following aspects:\n\n1. **Motivation:** Clearly motivate why each dataset requires a customized architecture and showcase how this is taken into account by your framework.  \n2. **Framework correction:** Fix the framework so that the models optimize the correct objective and provide code for the new models. Ensure that the provided models are not handpicked and that their code is not curated by humans, unless the premise of the paper explicitly changes to describe a framework for *agent–human collaboration*.  \n   > *(To fix the models, I would advise focusing on the dataloader, since the LLMs consistently miss the point that the input to the model should be unperturbed data and the output should be perturbed.)*  \n3. **Evaluation strategy:** Fix the evaluation strategy. The code for it should be written by a human and it should be the same evaluation script applied across all methods.  \n4. **Fair optimization:** Since the implemented framework performs automatic hyperparameter optimization using an algorithm that is not part of the paper, it is reasonable to suspect that the observed differences between synthetic models and benchmarking methods are due to the benchmarking methods not being optimized. Apply the same optimization strategy to all compared methods.  \n5. **Extended benchmarking:** Extend the set of models used in benchmarking to include state-of-the-art models such as **STATE**, as well as simpler **PCA-based** and **VAE-based** baselines, since these often outperform more complex architectures (*“Benchmarking Transcriptomics Foundation Models for Perturbation Analysis: one PCA still rules them all,” 2024*).  \n\n---\n\n### Typos and minor issues\n\n1. “agent collaboration mechanismsgraph-structured discussions”  \n2. “re-executes itrepeating”  \n3. “while CELLFORGE ranges from near state-of-the-art to substantially weaker”  \n4. “integrates genegene co-regulatory”  \n5. “L.2 BIOLOGICAL INTERPRETATION OF ARCHITECTURAL CHOICES” — the content of this section appears to be missing.  \n6. “Figure 17” — describe how UMAPs are computed. Are they computed on all data (predicted + original) or only on original data? It may be that you project perturbed data onto a subspace."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QlbRX9iOCn", "forum": "6v7GkwSwtp", "replyto": "6v7GkwSwtp", "signatures": ["ICLR.cc/2026/Conference/Submission9107/Reviewer_5ETr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9107/Reviewer_5ETr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761219372814, "cdate": 1761219372814, "tmdate": 1762920804917, "mdate": 1762920804917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces cellforge, an autonomous AI framework that takes a biological dataset and a task and, with no human intervention, produce a fully-trained, high-quality neural network model to solve that task."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The idea is original and the findings are significant. The problem formulation is clever."}, "weaknesses": {"value": "The paper has a significant number of typos, including in the abstract (e.g., componentssuch).\n\nAs the authors acknowledge, performance and outcome significantly varies across runs. While the framework *can* discover models that match or exceed hand-designed ones, there are no guarantees. \n\nThe multi-agent framework is computationally expensive and significantly slower than other approaches."}, "questions": {"value": "Regarding the performance variability, have you analyzed what is the primary source of this instability?\n\nWhy is the agentic design and debugging process so inefficient that it fails 80% of the times, and takes as long as model training? Especially since a significant number of failures are computation execution errors like tensor dimension mismatches, did you try providing a strict specification of all tensor shapes to mitigate simple bugs that prevent execution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FFOzlYTUWp", "forum": "6v7GkwSwtp", "replyto": "6v7GkwSwtp", "signatures": ["ICLR.cc/2026/Conference/Submission9107/Reviewer_qT8S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9107/Reviewer_qT8S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976155529, "cdate": 1761976155529, "tmdate": 1762920804377, "mdate": 1762920804377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an autonomous multi-agent system, CellForge, that designs and implements model architectures for single-cell perturbation prediction without human intervention. Rather than manual human design or single-LLM prompting, they take multi-agent collaboration mechanisms for autonomously dicoverying candidate neural network architectures tailored to specific single-cell datasets and perturbation tasks. They provide a detailed and comprehensive description of the overall multi-agent system architecture as well as the information exchange and fusion among multi-agents, clearly presenting the entire task analysis pipeline. Lastly, there are empirical experiments done to conclude the superior performance of models produced by CellForge incontrast to which by manual or template-based design. Their main contribution is proposing the effient multi-agent system in computational biology, representing a paradigm shift toward autonomous scientific method development."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper presents an original multi-agent system for single-cell perturbation prediction in computational biology, overcoming the prior reliance on human intervention and demonstrating highly competitive performance. Moreover, the paper is of high quality, providing a comprehensive and clear exposition of technical details, advancing beyond isolated task execution to enable end-to-end autonomous research workflows, and thereby demonstrating notable significance."}, "weaknesses": {"value": "This paper lacks sufficient concrete comparative analysis with methods from other works, for exampl, works such as CellAgent, C2S-Scale appear to achieved similar automated single-cell data  analysis, the paper should thoroughly discuss the differences from these approaches to better highlight its own contributions and value."}, "questions": {"value": "1. Does this multi-agent system depend on specific, highly capable LLMs? If the large models are replaced with smaller, less capable models (like Qwen2.5-3B) within the same framework, will the performance significantly decline?\n2. The paper mentions significant performance variations across different runs, particularly in drug perturbation tasks. Could this variation impact practical applications, and are there methods to control or reduce this randomness?\n3. While the paper focuses on single-cell perturbation prediction, it does not test other tasks such as protein structure prediction or drug discovery. Can it be generalized to other computational biology tasks?\n\nSuggestions:\n\n1. Conduct systematic ablation studies on LLM dependencies with more LLMs\n2. Implement and evaluate stability-enhancing techniques\n3. Explore generalization through pilot studies on related tasks"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FQNde5jIdT", "forum": "6v7GkwSwtp", "replyto": "6v7GkwSwtp", "signatures": ["ICLR.cc/2026/Conference/Submission9107/Reviewer_qrHz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9107/Reviewer_qrHz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981010798, "cdate": 1761981010798, "tmdate": 1762920803955, "mdate": 1762920803955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}