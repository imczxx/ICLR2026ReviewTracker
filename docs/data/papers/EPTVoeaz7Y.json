{"id": "EPTVoeaz7Y", "number": 19837, "cdate": 1758299856614, "mdate": 1759897016813, "content": {"title": "RANGER: Repository-Level Agent for Graph Enhanced Retrieval", "abstract": "General-purpose automated software engineering (ASE) includes tasks such as code completion, retrieval, repair, QA, and summarization. These tasks require a code retrieval system that can handle specific queries about code entities, or code entity queries (for example, locating a specific class or retrieving the dependencies of a function), as well as general queries without explicit code entities, or natural language queries (for example, describing a task and retrieving the corresponding code). We present RANGER, a repository-level code retrieval agent designed to address both query types, filling a gap in recent works that have focused primarily on code-entity queries. We first present a tool that constructs a comprehensive knowledge graph of the entire repository, capturing hierarchical and cross-file dependencies down to the variable level, and augments graph nodes with textual descriptions and embeddings to bridge the gap between code and natural language. RANGER then operates on this graph through a dual-stage retrieval pipeline. Entity-based queries are answered through fast Cypher lookups, while natural language queries are handled by MCTS-guided graph exploration. We evaluate RANGER across four diverse benchmarks that represent core ASE tasks, including code search, question answering, cross-file dependency retrieval, and repository-level code completion. On CodeSearchNet and RepoQA, it outperforms retrieval baselines that use embeddings from strong models such as Qwen3-8B. On RepoBench, it achieves superior cross-file dependency retrieval over baselines, and on CrossCodeEval, pairing RANGER with BM25 delivers the highest exact match rate in code completion compared to other RAG methods.", "tldr": "", "keywords": ["GraphRAG", "Monte Carlo Tree Search", "Repository-level", "Retrieval Agent", "Code Retrieval", "Retrieval-Augmented Generation", "Software Engineering", "Graph Traversal", "Multi-hop Reasoning", "Code Search"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f553aa67f10876b8f2c0831073e79594cc0f703e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents RANGER, a repository-level code retrieval agent capable of handling both symbolic (code-entity) and natural language queries within a unified framework.\nRANGER first builds a fine-grained repository knowledge graph via AST parsing and cross-file dependency stitching, covering modules, classes, functions, and variables. Each graph node is further enriched with LLM-generated semantic descriptions and embeddings.\nAt retrieval time, the system employs a dual-stage pipeline:\nCode-entity queries are processed via Cypher lookups in a Neo4j-style graph database.\nNatural language queries trigger an MCTS-based graph exploration, guided by a bi-encoder for candidate expansion and a cross-encoder for reward-based scoring.\nRANGER is evaluated across four diverse benchmarks — CodeSearchNet, RepoQA, RepoBench, and CrossCodeEval — and consistently outperforms strong baselines such as Qwen3-8B, BM25, RepoFuse, and CGM-MULTI in NDCG, Recall, and Exact Match metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Novel and unified design combining graph database querying with MCTS-based search.\nStrong empirical results across four benchmarks and multiple task types.\nDetailed methodology and reproducible description of implementation steps.\nBalances accuracy and computational efficiency via selective cross-encoder scoring.\nBroad potential impact on retrieval-augmented code reasoning and agentic systems."}, "weaknesses": {"value": "The performance gains, while consistent, are not large enough to be clearly transformative.\nThe static repository graph limits adaptability to evolving codebases.\nMCTS introduces nontrivial latency; no discussion on efficiency trade-offs.\nThe reward function (cross-encoder scores) lacks learning capability, potentially limiting generalization.\nExperiments are restricted to Python; cross-language tests are missing."}, "questions": {"value": "How sensitive is RANGER’s performance to MCTS parameters (c, k, α)?\nCould dynamic or incremental graph updates improve scalability?\nWhat is the trade-off between retrieval accuracy and runtime cost?\nHave you considered using a learned reward model instead of a fixed cross-encoder?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jks4oy0nLp", "forum": "EPTVoeaz7Y", "replyto": "EPTVoeaz7Y", "signatures": ["ICLR.cc/2026/Conference/Submission19837/Reviewer_ib2j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19837/Reviewer_ib2j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761446285704, "cdate": 1761446285704, "tmdate": 1762932010851, "mdate": 1762932010851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RANGER, a repository-level code retrieval agent that supports both code-entity queries and natural language queries. It first constructs a comprehensive knowledge graph (KG) for the entire repository, enriched with textual descriptions and embeddings to bridge code and natural language. Retrieval with failed Cypher lookups are handled through MCTS-guided graph exploration. RANGER is evaluated across multiple benchmarks, showing strong performance in code search, QA, retrieval, and completion tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is clearly written and well-structured, making the technical approach accessible and easy to follow.\n\n2. The unified treatment of both code-entity and natural language queries within a single retrieval framework is commendable."}, "weaknesses": {"value": "1. Limited and overstated contributions: (i) The knowledge graph construction closely resembles existing GraphRAG approaches. The use of a graph database is an engineering choice rather than a conceptual contribution. (ii) The only apparent innovation lies in the reward mechanism within the MCTS-guided exploration. (iii) the claimed contribution of a \"router\" to dispatch query types is misleading: the routing logic is a unified and simple fallback from Cypher lookup failure to MCTS.\n\n2. Missing or unclear implementation details: The KG in Figure 1 includes an edge type labeled \"USES\", which is not mentioned or defined anywhere in the main body. If this represents variable usage, function calls, or data dependencies, its construction method must be explicitly described.\n\n3. Potentially misleading experimental results: While RANGER uses smaller models and achieves competitive results, the latency of the MCTS process is concerning. Table 1 indicates that MCTS iterations take approximately 15 and 40 seconds, respectively. It is impractical for real-time developer tools and should be explicitly acknowledged and discussed. Additionally, Table 3 omits standalone results for RANGER itself and key baselines such as RepoCoder [1] and DraCo [2]. Moreover, the high Exact Match (EM) but low Edit Similarity (ES) in CrossCodeEval raises concerns: ES is a more user-centric metric, reflecting how much post-completion editing is needed. A high EM with low ES may indicate brittle, exact-string matches that are not practically useful, and this discrepancy warrants deeper analysis.\n\n4. Citation errors: The reference list contains several inaccuracies. For instance, references [2], [3], and [4] have mismatched author lists. While I did not conduct a full audit, this suggests broader issues with citation integrity that could affect the paper’s credibility. \n\n[1] RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation\n\n[2] Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion\n\n[3] REPOFUSE: Repository-Level Code Completion with Fused Dual Context\n\n[4] GraphCoder: Enhancing Repository-Level Code Completion via Code Context Graph-based Retrieval and Language Model"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yKRLyR0HRn", "forum": "EPTVoeaz7Y", "replyto": "EPTVoeaz7Y", "signatures": ["ICLR.cc/2026/Conference/Submission19837/Reviewer_pKpS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19837/Reviewer_pKpS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618190334, "cdate": 1761618190334, "tmdate": 1762932010481, "mdate": 1762932010481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper “RANGER: Repository-Level Agent for Graph-Enhanced Retrieval” presents a retrieval agent that unifies symbolic and semantic search for large code repositories. It builds a knowledge graph from Abstract Syntax Trees using the tree-sitter parser, linking modules, classes, and functions through hierarchical and cross-file dependencies. Each node is enriched with LLM-generated descriptions and embeddings, stored in a Neo4j graph database. For queries, RANGER first performs fast Cypher lookups for code-entity queries; if no direct match is found, it switches to a Monte Carlo Tree Search (MCTS) that combines bi-encoder exploration and cross-encoder scoring to locate relevant code. This dual-stage design allows RANGER to handle both structured and natural language queries effectively.\n\nAcross benchmarks including CodeSearchNet, RepoQA, RepoBench, and CrossCodeEval, RANGER consistently outperforms strong baselines such as Qwen3-8B embeddings and repository-level retrievers. It achieves higher retrieval precision for natural language queries and better cross-file dependency resolution, while also improving code completion accuracy when combined with BM25. These gains highlight the value of structured graph reasoning over flat vector search.\n\nRANGER’s main strengths are its unified handling of different query types, its efficient MCTS-guided search balancing accuracy and cost, and its persistent, semantically rich graph design. However, it still relies on static, offline graphs and introduces extra computation in MCTS, which may limit use in fast-changing repositories or interactive tools. Despite these trade-offs, RANGER marks a strong step toward scalable, agentic retrieval systems for repository-level code understanding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "A key strength of RANGER is that it handles both code-entity and natural language queries within one unified framework. It builds a persistent knowledge graph of the entire repository, capturing module, class, and function relationships while enriching them with LLM-based text descriptions and embeddings. This design allows the system to quickly resolve structured code lookups using Cypher queries while supporting more abstract natural language retrieval through guided graph exploration. Another strength is its Monte Carlo Tree Search (MCTS) mechanism, which balances accuracy and efficiency by using a bi-encoder for exploration and a cross-encoder for fine-grained scoring. This selective scoring method avoids the high cost of full reranking while maintaining near-equivalent quality, helping RANGER find relevant cross-file dependencies that standard embedding searches often miss. Finally, RANGER shows strong empirical results across multiple benchmarks, consistently outperforming established baselines such as Qwen3-8B and RepoFuse on retrieval and completion tasks. Its persistent Neo4j-based graph design also makes it adaptable to new languages and use cases, showing both practicality and consistency across diverse repository-level tasks"}, "weaknesses": {"value": "RANGER’s main weaknesses stem from both design choices and scope limitations that echo long-standing issues in graph-based code retrieval research. The most fundamental problem is that it inherits the narrow focus of many prior systems by restricting implementation and evaluation to Python, a language already overrepresented in past graph-based retrieval work. This makes the contribution feel incremental, as numerous earlier methods—such as RepoHyper, RepoGraph, GraphCoder, and DraCo—also relied on Python ASTs and similar dependency modeling. Without demonstrating generalization to statically typed or structurally complex languages like Java, C++, or Go, the system’s real-world relevance remains limited. \n\nBeyond that, RANGER’s static offline graph construction further reduces practicality; it cannot adapt to fast-changing repositories or continuous integration environments where dependencies evolve quickly. The MCTS mechanism, while novel, adds computational overhead and latency that are unsuitable for interactive settings, effectively turning retrieval into a heavy research prototype rather than a production-ready solution. Moreover, the system’s reliance on cross-encoder scoring as the sole reward signal lacks robustness and may overfit to benchmark data instead of scaling to diverse, large repositories."}, "questions": {"value": "1) Since the graph is constructed offline, how does RANGER handle repositories with frequent updates—would incremental graph updates or online synchronization be feasible without full recomputation?\n2) The MCTS mechanism adds an additional reasoning layer; have the authors analyzed the latency or cost trade-offs for interactive scenarios like IDE integration, and how might they optimize it for real-time retrieval?\n3) The system relies heavily on cross-encoder scoring as the reward function. Would using a learned or adaptive reward model (e.g., reinforcement learning or small LLM fine-tuned for relevance) yield more stable and scalable results?\n4) Given that the approach enriches nodes with LLM-generated descriptions, how sensitive is RANGER’s retrieval accuracy to the quality or bias of these generated descriptions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kqX8sLsD0g", "forum": "EPTVoeaz7Y", "replyto": "EPTVoeaz7Y", "signatures": ["ICLR.cc/2026/Conference/Submission19837/Reviewer_kgfN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19837/Reviewer_kgfN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145603882, "cdate": 1762145603882, "tmdate": 1762932009694, "mdate": 1762932009694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RANGER, a repository-level retrieval agent for code understanding and retrieval tasks. RANGER builds a comprehensive repository-wide knowledge graph that captures hierarchical and cross-file dependencies down to the variable level and augments each node with natural language descriptions and embeddings. The system integrates a dual-stage retrieval pipeline:\nCypher-based lookups for direct code-entity queries, and MCTS-guided graph exploration for natural language queries with bi-encoder expansion and cross-encoder scoring. Evaluated across CodeSearchNet, RepoQA, RepoBench, and CrossCodeEval, RANGER demonstrates superior retrieval and code completion performance compared to embedding-based and repository-level baselines, including Qwen3-8B, RepoFuse, and R2C2."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- address an important problem filling the gap between symbolic code-entity and natural language retrieval at the repository scale.\n\n- novel unified framework integrating graph construction, semantic augmentation with  MCTS-based graph traversal \n\n- performance gains across four benchmarks and multiple metrics (NDCG, Recall, EM, MRR).\n\n- Comprehensive experiments and analyses \n- Elaborated related works"}, "weaknesses": {"value": "- The main limitation of this work, I think is evaluation. Several key points: RAG level evaluation needs to addressed properly.  1) Repo level retrieval is a key part in SWE tasks where SWE-bench is a very popular codeRAG tasks. 2) Competition level code generation such as on LiveCodeBench is another popular benchmark. I suggest experiments on these two datasets. For SWE any agent AgentLess, Openhands, MiniSWEAgent can be fit with this retriever and other baseline retriever agnet. For LCB, Mapcoder, Self-Reflection, Xolver or any other simple or complex agents can also be used.  \n- missing baselines: SWE-RANK: https://arxiv.org/abs/2505.07849 could be another baseline as well. When RANGER is combined with BM25 other baselines can also be combined with that. \n- Comparison in CodeSearchNet or RepoQA also do not consider strong baselines, rather non retriever language model. Even simple BM25 is not considered. \n-MCTS is infact an expensive search method. How do you compare the time complexity? There is no experiments. Fig 3 combining Avg MCTS time with retrieval metics doe not make any sense. \n- in Section 4.2 what is the baseleine?\n- The finding are not robust, sometimes needs BM25 based hybrid sometimes RANGER only sometime others, no clear conclusion in findings. \n- Writing needs improvement and more abalation and analyses."}, "questions": {"value": "See wekanesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2NhWe9GwGm", "forum": "EPTVoeaz7Y", "replyto": "EPTVoeaz7Y", "signatures": ["ICLR.cc/2026/Conference/Submission19837/Reviewer_eG5f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19837/Reviewer_eG5f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19837/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762206082141, "cdate": 1762206082141, "tmdate": 1762932009097, "mdate": 1762932009097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}