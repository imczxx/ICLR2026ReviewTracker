{"id": "Z1TMV4bGuu", "number": 22122, "cdate": 1758326400422, "mdate": 1763687625448, "content": {"title": "Rethinking Multimodal Time-Series Forecasting Evaluation", "abstract": "We introduce a new context-enriched, multimodal time series forecasting benchmark TimesX. TimesX contains a wide selection of high-quality real-world time series with diverse domains and textual contexts obtained from an automated data generation pipeline, which helps address three main issues of existing multimodal forecasting benchmarks: (1) poor generalization due to the small scale and synthetic nature of benchmark data, (2) very limited types of textual contexts in the benchmarks, and (3) an inability to mitigate data leakage in evaluation. We conduct a thorough empirical study of zero-shot multimodal forecasting approaches on TimesX. Our results suggest that many approaches that perform well on existing benchmarks may fail on TimesX. In contrast, simple ensemble methods that leverage rich textual context accompanying time-series can outperform strong baselines on the TimesX benchmark.", "tldr": "", "keywords": ["Large Language Models (LLMs)", "Time Series Analysis", "Multimodal Learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/799ff9e44e56aec4e8b49a39eb77979d07599c6a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TimesX, a novel, context-enriched, multimodal time-series forecasting benchmark designed to address key shortcomings in existing evaluation datasets, specifically poor generalization due to synthetic and small-scale data, limited types of textual contexts, and the inability to mitigate data leakage. TimesX is the first real-world, large-scale benchmark, featuring 19 diverse domains and 190 variables, each linked to comprehensive and verifiable textual contexts including Metadata, Calendar features, Covariates, and Time-Stamped Events. A key contribution is its automated, leakage-free data generation pipeline, which employs a hypothesizer-verifier-enricher framework and strict time isolation to ensure context quality and long-term validity against continuously updated foundation models. The authors demonstrate that existing zero-shot approaches struggle with TimesX, and simple ensemble methods leveraging the rich contextual data can outperform strong baselines on this more realistic platform."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unprecedented Scale and Realism: TimesX is a first-of-its-kind, large-scale, cross-domain benchmark built entirely from real-world time series and textual data across 19 domains and 190 variables, directly addressing the poor generalization and synthetic bias prevalent in prior work.\n\n2. Robust Data Leakage Mitigation: The benchmark incorporates a novel, automated data generation pipeline with strict time isolation and an updating mechanism, ensuring tasks occur after a model’s knowledge cutoff date and guaranteeing the benchmark's leakage-free and long-lived validity against future foundation models.\n\n3. High-Quality and Comprehensive Textual Context: TimesX provides a rich, fine-grained collection of four distinct context types (Metadata, Calendar, Covariates, and Time-Stamped Events) and uses a sophisticated Hypothesizer-Verifier-Enricher framework to ensure their quality and verifiability. This high-quality context is empirically shown to be critical for model performance, reducing error compared to lower-quality contexts."}, "weaknesses": {"value": "1. Limited Focus on Novel Model Architectures: The paper's empirical evaluation focuses predominantly on existing zero-shot baselines (LLMs, TFMs) and compositional methods (TEXTREV, FUNCREV), but does not propose or evaluate a dedicated multimodal architecture specifically designed to leverage the unique, high-quality structure of the TimesX context, leaving the full potential of the benchmark unquantified.\n\n2. Complexity and Accessibility of Data Generation: The proposed Hypothesizer-Verifier-Enricher multi-agent pipeline for generating time-stamped events is highly sophisticated and relies on advanced LLMs and automated tools, which may make the reproducibility and cost-effective updating of the benchmark challenging for the broader academic community without access to comparable resources.\n\n3. Absence of Fine-tuning Evaluation: While the authors rightly focus on zero-shot evaluation, the paper explicitly notes that TimesX does not offer a pretraining or fine-tuning dataset, thereby limiting its utility for researchers aiming to develop and benchmark new fine-tuned multimodal TSF models that could potentially exploit the richness of the context more effectively than zero-shot methods."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zAXu3GD8Se", "forum": "Z1TMV4bGuu", "replyto": "Z1TMV4bGuu", "signatures": ["ICLR.cc/2026/Conference/Submission22122/Reviewer_ir7p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22122/Reviewer_ir7p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553860473, "cdate": 1761553860473, "tmdate": 1762942074281, "mdate": 1762942074281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new benchmark called TimesX for multimodal time series forecasting, addressing three key challenges: (a) insufficient generalization gap due to small-scale or synthetic datasets, (b) data leakage issues, and (c) limited types of textual context. To tackle (a), the authors propose 190 diverse datasets. For (b), they implement a time isolation approach, ensuring the target forecast date occurs after the model's public knowledge cutoff to prevent data contamination, along with an automated data refresh mechanism for evaluating future pretrained models. To address (c), the authors expand the textual context by including metadata, calendar events, covariates, and other event types. They benchmark several existing models on TimesX, demonstrating the limitations of previous benchmarks and highlighting the advantages of their proposed benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Benchmarking correctly multimodal time series forecasting is important and timely\n\n2. The benchmarks contain a lot of datasets"}, "weaknesses": {"value": "1. The justification for the dataset’s relevance is weak. The claim that small-scale and synthetic datasets hurt generalization is debatable. How do the authors demonstrate that their benchmark is more realistic than others? They mention that synthetic datasets focus on certain multimodal aspects, but how can they prove their benchmark doesn't have the same limitations, given its focus on specific elements (e.g., metadata, calendar events, covariates)?\n\n2. The authors should provide more details on the origin of the time series data. Are these datasets from previous benchmarks? What are the new datasets, and where do they come from? It would also be useful to visualize the datasets (e.g., PCA, t-SNE) to show their diversity, check for distribution shifts, and explain the train/test split process.\n\n3. The originality of the paper seems limited. The main innovation the three agents managing dataset generation resembles prompt engineering for time series description. More in-depth analysis is needed on the dataset’s relevance, including PCA representations, distribution shifts, outlier analysis, LLM hallucinations, and the quality of time series descriptions."}, "questions": {"value": "1. Could the authors clarify the origins of the time series datasets? Specifically, are they used in any well-known benchmarks (even non-multimodal ones)? A table listing the datasets and indicating where they have been previously used would be helpful.\n\n2. It would be useful to perform a basic analysis of the time series. What are the distributions over time, across samples, and over channels for each dataset? Do these distributions appear realistic, or do they seem trivial? Additionally, what is the correlation profile between channels and the time structure?\n\n3. The process involving the three agents for text generation seems promising, but how can we be sure it doesn't lead to hallucinations? Is the generated text redundant with information already present in the time series? Could the authors consider introducing a gradual difficulty in the generated text where the same dataset would have progressively more informative descriptions (even within the same event category)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4vXT6XgwxX", "forum": "Z1TMV4bGuu", "replyto": "Z1TMV4bGuu", "signatures": ["ICLR.cc/2026/Conference/Submission22122/Reviewer_JSEc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22122/Reviewer_JSEc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817921427, "cdate": 1761817921427, "tmdate": 1762942073750, "mdate": 1762942073750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TimesX, a large, real-world multimodal TS forecasting benchmark that pairs numeric series with four kinds of text context (metadata, calendar, covariates, and time-stamped events). It contains time series forecasting benchmark over 19 diverse domains and 190 variables in total. The key design choices are (i) time isolation (evaluate only after model knowledge cutoffs) with an auto-refreshable pipeline to mitigate leakage, and (ii) a multi-agent hypothesizer-verifier-enricher workflow to to create an event corpus with verifiable facts, URL, and accurate timestamps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark is large-scale and diverse covering different geographical locations, spanning over 2.5 years (Jan 2023 - Jun 2025), and over 19 domains with 10 variables per domain, and two granularities (weekly and daily)\n\n- The hypothesizer-verifier-encricher agent framework can help truthfulness and mitigate data leakage issues."}, "weaknesses": {"value": "- Evaluation horizons are selected to be after the model cutoff date, but there is no guarantee that the hypothesizer-verifier-encricher framework will not retrieve data from the future horizon/beyond the cutoff data (since it works based on web-search to my understanding).\n\n- The authors claim the benchmark can be refreshed for a new pretrained model with a later cutoff date, but the paper does not provide details on how the refresh works or what the refresh exactly entails? Does it amount to just prompting the model to not look beyond a certain date?  If so, that is not a contribution of this work since any LLMs can be prompted to search for information within a time window."}, "questions": {"value": "- How many tasks for each domain should be made clear in the main text\n\n- In figure 1, which tasks from CiK do the authors run the 3 models on? Is it over a single dataset, or over multiple CiK datasets?\n\n- Throughout the manuscript, authors report geometric mean. Is there a reason why this was chosen over the more commonly used arithmetic mean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hkYniXHd2r", "forum": "Z1TMV4bGuu", "replyto": "Z1TMV4bGuu", "signatures": ["ICLR.cc/2026/Conference/Submission22122/Reviewer_JSNx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22122/Reviewer_JSNx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149044825, "cdate": 1762149044825, "tmdate": 1762942073461, "mdate": 1762942073461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TimesX, a novel multimodal time series forecasting benchmark designed to address limitations in existing benchmarks. The key contributions are: (1) A large-scale, real-world dataset spanning 19 domains with 190 variables, featuring diverse textual contexts (metadata, calendar events, covariates, and timestamped events); (2) A data generation pipeline with strict time isolation to prevent data leakage and enable automatic updates; (3) Empirical analysis showing that simple ensemble methods outperform complex LLM-based revisions on real-world data, challenging conclusions from synthetic benchmarks. The benchmark emphasizes real-world applicability, leakage prevention, and high-quality context alignment through a multi-agent workflow."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "Large-scale: a new benchmark to combine real-world data, leakage prevention, and automated context generation. The multi-agent workflow for event extraction is innovative.\nQuality: Extensive evaluation (13 methods across 190 variables) with robust metrics (MASE). The pipeline design ensures reproducibility.\nClarity: Well-organized, with clear examples (e.g., gas price case study) and appendix support.\nSignificance: Challenges prevailing assumptions about LLM superiority in synthetic benchmarks and provides a practical foundation for future research."}, "weaknesses": {"value": "Coverage gaps: Most series are daily or weekly. Future versions could add lower-frequency series to broaden applicability.\nLanguage & region bias: Current textual contexts are English-centric and skew toward North-American events. Authors could discuss plans for multilingual expansion.\n\nHyper-parameter sensitivity: Peak-detection thresholds (θ, Kmax, Lmax) that drive event recall are fixed; an ablation or sensitivity plot would strengthen robustness claims.\n\nEvaluation metrics: Relies solely on MASE. Adding CRPS or interval scores could illuminate probabilistic calibration differences between TFMs and LLMs.\n\nThe context generation pipeline relies on LLMs and web searches, which may inherit biases or scalability limitations. A cost/error analysis of the pipeline would strengthen practicality."}, "questions": {"value": "How scalable is the multi-agent context generation pipeline to larger datasets or domains with sparse events (e.g., rare diseases)?\n\nCould the benchmark incorporate uncertainty quantification metrics (e.g., prediction intervals) to assess model reliability?\n\nHave you explored fine-tuning LLMs/TFMs on TimesX, and how might it alter the conclusions about ensemble methods?\n\nWhat steps are taken to mitigate potential biases in event data sourced from web searches (e.g., geographic or media bias)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns. Data sources are public, scraping abides by robots.txt, and no personally identifiable information is collected."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6gBXpEx2wG", "forum": "Z1TMV4bGuu", "replyto": "Z1TMV4bGuu", "signatures": ["ICLR.cc/2026/Conference/Submission22122/Reviewer_k69o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22122/Reviewer_k69o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762767550375, "cdate": 1762767550375, "tmdate": 1762942073186, "mdate": 1762942073186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Rebuttal"}, "comment": {"value": "We thank all reviewers for recognizing **the importance of multimodal time series forecasting** and for their positive feedback on the scale, realism, and time-isolation design of TimesX. Progress in multimodal TSF is largely limited by **the lack of a suitable benchmark**: existing datasets are often small or synthetic, provide only a narrow range and limited quality of context, and cannot be automatically refreshed, which leads to information leakage when evaluating rapidly evolving LLMs and TFMs.\n\nOur goal in this work is to mitigate this pressing evaluation bottleneck. To our knowledge, TimesX is the first **real-world, large-scale, leakage-free, and refreshable** multimodal TSF benchmark: it covers **19 domains and 190 variables**, aligns four kinds of context (metadata, calendar signals, covariates, and time-stamped events), based on an automated dataset agent to support automatic refresh. We conduct a comprehensive evaluation involving more than **312,000 independent LLM inferences** to assess the performance of three advanced TFMs, six advanced LLMs, and three agent variants.\n\nWe greatly appreciate the reviewers’ comments and suggestions. Based on this feedback, we made the **following changes**:\n\n**Dataset Scale and Diversity.**\n\n* We **refresh the evaluation set** for all 190 variables from 2025-06-30 to 2025-10-31.\n* We **construct a training set** for the same 190 variables from 2018-01 to 2022-12.\n* We add an **out-of-distribution evaluation set** with 11 multilingual variables and 5 rare-disease variables.\n* All extended datasets are available at:\n  `https://anonymous.4open.science/r/TimesX_UnderReview-387D/Datasets_Extended/`\n\n**Dataset Construction and Analysis.**\n\n* We add a complete workflow diagram for the dataset agent (Figure 34) and an execution example (Pages 92–93).\n* We describe the mechanisms to reduce hallucinations and report manual verification results (Page 94).\n* We include PCA and t-SNE visualizations (Pages 95–97).\n\n**Experiments.**\n\n* We add results on all remaining CiK subsets (Table 168).\n* We add a benchmark scale sensitivity study (Figure 33).\n* We add an uncertainty evaluation using CRPS (Table 169).\n* We add few-shot in-context learning results (Table 171).\n\nWe have revised the submission and added discussion of future plans on Pages 98–101, with all new content marked in red."}}, "id": "CV4myuhP88", "forum": "Z1TMV4bGuu", "replyto": "Z1TMV4bGuu", "signatures": ["ICLR.cc/2026/Conference/Submission22122/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22122/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission22122/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763692393376, "cdate": 1763692393376, "tmdate": 1763692393376, "mdate": 1763692393376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}