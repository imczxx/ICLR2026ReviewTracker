{"id": "pkfyHdq9Rl", "number": 17415, "cdate": 1758275790113, "mdate": 1759897176811, "content": {"title": "Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation", "abstract": "Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time. Over successive planning cycles, these errors compound, potentially resulting in severe failures. Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue. To this end, we propose a method termed Sequence of Experts (SoE)—a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. The key idea is to retain intermediate models from training that possess inherent differences in driving errors, and then alternate the activation of different models at certain temporal intervals. This approach not only preserves the consistency capability across multiple models but also leverages their differences to enhance robustness. As a plug-and-play solution for existing IL planners, our approach requires no architectural modifications or prior knowledge of scenarios, making it highly practical for real-world deployment. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance. This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.", "tldr": "This work proposes Sequence of Expert, a plug-and-play method to improve performance of IL planners. Through extensive evaluation, we show that SoE is universally effective for all IL planners.", "keywords": ["Imitation learning", "Autonomous driving"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab6b5f90b79ef152da16d8b2bce3022b9b6678e4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper \"Sequence of Expert: Boosting Imitation Planners for Autonomous Driving Through Temporal Alternation\" proposes a new method termed \"sequence of Experts (SoE) to enhance the pipeline's closed-loop performance without modifying the model architecture or applying data augmentation. The idea is to train multiple runs of the same planner architecture with different random seeds and, at inference time, alternate between the original policy and these \"expert\" models at fixed temporal intervals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important problem in autonomous driving, that is, the performance gap between open-loop training and closed-loop evaluation.\n-  The proposed approach is conceptually simple and easy to integrate into existing systems without retraining from scratch.\n- The authors provide empirical evaluations on the nuPlan benchmark and demonstrate improvements over baseline imitation planners."}, "weaknesses": {"value": "- Unclear scheduling mechanism:\n\nThe description of the temporal scheduling mechanism (Section 4.2) is insufficiently detailed. It is unclear whether the scheduling function $\\delta$ is a predefined periodic function or a trainable component. Additionally, the relationship between the policy $\\pi^*$ and the final SoE policy should be explicitly clarified.\n\n- Limited practicality for deployment:\n\nAlthough the method is described as plug-and-play, its deployment feasibility is questionable. Maintaining multiple large expert models (e.g., >1 GB each for UniAD-style planners) poses serious storage and memory challenges for real-world on-board systems.\n\n- Lack of theoretical grounding:\n\nThe approach relies on the empirical assumption that training with different random seeds yields complementary closed-loop behaviors. This assumption lacks theoretical justification and may not generalize across tasks or architectures."}, "questions": {"value": "1. What exactly is the scheduling function $\\delta$?\n2. In the section 5.1, $f()$ denotes the nuPlan validation score of a policy, is it the same as the notation in Eq. (4)?\n3. What is the meaning of \"R\", \"L\", and \"H\" in Table I?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SjwLzcXBB2", "forum": "pkfyHdq9Rl", "replyto": "pkfyHdq9Rl", "signatures": ["ICLR.cc/2026/Conference/Submission17415/Reviewer_4i59"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17415/Reviewer_4i59"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761243688174, "cdate": 1761243688174, "tmdate": 1762927311524, "mdate": 1762927311524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the persistent closed-loop performance degradation in imitation-learning (IL) planners, driven by the accumulation of small prediction errors under distribution shift. The proposed Sequence of Experts (SoE) method alternates between independently trained models (using different random seeds) across time steps, aiming to exploit their complementary error characteristics without adding parameters or inference cost. The authors provide empirical observations motivating this variance-based approach and evaluate SoE on the nuPlan benchmark, reporting consistent improvements across multiple planner architectures, including state-of-the-art results for a strong baseline system."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "S1. Explores temporal alternation rather than architectural scaling to mitigate closed-loop error accumulation, representing a rarely addressed improvement dimension for IL planners.\n\nS2. Introduces zero additional inference cost and requires no model or data modifications, making deployment highly practical.\n\nS3. Demonstrates consistent and meaningful closed-loop performance gains across diverse planners, including achieving SOTA on nuPlan.\n\nS4. Provides empirical evidence on OL–CL mismatch and seed-induced complementarity, offering useful insights into why IL planners degrade in closed-loop settings."}, "weaknesses": {"value": "W1. The claim that different seeds provide complementary error-accumulation behaviors is supported only by empirical observations; the paper lacks a deeper theoretical explanation or dynamic modeling of why such complementarity should reliably occur.\n\nW2. The experts differ solely by random seeds under identical architectures and data, raising concerns about whether this restricted diversity is consistently strong and generalizable beyond the evaluated cases, especially in larger amount of data.\n\nW3. Selecting experts using Val14 and then reporting gains largely based on Val14 introduces a risk of data leakage or overfitting to the validation set, potentially inflating the reported improvements.\n\nW4. While inference cost is unchanged, SoE requires multiple full training runs and quadratic validation combinations, making the total computational cost high and less scalable for large industrial models.\n\nW5. Although the paper claims “additional computational overhead,” the method requires multiple full training runs and maintaining several model instances, which substantially increases both computation and deployment overhead. These costs may exceed those of training a single stronger model, so the computational advantage is not guaranteed to scale. The authors should clarify that the zero-cost claim applies only to inference."}, "questions": {"value": "Same as Weakness.\n\nOther questions:\n\nQ1. Could the authors provide results where expert selection relies **only on the training set** (e.g., training loss or closed-loop proxy) while **evaluation fully on held-out test split**? This would clarify whether the current gains depend on information from the validation distribution.\n\nQ2. Does SoE introduce new types of closed-loop failures—e.g., oscillatory heading changes or unstable lateral control—caused by switching between policies with different behavioral biases?\n\nQ3. Table 1 shows identical inference latency for SoE and the original model. How is policy switching performed without introducing any runtime overhead in memory usage or scheduling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZCYkAsrOHX", "forum": "pkfyHdq9Rl", "replyto": "pkfyHdq9Rl", "signatures": ["ICLR.cc/2026/Conference/Submission17415/Reviewer_xYof"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17415/Reviewer_xYof"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761479101352, "cdate": 1761479101352, "tmdate": 1762927310888, "mdate": 1762927310888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This research addresses a critical limitation of imitation learning in autonomous driving, where small errors accumulate over time leading to failures in closed-loop scenarios. The proposed \"Sequence of Experts\" (SoE) method improves performance by alternating between different trained models at specific intervals, leveraging their diverse error profiles for enhanced robustness. As a simple and adaptable solution, SoE significantly boosts the performance of existing imitation learning systems without requiring architectural changes or additional data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* A very simple approach: just alternate experts (SoE) every 2nd timestamp\n* Working on nuPlan, a quite widely used benchmark"}, "weaknesses": {"value": "* Obvious weakness: validation set should be VERY close in terms of distribution to the test set in order to find the correct combination of experts for SoE\n* No any ablations / exploration on whether exists a situation when the best combination of experts on val is not the best on the test\n* Straightforward drawback: need to wait (and spoil resources) for training multiple models in order to include them into SoE (and usage of different ckpts during one training cycle is not the best strategy according to Table 2)\n* Diffusion-based planner is still the best according to the Table 1\n\nOverall, the approach is SUPER simple and no any theoretic considerations: let's just train with multiple random seed different planners and then alternate between them every n'th timestamp."}, "questions": {"value": "* Is the scheduling function $\\sigma(t)$ defined in Eq. (4)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cSUhIpdNzP", "forum": "pkfyHdq9Rl", "replyto": "pkfyHdq9Rl", "signatures": ["ICLR.cc/2026/Conference/Submission17415/Reviewer_gkWn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17415/Reviewer_gkWn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774426360, "cdate": 1761774426360, "tmdate": 1762927309900, "mdate": 1762927309900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"Sequence of Experts\" (SoE), a novel and simple plug-and-play framework designed to mitigate the issue of error accumulation in imitation learning-based autonomous driving planners. Instead of relying on a single, imperfect model, SoE employs a temporal alternation strategy, periodically switching between a primary policy and a pre-selected \"expert\" policy during closed-loop deployment. The authors hypothesize that this periodic switching disrupts the compounding of errors by introducing corrective actions from a model with a different failure mode, thereby enhancing robustness. The method requires no architectural modifications and no additional training data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses error accumulation problem from a new perspective. Instead of focusing on improving a single model's architecture or data, it reframes the problem as one of optimal policy deployment. The key insight is that models from different training stages exhibit complementary weaknesses.\n\n2. The authors demonstrate the effectiveness of SoE across a diverse set of baseline planners (rule-based, MLP-based, Transformer-based), proving its broad applicability.\n\n3. The \"plug-and-play\" nature and simple implementation make it highly practical and immediately applicable for researchers and practitioners in the field."}, "weaknesses": {"value": "1. It seems just blindly switching models over time. The system does not appear to detect or anticipate error accumulation before switching. It switches policies regardless of whether the current policy is performing well or poorly. This could be suboptimal, as it might unnecessarily interrupt a perfectly good trajectory or fail to switch at the most critical moment. Have the authors considered a more intelligent, state-dependent switching strategy (e.g., based on model uncertainty, trajectory deviation, or a learned gating function) that could trigger the expert policy \"on-demand\"? Such a comparison would better isolate the benefits of switching itself versus the specific temporal strategy proposed.\n\n2. The temporal scheduling, switching models with fixed time interval, doesn't make sense to me. There is ambiguity in the \"Expert\" Role and Selection Process. The term \"Sequence of Experts\" implies that the secondary policy (pi^\\*) acts as an \"expert\" in situations where the primary policy (pi^0) fails. However, the paper's description of this relationship lacks quantitative evidence and a clear mechanism. How is the \"expertise\" of a policy quantified? For example, at the moment of switching, is there any evidence that the chosen \"expert\" policy (pi^\\*) indeed has a higher probability of success or a better understanding of the current state than (pi^0)? The framework would be more convincing if it included a mechanism to justify why (pi^\\*) is the \"expert\" at that specific moment. Without this, the method appears to be more of a \"policy alternation\" or \"policy shuffling\" rather than a true expert consultation.\n\n3. The method currently selects only one expert policy (pi^\\*) to pair with the primary policy (pi^0). Given that different models might excel in different scenarios or under different evaluation metrics (e.g., one is better at collision avoidance, another at comfort), why limit the pool to a single expert? Have the authors explored a dynamic approach where the system could choose from a larger pool of candidate experts based on the current driving context? The current offline selection of a single, fixed expert seems to underutilize the full potential of model complementarity.\n\n4. The paper states that the method has a low computational overhead because it only runs one model per step. However, deploying SoE requires loading two models into memory (e.g., VRAM). For large-scale models, this could be a non-trivial memory cost. It would be beneficial for the authors to provide a clear analysis of the memory footprint and actual inference latency compared to a single-model baseline.\n\n5. The selection process is performed offline on a validation set. How robust is this selection? For instance, does the best pair (m-i, m-j) on the validation set consistently perform as the best pair on the test set?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "91t7CPuh6O", "forum": "pkfyHdq9Rl", "replyto": "pkfyHdq9Rl", "signatures": ["ICLR.cc/2026/Conference/Submission17415/Reviewer_3YTF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17415/Reviewer_3YTF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897267794, "cdate": 1761897267794, "tmdate": 1762927308987, "mdate": 1762927308987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}