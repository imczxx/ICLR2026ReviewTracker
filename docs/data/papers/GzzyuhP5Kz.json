{"id": "GzzyuhP5Kz", "number": 4545, "cdate": 1757704648340, "mdate": 1759898027347, "content": {"title": "RULERv2: From Basic Retrieval to Complex Reasoning, A Bottom-Up Benchmark for Long-Context Evaluation", "abstract": "Recent advances in long-context language models have spurred development of diverse benchmarks that often test multiple skills simultaneously, making it difficult to identify specific failure modes. To address this, we introduce RULERv2, a benchmark with systematic difficulty progression from basic synthetic retrieval to complex multi-step reasoning across three domains: multi-key NIAH, multi-value NIAH, and multi-doc QA.We conduct a large-scale evaluation of leading models, including seven closed-source and 26 open-weight models. Our findings reveal a notable performance gap between the two. Critically, we demonstrate that all models, including those claiming million-token context windows, exhibit performance degradation with increasing length, highlighting an unresolved challenge. Our analysis shows that explicit decomposition into a retrieve-then-solve strategy outperforms the implicit, single-step approach, and chain-of-thought reasoning enables models to discover effective decomposition autonomously. Finally, we find that even top-performing open-weight models struggle with fundamental retrieval and copying tasks, leading to degraded performance on more complex problems.", "tldr": "We propose a novel synthetic benchmark RULERv2 to evaluate long-context language models", "keywords": ["Long-context", "Evaluation", "Benchmark", "Synthetic"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff2f8c3afd11f96df04e78d39e14b6a055f932bc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper (RULERv2) aims to address a core issue in the current evaluation of long-context language models: existing benchmarks typically combine multiple skills such as retrieval, aggregation, and reasoning into a single test. This makes it difficult for researchers to accurately identify the root weakness when a model fails (e.g., whether the failure stems from poor retrieval or flawed reasoning).\n\nTo solve this problem, the authors propose RULERv2, which spans three task domains (multi-key NIAH, multi-value NIAH, and multi-document QA). It gradually increases task complexity, transitioning to complex tasks that require retrieval, comprehension, counting, and multi-step reasoning. Its core innovation lies in providing a \"structured, bottom-up diagnostic framework.\" RULERv2 is more than just a \"performance ranking list\"; it functions like a \"diagnostic toolbox.\" It helps researchers accurately pinpoint which fundamental step a model is failing at—whether it is basic retrieval, multi-value retrieval, counting, or task decomposition."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The most prominent strength of this paper lies in its innovative systematic \"bottom-up\" diagnostic framework. It is not merely a performance benchmark, but a powerful \"failure attribution\" tool that can accurately diagnose whether a model fails in basic retrieval tasks (e.g., Easy tasks) or advanced strategy tasks (e.g., Hard tasks). By comparing the scores of explicit decomposition (Medium tasks) and implicit solving (Hard tasks), the paper uses data to clearly demonstrate for the first time that the core flaw of models lies in the lack of \"autonomous task decomposition\" capability, rather than the absence of the capability itself. It further reveals that Chain-of-Thought (CoT) can compensate for this flaw by helping models independently plan and decompose task steps. Finally, the evaluation design cleverly avoids the impact of data leakage and even converts it into evidence, proving that retrieval failure is a more fundamental bottleneck than knowledge deficiency. In summary, this is an evaluation tool with an elegant design, solid experiments, and profound insights."}, "weaknesses": {"value": "While RULERv2 excels at diagnosing retrieval capabilities, its main drawback lies in the significant disconnect between its evaluation tasks and real-world needs. It overrelies on synthetic \"needle-in-a-haystack\" style tasks, which cannot effectively assess models' more critical real-world abilities of \"information aggregation\" and \"comprehensive distillation\". Secondly, since the evaluation data (e.g., MMLU) is highly likely to have been \"memorized\" by models, its \"reasoning\" component becomes hollowed out—reducing the evaluation to \"retrieval plus recall\" rather than genuine \"retrieval plus reasoning\". Finally, this also leads to an overly narrow definition of \"fundamental skills\": it one-sidedly equates long-context capabilities with \"fact localization\", while ignoring other equally important fundamental abilities such as \"aggregation\"."}, "questions": {"value": "1. Why isn’t \"Aggregation\" regarded as a foundational skill on par with Retrieval?\n- Many long-context tasks in the real world—such as \"summarizing a 100-page financial report\" or \"understanding the logic of a codebase\"—rely not on \"locating\" a discrete \"needle\" as their foundational skill, but on \"aggregating\" a large amount of \"diffuse\" information distributed throughout the text.\n- The paper criticizes other benchmarks (e.g., Longbench) for \"mixing multiple skills\" (because they include summarization tasks). However, RULERv2 itself fails to properly cite or discuss existing evaluation works that attempt to isolate the abilities of \"aggregation\" or \"summarization.\"\n- It seems to simply categorize \"aggregation\" under \"advanced reasoning\" (as mentioned in the introduction). Yet, I believe \"aggregation\" can fully be treated as another foundational skill—on par with \"retrieval\"—that requires \"bottom-up\" testing.\n2. Evasion of the Limitation of \"Hollowed-Out Reasoning\"\n- Using datasets like MMLU—whose content is highly likely to have been \"memorized\" by models—renders the \"reasoning\" step trivial.\nThe paper skillfully leverages this fact to prove that \"retrieval failure is a more fundamental bottleneck.\"\nHowever, it does not adequately address the new limitation this creates: the Hard tasks can no longer test \"retrieval + genuine reasoning,\" but only \"retrieval + autonomous decomposition.\"\n- As a result, the paper also fails to properly cite relevant literature—specifically works that discuss how to construct \"non-memorable\" evaluations that truly require \"on-the-fly reasoning.\"\n\nIn conclusion, this paper delves deeply into the vertical domain of \"retrieval\" and provides comprehensive citations. Yet, when arguing that \"retrieval\" is the sole or most important cornerstone, it does not sufficiently discuss relevant research on other foundational skills such as \"aggregation.\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aiIo10oUt9", "forum": "GzzyuhP5Kz", "replyto": "GzzyuhP5Kz", "signatures": ["ICLR.cc/2026/Conference/Submission4545/Reviewer_9KeH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4545/Reviewer_9KeH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751347478, "cdate": 1761751347478, "tmdate": 1762917432052, "mdate": 1762917432052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RULERv2, a benchmark that evaluates the retrieval and reasoning capabilities of LLMs in long context settings. RULERv2 incorporates systematic tests of varying difficulty, from basic synthetic retrieval to multi-step reasoning, across three domains: multi-key NIAH, multi-value NIAH, and multi-document QA. These tasks extend the widely used Needle-in-a-Haystack tests by introducing multi-key and multi-value variants that isolate retrieval from reasoning, effectively decomposing different failure modes. Through experiments on 33 popular LLMs (including 7 closed-source and 26 open-weight models), the authors showcase consistent performance degradation with longer contexts and highlight weaknesses in retrieval and copying."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written, and the proposed benchmark is clearly presented and described in sufficient detail.\n- The proposed task suite yields a fine-grained benchmark, where models are tested across tasks of varying difficulty and nature, from synthetic retrieval to multi-step reasoning.\n- The empirical evaluation is comprehensive, spanning a diverse set of model families and scales, and provides detailed results for many widely used and state-of-the-art LLMs.\n- The results in Sections 4 and 5 are particularly interesting. In particular, the observation that a retrieve-then-solve approach outperforms direct reasoning is insightful, as it implies that, in long-context settings, many models may underperform primarily due to inaccurate retrieval. Moreover, the paper provides strong evidence that the long-context capabilities claimed by several models are often overstated in practice. Overall, the results are insightful, and interesting."}, "weaknesses": {"value": "- As the authors also point out in the limitations section, while the benchmark is comprehensive, it is mostly artificial. Therefore, although the results are interesting and comprehensive, they may not immediately translate to real world deployment scenarios."}, "questions": {"value": "- In Figure 4, only the analysis of `Qwen3 235B Instruct 2507` is shown. Are the results qualitatively consistent across model families and scales?\n- In the scaling width experiments of Section 5, how exactly do you define a \"maximum score\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XBW5kiSUED", "forum": "GzzyuhP5Kz", "replyto": "GzzyuhP5Kz", "signatures": ["ICLR.cc/2026/Conference/Submission4545/Reviewer_6Pmo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4545/Reviewer_6Pmo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889079167, "cdate": 1761889079167, "tmdate": 1762917431820, "mdate": 1762917431820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark that progressively increases task difficulty from basic synthetic retrieval to complex multi-step reasoning across three domains. It is used to evaluate 33 long-context models, and results uncover the limitations in current long-context capabilities that challenge existing claims of solved long-context understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ a new benchmark that progressively increases task difficulty from basic synthetic retrieval to complex multi-step reasoning across three domains\n+ comprehensive evaluation using 33 long-context models to uncover the limitations in current long-context capabilities"}, "weaknesses": {"value": "- while the benchmark is good, the technical contributions are limited\n- the findings reported in the evaluation are less insightful"}, "questions": {"value": "What are the challenges/difficulties in constructing the benchmark? What can be learned from the evaluation results to improve the performance of the models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rGyBtzliI0", "forum": "GzzyuhP5Kz", "replyto": "GzzyuhP5Kz", "signatures": ["ICLR.cc/2026/Conference/Submission4545/Reviewer_MZgD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4545/Reviewer_MZgD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997747333, "cdate": 1761997747333, "tmdate": 1762917431443, "mdate": 1762917431443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents RULERv2, a long-context benchmark with 12 tasks across three domains and four difficulty levels, aiming to evaluate LLMs from basic retrieval to complex reasoning. It tests 33 models (closed- and open-source) over contexts up to 1M tokens, analyzing performance degradation, model scaling, and test-time compute methods such as few-shot, chain-of-thought, and majority voting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\t**Comprehensive task coverage**.\nThe paper proposes a benchmark with 12 distinct tasks across three domains and four difficulty levels, which provides a relatively rich and systematic test suite. The breadth of evaluation makes the work look comprehensive and empirically grounded.\n2.\t**Diverse model evaluation.**\nThe experiments cover multiple model families — dense transformers, hybrid architectures, and Mixture-of-Experts (MoE) — including both closed- and open-weight models. This diversity offers a balanced empirical comparison.\n3.\t**Exploration of test-time compute scaling.**\nThe paper includes additional analyses such as majority voting, few-shot scaling, and reasoning step scaling. These experiments are useful for understanding whether increased inference-time compute improves performance (finding: only marginal gains)."}, "weaknesses": {"value": "**1. Limited conceptual novelty.**  \nWhile the paper frames RULERv2 as a “bottom-up” benchmark progressing from basic retrieval to complex reasoning, this design direction is *not entirely new*.  \nEarlier work such as **NeedleBench** has already proposed a clear decoupling between retrieval and reasoning, with additional control over information density and retrieval difficulty.  \nRULERv2’s main contributions appear to be **engineering-oriented extensions** — expanding task coverage, model variety, and systematic structure — rather than introducing a fundamentally new methodological idea.  \n\n**2. Potential data contamination.**  \nThe benchmark relies on **HotPotQA** and **MMLU**, both of which are known to appear extensively in modern LLM pretraining corpora (e.g., Wikipedia-based content and academic QA).  \nWithout explicit measures to control for **memorization effects**, it is difficult to determine whether the reported results genuinely assess **long-context reasoning**, or instead reflect **retrieval of memorized facts**.  \nPrior studies (e.g., *NeedleBench, Realistic vs. Synthetic Multi-Needle Tasks*) have shown notable performance drops once contaminated datasets are replaced by synthetic or unseen data.  \nA discussion or ablation to address this issue would strengthen the paper’s claims.  \n\n**3. Highly synthetic and procedural task design.**  \nMany tasks are **programmatically generated** and exhibit structured templates that differ from **real-world long-context scenarios** such as document summarization or multi-document reasoning.  \nWhile such synthetic setups facilitate controlled evaluation, they may also make the tasks **susceptible to overfitting** through supervised fine-tuning, potentially limiting generalization.  \nMore **naturalistic benchmarks** (e.g., *LongBench v2*, Bai et al., ACL 2025) could complement RULERv2 by providing a stronger test of real-world long-context understanding. )\n\n[1] NeedleBench: Evaluating LLM Retrieval and Reasoning Across Varying Information Densities. Transactions on Machine Learning Research.\n\n[2] LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks (Bai et al., ACL 2025)"}, "questions": {"value": "**1. On dataset contamination.**  \nCould the authors elaborate on whether potential **data overlap** with pretraining corpora (e.g., from MMLU or HotPotQA) has been analyzed or mitigated?  \nIt would be helpful to understand how much of the observed performance may stem from **memorization** versus genuine **long-context reasoning**, perhaps through simple baseline comparisons or contamination checks.  \n\n**2. On task realism.**  \nHave the authors considered incorporating **more realistic long-context tasks**, similar in spirit to recent benchmarks that focus on **naturalistic document understanding**?  \nSuch inclusion could make the benchmark more reflective of real-world long-context applications.  \n\n**3. On positioning and contribution.**  \nIt would be valuable for the authors to clarify what they view as the **main conceptual advance** of RULERv2 relative to **prior long-context benchmarks**.  \nSince the idea of separating retrieval from reasoning has been explored before, highlighting what is **distinct or improved** in RULERv2 would help readers better appreciate its contribution and positioning. |"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6lFhurfjgk", "forum": "GzzyuhP5Kz", "replyto": "GzzyuhP5Kz", "signatures": ["ICLR.cc/2026/Conference/Submission4545/Reviewer_jbNo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4545/Reviewer_jbNo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762365823162, "cdate": 1762365823162, "tmdate": 1762917431149, "mdate": 1762917431149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}