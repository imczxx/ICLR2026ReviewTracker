{"id": "9fHuFi4Qsu", "number": 12165, "cdate": 1758206088243, "mdate": 1759897527857, "content": {"title": "ConceptBot: Knowledge-Graph–Grounded Commonsense for Task Decomposition in LLM Robot Planning", "abstract": "Robotic planning breaks down when commonsense reasoning is required to resolve linguistic ambiguity and to interpret objects correctly. To address this, we present ConceptBot, a modular planning framework that integrates large language models with knowledge graphs to produce feasible, risk-aware plans while jointly disambiguating instructions and grounding object semantics.\nConceptBot comprises three components: (i) an Object Properties Extraction (OPE) module that augments scene understanding with semantic concepts from ConceptNet; (ii) a User Request Processing (URP) module that resolves ambiguities and structures free-form instructions; and (iii) a Planner that synthesizes context-aware, feasible pick-and-place policies. Evaluations in simulation and on real-world setups show consistent gains over prior LLM-based planners—for example, +56 percentage points on implicit tasks (87% vs. 31% for SayCan) and +61 points on risk-aware tasks (76% vs. 15%)—and an overall score of 80% on SafeAgentBench. These improvements translate to more reliable performance in unstructured environments without domain-specific training.", "tldr": "We introduce ConceptBot, a hybrid framework that combines LLMs with commonsense knowledge graphs to produce safer and more reliable robotic task plans in unstructured environments.", "keywords": ["robotic planning", "task decomposition", "large language models", "knowledge graphs"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/106105de6747d7d8f209b9e218ca1152611e9e46.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "# Paper summary\nThis paper proposes a pipeline that integrates a knowledge base with LLMs for robot planning. The system consists of three main parts. First, the object properties extraction (OPE) module detects objects of interest in the image together with relations extracted from the knowledge base. Second, some keywords are extracted from the user prompt based on the available objects and their relations with other concepts. This part is aimed to remove the ambiguity in the user instruction (if any) by focusing on what’s available in the current scene, and that the output of this part can be thought of as the transcription of the user instruction. Lastly, a SayCan-like combination of LLM actions with affordance values is used as the ultimate policy.\n\n# Review summary\nThe paper’s motivation is interesting since one would expect the LLMs to have the commonsense knowledge present in these structured knowledge bases. On the other hand, we see that the overall pipeline performs better in some scenarios with such structure. As such, I believe there’s a contribution in the paper. Nevertheless, some questions still remain, mainly, to which part we attribute this improvement. For instance, does the knowledge base indeed play an essential role, or is it due to the structure that we get the increase? How would the results change if we were to have another LLM suggest those relations instead of a knowledge base? This might seem like an artificial reviewer2 type of question, but I really wonder whether it’s due to knowledge bases filtering out the hallucinations or the designed structure. This is important because the paper claims the main contribution is due to reducing the hallucinations through integration with knowledge bases. Somewhat as a minor concern, there are some clarity issues, which I detailed below."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The introduction of a structure in parsing the user input and producing the final output makes sense. Furthermore, the system not only relies on the inherent knowledge of the LLM but also queries a knowledge base for a more consistent and structured output. I think the main strengths of this paper are these proposed structures."}, "weaknesses": {"value": "While the paper goes over the details of the method, some parts are a bit unclear, making it hard to assess what’s happening under the hood. While I believe the method looks promising, clarity issues (detailed below) in the current presentation are a problem. The main one that stuck in my head is not being able to understand if the contribution is due to the structured input and outputs, or reliance on knowledge bases."}, "questions": {"value": "I've already tried to emphasize my main concern, so, you could think that as the main important question. I'm not saying the below questions/points are not important, but those could be figured out at some point.\n\n1. I’m not sure how ConceptNet reduces hallucinations, since LLMs already have some sort of commonsense, and the hallucinations are rather at a high level.\n2. How can we call this procedure grounding when the grounding happens just to another symbolic system, not to the robot’s sensorimotor substrate?\n3. Can’t LLMs figure out what the relevant properties of objects of interest are? I don’t understand how it fundamentally differs from a knowledge base.\n4. Please add a proper citation to ViLD instead of a footnote. From what I can see, there’s a relevant paper mentioned in the repo.\n5. I’m not sure if Figure 1 is necessary considering Figure 2 gives a better outline.\n6. What are the relationships (r) in the second step in OPE? At first glance, I thought these are relationships in the knowledge base representation, e.g., isA, partOf, etc. Then, it doesn’t make too much sense to compare the relation with the property. In the provided example ‘apple—hungry’, neither of them is a relation. I guess what’s happening here is just the cosine similarity comparison of objects with some concepts, but the word relationship creates some ambiguity there.\n7. Regarding the target properties, I guess they are set manually, not coming from ConceptNet. So, is it the case that the third step in the OPE module essentially binds objects with those target properties? I was not sure if this was the case—it’s not very clear from the text.\n8. I appreciate the effort to consider the risks of actions. This part could benefit from a definition of a risk, i.e., risks that are considered. For instance, it’s not clear whether we consider the risks of injuring the user or the robot itself. The robot policy might injure itself in ways that might be unpredictable with the knowledge base. As such, the phrase “the final policy guarantees risks,” (also unclear) sounds a bit overstatement unless there is a provable guarantee that the system will avoid such interactions. I think what you can claim is a safety mechanism that filters out some of the possibly hazardous everyday interactions.\n9. It’s not clear how the keywords are identified in Sec. 3.2, keyword extraction.\n10. \"ConceptBot also considers a set of relationships R_o retrieved from each detected object in the scene.\" I assume a cosine similarity between keywords and objects are meant, but a set of relationships for each object might be understood as relationships between an object and a keyword, such as, partOf, isA, hasProperty, and so on. What’s considered here is rather keyword-object pairs. Apart from that, this part is quite interesting. Essentially, we try to parse the user intent based on the available objects to the agent, which is, at a high-level, somewhat similar to affordances. Maybe, you could give some more insight on this part.\n11. \"Only those policies for which the evaluators reached at least 95% agreement were used as the reference.\" What do you mean by this? How do we get 95% agreement with three evaluators?\n12. It’s not clear how the evaluation took place. In Sec. 4.3, it’s mentioned that the success is reported as a percentage over 10 runs, but results in Table 2 report percentages such as 84%."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6EoS1OnyTD", "forum": "9fHuFi4Qsu", "replyto": "9fHuFi4Qsu", "signatures": ["ICLR.cc/2026/Conference/Submission12165/Reviewer_6JXH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12165/Reviewer_6JXH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831004822, "cdate": 1761831004822, "tmdate": 1762923118219, "mdate": 1762923118219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a planner that addresses hazardous situations caused by robot actions using a knowledge graph, ConceptNet. After augmenting object information from the graph, LLMs estimate seven predefined properties such as \"fragile\". The graph is also used to infer an implicit user request from a comment, such as \"I'm hungry\"; retrieved knowledge is used for plan reasoning. The experimental results report that the proposed method can avoid hazardous situations much more effectively than the original SayCan model with GPT-4o-mini. The result also reveals that the method performs similarly to two other LLM models, DeepSeek-R1 and Llama-3.3-70b."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Safety in language-guided robot motion generation is clearly an important topic. \n2. The proposed method clearly outperforms SayCan, although it is somewhat outdated, and GPT-4o-mini, its backbone, seems insufficient to ensure safety in this context."}, "weaknesses": {"value": "1. The ambiguous implementation of the NO-KG baseline makes it difficult to understand the necessity of ConceptNet. \nAs recent LLMs can output versatile knowledge without additional resources, or with external knowledge pools through RAG, the necessity of KG for this task should be carefully discussed. At least, it is unclear how the authors demonstrate the advantage of KG in these conditions due to insufficient explanation of the NO-KG condition. A fair comparison, in this reviewer's opinion, should involve giving LLMs a prompt in the URP/OPE process to generate knowledge without KG, or using standard RAG.\n\n2. SayCan with GPT-4o-mini (L397) seems like a weak baseline. \nAlthough it is desirable for a robot to work with lightweight LLMs, the authors should provide a stronger baseline to demonstrate the significance of the proposed method. What performance can be obtained when using a more robust LLM that outputs knowledge instead of relying on ConceptNet?\n\n3. Weak technical novelty significance.\nThe proposed method is based on standard knowledge retrieval from KG and hand-crafted prompts. A more substantial technical contribution is expected for an ICLR paper, in this reviewer's opinion."}, "questions": {"value": "1. Please provide a detailed explanation for the NO-KG baseline implementation.\n2. Is there any justification for the use of GPT-4o-mini instead of more strong models in this task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zaQLQ36Ixf", "forum": "9fHuFi4Qsu", "replyto": "9fHuFi4Qsu", "signatures": ["ICLR.cc/2026/Conference/Submission12165/Reviewer_jDNW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12165/Reviewer_jDNW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964086559, "cdate": 1761964086559, "tmdate": 1762923117757, "mdate": 1762923117757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors integrate Knowledge Graph (KG) into LLM robotics planner. Attempted task is pick and place operations for robot manipulator."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- Integration of knowledge graph into LLM based planner is a reasonable idea to ground  LLM  planner into common sense and safe trajectories. \n- Empirical comparison to closest (identified by the authors) competing model SayCan are reasonable and convincing."}, "weaknesses": {"value": "- At heart this is a rule-based AI system, where different trained models (LLM) for planning and text and YOLO for visual input are used in a well engineered system. In terms of contribution a more solid emprical evaluation would be needed to obtain new and surprising knowledge about the setup. Preferably it should be something that enables building a new learning -based complete solutions. \n- Where are VLA benchmarks in this? Now YOLO is used as a separate module, how would VLA perform when separate components would be combined into just one model (except KG module of course). \n- Empirical results do not have statistical significance tests included (typically in RL experiments that would require running same experiment  with multiple seeds and reporting mean and standard deviation of the success metric.) Other statistical significance testing methodologies are available if multiple seeds are not directly availabe for your setup."}, "questions": {"value": "- I was expecting to see experiments with physical robots. Have you experiments with physical robotic manipulation task?\n- How you could combine KG with monolithic VLA approach? Could you envision a new system with this methodology?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PL5u2DzPUM", "forum": "9fHuFi4Qsu", "replyto": "9fHuFi4Qsu", "signatures": ["ICLR.cc/2026/Conference/Submission12165/Reviewer_x9kL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12165/Reviewer_x9kL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12165/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992552847, "cdate": 1761992552847, "tmdate": 1762923117273, "mdate": 1762923117273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}