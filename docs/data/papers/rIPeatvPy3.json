{"id": "rIPeatvPy3", "number": 1024, "cdate": 1756828733076, "mdate": 1763732186884, "content": {"title": "The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs", "abstract": "Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling. \nHowever, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. \nTo this end, we present **DIJA**, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt. Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. Notably, our method achieves up to 100\\% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5\\% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of harmful content in the jailbreak prompt. Our findings underscore the urgent need for rethinking safety alignment in this emerging class of language models.", "tldr": "We present DIJA, the first jailbreak framework for diffusion LLMs, exposing unique vulnerabilities from bidirectional modeling and parallel decoding, and achieving state-of-the-art attack success rates.", "keywords": ["Diffusion LLMs", "Safety", "Jailbreak Attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a3eda9ec74308734659ce9c89d92036b70e99bbf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates jailbreak vulnerability in Diffusion-based Large Language Models (dLLMs), arguing that their unique bidirectional architecture is inadequately protected by existing safety alignment methods. To exploit this, the authors propose DIJA, an interleaved, masked jailbreak attack that achieves a high success rate against dLLMs and remains robust when tested against state-of-the-art defenses like Robust Prompt Optimization (RPO)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The paper identifies and explores an emergent safety vulnerability specific to dLLMs."}, "weaknesses": {"value": "## Major \n\n1. **Missing Ablation Study:** The DIJA method is a composite of several ideas: prompt refinement, different masking patterns (block-wise, fine-grained, progressive), and benign separator insertion. The paper fails to isolate the contribution of each component. Without an ablation study, it is impossible to determine whether the high ASR is due to the novel masking patterns, or simply the use of a more powerful Refinement LLM for prompt generation. The current analysis exploring the number of masked tokens is a hyperparameter sweep, not a methodological ablation.\n2. **Analysis of Off-the-shelf Baseline (especially ReNeLLM):** The authors' results show that ReNeLLM (a simple, off-the-shelf attack) is surprisingly competitive, even outperforming DIJA on the LLaDA-Instruct target model on the JailbreakBench dataset (ReNeLLM at 96.0% ASR-e vs. DIJA at 81.0% ASR-e). If a generic attack performs equally or better than the proposed, highly-specialized method, it questions the core claim of DIJA's necessity for attacking dLLMs. The authors must address this discrepancy. Somehow, this is also related to weakness 1-- as we don't really understand what part of the approach is actually contributing to the effectiveness of the attack\n\n3. **Methodological Ambiguity:** The methodology is presented at times in a unnecessarily complex way. Also, the role of the initial prompt 'a' in Algorithm 1 is confusingly presented. While it is the source of the harmful content, its relationship to the final attack prompt is unclear to me: is it a successfull JB? Against what victim model? etc. The paper should clearly state that 'a' is a seed/template for the attack, and the LLM $\\mathcal{L}$ is the attacker LLM that generates the final interleaved attack prompt $p_i$ used against the target dLLM $\\mathcal{D}$. This two-stage process needs clearer articulation.\n\n## Minor \n\n* **Equation (2) Notation:** The variable $\\Phi$ in Equation (2) is not explicitly defined. While context suggests it represents the set of possible masks or masking operations, this must be formally stated for a technical paper.\n* **Overclaiming of Effectiveness:** The claim on page 7 regarding the high ASR being \"rarely observed on autoregressive models\" is an overclaim. Given that several recent, powerful attacks (like GCG variants or certain adaptive attacks) routinely achieve near-100% ASR on state-of-the-art autoregressive models, this statement should be tempered or removed. The true novelty is the method of attack (masking), not necessarily the ASR magnitude.\n* **Clarity and Readability:** Section 3.2.2: The section should not start with \"Specifically,\".Section 3.2.1 Motivation: The section is titled \"Motivation\" but contains minimal motivational text, primarily diving straight into the technical details. \n* **Figure Clarity:** Figures 1, 2, 4, and 6 are unclear and difficult to read, especially in a double-column format. The authors should improve image quality and legend clarity for the final version."}, "questions": {"value": "I suggets  the authors should:\n\n- Implement a full ablation study to quantify the contribution of masking vs. prompt refinement vs. benign separators.\n\n- Explain the ReNeLLM discrepancy, perhaps with a discussion on why dLLMs' bidirectional nature makes them susceptible to both simple and complex attacks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aolSLZLeTk", "forum": "rIPeatvPy3", "replyto": "rIPeatvPy3", "signatures": ["ICLR.cc/2026/Conference/Submission1024/Reviewer_Jo4s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1024/Reviewer_Jo4s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846046256, "cdate": 1761846046256, "tmdate": 1762915657817, "mdate": 1762915657817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical safety vulnerability in diffusion-based large language models (dLLMs) arising from their core architectural features. The authors empirically validate this claim and propose DIJA, an automated jailbreak attack framework that constructs interleaved mask-text prompts to exploit these vulnerabilities: bidirectional modeling forces dLLMs to generate contextually consistent (even harmful) content for masked spans, while parallel decoding limits dynamic filtering of unsafe outputs.  Overall, this work highlights a previously overlooked threat surface in dLLM architectures and underscores the urgent need for rethinking safety alignment for this emerging class of language models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper features clear writing and a well-articulated motivation, making the research gap and significance intuitive to follow.\n- The experimental design is comprehensive and robust, covering multiple representative general-purpose and code-oriented dLLMs, three major jailbreak benchmarks, and direct comparisons with state-of-the-art attack baselines.\n- The paper proactively explores defensive mechanisms to add depth to safety analysis."}, "weaknesses": {"value": "- The DIJA method appears too simple and just relies on a prompt template for generating interleaved mask-text prompts via in-context learning. Additionally, the paper provides no systematic analysis of the diversity of these mask-text adversarial prompts.\n- The practical value of the research is limited due to the nascent stage of dLLM development. At present, dLLMs still suffer from noticeable gaps in training stability and the inference ecosystem, leaving few immediate landing scenarios."}, "questions": {"value": "1. Since the authors use an existing LLM to generate harmful instructions for attacking dLLMs, how can the uncertainty of attack reproducibility be measured? For example, the mask-text prompts generated in multiple runs may exhibit significant differences in terms of structure or the number of mask tokens.\n2. In lines 372-373, the authors claim: \"This is because our method exposes the harmful intent in the prompt directly.\" As far as I know, role-playing attacks (such as AIM in the baseline) also incorporate the complete harmful intent into the prompt. How do you explain this inconsistency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bvrXvLmRfc", "forum": "rIPeatvPy3", "replyto": "rIPeatvPy3", "signatures": ["ICLR.cc/2026/Conference/Submission1024/Reviewer_Jm6J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1024/Reviewer_Jm6J"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908839854, "cdate": 1761908839854, "tmdate": 1762915657633, "mdate": 1762915657633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a simple but effective decoding-time attack for circumventing the safety alignment of diffusion Large Language Models (dLLMs) called DiJA. The design of DiJA is simple: given a harmful prompt, first construct a template that contains the structure of an affirmative response with mask tokens at locations where generating harmful content is desired, and then leverage the dLLMs parallel decoding ability to perform infilling on these masked locations to generate the harmful content. The templates are constructed via a simple in-context learning procedure. The paper finds that DiJA is effective at generating harmful content on multiple dLLMs and safety benchmarks. A simple defense is proposed, and is shown to help improve robustness to DiJA attacks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The DiJA attack reveals a critical vulnerability in dLLMs and has strong implications for safely open-sourcing dLLMs. The attack appears effective against multiple dLLMs on multiple standard safety benchmarks, is easy to implement and computationally inexpensive.\n2. The in-context learning approach to generating the DiJA infilling templates is well-principled and clearly explained.\n3. An initial attempt is made at a training-time defense, which shows that robustness to the DiJA vulnerability can be significantly improved.\n4. An ablation study is provided on the number of mask tokens."}, "weaknesses": {"value": "1. (Minor) The first example provided for interleaved mask-text prompting in Figure 1, editing/rewriting, is a bit weak. It suggests an application of fixing small typos by resampling from the model, but typos can be trivially fixed by simple spell checkers after decoding. A stronger-motivated example for using a dLLM could be paraphrasing intermediate sentences/longer phrases.\n2. (Major) The proposed DiJA attack needs to be better contextualized within existing similar decoding exploits for autoregressive models, and the assumptions about when the attack can be applied should be more clearly stated. Specifically, the core of DiJA is essentially the prefilling attack ([1, 2]) generalized to dLLMs — as both exploit the model’s desire to preserve coherence and rely on the assumption that the user can alter the assistant response — but this is not mentioned in the work. Its also worth noting that [2] also took an in-context learning approach when constructing harmful prefills. Regarding the assumption about altering the assistant response, it is trivially satisfied by any open-source model, but for the closed-source setting the ability to perform infilling on the assistant response side needs to be explicitly provided as a feature (otherwise, DiJA can be easily guarded against by just disabling infilling). For prefilling, a good example of how the latter matters is the OpenAI API does not support prefilling (and hence prefilling attacks are not possible), but the Claude API does ([3]) (and hence was shown in [2] to be vulnerable to prefilling attacks). Are there similar real-world examples for closed-source dLLMs? Please provide some more discussion on comparing and contrasting DiJA to the aforementioned prior work and on the assumptions of the threat model.\n4. (Major) Similarly, the proposed Refusal-Aware Denoising Alignment defense should be contextualized within existing similar training-time interventions, such as [5].\n5. (Minor) Notational discrepancies:\n- Line 154 shows [MASK] being a part of the vocabulary already, such that the union in line 174 is not necessary.\n- Equation 5 is essentially the same as equation 4, just by renaming y to x. The only difference is that the initial x^K can include non-mask tokens. Perhaps the notation here can be unified a bit.\n6.  (Minor) Terminology\n- Line 199 (and elsewhere) mentions “original jailbreak prompt” — to my knowledge, a harmful behavior prompt without any actual jailbreak applied (e.g., those discussed in Section 2) is not referred to as a “jailbreak prompt,” but rather just the original (harmful) prompt.\n7. (Minor) Line 318 refers to “previous studies” for the judging prompt: please provide explicit citations of what these previous studies are (e.g., this template was used in [4]).\n8. (Minor) Line 363-364: Please provide some evidence for the robustness comparison to SOTA autoregressive models (e.g., citations, specific numbers).\n9. (Minor) Figure 4: I would suggest using a different icon than the OpenAI logo to represent the different dLLMs, otherwise it visually suggests these outputs were generated by GPT.\n\n\nReferences:\n\n[1] Vega, Jason, et al. \"Bypassing the safety training of open-source llms with priming attacks.\" arXiv preprint arXiv:2312.12321 (2023).\n\n[2] Andriushchenko, Maksym, Francesco Croce, and Nicolas Flammarion. \"Jailbreaking leading safety-aligned llms with simple adaptive attacks.\" arXiv preprint arXiv:2404.02151 (2024).\n\n[3] https://anthropic.mintlify.app/en/docs/build-with-claude/prompt-engineering/prefill-claudes-response\n\n[4] Qi, Xiangyu, et al. \"Fine-tuning aligned language models compromises safety, even when users do not intend to!.\" arXiv preprint arXiv:2310.03693 (2023).\n\n[5] Qi, Xiangyu, et al. \"Safety alignment should be made more than just a few tokens deep.\" arXiv preprint arXiv:2406.05946 (2024)."}, "questions": {"value": "1. Line 77-79: could you cite some specific examples of defenses for left-to-right models and why they cannot work for dLLMs? Some of the defenses mentioned in section 2 could arguably be applied to dLLMs as well (e.g., perplexity filter, SmoothLLM), as they do not rely on the method of decoding (left-to-right vs. parallel).\n2. For the Refusal-Aware Denoising Alignment defense, how exactly are the refusals generated? Are the refusals always generic or are they prompt-dependent (e.g., providing reasons why the request was refused instead of just refusing)? Also, suppose “I’m sorry, I can’t help with that.” is tokenized to M tokens — how would this fit into a masked section with more or less than M tokens (e.g., for a section with more than M tokens, are the remaining tokens filled with additional [MASK] tokens)? Or are the lengths of the masked sections chosen to be equal to the tokenized lengths of the refusals? Finally, are a variety of refusal lengths included in the fine-tuning dataset to help generalize to different mask lengths?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ShBdnrEEKr", "forum": "rIPeatvPy3", "replyto": "rIPeatvPy3", "signatures": ["ICLR.cc/2026/Conference/Submission1024/Reviewer_zmbS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1024/Reviewer_zmbS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962853840, "cdate": 1761962853840, "tmdate": 1762915657472, "mdate": 1762915657472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies safety vulnerabilities unique to diffusion-based LLMs (dLLMs) that perform bidirectional, parallel masked decoding. The authors introduce DIJA, a jailbreak framework that converts vanilla harmful prompts into interleaved text–mask prompts, exploiting the obligation of dLLMs to fill masked spans coherently while leaving surrounding (unmasked) harmful context fixed. Because decoding is parallel, common on-the-fly safety interventions fail, leading to harmful completions even in alignment-tuned models. Experiments across multiple dLLMs (LLaDA, Dream, MMaDA, and code-oriented variants) and three benchmarks (HarmBench, JailbreakBench, StrongREJECT) show very high success rates; the abstract reports up to 100% ASR-k on Dream-Instruct and large gains over prior attacks such as ReNeLLM. The paper also proposes a preliminary refusal-aware denoising alignment that substantially reduces ASR with modest utility cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "•Clear architectural insight. The paper crisply articulates why bidirectional infilling plus parallel decoding weakens standard guardrails, and formalizes the forcing effect created by fixing unmasked tokens while infilling masked spans.  \n•Simple, scalable attack. DIJA uses few-shot prompt construction with masking-pattern and separator diversification; Algorithm 1 and Section 3 detail an automated pipeline that doesn’t hide harmful intent yet reliably elicits unsafe content.  \n•Comprehensive evaluation. Results across three standard jailbreak suites show consistent, often dramatic improvements over baselines (Tables 1–3; code-model results in Tables 6–8). The paper also examines defense robustness (Self-reminder, RPO) and performs informative ablations on generation/Mask length (Figures 5–7). \n•Initial mitigation path. The refusal-aware denoising alignment substantially reduces ASR on DIJA inputs with modest impact on general tasks (Table 4–5), suggesting practical, architecture-aware safety training is feasible."}, "weaknesses": {"value": "•Evaluator dependence & metric triangulation. While the paper uses both keyword-based and evaluator-based metrics (including StrongREJECT), a large portion of the story still relies on LLM judges and prompts (e.g., GPT-4o for HS, DIJA* for construction). Some cross-checking with human raters or multiple independent evaluators would further bolster soundness.  \n•Interface assumptions for mask control. The attack presumes the user can inject mask tokens (e.g., [MASK] or <mask:N>) directly. Many practical deployments may not expose raw mask-infilling controls to end users, or may preprocess/normalize such tokens. Clarifying the assumed I/O interface per model (and what happens under more restricted UIs) would help generalize the threat model. (Figure 1 illustrates capabilities, but deployment exposure varies.)  \n•Defense generalization risk. The proposed alignment targets a specific attack pattern (interleaved mask–text). Although Table 4 is promising, the paper also notes open questions about generalization to unseen or modified patterns and potential trade-offs on legitimate masked-editing use cases. A broader evaluation of adaptive adversaries would strengthen the mitigation claim.  \n•Safety examples in the text. Figure 4 contains vivid harmful outputs. While necessary for evidence, trimming or redacting some specifics could reduce dual-use risk in the camera-ready."}, "questions": {"value": "1.Model interfaces: For each evaluated dLLM, what exact token or API is used for masks (e.g., [MASK] vs <mask:N>)? Are inputs sanitized by the serving stack? A short table mapping model ↔ mask syntax/entrypoint would clarify portability of DIJA.  \n2.Ablations on UI constraints: If masks are only allowed in delimited slots (e.g., structured templates) or subject to server-side stripping, how does DIJA perform? Can the attack be adapted with fewer/shorter masked spans while maintaining high ASR (beyond Figure 7’s token-count sweep)?  \n3.Evaluator sensitivity: Did you compare GPT-4o HS to an open evaluator (e.g., WildGuard) or limited human spot-checks to estimate judge variance? Even a small double-annotation subset would help calibrate effect sizes.  \n4.Defense coverage: Does refusal-aware alignment spill over to benign masked-editing workflows (Figure 1) by over-refusing? Could you report a small suite of benign infilling tasks before/after alignment?  \n5.Threat model clarity: Section 4 briefly treats “safety-aligned” as SFT with safety data. Please enumerate which victim models had what kind of alignment (SFT vs RLHF vs preference optimization) to better contextualize DIJA’s gains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ckpIJB4yjj", "forum": "rIPeatvPy3", "replyto": "rIPeatvPy3", "signatures": ["ICLR.cc/2026/Conference/Submission1024/Reviewer_wJxD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1024/Reviewer_wJxD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762186552628, "cdate": 1762186552628, "tmdate": 1762915657301, "mdate": 1762915657301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}