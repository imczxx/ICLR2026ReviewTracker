{"id": "IfyaPN8rwX", "number": 9359, "cdate": 1758119941284, "mdate": 1763713147400, "content": {"title": "Beyond Next-Token Alignment: Distilling Multimodal Large Language Models via Token Interactions", "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive cross-modal understanding capabilities, yet their substantial model size poses significant challenges for widespread deployment. Knowledge distillation (KD) presents a promising solution for compressing these large-scale MLLMs. However, existing KD methods primarily rely on static next-token alignment, neglecting to model the dynamic token interactions, which embed essential capabilities for multimodal understanding and generation. To this end, we introduce **Align-TI**, a novel KD framework designed from the perspective of **T**oken **I**nteractions. Our approach is motivated by the insight that MLLMs rely on two primary interaction types: vision-instruction token interactions to extract instruction-relevant visual information, and intra-response token interactions for dynamic reasoning and coherent generation. Accordingly, Align-TI introduces two components: Instruction-aware Vision Alignment (IVA) and Transition Probability Alignment (TPA). IVA enables the student model to imitate the teacher's ability to extract instruction-relevant visual information by aligning on salient visual regions. TPA captures the teacher's dynamic generative logic by aligning the sequential token-to-token transition probabilities. Extensive experiments on standard multimodal benchmarks demonstrate the superiority of Align-TI. Notably, our approach achieves 3.7% relative improvement over direct supervised fine-tuning across multiple benchmarks. Moreover, our distilled Align-TI-2B even outperforms LLaVA-1.5-7B (a much larger MLLM) by 7.0%, establishing a new state-of-the-art distillation framework for training parameter-efficient MLLMs.", "tldr": "A novel MLLM knowledge distillation framework designed from the perspective of token interactions.", "keywords": ["MLLM", "Token Alignment", "Token Interaction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e261ff58cb9e23c14de296edd150e9297ce056d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Align-TI, a token-level knowledge distillation framework for compressing multimodal large language models (MLLMs). It emphasizes token interactions through two components: Instruction-aware Vision Alignment (IVA), aligning visual tokens with instruction-relevant regions, and Transition Probability Alignment (TPA), transferring token-to-token generation dynamics. Experiments show Align-TI effectively distills MLLMs, offering a fine-grained approach to multimodal knowledge transfer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper carefully analyzes the current research landscape of vision–instruction token interactions and intra-response token interactions, and on that basis proposes Instruction-aware Vision Alignment (IVA) and Transition Probability Alignment (TPA) to address these issues.\n2. Experimental results demonstrate the effectiveness of Align-TI, showing that with fewer parameters, it still surpasses larger models like LLaVA-1.5-7B by 7.0%, offering a new perspective for parameter-efficient MLLM distillation.\n3. Overall, the paper is well-written and easy to understand."}, "weaknesses": {"value": "1. **Motivation:** The motivation of this paper is based on the premise that the current “static next-token alignment” has fundamental limitations, whereas “dynamic token interactions” capture critical capabilities for MLLM understanding and generation. However, this premise lacks both conceptual clarity and empirical support. First, the paper does not provide operational definitions for “static” and “dynamic” making it difficult to understand why existing methods (e.g., Vanilla KD) are classified as “static”. Second, the paper merely claims that static alignment is insufficient without offering concrete evidence or task-based examples.\n2. **Novelty:** 1) \"Align-KD: Distilling Cross-Modal Alignment Knowledge for Mobile Vision-Language Models\" has already proposed enhancing distillation via cross-modal alignment and applied it before the input is fed into the model. Your proposed IVA module is quite similar to Align-KD, by leveraging IRS, you apply this mechanism across different layers. Could you clarify the core design differences between the two? 2) The Instruction-Relevant Score (IRS), which serves as the basis for all subsequent experiments, lacks sufficient justification. 3) TPA increases computational complexity and it is unclear whether this increase is justified by the performance gains.\n3. **Experiments & Results：** 1) Unclear ablation explanation: In Table3, IVA alone brings a 0.8% improvement, but when introduced after TPA, the gain drops to only 0.3%. Do the two modules have overlapping or conflicting functionalities? 2) Why does Table 4 not include a comparison with the distilled model? \n4. **Format:** 1) The “6% / 4.6% improvement over Vanilla KD” shown in Figure 1 cannot be clearly verified in Table 1, and it is unclear which method is referred to as Vanilla KD. 2) Line 360: “GKD” is unclear—please specify whose abbreviation it is."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iSYlpfHng1", "forum": "IfyaPN8rwX", "replyto": "IfyaPN8rwX", "signatures": ["ICLR.cc/2026/Conference/Submission9359/Reviewer_dQg7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9359/Reviewer_dQg7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761556296052, "cdate": 1761556296052, "tmdate": 1762920981908, "mdate": 1762920981908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to compress the model size of MLLMs, using knowledge distillation techniques. Beyond traditional KD that focuses on next-token alignment, authors turn to achieve KD via token interactions and propose Align-TI, which involves two main components on vision and transition probability alignment. This paper explicitly discusses vision-instruction token interactions and intra-response token interactions, rather than directly aligning the next-token probabilities. \nThe vision-instruction token is weighted by aggregating the attention weights. The transition probability alignment is achieved by sampling from the student model and aligning the transition probabilities between teacher and student. Extensive experiments on various benchmarks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of aligning token interactions instead of next-token probabilities is straightforward and motivated with illustrative examples, as shown in Figure 2, making it easy to understand the intuition behind the method.\n2. The design of the proposed method is detailed and easy to follow.\n2. Extensive experiments are conducted, demonstrating the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The theoretical analysis is highlighted as a contribution in the introduction, but it is not well elaborated and limited in the main text, which seems to contribute less to the method. \n1. This paper only focuses on the projector-based multimodal language models, ignoring other research lines, like the unified multimodal model. \n2. No code release is mentioned, which hinders reproducibility and further research on related topics. The parallelized calculation seems non-trivial to implement. And there are many hyper-parameters to sample tokens. \n3. There are some differences between Equations 2 and 5 on the vanilla KD loss, which may confuse readers. Please make it consistent.\n4. The performance of the teacher model is not reported in the main text (I found it in the appendix, should be transferred to the main text) for a comprehensive comparison. Since the teacher model is already powerful, it is important to see how much performance drop occurs after distillation rather than comparing with other weaker baselines.\n5. The efficiency analysis seems redundant and unfair, since a smaller model is naturally more efficient than larger models. Do you want to highlight the efficiency of your distillation process (e.g., Parallelized Calculation) compared to the models of same size? If yes, please clarify it."}, "questions": {"value": "Vision-instruction token interaction alignment is quite straightforward. However, the transition probability alignment seems a little bit tricky. It seems you actually want to align next two tokens; however, at the k-th step, you have not determined $y_k$, so you sample $y_k$ from the student model, and then align the transition probability from $y_k$ to $y_{k+1}$ between teacher and student. So I have the following questions:\n1. Can you implement a direct alignment of the next two tokens, i.e., align $P(y_{k+1}|y_k, x)$ between teacher and student, where $y_k$ and $y_{k+1}$ are both from the ground-truth response? If yes, how does it perform compared to your proposed method?\n2. Can you sample from the teacher model instead of student model to compute the transition probability? How does it perform?\n3. Can you cancel the vanilla KD loss, only using the proposed two alignment losses? How does it perform?\n\nFor the experiments, I have the following questions need to be clarified:\n1. In Tables 2 and 3, do you only report the performance on adapting different methods to LLMs, not MLLMs? If yes, how do you implement IVA (the vision token alignment) on LLMs? Can you clarify it?\n2. Following up on the above question, in Table 3, why does Align-TI (w/ IVA and TPA) have the same performance as in Table 2? Table 3 should be an ablation study for MLLMs. In my understanding, it should have the same performance as Table 1 (the overall performance comparison in MLLM setting). \n3. In Table 6, selecting layer 0 also achieves a good performance, which seems to contradict your claim. This can help understand the importance of IRS. \n4. Also, the same question, why is the performance in Tables 6, 7, and 8 different from Tables 2 and 3? Are they all conducted on MLLMs or LLMs? Please clarify it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NeZv0yBrqS", "forum": "IfyaPN8rwX", "replyto": "IfyaPN8rwX", "signatures": ["ICLR.cc/2026/Conference/Submission9359/Reviewer_fYcx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9359/Reviewer_fYcx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728121832, "cdate": 1761728121832, "tmdate": 1762920981538, "mdate": 1762920981538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new knowledge distillation framework for multimodal large language models (MLLMs). Traditional distillation methods align student and teacher outputs at the next-token level, but they overlook dynamic token interactions essential for multimodal understanding and coherent generation.\nAlign-TI models distillation from the perspective of token interactions and introduces two components, i.e., Instruction-aware Vision Alignment (IVA) and Transition Probability Alignment (TPA). Experiments across standard benchmarks show that Align-TI significantly outperforms both vanilla KD and larger MLLMs (e.g., LLaVA-1.5-7B) while offering better computational efficiency. Ablation and scaling studies confirm that IVA and TPA contribute complementary benefits, making Align-TI a robust and efficient approach for distilling high-performing small-scale MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The shift from static next-token alignment to token interaction modeling (IVA and TPA) provides a theoretically grounded and empirically validated innovation in knowledge distillation.\n2. Extensive experiments and ablations across multiple benchmarks demonstrate robustness and scalability, including efficiency analysis and architectural generalization.\n3. The proposed TPA component explicitly addresses train-test distribution discrepancies, a critical and underexplored issue in multimodal distillation."}, "weaknesses": {"value": "1. Essentially, MLLMs already perform rich cross-modal and intra-modal token interactions through their attention layers. From this perspective, Align-TI appears to be an incremental refinement of existing attention mechanisms, i.e., reweighting visual focus (IVA) and regularizing output dynamics (TPA). It would be valuable to see further discussion or empirical evidence clarifying how Align-TI captures interactions beyond what standard attention already provides.\n2. The evaluation primarily uses image-text benchmarks. It remains unclear how Align-TI generalizes to other modalities or other multimodal tasks, such as the video-based understanding benchmark, which would strengthen claims of broad multimodal applicability."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wRJcFj59qh", "forum": "IfyaPN8rwX", "replyto": "IfyaPN8rwX", "signatures": ["ICLR.cc/2026/Conference/Submission9359/Reviewer_Kx6f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9359/Reviewer_Kx6f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838320543, "cdate": 1761838320543, "tmdate": 1762920981210, "mdate": 1762920981210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Align-TI, a knowledge distillation framework for MLLMs. By introducing Instruction-aware Vision Alignment (IVA) and Transition Probability Alignment (TPA), it aligns a student model's ability to extract instruction-relevant visual information and dynamic generation logic with a teacher's. The proposed method achieves state-of-the-art results across multiple benchmarks, proving effective for enhancing smaller MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Extensive experiments demonstrate state-of-the-art performance, while thorough ablation studies validate the method's generalization across different MLLM architectures.\n2.The insight is interesting. The work provides an interesting perspective by addressing the often-overlooked problem of insufficient visual token distillation in existing MLLM knowledge distillation methods.\n3.The proposed IVA and TPA components are novel and effective, successfully aligning the student’s visual attention and generation logic with the teacher’s.\n4.The paper is well-organized and easy to follow."}, "weaknesses": {"value": "1.A potential direction for future work could be applying this distillation approach during the pre-training stage. It would be insightful to discuss how this might enhance the vision-language alignment.\n2.The paper could be further strengthened by a more detailed analysis of the individual loss components. For instance, an ablation study quantifying the specific impact of each term ($L_{sft}, L_{kd}, L_{iva}, L_{tpa}$) would offer a clearer understanding of their respective contributions to the final performance."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PlwPRGBCFc", "forum": "IfyaPN8rwX", "replyto": "IfyaPN8rwX", "signatures": ["ICLR.cc/2026/Conference/Submission9359/Reviewer_SRQm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9359/Reviewer_SRQm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918698839, "cdate": 1761918698839, "tmdate": 1762920980682, "mdate": 1762920980682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Align-TI, a new knowledge distillation (KD) framework for Multimodal Large Language Models (MLLMs). The framework is composed of two new components: (1) Instruction-aware Vision Alignment (IVA) and (2) Transition Probability Alignment (TPA)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The insight for IVA is strong. Recognizing that many visual tokens are redundant and that distillation should focus on instruction-salient regions is a valuable contribution.\n\nThe paper provides a principled exploration of different transformer layers for visual-text attention distillation. The IRS metric is a good method for selecting the most relevant layer, rather than relying on manual design.\n\nThe TPA component's objective is practical. Aligning the full vocabulary's transition matrix is infeasible, so the insight to focus only on a sampled, high-probability set of tokens is a sensible and efficient approximation."}, "weaknesses": {"value": "- The two proposed modules, IVA and TPA, feel separate and not well-integrated. IVA is a VLM-specific technique for aligning visual-instruction interactions. TPA, however, is a general-purpose LLM distillation method for text generation. The paper does not convincingly unify them into a single coherent KD framework.\n\n- The paper is difficult for a reader unfamiliar with the field to assess. The related work section is in the appendix, so the main text lacks a necessary discussion of previous work. Without this, it is hard to judge the novelty of IVA against other attention distillation methods or TPA against other sampling-based KD strategies.\n\n- The objective for TPA is unclear. The notation y(k+1) conditions on y(k) misleadingly suggests a 2-gram (bigram) model, which seems to contradict the complex attention mechanisms of an LLM. This makes the core objective difficult to understand.\n\n- There appears to be a misalignment between the goal of TPA (aligning transition matrices) and its implementation (a sampling algorithm). This ambiguity makes it hard to justify the novelty. The authors should clarify if the sampling strategy itself is new for LLM KD.\n\n- The performance claims are not fully supported. Ablation: The performance gain from IVA is not significant. Table 3 shows adding IVA on top of TPA only provides a minimal 0.3-point gain (66.4 to 66.7), suggesting TPA does all the heavy lifting. Efficiency: TPA adds significant training overhead (509 / 355, about 1.43x). A critical baseline is missing: training Vanilla KD for the same amount of time as TPA (e.g., on 1.43x more data). It is possible that Vanilla KD could match the performance of TPA if given the same computational budget.\n\nThe paper proposes two methods that seem disjointed: one for VLM attention and one for general LLM generation. The novelty of the TPA component is difficult to justify due to a confusing formulation and a misalignment between its objective and its implementation. Finally, the empirical gains from the main IVA component are not significant, and the overall framework lacks a fair comparison against an efficiency-equivalent baseline."}, "questions": {"value": "NA. Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gVL9w7JXW5", "forum": "IfyaPN8rwX", "replyto": "IfyaPN8rwX", "signatures": ["ICLR.cc/2026/Conference/Submission9359/Reviewer_vd3n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9359/Reviewer_vd3n"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986350480, "cdate": 1761986350480, "tmdate": 1762920980061, "mdate": 1762920980061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Comment by Authors"}, "comment": {"value": "We sincerely thank all reviewers for the time and effort they have invested in the review process. \n\nWe are greatly encouraged by the reviewers' positive feedback. Reviewers Kx6f, SRQm and vd3n acknowledged the **novelty and insight** of our approach, with reviewers Kx6f and SRQm specifically pointing out that it **addresses an underexplored issue in multimodal distillation**. The **effectiveness and solid results** of our method were widely recognized by reviewers dQg7, fYcx, Kx6f and SRQm, who also commended our **extensive experiments** (fYcx, Kx6f, SRQm). Additionally, several reviewers (dQg7, fYcx, SRQm) found the paper to be **well-organized and easy to follow.**\n\nMeanwhile, based on the reviewers' valuable feedback and suggestions, we have revised our manuscript. The main changes are summarized below:  \n\n- Added Section 3.3 to introduce the overall objective for Align-TI (Lines 318-323).\n- Moved the table describing teacher performance into the main text for better accessibility (Lines 337-341).\n- Added a citation for GKD (Line 360).\n- Included the training efficiency of TPA in Table 6 and mentioned it in the text (Lines 374, 397-399).\n- Revised Section 3.4 to incorporate a new IVA visualization analysis (Lines 455-475).\n- Added a new section on Limitations and Future Work (Lines 531-537).\n- Included a Reproducibility Statement (Lines 540-556).\n- Added Appendix E, which contains additional experiments on the details of Figure 1, a loss contribution analysis and a performance comparison between the teacher and student models (Lines 1162-1216).\n\nWe have uploaded a revised version of our manuscript with all changes **highlighted in blue**. We hope our revisions and responses adequately address the reviewers' concerns and we are more than happy to engage in any further discussion."}}, "id": "tXbGNfyB7c", "forum": "IfyaPN8rwX", "replyto": "IfyaPN8rwX", "signatures": ["ICLR.cc/2026/Conference/Submission9359/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9359/Authors"], "number": 16, "invitations": ["ICLR.cc/2026/Conference/Submission9359/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763732308373, "cdate": 1763732308373, "tmdate": 1763732465754, "mdate": 1763732465754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}