{"id": "75ce9hnKne", "number": 20499, "cdate": 1758306842116, "mdate": 1763653679454, "content": {"title": "When Bias Helps Learning: Bridging Initial Prejudice and Trainability", "abstract": "Understanding the statistical properties of deep neural networks (DNNs) at initialization is crucial for elucidating both their trainability and the intrinsic architectural biases they encode prior to data exposure. Mean-field (MF) analyses have demonstrated that the parameter distribution in randomly initialized networks dictates whether gradients vanish or explode. Recent work has shown that untrained DNNs exhibit an initial-guessing bias (IGB), in which large regions of the input space are assigned to a single class. \nIn this work, we provide a theoretical proof linking IGB to MF analyses, establishing that a network’s predisposition toward specific classes is intrinsically tied to the conditions for efficient learning. This connection leads to a counterintuitive conclusion: the initialization that optimizes trainability is systematically biased rather than neutral. We validate our theory through experiments across multiple architectures and datasets.", "tldr": "We prove theoretically that the optimal trainable state is necessarily biased in a wide range of models.", "keywords": ["Trainability", "initial guessing bias", "mean field regime", "phase diagrams"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2bb0525829a8fc7614601620a6e05788a637af89.pdf", "supplementary_material": "/attachment/75626e09e978ea4e6b4d10d504e83e848e58bb6b.zip"}, "replies": [{"content": {"summary": {"value": "Develops a theoretical link between mean-field (MF) trainability and initial guessing bias (IGB) at random initialization. Shows an equivalence between MF quantities and IGB statistics, argues that the edge of chaos corresponds to a state of transient deep prejudice that enables fast learning, and provides supporting experiments on MLPs, a simplified ViT, and a large ImageNet-pretrained ViT fine-tuned on CIFAR-100."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear statement of the MF-IGB connection, including a formal mapping between MF covariances and IGB drift ratio, and phase-diagram reinterpretations for bounded and unbounded activations.\n- Reproducibility note and code availability claim.\n- Empirical plus theoretical curves illustrating $c^{(l)}\\rightarrow$1 for ReLU, with different rates in ordered vs. chaotic regimes."}, "weaknesses": {"value": "- Empirical scope is limited relative to broad claims. Experiments focus on binarized Fashion-MNIST, binarized CIFAR-10, CIFAR-10 with simplified MLP/ViT, and a single large ImageNet-pretrained ViT on CIFAR-100.\n\n- Assumptions narrow external validity. Core analysis relies on the infinite-width mean-field regime and an i.i.d. Gaussian input model in the IGB framework.\n\n- Fairness framing without fairness evaluation. The introduction and contributions connect learnability and fairness from initialization onward, but the experiments report only accuracy and maximum class fractions, with no group-attribute metrics.\n\n- Pretraining-bias angle is underdeveloped. For the large ViT, weights are uniformly rescaled to traverse phases, but there is no analysis disentangling ImageNet pretraining priors from weight-scaling dynamics.\n\n- Clarity/calibration: extrapolating MF to practice. Theoretical mapping claims that “best trainability” corresponds to a state of transient deep prejudice (Proposition 4.1), yet validation remains on toy or simplified settings."}, "questions": {"value": "- Representativeness of experiments. Can the empirical section include standard ImageNet-scale or at least non-binarized, multi-class tasks with production architectures (with normalization and residuals) to assess whether the MF–IGB link and the “transient deep prejudice” claim persist beyond toy settings?\n\n- Input-distribution assumptions. How sensitive are the theoretical conclusions to deviating from i.i.d. Gaussian inputs, for example, to real image statistics or correlated features? Any finite-width theory or experiments targeted at non-Gaussian inputs?\n\n- Fairness measurements. Since the paper motivates fairness, could group metrics on a dataset with sensitive attributes be provided to test whether initial “transient deep prejudice” impacts disparities during early training and at convergence?\n\n- Pretraining biases. For the large ViT, is it possible to disentangle the role of ImageNet pretraining from the weight-scaling operation, and measure whether class-prior skew or spurious correlations from pretraining amplify or dampen IGB on the target data?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "The paper states that they are finding related work with LLMs, but is that allowed?\nAlso, the entire paper appears to be LLM-written. Now I am not certain if they simply rephrased every human-written paragraph using LLMs or made LLMs generate the entire paper."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EgG1DX3WbM", "forum": "75ce9hnKne", "replyto": "75ce9hnKne", "signatures": ["ICLR.cc/2026/Conference/Submission20499/Reviewer_YDAS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20499/Reviewer_YDAS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949601365, "cdate": 1761949601365, "tmdate": 1762933930065, "mdate": 1762933930065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the relationship between initialization bias and trainability of DNNs, reconciling two theoretical perspectives:\n- Mean-Field (MF) theory — which studies gradient propagation and identifies the “edge of chaos” (EOC) as the optimal initialization boundary for stable training;\n- Initial Guessing Bias (IGB) framework — which describes how untrained networks can exhibit predictive bias (favoring one class) even before training.\n\nThe authors establish a formal equivalence between the MF and IGB frameworks, showing that the trainability boundary in MF corresponds to a biased initialization state in IGB. Contrary to the intuitive belief that the most trainable initializations should be neutral, they demonstrate theoretically and empirically that the optimal initialization is systematically biased rather than unbiased — a state they call transient deep prejudice."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Theoretical contribution: The paper establishes a clean mathematical connection between two previously distinct frameworks (MF and IGB), enriching both perspectives.\n2. Insight: It introduces a novel idea: bias at initialization can improve trainability; The new “prejudice-neutrality” phase view offers an intuitive explanation for initialization effects, linking bias to the dynamical stability of gradient flow."}, "weaknesses": {"value": "1. The theory is derived in the infinite-width limit and validated on small- to mid-scale settings. Its applicability to practical, large-scale deep networks (e.g., transformers with normalization and attention) is not demonstrated. Additionally, empirical evaluation focuses on synthetic and small vision datasets. It is of readers' interest to learn results on simple language tasks. \n2. Ambiguous practical relevance: While the “transient bias” insight is conceptually interesting, there is no clear recipe for practitioners (e.g., how to initialize weights to achieve the right level of prejudice). It would be helpful to add some executable takeaways.\n3. It would be helpful to introduce and compare with alternative trainability-enhancing initializations (e.g., orthogonal, LSUV, scaled ReLU, or NTK-based initializations)."}, "questions": {"value": "There can be some terminological confusion: It would be helpful to clarify that “bias,” “prejudice,” and “neutrality” refer to statistical asymmetry / symmetry in prediction space. Otherwise, readers may think about fairness or ethical bias."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jFJHcHLkRD", "forum": "75ce9hnKne", "replyto": "75ce9hnKne", "signatures": ["ICLR.cc/2026/Conference/Submission20499/Reviewer_h1rA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20499/Reviewer_h1rA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964566424, "cdate": 1761964566424, "tmdate": 1762933929674, "mdate": 1762933929674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper links two views of randomly initialized networks: initial guessing bias, which is when untrained models over-predict one class, with the mean field theory describing trainability at initialisation. The core result is that there is a mapping between both views, such that the same initialisation that can yield a good gradient flow also produces the most biases starting predictions. The paper extends this across architectural choices, including an experiment to perturb a pre-trained ViT to demonstrate these effects."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Clean conceptual claim that ties IGB statistics with network trainability\n2. Clear testable claim forhow models which start with maximal bias learn fastest\n3. Robust checks beyond toy MLPs to ViTs, both training and perturbing them"}, "weaknesses": {"value": "1. The infinite width setting is somewhat limiting on the theory side\n2. It's unclear how much the choice of norms matters or whether this result is idiosyncratic\n3. The pooling for the 2-dimensional case is similarly limited"}, "questions": {"value": "1. Is it possible to link the theory to regimes beyond the infinite width setup? \n2. How does this theory work under harder settings e.g. the vanilla MLPs are not the strongest baselines for performance?\n3. Is it possible to measure bias level in practice and across training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "V3gwS9pncf", "forum": "75ce9hnKne", "replyto": "75ce9hnKne", "signatures": ["ICLR.cc/2026/Conference/Submission20499/Reviewer_cZgs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20499/Reviewer_cZgs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985501113, "cdate": 1761985501113, "tmdate": 1762933929311, "mdate": 1762933929311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies initial guessing bias (IGB)—when a network exhibits a bias towards certain classes. It mathematically links IGB to mean-field theory of wide networks, thus connecting learnability properties to IGB. Using this, the authors show that initializing networks to be optimized for trainability and have stable gradients also yields class bias at initialization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Understanding how and why neural networks are biased is an important topic for fairness in AI systems. This paper considers the impact of weight initialization, independent of data, which could help us understand how learning dynamics affect bias. By connecting IGB to established mean-field theory of wide networks, it helps build a framework for this. The authors support their theoretical claims with experiments in several architectures and datasets."}, "weaknesses": {"value": "I found the paper difficult to follow and understand. I think it could have more strongly motivated why the link to MF theory is an important one to make and why the contribution made is important. It was also unclear to me at times what was background on previous work versus a novel contribution of the paper.\n\n“Prejudice” is a more loaded term than “bias” and I find the use of it here somewhat inappropriate for what is being referred to. Bias is used to refer to behavior a network might be more prone to do (e.g., inductive bias) and I think would be more apt.\n\nThe paper briefly mentions it, but by assuming the infinite-width limit, the paper focuses on the so-called lazy learning regime. While the lazy learning regime may be valuable to study, it seems like a limitation for a study motivated by fairness to not focus on the rich feature learning regime optimized for in practice. Furthermore, there is substantial literature on feature learning that discusses how different initializations impact learning and representations, which should probably be reviewed and discussed more in this paper.\n\nI think the contribution of the paper is limited. In particular, the main finding concerns how different initializations associated with trainability also reflect different bias. However, although they show that stable initializations are initially biased, this effect goes away with training. It’s unclear whether this specific initialization effect would persist in other settings and actually affect fairness. For example, this result would be strengthened by showing that it persists or is amplified by certain properties of the data."}, "questions": {"value": "What is the difference between “chaotic-deep prejudice” and “(chaotic) prejudice”? Why is it called “deep”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ote2fWfXns", "forum": "75ce9hnKne", "replyto": "75ce9hnKne", "signatures": ["ICLR.cc/2026/Conference/Submission20499/Reviewer_2DTJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20499/Reviewer_2DTJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129073478, "cdate": 1762129073478, "tmdate": 1762933928737, "mdate": 1762933928737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comment - relevance of mean-field"}, "comment": {"value": "# Relevance of Mean-field results\nOur work takes a very influential line of research on initialization (which e.g. enabled easier training of very deep networks), and connects it to a very recent one on initialization biases (from ICML2024). Both theories are obtained in the mean-field limit, so connecting them only makes sense within the mean-field limit, as these theories are not defined outside of it. Furthermore, our results are supported by a wide array of runs in finite-width networks (we now have additional experiments on ResNet and MLP-Mixer (see https://limewire.com/d/KZE1k#eEXbPM1yaP or Appendix E.1 of the temporary updated version of the paper) and we can provide more if reasonable requests are made soon enough). This reflects the fact that modern architectures are highly overparameterized and practically very wide (at least, wide enough for the phenomenology of our theory).\nAdditionally, our results provide further practical and theoretical insight:\n\n- Connecting MF to IGB means that we are connecting initialization gradients to biases. Once this connection is made, we are able to unveil and explain non-trivial effects happening in practice. A notable example, which had failed to highlight, is presented in App. G. When gradient exploding occurs, it is not the gradients related to all classes which explode, but only the unfavored ones. E.g. in a binary problem, if the model is initialized in a prejudiced phase, one class’s gradients explode, and the other class’s vanish. \n\n- When hyperparameter (HP) tuning, one performs many short runs and chooses the HPs based on the early time performance of the model. A result of our theory is that, depending on which phase one is, the early time performance could be only representative of a subset of the data, thus harming the quality of hyperparameter tuning.\n\nSince we acknowledge that these points were not clear in the previous version, in the final version of the manuscript, we:\n\n- Improve the clarity and highlight why our results are relevant within a mean-field framework.\n\n- Provide new experiments as suggested by one of the reviewers.\n\n- Highlight how our theory unveils previously unseen behaviors, such as per-class gradient exploding.\n\n## Scope and Generality of MF Theory\nMean-field (MF) theory is an established and widely used analytical framework in modern deep learning, with a long track record of highly cited contributions. Its strength lies in the breadth of its scope: MF provides a unified lens for understanding a wide variety of architectures (convolutional, residual, recurrent, or attention-based) under minimal assumptions such as large width. This generality stands in contrast to many classical theoretical approaches, which often rely on restrictive settings such as shallow networks or synthetic data distributions.\n\n## Practical Impact and Predictive Power\nA key reason MF has become influential is its ability to connect the statistical structure of random initializations with the downstream trainability of neural networks. This connection has yielded practical, experimentally validated guidelines for initialization and scaling, enabling faster and more stable training in real-world models. MF predictions, though derived in the infinite-width limit, remain surprisingly accurate at realistic widths: they underpinned the first successful training of very deep CNNs and form the basis of frameworks such as Tensor Programs that allow hyperparameters to transfer reliably across widths and depths. Importantly, MF is not confined to the lazy-training regime—recent work has shown that it can also capture nontrivial feature learning dynamics (e.g.  Yang et al. (ICML 2021))."}}, "id": "xSTjJhq3ku", "forum": "75ce9hnKne", "replyto": "75ce9hnKne", "signatures": ["ICLR.cc/2026/Conference/Submission20499/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20499/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20499/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763655345439, "cdate": 1763655345439, "tmdate": 1763678921743, "mdate": 1763678921743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comment - linking MF predictions to initialization bias, terminology and the infinite width assumption"}, "comment": {"value": "# Linking MF Predictions to Initialization Bias\n## Complementary Frameworks\nWhile MF theory has had substantial impact (see comment \"Relevance of Mean-field results\"), its strengths also reveal its limitations. MF describes the *average* behavior over random initializations and is largely agnostic to data structure. Consequently, it cannot capture initialization-specific effects or distinguish how different datapoint subgroups are treated at initialization.\nIGB fills this gap. Rather than working with ensembles, IGB characterizes the predictive state of a *single* untrained network and identifies when systematic initial biases arise. The two perspectives—ensemble-level averages (MF) and instance-level behavior (IGB)—are therefore naturally complementary.\n## Hypotheses Underlying Our Results\nOur main results require only that the network outputs at initialization follow a Gaussian distribution, a property that holds broadly for common architectures and initialization schemes. This assumption is simpler and more general than the full IGB analytical setting, which typically involves stronger structural constraints to derive exact output statistics.\nCrucially, the quantities linking MF and IGB—such as the correlation $c$ and the induced bias $\\gamma$—can be computed using MF tools across a wide variety of architectures, far beyond standard MLPs. This makes the connection between the two frameworks operational in settings where neither MF nor IGB alone would be sufficient: MF provides the required correlations, and IGB translates them into predictions about class-wise initial behavior.\n\n## Main Contributions\nOur work establishes a precise relationship between initial guessing bias and trainability. This allows us to augment MF phase diagrams with information about class behavior at initialization and use MF-derived quantities to predict when IGB will appear. Through this link, we also extend both frameworks: for MF, we refine the ReLU phase diagram; for IGB, we incorporate bias weights and address architectures with multi-variable activations (e.g., pooling).\n## Implications\nThe combined perspective reveals when initial bias can be quickly corrected by learning dynamics (near the edge of chaos) and when it persists due to vanishing gradients (in the ordered phase). This has practical consequences. During hyper-parameter tuning, a model evaluated through short runs may seem to perform well only on a subset of the data because persistent initialization bias temporarily favors that subgroup—potentially leading to misleading model selection.\n\n\n\n\n# Terminology: Bias, Prejudice and Neutrality\nIn this work we analyze how trainability conditions connect to systematic asymmetries in model predictions that arise *at initialization*. Because the term **bias** is heavily overloaded in machine learning—ranging from inductive bias to fairness-related bias—we introduce the term **prejudice** to refer specifically to a statistically skewed assignment of classes *before any data is observed or evaluated*. This usage is purely technical: at initialization the model has no knowledge of the dataset, and any imbalance in class guesses carries no implication of correctness or incorrectness.\nPrejudice therefore denotes an **initial skew in predictive tendencies** induced solely by the random initialization scheme and model design, rather than by properties of the data. In contrast, **neutrality** refers to the symmetric case in which all classes are treated equivalently at initialization. Adopting this terminology allows us to clearly separate initialization-driven statistical asymmetries from the other established meanings of “bias” in the literature.\nTo improve clarity, in the final version of the manuscript, we explain more clearly the terminology and its justification.\n\n# On the Infinite-Width Assumption\n\n The infinite-width regime is a standard and widely adopted setting in modern deep-learning theory. Most established analytical frameworks, MF theory included, derive asymptotically exact results in this limit, while systematic treatments of *finite-width* networks remain comparatively recent and far less developed. \nImportantly, finite-width analyses are not in opposition to infinite-width theory: they typically build upon it, adding finite-size corrections rather than replacing the underlying asymptotic picture. Infinite-width theories therefore serve as the natural foundation for refined finite-width studies. \nIn our case, this assumption is not introduced by our analysis; it is inherent to the two well-established frameworks we connect (MF and IGB) both of which are defined in the large-width limit. Beyond the theoretical rigor, both frameworks have repeatedly shown strong empirical robustness: their infinite-width predictions align closely with the behavior of realistic finite-size networks. Our experiments confirm this consistency."}}, "id": "TqKm8BfvSe", "forum": "75ce9hnKne", "replyto": "75ce9hnKne", "signatures": ["ICLR.cc/2026/Conference/Submission20499/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20499/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission20499/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763655590534, "cdate": 1763655590534, "tmdate": 1763678775244, "mdate": 1763678775244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}