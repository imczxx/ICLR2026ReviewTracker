{"id": "542c8KxeQt", "number": 7133, "cdate": 1758008966957, "mdate": 1759897871184, "content": {"title": "SyncKV: A Syncopated Scheduling Approach to KV Cache Compression for Efficient Long-Context LLM Inference", "abstract": "KV cache accelerates the inference of Large Language Models (LLMs) by caching the key and value states of previous tokens, but its linearly increasing memory footprint poses a huge bottleneck for long-context tasks. To mitigate this, many previous studies evict unimportant tokens based on attention scores from the prefill stage or cumulative attention. However, by permanently evicting tokens, such static compression algorithms fail to preserve globally important tokens, as they overlook the \"attention drift\" phenomenon inherent in inference. Our analysis highlights this drift, showing that after generating just 50 tokens, the set of important tokens retains only about a 30\\% overlap with the one identified during the prefill stage. To address this, our core innovative insight is twofold: (1) the set of important tokens exhibits high temporal locality across adjacent generation steps, and (2) this set is highly similar among attention heads within the same layer. Based on these insights, we propose SyncKV, a training-free dynamic KV cache compression method. SyncKV takes advantage of these properties through a novel syncopated strategy in which a few \"representative heads\" periodically identify important tokens, triggering an asynchronous upload of the relevant KV cache from the CPU. We designed a parallelization strategy that overlaps the I/O overhead with the subsequent forward computing stage, thereby effectively hiding the delay of data transmission and achieving an acceleration effect. Experiments show that SyncKV has achieved state-of-the-art performance in multiple long-context benchmarks, reducing the GPU memory usage of the KV cache by up to 80\\%. Our code will be open-source.", "tldr": "We proposed SyncKV, which compresses the KV cache by dynamically recalling important tokens, thereby reducing GPU memory usage and accelerating long context inference.", "keywords": ["NLP", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8024a2877273d4209dd925ca67e25152deb02107.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SyncKV, a training-free dynamic KV cache compression framework designed to mitigate the significant GPU memory overhead associated with long-context LLM inference. The core problem addressed is that existing static compression methods, which evict tokens based on initial importance scores, fail to account for \"attention drift\"—the phenomenon where token importance changes dynamically throughout the generation process. The authors' analysis shows that the overlap of important tokens between the prefill stage and later generation steps can drop to as low as 30% after just 50 tokens.\nSyncKV is built upon two key insights: (1) the set of important tokens exhibits high temporal locality, and (2) there is high intralayer similarity among attention heads within the same layer. The authors claim this approach reduces KV cache memory usage by up to 80% while achieving state-of-the-art performance on multiple long-context benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The observation of \"Temporal Locality\" is insightful and potentially effective.\n* The ablation studies are relatively sufficient and comprehensive."}, "weaknesses": {"value": "* The concept of \"attention drift\" is not entirely new [1,2]. Please compare with these in the related work section to clarify the contribution of this work.\n* The current method's utilization of Locality seems somewhat simplistic. Perhaps the Anticipation Step should be triggered dynamically based on the input.\n* On SCBench, the performance did not surpass CAKE. Could you explore the impact of adjusting `m` on the performance of multi-step reasoning tasks? I suspect that most tasks in LongBench, except for summarization, are not very sensitive to `m`.\n\n[1] InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management (OSDI 24)\n\n[2] OmniKV: Dynamic Context Selection for Efficient Long-Context LLMs (ICLR 25)"}, "questions": {"value": "* As shown in Figure 2b, there are some decoding steps that have a low overlap with all previous steps. This might indicate a difficult or pivotal token. If you do not perform an Anticipation Step at this point, would it lead to a degradation in performance?\n* Could you test the performance of SyncKV on a reasoning model such as DeepSeek-R1-Distill-Llama-8B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qV1F5d7S6S", "forum": "542c8KxeQt", "replyto": "542c8KxeQt", "signatures": ["ICLR.cc/2026/Conference/Submission7133/Reviewer_xT4B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7133/Reviewer_xT4B"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760860681963, "cdate": 1760860681963, "tmdate": 1762919298296, "mdate": 1762919298296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **SyncKV**, a training-free dynamic KV cache compression framework for long-context LLM inference. Based on key observations — attention drift, temporal locality, and intralayer similarity — SyncKV employs representative heads to periodically update important tokens and asynchronously transfer KV cache between CPU and GPU, significantly reducing memory usage without notable accuracy loss. Experiments on multiple open-source LLMs show state-of-the-art accuracy–efficiency trade-offs across long-context benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Observations: Identifies and analyzes key phenomena in long-context attention, including attention drift, temporal locality, and intralayer similarity, which inform the design of SyncKV.  \n2. Methodological Contribution: Proposes a syncopated scheduling strategy with representative heads and asynchronous KV cache transfer, enabling dynamic retrieval without retraining.  \n3. Empirical Performance: Demonstrates state-of-the-art accuracy–efficiency trade-offs and up to 80% GPU KV cache memory reduction across multiple open-source LLMs and benchmarks."}, "weaknesses": {"value": "1. It is unclear whether attention drift occurs uniformly across all heads and how fixed-pattern heads might be more efficiently handled.  \n2. The necessity and rationale of head clustering, as well as its design choices (e.g., cluster count), are not sufficiently justified or empirically validated.  \n3. SyncKV provides only modest accuracy gains over CAKE while incurring higher latency, raising concerns about its practical advantage in certain scenarios."}, "questions": {"value": "1. Is attention drift observed consistently across all heads? Prior works ([1][2][3]) have categorized heads into fixed-pattern and dynamically changing types. If some heads follow fixed patterns, could they omit dynamic cache updates or adopt a larger synchronization stride $m$?  \n2. Could the authors clarify the necessity of head clustering? Specifically, please explain the rationale behind this design, including whether the heads exhibit the same similarity across different decoding steps. In addition, provide evidence of the performance benefits it brings and clarify how the number of clusters was determined.\n3. Comparing Table 1 and Figure 5, SyncKV shows only modest accuracy improvements over CAKE but higher latency. Does this imply it may not be optimal in many scenarios? To better highlight SyncKV’s accuracy advantage, could additional benchmarks, such as RULER, be included for a more comprehensive evaluation?\n\nIf the authors can satisfactorily address these concerns and provide additional evidence, I would be inclined to raise my rating.\n\n[1] RazorAttention: Efficient KV Cache Compression Through Retrieval Heads\n\n[2] DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads\n\n[3] CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QJsU0CVBNX", "forum": "542c8KxeQt", "replyto": "542c8KxeQt", "signatures": ["ICLR.cc/2026/Conference/Submission7133/Reviewer_rt7o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7133/Reviewer_rt7o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941577794, "cdate": 1761941577794, "tmdate": 1762919297939, "mdate": 1762919297939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SyncKV, a dynamic KV cache offloading method that addresses the \"attention drift\" phenomenon in long-context LLM inference. The approach employs representative heads to identify important tokens and offload/reload KV cache between CPU and GPU memory."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of sparse attention mechanisms with offloading represents a valuable research direction for addressing long-context inference challenges.\n2. The paper makes a valuable observation by identifying and quantifying the attention drift phenomenon."}, "weaknesses": {"value": "1. The paper fails to compare against existing sparse attention methods with offloading capabilities, such as InfLLM and ShadowKV, which are directly relevant to the proposed approach.\n\n2. The primary benchmark used (LongBench-v1) has an average sequence length of only 8K tokens, which is insufficient to validate claims about long-context performance. The paper would benefit significantly from evaluation on more challenging benchmarks designed for longer contexts, such as RULER or LongBench-v2.\n\n3. The experimental results show only minimal average performance improvements over existing methods like QUEST and CAKE, while incurring substantially higher inference latency. This unfavorable accuracy-latency trade-off undermines the practical value of the proposed method."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NrnNJ43sgB", "forum": "542c8KxeQt", "replyto": "542c8KxeQt", "signatures": ["ICLR.cc/2026/Conference/Submission7133/Reviewer_o7Ke"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7133/Reviewer_o7Ke"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762054072499, "cdate": 1762054072499, "tmdate": 1762919297447, "mdate": 1762919297447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}