{"id": "e1Lmkw35DG", "number": 19352, "cdate": 1758295563415, "mdate": 1759897043772, "content": {"title": "When Simplicity Wins: Efficient Knowledge Graph Generation with Sequential Decoders", "abstract": "Knowledge Graph (KG) generation requires models to learn complex semantic dependencies between triples while maintaining domain validity constraints. State-of-the-art graph generation models rely on expensive attention mechanisms to capture complex dependencies, yet (head, relation, tail) triples can be straightforwardly represented as sequences, suggesting simpler architectures may suffice for KGs. We present $\\textbf{ARK}$ ($\\textbf{A}$uto-$\\textbf{R}$egressive $\\textbf{K}$nowledge Graph Generation), a family of RNN and transformer-based models that succesfully perform KG generation. We show that the RNN variant requires only 9-21\\% of the training time, a 3.7-11× speedup, compared to the transformer variant. The RNN generates semantically valid graphs with 89.2-100.0\\% validity on IntelliGraphs benchmarks, with less than 0.76\\% degradation compared to the transformer on real-world datasets, while achieving up to 10.7\\% better compression rates on synthetic datasets and 12.8–21.1\\% gains on real-world datasets. Our analysis reveals that for KG generation, model capacity (hidden dimensionality $\\geq$ 64) matters more than depth, with single-layer GRUs matching deep transformer performance. We also introduce $\\textbf{SAIL}$, an extension of ARK that adds variational latent variables for controlled diversity and interpolation in KG generation. Both models support unconditional sampling and conditional generation from partial graphs. Our findings challenge the assumption that structured data generation requires attention mechanisms. This efficiency gain can enable the generation of larger KGs and unlock new applications.", "tldr": "We show that simple GRU-based models match transformer performance for knowledge graph generation while being 3.7-11× faster, challenging the assumption that complex attention mechanisms are necessary for structured data tasks.", "keywords": ["knowledge graph generation", "autoregressive models", "structured data generation", "knowledge graph", "knowledge representation"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c6d1e6bb057a1824ed4b9ecd21931d934b791c96.pdf", "supplementary_material": "/attachment/8187ff001a841103e0dd50858f43e2586c1788c1.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes two autoregressive models for KG generation: ARK, a GRU-based sequential decoder, and SAIL, a variational extension. The authors argue that these simple recurrent models can achieve competitive semantic validity and compression compared to transformer-based alternatives, at a fraction of the computational cost. Experiments on the IntelliGraphs benchmark show 89-100% \"semantic validity\" and up to 11x faster training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The authors highlight an important practical question—whether graph generation requires expensive attention mechanisms—and attempt to quantify the trade-off between complexity and performance.\n- The reported efficiency gains are substantial and consistently measured.\n- The results are presented cleanly, with clear numerical comparisons to transformer variants and baselines."}, "weaknesses": {"value": "- The paper never clearly defines what the model is trained or evaluated to do. It oscillates between unconditional generation, conditional completion, and sampling from a prior, but the experiments appear to involve only unconditional sampling of new graphs composed of existing entities and relations. It is thus unclear what constitutes success or correctness. The task does not resemble realistic KG completion or open-world graph expansion.\n\n- The model linearizes graphs into sequences of triples and trains with next-token prediction. This violates the fundamental permutation invariance of graphs: the likelihood $p(G)$ depends on the arbitrary order of triples. Randomizing triple order during training (as mentioned in Section 3.1) is a heuristic, not a principled solution. As a result, the model may learn the distribution over sequences, not graphs.\n\n- Definition 2.2 lists semantic constraints as generic rules (e.g., type or temporal consistency) but provides no formalism, rule language, or enforcement mechanism. The notion of \"semantic validity\" is therefore dataset-specific and ill-defined. It could mean anything from simple type checks to arbitrary heuristics, making the reported validity scores difficult to interpret.\n\n- The models have no mechanism for constraint satisfaction. There appears to be no constraint-aware loss, masked decoding, or logical regularization. `Semantic validity is only evaluated post-hoc by checking generated graphs against dataset rules. Thus, high validity numbers may reflect statistical memorization of patterns rather than any explicit reasoning or rule satisfaction. \n\n- The IntelliGraphs benchmark treats small, disconnected subgraphs as independent samples and evaluates them on semantic validity, novelty, and compression. None of these metrics assess factual accuracy, plausibility, or usefulness for reasoning. Since the benchmark's constraints are synthetic and the datasets small, solving it may not indicate progress on real KG generation, where graphs are incomplete, open-world, and semantically rich. \n\n- In SAIL, the prior $p(z)$ is a fixed standard Gaussian, not learned from data. The model's latent space is therefore only loosely regularized and may not correspond to meaningful graph semantics. Sampling from $p(z)$ does not constitute learning a structured generative model of KGs.\n\n- The paper claims that ``simplicity wins,'' but the evidence is confined to a narrow and partially synthetic benchmark of limited realism. There is no evidence that the results generalize to realistic or large-scale knowledge graphs. The work would be more credible as a small-scale efficiency study rather than as a broad claim about model inductive biases."}, "questions": {"value": "- What precisely is the generation task—graph completion, sampling, or reconstruction?\n- How are semantic constraints represented and evaluated? Are they logical rules, templates, or dataset-specific checks?\n- Is the model aware of these constraints during training, or are they only applied post hoc?\n- How is permutation invariance handled beyond randomization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IoMfyLzAdU", "forum": "e1Lmkw35DG", "replyto": "e1Lmkw35DG", "signatures": ["ICLR.cc/2026/Conference/Submission19352/Reviewer_3iXx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19352/Reviewer_3iXx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761066903131, "cdate": 1761066903131, "tmdate": 1762931289456, "mdate": 1762931289456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ARK (Auto-Regressive Knowledge Graph Generation), a sequential decoder framework for KG generation. Unlike prior approaches that rely on transformer-based attention-heavy architectures, ARK uses lightweight GRU-based recurrent models to sequentially generate triples as token sequences.\n\nKey contributions for this paper are:\n- Proposing ARK, A GRU-based autoregressive decoder that generates semantically valid KGs by modeling dependencies across triples.\n- Proposing SAIL, An extension of ARK with a variational latent space, allowing controlled generation, interpolation, and diversity in KG generation.\n- This paper shows great efficiency, as RNNs achieve comparable semantic validity (89.2–100%) and compression performance to transformers while requiring only 9–21% of training time.\n- This paper provides empirical insights, such as model capacity (hidden size $\\geq$ 64) matters more than depth; shallow GRUs match deep transformers.\n- Extensive evaluation on IntelliGraphs benchmarks (synthetic + real-world datasets) shows ARK/SAIL outperform traditional KG embeddings and rival transformer baselines, while being computationally efficient\n\nThese results suggest that simplicity (RNNs) can outperform complex transformers for KG generation, overturning assumptions that attention mechanisms are essential for structured data generation.\nOveall, this paper is well-executed and insightful, with strong empirical support for its claim that simplicity can rival complexity in KG generation. Its main limitations concern writing quality, technical novelty, scalability, and open-world applicability."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper challenges the widespread assumption that transformers are necessary for structured data generation, showing that GRUs can be equally or more effective.\n- The paper proposes a sequential decoding perspective for KGs, framing them as token sequences of triples.\n- The SAIL extension with variational inference introduces controllability in KG generation, a significant step for practical applications.\n- Results could reshape how KG generation models are designed, encouraging efficient, lightweight models over resource-intensive transformers, and hopefully inspires reconsideration of simplicity vs. complexity tradeoffs in other structured generative tasks."}, "weaknesses": {"value": "- Only tested on IntelliGraphs (synthetic + Wikidata-derived). While useful, these datasets are relatively small (graphs up to 212 triples). It remains unclear how the models scale to very large or heterogeneous KGs (e.g., enterprise-scale KGs).\n- Vocabulary of entities and relations is fixed at training time. This limits open-world applicability, where new entities emerge dynamically. Future-proofing with compositional or inductive representations would strengthen the work.\n- By linearizing KGs into sequences, the model imposes an arbitrary ordering on inherently unordered graphs. Although randomization mitigates bias, it may still affect generalization.\n- While SAIL adds latent controllability, practical use cases (e.g., controllable interpolation beyond synthetic transitions) are underexplored. The semantic plausibility of interpolated graphs is not deeply validated (e.g., realistic KG completion).\n- The technical novelty of this architecture is limited. Core ideas (GRU-based autoregression, variational latent extension) are adaptations of established techniques. The novelty lies more in the application and empirical insight than in methodological innovation."}, "questions": {"value": "- How does ARK/SAIL perform on much larger graphs (e.g., >10k triples) beyond the tested benchmarks? Do GRUs retain efficiency and semantic validity under extreme sequence lengths?\n- Have you evaluated whether different linearization strategies (e.g., BFS vs. DFS traversal of the graph) influence generation quality?\n- Can ARK/SAIL be extended to inductive settings (e.g., using entity embeddings learned unseen, on-the-fly, or compositional representations)?\n- While interpolations show smoothness, how well does the latent space correspond to interpretable KG properties (e.g., temporal progression, ontology consistency)? Could disentanglement improve controllability?\n- Do you envision ARK/SAIL being deployed in real-world KG completion systems (e.g., Wikidata augmentation)? If so, how would you mitigate issues of bias, factual errors, and scalability mentioned in the limitations?\n- The authors had better attach the Appendix directly after References, not put them at Supplementary Material, which is hard to follow."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XtAXOJiA1X", "forum": "e1Lmkw35DG", "replyto": "e1Lmkw35DG", "signatures": ["ICLR.cc/2026/Conference/Submission19352/Reviewer_WfZt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19352/Reviewer_WfZt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761274203916, "cdate": 1761274203916, "tmdate": 1762931288725, "mdate": 1762931288725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ARK, a family of autoregressive models for knowledge graph (KG) generation, demonstrating that simple GRU-based sequential decoders can match or even outperform transformer-based models in both efficiency and quality. The authors challenge the prevailing assumption that attention mechanisms are essential for structured data generation and back their claims with extensive experiments on synthetic and real-world datasets. They also propose SAIL, a variational extension of ARK, to support controlled diversity in KG generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes some autoregressive models for KG generation, which is a new but practical task.\n\n- The paper shows some interesting findings. For example, single-layer GRUs with sufficient hidden dimensionality (≥64) can match deep transformer models in KG generation tasks. On IntelliGraphs benchmarks, the GRU variant achieves 89.2–100.0% semantic validity, with only <0.76% degradation compared to transformers."}, "weaknesses": {"value": "- In my opinion, the technical contribution of this paper is limited. The authors formulate the KG generation task as a sequence generation problem and build their method upon existing sequential models. From this perspective, the paper lacks originality in its technical methods. Moreover, it does not clearly articulate the inherent challenges of the KG generation task itself.\n\n- Although the paper presents extensive experimental data and comparisons, the methods used are all fairly basic. Many other sequence modeling architectures, especially those involving large generative models such as KGT5, could potentially be applied to this task but have not been evaluated."}, "questions": {"value": "- The task is called KG generation, but it seems to only generate subgraphs composed of known entities and relations. Is it unable to generate new entities or new relations?\n\n- What is the motivation for choosing ARK and VAE? Can a standard RNN perform this task as well? What are the technical challenges of this task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8zNpP06NzS", "forum": "e1Lmkw35DG", "replyto": "e1Lmkw35DG", "signatures": ["ICLR.cc/2026/Conference/Submission19352/Reviewer_4g4Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19352/Reviewer_4g4Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653619702, "cdate": 1761653619702, "tmdate": 1762931288329, "mdate": 1762931288329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ARK for generating semantically valid knowledge graphs by treating KGs as sequences of triples. The authors argue that despite the structured nature of KGs, simple sequential decoders can match or even outperform Transformer-based counterparts in terms of semantic validity. Experiments on the IntelliGraphs dataset demonstrate promising results of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper presents a well-defined, end-to-end autoregressive approach to KG generation, with explicit linearization, training, and decoding procedures.\n\nARK and SAIL achieve near-perfect semantic validity on synthetic tasks and strong performance on real-world subsets."}, "weaknesses": {"value": "The paper evaluates only on small graphs (3–212 triples), which are far from real-world KG scales. It is unclear whether the proposed sequential approach scales to larger, denser, or more diverse graphs.\n\nComparing against KGE models (TransE, DistMult) as generative baselines is misleading. These models were never designed for joint graph generation. They score triples independently and lack mechanisms to sample coherent subgraphs. In fact, there are multiple works exploring the generation of KG triples, e.g., [1–2].\n\nUsing RNNs to model triple sequences has already been explored [3]. The authors overstate the novelty of using RNNs for sequence generation. Also, it is confusing, as Transformers also generate sequences autoregressively (e.g., in LLMs).\n\nGiven the rise of LLMs for structured data generation, the paper’s exclusive focus on small GRUs/Transformers feels outdated.\n\nTreating KGs as sequences ignores their inherent unordered, set-like nature. While the authors randomize triple order during training, this does not fully address permutation sensitivity. By the way, is it more reasonable to generate BFS/DFS paths [3]?\n\n[1] Start from zero: Triple set prediction for automatic knowledge graph completion. TKDE 2024.\n\n[2] Revisit and outstrip entity alignment: A perspective of generative models. ICLR 2024.\n\n[3] Learning to exploit long-term relational dependencies in knowledge graphs. ICML 2019."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VmODDDSI5V", "forum": "e1Lmkw35DG", "replyto": "e1Lmkw35DG", "signatures": ["ICLR.cc/2026/Conference/Submission19352/Reviewer_sBSk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19352/Reviewer_sBSk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665887242, "cdate": 1761665887242, "tmdate": 1762931287941, "mdate": 1762931287941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}