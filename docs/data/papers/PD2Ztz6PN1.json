{"id": "PD2Ztz6PN1", "number": 13548, "cdate": 1758219098072, "mdate": 1759897429401, "content": {"title": "Function regression using the forward forward training and inferring paradigm", "abstract": "Function regression/approximation is a fundamental application of machine learning. Neural networks (NNs) can be easily trained for function regression using a sufficient number of neurons and epochs. The forward-forward learning algorithm is a novel approach for training neural networks without backpropagation, and is well suited for implementation in neuromorphic computing and physical analogs for neural networks. To the best of the authors' knowledge, the Forward Forward paradigm of training and inferencing NNs is currently only restricted to classification tasks. This paper introduces a new methodology for approximating functions (function regression) using the Forward-Forward algorithm. Furthermore, the paper evaluates the developed methodology on univariate and multivariate functions, and provides preliminary studies of extending the proposed Forward-Forward regression to Kolmogorov Arnold Networks, and Deep Physical Neural Networks.", "tldr": "Training Neural Networks to perform function regression without backpropagation using the Forward Forward method.", "keywords": ["Deep Learning", "Forward Forward algorithm", "Function Regression", "Physical Neural Networks", "Kolmogrov Arnold Networks"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/21c0c098302cc772422936f2ba82fac3be0d8c37.pdf", "supplementary_material": "/attachment/042adf2916ab2553ad36141d4c643f587dde513f.zip"}, "replies": [{"content": {"summary": {"value": "The paper extends the concept of forward-forward networks, originally used for classification tasks, to regression tasks by carefully assigning positive and negative labels to data points. The paper tests forward-forward for regression methods on eight problems. The results look significantly good for these problems. There are no comparative evaluations with any other methods to show any significant uptake of these FF-regression for solving real-world regression problems or regression benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a clever trick for converting an FF classification model into an FF regression model.  It has been tested over 8 regression functions and shows the usefulness of the method."}, "weaknesses": {"value": "The paper lacks comprehensiveness. For example, in the abstract and conclusion, it is mentioned that FF-regression is extended to Kolmogorov-Arnold Networks and Deep Physical Networks. These have not been discussed in the main paper at all. Only two Figures have been described in the Appendix without good detail. \n\nMethods lack any comparison with other standard methods or analysis to confirm the real-world usability of the FF-regression to achieve the energy efficiency goal.  \n\nTrivial thing, but I am wondering whether the Appendix in the same PDF, which counts to 14 pages, violates the page limit."}, "questions": {"value": "Are these functions evolved for metrics like R2\nWhat is the performance of FF on the simple regression problem on the UCL repository? Whether authors tested this algorithm on those problems."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uyzyAFuXxC", "forum": "PD2Ztz6PN1", "replyto": "PD2Ztz6PN1", "signatures": ["ICLR.cc/2026/Conference/Submission13548/Reviewer_vkkW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13548/Reviewer_vkkW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920859871, "cdate": 1761920859871, "tmdate": 1762924147911, "mdate": 1762924147911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper extends the Forward-Forward (FF) algorithm for classification to function regression. It treats regression as binary classification: points within a tolerance of the true value are \"in-tol\" (label 1), others are \"out-tol\" (label 0). The network uses cosine similarity as the metric to distinguish between correct and incorrect labels."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This is the first to apply the forward-forward (FF) learning algorithm to regression. It effectively extends a classification-only method to continuous function approximation. This fills a clear gap in the FF literature.\n2. The pseudocode is clear, and the algorithms are easy to understand. Algorithms 1 and 2 provide step-by-step, reproducible training and inference procedures. The accompanying figures further enhance clarity.\n3. The method successfully works on regressing low-dimensional functions. It produces reasonable approximations with meaningful uncertainty estimates on 1D, 2D, and 3D benchmarks. Results are well-visualized and supported by MSE metrics."}, "weaknesses": {"value": "1. The method is tested only on simple, low-frequency functions, not on complex cases (high-dimensional, non-smooth, or multi-frequency). Such limitations in computational complexity are critical. Focusing on KANs may not be appropriate, because the uniform grids of KAN raise these core issues. I suggest that the author read some recent MLP variants that have solved these problems using the multi-scale mesh (a standard data structure in the finite element method).\n2. The approach is low-efficient, with training and inference times orders of magnitude slower than backpropagation (Table 2). Despite avoiding backpropagation, the method still relies on gradient descent with no acceleration in convergence (Algorithm 1). Scaling-law analysis would likely reveal poor sample and computation efficiency.\n3. The evaluation metrics are inconsistent: $R^2$ score should be used for noise-free regression, and MSE for noisy data. No comparison is made with standard mathematical or ML regressors (e.g., splines, tree-based models). The paper compares only with physical Neural Networks, which is misleading. Neural networks primarily focus on classification tasks (e.g., image generation, language modeling) and are poor at highly accurate regression tasks."}, "questions": {"value": "Suggestions for Improvement\n\n1. Broader Experiments: Test on more datasets or higher-dimensional/multi-frequency functions. Compare with traditional mathematical/ML methods.\n2. Enhancements to Method: The author can explore using the multi-scale mesh instead of the uniform grid on neural networks, which is a solution to extend the application scope to higher-dimensional/multi-frequency functions and even challenge recognition tasks (such as image classification on ImageNet )"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2YV2AWOhWD", "forum": "PD2Ztz6PN1", "replyto": "PD2Ztz6PN1", "signatures": ["ICLR.cc/2026/Conference/Submission13548/Reviewer_a3DU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13548/Reviewer_a3DU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972364325, "cdate": 1761972364325, "tmdate": 1762924147271, "mdate": 1762924147271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the forward-forward (FF) framework for training and performing inference with regression neural networks. The proposed FF algorithm adapts the FF approach originally designed for neural classifiers to regression tasks.\n\nSimilar to its use in classification, the FF framework for regression relies on positive data, negative data, and a goodness function. The algorithm reframes regression as a classification problem by creating bins of target values based on training data and a user-defined tolerance level. If a bin contains the true value y for a given input  x, the algorithm assigns a label of 1 (positive data); otherwise, it assigns a label of 0 (negative data). This transformation enables binary classification between positive and negative data.\n\nDuring inference, the FF framework maps a query input x to its predicted value. It generates trial points to define candidate bins, identifies the bins where x qualifies as positive data, and collects the corresponding trial points labeled as 1. The algorithm then computes the mean of these trial points to produce the final prediction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents its contribution clearly and organizes its content effectively. The introduction and theoretical background offer relevant context  that positions the work and highlights its contribution. The presentation of the forward-forward approach for neural classifiers helps with understanding the new regression framework as a reader. The suitability of the forward-forward algorithm for the implementation of neuromorphic computing and physical analogs for neural networks underscores the significance of this work."}, "weaknesses": {"value": "The quality of the experimental result in the paper is low and this undermines the soundness and utility of this paper.  \n\n1)  Experimental results for the hyperparameters: This paper does not present experimental results that illustrates the effect of the hyperparameters (tol, $y_{min}$, $y_{max}$)) on the performance of this method. Subsection 3.1.1 summarizes the effects but fails to provide quantitative evidence to support this.\n\n2). Limited simulations: The simulations in this paper is restricted to datapoints from a closes form expressions. It is unclear if the performance on closed form expressions with translate to typical ML-based regression problems, where you only have access to input-output pairs and not a closed-form expression. \n\n3). No information about the computational cost: For the inference, the new methods uses trial points to define candidate bins. The inference maps a query input x to the bins that result from these trial points. It is important to understand the computational cost of this approach. In the classification case, the number of runs per inference is K where K is the number of classes or categories. For regression, it seems that the number of runs depends on the number of trial point. But there is no information about the impact on the computational cost."}, "questions": {"value": "Please refer to the points under weaknesses.\n\nI also noticed a typographical error on line 183: \" ... to obtain a the mean and the standard deviation...\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EeRkg1DyOl", "forum": "PD2Ztz6PN1", "replyto": "PD2Ztz6PN1", "signatures": ["ICLR.cc/2026/Conference/Submission13548/Reviewer_ZMCD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13548/Reviewer_ZMCD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152343794, "cdate": 1762152343794, "tmdate": 1762924146767, "mdate": 1762924146767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new methodology for approximating functions, that is, function regression, using the Forward-Forward algorithm."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The studied topic focusing on function regression with FFA is interesting.\n\n2. The algorithms are concrete, enabling the reproducibility and contributing to understanding of the proposed method."}, "weaknesses": {"value": "1. The experiments are almost conducted in a toy environment. For example, the target functions approximated here are of few structures, including only elementary functions and their linear combinations. Such regression tasks are extremely simple for neural networks.\n\n2. This paper does not discuss the computational complexity of the method, let alone how to implement it on dedicated hardware. In the main text and experiments, the authors only demonstrate the regression results. The data examples provided offer very limited support for the overall rationale of the method. The authors do not report the runtime or convergence."}, "questions": {"value": "1. The function regression in Figure 4(a) does not perform well. I am concerned about the relationship between the training data and the objective function here.\n\n2. What are the challenges in extending this method to multiple dimensions? Is it necessary to design a goodness function or positive-negative data on a case-by-case basis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m0BvaOebW5", "forum": "PD2Ztz6PN1", "replyto": "PD2Ztz6PN1", "signatures": ["ICLR.cc/2026/Conference/Submission13548/Reviewer_43M2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13548/Reviewer_43M2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762224601591, "cdate": 1762224601591, "tmdate": 1762924145666, "mdate": 1762924145666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}