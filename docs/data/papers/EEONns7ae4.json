{"id": "EEONns7ae4", "number": 16263, "cdate": 1758262487138, "mdate": 1759897251395, "content": {"title": "Compositional Visual Planning via Inference-Time Diffusion Scaling", "abstract": "Diffusion models excel at short-horizon robot planning, yet scaling them to long-horizon tasks remains challenging due to computational constraints and limited training data. \nExisting compositional approaches stitch together short segments by separately denoising each component and averaging overlapping regions. However, this suffers from instability as the factorization assumption breaks down in noisy data space, leading to inconsistent global plans. We propose that the key to stable compositional generation lies in enforcing boundary agreement on the estimated clean data (Tweedie estimates) rather than on noisy intermediate states. Our method formulates long-horizon planning as inference over a chain-structured factor graph of overlapping video chunks, where pretrained short-horizon video diffusion models provide local priors. At inference time, we enforce boundary agreement through a novel combination of synchronous and asynchronous message passing that operates on Tweedie estimates, producing globally consistent guidance without requiring additional training. Our training-free framework demonstrates significant improvements over existing baselines across 100 simulation tasks spanning 4 diverse scenes, effectively generalizing to unseen start-goal combinations that were not present in the original training data. \\noindent Project website: \\url{https://comp-visual-planning.github.io/}", "tldr": "We introduce an inference-time compositional sampling approach that scales to unseen and long-horizon tasks.", "keywords": ["Planning", "Compositionality", "Diffusion Models", "Robotics"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cbd5a511fe6482a06fee2bd22de1a8da415f910e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the challenge of generating long-horizon visual plans using diffusion models trained only on short video clips. The authors introduce CVP, a training-free inference-time framework that composes overlapping short-horizon segments through message passing on denoised (Tweedie) estimates rather than noisy diffusion states. This approach ensures global temporal consistency and smooth transitions across segments without retraining. Experiments on robotic manipulation and visual planning benchmarks show that CVP substantially improves video quality, coherence, and task success rates over prior compositional and policy-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Composing diffusion-based video planners in a training-free manner by, shifting consistency enforcement to denoised Tweedie space is novel. \n- The proposed factor-graph formulation analysis provide a clear and rigorous justification for the method.\n- The approach achieves large performance gains in both visual quality and planning success.\n- The framework shows robust out-of-distribution generalization."}, "weaknesses": {"value": "- The experiments, though extensive within robotic planning, are restricted to a single simulation domain (ManiSkill). Testing on real-world data or non-robotic visual domains would strengthen the claim of general applicability. Without at least some transfer or qualitative evidence on real-world data, it remains unclear how robust the approach is. \n- The paper could better position its contribution relative to classical planning and hierarchical control methods, making clearer how compositional diffusion planning aligns with them. \n- The paper doesn't analyze the computational efficiency or runtime implications of the added message-passing guidance."}, "questions": {"value": "- Since the current formulation assumes a linear chain structure, can the approach be extended to tree-structured or multi-goal planning problems? For example, tasks with subgoals or parallel branches.\n- To what extent are the observed gains attributable to the compositional inference versus the base diffusion model quality? Would weaker or stronger base models change the relative benefit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Nzk6TKZnOC", "forum": "EEONns7ae4", "replyto": "EEONns7ae4", "signatures": ["ICLR.cc/2026/Conference/Submission16263/Reviewer_geNQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16263/Reviewer_geNQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762073566681, "cdate": 1762073566681, "tmdate": 1762926413460, "mdate": 1762926413460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free, inference-time scheme to compose long-horizon visual plans by stitching overlapping short video segments from a pretrained diffusion model. The key idea is to enforce boundary agreement on Tweedie estimates (denoised (x_{0|t})) rather than on noisy states (x_t), implemented via (i) a synchronous Gaussian chain constraint and (ii) an asynchronous, stop-gradient message-passing loss. These constraints are injected into DDIM using diffusion-sphere guidance (DSG). Algorithm 1 describes the sampler. On a simulated benchmark of four manipulation scenes, the method reports higher success rates than a DiffCollage-style baseline and certain policy baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear modeling lens: Neat factor-graph formulation and explicit block-tridiagonal precision structure for boundary consistency; derivations are technically correct.\n\n- Practical sampler design: Integrates guidance into DDIM via DSG with synchronized/async constraints; the ablation indicates the async stabilization matters for performance.\n\n- Empirical gains on the paper’s benchmark: Improved temporal coherence metrics and task success relative to selected baselines."}, "weaknesses": {"value": "- Enforcing guidance on (x_{0|t}) is a standard mechanism in training-free guidance; the synchronous GMRF derivation is textbook\n- The primary compositional baseline (DiffCollage/GSC) has near-zero success across tasks, making relative gains hard to interpret and raising concerns about implementation/tuning rather than superiority.\n- The Noisy-Bethe identity does not establish that Tweedie-space constraints are superior;"}, "questions": {"value": "How does joint denoising baseline compare to this method and DiffCollage-style factorization? How much better is it as reference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GBYigNV6B7", "forum": "EEONns7ae4", "replyto": "EEONns7ae4", "signatures": ["ICLR.cc/2026/Conference/Submission16263/Reviewer_dR6s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16263/Reviewer_dR6s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762231972891, "cdate": 1762231972891, "tmdate": 1762926413030, "mdate": 1762926413030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel, training-free method for compositional planning leveraging diffusion models. The authors begin by theoretically interrogating the inaccuracies of prior compositional generation methods, such as DiffCollage, which rely on a Bethe approximation in the noised state. This inaccuracy is formalized in the paper as the \"Noisy-Bethe Gap Theorem\" (Theorem 1). To address this limitation, the paper proposes a novel inference-time guidance technique involving synchronous and asynchronous message passing. This mechanism is designed to enforce boundary agreement directly on the estimated Tweedie estimates (the predicted $x_0$​) rather than on the noisy intermediate states.\n\nThe proposed method is validated on a suite of robotic manipulation tasks. This is achieved by first generating long-horizon visual plans (scenes) using the diffusion model, and subsequently inferring executable actions via a separately trained inverse dynamics model. Furthermore, the authors conduct a comparative analysis against DiffCollage, evaluating not only task success but also the quality of the generated visual plans using metrics such as motion smoothness, background consistency, and aesthetic quality.\n\nThe results demonstrate that the proposed framework significantly outperforms the baseline (DiffCollage) across both task success rates and scene generation quality. Specifically, while DiffCollage fails on nearly all manipulation tasks, the proposed method achieves success rates ranging from 50-100%. Moreover, it demonstrates roughly a two-fold improvement in key video quality metrics like motion smoothness and background consistency.\n\nThe authors include ablation studies on the components of their novel composition rule (i.e., synchronous vs. asynchronous message passing) and the number of denoising steps. A practical comparison of sampling time against the baseline is provided in Appendix F, offering readers a concrete understanding of the computational trade-offs. The appendices also furnish the theoretical derivations for the inaccuracy of the noisy Bethe approximation (Theorem 1) and the formulation of synchronous message passing (Theorem 2). Finally, the authors provide comprehensive details on the experimental environments, tasks, and model hyperparameters used for validation."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Clear Articulation of Problem and Strong Theoretical Grounding:** The paper's primary strength lies in its clear theoretical articulation of a critical problem in compositional planning. The authors compellingly argue that prior methods (e.g., DiffCollage) are flawed due to their reliance on a Bethe approximation in the noisy state. This critique is not just qualitative; it is substantially supported by the \"Noisy-Bethe Gap Theorem\" (Theorem 1, detailed in Appendix A), which provides a strong formal foundation for the work.\n    \n- **Significant Empirical Gains and Thorough Analysis:** The proposed method demonstrates a striking and significant performance gap compared to the baseline. While DiffCollage fails on nearly all robotic manipulation tasks, the proposed method achieves high success rates (50-97%). This empirical success is well-supported by a thorough ablation study (Figures 4 and 5) that effectively isolates the contributions of the synchronous and asynchronous message passing components. The analysis also practically demonstrates how performance scales positively with increased denoising steps (up to 300).\n    \n- **Completeness and High Potential for Reproducibility:** The authors are commended for the paper's completeness. Beyond the main theorem, they provide a detailed theoretical derivation for the synchronous message passing mechanism (Appendix B) . This theoretical rigor, combined with comprehensive details on the simulation environments, tasks (Appendix D) , and hyperparameters (Appendix E) , ensures a high potential for reproducibility."}, "weaknesses": {"value": "- **Insufficient Discussion on the Nature of the Contribution:** The paper attributes its success to composing in the Tweedie ($x_0$​ estimate) space. However, the powerful composing rules ($L_\\text{sync}$​ / $L_\\text{async}$​) are themselves very strong, explicit guidance heuristics. A key insight may be that the Tweedie estimation space is the first domain that enables such strong, explicit guidance to be applied stably (whereas it would fail in the noisy state). The authors are encouraged to reframe their contribution to reflect this: composing on clean estimates provides not just better Bethe approximation but also a crucial platform for powerful heuristics, and their proposed message passing is one such successful instance.\n    \n- **Potential for Generating Unrealistic Plans:** One of unaddressed limitations is the potential for the guidance to create unrealistic plans. The method guides sampling to enforce only boundary smoothness between short plans. It is plausible that this guidance could force the generation of a physically impossible trajectory within a short plan if the boundaries are in difficult-to-connect states (e.g., on opposite sides of a wall). The system might prioritize a smooth interpolation (e.g., passing through the obstacle) over maintaining physical plausibility.\n    \n- **Lack of Adaptivity in Plan Length:** As the authors acknowledge in their limitations, the method lacks adaptivity in plan length, requiring the number of factors ($n$) to be manually specified. This prevents the planner from dynamically adjusting to task complexity, potentially over-planning for simple tasks or under-planning for difficult ones.\n    \n- **(Minor) Presentation and Clarity Issues:**\n    \n    - **Equation 5:** the second term's numerator appears to have a typo. It is written as $x_t​−\\sqrt{\\bar{\\alpha}_t}x_0$, but based on the formulation in Equation 3, it should likely be ​$x_t​−\\sqrt{\\bar{\\alpha}_t}x_{0|t}$.\n\n    - **Figure 2 Caption:** The terms “DiffCollage” and \"GSC\" are used without any prior definition. Furthermore, \"Diffcollage\" should be capitalized consistently as \"DiffCollage\" .\n        \n    - **Qualitative Comparison:** The paper would greatly benefit from a direct qualitative comparison (e.g., side-by-side frames) showing the failure modes of DiffCollage (e.g., blurriness, incoherence) against the successful generations of the proposed method.\n        \n    - **Table 2:** The spacing between the caption and the table is too tight, harming readability.\n        \n- **Compliance with ICLR Policy:** The manuscript does not appear to include the mandatory statement on the use of Large Language Models (LLMs) in its preparation, as required by the ICLR 2026 Author Guide."}, "questions": {"value": "- **Rationale for Discount Factor (Eq. 11):** Regarding the asynchronous loss, why is a discount factor γ (set to 0.6) used to down-weight connections far from the start and goal? Intuitively, temporal smoothness and boundary agreement would seem equally important across the entire trajectory, not just near the endpoints. Could the authors elaborate on this design choice?\n    \n- **Performance Saturation (Fig. 5):** In Figure 5, the performance on the Drawer scene is still clearly improving at 300 sampling steps. Have the authors explored running this experiment with more steps until performance saturates? It would be valuable to know if it plateaus, or if performance eventually degrades.\n    \n- **Sensitivity to Loss Weighting (Fig. 4):** The ablation in Figure 4 compares \"Sync Only\", \"Async Only\", and their combination. This reviewer is curious about the sensitivity to the  weighting between these two losses. What would a sweep of $w \\cdot L_\\text{sync}​+(1−w) \\cdot L_\\text{async}$​ reveal? This would provide insight into the robustness of the method and the complementary nature of the two losses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RLbkZpIDI9", "forum": "EEONns7ae4", "replyto": "EEONns7ae4", "signatures": ["ICLR.cc/2026/Conference/Submission16263/Reviewer_bi53"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16263/Reviewer_bi53"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240019735, "cdate": 1762240019735, "tmdate": 1762926412613, "mdate": 1762926412613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an inference-time compositional technique to extend existing short-horizon diffusion models to long-horizon robot planning without retraining. To solve the instability issue of prior work (e.g. DiffCollage/GSC) that composes segments in the noisy space ($x_t$)—a problem the paper theoretically identifies as the \"Noisy-Bethe Gap\" —this method enforces boundary consistency directly on the clean data estimates ($x_{0∣t}$). It achieves this by defining Synchronous ($L_{sync}$) and Asynchronous ($L_{async}$) message passing losses on $x_{0∣t}$ and guides the sampling process using Diffusion-Sphere Guidance (DSG)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. While using $x_0$ prediction (Tweedie estimates) for guidance is not new, its novel application to compositional generation is a impressive contribution, using it to enforce boundary consistency between factors.\n2. The paper provides a strong theoretical justification for why prior methods fail. By identifying and proving the \"Noisy-Bethe Gap\" (Theorem 1) , it formally explains that simple averaging in the noisy $x_t$​ space is fundamentally flawed.\n3. It addresses a critical and timely problem: how to achieve inference-time scaling by stitching pre-trained, short-horizon models to generate long-horizon content. This training-free, \"plug-and-play\" approach is highly practical and flexible, extending the capabilities of existing models without costly retraining."}, "weaknesses": {"value": "1. The justification for using Diffusion-Sphere Guidance (DSG) specifically is not entirely clear. The paper would be significantly strengthened by an ablation study that compares DSG to standard, simpler guidance mechanisms (e.g., conventional gradient-based guidance or other proposed guidance methods). This would help demonstrate that the proposed method is general enough to improve performance even when paired with various other guidance techniques, not just DSG.\n2. The paper's claims regarding compositional performance would be significantly strengthened by a broader comparison against other diffusion-based stitching and planning methods. The current experiments are heavily focused on DiffCollage/GSC. Including comparisons to other relevant works, such as CompDiffuser [1], would provide a more comprehensive validation.\n\n\n[1] Luo, Y., Mishra, U. A., Du, Y., & Xu, D. (2025). Generative trajectory stitching through diffusion composition. arXiv preprint arXiv:2503.05153."}, "questions": {"value": "1. The justification for using Diffusion-Sphere Guidance (DSG) specifically is not entirely clear. Is there any reason that this paper uses the DSG instead of other guidance methods?\n\n2. I am curious about the qualitative adherence of the generated plans to the method's design. When the authors inspected the planning results, did they observe that the plans actually follow the underlying compositional structure (i.e., the chain of factors) described in the appendix? To put this another way: if a setting existed with a metric that specifically measures this 'compositionality,' would the authors hypothesize that proposed method would achieve the best score compared to the baselines?\n\n3. The proposed method introduces significant inference overhead by requiring back-propagation at every DDIM step. The paper lacks a comparison under a fixed same computational budget. For example, does the 300-step proposed method (with costly guidance) outperform a baseline like DiffCollage allowed to run for many more steps (e.g., 1000 or 3000 steps) such that the total wall-clock time/Number of Function Evalutations (NFEs) are equivalent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1SvL9HFafH", "forum": "EEONns7ae4", "replyto": "EEONns7ae4", "signatures": ["ICLR.cc/2026/Conference/Submission16263/Reviewer_PERF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16263/Reviewer_PERF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762317834511, "cdate": 1762317834511, "tmdate": 1762926412242, "mdate": 1762926412242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper builds on DiffCollage, but extends it from image composition to visual planning: instead of stitching small image patches into a larger picture, it stitches short video clips into coherent, long-horizon visual plans. The motivation is to overcome diffusion planners’ inherent limitation: while they excel at short-horizon trajectory generation, scaling them to long-horizon tasks leads to instability and broken temporal consistency when naively composing multiple short plans.\n\nProblem Setting\n- Long-horizon robot planning using diffusion models suffers from compounding noise and the breakdown of factorization assumptions when combining multiple short rollouts.\n- Prior compositional methods (e.g., DiffCollage, Generative Skill Chaining) operate in noisy latent space, leading to discontinuities at segment boundaries and failure to generalize to unseen start–goal pairs.\n\nMethod: Compositional Visual Planning (CVP in short)\n- CVP reframes long-horizon planning as inference over a chain-structured factor graph built from overlapping short-horizon video segments.\n- Key insight: enforce boundary consistency on Tweedie estimates (denoised predictions) rather than on noisy diffusion states.\n- Two complementary message-passing schemes ensure global coherence: (a) Synchronous message passing: treats all factors jointly as a Gaussian linear system for order-invariant, parallel consistency enforcement. (b) Asynchronous message passing: uses one-sided, stop-gradient updates (TD-style) for faster and more stable convergence.\n- The two are combined via Diffusion-Sphere Guidance (DSG) to interpolate between unconditional sampling and guided refinement — preserving both stability and diversity.\n- Importantly, the method is training-free: it reuses a pretrained short-horizon diffusion backbone without fine-tuning.\n\nThe authors evaluate the proposed approach on 100 robotic manipulation tasks across four scenes (Tool-Use, Drawer, Cube, Puzzle) built from ManiSkill.\n- In terms of visual quality, CVP achieves smoother motion, consistent backgrounds and better image quality quantified by VBench++ metrics.\n- When tested for task success rate (when combined with a inverse dynamics model), CVP achieves major improvements in out-of-distribution generalization, and stronger in-domain performance than existing diffusion policy baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a clear conceptual insight: enforcing boundary agreement on denoised Tweedie estimates rather than noisy diffusion states. This shift is both theoretically justified and empirically validated.\n- The toy example in Figure 2 effectively illustrates the key failure mode of noisy-state composition (boundary drift) and how the proposed approach closes those gaps. It’s an unusually clear and intuitive visualization of the underlying problem.\n- The proposed framework is training-free, operating entirely at inference time, which makes it easy to apply on top of existing short-horizon diffusion backbones.\n- The empirical results demonstrate strong improvements in temporal coherence and OOD generalization across a large set of manipulation tasks, validating both stability and scalability.\n- The method has solid theoretical grounding, bridging diffusion models with message-passing in factor graphs, which adds conceptual depth beyond heuristic composition."}, "weaknesses": {"value": "- While OOD performance is impressive, the method still relies on the base video diffusion model having seen the intermediate motion fragments during training. The generalization is compositional rather than truly extrapolative, which limits deployment in fully novel environments.\n- The method requires a goal image as conditioning, restricting applicability to scenarios where both start and goal visual states are available. Many planning settings might not have explicit goal frames.\n- The approach introduces extra inference-time computation due to gradient-based guidance and message-passing updates. The paper lacks runtime comparisons with simpler compositional samplers.\n- The paper covers the preliminaries of factor-graph very quickly in section 3.1. Also. the way equation 1 puts the denominator as negative exponent makes it way less intuitive. This limits the accessibility for readers not already familiar with DiffCollage."}, "questions": {"value": "- Language conditioning: Could this approach generalize to language-conditioned visual planning? In other words, can the boundary or goal condition g be replaced by a text or semantic embedding that specifies the desired outcome, rather than an explicit goal image?\n- Boundary artifacts in DiffCollage: Figure 2 shows DiffCollage leaving visible boundary gaps, but the original DiffCollage image-generation results generally do not show such artifacts. Could this discrepancy arise because the noisy factorization assumption breaks more severely in video or temporal domains than in static image composition? A short clarification would help reconcile this difference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6KTdC2B3pD", "forum": "EEONns7ae4", "replyto": "EEONns7ae4", "signatures": ["ICLR.cc/2026/Conference/Submission16263/Reviewer_zF3D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16263/Reviewer_zF3D"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16263/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762742727098, "cdate": 1762742727098, "tmdate": 1762926411783, "mdate": 1762926411783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}