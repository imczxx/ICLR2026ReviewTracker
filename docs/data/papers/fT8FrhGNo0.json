{"id": "fT8FrhGNo0", "number": 5982, "cdate": 1757949426170, "mdate": 1763655958430, "content": {"title": "ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?", "abstract": "Omnidirectional images (ODIs) provide full 360$^{\\circ} \\times$ 180$^{\\circ}$ view which are widely adopted in VR, AR and embodied intelligence applications. While multi-modal large language models (MLLMs) have demonstrated remarkable performance on conventional 2D image and video understanding benchmarks, their ability to comprehend the immersive environments captured by ODIs remains largely unexplored. To address this gap, we first present ODI-Bench, a novel comprehensive benchmark specifically designed for omnidirectional image understanding. ODI-Bench contains 2,000 high-quality omnidirectional images and over 4,000 manually annotated question-answering (QA) pairs across 10 fine-grained tasks, covering both general-level and spatial-level ODI understanding. Extensive experiments are conducted to benchmark 20 representative MLLMs, including proprietary and open-source models, under both close-ended and open-ended settings. Experimental results reveal that current MLLMs still struggle to capture the immersive context provided by ODIs. To this end, we further introduce Omni-CoT, a training-free method which significantly enhances MLLMs’ comprehension ability in the omnidirectional environment through chain-of-thought reasoning across both textual information and visual cues. Both the benchmark and the code will be released upon the publication.", "tldr": "", "keywords": ["omnidirectional image", "benchmark", "virtual reality"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b3e016cd77bf041cfe47f58139439c813b08ec7.pdf", "supplementary_material": "/attachment/92682ce32169c2e7fbfc748e7faa76216bff3259.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces ODI-Bench, a 360°×180° omnidirectional-image benchmark for VR/AR/embodied settings: 2,000 ODIs, 4,000+ human-annotated QA pairs, 10 fine-grained tasks covering general semantics and spatial reasoning, with both close- and open-ended evaluation. Benchmarking 20 proprietary/open-source MLLMs shows they underperform on ODIs despite strong 2D results, indicating poor immersive/spatial understanding. The authors propose OmniCoT, a training-free chain-of-thought procedure that fuses textual and visual cues, yielding substantial accuracy gains across ODI tasks. Benchmark and code will be released upon publication to catalyze research in panoramic scene understanding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic is timely and relevant.\n2. The manuscript is clearly written and well structured.\n3. The proposed method Omni-COT delivers consistent, meaningful improvements."}, "weaknesses": {"value": "1. In Table 1, the authors name the proposed benchmark \"360Bench.\" For consistency with the main paper, I recommend renaming it to \"ODI-Bench.\"\n\n2. ODI-Bench focuses on 360-degree views, which the authors claim benefits AR/VR. I’m not fully convinced. Human field of view is roughly ~180 degrees; in AR/VR, users still turn their heads/bodies to see other views. Is it necessary to require models to reason directly on a single 360-degree image?\n\n3. The ODI-Bench images are 360-degree panoramas re-projected to 2D. A concern is that most current models were not trained on such projections and may treat them as \"wrapped images,\" causing train–inference mismatch and errors.\n\n4. As a human evaluator, my answer in Figure 4 would also be \"No.\" The image seems confusing for both VLMs and people. Without seeing Appendix Figure 1 (which reveals that the far left and far right edges are adjacent—i.e., the \"back\" region), I would likely misinterpret it.\n\n5. When benchmarking, consider giving VLMs minimal necessary priors, e.g., \"This is a 360-degree panoramic (pano) view image,\" to reduce avoidable misunderstandings. I have asked GPT-4o for the Appendix Fig. 6 question: \"This is a 360 degree pano view image. Standing under the shelter facing the railway tracks, where is the train in relation to me? A. Behind;B. Right;C. Left;D. Front\" The model could correctly solve this question.\n\n6. I’m curious whether performance improves if the panorama is split into multi-view images or converted into a short continuous video and then fed to the VLM, followed by inference on the benchmark questions (not viewpoint guiding—just direct inference)."}, "questions": {"value": "See weaknesses.\n\nBesides, authors claim the benchmark are high-resolution images. However, in recent paper [1, 2], the researchers discussed that in most general scenarios, even simple resizing can achieve strong performance, do not need such high resolution image. How do the authors view this issue? I look forward to some discussion on this point.\n\n\n[1] VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning\n\n[2] Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qS7jTmdrxJ", "forum": "fT8FrhGNo0", "replyto": "fT8FrhGNo0", "signatures": ["ICLR.cc/2026/Conference/Submission5982/Reviewer_Jjv6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5982/Reviewer_Jjv6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750014433, "cdate": 1761750014433, "tmdate": 1762918391840, "mdate": 1762918391840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper makes two main contributions:\n\n1.\tODI-Bench, an omnidirectional image question-answering (QA) benchmark for multimodal large language models (MLLMs), consisting of 2,000 omnidirectional images and over 4,000 manually annotated QA pairs.\n2.\tOmni-CoT, a training-free method designed to enhance MLLMs’ comprehension ability on omnidirectional image QA tasks.\n\nThe authors demonstrate that both open-source and proprietary MLLMs still struggle with reasoning and understanding in omnidirectional settings. The proposed Omni-CoT method improves performance by cropping and wrapping ODI images from multiple viewpoints before feeding them into the models. Experiments show that this approach consistently enhances MLLM performance across different architectures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is clearly written and well structured, making it easy to follow.\n* The dataset is carefully designed and systematically organized.\n* The evaluation of MLLMs is comprehensive and covers a wide range of models.\n* The related work section provides a thorough and insightful overview of prior research.\n* The proposed Omni-CoT method is simple yet effective, demonstrating consistent improvements across various models."}, "weaknesses": {"value": "* Limited technical novelty of Omni-CoT: While Omni-CoT is effective, its core idea mainly involves viewpoint decomposition and prompt-based aggregation, which may be seen as a straightforward extension of existing multi-view prompting techniques.\n    * Clarification: In lines 396–402, the authors discuss the drawbacks of directly splitting ODIs and feeding them into the model. Could the authors clarify how this baseline differs from Omni-CoT? Is Omni-CoT’s improvement primarily due to its CoT reasoning structure, or due to the view cropping itself? An ablation that isolates these effects would significantly strengthen the paper.\n* Dataset scale and reliability: With only 2,000 images and ~4,000 QA pairs, the benchmark is relatively small for evaluating large-scale models. The results may be statistically unstable\n    * I would suggest reporting mean ± standard deviation over multiple runs or random seeds to quantify evaluation noise and ensure reproducibility.\n* Broader impact and future directions: The paper could benefit from a brief discussion on how ODI-Bench might be used for training, not just evaluation — for instance, as a pretraining or fine-tuning resource for spatial reasoning in immersive environments."}, "questions": {"value": "* Line 355: Please clarify what is meant by “absolute directions”. Does it refer to directions with respect to a global (earth-fixed) frame, or relative to the ego’s orientation in the scene?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d88b5lrQey", "forum": "fT8FrhGNo0", "replyto": "fT8FrhGNo0", "signatures": ["ICLR.cc/2026/Conference/Submission5982/Reviewer_FDum"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5982/Reviewer_FDum"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774543996, "cdate": 1761774543996, "tmdate": 1762918391602, "mdate": 1762918391602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the gap in evaluating the ability of Multi-modal Large Language Models (MLLMs) to understand Omnidirectional Images (ODIs) and constructs ODI-Bench, the first comprehensive benchmark for this task. The benchmark comprises 2,000 high-quality omnidirectional images and over 4,000 manually annotated question-answering (QA) pairs, covering 10 fine-grained tasks. It supports both close-ended and open-ended evaluations, enabling a thorough assessment of MLLMs’ general-level and spatial-level understanding of ODIs. Experiments on 20 representative MLLMs reveal significant shortcomings in current models’ ability to comprehend immersive ODI environments. To tackle this, the paper proposes Omni-CoT, a training-free framework that enhances MLLMs’ ODI understanding through step-by-step reasoning—including viewpoint-guided answering, crop cue grounding and refinement, and response refinement—with its effectiveness validated across multiple models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Benchmark Construction Fills a Critical Domain Gap: ODI-Bench addresses key flaws of existing ODI benchmarks (e.g., low resolution, limited scene diversity, constrained question domains) by providing high-resolution images, covering both indoor and outdoor scenes, and designing diverse tasks. It adopts a hybrid annotation approach (automated pipeline + human verification) to ensure data quality, serving as a unified, reliable benchmark for evaluating MLLMs’ ODI understanding and promoting standardized research in this field.\n\n2. Comprehensive Evaluation Dimensions and Rigorous Experimental Design: For the first time, the paper employs both close-ended (multiple-choice/yes-no) and open-ended evaluation settings. This dual design not only assesses models’ recognition accuracy under constrained options but also measures their generative reasoning ability in unconstrained scenarios. Experiments cover 20 MLLMs of varying types (proprietary/open-source) and parameter scales, with additional baselines (Blind GPT-4o, random choice) for comparison. In-depth result analysis effectively reveals the challenges MLLMs face in ODI understanding.\n\n3. Innovative and Practical Training-Free Enhancement Framework: Omni-CoT targets MLLMs’ insufficient comprehension of immersive ODI environments by introducing a human-like step-by-step chain-of-thought strategy. It guides models to interpret ODI scenes via compact textual prompts (instead of additional image inputs) and refines reasoning using crop cues, avoiding the high resource consumption of training-based methods. The framework demonstrates strong versatility, achieving performance improvements on both proprietary and open-source models."}, "weaknesses": {"value": "1. Stepwise Ablation of Omni-CoT’s Reasoning Stages Is Insufficient: Existing experiments validate the overall effectiveness of Omni-CoT but fail to disassemble and analyze the individual contributions of its three core steps (viewpoint-guided answering, crop cue grounding and refinement, response refinement). For example, it remains unclear how much each step independently improves performance on spatial-level tasks, or whether crop refinement (a key sub-step) effectively filters out irrelevant cues. Supplementing stepwise ablation experiments will help clarify the role of each component and strengthen the framework’s interpretability.\n\n2. Evaluation of Reasoning Efficiency Is Lacking: Omni-CoT enhances performance through multi-step reasoning but does not report the increase in inference time compared to direct answering. It is recommended to add quantitative analysis of inference efficiency—such as comparing Omni-CoT with direct answering and Zero-shot CoT in terms of average reasoning time per sample—to balance performance gains against time costs."}, "questions": {"value": "See weaknesses \"Evaluation of Reasoning Efficiency Is Lacking\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o90mKGJ2zM", "forum": "fT8FrhGNo0", "replyto": "fT8FrhGNo0", "signatures": ["ICLR.cc/2026/Conference/Submission5982/Reviewer_RHjD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5982/Reviewer_RHjD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795964482, "cdate": 1761795964482, "tmdate": 1762918390518, "mdate": 1762918390518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ODI-Bench, a benchmark designed to evaluate the spatial and reasoning capabilities of multimodal large language models (MLLMs) in immersive omnidirectional environments. The benchmark covers 10 fine-grained tasks across 2,000 images with over 4,200 QA pairs. The authors further propose Omni-CoT, a training-free chain-of-thought prompting framework that decomposes reasoning into multiple stages. Experiments on a wide range of MLLMs are performed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe proposed benchmark for ODI is timely.\n2.\tThe paper is generally easy to follow and polished.\n3.\tThe results are promising with the proposed Omni-CoT."}, "weaknesses": {"value": "1.\tThe dataset scale is somewhat limited. Could the diversity of ODI-Bench cover the real-world scenes? \n2.\tThe benchmark relies heavily on automatic template-based question synthesis, which may restrict linguistic diversity and introduce annotation bias.\n3.\tCould the authors provide ablation studies to show the effects of viewpoint, crop, and refinement stages? It is suggested to provide more hyperparameter ablation to provide more insights.\n4.\tThe comparison focuses only on MLLMs. Could the authors compare with the method that first reconstructs 3D, followed by evaluation by 3D-aware LLM methods?\n5.\tThe authors should clarify the data licenses. \n6.\tFigure 1 is confusing, especially the upper right figure."}, "questions": {"value": "The questions are listed above."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YnR8ZR4h8b", "forum": "fT8FrhGNo0", "replyto": "fT8FrhGNo0", "signatures": ["ICLR.cc/2026/Conference/Submission5982/Reviewer_jfZu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5982/Reviewer_jfZu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762068990724, "cdate": 1762068990724, "tmdate": 1762918389680, "mdate": 1762918389680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Our Responses & Paper Revisions"}, "comment": {"value": "We sincerely thank all the reviewers for their valuable and constructive reviews. We appreciate that the reviewers acknowledged that ODI-Bench **fills an important domain gap** (Reviewer RHjD), **is timely** (Reviewer jfZu, Jjv6), **comprehensive** (Reviewers RHjD, FDum), **easy to follow** (Reviewers jfZu, FDum), and recognized **the motivation and superior effectiveness** of our proposed Omni-CoT framework (Reviewers jfZu, RHjD, FDum, Jjv6).\n\nWe have made our best effort to address the concerns and revise the paper accordingly. The major modifications are summarized as follows.\n\n1. **Paper organization and proofreading:**\nIn response to the reviewers’ feedback regarding clarity in figures, tables, and textual descriptions, we have revised the manuscript accordingly. Specifically, we have updated Figure 1 (including both the figure and the caption) to improve readability, and corrected the benchmark name in Table 1 to ensure accuracy and consistency. Additional minor textual refinements are also made throughout the paper to enhance clarity and presentation quality.\n\n2. **Additional baseline experiment:**\nWe have included new baseline experiments to address the reviewers’ concerns. Specifically, we have provided results for multi-view perspective image input and ODI-to-video input in Table 7, enabling a more comprehensive benchmark. These baselines further validate the effectiveness of our proposed Omni-CoT and offer deeper insights into model behavior under different input modalities.\n\n3. **More ablation studies:**\nWe have incorporated more comprehensive ablation studies to rigorously validate the reliability of Omni-CoT. Table 5 presents a detailed step-wise ablation that isolates and examines the contribution of each of the three core stages within Omni-CoT. Table 10 reports the filter ratio of the crop refinement step, demonstrating its effectiveness in removing irrelevant cues and improving ODI comprehension performance. In addition, Table 6 provides an ablation on the field of view (FoV) used in the viewpoint guiding step, further confirming the rationale behind our selected perspective FoV.\n\n4. **Benchmark reliability evaluation:**\nWe have conducted further experiments to verify the reliability of our benchmark. Table 12 reports the results of multiple runs, along with the task-wise mean and variance, demonstrating the reproducibility and stability of ODI-Bench. Moreover, Table 13 compares the effects of different prompting strategies, showing that the benchmark performance remains consistent under varying prompt formulations.\n\n5. **Discussion on high-resolution image and token reduction:**\nWe have added a meaningful experiment in Table 11 to investigate the impact of direct image resizing for token reduction, showing that reducing the input size generally decreases the model performance on ODI-Bench.\n\nThe major revised contents in the manuscript are highlighted in blue. Point-for-point responses to specific comments are given in the following reviewer-specific responses. We welcome any further discussions and will address any remaining concerns."}}, "id": "4aOTE3DBu4", "forum": "fT8FrhGNo0", "replyto": "fT8FrhGNo0", "signatures": ["ICLR.cc/2026/Conference/Submission5982/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5982/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission5982/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763659534280, "cdate": 1763659534280, "tmdate": 1763659534280, "mdate": 1763659534280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}