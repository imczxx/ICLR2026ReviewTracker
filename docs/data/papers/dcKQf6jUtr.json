{"id": "dcKQf6jUtr", "number": 17233, "cdate": 1758273739826, "mdate": 1759897189386, "content": {"title": "Cross-Modal Content Optimization for Steering Web Agent Preferences", "abstract": "Vision–language model (VLM)-based web agents increasingly power high-stakes selection tasks like content recommendation or product ranking by combining multimodal perception with preference reasoning. Recent studies reveal that these agents are vulnerable against attackers who can bias selection outcomes through preference manipulations using adversarial pop-ups, image perturbations, or content tweaks. Existing work, however, either assumes strong white-box access, with limited single-modal perturbations, or uses impractical settings. In this paper, we demonstrate, for the first time, that joint exploitation of visual and textual channels yields significantly more powerful preference manipulations under realistic attacker capabilities. We introduce Cross-Modal Preference Steering (CPS) that jointly optimizes imperceptible modifications to an item’s visual and natural language descriptions, exploiting CLIP-transferable image perturbations and RLHF-induced linguistic biases to steer agent decisions. In contrast to prior studies that assume gradient access, or control over webpages, or agent memory, we adopt a realistic black-box threat setup: a non-privileged adversary can edit only their own listing’s images and textual metadata, with no insight into the agent’s model internals. We evaluate CPS on agents powered by state-of-the-art proprietary and open source VLMs including GPT-4.1, Qwen-2.5VL and Pixtral-Large on both movie selection and e-commerce tasks. Our results show that CPS is significantly more effective than leading baseline methods. For instance, our results show that CPS consistently outperforms baselines across all models while maintaining 70% lower detection rates, demonstrating both effectiveness and stealth. These findings highlight an urgent need for robust defenses as agentic systems play an increasingly consequential role in society.", "tldr": "We introduce Cross-Modal Preference Steering (CPS), a black-box attack framework that manipulates VLM-based web agents' decisions by jointly exploiting visual vulnerabilities and textual biases, achieving high success rates while remaining stealthy.", "keywords": ["Web Agent", "Agent Safety", "Vision Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8af0cb76d4fe272e694163e0af1ff435f0d7916.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes **CPS (Cross‑Modal Preference Steering)**, an attack for manipulating the choices of multimodal web agents by jointly optimizing **images** and **text** associated with a single listing (e.g., a product or movie). On the **visual** side, CPS learns small, transferable perturbations on an ensemble of CLIP‑like encoders using multi‑crop PGD with an $\\ell_\\infty$ budget (typically noted as $8/255$). On the **text** side, CPS iteratively edits the item description to exploit LLM‑induced biases while claiming to preserve semantics and platform policy compliance. The attacker is assumed to control only their own listing’s thumbnail(s) and text; the victim agent is treated as a black box.\n\nExperiments use two environments (a movie‑selection page and a shopping scenario) and measure **Preference Manipulation Rate (PMR)**. CPS is reported to outperform baselines, including in **visual‑only** ablations, and to transfer from CLIP surrogates to proprietary VLMs. A small detection study (a prompted LLM detector told that exactly one item is adversarial) suggests low **Manipulation Detection Rate (MDR)**.\n\nI find the cross‑modal framing relevant and the visual objective technically plausible. However, I’m not convinced the **practicality** and **stealth** claims are adequately supported under realistic constraints, especially for the textual component."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Originality / Framing.** I like the positioning of content‑steering as cross‑modal optimization (image + text) as it aligns with how real agents consume web content.  \n- **Technical plausibility (visual).** Multi‑crop, ensemble‑CLIP PGD is a sensible black‑box strategy to mitigate unknown preprocessing; the $\\ell_\\infty$ budget is explicit, and transfer to proprietary VLMs suggests generality.  \n- **Empirical signal.** Visual‑only ablations show images alone can tilt preferences; cross‑model transfer indicates the method isn’t tightly coupled to one backbone.  \n- **Clarity & scope.** The manuscript is mostly clear, figures are helpful, and the evaluation spans two environments rather than a single toy task.  \n- **Significance (conditional).** If claims held under realistic constraints and stronger defenses, the impact for agent‑safety and content‑moderation would be substantial."}, "weaknesses": {"value": "1. I think the largest gap is between the paper’s **practicality narrative** and the operational reality it actually assumes. The method leans on conditions that are hard to guarantee in production: awareness of or conditioning on the user’s current goal, the ability to **probe the deployed agent repeatedly** and **adapt** based on its responses, and the freedom to **continuously modify** a listing’s text and thumbnail until the manipulation “sticks.” In real marketplaces, repeated probing runs into rate limits and caching; text/image edits are often throttled or moderated; rendering to the *same* user session isn’t instantaneous or even guaranteed; and you rarely see the end‑user’s exact prompt. Even if the visual perturbations can be prepared offline, the paper’s **textual loop** explicitly depends on multiple online interactions. I would need to see results under **tight edit/eval budgets** (e.g., one shot or a handful of cycles), without access to user‑specific prompts, and with realistic delays and platform constraints to believe this is meaningfully more “practical” than the attacks the authors criticize.\n\n2. Closely related—and, to me, just as serious—is the **tension between the claimed “semantics‑preserving” textual edits** and the examples shown. The narrative says the text stays truthful and policy‑compliant, but the qualitative case freely drifts genre/attributes. If the attack is allowed to **persuade with unconstrained language** (e.g., “this is the best choice for you,” or by reframing content to match the agent’s latent preferences), then the hard part isn’t *making* the agent pick the item—of course persuasive text can sway an LLM agent—it’s doing so while **remaining faithful to the original item** and staying within platform rules. That’s the standard they set for themselves, and I don’t see it met. Conversely, if the authors lean into the persuasion framing, then **detectability becomes the core question**: show that such edits aren’t trivially caught by policy filters, cross‑modal consistency checks, or simple rule‑based moderation. Right now the paper sits in an uncomfortable middle: enough freedom to make the attack work, but not enough constraint to substantiate the “benign/stealthy” claim.\n\n3. I also find the **stealth/detection evaluation too narrow** to support strong claims. A single large model, prompted as a detector and told in advance that exactly one of the items is adversarial, is not a robust proxy for a platform defense. That prior (“1‑in‑$K$”) makes the task much easier than **binary detection** in the wild, and using a model from the same family as the agent invites correlated blind spots. If stealth is part of the headline, I would expect heterogeneous detectors (a vision‑only forensics model, an independent VLM, a cross‑modal consistency scorer), *plus* human raters for perceptual visibility, and reporting that looks like **AUC/ROCs and calibrated operating points**, not just a single MDR number.\n\n4. On **reproducibility and statistical rigor**, the paper comes up short. The visual attack hinges on multi‑crop PGD with an $\\ell_\\infty$ budget, but the **numerical hyperparameters** that actually matter (step size, number of steps, number of crops, outer‑loop iterations/early stopping) are not all specified. The text‑refinement loop similarly lacks **clear stopping rules, query counts, and search/beam parameters**. Main tables appear to report single percentages (often over 100 trials) **without uncertainty**. Also, there are no confidence intervals or variance across seeds/sessions.\n\n5. There’s also a bit of **threat‑model drift** in the methodology. The paper self‑identifies as black‑box with respect to the victim agent, yet it relies on **surrogate models** (and, at points, surrogate introspection) to guide optimization and concept search. That’s a reasonable engineering choice, but in practice an attacker may not have access to a high‑fidelity surrogate—or, more importantly, a way to validate that the surrogate’s gradients align with the victim’s behaviors on the specific UI. The paper can benefit from squarely addressing how much of the reported success depends on this surrogate alignment, and how often the attack fails when the surrogate is misspecified.\n\n7. The **external validity** of the evaluation is limited. The two environments—a synthetic movie page and a benchmark shopping setting—are useful, but they don’t capture the messiness of live e‑commerce or content platforms. Real agents often mix page perception with **non‑visual ranking signals** (price, stock, popularity, reputation), and small changes in UI or copy can be overridden by those signals. I don’t see evidence that CPS meaningfully moves choices when such competing signals are present, or when the page layout, fonts, and component positions deviate from the testbed.\n\n8. On the **imperceptibility** claim for images, I remain unconvinced without **perceptual metrics** or a human study. An $\\ell_\\infty$ budget of $8/255$ is frequently used, but depending on content and scale it can still introduce visible speckling or edge artifacts, especially after resampling. A small LPIPS/SSIM/PSNR report (or a blinded user test) would make the “imperceptible” adjective feel earned rather than asserted."}, "questions": {"value": "Every item raised in the Weaknesses section can be viewed as a question for the authors (I'm particularly interested in the first 2 items). \nI may well be mistaken on several of these points, and I would sincerely appreciate clarification or correction wherever appropriate.\nIf the authors can address or resolve even part of these concerns—whether by showing that I misunderstood something or by providing additional detail—it would be very helpful."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Potentially harmful insights, methodologies and applications", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "*   I see clear dual‑use risk: the method is explicitly designed to manipulate autonomous agents on consumer platforms. If operationalized, it could be used to steer purchases or decisions without users’ awareness.\n*  The attack scenario plausibly conflicts with typical marketplace ToS (manipulative content, false or misleading claims, adversarial modifications). If data, code, or demos encourage violating site policies or scraping constraints, that also warrants scrutiny.\n*  The paper offers step‑by‑step guidance for cross‑modal manipulation. Without strong guardrails (e.g., release restrictions, red‑teaming, mitigation guidance), I worry about straightforward weaponization.\n*  If the authors release perturbations, prompts, or a live demo, I’d like to see a concrete harm‑mitigation plan (rate‑limits, gating, detection baselines) and clarity around any human evaluations (consent/compensation)."}}, "id": "qHkT7naOse", "forum": "dcKQf6jUtr", "replyto": "dcKQf6jUtr", "signatures": ["ICLR.cc/2026/Conference/Submission17233/Reviewer_Hgoh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17233/Reviewer_Hgoh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761156241488, "cdate": 1761156241488, "tmdate": 1762927192406, "mdate": 1762927192406, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates cross-modal adversarial manipulation of web agents powered by vision–language models (VLMs). The authors propose Cross-Modal Preference Steering (CPS) — a black-box attack framework jointly optimizing imperceptible textual and visual perturbations to steer agent preferences during selection tasks such as movie recommendation or e-commerce ranking. Under a realistic constraint where the attacker can only modify their own listing’s image and text, CPS achieves high success rates (up to 71%) while maintaining low detection rates across models including GPT-4.1, Qwen-2.5-VL, and Pixtral-Large."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Timely and practically relevant topic: The work explores real-world vulnerabilities in AI web agents, which are becoming increasingly common in recommendation and decision systems.\n\n- Comprehensive evaluation: The experiments are extensive, covering multiple commercial and open-source VLMs, two distinct domains (movies, shopping), and metrics for both effectiveness and stealth."}, "weaknesses": {"value": "- The second contribution (black-box visual attack) adds limited conceptual value. While the paper’s second contribution highlights a black-box visual attack achieving imperceptible perturbations without model access, the value of this component is somewhat limited in light of recent progress in multimodal jailbreak research. Prior works such as Implicit Jailbreak Attacks via Cross-Modal Information Concealment (Wang et al., 2025), Jailbreak Large Vision-Language Models through Multi-Modal Linkage (Wang et al., 2024), and Visual Contextual Attack (Miao et al., 2025) have already demonstrated that imperceptible or hidden image-based manipulations can reliably steer the behavior of large VLMs under black-box conditions. Compared with these studies, the visual-attack module here mainly reuses known CLIP-transferable perturbation techniques and applies them within a web-agent scenario. Although the adaptation is technically solid and empirically strong, its conceptual novelty is modest. The paper would benefit from clarifying how this visual component differs substantively from previous imperceptible black-box perturbations beyond its integration into the CPS pipeline.\n\n  - Miao, Z., Ding, Y., Li, L., & Shao, J. (2025). Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection. arXiv preprint \n  - Wang, Y., Zhou, X., Wang, Y., Zhang, G., & He, T. (2024). Jailbreak Large Vision-Language Models Through Multi-Modal Linkage. arXiv preprint arXiv:2412.00473. \n  - Wang, Z., Wang, H., Tian, C., & Jin, Y. (2025). Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models. arXiv preprint arXiv:2505.16446.\n\n- The cross-modal synergy lacks mechanistic understanding. The third contribution—showing that combining textual and visual perturbations outperforms single-modal attacks—is purely empirical. The work does not analyze why such synergy occurs, whether it arises from attention redistribution, representation coupling, or bias amplification within multimodal fusion layers. Without internal analyses such as attention or gradient tracing, mutual-information measurement, or ablation on modality interactions, the study reads more like an empirical report than a scientific explanation. Adding interpretive experiments to uncover the mechanism behind the joint optimization would greatly strengthen the contribution.\n\n- Defense and ethical analysis remain shallow. The defense analysis (Table 2) only uses an informed GPT-4.1 detector and reports near-random detection rates. There is little discussion on countermeasures, such as adversarial training, cross-modal consistency checks, or human-in-the-loop oversight."}, "questions": {"value": "- How does the proposed visual attack technically differ from existing imperceptible black-box perturbation or multimodal concealment methods\n- For inspiration, the authors might look at Contrasting Subimage Distraction Jailbreaking (Yang et al., 2025), which provides a good example of how deeper mechanistic analyses can reveal why multimodal attacks succeed. The intention is not to apply that specific method here, but to encourage a similar level of analytical depth in explaining the underlying dynamics of CPS.\n  - Yang, Z., Fan, J., Yan, A., Gao, E., Lin, X., Li, T., ... & Dong, C. (2025). Distraction is all you need for multimodal large language model jailbreaking. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 9467-9476)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eOgoar5aqI", "forum": "dcKQf6jUtr", "replyto": "dcKQf6jUtr", "signatures": ["ICLR.cc/2026/Conference/Submission17233/Reviewer_v9mh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17233/Reviewer_v9mh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935403139, "cdate": 1761935403139, "tmdate": 1762927192030, "mdate": 1762927192030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Cross-Modal Preference Steering (CPS), an adversarial attack approach to steering VLM's decision making. The authors formalize a realistic attack scenario and achieves >50% success rate on shifting VLM's perceptions on images without requiring knowledge of such block-box models. Specifically, the paper introduces noise optimization for visual content perturbation and jointly optimizes for text description to inject preference for a specific concept. Visual content perturbation is done through CLIP optimization and text optimization utilizes white-box VLM such as Qwen-2.5VL-32B for attacker to monitor changes in logits and bias selection probability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a novel joint optimization approaches to bias a VLM's preference for a specific visual content.\n- It is successfully demonstrated that this process works for commercial models such as GPT-4.1 and does not require any knowledge about the target models themselves. \n- Comprehensive analysis and ablation studies are done on the effectiveness of joint visual-text perturbation over single-modal attacks, and results show that some models are more susceptible to attacks in the visual domain."}, "weaknesses": {"value": "- The attack optimization still requires white-box models as surrogates. How can we reason that this optimization transfers to other black-box models? If a black-box model is using very different visual encoder, this attack approach would not make much sense?\n- Some of the notations are confusing, such as in eq. 9, what is $\\Pi$? and what is the argument to $\\mathcal{L}$ in eq. 8, and how is this used in equation 10, which seems inconsistent with its usage in eq. 12"}, "questions": {"value": "I'd like the author to address some concerns above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "6BcDG4w1PT", "forum": "dcKQf6jUtr", "replyto": "dcKQf6jUtr", "signatures": ["ICLR.cc/2026/Conference/Submission17233/Reviewer_tazK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17233/Reviewer_tazK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981143143, "cdate": 1761981143143, "tmdate": 1762927191662, "mdate": 1762927191662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes *Cross-Modal Preference Steering*, an attack framework that modifies an item's thumbnail and description to bias VLM web agents under a black-box setting where the adversary can only edit their own listing. It formalizes the threat, defines manipulation and detection objectives, and presents an agent pipeline and optimization procedures for textual refinement and transferable visual perturbations, with evaluations on movie-selection and shopping-style tasks using multiple agent backbones, as well as a detector-based analysis and ablations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper states a practical black-box threat model aligned with real content-publisher permissions.\n- The paper is well organized and easy to follow."}, "weaknesses": {"value": "- The method aims to exploit \"RLHF-induced preference biases\" (Section 4.5.1) in descriptions, yet the paper does not isolate which biases drive selection shifts for each backbone or provide targeted probes.\n\n- Beyond noting survival under downsampling/cropping, the paper does not test JPEG recompression, bit-depth changes, or typical input sanitization. Input transforms and real-world variations can sharply alter attack success, so attack robustness here is unclear.\n\n- The threat model assumes the attacker \"can observe user queries\" and then tailor cross-modal edits (Section 3.1), which many real marketplaces do not expose to individual sellers. This suggestion might be strong."}, "questions": {"value": "- How does attack success change when using strong black-box transfer methods like MI-FGSM?\n- For one end-to-end attack, how many surrogate VLM evaluations, GPT-4.1 calls, and optimization steps are used on average, what is the wall-clock time and cost per targeted item, and how do success rates degrade under reduced budgets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "na8thT2eQB", "forum": "dcKQf6jUtr", "replyto": "dcKQf6jUtr", "signatures": ["ICLR.cc/2026/Conference/Submission17233/Reviewer_S5C5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17233/Reviewer_S5C5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989345996, "cdate": 1761989345996, "tmdate": 1762927191281, "mdate": 1762927191281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}