{"id": "cu8z4V1nJt", "number": 15415, "cdate": 1758251129419, "mdate": 1763757187714, "content": {"title": "Grounding as Feedback: Vision-Language Alignment via Sampling-based Visual Projection", "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable potential in integrating visual and linguistic information, but frequently produce text that is unfaithful to the visual world, leading to object hallucinations and inaccurate descriptions that limit their real-world applicability. To address this core problem, we introduce Sampling-based Visual Projection (SVP), a framework for guided self-improvement that efficiently enhances vision-language alignment. Our key insight is to use a pre-trained grounding model as an expert guide to provide feedback on descriptions generated by the VLM itself. SVP uses this feedback in an iterative loop to score, select, and adapt the VLM on high-quality, feedback-refined samples. This process transfers the spatial reasoning skills of the expert guide into the generalist VLM without requiring new, manually curated text-image pairs or preference annotation. Our experiments show that SVP yields significant gains across a range of tasks, including a 14\\% average improvement in captioning and a 12\\% increase in object recall, significantly reduced hallucinations, while maintaining question-answering capabilities. The result is a more robust and reliable VLM, demonstrating that targeted, feedback-driven improvement is a powerful method for enhancing vision-language alignment.", "tldr": "Leveraging self-captioning and grounding feedback to elicit latent information in vision-language models", "keywords": ["vision-language-models", "visual-grounding", "feedback-based-alignment"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd4fe81fcb2798771aaf5648159b1e95e3a89fe2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tackles unreliable vision–language alignment in VLMs (object hallucinations, missed objects) and proposes Sampling-based Visual Projection (SVP), a guided self-improvement framework. SVP (i) has the base VLM self-caption images, (ii) feeds those captions to an external grounding model (e.g., GroundingDINO) to obtain spatial feedback, then (iii) scores, selects, and fine-tunes the VLM on the highest-quality, feedback-refined samples using lightweight LoRA. Two scoring variants (log-ratio and weighted-difference) identify captions where grounding most changes the model’s token probabilities; two training variants are explored: The approach avoids new manual annotations or preference data, but depends on a competent grounding model and brings benefits to spatially grounded tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tQuality and breadth. The paper reports consistent gains across ten standard benchmarks spanning six task families (captioning, referring expressions, hallucination control, object recall, VQA, and multitask), with results summarized clearly and backed by extensive comparisons. \n2.\tClarity and reproducibility. The writing is generally clear, with informative figures/graphical models, explicit algorithm overviews, and implementation details. The appendices provide pseudo-code, prompt templates, and full hyperparameters, which materially aid reproducibility."}, "weaknesses": {"value": "1.\tThe comparison is unfair. Although there are no new human annotations, the inner loop explicitly constructs a curated pseudo-labeled set guided by the grounder, which functions as additional supervision.\n2.\tEvaluation granularity for localization. Referring performance is summarized with language-centric metrics (e.g., CIDEr), but the paper does not report region-level localization metrics (e.g., bbox/mask IoU or Acc@0.5) that would more directly substantiate the “spatial alignment” claims.\n3.\tDependence on an external grounder. The pipeline hinges on a pretrained grounding model (e.g., GroundingDINO-tiny) to generate feedback and curate data, which may limit portability in domains lacking strong grounders or where their biases are problematic."}, "questions": {"value": "1.\tIs this method essentially a method of using ground truth data for data augmentation? That is, after the location output of GDINO is textified, \"more easily aligned\" pseudo-label pairs are constructed and selected, which can be regarded as \"grounded-driven data augmentation + selection\", and then fine-tuned?\n2.\tAdd localization metrics. Could you report standard localization metrics (e.g., bbox/mask IoU, Acc@0.5 on RefCOCO/+/g) in addition to CIDEr, to directly evidence spatial alignment improvements?\n3.\tTop-fraction discrepancy. Figure 5 states that the inner loop “selects the top 20%,” whereas §4.1/Implementation Details say the pipeline “selects the top 10%.” Which fraction is actually used for your headline results and for ablations? Please reconcile these settings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cxhD08LGVC", "forum": "cu8z4V1nJt", "replyto": "cu8z4V1nJt", "signatures": ["ICLR.cc/2026/Conference/Submission15415/Reviewer_CBKU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15415/Reviewer_CBKU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761032617705, "cdate": 1761032617705, "tmdate": 1762925692455, "mdate": 1762925692455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses how to address VLMs's hallucination and information omission by proposing the Sampling-based Visual Projection (SVP). Unlike existing methods (e.g., SFT) that rely on expensive human annotations or curated preference data, SVP leverages a pre-trained grounding model (GroundingDINO-tiny here) as an expert feedback tool to enable VLM self-improvement.​ The SVP framework operates in three iterative stages: (1) Prior Sampling, where the base VLM generates flawed descriptions without grounding guidance; (2) Grounding Feedback, where the grounding model provides object bounding boxes, category confidence scores to identify hallucinations or omissions; (3) Guided Sampling, where the VLM generates refined, alignment-enhanced descriptions using the grounding feedback. These generated samples are then scored via log-ratio or weighted difference metrics to select high-quality pairs (Top 10–20%), which are used to fine-tune the VLM via LoRA (low-rank adaptation).​ Evaluations across benchmarks demonstrate SVP’s efficacy: it achieves an average 14% improvement in caption generation (CIDEr score), a 12% boost in object recall, and a POPE F1 score of 88.33 (reducing hallucinations). Notably, SVP preserves multi-task capabilities (e.g., VQA on ScienceQA) and generalizes to diverse VLM architectures (LLaVA-1.5/1.6/OV) and scales (0.5B–13B parameters)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Annotation Efficiency: SVP eliminates the need for additional human labels or preference data by using self-generated samples and machine feedback from grounding models. This addresses a major cost bottleneck of existing VLM optimization methods (e.g., SFT requires millions of annotated image-text pairs).\n\nMulti-Task Preservation: Unlike methods that overfit to single tasks (e.g., caption-only fine-tuning), SVP maintains VQA performance (ScienceQA accuracy: 78.54 → 78.40) and even improves multi-task capabilities (MMBench accuracy +9.76%), making it suitable for real-world multi-modal applications."}, "weaknesses": {"value": "Missing 3D Depth and Irregular Object Size Cues: The grounding model relies on 2D bounding boxes (bboxes) to provide spatial feedback, which only captures planar (left/right, top/bottom) coordinates, however,  3D depth information (e.g., \"the cup in front of the book\" vs. \"the book behind the cup\") is entirely absent. This leads to ambiguous spatial descriptions for scenes with overlapping objects. Additionally, 2D bboxes fail to represent irregularly shaped objects: the rectangular bbox includes irrelevant background pixels, making it impossible for the VLM to infer the object’s actual size or shape (e.g., describing a small coiled cable as a \"large rectangular object\").\n\nUnaddressed Efficiency Comparison with Automated High-Quality Data SFT: SVP’s core value lies in avoiding manual annotation via self-evolution, but it does not compare with an alternative scenario: automated generation of high-quality annotated data (e.g., Image-Textualization [NeurIPS 2024]). If such automated data is available, a single-step SFT (instead of SVP’s iterative sampling-scoring-adaptation) might be more efficient since it requires less computational overhead (no multi-round sampling) and shorter training cycles (direct parameter update with high-quality data). The paper’s failure to evaluate this trade-off leaves a gap in assessing SVP’s efficiency in data-rich, automated annotation scenarios.​\n\nDependence on Grounding Model Quality: SVP’s performance is tightly coupled to the grounding model’s accuracy. If the grounding model fails (e.g., misdetecting objects in occluded/blurry images or misclassifying rare items), the feedback becomes unreliable, degrading SVP’s optimization effect. The paper does not test SVP with low-precision grounding models, limiting its robustness analysis.​"}, "questions": {"value": "1. The paper only uses GroundingDINO-tiny (a lightweight model) for feedback. How would SVP perform with more powerful grounding models? Would higher-precision feedback further enhance alignment, or introduce over-reliance on the grounding model’s biases?\n\n2. Given the computational overhead of K=20 samples, is there a way to dynamically adjust K while maintaining SVP’s performance? Could techniques like active learning select the most informative samples to minimize K?\n\n3. Would SVP extend to video-language models (e.g., Video-LLaVA)? Video introduces temporal dynamics (e.g., object motion) that static grounding models cannot capture, which is more exciting than just focusing on static images.\n\n4. The paper tests up to 3 SVP iterations. Could multi-round iterations (e.g., 5+ rounds) lead to overfitting to the grounding model’s systematic misdetections? How to balance alignment improvement and overfitting prevention in long-term self-improvement?\n\n5. The paper evaluates SVP on daily-scene datasets (COCO, Flickr30k). Would SVP work for specialized domains (e.g., remote sensing, pathology), where objects have unique visual features (e.g., medical scans of tumors)? Would pre-training the grounding model on domain-specific data be a prerequisite?\n\n6. How could SVP integrate 3D grounding feedback (e.g., from depth estimation models like MonoDepth + GroundingDINO, or 3D detectors like VoteNet) to capture depth relationships? For irregular objects, would replacing 2D bboxes with segmentation masks (seg masks) improve size/shape inference, and how would this change the scoring metric (e.g., log-ratio adapted to mask overlap instead of bbox coordinates)?\n\n7. For cluttered scenes (e.g., a desk with overlapping notebooks, pens, and a phone), would 3D depth feedback enable SVP to generate more accurate spatial descriptions (e.g., \"the pen on top of the notebook\" vs. \"the notebook under the pen\")? Could this reduce ambiguity in referring expressions (e.g., RefCOCO+ tasks requiring fine-grained spatial reasoning)?\n\n8. If automated tools generate 100k high-quality image-text pairs (with 3D depth, size, and shape annotations), how would a single-step SFT compare to SVP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "anA3awmdxr", "forum": "cu8z4V1nJt", "replyto": "cu8z4V1nJt", "signatures": ["ICLR.cc/2026/Conference/Submission15415/Reviewer_hcG4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15415/Reviewer_hcG4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890122338, "cdate": 1761890122338, "tmdate": 1762925692006, "mdate": 1762925692006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address hallucinations in VLMs concerning objects, locations, and their relationships. It proposes a method that uses a grounding model to correct the VLM's intermediate reasoning steps. This approach has shown good results on LLaVA-based models."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Using an expert model, like a grounding model, to fix the VLM's internal steps is a good idea to reduce hallucinations. Besides, the approach achieves good results on the benchmarks."}, "weaknesses": {"value": "1. A major weakness of this paper is its lack of clarity. For example, the authors should explicitly describe the inputs and outputs for each stage of their method in a clear, structured way. Currently, this information is scattered throughout the text, and in some cases, it seems to be missing entirely. This forces the reader to search for details or guess how the components connect, which makes the method very difficult to understand and reproduce.\n2. The experiments are a bit limited because they only use LLaVA-based models. The method itself seems general and should work for other models too. It would be much more convincing if the authors also showed results on different models, like Qwen-VL, to prove the method is widely useful.\n3. I am confused by the strong results on ScienceQA shown in Table 3. This benchmark seems to focus on science questions, not the object and location hallucinations this paper targets.\nWhy does the method work so well here? This needs to be explained. It makes me wonder if the performance gain is coming from another reason that isn't mentioned in the paper. The authors should clarify this point.\n4. The paper needs to explain how it gets the noun list for grounding (from Figure 5). This step is not described. What happens if this process misses important nouns? The authors should report the recall of their noun extraction method.\n5. The paper does not explain how the method disambiguates between multiple instances of the same object category (e.g., several different people). It is unclear how GDINO can ground the specific instance being referred to. I think it is a crucial limitation of this paper."}, "questions": {"value": "Please refer to the issues detailed in the Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WqsW7tqFtp", "forum": "cu8z4V1nJt", "replyto": "cu8z4V1nJt", "signatures": ["ICLR.cc/2026/Conference/Submission15415/Reviewer_nkcU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15415/Reviewer_nkcU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927878235, "cdate": 1761927878235, "tmdate": 1762925691368, "mdate": 1762925691368, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Sampling-based Visual Projection (SVP), a method for improving vision-language alignment in VLMs by using a existing grounding model to provide feedback on self-generated captions. Specifically, the proposed method use iteratively sampling descriptions, scoring them based on grounding feedback, and fine-tuning the model on high-quality samples. Experiments using LLaVA series models demonstrate improvements on image captioning and hallucination reduction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper investigates a well-understood problem in VLMs: the generated output may be inaccurate to the object, i.e., object hallucination.\n2. The proposed method avoids the need for new, large-scale human annotations or explicit preference pairs, which can be bottlenecks for alignment methods such as SFT and DPO.\n3. The experiments involve diverse benchmarks, tasks and model scales, which are relatively solid."}, "weaknesses": {"value": "1. The major concern about this paper is the core idea looks like distillation, where it distills some knowledge from the grounding model into the target VLMs. That means we still have to rely on a grounding model instead of truly self-improvement. The grounding model obviously requires training data (or even annotated).\n2. SVP's performance is capped by the quality of the \"expert\" grounding model, which should be further analyzed. If the grounding model provides incorrect feedback, SVP can make the target model degenerate. Also, it seems like the method does not work well on Multitasking in table 3. It is also suggested to experiment with different grounding models.\n3. Additionally, SVP requires K\\=20 samples per image and iterative fine-tuning, making it much more costly than standard method. It would be good to see more analysis on this.\n4. While I don't carefully check Appendix F, I think the connection to ELBO is informal and may even be wrong. SVP doesn't actually optimize the ELBO in any way. And the claim about \"posterior inference\" is overclaimed."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DpVHdL8ZB1", "forum": "cu8z4V1nJt", "replyto": "cu8z4V1nJt", "signatures": ["ICLR.cc/2026/Conference/Submission15415/Reviewer_tCTj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15415/Reviewer_tCTj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762392856306, "cdate": 1762392856306, "tmdate": 1762925690311, "mdate": 1762925690311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}