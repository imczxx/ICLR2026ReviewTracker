{"id": "ABp8HrvzJJ", "number": 7180, "cdate": 1758010742509, "mdate": 1759897868322, "content": {"title": "BioRNN: Bio-Inspired Synergistic Integration of Neuromodulation and Wave Propagation in Recurrent Networks", "abstract": "Training recurrent networks that directly implement physical wave equations has been hindered by numerical instability and incompatibility with gradient-based optimization. We introduce BioRNN, a recurrent architecture that embeds two-dimensional wave propagation dynamics on a neural grid and achieves stable training via a mixed finite-difference scheme with learnable damping. Inspired by neuromodulation in biological systems, BioRNN incorporates a lightweight frequency-modulation stage that transforms inputs into oscillatory patterns, enabling the recurrent layer to exploit resonance and frequency selectivity. This combination allows BioRNN to model spatiotemporal dependencies through constructive interference while retaining theoretical guarantees of stability during backpropagation. On sequential visual (sMNIST, noisy CIFAR-10) and auditory (ESC-50) benchmarks, BioRNN achieves competitive performance across domains, with pronounced gains on frequency-rich auditory tasks and comparable accuracy on vision. This work demonstrates that integrating biologically inspired neuromodulation with physically grounded wave dynamics yields recurrent models that are both biologically grounded and reliably trainable within modern deep learning.", "tldr": "", "keywords": ["Bio-inspired neural networks", "Wave propagation", "Oscillatory dynamics", "Spatiotemporal dynamics", "Frequency-selective processing"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8cd296a37e0573b513882ab383fdcf095a66519.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces an input-modulated wave-based RNN architecture that leverages oscillatory resonance to process sequences. The authors provide theory demonstrating that this model has robust spectral properties, and further demonstrate that it has decently strong performance on long sequence modeling tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper makes an interesting connection between input modulation and wave dynamics, relating this to neuromodulation and spatiotemporal dynamics in biological neural networks. \n- The topic is very relevant for the NeuroAI community where modulation effects have yet to be abstracted to a computationally useful level, especially in combination with wave dynamics. \n- The plots studying performance vs. model parameters are welcome and interesting. \n- The ablation study in Table 3 is helpful to understand the impact of the different model components, and highlights the difference between visual and audio processing."}, "weaknesses": {"value": "-  The discussion of the input modulation in Section 2.1 is too short and may lead to confusion. It would be helpful if some examples of modulators could be included earlier. \n- The main results in Table 2 have no error bars or standard deviation. While I know this is standard for the field, this makes the minor differences between models mean relatively little in practice. \n- The authors claim \"In summary, the modulator acts as a task-adaptive transformation that aligns well with BioRNN’s resonant dynamics, enabling competitive vision performance and clear advantages on spectrally rich audio tasks.\" However, the high performance of the BioRNN on the ESC50 task (where it performs the best) appears to not be due to the modulation (modulation only helps slightly). \n- Similarly to the above, the main contribution of this paper appears to be the input modulation, however, this appears to have only a minor impact on model performance. If the authors could clearly enumerate their contributions that would help greatly. \n- The paper accentuates the importance of input modulation and resonant dynamics, but the theory section does not focus on this and instead focuses on stability. The empirical results are then only minimally supportive of the benefits of these dynamics."}, "questions": {"value": "- In equation three you say: for efficient computation we re-parameterize o'=co. How does this enable efficient computation? And efficiency in what sense?\n- Why does the modulation help so significantly on sequential MNIST, but appears to only have a minor effect on other tasks? \n- Is there any more concrete or analytic way that one can interpret the interaction of the input modulation and the recurrent dynamics? \n- In the ablations, how do you disable wave propagation but retain spatial coupling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dDTk6AY28X", "forum": "ABp8HrvzJJ", "replyto": "ABp8HrvzJJ", "signatures": ["ICLR.cc/2026/Conference/Submission7180/Reviewer_do4L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7180/Reviewer_do4L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938096531, "cdate": 1761938096531, "tmdate": 1762919340249, "mdate": 1762919340249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BioRNN, an RNN that first modulates input sequences with time-dependent M(t), before passing the input to a 2D neural sheet that acts as the dynamical transition function of the RNN.\n\nThe authors go on to prove the stability properties of BioRNN - a property missing in existing approaches that incorporated physical waves into their dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Good empirical benchmarking against relevant baselines and similar approaches, with solid ablations to ensure value of modulation module (and other design choices). The paper provides rigourous stability guarantees, which are again missing in previous works (according to the authors - this is not my area of expertise)"}, "weaknesses": {"value": "Minor: citations should be in brackets, i.e. using \\citep\n\nNotation is confusing, given that IIUC x and y are used as both the input and the coordinates of a 2D input\n\nIt's also not immediately obvious how the 2D neural sheet relates to the transition function S - making this link more clear in the main text would help motivate the in-depth analysis below\n\nThere is not much done in the way of interpretability of the wave properties. Given the biological motivation of these design features, some comparison to the role of waves in biological neural networks would have been warranted.\n\nThere's no information about training time, memory requirements, or inference speed, which makes it difficult to assess practical trade-offs for the more sophisticated architecture."}, "questions": {"value": "Why were these specific forms of modulation used for each dataset? Did you try alternative forms/using modulation from different datasets?\n\nFurthermore, the modulation functions are manually designed for each dataset. This limits practical applicability - how would one design modulation for new tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gyfb1QROt4", "forum": "ABp8HrvzJJ", "replyto": "ABp8HrvzJJ", "signatures": ["ICLR.cc/2026/Conference/Submission7180/Reviewer_LSat"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7180/Reviewer_LSat"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762091866512, "cdate": 1762091866512, "tmdate": 1762919339848, "mdate": 1762919339848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BioRNN, a recurrent model that combines a lightweight input “modulator” with a 2D grid of units designed to mimic wave-like propagation and damping. The modulator transforms inputs into oscillatory patterns that the grid can process, aiming for an interpretable, physically inspired alternative to standard RNNs. Experiments on sequential MNIST, noisy CIFAR-10, and ESC-50 report competitive results. A stability-aware update scheme is proposed to improve reliability at larger time steps, and diagnostics relate performance to key hyperparameters. While promising—especially for audio—the approach is not consistently stronger than GRU baselines, and more controlled comparisons are needed to separate the impact of the modulator from the recurrent dynamics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea to explicitly separate neuromodulatory preprocessing from physically inspired wave dynamics is well-argued and visually communicated. \n- The p/o split and use of divergence/gradient operators make the mechanism interpretable as storage vs. transport.\n- The mixed forward/backward finite differences plus implicit damping are a effective contribution\n- Further investigations give transparency about which hyperparameters actually matter task-specifically.\n- Nice ablations show that wave propagation, spatial coupling, and the auxiliary field each add measurable value.\n\nPersonal note: I really like this approach as an unusual way to process time series data, using a modulator plus wave-like recurrent dynamics to spark rich transients, and I see real potential here for reservoir computing, even if the core is learned rather than fixed."}, "weaknesses": {"value": "- The paper does not convincingly demonstrate competitiveness. On ESC-50, BioRNN (mod.) is noticeably below a simple GRU. On nsCIFAR-10, BioRNN (54.2%) is far below coRNN. On sMNIST (not a good selection as a core benchmark anymore - why not at least permuted?) 98.1% cannot really be seen as competitive, because this is an accuracy that can be achieved easily with much simpler methods. In a nutshell, the approach’s absolute effectiveness even over standard gated RNNs is not yet demonstrated. A comparison against actual state-of-the-art recurrent sequence learning models (such as several state space model-like networks) is entirely missing.\n\n- The modulator is crucial for BioRNN (e.g. for sMNIST is causes a huge jumps), but LSTM/GRU are not evaluated with the same modulated inputs. Without that control, it’s hard to ascribe gains to the recurrent dynamics vs. the input transformation.\n\n- There is no information (plots) about the convergence behavior during training. Is there any advantage? Resonator-based neurons (damped harmonic oscillators), for instance, are known to converge much faster than usual RNN structures."}, "questions": {"value": "- What did the modulator learn? Can you visualize the learned f, ϕ, α (for audio) and their distribution over Mel bins/time? Do they concentrate on critical bands, or track class-discriminative frequencies? \n\n- Which boundary conditions are used on the grid and how sensitive are results to that choice? Just 0?\n\n- How well does BioRNN perform in comparison with strong recent sequence learning models (cf. weaknesses)?\n\nAs mentioned above, I appreciate the idea and find it genuinely interesting, but I cannot recommend acceptance in its current form, as the paper lacks sufficient evidence to support its potential. Instead, I have two suggestions:\n- Explore other problem domains where the system’s inductive bias may better match the task.\n- Consider developing this approach further in the direction of reservoir computing, where it could be a natural fit."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x6vKofyeVK", "forum": "ABp8HrvzJJ", "replyto": "ABp8HrvzJJ", "signatures": ["ICLR.cc/2026/Conference/Submission7180/Reviewer_U5uu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7180/Reviewer_U5uu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762248597716, "cdate": 1762248597716, "tmdate": 1762919339370, "mdate": 1762919339370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a recurrent neural network model that implements a physical wave equation in its recurrent dynamics, with two innovative features. First, it has an time-dependent input modulation scheme that adds oscillatory patterns to the inputs. Second, it uses a discretization scheme accompanied by theoretical stability guarantees. It finds that these features enhance model performance by experimentation and, in the case of the discretization scheme, mathematical analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The text was well-written, and the presentation was elegant and easy to follow.\n\nThe input modulation and discretization schemes appear innovative.\n\nThe motivation for combining short-term (waves) and long-term (neuromodulator) dynamics in RNNs is interesting."}, "weaknesses": {"value": "1. The biological motivation for BioRNN is weak. It is not at all clear why a physical wave equation should make for a more suitable recurrence in a biologically inspired model, in comparison to the oscillator networks mentioned in the paper. As for the time-dependent input modulation scheme, the introduction makes a vague link to neurotransmitters, but no compelling link. Appendix A.2 alludes to “neuroscience findings that emphasize the role of input-driven oscillations in shaping cortical activity,” but gives no citation. Therefore, the biological motivation for this network is unpersuasive.\n\n2. A major claim of the paper is that BioRNN resolves a widespread difficulty of training RNNs based on physical wave equations: “By embedding a mixed finite-difference scheme with learnable damping, BioRNN resolves the long-standing instability and incompatibility of physical wave equations with gradient-based training.” However, there is no experimental comparison between the chosen mixed finite-difference discretization scheme and other discretization schemes in BioRNN, nor is there any citation to support the claim of such a longstanding difficulty in the literature. Consequently, we do not know whether this really is a longstanding problem, or whether this choice of discretization scheme really eases training and performance in practice for BioRNN in practice. The results of applying this discretization scheme to other wave-equation RNNs could also be reported.\n\n4. The paper claims the time-dependent input modulation scheme as a major innovation. The comparisons to oscillator network RNN models might be an informative supplemental analysis, but why not compare BioRNN to other wave-equation RNNs, implemented with and without the input modulation? Ideally, one can show that this input modulation scheme improves performance for those models as well. If the paper claims the time-dependent input modulation operator as a key innovation, why not report the results of selectively ablating it, instead of jointly ablating the auxiliary field and the input modulation? \n\n5. The task performance results are not very strong."}, "questions": {"value": "What are the citations to support the claim of the \"long-standing instability and incompatibility of physical wave equations with gradient-based training\"?\n\nIt would be helpful to clarify in more detail the link between the input modulation and neurotransmitters."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "svKsBhM4o1", "forum": "ABp8HrvzJJ", "replyto": "ABp8HrvzJJ", "signatures": ["ICLR.cc/2026/Conference/Submission7180/Reviewer_QrBi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7180/Reviewer_QrBi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7180/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762520257277, "cdate": 1762520257277, "tmdate": 1762919338947, "mdate": 1762919338947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}