{"id": "2jkAk3EP0v", "number": 6214, "cdate": 1757958779526, "mdate": 1763759684907, "content": {"title": "Latent Thinking Optimization: Your Latent Reasoning Language Model Secretly Encodes Reward Signals in its Latent Thoughts", "abstract": "Large Language Models (LLMs) excel at problem solving by generating chain of thoughts in natural language, but such verbal thinking is computationally costly and prone to overthinking. Recent work instead proposes a latent thinking architecture Huggin-3.5B, which represents intermediate reasoning steps as sequence of latent representations. However, latent thoughts lack interpretability and are difficult to supervise, raising concerns about the correctness and reliability of its latent thinking processes. In this paper, we provide a systematic study of how Huggin-3.5B thinks in the latent space and how external supervision signals can improve its latent thinking processes. We show that latent thoughts leading to correct versus incorrect answers exhibit highly distinguishable patterns, and that a latent classifier can reliably predict answer correctness directly from latent thoughts. Leveraging these insights, we propose Latent Thinking Optimization (LTO), a probabilistic algorithm that employs the latent classifier as a Latent Reward Model (LRM) to optimize the latent thinking processes. Extensive experiments across diverse reasoning tasks demonstrate that LRM is highly effective in detecting incorrect latent thinking patterns, and LTO can significantly improve the latent thinking processes. Furthermore, we show that LRM can generalize across diverse domains, and LTO can be seamlessly applied to general LLMs to improve their thinking processes. In contrast to verbal thinking, our method demonstrates that reward modeling and scaling test-time thinking with supervision can be performed directly in the latent space, highlighting its potential as a general, efficient, and domain-agnostic approach to improving the thinking processes of LLMs.", "tldr": "We demonstrate that latent thoughts of LLMs contain rich reward signals, and scaling test-time thinking with supervision can be directly performed in the latent space.", "keywords": ["Latent representation learning", "scaling test-time compute"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dcd4279d48dc0afea2eb75a4754151b388941294.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies latent thinking in the latent-reasoning LM Huggin-3.5B and claims that correctness signals are encoded in latent trajectories. The authors (i) empirically show separable patterns between correct vs. incorrect latent thoughts via visualization and four representation metrics (entropy, effective rank, anisotropy, intrinsic dimension); (ii) train a latent classifier that predicts answer correctness from partial latent trajectories with high ROC-AUC; and (iii) propose Latent Thinking Optimization (LTO): use the classifier as a Latent Reward Model (LRM) and perform KL-regularized probabilistic selection of latent trajectories via an acceptance–rejection sampler with proofs (Thm. 1/2). LTO improves correctness over voting/self-correction and latent heuristics and is further applied to general LLMs by treating intermediate layer states as “latent thoughts,” showing sizable gains and cross-domain transfer of the LRM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. neat empirical evidence that correct trajectories differ from incorrect ones, with stepwise analyses and a probing classifier that improves with longer prefixes. \n\n2. LTO formulates a KL-regularized objective and provides a closed-form reweighting plus an accept-reject sampler with correctness guarantees (Thm. 1/2). \n\n3. Broad task coverage & signal utility: improvements across math/commonsense/code; LRM used as weights already helps; LTO helps more. \n\n4. Generality claims with initial evidence: application to general LLMs (OLMo/Llama/Mistral) and cross-dataset LRM transfer; training-data footprint noted as modest."}, "weaknesses": {"value": "1. Entropy/effective-rank/anisotropy/intrinsic-dimension are scale- and whitening-sensitive; without controlling for per-step activation scaling or layer-norm statistics, a change in metric may reflect variance rescaling rather than “better thinking.” Please specify invariances and show robustness across metric definitions and layers.\n\n2. The LRM is trained from the same model’s latent trajectories using final-answer correctness as labels; if the generator exhibits systematic artifacts (e.g., decoding shortcuts at later steps), the classifier may learn spurious correlates. The strong AUC near the end steps could partially capture answer-formation traces rather than “process quality.” A control where latents are early-stopped and the answer is masked/not decoded would help.\n\n3.  The reward signa is derived from LTO are binary and can only indicate if the latent thinking processes will lead to the correct answer, as you mentioned in appendix"}, "questions": {"value": "1. As mentioned in Author's guidence, you should disclose the use of Large Language Models. I don't find it in your paper. Could you disclose it here? or you may violate the guidence and lead to desk rejection.\n\n2. Please see weaknesses and try to answer 1-2. For 3, I am curious about methods you have tried."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bxA5kWLGBn", "forum": "2jkAk3EP0v", "replyto": "2jkAk3EP0v", "signatures": ["ICLR.cc/2026/Conference/Submission6214/Reviewer_qGuA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6214/Reviewer_qGuA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921491274, "cdate": 1761921491274, "tmdate": 1762918549787, "mdate": 1762918549787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how reasoning language models internally represent their “thoughts” in latent space. Using the Huggin-3.5B model, the authors show that hidden-state trajectories corresponding to correct and incorrect answers form clearly separable patterns. They train a small latent classifier to detect these patterns and use it as a Latent Reward Model (LRM) in a new method called Latent Thinking Optimization (LTO)—a test-time sampling procedure that reweights latent trajectories toward those predicted to be correct. Without retraining the base model, LTO consistently improves reasoning accuracy across five benchmarks and generalizes to other LLMs (e.g., Llama-2, Mistral), demonstrating that latent representations implicitly encode reward-like signals useful for optimizing reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* The author conduct in-depth analysis of the latent reasoning trajectories, both qualitatively and quantitatively through metrics including entropy, effective rank, anisotropy, and intrinsic dimension.\n* The proposed method is simple that only involves computing evaluation metrics as input features to a small LRM, which is later used to guide the selection of answers through LTO.\n* The paper is well organized, balancing theoretical exposition with extensive experiments and visualizations."}, "weaknesses": {"value": "* Optimization vs. resampling: LTO is effectively a weighted rejection sampling procedure, not true optimization of the model’s latent policy; the optimization problem formulation is confusing. Also, it not clear if the KL divergence constraint is actually needed. \n* Limited interpretability: Despite excellent quantitative separation, there is little qualitative analysis linking specific latent dimensions or manifolds to interpretable reasoning concepts.\n* Minor overclaiming: Phrases like “generalist reward model” or “domain-agnostic optimization” are ambitious relative to the experimental evidence."}, "questions": {"value": "Q1: can author explain the KL divergence term. This is is common in LLM post-training, but LTO is essentially not a optimization method, the distribution won't deviate from base policy a lot.\nQ2: can author provide more insight about why correct and incorrect latent trajectories lead to different patterns? For example, why correct latent trajectories tend to be more compact dispersed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Dcp8et1am4", "forum": "2jkAk3EP0v", "replyto": "2jkAk3EP0v", "signatures": ["ICLR.cc/2026/Conference/Submission6214/Reviewer_jnr6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6214/Reviewer_jnr6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957545944, "cdate": 1761957545944, "tmdate": 1762918549416, "mdate": 1762918549416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper first empirically investigates the property of latent in latent thinking LLMs, such as Huggin-3.5B to understand their internal reasoning processes. They discover that the latent trajectories leading to correct versus incorrect answers are highly distinguishable and provide both visual and quantitative measurements to support this claim. Based on this finding, they successfully train a Latent Reward Model (LRM) that predicts the correctness of the final answer directly based on the sequence of latent thoughts. Finally, the paper proposes Latent Thinking Optimization (LTO), a probabilistic sampling algorithm that leverages the LRM's reward signal to guide the model's generation at test-time. This LTO algorithm is supported by both theoretical justification and strong empirical results, demonstrating performance improvements on math, coding, and general QA tasks.\n\nIn general, this paper validates the separation of correct/incorrect latent trajectories and train a classifier LRM that predicts the trajectory correctness sorely based on the latent. Using LRM as reward model to guide the sampling with LTO yields better results than other test-time scaling method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation and empirical insight: convincingly shows correct vs. incorrect latent trajectories are separable, supported by PCA visualizations and four representation metrics (entropy, effective rank, anisotropy, intrinsic dimension). A lightweight sequence classifier (LRM) can achieve strong predictive power.\n2. The sampling algorithm LTO is principled and theoretically sound.\n3. Consistent empirical gains on math/coding/QA vs. majority voting, self-correction, and latent heuristics (CoE-R/CoE-C)."}, "weaknesses": {"value": "1. Primary Application Targets a Niche Architecture: The paper's methodology is heavily anchored to the Huggin-3.5B model, which features a specific, recurrent \"latent thinking\" architecture. This model is not a widely adopted or standard foundation model, making it a niche target. \n2. Ambiguous Application to General LLMs. The paper's method for applying LTO to standard transformers (like Llama 2 or Mistral) is conceptually ambiguous. In section 6, the author mentioned that \"train LRMs using the latent representations from general LLMs\". In the appendix D, the authors state \"The latent representations of general LLMs from all the layers are regarded as latent chain of thoughts.\". Does that implies that hidden states in all layers are concatenated together as input to LRM? Meanwhile, efficiency of such methods on the application to general LLM is not sufficiently discussed.\n3. Limited Evaluation Scope for General LLMs: The experiments on general LLMs are constrained to relatively weak benchmarks such as GSM8k, it is unclear if these gains would translate to more capable models on frontier benchmarks."}, "questions": {"value": "1. Can the authors specify how multi-layer latents in general LLM are composed for the LRM as well as the efficiency analysis of LRM on general LLMs?\n2. Can the authors provide more experiment results for general LLM on recent benchmarks such as MATH/GPQA-Diamond/AIME?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hfB73Xe7OQ", "forum": "2jkAk3EP0v", "replyto": "2jkAk3EP0v", "signatures": ["ICLR.cc/2026/Conference/Submission6214/Reviewer_Pyxq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6214/Reviewer_Pyxq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971314075, "cdate": 1761971314075, "tmdate": 1762918548808, "mdate": 1762918548808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates latent thinking in reasoning language models, focusing on the Huginn-3.5B model which generates intermediate reasoning steps as latent representations rather than natural language. The authors analyze differences in latent thought trajectories between correct and incorrect answers using visualizations and representation quality metrics, revealing distinguishable patterns. They train a latent classifier as a Latent Reward Model (LRM) to predict correctness from these trajectories and propose Latent Thinking Optimization (LTO), a probabilistic sampling algorithm that optimizes latent policies to favor trajectories likely to yield correct answers. Experiments on math and programming tasks show improvements, and the approach is extended to general LLMs with cross-domain generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The exploration of latent thinking patterns provides novel insights into how LLMs encode reasoning internally, bridging cognitive science inspirations with practical LLM analysis, which could inspire future work on interpretable latent spaces.\n\n* LTO is a computationally efficient alternative to verbal chain-of-thought methods, avoiding overthinking and verbosity, with theoretical grounding in reward optimization and empirical gains on benchmarks like SVAMP and MBPP.\n\n* The generalization of LRM and LTO to standard LLMs (beyond Huginn-3.5B) and across domains is promising, suggesting a scalable, domain-agnostic way to enhance LLM reasoning without heavy natural language generation."}, "weaknesses": {"value": "* The analysis heavily relies on Huginn-3.5B, a specific latent reasoning model; while extensions to general LLMs are claimed, the paper lacks detailed comparisons or ablation on how well LTO performs on diverse LLM model families.\n\n* Evaluation metrics for latent representations (e.g., entropy, anisotropy) are insightful but somewhat indirect; the paper could benefit from more direct interpretability probes or causal interventions to confirm that observed patterns truly reflect \"thinking\" rather than spurious correlations or memorization artifacts.\n\n* Experimental details on training the LRM (e.g., data scale, hyperparameters) are sparse in the provided sections, and results focus on two datasets; without broader benchmarks or robustness tests (e.g., against adversarial inputs), it's unclear how LTO scales to real-world, noisy reasoning scenarios."}, "questions": {"value": "* How sensitive is LTO to the number of sampled trajectories N or the KL regularization weight β? Did you observe trade-offs between optimization gains and deviation from the reference policy?\n\n* The paper mentions cross-domain generalization of LRM with small training data; what specific domains were tested for transfer (e.g., beyond math/programming to commonsense or scientific reasoning), and how much data was used for fine-tuning in those cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PCKfJwpdzy", "forum": "2jkAk3EP0v", "replyto": "2jkAk3EP0v", "signatures": ["ICLR.cc/2026/Conference/Submission6214/Reviewer_aTJi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6214/Reviewer_aTJi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021173003, "cdate": 1762021173003, "tmdate": 1762918548222, "mdate": 1762918548222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}