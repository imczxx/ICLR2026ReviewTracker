{"id": "qyjaVda7t2", "number": 9568, "cdate": 1758128021765, "mdate": 1763690043350, "content": {"title": "Scaling Laws and Symmetry, Evidence from Neural Force Fields", "abstract": "We present an empirical study in the geometric task of learning interatomic potentials, which shows equivariance matters even more at larger scales; we show a\nclear power-law scaling behaviour with respect to data, parameters and compute\nwith “architecture-dependent exponents”. In particular, we observe that equivariant\narchitectures, which leverage task symmetry, scale better than non-equivariant\nmodels. Moreover, among equivariant architectures, higher-order representations\ntranslate to better scaling exponents. Our analysis also suggests that for computeoptimal\ntraining, the data and model sizes should scale in tandem regardless of the\narchitecture. At a high level, these results suggest that, contrary to common belief,\nwe should not leave it to the model to discover fundamental inductive biases such\nas symmetry, especially as we scale, because they change the inherent difficulty\nof the task and its scaling laws.", "tldr": "We found “architecture-dependent” scaling exponents across architectures with increasing levels of symmetry expressivity in the area of learning interatomic potentials.", "keywords": ["compute-optimal scaling laws", "geometric deep learning", "interatomic potentials"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7460793d9dbdfa5eaf5a5f67e0bd1f1d981011a4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents an empirical investigation of scaling laws for neural force fields used in learning interatomic potentials. The authors systematically study how equivariant and non-equivariant architectures scale with respect to data size, model parameters, and compute budget. The key finding is that equivariant architectures exhibit superior scaling exponents compared to non-equivariant models, with higher-order equivariant representations showing the best scaling behavior. The study provides power-law fits demonstrating \"architecture-dependent exponents\" and suggests that compute-optimal training requires scaling data and model size together. The authors argue that fundamental inductive biases like symmetry should be built into architectures rather than left for models to discover, especially at scale."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Rigorous empirical analysis: The paper provides tight power-law fits across the tested regime, demonstrating clear architecture-dependent scaling exponents with convincing statistical evidence.\n- Clear presentation of core results: The main findings about equivariant architectures exhibiting better scaling behavior than non-equivariant models are presented clearly.\n- Practical insights: The finding that data and model size should scale in tandem for compute-optimal training is actionable regardless of architecture choice.\n- Important research question: Investigating whether fundamental inductive biases matter at scale is highly relevant to the broader machine learning community."}, "weaknesses": {"value": "- Severely limited training regime: Training on only a single epoch is a poor experimental choice that prevents data augmentation and fails to test the most interesting hypothesis from Brehmer 2025—that equivariance benefits may disappear with sufficient augmentation over multiple epochs. While the authors may draw inspiration from language model training, molecular datasets are orders of magnitude smaller, making this analogy weak.\n- Insufficient scale: The maximum compute budget of ~30 GPU hours is quite small for claiming insights about \"large-scale\" behavior on molecular datasets. True scaling studies typically involve orders of magnitude more compute.\n- Limited symmetry analysis: What the paper calls \"unconstrained MLP\" is actually translation-invariant (operating on relative positions). The study only assesses rotational equivariance on top of translational invariance, not the full contribution of geometric symmetries. This should be explicitly clarified as it affects the interpretation of results.\n- Unexplained contradictions with recent work: The conclusions don't adequately address the success of models like Orb, which achieve near state-of-the-art performance on MatBench Discovery despite being only translational equivariant (like the unconstraint MPNN in this paper). This weakens the paper's strong claims about the necessity of equivariance at scale. Could it be that the non-rotationally-equivariant model chosen by the authors is a worse choice than Orb?"}, "questions": {"value": "- How do the resulting models compare to state-of-the-art models on the OpenMol dataset? That would be good to know how well the results represent real-life results on such datasets.\n- Have the authors considered taking a model like eSEN and manipulating just its equivariant convolution layers to be unconstrained? That would remove some of the impact of architectural design on the comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oiGIywyvFq", "forum": "qyjaVda7t2", "replyto": "qyjaVda7t2", "signatures": ["ICLR.cc/2026/Conference/Submission9568/Reviewer_RXkz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9568/Reviewer_RXkz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856138604, "cdate": 1761856138604, "tmdate": 1762921122622, "mdate": 1762921122622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (Part 2)"}, "comment": {"value": "**4. Why neutral subset of OMol? Are there any implications?**\n\nWe used neutral split because it reflects the performance of widely used models in this domain. Particularly, here we quote the claim of [9]: **\"This split is intended to measure the performance\nof models on datasets the community is familiar with, without worrying about the complexity of charge and spin\"**\n\nNevertheless, in the updated version of the paper, we have included results on a smaller subset that includes charged molecules; the scaling behaviour remains similar; see Appendix G.\n\n**5. Are your choice of architectures representing the best equivariant and unconstrained models? How/why did you select these specific models?**\n\nYes, our choices represent performant models in different categories. This choice is supported with performance on leaderboards, popularity and coverage of different categories. The categories are based on previous works studying expressiveness of geometric graph neural networks, and we select the most representative in each category, as discussed in the paper. eSEN is at the top of leaderboard here [4], and our unconstrained model is similar to ORB [5]. Our decision to focus on message passing methods rather than transformer models was due to our inability to scale vanilla transformer (see new reported results for scaling of vanilla transformer in Appendix K).\n\n### References \n[1] Muennighoff, N., Rush, A., Barak, B., Le Scao, T., Tazi, N., Piktus, A., Pyysalo, S., Wolf, T. and Raffel, C.A., 2023. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36, pp.50358-50376.\n\n[2] Lin, L., Wu, J. and Bartlett, P.L., 2025. Improved Scaling Laws in Linear Regression via Data Reuse. arXiv preprint arXiv:2506.08415\n\n[3] Anonymous. Larger datasets can be repeated more: A theoretical analysis of multi-epoch scaling in linear regression. In Submitted to The Fourteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=0CXjpAxHUE. under\nreview.\n\n[4] https://huggingface.co/spaces/facebook/fairchem_leaderboard\n\n[5] https://www.orbitalindustries.com/posts/orbmol-extending-orb-to-molecular-systems\n\n[6] Wood, B.M., Dzamba, M., Fu, X., Gao, M., Shuaibi, M., Barroso-Luque, L., Abdelmaqsoud, K., Gharakhanyan, V., Kitchin, J.R., Levine, D.S. and Michel, K., 2025. UMA: A Family of Universal Models for Atoms. arXiv preprint arXiv:2506.23971.\n\n[7] Batatia, I., Benner, P., Chiang, Y., Elena, A.M., Kovács, D.P., Riebesell, J., Advincula, X.R., Asta, M., Avaylon, M., Baldwin, W.J. and Berger, F., 2025. A foundation model for atomistic materials chemistry. The Journal of Chemical Physics, 163(18).\n\n[8] Kim, K., Kotha, S., Liang, P. and Hashimoto, T., 2025. Pre-training under infinite compute. arXiv preprint arXiv:2509.14786.\n\n[9] Levine, D.S., Shuaibi, M., Spotte-Smith, E.W.C., Taylor, M.G., Hasyim, M.R., Michel, K., Batatia, I., Csányi, G., Dzamba, M., Eastman, P. and Frey, N.C., 2025. The open molecules 2025 (omol25) dataset, evaluations, and models. arXiv preprint arXiv:2505.08762\n\n[10] Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M.M.A., Yang, Y. and Zhou, Y., 2017. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409.\n\n[11] Frey, N.C., Soklaski, R., Axelrod, S., Samsi, S., Gomez-Bombarelli, R., Coley, C.W. and Gadepally, V., 2023. Neural scaling of deep chemical models. Nature Machine Intelligence, 5(11), pp.1297-1305.\n\n[12] Trikha, A., Chu, K., Gosai, A., Szachta, P. and Weiner, E., 2025. Scaling Laws for Neural Material Models. arXiv preprint arXiv:2509.21811."}}, "id": "pomuNgkDOG", "forum": "qyjaVda7t2", "replyto": "qyjaVda7t2", "signatures": ["ICLR.cc/2026/Conference/Submission9568/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9568/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9568/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763688967598, "cdate": 1763688967598, "tmdate": 1763690594005, "mdate": 1763690594005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an empirical study on the scalability of equivariant models vs non-equivariant models (w.r.t.  SO(3) symmetry group) for the molecular force fields task (predicting energy and forces of molecules). More specifically, the authors show how models like GemNet, eSEN, and EGNN tend to scale better than unconstrained GNN architectures on the OpenMol dataset. They study the scalability of these models across multiple access, including: model parameters, PFLOPs, and GPU hours, against validation loss of the molecular force fields task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* I think the problem being studied is interesting and quite relevant to the current ongoing discussion on equivariant vs non-equivariant models design, in the area of geometric deep learning.\n* Interesting to see that different equivariant models have different scalability behavior, for example, eSEN has lower validation loss with more GPU hours compared to the GemNet architecture."}, "weaknesses": {"value": "* I think the evidence is not sufficient to support the claims; some rewrites might be required to not make it a general claim or show more results on other tasks. \n* The study is limited to a single-epoch training regime, and studying the GPU hours in the range of less than 100 hours is limited. It would be beneficial to see how this could be extended for longer training times, and if the same trend holds or not. \n* Also, comparing recent models like eSEN to vanilla GNN might limit the claims of the paper, as GNN/ MPNN is an old baseline now. More unconstrianed architectures should be included (e.g., how this is applied to Graph Transformers)."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tX6cPH5frX", "forum": "qyjaVda7t2", "replyto": "qyjaVda7t2", "signatures": ["ICLR.cc/2026/Conference/Submission9568/Reviewer_SZDj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9568/Reviewer_SZDj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927895016, "cdate": 1761927895016, "tmdate": 1762921122073, "mdate": 1762921122073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response (Part 1)"}, "comment": {"value": "We thank all reviewers for their valuable feedback which has helped improved the paper. Below we briefly answer key concerns and related new results and changes to address them. \n\nThe new results are presented within the updated manuscript. Please consult the blue-highlighted sections for the location of these additions.\n\n**1. Do our conclusions hold in multi-epoch setting?**\n\nShort answer is yes, they remain correct for up to tens of epochs, which is the current norm in some related foundation models; e.g., see [6, 7]. \nWe present new results showing that the linear trend of loss vs log-compute remains intact if instead of using the entire dataset only use 1% of dataset for 100 epochs (see _Section 4.5_ in the updated paper). This agrees with [1, 2, 3] which shows that when the number of training epochs is relatively small, the order of the loss remains consistent with that of one-pass SGD under the same number of iterations. For unconstrained models, the linear trend holds for fewer epochs due to overfitting compared to equivariant models. Using a small subset helps highlight the effect of overfitting in multi-epoch setting. \n\n**2. What about data-augmentation? Can it close the gap in scaling laws?**\n\nSince the data is not canonicalized, data augmentation only makes sense in multi-epoch training. \nOur new results show that data-augmentation helps the unconstrained model retain its linear trend similar to equivariant networks. However, the power-law itself does not change.\nThis basically means that as compute increases, the performance gap between unconstrained model with data augmentation and equivariant architectures continues to grow, and our conclusions remain valid; see _Section 4.5_ in the updated paper.\n\nIn addition to training time augmentation, we also added new results on _test-time_ augmentation for the unconstrained model; see Appendix H. The results suggest that test-time augmentation can help by improving the multiplicative coef. rather than the exponent in the scaling law of unconstrained model. Moreover, we see that the benefit of test-time augmentation saturates quickly for this task.\n\n\n**3. Is our model, dataset and compute large enough to support our conclusions? Do we have enough variation in orders of magnitude for each dimension?**\n\nOur compute optimal scaling law spans almost five orders of magnitude. To further extend this on either end we need to consider increase/decrease in \"both\" the model size and the dataset size, otherwise we cannot claim/identify compute-optimal scaling. Even if we use multi-epoch training to virtually increase the dataset size, our largest models already saturate the 40GB GPU memory of A100 GPUs and we cannot scale both. We are running experiments using _smaller_ model sizes (down to single channel). We cannot further decrease the dataset size because one needs enough training to move beyond small-data regions where models can only perform as well as random guessing; see [10]. So we have exhausted our options in scaling for producing our main compute-optimal frontier results. \nSeparate results for scaling compute with data _alone_ using multi-epoch is possible as discussed earlier and we have included those. Also note that the scale of data used in this study is substantially larger than that of prior or concurrent works in this area [11, 12] ; we have close to one Billion tokens, which  exceeds the token counts used in some small- to medium-scale studies in NLP [8]."}}, "id": "cJLRzIHPls", "forum": "qyjaVda7t2", "replyto": "qyjaVda7t2", "signatures": ["ICLR.cc/2026/Conference/Submission9568/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9568/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9568/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763689093453, "cdate": 1763689093453, "tmdate": 1763694916660, "mdate": 1763694916660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces large-scale experimental results illustrating scaling laws for validation loss across equivariant message passing architectures and for NNIP tasks. As a result, several practical observations are made, including optimal architecture choices based on available compute."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles an important and timely problem, studying the effect of higher-order equivariant features on accuracy/compute tradeoffs. It presents large-scale experiments and reports empirical scaling behavior with clear ablations. The approaches used in the study are well motivated and clearly written."}, "weaknesses": {"value": "Some claims appear stronger than what the experiments directly support."}, "questions": {"value": "**Q1. Generalization.** The claim on line 80\n\n> While our study is limited in scope to Special Euclidean symmetry of neural interatomic potentials and force fields, as well as a few representative architectures, there is no reason to believe the results should remain confined to this particular domain and the choice of efficient equivariant models.\n\nappears at odds with the line 843 on related works stating\n\n>Despite using the eSEN same backbone, Wood et al. (2025) report that, for dense models 5, the compute-optimal strategy scales model size N faster than data size D, whereas in our setting we observe nearly equal scaling between N and D; though the tasks are different\n\nI suggest that the authors reconsider the claims of generalization of these scaling curves beyond the scope of the present work.\n\n**Q2. Undefined acronyms.** Please be sure to introduce acronyms like NNIPs that may be familiar to those working on these specific tasks, but not to a more general audience.\n\n**Q3. Energy vs. direct-force training.** Can the authors please substantiate the claim in line 138\n\n> While it is sufficient to learn the energy for predicting conservative forces, direct force prediction is significantly more scalable.\n\n**Q4. Transformer instability.** Can the authors provide evidence for the line 149 where they observed\n\n> instability issues when scaling vanilla transformers for this task, we focused on message-passing architectures."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OQ6cMgb2pZ", "forum": "qyjaVda7t2", "replyto": "qyjaVda7t2", "signatures": ["ICLR.cc/2026/Conference/Submission9568/Reviewer_AQ3A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9568/Reviewer_AQ3A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978046817, "cdate": 1761978046817, "tmdate": 1762921121464, "mdate": 1762921121464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study how machine learning interatomic potentials (MLIPs) scale with data, parameters, and compute, comparing architectures with increasing degrees of equivariance. They train on the OMol neutral-molecule subset, treat atoms as “tokens,” and adopt a single-epoch training regime to mirror LLM practice. They evaluate scaling against both FLOPs and GPU-hours, arguing this better reflects practical costs for (often less GPU-friendly) equivariant models. Empirically, they report clear power-law behavior and that scaling exponents grow with architectural equivariance, so performance gaps widen at larger scales; they also observe that compute-optimal training should scale model size and dataset size in tandem. A symmetry loss regularizer improves sample efficiency but does not match the benefits of fully equivariant architectures."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper applies the modern neural-scaling methodology to neural interatomic potentials, an important area in AI-driven chemistry.  This fills a gap for the field.\n2. The paper is well written and easy to follow."}, "weaknesses": {"value": "As the first paper (that I am aware of) to investigate the neural scaling law in the MLIP domain, I think the standard / stakes are high, and incorrect claims could lead to consequences in following model design. In this paper, the authors draw a lot of methodology directly from NLP, but I think these two fields have some fundamental differences. I found the following critical flaws in the three aspect of scaling law: Compute (1, 2, 3), Data (3, 4), and Parameters (5), and additional issues (6,7)\n1. Insufficient training / compute to claim asymptotic scaling: The scaling-law fits are based on small training budgets.  From Figure 1 and Section 4, the largest models run correspond to only \\~10^1 GPU-hours (per run) or \\~10^3 PFLOPs of compute.  This is tiny compared to typical scaling-law studies, which span orders-of-magnitude more compute (e.g. billions of tokens or thousands of TPU/GPU years). Even for MLIP training, this is too small: the eSEN small direct model is trained for around 10^5 PFLOP (80 Epochs) in OMol 4M, where the authors is training 10^3 PFLOPs on OMol 34M. That is **100x less FLOPs on 8x more data**. Moreover, each model is trained for only a single epoch through 34M samples (no repeated passes).  For neural potentials, one epoch training is extremely light: standard MLIP practice often trains tens of epochs to convergence.  The plots (e.g. Figure 1) do not show curves flattening -- the losses keep decreasing.  This suggests models are not converged even at the largest scale tried.  Fitting power laws in such a sub-convergent regime is problematic: the “effective” exponent can vary dramatically in early vs late training. In short, scaling laws are **asymptotic** statements, and here the training scale is too small to robustly infer asymptotic behavior.  This undermines the claim that explicit symmetry consistently improves scaling: with more compute, any performance gap might shrink or change.\n2. Single-epoch training regime. In Section 3.1, the authors intentionally use **only one epoch** (each sample seen once) to mirror LLM practice. However, this contradicts common MLIP training, where models typically see the data many times (e.g. \\~**80 epochs** in the OMol 4M baselines). Single-epoch training likely means none of the models fully fit the data distributions; indeed the validation losses are still dropping at the end of training. This choice can distort scaling-law estimates. For instance, early in training, increasing model size might seem more beneficial (higher \\alpha) simply because larger models learn faster per data pass, but with multi-epoch training a smaller model could catch up. By never allowing converged fits, the authors effectively inflate the “scaling advantage” of larger/equivariant models. They justify this by wanting to “avoid confounding effects”, but do not analyze how one vs multiple epochs alters the conclusions. Without at least one multi-epoch comparison, we cannot be sure the reported power-law behaviors would hold under standard MLIP training. This is a **major methodological gap**: the conclusions about scaling hinge on a very nonstandard (for this domain) training regime.\n3. Global target vs. token-wise analogy. The paper frequently draws analogies to language scaling laws, but **the supervised targets here are fundamentally different**. In language models, the loss (cross-entropy) is accumulated over **each token prediction**. Here, the primary target is the global energy of a molecule (plus per-atom forces, calculated by the gradient of the energy, thus is heavily correlated with the total energy). Each system yields **only one energy scalar despite many atoms**. The authors do treat atoms as “tokens” in counting dataset size, but this is a loose analogy: the model must capture a global property that depends on all atoms jointly. This could be a much harder task than NLP cross entropy target. Thus, as we discussed in weakness 2, seeing each atom once may not be analogous with LLM training, and the model requires much more training time to converge. As a result, statements like “we follow LLM scaling methodology” may be overreaching without addressing that molecular systems are global-structure tasks.\n4. Limited dataset regime (neutral split only). The OMol dataset is a high quality and diverse dataset, but all experiments use only the **neutral-molecule subset** of OMol (\\~34M samples), despite the full dataset being ~100M with diverse splits. This choice (claims to be taken due to memory limits, which if that's because of the system size, the authors could always decrease the batch size, since the batch size they chose was quite large (64) ) of using the neutral split, rather than **random sample**, could bias the domain. As described in the OMol paper, **non-equivariant models, such as GemNet, performs much better than equivariant models in EF error** when trained on all splits, such as biomolecules and electrolytes. This dataset selection clearly favors the equivariant baselines, and leads to a misleading or even incorrect results. I.e. the conclusions about “symmetry matters more at scale” could be dataset-specific. The authors should justify why neutral-only results would hold in the full OMol or other datasets (none of which is provided).\n5. Unfair comparison by raw parameter count. In NLP, comparing models by parameter count is largely meaningful because mainstream LLMs use a homogeneous Transformer architecture: blocks are architecturally identical, most weights live in token-wise MLPs and attention projections, and the compute per parameter and weight reuse pattern are effectively uniform across models. As a result, plotting loss vs. parameters is reasonably apples-to-apples. In contrast, **for GNN-based MLIPs the notion of “one parameter” does not carry across architectures**: (i) parameters are shared and reused across all nodes/edges; (ii) different designs incur very different work per parameter (e.g., dihedral terms in GemNet-OC, high-order tensor ops in eSEN); (iii) body order and tensor order change effective capacity and FLOP intensity; and (iv) throughput at the same N can differ drastically. This is largely why the kappa number varies between architectures. Consequently, the Fig. 5 style plots of loss vs. N conflate capacity, compute, and inductive bias, and can be misleading. A fair comparison should be iso-compute (FLOPs or GPU-hours), or at least normalized by an architecture-dependent cost.\n6. Exponent reliability and fitting issues. The fitted power-law exponents (α, β, γ) are central to the paper’s claims, but their reliability is questionable. First, as noted, the data range is relatively narrow (datasets from 10%-100%, models \\~10^6-10^7 parameters): typically one requires several orders of magnitude variation to robustly determine an exponent.  Here both N and D spans are \\~1-2 orders of magnitude at best. Second, the assumption $L_\\inf \\approx 0$ (irreducible loss) is ad hoc and may bias exponents high.  In real molecular data there should be some finite noise/error floor (analogous to the entropy in NLP); assuming zero means the model is “expected” to achieve perfect prediction eventually.  Small positive L_\\inf can drastically change a fitted β or γ in a power law fit (as studied in Hoffmann et al. 2022).  Third, Figure captions reveal instability: GemNet-OC needed smoothed loss, and the authors exclude the first 1–10% of training from fits.  These steps suggest the raw curves were not clean power laws. With such variance, the numeric exponents conflict across FLOPs vs wall-time (Fig.1).  Without error bars or fit-statistics on α,β (only γ had CIs in Table 1), it’s hard to trust these values.  In summary, the power-law behavior is claimed too strongly: given the limited and noisy regime, the reported exponents may not reflect true asymptotic trends. The results could be artifacts of the experimental choices (one-shot training, hyperparams, smoothing).\n7. Additional concerns: (i) The study considers only four architectures; it is not clear if these are representative. For example, EGNN and GemNet are relatively simple and old models -- how would a more modern one (PaiNN, DPA-1/2, EScAIP, even NequIP) behave? (ii) The paper also mixes different body-order and tensor-order notions without clarity. (iii) Hyperparameters (depth, widths) are tuned only at ~1M params and then scaled; it’s possible that later models were not fully optimized. (iv) The authors do not report if they repeated experiments to measure variability (confidence intervals are absent for N,D scaling). (v) Finally, the broad claim that one should “not leave symmetry to be discovered by scaling” may overstate the results. The bitter-lesson cited in the intro suggests that even biased models can eventually be outperformed by larger unconstrained ones; these experiments do not go far enough to test that (no extremely large unconstrained model was trained). In its current form, the paper’s strong conclusion about avoiding bitter lesson is not fully justified by the limited empirical data."}, "questions": {"value": "1. Why restrict training to a single epoch?  Have you tried multi-epoch training (e.g. 10 or 80 epochs) on a smaller scale to see if exponents change?  How do you justify that one pass through data captures the scaling behavior of fully trained models?\n2. How do you expect inclusion of the charged/large molecules (omitted in the neutral split) to affect your conclusions?  Could the symmetry advantages reverse or diminish in chemically diverse subsets?\n3. How sensitive are your results to the learning-rate and batch-size choices? You tuned these at 1M parameters and then scaled. Is it possible that some models (e.g. the largest eSEN) were suboptimally trained? \n4. The introduction cites Sutton’s “bitter lesson” about scale overtaking bias. Given your modest scaling regime, how confident are you that “we should not leave symmetry to be discovered by the model”? Could larger-scale experiments eventually reverse the trend you observe?\n5. Nit: where is Figure 6?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JSspl5K6RO", "forum": "qyjaVda7t2", "replyto": "qyjaVda7t2", "signatures": ["ICLR.cc/2026/Conference/Submission9568/Reviewer_58Kv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9568/Reviewer_58Kv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9568/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762207429441, "cdate": 1762207429441, "tmdate": 1762921121019, "mdate": 1762921121019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}