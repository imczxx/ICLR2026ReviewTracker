{"id": "0mUiXz1TNq", "number": 15564, "cdate": 1758252698046, "mdate": 1763729270686, "content": {"title": "VUDG: A Dataset for Video Understanding Domain Generalization", "abstract": "Video understanding has made remarkable progress in recent years, largely driven by advances in deep models and the availability of large-scale annotated datasets.\nHowever, the robustness of these models to domain shifts encountered in real-world video applications remains a critical yet underexplored problem, limiting their practical reliability.\nTo address this problem, we introduce \\textbf{V}ideo \\textbf{U}nderstanding \\textbf{D}omain \\textbf{G}eneralization (\\textbf{VUDG}), the first dataset designed specifically for evaluating domain generalization in video understanding.\nVUDG contains videos from 11 distinct domains that cover three types of domain shifts, and maintains semantic consistency across different domains to ensure fair and meaningful evaluation. We propose a multi-expert progressive annotation framework to efficiently annotate videos with structured question-answer pairs designed for domain generalization.\nExtensive experiments on 9 representative Large Vision-Language Models (LVLMs) and several traditional video question answering methods show that most models (including state-of-the-art LVLMs) suffer performance degradation under domain shifts. \nThese results highlight the challenges posed by VUDG and the difference in the robustness of current models to data distribution shifts. We believe VUDG provides a critical resource to benefit future research in domain generalization for video understanding.", "tldr": "", "keywords": ["Video Understanding", "Dataset", "Domain Generalization"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea6a9b946aaefecd7bfec0975f5a454db65149bb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new dataset, VUDG, to test domain generalisation in video understanding models. It includes 11 domains across three kinds of shifts: **semantic** (e.g., cartoon), **viewpoint** (e.g., egocentric), and **environmental conditions** (e.g., foggy). VUDG has the following qualities:\n1. There is semantic consistency across the domains, i.e., they show similar actions.\n2. The annotation is based on a generating QA pairs from a cascade of LLMs with a final human review component.\n3. The videos are sourced from existing datasets as well as from online video sites.\n\nA suite of video models are evaluated on VUDG across 4 settings:\n1. Full domain (train on all domains, test on all domains)\n2. Multi-domain (train on all-but-one domain, test on the one remaining domain)\n3. Single-domain (train on a single domain, test on all other domains)\n4. Zero-shot\n\nSome key findings include:\n* Video models perform worse in settings (2) and (3) compared to (1).\n* LLM based models are much superior than traditional domain generalisation methods.\n* Qwen2.5VL-7B is very strong in zero-shot performance"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Domain generalisation is an interesting problem to study in video context (especially for something like exo-to-ego generalisation).\n2. The use of a cascade of strong LLMs to obtain QA annotations seems scalable and can possibly help scale up the proposed dataset. Including newer LLMs can also improve the current dataset.\n3. The inclusion of \"Temporal understanding\" in the designed questions (e.g., related to temporal ordering of actions) is valuable and worth studying in context of domain generalisation."}, "weaknesses": {"value": "1. **General concern about relevance of domain generalisation in LLM era**. Given the increasingly massive sizes and extent of the training datasets for modern video LLMs (e.g., Tarsier 2 is trained on 40 million videos), the gap between what we deem as training and testing is shrinking. This is likely why Qwen2.5VL-7B zero-shot (~72%) is stronger than any fine-tuned models. I am sure the closed models are even stronger just zero-shot. Thus, I am not sure if studying domain generalisation (and this dataset in particular) brings a lot of novel insights. \n\n2. **Some stronger models can be included in evaluation.** An open model like Tarsier/Tarsier2 is shown to be quite a strong video LLM. It may already achieve very strong zero-shot performance, so it should be included in my view. Likewise, no closed models are included. While I understand the cost aspect, at least one model should be included because I suspect it may already be very strong zero-shot. \n\n3. **Value added by this dataset.** While domain generalisation in video context sounds interesting, I did not gather what new, surprising insights this dataset would bring to the community. Studying it for special applications like medical imaging or private videos is valueable, but this dataset still has domains that are likely present in the train dataset of some of these big models."}, "questions": {"value": "1. Domain generalisation is well studied for images. What about videos is special that makes it interesting in this context? If the authors can motivate this better, it can be useful.\n\n2. it is not clear how you get data for different domains. The source datasets are stated but how does one get videos from different domains?\n\n3. \"Attribute identification\" task may be more relevant for images than videos. Can you provide examples for attribute identification from videos?\n\n4. Most existing video benchmarks suffer from static biases (a single frame or even without any visual frames, models can get reasonable accuracies). Have you done any analysis in this regard?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zK0nQjomNN", "forum": "0mUiXz1TNq", "replyto": "0mUiXz1TNq", "signatures": ["ICLR.cc/2026/Conference/Submission15564/Reviewer_Qtow"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15564/Reviewer_Qtow"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932852501, "cdate": 1761932852501, "tmdate": 1762925838197, "mdate": 1762925838197, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new dataset to evaluate model capability for domain generalization in a video comprehension task. Videos are filtered using a multimodal LLM to ensure they fall into certain domains. The videos are then filtered based on the actions again, using a multimodal LLM so that videos in the different domains have similar semantics. QA pairs are automatically generated for each video, which are then reviewed by human experts. The dataset offers both training and test sets. The paper provides evaluation results over multiple models in multi-domain fine-tuning, single-domain fine-tuning, and zero-shot settings. Human evaluation is also provided to show the relevance and correctness of the automatically generated WA pairs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. I believe the paper offers a useful dataset (once it is published online), which should contribute to the community. \n2. The paper also offers comprehensive details of how the dataset is created."}, "weaknesses": {"value": "1. This may be a minor concern, but since the dataset only offers limited semantics, the evaluation capability may also be limited. It would be nice if the paper could provide some discussion on this point (e.g., whether the set of actions is sufficient for evaluating general performance, and why). \n2. As domain generalization can involve fine-tuning, it is desirable to evaluate possible bias in QA pairs and semantics. This may be done by training a language model with only QA pairs and some tokens to represent semantics. \n3. It’s hard for me to see what the human evaluation results in Table 10 mean. Human-annotated QAs and some randomization (e.g., randomly pairing up a video and QA in different domains but in the same semantics, or just random video and QA pairs) may help understand it. \n4. I also don’t see why human evaluation uses a 5-point scale. Relevance and correctness sound more like binary."}, "questions": {"value": "I want to see some discussion on the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cj7Tq64cpK", "forum": "0mUiXz1TNq", "replyto": "0mUiXz1TNq", "signatures": ["ICLR.cc/2026/Conference/Submission15564/Reviewer_RsGD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15564/Reviewer_RsGD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001396799, "cdate": 1762001396799, "tmdate": 1762925837731, "mdate": 1762925837731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Video Understanding Domain Generalization (VUDG) dataset, a dataset designed to evaluate domain generalization of video understanding models. The authors key motivation is that existing video understanding benchmarks either do not measure domain generalization, or are ineffective at measuring it due to large semantic gap between domains. VUDG aims to address this by collecting videos from 11 distinct domains while keeping the underlying semantic content consistent through a shared set of daily human activities. the authors employ a progressive multi-expert annotation framework that combines multiple large vision-language models for automated question generation, verification, and filtering, followed by a human verification. This process yields a training dataset of 31k QA pairs, and a testing dataset of 4k QA pairs. The authors additionally benchmark existing video understanding models on VUDG."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed benchmark is the first large-scale benchmark designed to measure domain generalization in VLMs, consisting of a dedicated training/testing set and clear protocols for evaluation\n\nThe choice of domains and corresponding videos is high quality: The domains selected in VUDG are broad and diverse and a large portion of the videos used for testing are newly collected by the authors, reducing the risk of data leakage with existing VLM training data"}, "weaknesses": {"value": "The reviewer understands the primary motivation behind VUDG is to unify the semantic space across domains, and this intuitively makes sense to the reviewer. However, it is not scientifically shown why existing benchmarks fail to assess domain generalization due to the lack of cross-domain semantic alignment\n* What evidence exists to motivate the community to evaluate the domain generalization ability of their models with VUDG, instead of a benchmark like Video-MME that also contains multiple video domains?\n* The t-SNE in Figure 6 does show the semantic alignment between domains in VUDG, but it is not compared with other benchmarks\n\nIt is unclear if the testing dataset exhibits language or visual bias in the MCQ questions, which is a common pitfall of LLM generated MCQs (Zohar et al., Apollo: An Exploration of Video Understanding in Large Multimodal Models, CVPR 2025). The authors mention there is a human verification step, but do not explicitly state that it addresses this bias.\n* Additionally there are no qualitative examples of MCQs in the paper. As this is a dataset paper the reviewer expected to see many more examples of QA pairs in the dataset\n\nThe choice of VLMs used for evaluation in the full fine-tuning and single-domain generalization settings is relatively shallow, reducing the insight gained from these settings"}, "questions": {"value": "What evidence exists to motivate the community to evaluate the domain generalization ability of their models with VUDG, instead of a benchmark like Video-MME that also contains multiple video domains? Can this semantic shift be quantified and shown to hurt a benchmarks ability to assess domain generalization? Or can it be qualitatively shown?\n\nHow can the authors be certain that the MCQs in VUDG do not suffer from visual or language bias? Are there any failsafes in the data curation to mitigate this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PjvBU07Pmz", "forum": "0mUiXz1TNq", "replyto": "0mUiXz1TNq", "signatures": ["ICLR.cc/2026/Conference/Submission15564/Reviewer_M1bM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15564/Reviewer_M1bM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053690216, "cdate": 1762053690216, "tmdate": 1762925836974, "mdate": 1762925836974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors introduce VUDG, a video-understanding domain generalization benchmark spanning 11 domains with a shared semantic space. QA pairs are built via a multi-expert (model-assisted + human) annotation pipeline. Across multi-source, single-source, and zero-shot protocols, both classic VideoQA methods and state-of-the-art LVLMs show sizable accuracy drops under domain shifts, revealing uneven robustness and a clear gap from in-domain fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Innovative Benchmark Design\n\nThe paper introduces a novel benchmark specifically crafted to test how well video understanding models generalize across domain shifts, filling an important gap in current evaluation practices.\n\nThorough and Systematic Evaluation\n\nIt provides a comprehensive assessment of multiple domain shift scenarios (style, viewpoint, weather, lighting) using both traditional VideoQA models and modern LVLMs, offering a well-rounded perspective on robustness."}, "weaknesses": {"value": "Insufficient Distractor Analysis\n\nWhile the paper reports aggregate counts and durations (Fig. 4–5), it provides no fine-grained QA/distractor analyses - e.g., answer-length distributions, length-matching of options, distractor similarity, so on. For example, the prompt states to have distractors be the same length but from my experience LLMs might just ignore it completely and there is no statistics to verify if it is the case. Further investigation into questions and answer options would strengthen the benchmark’s validity\n\nLLM dependence and possible circularity\n\nThe pipeline relies on LLMs for generation, screening, and even as an automated judge for open-ended scoring, which can introduce model bias and evaluation circularity despite a final human pass."}, "questions": {"value": "refer to weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "L9PrxCqvpi", "forum": "0mUiXz1TNq", "replyto": "0mUiXz1TNq", "signatures": ["ICLR.cc/2026/Conference/Submission15564/Reviewer_xqBB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15564/Reviewer_xqBB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762168086733, "cdate": 1762168086733, "tmdate": 1762925836194, "mdate": 1762925836194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}