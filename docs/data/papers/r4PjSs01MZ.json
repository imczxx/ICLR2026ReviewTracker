{"id": "r4PjSs01MZ", "number": 15384, "cdate": 1758250836256, "mdate": 1759897310311, "content": {"title": "RePAIR: A Rule-based Process-Adaptive Reinforcement for Large Language Model Training", "abstract": "Although reinforcement learning (RL) has demonstrated promise in enhancing the reasoning capabilities of Large Language Models (LLMs), the difficulty of reward design has prohibited exploiting the full potential of RL. Previous methods mainly fall into two categories: training a reward model based on human preferences, or designing verifiable outcome rewards. However, reward models often suffer from poor interpretability and require extensive annotation for effective training. Verifiable outcome rewards provide sparse signals only, which leads to an ambiguous credit assignment and low training efficiency in RL. These limitations necessitate rewards that provide more efficient, fine-grained supervision. In order to address these, we propose Rule-based Process-AdaptIve Reinforcement (RePAIR) that constructs adaptive verifiable process rewards through symbolic reasoning rules. These rules are automatically derived through the integration of common pattern mining and semantic summarization over the reasoning trajectories of LLMs. For stable training purposes, RePAIR defines a reward informativeness metric that dynamically adjusts the rule's weights based on policy updates. Extensive experiments across three reasoning tasks demonstrate that RePAIR achieves a 6.03% improvement on average and combines well with various advantage functions. Code and data will be available at https://anonymous.4open.science/r/RePAIR-8EFC.", "tldr": "We construct adaptive verifiable process rewards for the reinforcement learning of large language models  through symbolic reasoning rules which automatically extracted from LLM-generated reasoning trajectories.", "keywords": ["Symbolic Reasoning Rule", "Verifiable Process Reward", "Reinforcement Learning", "Large Language Model"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67fc3fb392cd8a6534aa288822e9b6f9fa7ebc3f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces RePAIR, a Rule-based Process-Adaptive Reinforcement framework aimed at improving reinforcement learning for LLMs. It tackles the challenges of sparse and ambiguous reward signals in traditional RLHF and RLVR by incorporating symbolic reasoning rules that yield verifiable and adaptive process-level rewards. These rules are automatically extracted from model-generated reasoning trajectories via frequent subgraph mining and semantic summarization, enabling interpretable and fine-grained feedback throughout training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction of symbolic, rule-based process rewards enhances interpretability, verifiability, and adaptability, effectively addressing the limitations of outcome-based and black-box reward models.\n\n2. The framework automatically derives reasoning rules from LLM trajectories, reducing dependence on manual reward engineering and human annotation.\n\n3. RePAIR achieves consistent and substantial improvements across multiple reinforcement learning algorithms and reasoning benchmarks.\n\n4. The paper is well written, with clear algorithmic descriptions, implementation details, and a stated plan for open-source release."}, "weaknesses": {"value": "1. In Table 3, the performance gain over the unverified RULE baseline is modest for GRPO and REINFORCE++, suggesting that improvements may primarily stem from adaptive weighting rather than rule validation.\n\n2. Experiments are conducted mainly on small-scale models (≤3B parameters), leaving it unclear whether RePAIR scales effectively to larger models (e.g., 7B+).\n\n3. The influence of the informativeness update parameters ($\\alpha$, $\\beta$, $\\eta$) is not systematically analyzed, which may limit reproducibility."}, "questions": {"value": "1–3. See weaknesses above.\n\n4. How does RePAIR generalize to tasks beyond mathematical or structured reasoning, such as commonsense or dialogue-based reasoning?\n\n5. What is the computational overhead of rule extraction and verification relative to standard RL training?\n\n6. The adaptive weighting mechanism depends on the success rate, which requires access to the final outcome. Does this constrain applicability in settings with only process-level feedback (i.e., without a final answer)?\n\n7. In tasks where intermediate reasoning cannot be easily decomposed into discrete steps, how would graph construction and rule matching be adapted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iO5OqY0Xrh", "forum": "r4PjSs01MZ", "replyto": "r4PjSs01MZ", "signatures": ["ICLR.cc/2026/Conference/Submission15384/Reviewer_cA2d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15384/Reviewer_cA2d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825302122, "cdate": 1761825302122, "tmdate": 1762925667576, "mdate": 1762925667576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors proposed an innovative framework called RePAIR. This framework can automatically extract symbolic rules and transform them into dynamic, fine-grained process reward signals. The authors compare the method with a series of baselines and evaluate its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. In this paper, the authors provide sufficient experimental details and hyperparameters. These settings cover the settings for reproducibility.\n\n2. The structure and format of this paper are clear and easy to follow. The authors also provide clear figures and tables to enhance the readability."}, "weaknesses": {"value": "1. In the related work section, the authors mention PRM but fail to discuss these methods. In recent research, there are many methods of LLM-as-a-judge using strong LLMs or token-level process reward. These methods are direct competing solutions for the method. The lack of comparison between these methods weakens the persuasiveness of the method.\n\n2. The experiments in this paper are primarily based on the Qwen model. Although they cover different scales of LLM, the model architecture is singular. This setting limits the generalizability of the method and contradicts the model-agnostic assumption mentioned in the paper. In addition, the method relies on external strong teacher models like GPT-4o or Deepseek-R1. This raises the question of whether the method remains effective when using weaker models for rule extraction."}, "questions": {"value": "1. The authors mention that rules can be extracted from successful and failed trajectories. However, the authors only discuss positive rules. An issue worth discussing and studying is how negative rules have an impact. The authors may need to present a case study of negative rules.\n\n2. In this paper, the authors use first-order logic to represent rules. This strategy is clear and verifiable. However, this hard-coded approach may limit the LLM's reasoning ability since the process of LLM reasoning may be more ambiguous. An open question is whether more ambiguous soft rules would lead to better results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b6MlVRsyFr", "forum": "r4PjSs01MZ", "replyto": "r4PjSs01MZ", "signatures": ["ICLR.cc/2026/Conference/Submission15384/Reviewer_4qNx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15384/Reviewer_4qNx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980657081, "cdate": 1761980657081, "tmdate": 1762925667156, "mdate": 1762925667156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new approach to accelerate LLM-based reasoning by specifying an intermediate reward signal learned from successful reasoning trajectories. The approach involves first constructing multiple symbolic graphs from these trajectories, where each graph represents a rule-based reward signal. At each step, these signals are combined using adaptive weights to form a single intrinsic reward. These weights are updated based on the hit rate and success rate of each rule. The paper demonstrates the empirical benefits of this method on several reasoning tasks, including GSM-8k, AIME-23, and AIME-24, using a 1.5B Qwen model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The general idea of constructing an intrinsic reward signal to accelerate the learning process is a promising direction, particularly for problems where the true reward signal is sparse and obtained after long trajectories. The paper introduces a valuable contribution in this area.\n- The proposed approach is algorithm-agnostic, demonstrating its effectiveness with GRPO, Reinforce, and Dr. GRPO on various reasoning benchmarks.\n- The experiments section is well-organized, and the paper aims to answer several interesting questions regarding the proposed approach, such as its compatibility with several RL algorithms, the generalizability of the reward signals, and its comparison to handcrafted rules.\n- In general, the paper is well-written and easy to follow."}, "weaknesses": {"value": "- The primary weakness of the paper is its questionable technical soundness:\n    - It is well-known in RL that constructed intrinsic reward signals must follow specific rules for the optimal policy to remain invariant [1]. However, it is unclear whether the proposed intermediate reward signal satisfies these requirements.\n    - The advantage term (Eq. 5), which combines the intrinsic and extrinsic rewards, appears mathematically unsound and biased. The paper lacks explanation or proof of how it leads to the update described in Eq. 1.\n    - The reward informativeness metric and the adaptive weight update rule lack sufficient mathematical explanation. It is unclear why Eq. 4 has the desirable effect. Intuitively, the rules that appear in unsuccessful trajectories are equally emphasized as long as the same rule appears in successful trajectories, as the metric does not use information about unsuccessful trajectories.\n- The overall empirical results are underwhelming, partly due to the algorithmic inconsistencies noted above and partly due to the training procedure. A 1.5B model achieves a score of 25+ on AIME-24 (https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B), but the presented results are significantly lower. Furthermore, the benchmarks used may be too simple, as many are known to be relatively easy and might not adequately highlight the usefulness and scalability of the approach. A more thorough evaluation would also consider baselines from process reward modelling (PRMs).\n- Some parts of the paper require more explanation; for instance, the concept of “frequent subgraph mining” is not clearly defined for the reader.\n\n[1] Ng, Andrew Y., Daishi Harada, and Stuart Russell. \"Policy invariance under reward transformations: Theory and application to reward shaping.\" Icml. Vol. 99. 1999."}, "questions": {"value": "- The paper should provide more details on the rule-generation process. Since the approach uses an LLM to both extract semantic features and generate executable rules, it is unclear how the validity and reliability of these rules are ensured.\n- The authors should elaborate on the mechanisms that prevent the generation of numerous, question-specific rules. It is unclear how the method maintains a bounded total number of rules and, by extension, ensures the generalizability of these symbolic rules beyond the specific questions used for training.\n- The methodology of the generalization experiment requires clarification. The paper should explicitly state whether this experiment was designed to test the generalizability of the generated symbolic rules themselves or the final model trained with them. The former is more interesting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tXc4eC44zr", "forum": "r4PjSs01MZ", "replyto": "r4PjSs01MZ", "signatures": ["ICLR.cc/2026/Conference/Submission15384/Reviewer_N7aE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15384/Reviewer_N7aE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129273533, "cdate": 1762129273533, "tmdate": 1762925666783, "mdate": 1762925666783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework called RePAIR to improve LLM's reasoning ability. To deal with the challenges in human preference reward models and verifiable outcome reward models, RePAIR extracts symbolic reasoning rules from model reasoning trajectories, and turn them into verifiable process-level rewards which guide training at finer granularity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed idea is novel. RePAIR can extract symbolic reasoning rules from LLM-generated trajectories, which formalize common reasoning patterns as a computable function to provide a verifiable and interpretable basis for process supervision.\n2. The dynamic weighting strategy ensures that the most informative rules guide training.\n3. The rule extraction is lightweight and computationally inexpensive."}, "weaknesses": {"value": "1. The convergence of the adaptive rewards needs justification. As  RePAIR continuously adjusts rule weights based on “reward informativeness,” the reward function itself changes during training. With a dynamically changing reward, the RL policy will get confused and divergent. \n2. The scalability is limited. The rule extraction pipeline is based on subgraph mining and symbolic reasoning. It is effective for structured reasoning (e.g., math or logic) but may not scale efficiently to open-ended tasks such as dialogue, coding, or multimodal reasoning.\n3. Small models tend to memorize symbolic rules instead of learning true reasoning patterns. It causes overfitting to specific rule structures and poor generalization to new tasks. As a result, training appears successful on seen data but fails to transfer to diverse reasoning scenarios."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CfGXGz0UOE", "forum": "r4PjSs01MZ", "replyto": "r4PjSs01MZ", "signatures": ["ICLR.cc/2026/Conference/Submission15384/Reviewer_bimJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15384/Reviewer_bimJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762399997091, "cdate": 1762399997091, "tmdate": 1762925666459, "mdate": 1762925666459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}