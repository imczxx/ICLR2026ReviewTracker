{"id": "CTGpC7xWHM", "number": 14025, "cdate": 1758227202482, "mdate": 1759897395438, "content": {"title": "How reinforcement learning after next-token prediction facilitates learning", "abstract": "Recent advances in reasoning domains with neural networks have primarily been enabled by a training recipe that optimizes Large Language Models, previously trained to predict the next-token in a sequence, with reinforcement learning algorithms. We introduce a framework to study the success of this paradigm, and we theoretically expose the optimization mechanisms by which reinforcement learning improves over next-token prediction in this setting. We study learning from mixture distributions of short and long “chain-of-thought” sequences encoding a single task. In particular, when the task consists of predicting the parity of $d$ bits and long sequences are rare, we show how reinforcement learning after next-token prediction enables autoregressive transformers to generalize, whereas mere next-token prediction requires extreme statistical or computational resources to do so. We further explain how reinforcement learning leverages increased test-time computation, manifested in longer responses, to facilitate this learning process. In a simplified setting, we theoretically prove that autoregressive linear models following this training recipe can efficiently learn to predict the parity of $d$ bits as long as the proportion of long demonstrations in the data mix is not exponentially small in the input dimension $d$.\nFinally, we demonstrate these same phenomena in other settings, including the post-training of Llama-series models on mixture variations of common mathematical reasoning benchmarks.", "tldr": "We study and explain how and why RL after next-token prediction enables learning in cases where mere next-token prediction fails to improve performance.", "keywords": ["large language models", "reinforcement learning", "length increase", "theory"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a3485de127337873eb49a4fa2f580314f85c6b57.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explains why large language models improve dramatically when reinforcement learning (RL) follows next-token prediction training. The authors prove that next-token prediction alone cannot generalize on hard tasks if long reasoning chains are rare, whereas RL amplifies these rare sequences and enables efficient learning. Experiments on both synthetic and real-world reasoning tasks confirm that RL boosts accuracy and increases response length. The results highlight RL’s role in unlocking generalization from limited but valuable reasoning data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is the first to provide a formal separation between next-token prediction and next-token prediction plus RL in autoregressive models, giving a novel theoretical account of why the widely-used “pre-train then RL” recipe succeeds.  \n* By showing that RL can turn a sample-hard, exponentially data-hungry problem into a polynomial-time solvable one whenever long demonstrations are merely polynomially rare, the work directly informs how scarce reasoning data should be leveraged in large-scale model development."}, "weaknesses": {"value": "* All conclusions of the paper are drawn around the task of predicting the parity of d bits given access to a source of sequences, and its generalization to more common reasoning tasks (science or open-ended) is questionable.\n* There is a lack of experiments on a broader range of and more recent LLMs, e.g., qwen3."}, "questions": {"value": "* Theorem 1 gives $p_{cot} < 1/3$ as the exact point where greedy decoding stays short. Is this an artifact of the two-step linear decision model, or does it survive richer embeddings (e.g., transformers with non-linear MLPs)? An ablation that keeps the data distribution but increases model expressivity would clarify whether the threshold is distribution- or architecture-specific.\n\n* The post-training bound hides the per-round sample size inside Õ(·) and assumes fresh data every round. How many total unique prompts does the algorithm really need? \n\n* Parity has a single deterministic “long path”. Real reasoning data often contain many valid chains of varying length and quality. Does the two-component mixture still capture the dynamics, or does the presence of noisy/partial chains shift the critical $p_{cot}$ or require a different RL objective?\n\n* The theory 2 requires $p_{cot} \\in \\Omega (d^{-\\kappa})$. For $d\\approx 1,000$ (typical for LLM prompts) this seems prohibitive. Is the polynomial dependence tight, or do transformers empirically succeed with much smaller constants?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nhIrK8OjOH", "forum": "CTGpC7xWHM", "replyto": "CTGpC7xWHM", "signatures": ["ICLR.cc/2026/Conference/Submission14025/Reviewer_Hty6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14025/Reviewer_Hty6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761327325342, "cdate": 1761327325342, "tmdate": 1762924515782, "mdate": 1762924515782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework to theoretically understand the success the popular LLM training paradigm that RL post-training after SFT. This paper also uses the parity check experiments to show that RL enables the model to generalize to difficult tasks while SFT cannot. The paper also demonstrated this phenomenon on math problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the paper is well-motivated - trying to understand the reason behind the current successful LLM training paradigm.\n- the paper proposed a relatively comprehensive theoretical analysis framework.\n- the paper empirically validated its perspective through a cleverly designed and controlled parity check experiment."}, "weaknesses": {"value": "1. The theoretical proof relies on a simplified linear autoregressive model.\n2. Some of the ideas of this paper are already pointed out in papers like DeepSeek-R1.\n2. This paer lacks of practical guidance for future LLM training and new algorithm."}, "questions": {"value": "- Could similar phenomena be observed when in a worded version of a parity task? Such as giving the task description as language input of a LLM.\n- The theoretical analysis relies relies on a simple linear framework, how is it extended to non-linear structures like the Transformers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "i9WLpwif1R", "forum": "CTGpC7xWHM", "replyto": "CTGpC7xWHM", "signatures": ["ICLR.cc/2026/Conference/Submission14025/Reviewer_pXVH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14025/Reviewer_pXVH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807591432, "cdate": 1761807591432, "tmdate": 1762924515383, "mdate": 1762924515383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies why RL after next-token prediction helps large language models learn better reasoning. The authors build a simple setting where training data mixes short and long “chain-of-thought” examples. They theoretically and experimentally prove that next-token prediction alone often fails when long samples are less than 1/3 in pretraining dataset. And RL can quickly improve learning by focusing on longer samples, which leads to longer and more correct responses."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Strong theoretical proof combined with solid experiments, showing when and why RL succeeds.\n2. The paper gives a simple and convincing explanation of why RL helps reasoning. Providing clear insights.\n3. The answer of  two core questions—\"why RL works\" and \"why length increases\" could offer a new design direction for LLM reasoning optimization."}, "weaknesses": {"value": "1. The theory assumes fully correct CoT examples. However, the real pretrain data usually contains noisy or wrong data. And even positive RL trajectories could contain noise or false-positive ones. The paper lacks discussion of these factors.\n2. The theoretical proof could be written in a more organized manner, including formulas. This would make that part easier to understand."}, "questions": {"value": "1. How robust are the theoretical conclusions if long CoT samples contain noise?\n2. Can you still observe “length-driven generalization” when RL rewards contain a certain proportion of length penalty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xeref2dEv5", "forum": "CTGpC7xWHM", "replyto": "CTGpC7xWHM", "signatures": ["ICLR.cc/2026/Conference/Submission14025/Reviewer_StSb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14025/Reviewer_StSb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14025/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834205670, "cdate": 1761834205670, "tmdate": 1762924515076, "mdate": 1762924515076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}