{"id": "dHWtOTiceO", "number": 9589, "cdate": 1758128974482, "mdate": 1759897710699, "content": {"title": "Text2Arch: A Dataset for Generating Scientific Architecture Diagrams from Natural Language Descriptions", "abstract": "Communicating complex system designs or scientific processes through text alone is inefficient and prone to ambiguity. A system that automatically generates scientific architecture diagrams from text with high semantic fidelity can be useful in multiple applications like enterprise architecture visualization, AI-driven software design, and educational content creation. Hence, in this paper, we focus on leveraging language models to perform semantic understanding of the input text description to generate intermediate code that can be processed to generate high-fidelity architecture diagrams. Unfortunately, no clean large-scale open-access dataset exists, implying lack of any effective open models for this task. Hence, we contribute a comprehensive dataset, \\system, comprising scientific architecture images, their corresponding textual descriptions, and associated DOT code representations.  Leveraging this resource, we fine-tune a suite of small language models, and also perform in-context learning using GPT-4o. Through extensive experimentation, we show that \\system{} models significantly outperform existing baseline models like DiagramAgent and perform at par with in-context learning based generations from GPT-4o. We have added code and data as Supplementary material, and will make them (and models) publicly available on acceptance.", "tldr": "", "keywords": ["NLP: Generation", "NLP: Applications"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d54aab6237ddf2b90db83ff91223e23a427119ca.pdf", "supplementary_material": "/attachment/7969e349686f75206741ad33c07bcfd2d0a8fb14.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces TEXT2ARCH, a new task and large-scale dataset (75,127 text–DOT–image triplets; 60,519/7,565/7,043 split) for generating scientific architecture diagrams from natural-language descriptions via intermediate DOT code. The dataset is curated through a three-stage pipeline—(1) filtering architecture figures, (2) DOT extraction with OCR/object detection + GPT refinement (DOT1→DOT2→DOT3), and (3) description refinement—illustrated in Fig. 2, with distributions and statistics in Fig. 3. The authors propose graph-level metrics (node/edge precision/recall/F1, PR-AUC, Jaccard) in addition to NLG metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Scoped, high-impact dataset for architecture diagrams with aligned text–code–image triplets and complexity bucketing; useful beyond the paper’s models.\n- Clear curation pipeline (classifier + OCR/detection + GPT refinement) with ablations across DOT variants (DOT3≫DOT1/DOT2).\n- Consistent empirical gains from finetuning small models (DeepSeek‑7B best on both automatic and GPT-based judging)."}, "weaknesses": {"value": "- Baseline fairness: DiagramAgent (TikZ) → DOT via GPT may degrade/alter structure; results could change with a native TikZ-based evaluation.\n- Label/eval circularity risk: GPT‑4o is used to generate/refine DOT labels (DOT1→DOT3) and to score outputs; this can bias comparisons and makes “ground truth” partly model-dependent. The human set (n=99) helps but is small.\n- string-similarity matching (Hungarian with τ=0.5) ignores diagram layout and may over/under-match aliases; multi-edges and duplicates aren’t handled; layout attributes are ignored.\n- no analysis of near-duplicate figures across splits or of overlap between description sources and DOT generation that could inflate performance."}, "questions": {"value": "- Beyond Table 3, can you expand the human-labeled set (≥500) and report the same metrics to better calibrate DOT3 quality?\n- Provide TikZ-native evaluation for DiagramAgent (and a DOT-native variant for your model) to avoid cross-format conversion via GPT.\n- Did you de-duplicate near-identical figures/descriptions across splits? Please report a hash/similarity analysis.\n- Show results across node-match thresholds and with alternative label normalization; report effects of handling multi-edges/duplicates.\n- Quantify contributions of each curation stage (classifier, OCR/detection, GPT refinements) to final performance; show training with DOT1-only/DOT2-only."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CiWs9QxZLV", "forum": "dHWtOTiceO", "replyto": "dHWtOTiceO", "signatures": ["ICLR.cc/2026/Conference/Submission9589/Reviewer_yJFJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9589/Reviewer_yJFJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540967759, "cdate": 1761540967759, "tmdate": 1762921137739, "mdate": 1762921137739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**TEXT2ARCH** introduces a 75K+ text–DOT–image dataset that fills a clear gap for text-to-architecture generation, compares **DOT1/2/3** variants, and adds **novel graph-level metrics**; fine-tuned 7B–8B models (notably DeepSeek-7B) beat DiagramAgent and few-shot baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **TEXT2ARCH fills a clear data gap.** The dataset addresses a previously missing resource for text-to-architecture diagram generation with aligned text–code–image triplets.\n2. **Comprehensive comparison of DOT variants.** Evaluating DOT1/DOT2/DOT3 provides a thorough view of how different curation/refinement stages affect quality.\n3. **Novel, task-appropriate metrics.** The graph-level evaluation (node/edge F1, PR-AUC, Jaccard) is thoughtful and well-aligned with the problem’s structural nature."}, "weaknesses": {"value": "1. **Compilation success rate is unreported.** The paper does not quantify the percentage of generated DOT that compiles successfully, which is critical for practical usability.\n2. **Limited qualitative evidence.** Case studies mostly show prompts and code; they lack rich visual side-by-side outputs and analyses of both good and bad generations across compared methods. It would be stronger to show the rendered diagrams and juxtapose multiple model outputs with brief error analyses.\n3. **Insufficient SFT details (around line 322).** SFT setup is under-specified, only the prompt is given. Training loss/objectives should be described to ensure reproducibility.\n4. **Missing TikZ results despite related discussion.** While focusing on DOT is reasonable, the paper cites TikZ-based prior work; a small TikZ transfer study (even limited) would help position the approach and set expectations for broader applicability."}, "questions": {"value": "See weaknesses; I’m open to revising the score if the rebuttal provides sufficient evidence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y8nYB6EPXw", "forum": "dHWtOTiceO", "replyto": "dHWtOTiceO", "signatures": ["ICLR.cc/2026/Conference/Submission9589/Reviewer_uvQ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9589/Reviewer_uvQ1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683334208, "cdate": 1761683334208, "tmdate": 1762921137521, "mdate": 1762921137521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a dataset for diagram generation, based on descriptions. The paper provides a dataset involving the DOT language. It then continues to explore closed-source models such as GPT4o and fine-tuned models, leveraging automatic metrics for evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- a new dataset\n\n- insights on the performance of different models, e.g. fine-tuning helps\n\n- discussion of a relevant new problem, even though there are already some existing works on similar tasks"}, "weaknesses": {"value": "- There is no human evaluation at all\n\n- While competitor approaches such as Automatikz are mentioned, there is no comparison\n\n- Some competitor models are missing, e.g. TikZero\n\n- Arguably, just fine-tuning a model on the dataset could be considered a bit incremental in contribution\n\n- I feel more sophisticated automatic metrics such as the ones proposed in TikZero or Automatikz should be explored\n\nTikZero: https://iccv.thecvf.com/virtual/2025/poster/51"}, "questions": {"value": "l. 126: I don't understand the argument why you don't want to evaluate the image: comparing the ground-truth image to the generated image is very important from a user perspective\n\n- Why did you exclude GPT5?\n\n- In l.203ff: you do some human filter - where are the agreements? How reliable are the humans involved?\n\n- l.310: so, few-shot prompting is meaingless?\n\n- l.322ff: But SFT mostly leverages training on the training dataset, right?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RdBDI9weG7", "forum": "dHWtOTiceO", "replyto": "dHWtOTiceO", "signatures": ["ICLR.cc/2026/Conference/Submission9589/Reviewer_4mvK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9589/Reviewer_4mvK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737005847, "cdate": 1761737005847, "tmdate": 1762921137234, "mdate": 1762921137234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a dataset for training models to automatically convert natural-language descriptions into scientific architecture diagrams. The authors report that a compact model trained on the proposed data outperforms larger models (e.g., GPT-4o) on the benchmark tasks defined in the paper."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Framing NL to diagram generation for scientific architectures is a well-scoped and timely problem with practical value for documentation and education.\n2. The paper provides a purpose-built dataset aligned with the task, which can catalyze further research and standardized evaluation.\n3. Initial experiments indicate that a smaller, task-specialized model can surpass much larger general-purpose models, suggesting meaningful gains from domain-specific supervision."}, "weaknesses": {"value": "1. The paper’s core innovations and their separation from prior art are not sufficiently explicit. \n2. The definition of scientific architecture, the selection of the 99 images in Fig. 3, and the complexity distribution of diagrams across the full dataset are unclear.\n3. It is unclear whether a diagram has a single “correct” \\texttt{DOT} representation, and how correctness is measured when multiple valid encodings exist. \n4. For the same digram, only one correct DOT exist? If not, how they meature the correctness?\n5. If many diagrams resemble the simple patterns in Fig. 1, current VLMs may already perform well."}, "questions": {"value": "1. Definition and coverage of “scientific architecture.”\n2. How were the 99 images in Fig. 3 selected, and how does their complexity compare with the full dataset?\n3. Is the target \\texttt{DOT} constrained by a formal grammar during training/inference?\n4. How robust is the model to paraphrase, long/underspecified descriptions, or OOD domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H22Gmry7Gt", "forum": "dHWtOTiceO", "replyto": "dHWtOTiceO", "signatures": ["ICLR.cc/2026/Conference/Submission9589/Reviewer_pUFM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9589/Reviewer_pUFM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970572781, "cdate": 1761970572781, "tmdate": 1762921136950, "mdate": 1762921136950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}