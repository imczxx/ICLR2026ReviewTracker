{"id": "SPElkPRurl", "number": 19371, "cdate": 1758295738754, "mdate": 1759897042712, "content": {"title": "DMAP: A Distribution Map for Text", "abstract": "Large Language Models (LLMs) are a powerful tool for statistical text analysis, with derived sequences of next-token probability distributions offering a wealth of information. Extracting this signal typically relies on metrics such as perplexity, which do not adequately account for context; how one should interpret a given next-token probability is dependent on the number of reasonable choices encoded by the shape of the conditional distribution. In this work, we present DMAP, a mathematically grounded method that maps a text, via a language model, to a set of samples in the unit interval that jointly encode rank and probability information. This representation enables efficient, model-agnostic analysis and supports a range of applications. We illustrate its utility through three case studies: (i) validation of generation parameters to ensure data integrity, (ii) examining the role of probability curvature in machine-generated text detection, and (iii) a forensic analysis revealing statistical fingerprints left in downstream models that have been subject to post-training on synthetic data. Our results demonstrate that DMAP offers a unified statistical view of text that is simple to compute on consumer hardware, widely applicable, and provides a foundation for further research into text analysis with LLMs.", "tldr": "We introduce DMAP, a tool for analyzing statistical properties of text and large language models.", "keywords": ["Large Language Models", "Entropy", "Statistical Text Analysis", "Post-training", "Supervised fine-tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1d9dd482ff4a0a0abe66c5d91823ce099785e8c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a straightforward method for visualizing a probability distribution on the unit interval which measures how the tokens in a piece of text relate to the token distribution of an evaluator language model. The visualization method has the property that if the same model M is used both to sample the piece of text, and to generate the visualization, then a uniform distribution should be produced. Furthermore, standard statistical tests of uniformity can be used to confidently reject the hypothesis that a given text dataset was produced by a particular model.\n\nThe authors then use this visualization method to point out an error in a significant line of work on detecting LLM generated text. In particular, this line of work inadvertently used top-k sampling when generating text, which clearly produces a bias towards more likely tokens. The property of bias towards more likely tokens was then proposed as a method for LLM generated text detection. The visualization method of the submission can be used to immediately see that this property is true for top-k sampling, but not for standard sampling. This implies that standard sampling can easily bypass these prior detection techniques. The method can also be used to as a straightforward statistical validation step, to verify the sampling model and settings (temperature, top-k, nucleus sampling etc) for any given text dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is straightforward and the visualizations are easy to understand. Most importantly, the method was used to identify a serious error in prior work on LLM generated text detection. The identification of the error alone seems to me a valuable contribution. I am not an expert in the LLM-generated text-detection research area, and so would defer to other reviewers if they have additional insight into the significance of this result."}, "weaknesses": {"value": "Beyond the particular case of finding the error in prior work on LLM text detection, it is unclear how useful this method will be. In particular, the method is rigorous and clearly useful for saying that a particular text was *not* generated by an LLM, but it is unclear how robust this method is. For example, if the prompt used to generate the text is not available (as it generally is not when attempting to detect LLM generated text) it is unclear how this method will perform. The rigorous theoretical results do not hold without access to the prompt, and it is very unclear whether the empirical results will either."}, "questions": {"value": "Does the method still at least give interesting empirical results without access to the prompt, but only the model response? Evidence in this direction would cause me to increase my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QA2SfnFmIm", "forum": "SPElkPRurl", "replyto": "SPElkPRurl", "signatures": ["ICLR.cc/2026/Conference/Submission19371/Reviewer_5pHM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19371/Reviewer_5pHM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821702893, "cdate": 1761821702893, "tmdate": 1762931302072, "mdate": 1762931302072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DMAP, a simple yet effective algorithm that maps natural language text into a probability distribution on the [0,1] interval using a language model. DMAP provides an intuitive statistical visualization of how closely a text aligns with the model's generation configuration. The authors further demonstrate three possible applications of DMAP through concrete examples, supported by theoretical analysis or experimental evidence, to present its versatility and potential."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a clear and effective mapping algorithm that projects text into a probability distribution over [0,1] via a language model, enabling intuitive visualization and analysis.\n2. The authors explore three potential applications of DMAP, each supported by either experimental results or theoretical analysis."}, "weaknesses": {"value": "1. The core objective of DMAP is similar to the well-known Probability Integral Transform (PIT) used for model calibration [1–3]. Both map predicted probabilities to the [0, 1] interval and yield a uniform distribution when predictions are perfectly aligned with the model. DMAP can be interpreted as an LLM-specific extension of PIT that replaces i.i.d. sampling with conditional sampling and adds an entropy-weighting. The authors should explicitly discuss this connection and compare DMAP against a simple PIT-based baseline to clarify what additional insight DMAP provides in the context of LLM.\n\n2. The paper claims that existing metrics such as log-likelihood and entropy fail to reflect \"context\" of the next-word prediction, yet these metrics already capture contextual information through the softmax normalization over the full next-token prediction's distribution. DMAP does not introduce new contextual information beyond this. Instead, it re-expresses and visualizes the same distributional information through a conditional sampling perspective. The authors should clarify how their notion of context differs from that already implicit in standard probability-based metrics.\n\n3. DMAP effectively visualizes the inherent limitation of curvature-based AI-text detection methods: AI-generated text tends to be more \"self-consistent\" with the scoring model than human text, but this assumption breaks when the source model or decoding parameters differ substantially. While the paper illustrates this limitation, it does not demonstrate how DMAP may mitigate it as the authors claim in the paper. The authors should show how DMAP could improve detection robustness under cross-model and cross-config scenarios by applying DMAP to existing curvature-based AI-text detection methods empirically.\n\n4. The paper does not specify how the reliability of DMAP visualizations changes with the input length. Since DMAP samples one value per token, short sequences may yield unstable or misleading results. The authors should provide theoretical or empirical guidance on the minimal input length required by DMAP, given the model and decoding parameters used.\n\n\n[1] Fisher, Ronald Aylmer. Statistical methods for research workers. No. 5. Oliver and Boyd, 1928.\n\n[2] David, F. N., and N. L. Johnson. \"The Probability Integral Transformation When Parameters Are Estimated from the Sample.\" Biometrika, vol. 35, no. 1/2, 1948, pp. 182–90.\n\n[3] Gneiting, Tilmann, Fadoua Balabdaoui, and Adrian E. Raftery. \"Probabilistic forecasts, calibration and sharpness.\" Journal of the Royal Statistical Society Series B: Statistical Methodology 69.2 (2007): 243-268."}, "questions": {"value": "Please check my questions in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3t1h8y9ZRx", "forum": "SPElkPRurl", "replyto": "SPElkPRurl", "signatures": ["ICLR.cc/2026/Conference/Submission19371/Reviewer_soHE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19371/Reviewer_soHE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968024040, "cdate": 1761968024040, "tmdate": 1762931301535, "mdate": 1762931301535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents DMAP that maps texts onto a distribution in the unit interval under a reference evaluating LLM, which provides both a direct visualization and quantifiable metrics for texts against the LLM, enabling identification of LLM-generated texts, validation of generation parameters, and the analysis of downstream LLMs."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The main proposal, DMAP, is well-grounded, intuitive, innovative, and immediately useful. \n\n2. The effectiveness of DMAP is demonstrated through both comprehensive visualizations and analyses, and a good suite of downstream problems backed by solid experiments. \n\n3. The authors points out a crucial problem with current state-of-the-art machine-generated text detection methods, and demonstrated DMAP's power in its identification."}, "weaknesses": {"value": "To clarify, despite these weaknesses, I still think the paper has significant contributions to recommend acceptance. I'd like my review & subsequent discussions with the authors to work towards improving the paper quality. \n\n1. As an antithesis to DMAP's ability to validate model sampling decoding strategies, DMAP is sensitive to decoding strategies and may not work as well if the temperature / top-K / top-p parameters change. This also may hinder DMAP's ability of becoming an effective tool for actual machine-generated text detection. I suspect there are ways to improve this, but it would require extensive additional experiments, especially more studies of DMAP on human-generated texts. \n\n2. While the idea of DMAP becomes simple once understood, the paper could do a better job presenting the mathematical formulation. Here are some specific pointers: \n    1. The introduction of $V_i^a$ and $V_i^b$ make the presentation bloated and unintuitive, and there should be some way around it. It may also be helpful if the intuitive explanation on Lines 155-157 are moved to the front, so the technical passage become easier to read (this is my recommended principle for writing in general). \n    2. I fully understand the motivation of Section 3.2, and why it's a natural extension of the discrete, unweighted version of DMAP in Section 3.1. However, it will be helpful if, before the technical definitions are given, a quick explanation of the two things being done is given on Line 187, for example describing the extension of a single, uniformaly sampled point in an interval to a more \"quantum\" approach of keeping the interval intact as a step function, and the entropy weighting technique. \n    3. Proposition 3.1 mentions pure sampling without first defining it. \n    4. The example of temperature $\\tau$ in Lines 172-176 can also end with \"sampling from $p$ with temperature $\\tau$\" instead of \"sampling from $p$ with the appropriate decoding strategy\". \n\n3. There's currently no analysis on the efficiency and sample complexity of DMAP, theoretical or empirical. \n\n4. The visualization in Figure 1 is an important direct illustration that almost immediately explains the mechanism of DMAP. However it's too small to read and could use some more work."}, "questions": {"value": "Q1. What, if anything, is stopping DMAP from developing into a tool for machine-generated text detection? \n\nQ2. Why is the entropy directly used to weight the step functions at different tokens for Section 3.2's definition of DMAP? Did you consider other approaches rather than clipping, such as using the entropy of top-K tokens? \n\nQ3. Figure 4 sees a heavy \"tail\" around the far end of the interval at 1.0. If I understand correctly, this corresponds to highly unusual tokens according to the validating LLM. Can you explain this further? Is this a common artifact of fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VqPh2XbCyi", "forum": "SPElkPRurl", "replyto": "SPElkPRurl", "signatures": ["ICLR.cc/2026/Conference/Submission19371/Reviewer_bpqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19371/Reviewer_bpqg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991845536, "cdate": 1761991845536, "tmdate": 1762931301078, "mdate": 1762931301078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DMAP, a mathematically grounded method to map next-token distributions from an LLM into samples on the unit interval, producing a unified representation that captures both rank and probability information. The authors show DMAP can (i) validate generation parameters, (ii) highlight weaknesses in machine-generated text detectors based on probability curvature, and (iii) reveal statistical fingerprints from instruction-tuning, especially when synthetic data is used."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to read. \n\n- The paper has discussed mathematical connections and also provided empirical results to support the claims made. \n\n- This work provides some fresh ideas of going beyond perplexity while using LLMs to evaluate a given text. \n\n- The paper presents nice visualization to explain the ideas."}, "weaknesses": {"value": "- As a reviewer, I am confused about the problem setting in introuction, specifically the discussion of \"contextualization\nproblem \", and why it is important. \n\n- Following to that, the connection to AI generated text detection seems vague and not clearly established, and then authors introduce the DMAP algorithm without motivating for the need for it. This raises concerns about the novelty and not clear what is the key contribution of this work.\n\n-  The authors talk about rannk being higher of v, but in the mathematical notation, it is lower? Am I missing something? \n\n- The authors have mentioned about the limitaitons of DMAP and then proposed entropy weighted DMAP, this section discussion is confusing, is it possible to empirically highlight or show the limitations to make it clear what is being addressed with entropy weighted DMAP? \n\n- As discussed in Table 1, it isnot clear why one would not use top-k sampling in practice? Ad what exact DMAP is predicting for these experiments? \n\n- Can author comment more about the connection with uncertainty quantification literature?"}, "questions": {"value": "Please refer to the discussion in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3IkJysO0SZ", "forum": "SPElkPRurl", "replyto": "SPElkPRurl", "signatures": ["ICLR.cc/2026/Conference/Submission19371/Reviewer_BUmF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19371/Reviewer_BUmF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762043893223, "cdate": 1762043893223, "tmdate": 1762931300761, "mdate": 1762931300761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}