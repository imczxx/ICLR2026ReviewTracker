{"id": "W8VCH9x1HZ", "number": 18278, "cdate": 1758285963176, "mdate": 1759897114657, "content": {"title": "Knowledge Distillation as Decontamination? Revisiting the “Data Laundering” Concern", "abstract": "Concerns have been raised that knowledge distillation may transfer test-set knowledge from a contaminated teacher to a clean student—a “data laundering” effect that potentially threatens evaluation integrity. In this paper, we assess the severity of this phenomenon. If these concerns regarding data laundering are minor, then distillation could be used to mitigate risks of direct data exposure. Across eight benchmarks, we find that substantial laundering is the exception rather than the rule: unlike the large performance gains from direct contamination, any accuracy inflation from laundering is consistently smaller and statistically insignificant in all but two cases. More broadly, we find that the two phenomena are weakly correlated, suggesting that laundering is not simply a diluted form of contamination but a distinct effect that arises primarily when benchmarks exhibit large train–test distribution gaps. Motivated by this, we conduct controlled experiments that systematically enlarge the train–test distance on two benchmarks where laundering was initially negligible, and observe that laundering becomes more significant as the gap widens. Taken together, our results indicate that knowledge distillation, despite rare benchmark-specific residues, can be expected to function as an effective decontamination technique that largely mitigates test-data leakage.", "tldr": "", "keywords": ["Knowledge Distillation", "Data Contamination", "Benchmark Integrity", "Data Decontamination"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91bc579575a0c4e66f2842e126bf83b0b0bec9dc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates Data Laundering. It transfers test-set knowledge from a contaminated teacher to a clean student via distillation. Through a systematic study on eight classification benchmarks, the authors find these concerns are largely overstated. They demonstrate that performance inflation from laundering is minimal compared to direct contamination and often statistically insignificant. They identify the train-test distributional gap as the key driver for the rare instances of laundering. The paper argues that knowledge distillation, rather than being a liability, acts as an effective \"decontamination\" technique, robustly mitigating test-data leakage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Timely and Significant Topic:** As concerns about data contamination in large pre-training corpora grow, understanding the downstream effects of \"laundered\" knowledge through common techniques like distillation is critical for the ML community. This paper provides a data-driven analysis of a widely held concern.\n\n2. **Empirical Rigor:** The study systematically isolates the laundering effect from direct contamination across eight diverse benchmarks. The use of bootstrapping for statistical significance and the robustness checks across different distillation strategies (soft/hard labels, mixed/pure loss) strengthen the credibility of the findings.\n\n3. **Novel Insights:** The paper's most valuable contribution is not just in quantifying the laundering effect but in identifying when it occurs. The hypothesis and subsequent controlled experiments that link laundering to the train-test distributional gap are a novel and important finding."}, "weaknesses": {"value": "1. **Scope of Models:** The experiments are confined to BERT-base models. While this is a controlled and reasonable setup, the community's primary contamination concerns now center on massive-scale decoding LLMs.\n\n2. **Scope of Tasks:** The study is focused exclusively on classification. Data laundering in generative tasks (for exmaple,  text generation) is a more complex and arguably more insidious problem, as the \"knowledge\" being transferred is far richer than class logits."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "liwyyoEccz", "forum": "W8VCH9x1HZ", "replyto": "W8VCH9x1HZ", "signatures": ["ICLR.cc/2026/Conference/Submission18278/Reviewer_TbMs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18278/Reviewer_TbMs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761425803211, "cdate": 1761425803211, "tmdate": 1762928001157, "mdate": 1762928001157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines whether knowledge distillation (KD) can transfer test-set leakage from a contaminated teacher to a clean student. Across eight benchmarks, the study finds that while data laundering exists, its impact is generally small and far weaker than direct contamination. The results suggest that KD largely mitigates test-set leakage rather than amplifying it, indicating that KD can serve as an effective decontamination technique in most cases."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an interesting question: the role of KD in data laundering.\n- The authors conduct a large-scale assessment across eight benchmarks to determine the prevalence and magnitude of data laundering. \n- The authors identify that the effect of data laundering via KD is substantially smaller than that of direct contamination."}, "weaknesses": {"value": "- The clarity and readability of the paper would benefit from a more explicit and formal definition of direct contamination in the introduction.\n- The study focuses solely on encoder-based BERT and classification task. Given the prominence of decoder-only LLMs and the rising importance of generative tasks, where KD and contamination could be more complex and potentially more harmful, the generality of the current findings to modern LLMs is unclear. Furthermore, the applicability of the conclusions to other modalities such as images also remains unaddressed.\n- The authors' experiments assume KD is performed strictly using clean ID data. However, in generator-based data-free KD settings (e.g., [1–2]) or KD pipelines for generative models (e.g., [3]), if a teacher is contaminated, the generated synthetic data itself may embed and transfer leaked knowledge, making laundering more pronounced. The authors should at least discuss the applicability and limitations of their conclusions under such settings, where leakage pathways may differ from those in standard supervised KD and classification tasks.\n- Some references are incorrect: \n\"Mingyuan Hong, Yicheng Li, Zijian Yang, Yisen Wang, Zhangyang Wang, et al. Revisiting data-free knowledge distillation with poisoned teachers.\" the first author should be Junyuan Hong.\n\n[1] Revisiting data-free knowledge distillation with poisoned teachers. ICML 2023\\\n[2] When Data-Free Knowledge Distillation Meets Non-Transferable Teacher: Escaping Out-of-Distribution Trap is All You Need. ICML 2025\\\n[3] A Survey on Knowledge Distillation of Large Language Models. ArXiv 2024"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BeK3VNYzPS", "forum": "W8VCH9x1HZ", "replyto": "W8VCH9x1HZ", "signatures": ["ICLR.cc/2026/Conference/Submission18278/Reviewer_Vwqr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18278/Reviewer_Vwqr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998294894, "cdate": 1761998294894, "tmdate": 1762928000815, "mdate": 1762928000815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systemically revisits the problem of data laundering—the unconscious transfer of test-set knowledge via knowledge distillation (KD) from a contaminated teacher model to a clean student. \n\nFirst, this paper conduct systematic experiments across eight text classification benchmarks using BERT-based teachers and DistilBERT students. They find that performance gains from laundering are significantly smaller and often statistically insignificant compared to those from direct contamination, which suggests that KD may indeed function as an effective decontamination method.   \nSecondly, through sample-level correlation analysis, this paper shows that laundering is a distinct mechanism rather than a diluted form of contamination.   \nFinally, this paper demonstrates that laundering effects intensify when the train–test distributional gap widens, suggesting benchmark-specific susceptibility rather than universal vulnerability."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides the first large-scale, controlled study that quantifies the prevalence and mechanisms of data laundering. Compared to the pioneer work (Mansurov et al. 2025), the experimental setting in this paper is more scientific and comprehensive, which provides more convinced understanding for data laundering problem in KD. \n\n2. This paper suggests that KD actually generally reduces contamination rather than propagating it, which mitigates the concern of its usage in practical, especially in the LLM era."}, "weaknesses": {"value": "1. This paper mainly explores BERT-based models, which is kind of out-of-date in the LLM era. It would be better that if this paper could explores the data laundering issue in GPT-style models in future works, as it's more popular and practical recently. \n\n2. Based on thorough experiments in this paper, it would be better if this paper can provides some guidance for test set or benchmark curation in terms of train-test domain gap. I believe this can be a potential application for the data laundering issue in practical."}, "questions": {"value": "1. Given the train-test domain gap finding in this paper, can the author provide some guidance or suggestions for benchmark curation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZrYWF2a7Od", "forum": "W8VCH9x1HZ", "replyto": "W8VCH9x1HZ", "signatures": ["ICLR.cc/2026/Conference/Submission18278/Reviewer_Uoo6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18278/Reviewer_Uoo6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152948938, "cdate": 1762152948938, "tmdate": 1762928000321, "mdate": 1762928000321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether knowledge distillation (KD) from a contaminated teacher to a clean student actually “launders” test-set knowledge, thereby inflating evaluation results even when the student never directly sees test data. Using eight text classification/NLI benchmarks, the authors compare four families of models per dataset: (i) clean vs. contaminated baselines trained directly on data; (ii) clean vs. contaminated teachers; and (iii) students distilled from clean vs. from contaminated teachers, always on clean data. They define benchmark-level leakage ($\\Delta$ Acc) and sample-level leakage ($\\Delta$ laund vs. $\\Delta$ contam) to separate direct contamination from laundering and show three main things:\n\n1. direct contamination is large and highly significant across all tasks;\n\n2. laundering exists but is much smaller and often statistically insignificant;\n\n3. laundering and contamination are only weakly correlated at the sample level, so laundering is not just a \"weaker\" version of contamination.\n\nThe authors further hypothesize and confirm via controlled experiments on emotion and rotten_tomatoes, that laundering becomes more visible when the train-test distributional gap is deliberately widened, i.e. when test data is semantically farther from training data. The conclusion is cautiously positive: KD can serve as a decontamination buffer in many realistic settings, though it is not perfect and can fail on benchmarks with large train-test shifts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear problem formulation and motivation: The paper isolates a concrete, timely question that sits right at the intersection of model compression and benchmark integrity. \n\n2. Well-controlled experimental design: The two-stage setup (dirty vs. clean teacher to distilled student trained only on clean data) is clean and makes causal interpretation easier: any gain of the dirty student over the clean student must have come through the teacher. The inclusion of clean/dirty baselines with the same student architecture is a good control. \n\n3. Breadth across 8 benchmarks: Using topic, sentiment, intent, emotion, NLI gives the results some external validity, and they correctly note that laundering is benchmark-specific. \n\n4. Sample-level analysis: Moving from aggregate accuracy to per-sample difficulty and leakage scores ($\\Delta$ laund, $\\Delta$ contam) is a real contribution-it shows weak Pearson correlations and even slightly negative for SNLI, supporting the \"distinct mechanisms\" claim. \n\n5. Distribution-gap experiment: The controlled 5-level stratification that gradually pushes training data away from the test distribution is thoughtful and directly tests their hypothesis; it’s rare to see this done for contamination work. \n\n6. Takeaway for benchmark designers: The paper ends with an actionable message. Multiple test sets at different distances reduce the risk of KD-based laundering. It's not just diagnosis but also design guidance."}, "weaknesses": {"value": "1. Limited architecture and task scope: All core results are on BERT-base to DistilBERT, encoder-only, classification tasks, which the paper itself mentioned this. This significantly limits the generalizability of the findings. The conclusions from this paper may be spurious and may not extend to other conditions (task, architecture, scale). Additionally, the original \"data laundering\" issue is strongest for decoder-only LLMs and generative or instruction-following tasks, where KD often uses richer signals. The current evidence base is therefore narrower than the title (\"revisiting the concern\") suggests. A reviewer can accept the current claim only as: \"for small/medium English classification with BERT-like models, KD is mostly a buffer\". Generalizing to LLMs is still open. \n\n2. Contamination protocol is single-style and generous. They inject the full test set and remove an equal number of training samples. That's an extreme, clean, known contamination pattern; real-world pretraining leakage is usually partial, fuzzy, or paraphrased. It’s not obvious that a KD bottleneck will stay this effective under messier or low-rate contamination. The paper should at least discuss low-contamination regimes or partial overlaps. \n\n3. Effect sizes sometimes tiny, but emphasized strongly. On several datasets the student’s dirty–clean gap is <=1% and sometimes statistically non-significant. The paper’s narrative (\"KD as decontamination\") leans a bit harder than the numbers strictly justify; one could also say \"we failed to induce consistent laundering except under adversarially widened gaps\". A more cautious framing would help. \n\n4. Train-test gap story is suggestive, not airtight. They do show that laundering becomes more detectable as the gap widens, but on emotion the magnitude itself doesn't clearly grow-only the p-values get smaller. That weakens the causal reading (\"gap -> laundering magnitude\"), at least for one dataset. The paper should disentangle: (i) teacher now actually learns test-specific patterns vs. (ii) student variance shrinks so we can just see the effect. \n\n5. No comparison to other KD objectives meant for privacy/decontamination. They mention reverse KL and mixed-objective settings in the appendix, but the main narrative doesn't tell us whether changing the distillation loss makes laundering harder or easier. That would be highly actionable. \n\n6. Evaluation mostly on accuracy. Since laundering is about \"benchmark integrity\", one might want to know whether rank-based, calibration-based, or subset metrics are more vulnerable. If laundering only moves 0.6-1.4 accuracy points, is that actually a practical threat, e.g. to leaderboard positions? The paper doesn’t quantify that."}, "questions": {"value": "1. Low-contamination regime: What happens if the teacher sees only 5–10% of the test set (or paraphrased test items) during fine-tuning? Does the KD bottleneck still suppress laundering to sub-1%?\n\n2. Other teacher-student pairs: Would the same conclusion hold for (T5, decoder style) where the capacity gap is bigger and the teacher is stronger? Right now we don’t know if stronger teachers launder more.\n\n3. Generative settings: Can the authors comment on summarization or code evaluation benchmarks, where \"exact match\" is less relevant but pattern transfer is easier?\n\n4. Realistic leakage patterns: Your contamination is \"inject full test + remove train\". Have you tried something like \"mix 10% test into train without removal\", i.e. inflate total size, which is closer to pretraining-style accidental inclusion?\n\n5. Metric sensitivity: On the datasets where laundering was \"not statistically significant\", what were the actual deltas and Cls? A small but systematic 0.5-0.8pt lift is still operationally important for competitive leaderboards.\n\n6. Defense implication: If KD is to be used as a decontamination step, what hyperparameters (α, temperature, loss type) minimize laundering while keeping accuracy? A table of “safe KD settings” would make the paper much more useful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xEATEnyb3g", "forum": "W8VCH9x1HZ", "replyto": "W8VCH9x1HZ", "signatures": ["ICLR.cc/2026/Conference/Submission18278/Reviewer_7UAR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18278/Reviewer_7UAR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18278/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762180410089, "cdate": 1762180410089, "tmdate": 1762927999843, "mdate": 1762927999843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}