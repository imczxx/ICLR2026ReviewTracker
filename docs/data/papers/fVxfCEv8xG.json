{"id": "fVxfCEv8xG", "number": 8498, "cdate": 1758087724878, "mdate": 1759897780366, "content": {"title": "A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory", "abstract": "Large Language Model (LLM) agents use memory to learn from past interactions, enabling autonomous planning and decision-making in complex environments. However, this reliance on memory introduces a critical security risk: an adversary can inject seemingly harmless records into an agent's memory to manipulate its future behavior. This vulnerability is characterized by two core aspects: First, the malicious effect of injected records is only activated within a specific context, making them hard to detect when individual memory entries are audited in isolation. Second, once triggered, the manipulation can initiate a self-reinforcing error cycle: the corrupted outcome is stored as precedent, which not only amplifies the initial error but also progressively lowers the threshold for similar attacks in the future. To address these challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive defense framework for LLM agent memory. The core idea of our work is the insight that memory itself must become both self-checking and self-correcting. Without modifying the agent's core architecture, A-MemGuard combines two mechanisms: (1) consensus-based validation, which detects anomalies by comparing reasoning paths derived from multiple related memories and (2) a dual-memory structure, where detected failures are distilled into ``lessons'' stored separately and consulted before future actions, breaking error cycles and enabling adaptation.\nComprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. This work shifts LLM memory security from static filtering to a proactive, experience-driven model where defenses strengthen over time. Our code is available in https://anonymous.4open.science/r/A-MemGuard-4775", "tldr": "", "keywords": ["agent", "trustworthy", "safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/66ecaeba00bc2a29fe265213c99b6883607594d8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "A-MemGuard proposes a proactive defense framework for LLM-based agents against memory-poisoning attacks. The key novelty is a structured reasoning-graph representation for each retrieved memory and a corresponding divergence metric S_div, computed via an LLM-as-a-judge approach. By comparing reasoning paths derived from multiple retrieved memories, the system detects anomalous (potentially malicious) paths whose structures deviate from the consensus (assumed to be benign).\n\nDetected anomalies are stored in a lesson memory, forming a dual-memory design where past failures are distilled into explicit “lessons” and injected as warnings before future reasoning steps. This self-taught correction mechanism allows the agent to adapt and avoid repeating prior error patterns. The evaluations show good improvements in ASR without losing utility compared to the considered baselines across LLM-backbones, attack modalities, and datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The main strength of this paper is the creation of a structured reasoning graph representation of agent-reasoning and a corresponding metric S_div, which enables separability of benign retrieved memories, and adversarial retrieved memories. This enables an interpretable signal for anamoly detection that generalizes across different LLM backbones.\n\n2. Storing lessons from previous failures enables self-correcting behavior, and enables long-term adaptation."}, "weaknesses": {"value": "1. Targeted attacks. --- The system model considered uses retrieved memories based on similarity to the prompt. Given this system and a fixed attack budget, an attacker would concentrate all his attacks on a specific domain (eg - tax information). This would break the assumption of the defense that a majority of the retrieved paths are benign. Since the top-k cannot be set very high, a small number of targeted memory poisoning conversations can dominate consensus and break the defense.\n\n2. Relies on LLM-as-a-judge - The defense relies on effective computation of S_div - essentially safety is now shifted from the base LLM to this LLM-as-a-judge - this is a single point of failure. In a white-box operation, this is easy to break (eg - adversarial suffixes that mimic benign reasoning structure that can break a judge. These adaptive attacks are typically transferable across unknown black-box models too). Empirical evaluations of adaptive attacks must be considered - and defended (Without such results, the defense remains empirical and adversary-specific). For such practical guarantees, the only waterproof defenses are ones like [1], where the judge LLM is outside the data path, and only in the control path (in context of prompt injections, but I think the same logic applies here)\n\n3. This defense increases token length for each reasoning step : i) user query, ii) validated memory block, iii) targeted lessons (how large are these?). Context pollution is a major concern, especially in complex real-world tasks - these extra tokens compete with task-releavtn information and may reduce utility for complex tasks. The authors' evaluation already shows that raising top-k can hurt utility. I have observed empirically (and there may be papers studying this, eg - [2,3]) that show that increasing number of security guidelines is not robust enough to improve safety, especially in smaller models considered in the paper.)\n\n4. The “lesson memory” introduces a long-term persistence channel: if an attacker succeeds in injecting even a single incorrect lesson (e.g., via point 1), that lesson becomes part of the permanent warning prompt. Because lessons are re-injected in every subsequent reasoning step, such corruption can propagate indefinitely, degrading utility.\n\n[1] Debenedetti, Edoardo, et al. \"Defeating prompt injections by design.\" arXiv preprint arXiv:2503.18813 (2025).\n[2] Liu, Nelson F., et al. \"Lost in the middle: How language models use long contexts.\" arXiv preprint arXiv:2307.03172 (2023).\n[3] Du, Yufeng, et al. \"Context Length Alone Hurts LLM Performance Despite Perfect Retrieval.\" arXiv preprint arXiv:2510.05381 (2025)."}, "questions": {"value": "See weaknesses.\nWilling to update score if concerns are met with empirical robustness evidence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "153Ls2DRZq", "forum": "fVxfCEv8xG", "replyto": "fVxfCEv8xG", "signatures": ["ICLR.cc/2026/Conference/Submission8498/Reviewer_BbHp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8498/Reviewer_BbHp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767962499, "cdate": 1761767962499, "tmdate": 1762920371252, "mdate": 1762920371252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces A-MemGuard, the first proactive defense framework designed to protect LLM agent memory systems against poisoning attacks. The framework addresses two critical vulnerabilities: (1) context-dependent attacks where malicious records appear benign in isolation but trigger harmful behavior in specific contexts, and (2) self-reinforcing error cycles where corrupted outputs become trusted precedents. A-MemGuard employs two synergistic mechanisms: consensus-based validation that detects anomalies by comparing reasoning paths derived from multiple memories, and a dual-memory structure that stores detected failures as \"lessons\" to prevent future mistakes. Extensive experiments across multiple benchmarks (ReAct-StrategyQA, EHRAgent, MMLU, and multi-agent systems) demonstrate that A-MemGuard reduces attack success rates by over 95% while maintaining high performance on benign tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work addresses a critical yet under-explored security vulnerability in LLM agent systems. The identification of context-dependent memory poisoning and self-reinforcing error cycles represents a significant contribution to understanding emerging threats in agentic AI systems. The problem formulation is clear and well-motivated with concrete examples that effectively illustrate the severity of these attacks.\n\n2. The consensus-based validation mechanism is elegant and well-designed. Rather than relying on isolated auditing of memory entries (which prior work has shown to be ineffective), the framework leverages multiple parallel reasoning paths to detect anomalies through structural divergence. \n\n3. The evaluation is thorough, covering diverse attack scenarios (direct poisoning, indirect injection, multi-agent systems), multiple LLM backbones (GPT-4o-mini, LLaMA-3.1-8B), and different retrieval architectures (DPR, REALM)."}, "weaknesses": {"value": "1. The paper lacks formal theoretical guarantees or analysis of when and why the consensus mechanism succeeds or fails. While the empirical knowledge graph analysis (Section 5.8) shows <1% overlap between benign and malicious reasoning paths, there is no characterization of the conditions under which this separability holds. What happens when adversaries specifically craft attacks to mimic the structural patterns of benign reasoning paths? The paper would benefit from a more rigorous theoretical framework that characterizes the adversarial space where consensus-based validation remains effective, and discusses potential failure modes when the assumption of structural separability is violated.\n\n2. While the paper mentions token cost analysis in Appendix E, the scalability discussion is insufficient. The framework requires generating K parallel reasoning paths for each query, applying LLM-based judgment for consistency checking, and maintaining/querying a separate lesson memory. For real-world deployments with high query volumes or limited computational budgets, these overheads could be prohibitive. The paper shows 7.8K vs 3.6K tokens (more than 2× increase over no defense), but doesn't discuss: (1) wall-clock time latency, (2) how performance degrades with varying memory sizes, (3) strategies for efficient implementation in production systems, or (4) trade-offs between K (number of retrieved memories) and defense effectiveness vs. computational cost.\n\n3. The threat model assumes adversaries operate with \"limited\" injections to avoid detection, but the exact constraints are not formalized. The evaluation primarily focuses on existing attack methods (AgentPoison, MINJA) without considering adaptive adversaries who know about the defense mechanism. Key questions remain unanswered: (1) Can adversaries craft memories that pass consensus validation by ensuring structural similarity to benign paths? (2) What if adversaries inject a larger proportion of malicious memories to shift the consensus itself? (3) How does the framework perform against attacks that gradually poison memory over time rather than through isolated injections? The paper would significantly benefit from evaluating against adaptive attacks specifically designed to evade consensus-based detection.\n\n4.  The dual-memory structure is a core contribution, but critical practical aspects are under-specified. The paper doesn't adequately address: (1) How lesson memory is maintained over long time horizons—does it grow unbounded? (2) What strategies exist for pruning outdated or redundant lessons? (3) How to handle contradictory lessons that may accumulate over time? (4) The potential for lesson memory itself to be poisoned if adversaries can trigger false positive detections strategically. Section 5.7 shows that lesson memory top-k=6 is optimal, but the relationship between lesson memory size, retrieval strategy, and long-term effectiveness needs deeper investigation. Additionally, the paper doesn't discuss failure cases where incorrect memories are mistakenly stored as lessons, potentially degrading future performance."}, "questions": {"value": "Can you provide theoretical analysis or bounds on when consensus-based validation is guaranteed to detect malicious memories?\nHave you evaluated the framework against adaptive adversaries who are aware of the consensus mechanism and specifically design attacks to evade it?\nWhat is the wall-clock latency overhead of A-MemGuard in real-time deployment scenarios, and how does it scale with memory database size?\nHow do you handle lesson memory management in long-running agents where the lesson repository may grow very large?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "peKEBPWdCe", "forum": "fVxfCEv8xG", "replyto": "fVxfCEv8xG", "signatures": ["ICLR.cc/2026/Conference/Submission8498/Reviewer_pQxo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8498/Reviewer_pQxo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848721495, "cdate": 1761848721495, "tmdate": 1762920370644, "mdate": 1762920370644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To protect LLM agents from agent memory attacks, the work proposes A-MemGuard, a non-invasive, proactive defense that 1) performs consensus-based validation by generating parallel, structured reasoning paths from multiple retrieved memories and detecting path divergence, and 2) introduces a dual-memory structure that distills detected anomalies into “lessons” stored separately and consulted to proactively revise future actions. Experiments show that A-MemGuard substantially reduces attack success rates across diverse scenarios\nwhile maintaining the highest utility on benign tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The research question of defending LLM agents against indirect and direct memory injection into agent memories is timely and interesting.\n\n2. The proposed framework performs effectively across different domains and settings.\n\n3. The experiments are comprehensive. In addition to demonstrating the effectiveness of the proposed framework, the author also provides good mechanistic probes, such as the knowledge-graph overlap analysis \n\n4. The paper is clearly written and easy to follow"}, "weaknesses": {"value": "1. The method presumes that benign paths dominate, but it does not evaluate attacks that bias retrieval so that a majority of retrieved memories are poisoned or highly correlated. An adaptive adversary can craft coherent-but-malicious clusters to win the in-context vote.\n\n2. The lesson memory is treated as trusted, but the paper does not analyze adversaries that target the lesson store explicitly (e.g., false positives inducing harmful “lessons”).\n\n3. Current baselines (LLM Auditor, Distil classifier, PPL) are relatively weak for this specific threat. Consider adding stronger retrieval- and reasoning-level defenses: e.g., query/answer cross-checking with self-consistency, fact-grounding verifiers, or recent agent security strategies (e.g., safety-aware reranking, adversarial tool-call vetting). This would better substantiate SOTA claims.\n\n4. Some wrong citations and typos: The two memory retrieval architectures, REALM and DPR, are both wrongly cited as irrelevant articles. Line 301, \"Applyment\", Line 460, \"thiough\""}, "questions": {"value": "1. How robust is the LLM-as-judge to adversarial prompt injection, especially if the raw memory snippets include instructions targeting the judge? Have you tried rule-based or programmatic extractors for the path graph to minimize susceptibility?\n\n2. How do you mitigate false positives in consensus validation to prevent accumulating incorrect “lessons” and over-conservatism over time?\n\n3. In Table 6, why under the main memory, top-k=8 setup, is the ASR significantly lower than other settings (especially for ASR-t)?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The paper studies defense strategies for LLM agent attacks."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wk0yp5AmkX", "forum": "fVxfCEv8xG", "replyto": "fVxfCEv8xG", "signatures": ["ICLR.cc/2026/Conference/Submission8498/Reviewer_deTy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8498/Reviewer_deTy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935618406, "cdate": 1761935618406, "tmdate": 1762920369865, "mdate": 1762920369865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "this paper introduces A-MemGuard, which is a new defense for LLM agent memory. The authors are trying to solve a pretty tricky problem: attackers can poison the memory with stuff that looks fine on its own but is actually malicious in a specific context. This also causes this nasty \"self-reinforcing error cycle\" where the agent learns from its own bad outputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* First off, the problem they're tackling is **super relevant**.Agent memory security is a big, new vulnerability, and the authors are right to point out that just checking memories one-by-one (isolated audits) isn't going to work.\n* The core idea of using \"consensus\" is **genuinely clever**. It's a really smart, original way to use the agent's own data to spot an attack, rather than relying on some external filter that doesn't have the right context.\n* The paper is also just **really well-written**. The figures, especially 1 and 2, make the concept very clear and easy to grasp.\n* They did a **good job on the experiments**. They tested against different kinds of attacks (direct poisoning and indirect interaction-based ones)and even showed it scales to multi-agent systems. Their claim about not hurting \"benign\" task accuracy seems to hold up well in the data (Table 3)."}, "weaknesses": {"value": "* My main issue is the cost. This thing is **wildly expensive**. You're taking what should be one LLM call and blowing it up to $K+1$ calls (K paths, plus the LLM Judge to compare them). The authors even admit in the appendix (Fig 7) that it more than doubles the token cost (from 3.6K to 7.8K tokens). This just isn't practical for any real-world application; the latency would be terrible.\n* The whole \"consensus\" idea is also **super fragile** at its core. It completely falls apart when $K=2$. Their own results in Table 6 show this: when it's a 1-vs-1 disagreement, the attack success rate shoots up to 42% and the normal accuracy tanks [cite: 788-789, 793]. This is a critical failure case they don't really solve.\n* They also don't seem to have considered a **\"majority attack.\"** What happens if I'm a smart attacker and I poison the memory with *three* malicious entries (and $K=4$)? Their system would form a *malicious* consensus and filter out the one *good* memory. This seems like a massive, unaddressed loophole.\n* Finally, that whole **\"Lesson Memory\" thing feels... tacked on**. It adds a ton of engineering complexity, and for what? The ablation (Table 5) shows removing it (\"w/o Lessons\") barely changes the final ASR (36.17% vs 40.63%). Worse, their own tuning (Table 6) shows that retrieving too many \"lessons\" *hurts* performance by adding \"distracting noise\", which *increases* the ASR-a and *decreases* benign accuracy. It seems to create more problems than it solves.\n* Overly Broad Definition of \"Memory\" Conflates Problem Scenarios: A key weakness is the paper's broad definition of \"agent memory.\" It conflates two distinct scenarios: 1) episodic memory learned from past interactions and 2) semantic memory retrieved from a knowledge base (i.e., a standard RAG scenario). The core motivating examples, such as the tax-query problem (Figure 2)  and the MMLU task (Figure 13), are clearly RAG/KB-style problems. This feels like the paper is \"force-fitting\" a defense for RAG/KB poisoning into the more general \"agent memory\" framing, which muddles the paper's true contribution."}, "questions": {"value": "1.  I really need the authors to clarify what happens *exactly* when $K=2$.When it's a 1-vs-1 disagreement, how does the \"LLM-as-a-Judge\" (Appendix A.1) break the tie? Does it just guess? Or does it fail safe and reject both, which would explain why the accuracy plummets?\n2.  What is the defense against a \"majority attack\" (i.e., when more than half the retrieved memories are malicious but self-consistent)? As far as I can tell, the system would fail. Am I missing something?\n3.  Related to the cost: have you even *tried* to make this cheaper? Could you batch the $K$-path generation into one call? Could you distill the expensive LLM Judge into a simple, fast classifier?\n4.  Honestly, is the \"Lesson Memory\" really pulling its weight? It adds noise and complexity for what seems like a very small benefit (Table 5) and can even make things worse (Table 6). Can you justify why this component is necessary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0WhpWl6n3C", "forum": "fVxfCEv8xG", "replyto": "fVxfCEv8xG", "signatures": ["ICLR.cc/2026/Conference/Submission8498/Reviewer_DjgL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8498/Reviewer_DjgL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8498/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962317149, "cdate": 1761962317149, "tmdate": 1762920369482, "mdate": 1762920369482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}