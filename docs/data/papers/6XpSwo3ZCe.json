{"id": "6XpSwo3ZCe", "number": 2893, "cdate": 1757298405996, "mdate": 1759898120807, "content": {"title": "Enhancing Deep Imbalanced Regression via Frobenius Norm Regularization", "abstract": "Deep Imbalanced Regression (DIR) aims to train a deep neural network (DNN) model specified for the regression tasks from an imbalanced training distribution and generalize well on an unseen balanced testing distribution. While modern solutions have achieved significant progress in DIR, the performance of the samples still varies a lot across the different shots. For instance, the samples in the majority-shot always outperform the underrepresented (median and few-shot) samples, which motivates us to investigate whether we can leverage the well-trained majority-shot samples to help the other under-trained samples. Empirically, we observe that previous solutions in DIR often \nproduce ordinal feature Frobenius norms across the majority-shot samples and considerably lower training Mean-Absolute-Error (MAE).\nMeanwhile, the underrepresented samples often violate the ordinality of the majority-shot Frobenius norms and exhibit a high training MAE.\nAs a result, this demonstrates that compared to the majority-shot samples, the underrepresented samples are still under-fitted during the training process. More importantly, we can identify the training performance through the lens of the ordinality of the Frobenius norm.\nMotivated by this observation,  we first analyze why the ordinality of the Frobenius norm can result in good training performance across the labels. Then, we introduce a feature regularization to encourage the feature Frobenius norms to be ordinal for all labels during the training process.  Moreover, we propose a novel model training strategy that incorporates the knowledge from the well-trained majority samples to help the underrepresented samples. By training a linear model from the majority-shot samples to predict the feature Frobenius norm of underrepresented samples, we fine-tune the previously trained model to enhance the outcomes of underrepresented samples.\nExtensive experiments over the real-world datasets also validate the effectiveness of our proposed method.", "tldr": "", "keywords": ["Deep Imbalanced Regression"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c45e2e27464fe020fddf9b977cdfaca9f118e040.pdf", "supplementary_material": "/attachment/28aa55431cce10b59fd56f498f363912538e0b8a.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a three-phase method for Deep Imbalanced Regression that (i) regularizes feature Frobenius norms to be ordinal across labels, (ii) learns a linear “F-norm predictor” from majority-shot prototypes, and (iii) fine-tunes the model to align minority examples’ feature norms to those predictions. Experiments on AgeDB-DIR, IMDB-WIKI-DIR, and STS-B-DIR report gains—especially on medium/few shots—while sometimes trading off many-shot accuracy. I currently see the contribution as interesting but incremental, with theoretical justification and evaluation rigor not yet at the level I expect for acceptance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "There is a lot of things I like about this paper and I need to commend Authors for: \n\n+ The proposed pipeline is clear and modular (ordinal regularization → majority-guided linear predictor → student fine-tuning), should be easy to implement and there is a clear reason why it boosts performance. \n+ The offered analysis connecting ridge regression scaling to a monotonic relation between label magnitude and feature norm is intuitive and motivates the norm-ordering prior.\n+ I appreciate the per-shot reporting and the explicit emphasis on medium/few shots; the narrative acknowledges a conscious trade-off on many-shot accuracy.\n+ Breadth across three datasets (two vision, one NLP) with concrete architecture details (ResNet-18/50; BiLSTM+GloVe) helps with generalization claims."}, "weaknesses": {"value": "However, I see multiple weak points in this paper, that while not being disqualifying for the presented approach, inhibit the potential of this work to be published in the top venue such as ICLR:\n\n+ The core ideas—prototype alignment, ordinal/entropy regularization, and rank/contrastive structure—feel close to prior lines (e.g., ranking, ordinal entropy). The “majority-guided linear predictor” is a small post-hoc head on top of prototypes. I’m missing a sharper conceptual advance beyond combining known ingredients.\n+ The ridge-based argument does not establish that enforcing global ordinal F-norms improves test MAE under imbalance; it also ignores invariances (e.g., rescaling features vs. regressor). The leap from “larger ∥z∥ correlates with label magnitude” to “make ∥z∥ globally monotone” is under-theorized.\n+ The method bins the continuous label space into B bins and computes mini-batch prototypes for losses. This introduces discretization noise and batch-dependence that may undermine the continuous-label objective the paper aims to improve.\n+ The three-phase schedule adds extra stages and epochs, yet the paper does not quantify wall-clock or compute overhead vs. baselines, an important consideration for ICLR-caliber claims.\n+ Reported improvements concentrate on medium/few shots, while many-shot degrades on AgeDB; the paper frames this as a “strategic trade-off,” but the real-world desirability depends on application. I am missing a thorough analysis of when that trade-off is net-positive.\n+ No statistical variability (seeds/CI) is reported; conclusions rely on single numbers and qualitative norm plots.\n+ For STS-B-DIR, the backbone is BiLSTM+GloVe rather than modern Transformers, weakening the generality claim on NLP.\n+ The paper toggles between MSE (method description) and MAE (implementation) as the main prediction loss, causing confusion about the training objective."}, "questions": {"value": "I would like for the Authors to address the following questions and I hope that the follow-up discussion will be useful for the Authors, even if helping to improve the paper for the next submission:\n\n+ Can you provide a theoretical guarantee (or at least a generalization bound/assumption set) under which enforcing global ordinal Frobenius norms provably reduces test MAE in DIR, beyond the ridge-scaling heuristic?\n+ How sensitive is the approach to the choice of binning (B) and to mini-batch prototype noise? Have you tried dataset-level prototypes or EMA prototypes to stabilize the losses?\n+ Please report compute/time and multiple-seed results (mean±std) for all datasets, and discuss the practical trade-off w.r.t. methods that do not require post-hoc phases.\n+ For AgeDB-DIR where many-shot degrades, can you quantify when the few/medium gains outweigh this loss (e.g., a cost-weighted metric or application-motivated utility)? \n+ On STS-B-DIR, would the conclusions hold with a Transformer encoder (e.g., DeBERTa/BERT) under the same imbalance protocol?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QF659eRN3V", "forum": "6XpSwo3ZCe", "replyto": "6XpSwo3ZCe", "signatures": ["ICLR.cc/2026/Conference/Submission2893/Reviewer_6WnH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2893/Reviewer_6WnH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771430403, "cdate": 1761771430403, "tmdate": 1762916433001, "mdate": 1762916433001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of DIR by introducing an ordinal regularization on the Frobenius norms of features. The authors propose a three-phase training framework: training a baseline regression network + learning a mapping from labels to prototype Frobenius norms +  enforcing the model to preserve ordinality in Frobenius norms of few-shot label regions. The approach is evaluated on three benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The motivation of enforcing ordinal consistency on feature scaling is interesting. The overall idea also demonstrates good empirical performance."}, "weaknesses": {"value": "### Theoretical\n- Although the linear ridge regression case suggests a correlation between label magnitude and feature scale, this relationship does not hold theoretically for deep neural networks, where the feature extractor is nonlinear and both $z$ and $\\omega$ are learned jointly. It feels more like the core idea (larger label leads to larger feature norms) is just empirically observed.\n\n- The related work [1] that the authors cite actually suggests the opposite: regression performance is sensitive to the overall feature scale, and changing it can hurt transferability. That means we usually want to keep the feature norms stable, not explicitly regularize them.\n\n- In Phases 2 and 3, since the Frobenius norms are mainly learned from majority labels, I think the imbalance still persists. \n\n- In standard deep networks, components like batch normalization and weight decay might destroy any monotonic relationship between label value and feature magnitude. \n\n[1] Xinyang Chen, Sinan Wang, Jianmin Wang, and Mingsheng Long. Representation subspace distance\nfor domain adaptation regression. ICML 2021\n\n### Experimental \n\n- The method introduces multiple loss terms and training phases, so more ablations should be presented in the main paper rather than in the appendix. It’s hard to evaluate the individual contribution of each phase or loss from the current presentation.\n\n- The improvement on STS-B-DIR is marginal. In Table 3 shows no significant improvement even on the medium-shot subset. \n\n### Writing\nThe paper also contains several confusing phrases and typos, and many figures are too vague or low-resolution to read clearly.\n\nFor example:\n\n(1) Figure 1: what metric is used here?\n\n(2) Lines 95–96: the reference \"Fig. 2 (1, 2, 3, 5, 6) in Fig. 2(4)\" is unclear.\n\n(3) \"Figure 1.3\", Line 124 in the appendix"}, "questions": {"value": "- Does Figure 5 in the appendix refer to the overall MAE? The proposed method mainly improves performance on medium & few-shot labels, so the analysis should focus more on these subsets. \n\n- Ep5 + Ep6 ($\\beta * L_{KL} + \\beta * \\alpha * L_{OE} + L_{any}$) contains hyper-parameter combinations and Fig 3 & 4 in Appendix is not enough. A more detailed 2D ablation (better in main manuscript) would better show how each term interacts.\n\n- I think Eq. (3) is the key part of the method, but there’s no ablation that evaluates its actual effect, for example by removing it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DpObXTzQu5", "forum": "6XpSwo3ZCe", "replyto": "6XpSwo3ZCe", "signatures": ["ICLR.cc/2026/Conference/Submission2893/Reviewer_JQPK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2893/Reviewer_JQPK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807940871, "cdate": 1761807940871, "tmdate": 1762916432823, "mdate": 1762916432823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the deep imbalanced regression (DIR) problem. Inspired by the influence of the Frobenius norm, the authors study the relationship of the Frobenius norm and the performance for different labels. Based on the empirical results, the authors propose a label-distance-based regularization method which focuses on the ordinality of feature Frobenius norm across the labels. The experiments on three datasets demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation of the paper is reasonable with the empirical investigation.\n- The proposed method can provide some new perspectives for the community.\n- The experimental results make sense to some extent."}, "weaknesses": {"value": "- The novelty is limited. The effect of the Frobenius norm on regression tasks is studied by the previous work. Therefore, the contribution of this paper is limited.\n- The texts in the figures are too small, blurry, and quite hard to read.\n- What is the meaning of y-axis of Figure 1? The authors claim that the performance of the few-shot and median-shot samples are worse than that of the majority-shot samples. However, it seems that the bars of few-shot performance are much higher than major- and medium-shots in Figure 1.\n- Although the experimental results demonstrate the effectiveness on few-shot labels, the overall performance is sub-optimal. The proposed method cannot achieve best overall performance on any datasets. Unless the authors can demonstrate the importance of few-shot accuracy, which should be quite important than overall accuracy, otherwise the experimental results will be unconvincing.\n- More implemental details of hyperparameters should be considered, such as loss weights, learning rate, weight decay, momentum, training epochs. It is absolutely possible that previous methods can also improve few-shot performance by modifying their hyperparameters.\n\n[1] Representation subspace distance for domain adaptation regression."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rZ2thQ4cXl", "forum": "6XpSwo3ZCe", "replyto": "6XpSwo3ZCe", "signatures": ["ICLR.cc/2026/Conference/Submission2893/Reviewer_oRTS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2893/Reviewer_oRTS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762256301561, "cdate": 1762256301561, "tmdate": 1762916432592, "mdate": 1762916432592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses Deep Imbalanced Regression (DIR), where training data is highly skewed but testing data is balanced1. The authors observe that well-performing majority-shot samples exhibit an ordinal relationship in their feature Frobenius norms and have low training Mean-Absolute-Error (MAE), whereas underrepresented (median and few-shot) samples violate this ordinality and have high training MAE2. Motivated by this, they propose a three-phase training strategy: (1) an ordinal regularization ($\\mathcal{L}_{ordinal}$) to encourage ordinal feature Frobenius norms across all labels3; (2) a majority-guided linear network ($f_F$) trained to predict these norms using only well-trained majority samples; and (3) a fine-tuning phase where the main model is regularized to match the norms predicted by $f_F$ for all samples, specifically benefiting the underrepresented ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The paper introduces a novel perspective on DIR by focusing specifically on the ordinality of the feature Frobenius norm as an indicator of training quality. Leveraging a simple linear predictor trained only on majority samples to guide the minority samples in a later phase is an interesting and seemingly effective strategy.\n\nQuality: The empirical motivation is well-demonstrated in Figure 2, showing the clear divergence in norm ordinality for underperforming classes. The method is tested against a comprehensive list of standard baselines (e.g., RankSim, Con-R, SupCR) on established DIR benchmarks (AgeDB-DIR, IMDB-WIKI-DIR, STS-B-DIR).\n\nSignificance: The method achieves substantial improvements on the most challenging sub-groups. For instance, on AgeDB-DIR, it reports a 14.6% relative improvement on few-shot samples compared to the best-performing baseline SupCR (9.24 vs 10.82 MAE). It also establishes new SOTA results for few-shot and medium-shot on IMDB-WIKI-DIR."}, "weaknesses": {"value": "Performance Trade-offs: While few-shot performance improves, there is a notable degradation in majority-shot performance in some cases. For AgeDB-DIR (Table 1), the proposed method achieves 7.35 MAE on \"Many\" shots, noticeably worse than SupCR (6.20) and VIR (6.39). The authors acknowledge this as a \"strategic trade-off\", but it raises concerns about whether strict ordinality regularization might be over-constraining the rich features of majority classes.\n\nProcedural Complexity: The proposed method requires a three-phase training process ($e_{main} + e_F + e_{SFT}$)14141414. This is more complex to implement and tune compared to end-to-end baselines.\n\nTheoretical Justification: The motivation in Section 3.2 relies on a simple ridge regression analysis to justify regularizing the Frobenius norm. While it serves as a decent heuristic, it may be too simplistic to fully explain the dynamics in deep non-linear networks."}, "questions": {"value": "Could the authors elaborate further on the significant performance drop for \"Many\" shot samples in AgeDB-DIR (7.35 vs SupCR's 6.20)? Does forcing the majority classes to adhere strictly to the predicted ordinal norm curve prevent them from learning more complex, discriminative features they otherwise would?\n\nIs the three-phase training strictly necessary? Specifically, could Phase 1 ($\\mathcal{L}_{ordinal}$) be combined with standard training, or is the sequential nature critical for the stability of the linear predictor in Phase 2?\n\nHow sensitive is the method to the choice of hyperparameters $\\alpha$ (Eq. 5) and $\\beta$ (Eq. 6)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Za7UvPU2f6", "forum": "6XpSwo3ZCe", "replyto": "6XpSwo3ZCe", "signatures": ["ICLR.cc/2026/Conference/Submission2893/Reviewer_RTFE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2893/Reviewer_RTFE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762474406521, "cdate": 1762474406521, "tmdate": 1762916432388, "mdate": 1762916432388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper finds feature Frobenius norm and training performance have an obvious relationship, and thus, proposes a three-step training pipeline to further enhance the performance on minority-shot samples by keeping their Frobenius norm."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. An interesting phenomenon is obversed: the correlation between feature Frobenius norm and training performance.\n\nS2. An effective three-step strategy to keep the ordinality of feature Frobenius norm for minority-shot samples.\n\nS3. Experiments exhibit that the proposed method could achieve a siginitant improvement on med. and few-shot labels."}, "weaknesses": {"value": "Major Weaknesses:\n\nW1: In abs., the authors claim they analysize why the ordinality of the Frobenius norm can result in good training performance, but this reviewer cannot find the analysis.\n\nW2: The proposed pipeline involves three traininig stages. It seems a little complex.\n\nW3: The experimental results indicate that this method sacrifices the performance of majority-shot samples.\n\nMinor Weaknesses:\n\nW4: Unreasonable paragraph allocation. E.g., the line 75~76 is an one-sentence paragraph.\n\nW5: low resolution and tiny fontsize of figures."}, "questions": {"value": "Q1: why could the Frobenius norm indicate the performance?\n\nQ2: why does the performance on major-label samples reduces?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QHxrI9pJA9", "forum": "6XpSwo3ZCe", "replyto": "6XpSwo3ZCe", "signatures": ["ICLR.cc/2026/Conference/Submission2893/Reviewer_T3PB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2893/Reviewer_T3PB"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2893/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762610875414, "cdate": 1762610875414, "tmdate": 1762916432212, "mdate": 1762916432212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}