{"id": "LktUOZayG9", "number": 4356, "cdate": 1757665990857, "mdate": 1759898037760, "content": {"title": "Unleashing LLMs in Bayesian Optimization: Preference-Guided Framework for Scientific Discovery", "abstract": "Scientific discovery is increasingly constrained by costly experiments and limited budgets, making efficient optimization essential for AI for science. Bayesian Optimization (BO), while widely adopted for balancing exploration and exploitation, suffers from slow cold-start performance and poor scalability in high-dimensional settings, limiting its effectiveness in real-world scientific applications. To address these challenges, we propose LLM-Guided Bayesian Optimization (LGBO), the first LLM preference-guided BO framework that continuously integrates the semantic reasoning of large language models (LLMs) into the optimization loop. Unlike prior works that use LLMs only for warm-start initialization or candidate generation, LGBO introduces a region-lifted preference mechanism that embeds LLM-driven preferences into every iteration, shifting the surrogate mean in a stable and controllable way. Theoretically, we prove that LGBO is not perform significantly worse than standard BO in the worst case, while achieving significantly faster convergence when preferences align with the objective. Empirically, LGBO achieves consistent improvements across diverse dry benchmarks in physics, chemistry, biology, and materials science.  Most notably, in a new wet-lab optimization of Fe–Cr battery electrolytes, LGBO reaches \\textbf{90\\% of the best observed value within 6 iterations}, whereas standard BO and existing LLM-augmented baselines require more than 10 iterations. Together, the results suggest that LGBO offers a promising direction for integrating LLMs into scientific optimization workflows.", "tldr": "", "keywords": ["Bayesian Optimization; Large Language Models; AI for Science; Scientific Discovery"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9255352250f1d57706901b91dc7f04802da86fc4.pdf", "supplementary_material": "/attachment/c0d541a1572b98e4d946737b53d8700dd3e6709c.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces LLM-Guided Bayesian Optimization (LGBO), a novel framework for integrating LLMs into the BO loop. The core contribution is a \"region-lifted preference\" mechanism, where the LLM provides continuous guidance by suggesting promising regions in the search space. This guidance is incorporated as a stable mean shift in the GP surrogate model, leaving the covariance structure intact. The authors provide theoretical regret bounds demonstrating that the method is safe in the worst case (i.e., with misaligned guidance) and can significantly accelerate convergence when the LLM's guidance is accurate. The framework is evaluated on several \"dry\" scientific optimization benchmarks and a \"wet-lab\" experiment, showing improved performance over standard GPBO and a prior LLM-augmented method, LLAMBO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The core idea of using an exponential lift on a discretized region, and showing its equivalence to a GP mean shift, is novel and technically sound\n- Theorem 1 is useful, as it formally establishes that the framework is robust to poor/misleading LLM guidance (worst-case) and can provably accelerate convergence when guidance is informative\n- The wet experiment on Fe-Cr battery electrolyte optimization is a great example to demonstrate the method's applicability to real scientific discovery problems beyond simulated benchmarks"}, "weaknesses": {"value": "- The paper motivates the work by citing the poor scalability of BO in high-dimensional settings. However, the experiments are conducted in search spaces of relatively low dimensionality (i.e., all experiments are conducted in low-dimensional settings with $d \\leq 7$). Hence, the effectiveness of the proposed method in a truly high-dimensional problem is not sufficiently demonstrated\n- The choice of baselines is far from adequate. The authors cite LLINBO and ReasoningBO as more systematic \"LLM-in-the-loop\" frameworks, yet fail to include them in the comparison. Comparisons with other related LLM-driven BO approaches, such as BOPRO [1] and CAKE [2], are also missing\n- To the best of my understanding, the LLM's guidance is limited to a single point or a hyper-rectangular region. I believe this format may be too simplistic for real-world problems where promising regions could be complex (e.g., non-convex, disjoint, or like a manifold). Hence, it is not clear if this method can be applied to problems with more complex geometries\n\n[1] D. Agarwal et al., \"Searching for optimal solutions with LLMs via Bayesian optimization,\" ICLR, 2025.\n\n[2] R. C. Suwandi et al., \"Adaptive kernel design for Bayesian optimization is a piece of CAKE with LLMs,\" arXiv preprint arXiv:2509.17998, 2025."}, "questions": {"value": "- How does LGBO handle optimization landscapes where the promising regions are have complex geometries that cannot be well-approximated by this format? Does this structural constraint limit the framework's applicability?\n- How do we translate the LLM's confidence score into the guidance strength $\\lambda$ and the region's properties (e.g., radius in point mode)? Since the parameter $\\lambda$ is critical to the regret bounds and practical performance, the authors should provide more discussion on its selection, tuning, or sensitivity\n- Based on my experience, a strong mean shift in an incorrect region without a corresponding increase in uncertainty might overly encourage the acquisition function to exploit that area. Since the proposed mechanism only shifts the surrogate's mean while leaving the covariance unchanged, could this lead to premature convergence if the LLM is confidently wrong?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Sm6Z1WAyN9", "forum": "LktUOZayG9", "replyto": "LktUOZayG9", "signatures": ["ICLR.cc/2026/Conference/Submission4356/Reviewer_oNUD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4356/Reviewer_oNUD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760692001056, "cdate": 1760692001056, "tmdate": 1762917313855, "mdate": 1762917313855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LLM-Guided Bayesian Optimization (LGBO), a new framework that continuously integrates LLM preferences into the BO process. Unlike prior work that uses LLMs only for warm-start initialization or candidate proposal, LGBO employs a region-lifted preference mechanism that adjusts the GP surrogate mean based on LLM-specified regions of interest. The authors show that this modification maintains theoretical rigor, preserving covariance structure and provide regret bounds demonstrating that LGBO performs no worse than standard BO in the worst case and converges faster when LLM preferences align with the true objective.\n\nEmpirically, LGBO is evaluated on four dry scientific benchmarks (LNP3, Cross-barrel, Concrete, HPLC) and a new wet-lab experiment on Fe–Cr battery electrolytes. Across all tasks, LGBO outperforms both standard GP-based BO and LLMABO, showing faster convergence, higher final performance, and lower variance. Ablation studies further confirm that performance gains arise from the continous preference integration rather than initialization or random region lifting."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Novel mechanism for incorporating LLM guidance into BO: the region-lifted preference is mathematically principled and computationally tractable. Making UCB-type bound natural to derive. \n- This paper is well-motivated, with a wide range of real-world scientific discovery problems tested in the experiments. \n- The methodology in this paper provides new insights about how **continuously** incorporating LLM preference could stabilize BO convergence compared to LLMABO, which integrates LLM preference indirectly, with robust ablation study.\n- The framework is general and modular, compatible with standard GP surrogates and acquisition functions."}, "weaknesses": {"value": "**The writing can be improved a lot** \n- The definitions and assumptions are not stated in the main paper. The definitions of $||\\cdot||$ in Line 293 and $R_T$ in Line 302 are missing, and the assumption for GP regression in the RKHS setting is not clearly stated in the main paper.\n    \n- The statement _“running GP-UCB on residual labels”_ in Line 295 is vague. Moreover, how it is equivalent to executing the proposed LGBO algorithm may be unclear to readers at first glance.\n    \n- The definition of _alignment_ being equivalent to $c > 0$ in Theorem 1 is not clearly stated and explained. Its definition appears only in the proof of Theorem 1 in Appendix A.\n    \n- In addition, there is a major typo in Theorem 1. According to the proof,  \n    $c = \\frac{\\langle f - \\tau, g \\rangle}{||f - \\tau||\\cdot||g||}$,  \n    while in the main text it is written as  \n    $c = \\frac{\\langle f - \\mu, g \\rangle}{||f - \\mu||\\cdot||g||}$.  \n    This is misleading, since $\\mu$ is typically the GP mean, whereas $\\tau$ is related to the lifting function.\n    \n**Theoretical result is not consistent with the proposed method.** Theorem 2 only describes the algorithm’s behavior when the mean adjustment is a fixed  \n    $g(\\cdot) = \\sum_{i=1} a_i  k(x_i, \\cdot)$ through out all iterations \n    whereas the proposed algorithm proposes a different $g$ through the LLM at each BO iteration. Thus, the theoretical results only partially explain the performance of LGBO.\n    \n**Lack of baseline comparison.** Only two baselines are included, and among them, only LLAMBO is an LLM-based BO method. Other LLM-integrated BO methods mentioned in the related work, such as ColaBO and LLINBO, are not included. Therefore, the statement about the instability and potential divergence of ColaBO in Line 128 is not well-supported by evidence."}, "questions": {"value": "1. Why $a_g$ are chosen to be greater than or equal 0? ( why don't we define a general LLM-based adjustment, not just lifting potential points, but also penalizing undesirable queries by allowing negative $a_g$s?)\n2. The novelty of the region-lifted preference is not emphasized until conclusion (by searching the key word \"novel\"). Could the authors re-confirm this is a novel idea?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BYPkgVGNcz", "forum": "LktUOZayG9", "replyto": "LktUOZayG9", "signatures": ["ICLR.cc/2026/Conference/Submission4356/Reviewer_M7Dg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4356/Reviewer_M7Dg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760802456784, "cdate": 1760802456784, "tmdate": 1762917313564, "mdate": 1762917313564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LLM-Guided Bayesian Optimization (LGBO), a framework that integrates priors from large language models (LLMs) into Bayesian Optimization (BO) for scientific experimentation. Unlike previous approaches that use LLMs only for the initialization of BO exploration or candidate generation, LGBO continuously incorporates LLM-derived region-lifted preferences into every iteration of the optimization loop. The authors show that this mechanism effectively shifts the Gaussian process (GP) surrogate’s mean without affecting its covariance, allowing semantic LLM guidance while retaining BO’s statistical guarantees. Theoretical results prove that LGBO is not worse than standard BO in the worst case and can achieve faster convergence when LLM guidance aligns with the true objective. Experiments on four “dry” scientific benchmarks (LNP3, Cross-barrel, Concrete, HPLC) and one “wet-lab” Fe–Cr battery optimization demonstrate consistent acceleration and improved stability over GPBO and LLAMBO baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Novel integration: Proposes a principled and theoretically grounded method to embed LLM preferences directly into the BO surrogate, moving beyond heuristic or warm-start use of LLMs.\n\n- Theoretical formulation: Provides formal regret bounds showing bounded degradation under misalignment.\n\n- Evaluation: Including both simulation and real-world experiments, showing convincing empirical improvements across diverse domains.\n\n- Reproducibility: Clearly describes prompt templates, datasets, and experimental protocols; theoretical and implementation details are provided.\n\n- Scientific relevance: Demonstrates the potential for LLMs to accelerate experimental optimization in time-limited domains.\n\n- Stable framework design: The region-lifted preference mechanism elegantly maintains GP structure and stability, avoiding instability issues typical of preference-based methods."}, "weaknesses": {"value": "- Evaluation baselines: Evaluation omits other preference-based or human-in-the-loop BO methods (e.g., ColaBO, Preferential BO with human experts), which could contextualize LGBO’s relative contribution.\n\n- Dependence on prompt engineering: Performance and robustness may depend strongly on prompt quality and LLM capabilities, but sensitivity analyses on prompt design are limited.\n\n- Scalability questions: Experiments focus on low- to medium-dimensional tasks (≤6 variables); it remains unclear how LGBO scales to high-dimensional or multi-objective settings.\n\n- Computational cost: Continuous LLM querying at each iteration may incur significant computational or latency overhead; this is not quantified or mentioned in the paper. \n\n- Interpretability of LLM Guidance: While the framework embeds preferences mathematically, the semantic validity or interpretability of the generated regions is not thoroughly analyzed.\n\n- Limited real-world task diversity: Only one wet-lab experiment is included; broader real-world validations would strengthen generality claims. It is also not clear why these particular tasks were selected for the evaluation."}, "questions": {"value": "- What is really the motivation for choosing preference optimisation for this setting? It is not immediately obvious to me. Better motivation as context for the approach would be helpful.\n\n- The region-lifted preference is formalized as an exponential mean shift. Could similar effects be achieved using other functional forms (e.g., linear or kernel-weighted lifts), and what motivated the specific exponential choice?\n\n- Since LGBO queries the LLM at every iteration, what is the computational or latency overhead compared to traditional BO? Could lightweight surrogates or cached reasoning traces mitigate this cost?\n\n- How sensitive are the results to the exact prompt design or LLM choice? While Appendix B provides structured prompts, have you quantified performance variance across alternative phrasing or reasoning styles?\n\n- Have you analyzed the semantic quality of the LLM-suggested regions—do they align with known scientific heuristics or physical laws?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "moAo0ARehj", "forum": "LktUOZayG9", "replyto": "LktUOZayG9", "signatures": ["ICLR.cc/2026/Conference/Submission4356/Reviewer_W7ee"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4356/Reviewer_W7ee"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935470560, "cdate": 1761935470560, "tmdate": 1762917313350, "mdate": 1762917313350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a way to integrate prior information in large language models into Bayesian optimization.\nAt its core, the method is based on a simple idea.\nIt prompts the language model to generate a promising region in the search space.\nThen, the proposed method updates the GP mean function so that the mean function has higher values in the promising region.\nExperiments on benchmark test functions across several areas show promising results against conventional BO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper works in the intersection of Bayesian optimization and large language models, and shows how to effective utilize the prior information stored in LLMs.\nThose prior information is often hard to encode into conventional Bayesian optimization by kernel designs.\n\n1. Using the lifting functional to encode the prior information from the language model makes sense to me.\nThough I have not checked the math, the part that the functional in the exponential form turns out to shift the mean function makes sense to me, as it is similar to exponential tilting for Gaussian distributions."}, "weaknesses": {"value": "1. The interpretation of Theorem 1 seems to be over claimed.\nThe correct interpretation should be weaker than what's claimed in the paper.\nThe theorem assumes the lift is given in advanced, and **does not change** during BO.\nHowever, the method proposed in this paper actually utilizes language models interactively in that the lifted region and or points get updated in each iteration of BO.\nThus, if the language model's prediction is bad, the regret bound could be unbounded.\n\n1. Many important technical details are missing.\n    - How to set the guidance strength parameters \\\\(a_g\\\\)?\n    Do you estimate them during GP model fitting with maximum likelihood, or do you set them to fixed numbers?\n    - How the discretization \\\\(x_g\\\\) is chosen in a lifted region?\n    - How does the confidence scores generated by LLMs affect the lifting functional?"}, "questions": {"value": "1. What's the exact definition of misaligned lift in Theorem 1? Isn't the misaligned case the same as \\\\(c = 0\\\\), i.e., small cosine similarity?\n\n1. Line 293: Is the norm in theorem 1 the RKHS norm?\nIf so, it would be better to make it explicit.\n\n1. Line 162: \"Intuitively, \\\\(p(\\cdot)\\\\) here denotes a probability distribution...\".\nShouldn't it be \\\\(\\rho\\\\)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ChkaFyd3QM", "forum": "LktUOZayG9", "replyto": "LktUOZayG9", "signatures": ["ICLR.cc/2026/Conference/Submission4356/Reviewer_935h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4356/Reviewer_935h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939003551, "cdate": 1761939003551, "tmdate": 1762917313060, "mdate": 1762917313060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}