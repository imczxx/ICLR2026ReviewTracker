{"id": "gePt0CLtHl", "number": 20360, "cdate": 1758305082639, "mdate": 1759896981869, "content": {"title": "SIDGEN - Structure-Informed Diffusion for Generative Modeling of Ligands for Proteins", "abstract": "Designing ligands that are both chemically valid and structurally compatible with protein binding pockets is a key bottleneck in computational drug discovery. Existing approaches either ignore the structural context or rely on expensive, memory-intensive encoding, which limits throughput and scalability. We present SIDgen (Structure Integrated Diffusion Generator), a protein-conditioned diffusion framework that integrates masked SMILES generation with lightweight folding-derived features for pocket awareness. To balance expressivity with efficiency, SIDGen supports two conditioning pathways: a streamlined mode that pools coarse structural signals from protein embeddings and a full mode that injects localized pairwise biases for stronger coupling. A coarse-stride folding mechanism with nearest-neighbor up-sampling alleviates the quadratic memory costs of pair tensors, enabling training on realistic sequence lengths. Learning stability is maintained through in-loop chemical validity checks and an invalidity penalty, while large-scale training efficiency is restored via selective compilation, dataloader tuning, and gradient accumulation. In automated benchmarks, SIDGen generates ligands with high validity, uniqueness, and novelty, while achieving strong enrichment in docking-based evaluations (EF, BEDROC) and competitive pose quality (RMSD, score-based ROC/PR). Robust receptor preparation and ligand-derived docking boxes further ensure reliable assessment across diverse protein families. These results demonstrate that SIDGen can deliver scalable, pocket-aware molecular design, providing a practical route to conditional generation for high-throughput drug discovery.", "tldr": "We present SIDgen (Structure Integrated Diffusion Generator), a protein-conditioned diffusion framework that integrates masked SMILES generation with lightweight folding-derived features for pocket awareness.", "keywords": ["Diffusion models", "Computational drug discovery", "Ligand design", "protein-conditioned diffusion framework"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/95e605d04e5473943b38eb76f40783f2fb501181.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a protein-conditioned diffusion model that generates ligand SMILES while explicitly leveraging binding-pocket information. However, the paper **shows clear signs of being rushed and unfinished**，numerous writing errors hinder readability."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. It tackles multiple tasks including de-novo molecule generation and virtual screening, demonstrating strong versatility of the proposed method."}, "weaknesses": {"value": "1. Although the method claims to improve efficiency, its reliance on pre-trained models such as ESM inevitably adds computational overhead; moreover, it offers no clear advantage over previous SBDD approaches.\n\n2. The manuscript is riddled with typographical errors (e.g. line 224 227), and many references leave out the conference or journal names entirely.\n\n3. The experimental section’s analysis is overly simplistic."}, "questions": {"value": "Refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oi8m4k2Q7p", "forum": "gePt0CLtHl", "replyto": "gePt0CLtHl", "signatures": ["ICLR.cc/2026/Conference/Submission20360/Reviewer_PcZ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20360/Reviewer_PcZ9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707878339, "cdate": 1761707878339, "tmdate": 1762933814301, "mdate": 1762933814301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SiDGen, a structure-informed diffusion model for protein-conditioned ligand generation that integrates masked SMILES diffusion with lightweight structural embeddings derived from protein folding features. To balance structural fidelity and computational scalability, SiDGen introduces a coarse-stride folding mechanism that reduces quadratic memory costs and supports dual conditioning pathways for efficiency versus expressivity. The model achieves high chemical validity and novelty (100%), competitive docking and affinity metrics across benchmarks (MOSES, DUD-E, CrossDocked2020, PDBBind), and significant memory savings, demonstrating a practical and scalable approach to pocket-aware molecular design for structure-based drug discovery"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "+ Proposes a practical diffusion framework balancing structure-awareness and computational scalability.\n+ Introduces the coarse-stride folding mechanism that effectively reduces O(L²) complexity."}, "weaknesses": {"value": "+ Innovation mainly lies in engineering integration rather than new theory.\n\n+ Missing ablations on stride size, conditioning mode, and validity penalty.\n\n+ No 3D structural evaluation despite being “structure-informed.”\n\n+ Limited discussion of recent geometric diffusion baselines."}, "questions": {"value": "1. How sensitive is SiDGen’s performance to the stride parameter s? Could a learnable stride or adaptive pooling improve structural fidelity?\n\n2. Does the invalidity penalty meaningfully alter the diffusion loss landscape, or merely act as a post-hoc filter?\n\n3. Can the coarse-stride structural representation be used to initialize a lightweight 3D decoder (e.g., predicting ligand coordinates)?\n\n4. What is the runtime and memory profile compared to fully 3D approaches (TargetDiff, DiffSBDD)?\n\n5. How generalizable is SiDGen to unseen protein families or homology-reduced sets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bfHbi2DLTY", "forum": "gePt0CLtHl", "replyto": "gePt0CLtHl", "signatures": ["ICLR.cc/2026/Conference/Submission20360/Reviewer_ojWf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20360/Reviewer_ojWf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879559177, "cdate": 1761879559177, "tmdate": 1762933813958, "mdate": 1762933813958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SiDGen, a protein-conditioned ligand generator that operates in discrete SMILES space using a masked diffusion language model. Its main idea, coarse-stride folding, down-samples protein single/pair features, applies AlphaFold-style operators on a coarse grid, then up-samples, yielding large reductions in structural-conditioning cost while retaining pocket cues. The model provides streamlined and full conditioning modes and trains with a timestep-weighted token loss plus simple validity regularization. Empirically, the approach delivers competitive screening and affinity results while emphasizing practical scalability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-specified discrete diffusion.  The author proposes masked SMILES diffusion with timestep-weighted CE, curriculum, and invalidity penalties is clearly described and targeted at generation stability.\n2. Scalable structural conditioning. The author uses coarse-stride folding reduces quadratic memory to  O((L/s)^2)  and claims large savings while preserving salient pocket information."}, "weaknesses": {"value": "1. Missing compute–quality Pareto evidence. The paper proposes streamlined vs full conditioning but does not report systematic VRAM/GPU-hours vs EF@1%/ROC-AUC curves across datasets and stride s. This undermines the central claim of deployable scalability.\n2. No pocket-level fidelity quantification for coarse stride. While complexity drops from O(L^2) to  O((L/s)^2), there is no analysis of local interaction loss (contact maps, hydrogen bonds, pocket geometry) as sincreases, nor its effect on VS metrics. It leaves the core approximation weakly supported.\n3. Under-comparison to strong 3D/pose-aware baselines. The method outputs SMILES and relies on docking, but lacks equal-compute, end-to-end comparisons against recent direct 3D/pose generators; the accuracy–latency position is unclear.\n4. Unclear source of training stability. Timestep weighting, curriculum, and invalid-SMILES penalty are introduced together without removal ablations (invalid rate, convergence, EF/AUC). Gains may stem from engineering stabilizers rather than the modeling contribution.\n5. Potential data leakage and weak cross-family generalization reporting. PFAM/clusters are mentioned, but leak-resistant splits (family/similarity-aware) and PFAM-bucketed results are not detailed; VS scores may be inflated by family overlap.\n6. Underspecified conditioning length/alignment. The upsampling/broadcast from protein length L_pto ligand length L, pocket residue selection/weighting, and cross-attention masking/boundaries are not clearly described in the main text, hurting reproducibility."}, "questions": {"value": "1. Pocket-fidelity diagnostics. How does downsampling affect contact maps, hydrogen-bond counts, and pocket RMSD as sincreases? Please correlate these errors with screening metrics to justify the coarse-stride approximation.\n2. 3D/pose-aware parity. Under equal compute and docking budgets, how does the “SMILES → docking” pipeline compare to recent direct 3D/pose generation methods in hit-rate and wall-clock time?\n3. Conditioning alignment details. Clarify how L_pis upsampled/aligned to L, how pocket residues are selected/weighted, and how cross-attention boundaries/masks are implemented; pseudocode or a diagram would help.\n4. What is the theoretical/empirical motivation for the timestep/substitution weight? Is it related to gradient scale matching or noise distribution? Please compare against plain CE and alternative weights."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y7v6uX4Niv", "forum": "gePt0CLtHl", "replyto": "gePt0CLtHl", "signatures": ["ICLR.cc/2026/Conference/Submission20360/Reviewer_Eiy9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20360/Reviewer_Eiy9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990824448, "cdate": 1761990824448, "tmdate": 1762933813199, "mdate": 1762933813199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a structure-informed diffusion model for protein-conditioned ligand generation. The main motivation is to reduce the computational cost of modeling protein in structure-informed drug design. The authors use ESM-2 to extract protein embeddings and introduce a coarse-stride folding mechanism to downsample protein features, aiming to achieve efficiency while maintaining structural awareness. The ligand is generated through a masked diffusion process on the SMILES representation. Experiments on standard molecular benchmarks such as MOSES and CrossDocked2020 are reported."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The goal of reducing computational cost for protein-conditioned ligand generation is relevant and meaningful for scalable drug discovery.\n- The motivation for addressing the memory bottleneck in protein feature encoding is clear and aligns with current trends in generative molecular modeling."}, "weaknesses": {"value": "- Poor clarity and structure. The presentation is weak. Many sections are described in a superficial way without formal definitions or equations where needed.\n- Lacks critical methodological details. The paper omits key implementation and architectural details that are essential for assessing its validity. Several descriptions in the methodology and experiment sections are overly simplified and ambiguous. Without sufficient detail, it is impossible to assess the soundness or reproducibility of the proposed approach.\n- Insufficient experimental reporting. The experimental setup lacks background context (e.g., preprocessing, data splits, baseline configurations). Key ablation studies and inference efficiency analyses are missing."}, "questions": {"value": "1. What is the rationale for the downsampling and upsampling in the coarse-stride folding? Since ESM2 is not pretrained with such operations, how is this compatible with the embeddings? Is the folding module pretrained, or trained jointly with diffusion?\n2. Section 3.5 introduces substitution parameterization, curriculum learning, and validity regularization, but lacks concrete implementation details. Have the authors conducted ablation studies to show their impact?\n3. The paper emphasizes computational efficiency but does not report inference time or memory usage compared to other methods.\n4. The experiments on virtual screening and binding affinity prediction lack explanation of how SiDGen is applied to these tasks. The setup and evaluation protocol should be clarified.\n5. For CrossDocked2020 benchmark, key recent baselines such as MolCRAFT[1] is missing. \n6. Highlighting results in bold without being state-of-the-art is misleading. I suggest the authors to highlight the best performing model with bold.\n7. Figure 2 should be redrawn for clarity, and the term held-out should be properly defined. \n8. The paper contains grammatical errors that require revision.\n\n[1] Qu, Y., Qiu, K., Song, Y., Gong, J., Han, J., Zheng, M., ... & Ma, W. Y. MolCRAFT: Structure-Based Drug Design in Continuous Parameter Space. In Forty-first International Conference on Machine Learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FsW3y2D1GT", "forum": "gePt0CLtHl", "replyto": "gePt0CLtHl", "signatures": ["ICLR.cc/2026/Conference/Submission20360/Reviewer_53TB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20360/Reviewer_53TB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20360/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762068749106, "cdate": 1762068749106, "tmdate": 1762933812667, "mdate": 1762933812667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}