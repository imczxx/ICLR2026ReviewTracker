{"id": "jp4dzBilqH", "number": 13129, "cdate": 1758213871264, "mdate": 1759897462350, "content": {"title": "X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests", "abstract": "Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba²-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.", "tldr": "We explore a fully synthetic approach by training Code LLMs on entirely generated tasks, solutions, and test cases to develop the X-Coder series, which advances code reasoning while reducing reliance on real-world coding data.", "keywords": ["Code Generation", "Competitive Programming", "Large Reasoning Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b85a39a34b94b129f0e7837b227850047654b1ce.pdf", "supplementary_material": "/attachment/a8ded4296038ded41f57eaedfa743d43d33a038a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes SynthSmith, a feature-based pipeline to fully synthesize competitive programming dataset including problem statements, solutions, and unit tests. Starting from Qwen2.5-Coder-7B-Instruct and Qwen3-8B-Base, the authors use the synthetic SFT and RL dataset to train the X-Coder series. The manuscript is centering around the synthetic dataset pipeline, covering task (statement) generation, solution generation, and unit tests generation, in which the authors use \"dual-verification\", a variant based on majority voting with custom weights to select ground truth solutions and unit tests set. The authors follow a standard SFT-then-RL training to show the effectiveness of the synthetic dataset. The paper further analyzes scaling (unique tasks > multiple solutions per task), SFT-then-RL dynamics (“good-gets-better”), and ablations (verification, long- vs short-CoT, task style)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The work is well-executed (the various prompt pipelines), and the results look strong. The main novelty of the paper is the synthetic code pipelines, where the authors detail the prompt used in the Appendix to demonstrate how the features (algorithm/data structure) are extracted, grow, and generate a problem statement. Additional experiments are conducted to show the scaling effect of the dataset. Some extra ablations are also conducted, such as the tool-based generation vs prompt-based generation for unit tests. I also appreciate the authors' commitment to reproducibility (while I leave some doubts on it with details later)."}, "weaknesses": {"value": "Since the manuscript does not claim algorithmic novelty (which is not inherently an issue) and the major contribution is the synthetic data pipeline and the artifact. My concerns for not recommending an acceptance lie in that I'm not fully convinced of the resulting dataset's superiority and the clarity of the data construction process.\n\n1. **Details of the pipeline stats**: Could the authors give details about the stats of the dataset in each step (not just the final token stats for the final dataset in Table 7)? It'd be helpful to assess the bottleneck of the designed pipeline. For example, the stats of the features extracted, how many additional datapoints are generated by the step \"Evolve and Merge\", how many problem statements are generated (do the authors employ filtering steps? If so, the % of the filtered entries?), etc.\n\n2. My biggest concern is how good the dataset is compared to existing code datasets: The paper only ablates on the EpiCoder dataset for the reason that they share the same synthetic nature. However, it remains unclear how the collected synthetic dataset compares to existing non-synthetic datasets such as OpenCodeReasoning. It would be good to know how large the gap if it falls behind, or if the collected dataset is even better.\n\nDespite the authors comparing with AceReasoner 1.1 (which trains on OpenCodeReasoning) in Table 1, it uses a different backbone from what the manuscript uses, which hinders the comparison of the dataset. I will highly appreciate it if the authors could conduct SFT & RL based on the same backbone using OpenCodeReasoning, which should strengthen the claim of the paper.\n\n3. The experiment design of the scaling effect of SFT resembles largely to AceReason-Nemotron 1.1 https://arxiv.org/abs/2506.13284, in which they also separate SFT set from v1 to v5 (growing the number of prompts) and v6 - v7 (growing the solutions per prompt) and building on top they fit a coefficient for the 2 axes for eval performance. Maybe I have missed it, but I think a reference to this is missing.\n\n4. The first step of dual-verification sets provisional ground truth by majority vote of candidate solutions. Is there any way to get a sense of how many false positives are in the whole dataset?\n\n5. **Reproducibility**: The authors promise to release code/data, and provide an anonymous repo link, but end-to-end reproduction of the synthetic data itself hinges on full prompt templates and sampling settings. In particular, few details are laid out for the \"difficulty-weighting function\"."}, "questions": {"value": "- Could the authors show how the difficulty weights are computed? L190 says \"The weight w_i is determined by a set of heuristics based on input characteristics, such as character or token count, structural complexity, or semantic novelty, which serve as proxies for difficulty.\" What specific heuristics are used? In particular, I do not see any further mention about the \"structural complexity\" and “semantic novelty\", how does the authors assess these quantitatively?\n\n- I briefly checked the codebase: some prompts do not seem to be there, such as the prompt mentioned in C.2. \n\n- For the error type analysis in Table 4, it seems that the runnable failing code is all put under Assertion Error. Is it possible to break it down into more fine-grained categories, such as Wrong Answer, Time Limit Exceeded, Memory Limit Exceeded, etc?\n\n- Is there any plan for releasing the dataset? It's okay if it's not decided yet but would be good to know whether the community should expect an open-source dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZO9bzT0NeC", "forum": "jp4dzBilqH", "replyto": "jp4dzBilqH", "signatures": ["ICLR.cc/2026/Conference/Submission13129/Reviewer_vHC5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13129/Reviewer_vHC5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13129/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608974525, "cdate": 1761608974525, "tmdate": 1762923851181, "mdate": 1762923851181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SynthSmith, a synthetic coding-data generation pipeline designed to produce competition-level programming problems (along with test-cases and golden solutions) for training large code models. The key components of the pipeline are:\n- Feature-tree generation and evolution: Starting from seed programming concepts/features, the pipeline generates and evolves feature-trees. An LLM is then tasked with selecting a suitable sub-tree (via feature‐role annotation → sub-tree selection → integration strategy) and formulating it into a natural-language task statement.\n- Candidate solution + test-case generation: For each generated problem, candidate solutions and accompanying test-cases are created.\n- Golden solution identification: Among the candidates, a “golden” solution is selected via a weighted scoring procedure across test-cases, forming the ground-truth entry for model training.\n\nThe resulting dataset is used to train models (the “X-Coder” family) by fine-tuning and then reinforcement learning on base models (e.g., Qwen2.5-Coder-Instruct and Qwen3-8B-Base)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The authors describe the stages (feature-tree evolution, problem formulation, solution/test synthesis, golden-selection) in a clear and detailed manner.\n- A comprehensive set of experiments was conducted on model training and performance.\n- Insightful experiment design around problem style: For example, the authors examine how the style in which a problem is specified (e.g., “competitive” vs. “educational” style) affects model learning and how picking problems whose solutions require longer reasoning improved model performance relative to short-reasoning or simple difficulty-based selection."}, "weaknesses": {"value": "**Overall**\n- The idea of using concepts from seed problems and evolving them into a larger bank of problems is not entirely novel in the domain of synthetic code-data generation. The authors do not sufficiently situate their work in relation to recent pipelines such as SelfCodeAlign (Wei et al., 2024a) or CodeEvo (Sun, Qiushi et al., 2025) etc. A clear performance comparison (or final model accuracy / generation-cost comparison) with those pipelines is missing.\n\n**Programming-Problem Generation (feature-tree → task)**\n- Verification of sub-tree → task mapping: It is unclear how the authors verify that the selected sub-tree and integration strategy reliably produce reasonable programming problems (solvable by humans/LLMs, non-trivial, properly scoped). Do they evaluate the generated problem set for solvability, human-readability, or difficulty calibration? What fraction of generated problems are too ambiguous or unsolvable?\n- Can an expanded feature tree give multiple integration strategies corresponding to different sub-trees? If so, are you generating multiple problems from a single feature tree?\n- An ablation or experiment that simply prompts an LLM to go from feature-tree → task in one shot (without the explicit sub-tree selection/integration step) to justify the pipeline’s modular structure.\n\n**Golden Solution Identification**\n- Missing clarity on how the weights for each test case are computed (i.e., what criteria drive higher weights and by how much?)\n- Your weighted-score based selection of $A'\\_{golden}$ in Step-3 of Algorithm 1 does not affect Step-4. In Step-4, you are always returning the solution which has the max score on held-out validation split regardless of whether it matches the solution $A'\\_{golden}$ from weighted-selection step or not (i.e. either $A\\_j^+ = A\\_j^* = A'\\_{golden}$ or not, you are still returning $A\\_j^+$). If my understanding is correct, I don’t see the point of doing weighted selection (seems superfluous).\n- Further, what’s the point of assigning weights to test cases? Your selection strategy seems to implicitly allow solutions that pass some but not all test cases. Since the provisional outputs are generated using a candidate set of solutions, it ensures that there exists at least one candidate solution that passes a given test case. Beyond this point, if there exists at least one solution that passes all test-cases, we have a golden solution. However, if you are assuming that there exist programming problems for which no one candidate solution can pass all the test cases, and weights are being used to identify the solution which passes, say “more important ones”, either you are saying test cases being used are not reliable, or you can treat partially-correct solutions as golden. This may reduce the ground-truth quality. Unless the pipeline ensures fully correct solutions (pass all test-cases) this could degrade training fidelity.\n\n**Experiments**\n\n- The paper lacks an experiment that isolates the impact of the synthetic data generation method itself. For example: take the same base model (e.g., Qwen3-8B), fix training algorithm (SFT + RL), and compare performance when training on (a) only real data, (b) synthetic data from SynthSmith, (c) synthetic data from other pipelines. This would demonstrate whether the pipeline’s specific design yields a measurable benefit over generic synthetic methods.\n- How do the authors define if a generated task is “unique”? Is uniqueness driven by selecting different sub-trees from the same feature-tree, or only by selecting entirely distinct feature-trees?\n- Generality across model families: The experiments are limited to the X-Coder models derived from Qwen2.5/Qwen3.8B. It would strengthen the claim if the authors showed consistent improvements across other model families (e.g., Phi, DeepSeek, CodeLlama).\n- Why do you think you are getting performance variations when the problem is being defined in different styles? Why is Leet-Code style consistently outperformed by AtCoder and CodeForces style? Can you share some insights?\n- Failure-case analysis: The model trained on SynthSmith data reportedly has an increased tendency to generate no code blocks (i.e., fail to generate code) — but the authors do not provide an analysis of why this happens.\n- Benchmark breadth: The evaluation focuses on one benchmark (LiveCodeBench). It lacks evaluation on other established code-benchmarks such as HumanEval+, MBPP+, EvoEval (Xia et al.), ClassEval (Du, Xueying et al.), or DS‑1000 (Lai, Yuhang et al.). This limits the external validity of the claimed improvements.\n\n**References:**\n\nWei, Y., Cassano, F., Liu, J., Ding, Y., Jain, N., Mueller, Z., de Vries, H., von Werra, L., Guha, A., & Zhang, L. (2024a). SelfCodeAlign: Self-Alignment for Code Generation. Advances in Neural Information Processing Systems, 37.\n\nSun, Qiushi, et al. \"CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback.\" arXiv preprint arXiv:2507.22080 (2025).\n\nXia, Chunqiu Steven, Yinlin Deng, and Lingming Zhang. \"Top leaderboard ranking= top coding proficiency, always? evoeval: Evolving coding benchmarks via llm.\" arXiv preprint arXiv:2403.19114 (2024).\n\nDu, Xueying, et al. \"Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation.\" arXiv preprint arXiv:2308.01861 (2023).\n\nLai, Yuhang, et al. \"DS-1000: A natural and reliable benchmark for data science code generation.\" International Conference on Machine Learning. PMLR, 2023."}, "questions": {"value": "See the Weaknesses Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "C1IJeCJkqy", "forum": "jp4dzBilqH", "replyto": "jp4dzBilqH", "signatures": ["ICLR.cc/2026/Conference/Submission13129/Reviewer_pnxs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13129/Reviewer_pnxs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13129/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867274274, "cdate": 1761867274274, "tmdate": 1762923850884, "mdate": 1762923850884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SynthSmith, a fully synthetic data generation pipeline for training code reasoning models on competitive programming tasks. The pipeline generates novel tasks through feature-based synthesis, creates comprehensive test cases, produces multiple candidate solutions, and employs a dual-verification strategy to ensure quality. Based on this synthetic data, the authors train X-Coder models using both supervised fine-tuning (SFT) and reinforcement learning (RL), achieving strong performance on LiveCodeBench v5 (62.9 avg@8) and v6 (55.8 avg@8) despite having only 7B parameters. The work demonstrates that fully synthetic data can effectively train code reasoning models without relying on real-world competitive programming problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a complete pipeline covering all aspects of synthetic data generation, from tasks to solutions to test cases, with thoughtful verification strategies.\n- X-Coder achieves impressive performance, outperforming larger 14B models despite having only 7B parameters, demonstrating the effectiveness of the synthetic approach.\n- Extensive ablations examining verification, CoT length, task styles, data selection, and test generation methods. Detailed analysis of scaling laws showing which dimensions scale more favorably. Investigation of RL behavior including \"good-gets-better\" principle and resilience to noisy supervision\n- Concrete insights about what makes synthetic data effective (e.g., long-CoT > short-CoT, task diversity > solution diversity)\n- Section 5 provides valuable discussion of error distributions, reasoning length vs. pass rate, and undesirable patterns like reward hacking."}, "weaknesses": {"value": "- Unclear how many tasks have incorrect \"golden\" solutions despite verification. The dual-verification strategy's actual error rate is not quantified\n- Strong reliance on EpiCoder's feature-based framework. Significant performance gains may come from using stronger teacher models (GPT-o3-mini, Deepseek-R1-0528) rather than methodological improvements\n- From Table 4, one can see that SFT or RL is increasing the number of no-code solutions from the base model, any reason why that is happening ?"}, "questions": {"value": "- Refer to weaknesses\n- You mention tool-based test generation achieves 87.9% pass rate vs. 77.4% for prompting-based, but this still means 12% of test cases may be incorrect. How does test case error rate affect training?\n- How do you ensure generated tasks are genuinely novel and not similar to existing competitive programming problems? Have you analyzed the diversity of generated tasks quantitatively?\n- Why weren't results from all the models included for LCB v6?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TJpf7ZoN9L", "forum": "jp4dzBilqH", "replyto": "jp4dzBilqH", "signatures": ["ICLR.cc/2026/Conference/Submission13129/Reviewer_Kesq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13129/Reviewer_Kesq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13129/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762243373860, "cdate": 1762243373860, "tmdate": 1762923850465, "mdate": 1762923850465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}