{"id": "YcoS3uq5kt", "number": 8549, "cdate": 1758090662134, "mdate": 1763636531050, "content": {"title": "GuardAlign: Robust Safety Alignment in Multimodal Large Language Models", "abstract": "Multimodal large language models (MLLMs) have achieved remarkable progress in vision–language reasoning tasks, yet ensuring their safety remains a critical challenge. Recent input-side defenses detect unsafe images with CLIP and prepend safety prefixes to prompts, but they still suffer from inaccurate detection in complex scenes and unstable safety signals during decoding. To address these issues, we propose **GuardAlign**, a training-free defense framework that integrates two strategies. First, OT-enhanced safety detection leverages optimal transport to measure distribution distances between image patches and unsafe semantics, enabling accurate identification of malicious regions without additional computational cost. Second, cross-modal attentive calibration strengthens the influence of safety prefixes by adaptively reallocating attention across layers, ensuring that safety signals remain consistently activated throughout generation. Extensive evaluations on six representative MLLMs demonstrate that GuardAlign reduces unsafe response rates by up to 39% on SPA-VL, while preserving utility, achieving an improvement on VQAv2 from 78.51% to 79.21%.", "tldr": "", "keywords": ["MLLM", "VLMs", "Safety", "Alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a88ae45f7c5e4a5b90960d9e4d245b5e45f6e5cf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on safety alignment in multi-modal large language models (mllms). Recent studies reveal that input-side defenses detect unsafe images with CLIP and prepend safety prefixes to prompts, but most MLLMs suffer from inaccurate detection in complex scenes and unstable safety signals during decoding. To this end, the authors propose GuardAlign, which is a training-free defense framework that first leverages optimal transport to measure the distribution distances between image patches and unsafe semantics, then cross-modal attentive calibration ensures the safety signals remain consistently activated throughout generation. Extensive evaluations on six representative MLLMs demonstrate that GuardAlign reduces unsafe response rates by up to 39% on SPA-VL."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper starts from an interesting motivation. In order to ensure the MLLMs are not guided by the dangerous information carried in the images, the authors come up with a method to mask the dangerous regions out of the images.\n\n2.\tThe proposed method GuardAlign makes sense to me. To achieve the above goal, the authors propose OT-enhanced safety detection module, which first divides the image into patches, then measures the similarity between the patches and potential harmful content."}, "weaknesses": {"value": "My main concern is about the negative effect that could be brought by the GuardAlign. Even though Table 6 demonstrates that their method preserves utility and yields consistent gains, the reasons behind are not revealed and discussed enough. Intuitively, masking the image leads to missing information about the image, but the performance is not affected. Is it because the masking region is small or something? If so, relevant analysis should be performed."}, "questions": {"value": "Please refer to Weaknesses No.1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HvVfNNtgNW", "forum": "YcoS3uq5kt", "replyto": "YcoS3uq5kt", "signatures": ["ICLR.cc/2026/Conference/Submission8549/Reviewer_Zbe8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8549/Reviewer_Zbe8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807242140, "cdate": 1761807242140, "tmdate": 1762920403190, "mdate": 1762920403190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GuardAlign, a training-free defense framework for enhancing the safety of MLLMs. The method integrates two key strategies: OT-enhanced safety detection and cross-modal attentive calibration. Extensive evaluations on six MLLMs demonstrate that GuardAlign significantly reduces unsafe response rates while preserving the utility of MLLMs. The approach is efficient, adding minimal inference overhead compared to normal inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Training-free efficiency: GuardAlign operates entirely at inference time without requiring additional data or fine-tuning, making it highly practical and resource-efficient. \n\n- Comprehensive experimental validation: The paper provides thorough evaluations across multiple safety benchmarks and utility tasks, including detailed ablation studies and efficiency analyses. \n\n- Low inference overhead: Compared to existing inference-time defenses like ETA, GuardAlign achieves better safety with moderate computational cost, striking a balance between effectiveness and efficiency."}, "weaknesses": {"value": "- Utility improvement: While the paper reports that GuardAlign avoids performance degradation and even boosts utility (e.g., VQAv2 accuracy improves from 78.51% to 79.21%), the underlying mechanism is not sufficiently analyzed. It remains unclear why masking unsafe patches or calibrating attention would enhance general capabilities—this warrants further theoretical or empirical justification.  \n\n- Limited model scale evaluation: Experiments are confined to MLLMs up to 13B parameters (e.g., LLaVA-1.5-13B). Given the trend toward larger models (e.g., 70B+), validating GuardAlign on more scalable architectures would strengthen its generalizability and impact.  \n\n- Practicality concerns: The cross-modal attention calibration requires direct manipulation of attention maps within the MLLM backbone, which may involve accessing and altering internal model structures. This could limit applicability in black-box or proprietary systems where such modifications are restricted, reducing the method's versatility in real-world scenarios."}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "21yP0ozd2y", "forum": "YcoS3uq5kt", "replyto": "YcoS3uq5kt", "signatures": ["ICLR.cc/2026/Conference/Submission8549/Reviewer_vS6s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8549/Reviewer_vS6s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004466524, "cdate": 1762004466524, "tmdate": 1762920402652, "mdate": 1762920402652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GuardAlign, a training-free framework to enhance the safety of multimodal large language models (MLLMs). It integrates two strategies: (1) OT-enhanced safety detection using optimal transport to accurately identify unsafe image regions, and (2) cross-modal attention calibration that reinforces safety prefix signals during text generation. Experiments on six major MLLMs demonstrate that GuardAlign significantly reduces unsafe response rates while maintaining or slightly improving task performance, achieving up to 39% reduction in unsafe outputs without retraining or additional data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is original in combining optimal transport-based detection with attention calibration for inference-time safety alignment. \n\nThe technical quality is solid, with rigorous theoretical analysis and comprehensive evaluations across models and datasets. \n\nClarity is high. both intuition and formulation are clearly articulated, and experimental design is systematic."}, "weaknesses": {"value": "The method, while efficient, introduces several hyperparameters (e.g., τ, γ) that are not fully analyzed for stability or generalizability. \n\nEvaluation is limited to vision–language reasoning; applicability to other modalities remains untested. \n\nThe detection component depends on CLIP backbones, which could inherit existing biases."}, "questions": {"value": "How sensitive is GuardAlign's performance to its threshold and amplification parameters across unseen datasets? \n\nCould the OT-based detection misclassify benign but semantically rich regions (false positives)? \n\nWould combining GuardAlign with post-hoc fine-tuning methods yield additive benefits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DkgV3wfrkn", "forum": "YcoS3uq5kt", "replyto": "YcoS3uq5kt", "signatures": ["ICLR.cc/2026/Conference/Submission8549/Reviewer_xpBF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8549/Reviewer_xpBF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059647969, "cdate": 1762059647969, "tmdate": 1762920402244, "mdate": 1762920402244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GuardAlign, a training-free, inference-time defense for multimodal LLMs (MLLMs) that combines two complementary modules:\n\nOT-enhanced safety detection — use patch-level Optimal Transport (OT) between image patch features and a set of predefined “unsafe” prompt embeddings to identify and mask visual regions that align with harmful semantics;\n\nCross-modal attention calibration — amplify attention toward a prepended safety prefix across selected middle fusion layers so that the safety signal remains activated during autoregressive decoding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ GuardAlign operates at inference time and requires no fine-tuning, which is attractive for rapid deployment on large deployed MLLMs.\n+ The coupling of fine-grained OT-based patch scoring with attention-level calibration for safety prefixes is an intuitive and novel pairing: detect & sanitize visual evidence, then ensure the LLM heeds the safety prefix."}, "weaknesses": {"value": "- The method is evaluated on many benchmarks but primarily in a black-box or benchmarked adversary setting. An adaptive attacker that crafts images to both avoid OT detection and trigger unsafe generations (e.g., by distributing harmful signals over many patches or embedding signals in texture) is not evaluated. GuardAlign’s resilience to adaptive/strong adversaries is unclear."}, "questions": {"value": "refer to weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ivEt1mbi7X", "forum": "YcoS3uq5kt", "replyto": "YcoS3uq5kt", "signatures": ["ICLR.cc/2026/Conference/Submission8549/Reviewer_uARe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8549/Reviewer_uARe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117154423, "cdate": 1762117154423, "tmdate": 1762920401843, "mdate": 1762920401843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}