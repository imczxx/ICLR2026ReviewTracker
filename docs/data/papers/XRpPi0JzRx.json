{"id": "XRpPi0JzRx", "number": 19592, "cdate": 1758297513885, "mdate": 1759897031311, "content": {"title": "Strategic AI Training Sabotage: State Attacks on Advanced Systems' Development", "abstract": "Much attention has been given to the possibility that states will attempt to steal the model weights of advanced AI systems. We argue that in most situations, it is more likely that a state will attempt to sabotage the training of the models underpinning these systems. We present a threat modelling framework for sabotage of AI training, including both the necessary technical background and a taxonomy of strategic considerations and attack vectors. We then use this to examine different attacks and assess both their technical plausibility and the mitigations required to defend against them.", "tldr": "We examine when states might sabotage AI training runs, create a threat modelling framework and assess plausibility and mitigations of different attacks.", "keywords": ["sabotage", "threat modelling", "AI security", "cybersecurity"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b7923d6b2876d324875961105a34ec2dbbbabfd6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper “Strategic AI Sabotage: State Attacks on Advanced Systems’ Development” examines how state actors could target the training pipelines of advanced AI systems as a form of strategic sabotage. It proposes a threat modeling framework outlining sabotage goals, attack vectors, and motivations, analyzing methods such as data and model poisoning that could degrade performance or delay rival progress. Drawing parallels to Stuxnet and Cold War covert operations, the paper situates AI sabotage within modern grey-zone warfare and calls for stronger defensive measures, cooperation, and safeguards to protect AI development from state-level interference."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is highly original, introducing the idea of state-level AI training sabotage as a new and important threat. It effectively blends technical and geopolitical analysis, supported by clear threat modeling, a well-defined taxonomy, and credible historical analogies. The writing is clear and well-structured, with visuals that enhance understanding. Overall, it makes a significant and timely contribution by spotlighting an overlooked risk at the intersection of AI safety, national security, and global governance."}, "weaknesses": {"value": "The paper is largely conceptual, lacking empirical validation or quantitative modeling to support its threat scenarios. Some proposed sabotage methods are technically plausible but insufficiently detailed, leaving questions about real-world feasibility. The defensive measures are high-level and could be strengthened with concrete detection or mitigation strategies. Adding empirical case studies or simulations would make the work more practical and evidence-based."}, "questions": {"value": "Can the authors provide quantitative or simulated case studies to estimate the real-world feasibility and impact of the proposed sabotage methods?\n\nHow do the identified attack vectors differ in difficulty and detectability across various AI training architectures (e.g., centralized vs. federated training)?\n\nCould the authors expand on specific defensive mechanisms—technical or organizational—that could help detect or mitigate training sabotage in practice?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The paper discusses state-level AI sabotage techniques that could be dual-use if misapplied. An ethics review is recommended to ensure sensitive details are responsibly handled and publication aligns with ethical disclosure standards. Reviewers with expertise in AI security and dual-use governance should assess potential risks."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zQ7fQCNBwI", "forum": "XRpPi0JzRx", "replyto": "XRpPi0JzRx", "signatures": ["ICLR.cc/2026/Conference/Submission19592/Reviewer_x7UV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19592/Reviewer_x7UV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760565011891, "cdate": 1760565011891, "tmdate": 1762931460043, "mdate": 1762931460043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the possibility that nation states will sabotage frontier AI training runs of their rivals, causing the system to have degraded capabilities or hidden goals. Sabotage is seen as potentially less onerous than stealing model weights, which is an attack vector that has received a lot more attention. Sabotage might be less escalatory and therefore an attractive option for a nation state in a disadvantageous position compared to a more powerful rival. The paper presents some breakdown of the different types of sabotage to help understand the attack surface and attacker motivations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A very interesting extension to MAIM and the RAND report on securing model weights. This paper fits more with what I think threat actors are likely to actually want to do in the real world. It is also clearly written from a position of expertise with how governments think and operate. The main text is clear and tells an interesting story."}, "weaknesses": {"value": "The paper feels quite truncated now with some of the more sensitive parts removed. There aren't any experiments or even example applications of their taxonomy to different real world instances. I feel like I read a very detailed breakdown of a problem, but not a taxonomy.\n\nIt's worth noting that the citations in this work are nearly all in footnotes. I don't often see this in technical papers. I suppose it keeps the text more compact due to the citation style, but it is particularly unusual when a footnote has nothing except a citation in it. I think I would recommend merging some of the footnotes in. If the text gets too wordy, the paper can have more detailed sections that revisit topics. This is not a strong suggestion on my part, just an idea.\n\nSpecific comments on sections:\n\nSection 2 is quite interesting and is probably new background for many readers. I think it could use more citations, especially in the second last paragraph. I had read that the Ukraine war had initially reduced the number of ransomware attacks on the rest of the world (though it's true that this trend seems to have reversed). This paragraph is also in the context of what's happening in wartime, which goes against the bullet point earlier “the situation does not yet justify overt military action”. Perhaps this list should use “or” rather than “and”. Or somehow reworded to include the Ukrainian war scenario.\n\nThe “salami slicing” done by China could use more citations as well, with the construction of artificial islands, Philippines, Vietnam, etc.\n\nIn section 3.1: The separation between the first stage (design and planning) and the second stage (data gathering and pre-processing) does not seem crisp. It seems likely that the second stage can proceed partially in parallel with the first stage. Also, creating a complete plan and deciding how large the model is going to be might depend fairly heavily on how much data can be obtained. The other stages are well known, pre-training and post training, and cannot really overlap. I wonder if it might make more sense to have a single first stage of planning, with several substeps that happen in parallel.\n\nTypo in section 4: “efficacy of removal is a an open question.”\n\nMinor typos in footnote 39: See is italic, and the semicolons aren't really grammatical."}, "questions": {"value": "Why did you choose ICLR as a venue for this work? Overall, I am not sure whether it is the best venue. As someone in or adjacent to this field, I find it very interesting. Perhaps the machine learning community in ICLR could be one of the best audiences for this work, even if it is not a typical paper. But a dedicated AI safety conference/workshop may also be a good fit."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S0BipLCBhq", "forum": "XRpPi0JzRx", "replyto": "XRpPi0JzRx", "signatures": ["ICLR.cc/2026/Conference/Submission19592/Reviewer_XQsL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19592/Reviewer_XQsL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954519830, "cdate": 1761954519830, "tmdate": 1762931459513, "mdate": 1762931459513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that, for state actors, sabotaging AI training will often be more attractive than stealing model weights. It contributes (i) a threat-modeling framework covering strategic objectives (capability degradation vs. value misalignment; overt/covert; attributable/anonymous), (ii) a taxonomy of attack vectors (data poisoning, model poisoning, process disruption, with direct and indirect/organizational routes), and (iii) a walkthrough of the training pipeline highlighting plausible sabotage points and mitigations. It situates the analysis in historical context (e.g., Stuxnet; “grey-zone” operations) and discusses the MAIM (“Mutual Assured AI Malfunction”) dynamic. The Ethics Statement notes that some sensitive details were intentionally omitted; the authors also disclose substantial LLM assistance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Makes a clear case that sabotage of training can be strategically favored in realistic scenarios (e.g., when compute deployment is infeasible for the attacker, or when preventing a rival’s capability matters more than copying it). \n\n2. Useful decomposition by objective (degradation vs. misalignment) and overt/covert/attributable axes; ties choices to strategic implications and examples (Table 1). \n\n3. Training pipeline stages are enumerated with concrete touchpoints for sabotage/mitigation—helpful for practitioners performing risk assessments. \n\n4. The historical review (Stuxnet; Cold War precedents; grey-zone warfare) provides external validity; the paper clarifies where MAIM might or might not hold. \n\n5. Dual-use considerations are acknowledged; sensitive technical details are intentionally withheld; LLM usage is disclosed."}, "weaknesses": {"value": "1. Scope is largely qualitative, with limited technical novelty for ICLR. The piece reads more like a policy/security position paper than a machine-learning research paper; there are no models/algorithms, formal analyses, or empirical evaluations to test claims (e.g., attacker feasibility, detectability, or defense efficacy). Much of the content is synthesis + taxonomy. (The authors themselves note that a fuller threat-modeling study would require non-public information.) \n\n2. The plausibility assessments (e.g., insider-driven model poisoning, RLHF data poisoning) are plausible but lack quantitative risk estimates (likelihoods, cost-to-attack, time-to-detect), red-team case studies, or operational measurements to anchor recommendations. \n\n3. The paper cites J-AISI/NIST but would benefit from a side-by-side mapping showing exactly what is new (e.g., “process disruption” slice; organizational attacks) and what is reframed—ideally as a comparison table or figure. \n\n4. As a primarily strategic/cybersecurity paper with no ML experiments, it may struggle to meet ICLR’s contribution bar without a stronger technical component (e.g., detection methods, formal threat models with testable implications, or empirical audits across pipelines). (ICLR asks that reviews focus on value/new knowledge; here the value is practical but the ML research delta is thin.)"}, "questions": {"value": "1. Re pperationalization: Can you include at least one worked case study (anonymized) that quantifies attacker cost, access required, expected detection latency, and impact, for one data-poisoning and one process-disruption path? This would materially strengthen practitioner utility. \n\n2. Re comparative taxonomy: Provide a comparison matrix vs. J-AISI/NIST categories to make the paper’s unique framing absolutely explicit (what’s added vs. relabeled). \n\n3. Re MAIM implications: Could you formalize simple game-theoretic conditions (signals, attribution noise, cost curves) under which sabotage dominates theft, and simulate parameter regimes to test robustness of your MAIM-related claims? \n\n4. Re Defense playbooks: Your mitigations are high-level; can you attach a practitioner checklist per pipeline stage (e.g., specific logging/audit artifacts for RLHF datasets, insider-risk controls, compromise-resilience drills)? \n\n5. Re Responsible disclosure: Since some analyses were redacted, can you clarify what can be shared with vetted reviewers under confidentiality to better evaluate your novelty/insight claims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical violations evident; dual-use handled responsibly. Paper explicitly discloses use of Claude Sonnet 4 for drafting, no issues from me."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r3iCRDOESX", "forum": "XRpPi0JzRx", "replyto": "XRpPi0JzRx", "signatures": ["ICLR.cc/2026/Conference/Submission19592/Reviewer_Pskk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19592/Reviewer_Pskk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094566223, "cdate": 1762094566223, "tmdate": 1762931459056, "mdate": 1762931459056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}