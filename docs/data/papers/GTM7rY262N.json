{"id": "GTM7rY262N", "number": 16294, "cdate": 1758262772595, "mdate": 1759897249683, "content": {"title": "NeSy-MMCAD: A Neuro-Symbolic Multimodal Framework for Child-Abusive Meme Detection and Explanation with Emotion Consistency", "abstract": "Child-abusive memes pose a serious online safety threat by combining imagery, overlaid text, and humor to mask coercive or exploitative cues. Standard multimodal classifiers, while effective on surface features, often fail in subtle or low-resource cases. We present NeSy-MMCAD, a neuro-symbolic multimodal framework for child-abusive meme detection and explanation with emotion consistency. Our architecture integrates neural perception with symbolic reasoning: neural modules extract probabilistic predicates from images and text, capturing child/adult presence, nudity, violence, toxic language, coercion, and affective signals, while domain-informed rules encode commonsense constraints. A differentiable rule loss is jointly optimized with the classification loss, enforcing symbolic consistency while retaining flexibility to learn from data. Emotion-aware rules capture affective incongruities, and mitigation rules reduce false positives in benign contexts. To support this work, we curate DACAM (Dataset for Analysis of Child-Abusive Memes), a benchmark resource for evaluating harmful content detection. Experiments on DACAM demonstrate improvements in classification accuracy and interpretability over baseline multimodal models. Importantly, rule activations provide transparent explanations that link predictions to explicit constraints. These results demonstrate the effectiveness of combining neuro-symbolic reasoning, multimodal representation learning, and emotion consistency to enhance the reliability and accountability of AI systems for socially critical tasks such as child-abuse detection.", "tldr": "", "keywords": ["Neuro-Symbolic AI", "Multimodal Representation Learning", "Harmful Content Detection", "Emotion-Aware Classification", "Knowledge-Guided Regularization"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7398ccd989fadc2337d3052f657e2e5dc6e1e282.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MM-CAD, a neuro-symbolic multimodal framework designed to detect and explain child-abusive memes, a critically underexplored yet highly sensitive area of harmful content moderation. The approach integrates visual cues from CLIP, textual features from OCR and LLM encoders, and a Quantum-inspired Embedding Enhancement (Q-EE) module that maps multimodal features into a higher-dimensional Hilbert space to better capture subtle, entangled abuse patterns. To support the task, the authors curate DACAM, the first benchmark dataset specifically focused on child-abusive memes, with balanced labels and strong annotator agreement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed work introduces the first dedicated framework and dataset for detecting child-abusive memes, addressing an important yet underexplored safety problem.\n- The work combines neuro-symbolic reasoning, CLIP vision features, OCR text, and quantum-inspired embedding to achieve robust and interpretable detection."}, "weaknesses": {"value": "- DACAM focuses narrowly on child-abusive memes and may not generalize to broader abusive or multimodal harm categories [a, b].\n- The Q-EE component is empirically useful but lacks a deeper explanation of why quantum-inspired embeddings outperform standard high-dimensional mapping.\n- The pipeline relies heavily on OCR quality; noisy or stylized text could degrade performance and reduce robustness.\n- Although rationales are generated, the paper provides limited analysis of whether these explanations are reliable, faithful, or helpful for real moderation workflows [c].\n\n[a] \"Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models.\" The 2023 Conference on Empirical Methods in Natural Language Processing.\n[b] \"Pro-cap: Leveraging a frozen vision-language model for hateful meme detection.\" Proceedings of the 31st ACM international conference on multimedia. 2023.\n[c] \"Towards explainable harmful meme detection through multimodal debate between large language models.\" Proceedings of the ACM Web Conference 2024. 2024."}, "questions": {"value": "- Can the authors provide stronger evidence that Q-EE captures meaningful “quantum-like” interactions rather than simply acting as a high-dimensional projection layer?\n- To what extent might DACAM’s limited scope introduce dataset bias, and how would the model behave on broader abusive-meme domains like GOAT-Bench?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "70rF0cyovl", "forum": "GTM7rY262N", "replyto": "GTM7rY262N", "signatures": ["ICLR.cc/2026/Conference/Submission16294/Reviewer_GwKx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16294/Reviewer_GwKx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723958022, "cdate": 1761723958022, "tmdate": 1762926437367, "mdate": 1762926437367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper makes a socially important contribution by focusing on child-abusive memes and providing a dedicated dataset and a detect-explain pipeline. The quantum-inspired embedding enhancement also appears to yield consistent, though modest, gains across several LLM backbones, and the empirical sweep on a single dataset is relatively thorough. However, several high-impact issues remain unresolved: the entire evidence base is on one small, in-house dataset with no cross-benchmark validation; there is no comparison to established multimodal meme/hate detectors, so the paper’s position in the literature is unclear; the reported gains are not backed by statistical significance or multi-seed reporting, which weakens the Q-EE claim; and dataset sourcing/release details are too loose for a sensitive domain. This paper will have much higher chances if it is submitted to a dataset track."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: This paper addresses a high-impact, underserved harm category (child-abusive memes) with clear societal relevance and links to real moderation needs.\n\nS2: Introduces a dedicated, IRB-reviewed dataset (DACAM) specifically for child-abusive meme detection, with balanced abusive/non-abusive samples and annotated modality information.\n\nS3: Shows that the quantum-inspired embedding enhancement consistently improves both classification F1 and explanation quality across several open LLM backbones.\n\nS4: Provides a comparatively broad empirical sweep on the dataset (zero-shot, few-shot, fine-tuning; text-only vs multimodal; multiple ablations), which strengthens the evidence for the design.\n\nS5: Includes human evaluation on explanations (fluency, consistency, informativeness) over 200 abusive memes, supporting the interpretability claim."}, "weaknesses": {"value": "W1: Results are shown only on a single, relatively small in-house dataset (2,103 memes), so it is hard to tell how well the method would transfer to broader meme/hate benchmarks or real-world distribution shifts, even though the dataset itself is well curated.\n\nW2: Despite the multimodal design that pulls in image, OCR, and title text, the claimed robustness to incomplete modalities is not actually stress-tested; the dataset has no image-only cases and overlapping text modalities, so we cannot see performance under genuinely missing inputs.\n\nW3: Even though the quantum-inspired embedding enhancement component improves scores across several backbones, the paper does not provide a strong classical control to prove that the gains come from the “quantum-inspired” mechanism rather than from a generic non-linear projection.\n\nW4: Lacks comparisons to established multimodal hateful-meme or harmful-content models/datasets (e.g., Hateful Memes, SemEval/MAMI-style tasks), which makes the positioning of the approach within existing literature unclear.\n\nW5: Although a human evaluation of explanations is provided, the section is under-specified (annotator profiles, agreement, protocol), which weakens the strength of the interpretability claim.\n\nW6: The overall pipeline is fairly heavy (CLIP + OCR + LLM + Q-EE + explanation); without an inference-time or resource/latency analysis, it is unclear whether this otherwise practical detect-explain design can be deployed in real moderation settings.\n\nW7: Data collection and release details are only loosely described, even though the dataset is IRB-reviewed, sources, licensing, and handling of potentially illegal CSAM-like material are not spelled out, making reproduction and safe sharing harder."}, "questions": {"value": "Q1: Is there a reason why no other harmful meme datasets are benchmarked?\n\nQ2: Can the authors provide more information about the human evaluation? Expand the human-evaluation section with annotator profiles (number, background), agreement measures, task instructions, and an example rubric, so readers can assess the reliability of the 3.48–3.55 scores."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CKnvAryf0n", "forum": "GTM7rY262N", "replyto": "GTM7rY262N", "signatures": ["ICLR.cc/2026/Conference/Submission16294/Reviewer_A2NE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16294/Reviewer_A2NE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815317676, "cdate": 1761815317676, "tmdate": 1762926436889, "mdate": 1762926436889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a database of memes that relate to or allude to child abuse messaging. The dataset also contains 50% non-abusive memes. The paper then presents a method for detecting such memes and compares the results to several LLM-based baselines. However, I think the two parts of the paper are not very well combined -- I see no reason to use this very specific methodology in this specific context."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Important topic in the context of data moderation.\n- Considerable manual work relating to the dataset creation."}, "weaknesses": {"value": "- I am not sure the embedding present is superior to the embeddings of the picture, or even for ingesting the picture directly (in a VLM model). The comparison only shows improvement compared to a text-only baseline. This is likely because some of the signal comes from the photo, but the specific methodology presented isn't validated by the experiments presented.\n\n- I would have liked to see how the accuracy changes if the dataset is added to a larger meme dataset, showing a more challenging, yet more realistic setting, in which this type of meme is just one type of abusive memes to be detected and taken down. \n\n- In the context of ethics, I would like the authors to discuss more of how they see their work being used."}, "questions": {"value": "Can you improve the paper in relation to the weaknesses pointed out above?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "I think more details about future use are needed."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hckvITL0Of", "forum": "GTM7rY262N", "replyto": "GTM7rY262N", "signatures": ["ICLR.cc/2026/Conference/Submission16294/Reviewer_BbgV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16294/Reviewer_BbgV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994511202, "cdate": 1761994511202, "tmdate": 1762926436381, "mdate": 1762926436381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}