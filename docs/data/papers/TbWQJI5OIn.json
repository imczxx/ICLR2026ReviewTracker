{"id": "TbWQJI5OIn", "number": 833, "cdate": 1756820227239, "mdate": 1759898239596, "content": {"title": "The Telephone Game: Evaluating Semantic Drift in Unified Models", "abstract": "Employing a single, unified model (UM) for both visual understanding (image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened a new direction in Visual Language Model (VLM) research. While UMs can also support broader unimodal tasks (e.g., text-to-text, image-to-image), we focus on the core cross-modal pair T2I and I2T, as consistency between understanding and generation is critical for downstream use. Existing evaluations consider these capabilities in isolation: FID and GenEval for T2I, and benchmarks such as MME, MMBench for I2T. These single-pass metrics do not reveal whether a model that understands a concept can also render it, nor whether meaning is preserved when cycling between image and text modalities. To address this, we introduce the Unified Consistency Framework for Unified Models (CFR-UM), a cyclic evaluation protocol that alternates I2T and T2I over multiple generations to quantify semantic drift. CFR formulates 3 metrics: (i) Mean Cumulative Drift (MCD), an embedding-based measure of overall semantic loss; (ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii) Multi-Generation GenEval (MGG), an object-level compliance score extending GenEval. To assess generalization beyond COCO, which is widely used in training; we create a new benchmark ND400, sampled from NoCaps and DOCCI and evaluate on seven recent models. CFR-UM reveals substantial variation in cross-modal stability: some models like BAGEL maintain semantics over many alternations, whereas others like Vila-u drift quickly despite strong single-pass scores. Our results highlight cyclic consistency as a necessary complement to standard I2T and T2I evaluations, and provide practical metrics to consistently assess unified model's cross-modal stability and strength of their shared representations.", "tldr": "", "keywords": ["Unified Models", "Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8830fa3e75ac6eaa71c30ad06690939eebade060.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an evaluation benchmark for unified models, termed the Semantic Drift Protocol (SDP), which focuses on measuring semantic drift during the image-to-text (I2T) and text-to-image (T2I) generation cycle. Specifically, the authors introduce two metrics:  (i) Mean Cumulative Drift (MCD), an embedding-based measure of overall semantic loss; and  (ii) Multi-Generation GenEval (MGG), an object-level compliance score that extends GenEval.  Extensive experiments demonstrate the effectiveness of the proposed evaluation framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* SDP takes multi-turn generation tasks into account, addressing a gap in previous evaluation frameworks.  \n* The experimental design is thorough, and the results yield insights with meaningful implications for the research community."}, "weaknesses": {"value": "* The proposed framework lacks novelty. The main contributions, MCD and MGG, are relatively straightforward: MCD is essentially the mean of multi-turn embedding similarity scores, while MGG is a direct extension of GenEval.\n* The motivation is somewhat unclear. Although multi-turn dialogue tasks for unified models represent an important research direction, the proposed evaluation framework does not appear to align well with realistic application scenarios. In most real-world settings, after obtaining an initial dialogue output, users would typically modify their prompts to refine the result rather than repeatedly generating the same (I, T) pair as described in this paper.\n* While the paper introduces an evaluation framework, it would be more compelling if the authors demonstrated its practical utility, for instance, by performing post-training or fine-tuning based on the proposed scores and showing measurable improvements in model performance.\n* The paper includes human evaluation results but provides insufficient details about the evaluation procedure. A more comprehensive description of the human evaluation protocol is necessary to ensure the fairness and reliability of the results.\n* The writing lacks clarity and coherence, and the paper contains numerous typographical errors (e.g., line 25: “image-totext” should be “image-to-text”)."}, "questions": {"value": "See 'Weakness'."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gBdoJP6jDM", "forum": "TbWQJI5OIn", "replyto": "TbWQJI5OIn", "signatures": ["ICLR.cc/2026/Conference/Submission833/Reviewer_MboD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission833/Reviewer_MboD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804734986, "cdate": 1761804734986, "tmdate": 1762915622521, "mdate": 1762915622521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Semantic Drift Protocol, an evaluation framework for unified multimodal models that perform both text-to-image and image-to-text tasks. It introduces two key metrics — Mean Cumulative Drift and Multi-Generation GenEval — to measure how well models preserve meaning across multiple modality conversions, addressing a gap in existing benchmarks that only evaluate single-pass performance. The authors also present a new benchmark dataset, NoCaps + DOCCI400, and evaluate seven recent models, finding large differences in their cross-modal semantic stability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Most existing multimodal benchmarks assess text-to-image and image-to-text performance separately, giving little attention to semantic consistency across iterative modality exchanges.  Such consistency is especially important for tasks like multi-turn visual question answering, vision–language dialogue, video captioning, and related multimodal tasks, where meaning must remain stable across modalities.\n\nThe proposed evaluation protocol fills this gap by simulating successive T2I–I2T transformations, capturing how meaning shifts across repeated modality exchanges — a property overlooked by single-pass metrics.\n\nBy comparing shared-weight and decoupled architectures, the paper further shows that shared models tend to preserve meaning more effectively across rounds."}, "weaknesses": {"value": "The paper presents an interesting idea and addresses a gap in multimodal evaluation, but its conceptual and empirical grounding are weak.\nThe definition of *semantic drift* lacks rigor, the metrics suffer from representational bias, and the experiments fail to validate the stated motivation — that higher semantic consistency benefits downstream multimodal reasoning tasks.\nOverall, the work remains exploratory rather than a robust or generalizable benchmark.\n\n1. Conceptual Ambiguity in the Definition of Semantic Drift\nThe central concept is poorly defined. Treating semantic drift as embedding similarity decay is conceptually weak and conflates representation change with genuine semantic degradation. This assumption is unverified—embedding distance measures variation in representation, not meaning. As a result, benign paraphrasing or stylistic variation can be incorrectly penalized as drift, while the metric itself merges object-, attribute-, and relation-level shifts into a single opaque score.\n\n 2. Bias from Embedding-Space Alignment\nBoth metrics (MCD and MGG) rely on pretrained embeddings and detectors, introducing representational bias.  Models trained in similar embedding spaces may appear more consistent—not due to true semantic stability, but because their representations align with the evaluation backbone.\nWithout checks using alternative embeddings or detectors, it is unclear whether the reported rankings reflect genuine stability or embedding overlap, undermining the claim that SDP is architecture-agnostic.\n\n3. Lack of Downstream or Practical Validation\nAll experiments are limited to synthetic T2I–I2T cycling and a human-correlation study, with no evidence that higher semantic consistency improves real multimodal tasks such as VQA, captioning, or visual dialogue. Without quantitative or qualitative downstream validation, the claimed importance of the proposed metrics remains speculative, and it is unclear whether the evaluation protocol reflects meaningful model behavior beyond the test loop."}, "questions": {"value": "Definition of Semantic Drift\nThe definition of “semantic drift” as embedding similarity decay is ambiguous. It does not distinguish genuine semantic loss (e.g., object or relation errors) from benign changes like paraphrasing or stylistic variation. How can the authors ensure that their metric reflects semantic degradation rather than surface variation?\n\nEmbedding Dependence\nSince both MCD and MGG rely on pretrained embeddings (CLIP, DINO, MPNet), results may favor models trained in similar representation spaces. Have the authors tested whether model rankings remain consistent under alternative embeddings or random projections?\n\nDownstream Relevance\nThe paper claims that higher semantic consistency benefits multimodal task, but provides no downstream validation. Can the authors show whether SDP scores correlate with task performance on VQA, captioning, or dialogue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RbOW7SiNz1", "forum": "TbWQJI5OIn", "replyto": "TbWQJI5OIn", "signatures": ["ICLR.cc/2026/Conference/Submission833/Reviewer_iUSY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission833/Reviewer_iUSY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827597191, "cdate": 1761827597191, "tmdate": 1762915622113, "mdate": 1762915622113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Evaluation of unified multimodal models remains limited, as image understanding and generation are evaluated in isolation. This paper formulates the semantic drift and cross-consistency problems, and shows that existing single-pass metrics cannot thoroughly measure the gap between understanding and generation capabilities. To address this, the paper proposes the Semantic Drift Protocol (SDP), a cyclic evaluation framework that measures how well a unified model preserves semantic fidelity by alternating between image-to-text (I2T) and text-to-image (T2I) generation. They also introduce a new benchmark (Nocaps+Docci400) evaluating seven recent models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed evaluation protocol effectively measures a unified model's ability to preserve semantic fidelity across modalities, which existing metrics or benchmarks fail to capture. The results in Figure 5 clearly demonstrate this point, providing intriguing evidence of how semantic drift can occur across different categories.\n* The main figures (Figures 1 and 2) clearly explain the core problem the paper is addressing.\n* The categorization of unified models in Section 2 is clear and well-motivated, and the experiments thoughtfully include models from different categories.\n* They provide a human study that confirms a strong correlation between the proposed metric and human-perceived quality."}, "weaknesses": {"value": "* Including a guideline on the recommended number of evaluation cycles would improve reproducibility and facilitate adoption of this protocol in future research.\n* It would be helpful to improve the clarity of the x-axis and y-axis labels in Figure 6 (e.g., Sδ(g) distance score vs. Number of generations). Also, the figure should be referenced accurately in the main text (line 398: change \"Plot\" to \"Figure\" 6).\n* Typos in line 252."}, "questions": {"value": "* It is very interesting that BLIP-3o, a partially shared model, performs exceptionally well on this metric, achieving the second-best result. Do the authors have any insights into why this might be the case?\n* Overall, the paper suggests a novel evaluation protocol that can effectively measure a qualitatively different aspect of unified models. Minor presentation quality (e.g., titles, axis labels, and font size of plots, and typos) could be improved for greater clarity and impact."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nDoQXWPz7Y", "forum": "TbWQJI5OIn", "replyto": "TbWQJI5OIn", "signatures": ["ICLR.cc/2026/Conference/Submission833/Reviewer_KpeP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission833/Reviewer_KpeP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005204477, "cdate": 1762005204477, "tmdate": 1762915622005, "mdate": 1762915622005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}