{"id": "qtUw7Wwu0t", "number": 4796, "cdate": 1757769441117, "mdate": 1759898012489, "content": {"title": "The EEG activation maps in recent work are uninterpretable by experts", "abstract": "Recent papers claim to decode object class from EEG recordings of\nsubjects viewing image stimuli from ImageNet and to use that\nclassifier to construct activation maps for the depicted object class\nthat are consistent with neuroscience knowledge.  Empirical evaluation\nof the activation maps by EEG experts calls this claim into question.", "tldr": "We refute a central claim used to justify validity of a confounded dataset used in about one hundred papers.", "keywords": ["EEG"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a4cd0500ee1a81c7e5cc8cae88b90e0365ef6e61.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents an empirical evaluation of activation maps (generated in another paper) and argues based off the result that the evaluators were unable to map the object class and activation map with above chance accuracy that those activation maps therefore call the validity into question. Furthermore, based off this result the paper argues that this implies that the dataset used in the other paper (Spampinato et al 2017) and other datasets with a similar temporal confound should be avoided and any results drawn from these datasets should be discounted."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "It is meaningful to highlight limitations in prior work to both help correct misunderstandings that may arise from that prior work and to help push the field forward."}, "weaknesses": {"value": "1. Relevance to call for papers: This work is not aligned with the focus of this conference as defined in the call for papers (https://iclr.cc/Conferences/2026/CallForPapers). While \"applications to neuroscience & cognitive science\" is a subject area in the call for papers, this is not an application of \"feature learning, metric learning, compositional modeling, structured prediction, reinforcement learning, uncertainty quantification and issues regarding large-scale learning and non-convex optimization\" or related topics to neuroscience & cognitive science.\n2. Relevance to ICLR audience: While highlighting limitations in prior work is valuable, of all the prior work that is being highlighted none of these works were previously published in ICLR. This further supports that this workshop is not the correct venue for this paper, as this limitation will likely be of little interest to the ICLR audience. If there is interest, as the paper itself mentions there are other papers that have previously pointed out limitations to the same body of work this paper focuses on that could be read. \n3. Soundness, Claim is not scientifically rigorous: The paper presents extremely limited details about how the \"experts\" who conducted the empirical evaluation were identified, which limits the strength of the only claim in the paper. Additionally, only summary statistics are presented which prevents the reader from being able to assess the data and draw conclusions on their own about the implication of the empirical evaluation. Furthermore, one of two examples of the empirical evaluation that is shown (Fig 2) raises the question of how the experts would be expected based off their neuroscience knowledge identify which activation map was correct as both overlap with the occipital cortex and the specific localization of jack-o-lanterns is not something that is taught as part of EEG analysis even to experts. \n4. Presentation, Motivation is unsupported: The paper boldly claims that \"Palazzo et al\" generally claims that the \"activation maps are consistent with neuroscience knowledge\" and then presents 7 quotes from \"Palazzo et al\" papers. From these 7 Palazzo et al\" quotes, only 1 quote presents any support for this claim. The fact that the evidence presented in the paper to support the motivation for highlighting a limitation in the \"Palazzo et al\" papers is unclear in how it supports the claim is a key weakness."}, "questions": {"value": "It would be valuable if the paper explained: \n1. How the experts expertise in EEG analysis was tested before they participated in the empirical evaluation? \n2. Whether the experts are in fact experts at object classification activation maps, or if there EEG expertise in in another area? \n3. What were the raw results of the empirical analysis? Were there any categories that the experts agreed with the activation maps? \n4. The reason why quotes were included in the intro (taking up almost 1 full page of text) when 6 of the 7 quotes do not support the claim they are included to support? Perhaps there is nuance that is not clear from merely reading the quotes. \n\nHowever, clarifying these points will not chance that the paper is not relevant to the call for papers nor for the ICLR audience. Therefore, I'd recommend the paper be updated to address the above points and submitted at a neuroscience focused venue instead."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4I6xcLAN1x", "forum": "qtUw7Wwu0t", "replyto": "qtUw7Wwu0t", "signatures": ["ICLR.cc/2026/Conference/Submission4796/Reviewer_ZyiH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4796/Reviewer_ZyiH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462720408, "cdate": 1761462720408, "tmdate": 1762917581476, "mdate": 1762917581476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical issue in EEG-based visual decoding: the lack of expert interpretability of EEG activation maps proposed in a series of high-impact works. It shows that the activation maps—purported to align with neuroscience knowledge—are not interpretable by EEG experts."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed question is interesting.\n2. The references are comprehensive."}, "weaknesses": {"value": "1. Asking humans to interpret EEG spectrograms and map them with true stimuli is an excessively difficult task. Generally, EEG can only capture superimposed general visual features. Hence, the proposed experimental paradigm is not feasible. Meanwhile, this design cannot verify that deep models are unable to extract distinguishable features, as human visual perception is inherently limited.\n2. Lack of analysis: only averaged results are reported, no detailed analysis and case-by-case discussions are provided. For example, is there any case where humans can reach high accuracy?\n3. The study only demonstrates that the EEG response to a single image cannot be identified by humans, but it cannot prove that the EEG response related to visual perception is uninterpretable.\n4. Lack of insight and contribution on how this research can advance the area."}, "questions": {"value": "Could you clarify the definition of \"activation maps are consistent with neuroscience knowledge\"? Is it a necessary conclusion that the EEG response to a single image cannot be identified by humans?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EfX3uUvJSJ", "forum": "qtUw7Wwu0t", "replyto": "qtUw7Wwu0t", "signatures": ["ICLR.cc/2026/Conference/Submission4796/Reviewer_oV4Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4796/Reviewer_oV4Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485889634, "cdate": 1761485889634, "tmdate": 1762917581100, "mdate": 1762917581100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper mentioned a crucial problem faced by the field of EEG-based visual decoding, which was incurred by the flawed dataset for a previous paper. However, this paper is more like an unfinished comment that needs substantial data analysis."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "This work pointed out an important issue that is easily faced in the experimental design for brain decoding."}, "weaknesses": {"value": "Hardly any analysis was presented in the article."}, "questions": {"value": "The paper from Li et al. [1] thoroughly analyzed the shortcomings of the work from Spampinato et al. On this basis, what improvements does this article propose?\n\n[1] R. Li et al., \"The Perils and Pitfalls of Block Design for EEG Classification Experiments,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 1, pp. 316-333, 1 Jan. 2021\n[2] C. Spampinato, S. Palazzo, I. Kavasidis, D. Giordano, N. Souly, and M. Shah, “Deep learning human mind for automated visual classification,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 6809–6817."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Mt3eZ761X3", "forum": "qtUw7Wwu0t", "replyto": "qtUw7Wwu0t", "signatures": ["ICLR.cc/2026/Conference/Submission4796/Reviewer_heEM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4796/Reviewer_heEM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761486363256, "cdate": 1761486363256, "tmdate": 1762917580417, "mdate": 1762917580417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Relevance and Significance"}, "comment": {"value": "To understand this work's significance, consider this brief historical\noverview.\n\nSpampinato et al. (2017) introduced a block-designed dataset\n(\"Perceive\") and methods that claim to achieve extremely high accuracy\ndecoding stimulus image class from EEG recordings. This was amplified\nby follow on papers (Kavasidis et al. 2017, Palazzo et al. 2018,\n2020a, 2020b, 2021), many of which claim to do things like reconstruct\nstimulus images from EEG recordings. Further, Tirupattur (2018) does\nthis with a fresh dataset (Kumar 2018) that has the same block design.\n\nLi et al. (2021) debunked all of the above, demonstrating that the\nPerceive dataset suffers from a block confound. EEG exhibits drift,\nencoding a clock in the signal. Since Perceive was collected with all\nand only stimuli of the same class being temporally adjacent, the\nclassifier can mistakenly classify the clock/drift instead of the\nstimulus-related EEG response. Follow on papers (Ahmed et al. 2021,\n2022, Bharadwaj et al. 2023) added novel independent confirmation of\nthe results of Li et al. (2021).\n\nDespite this, Palazzo et al. (2020b, 2021, 2024) continue to argue\nthat their dataset is valid. At this point, there are over one hundred\npapers that use the Perceive dataset, the Kumar (2018) dataset, or\nother datasets that suffer from the same block confound. Many new\ndatasets have been collected with this same block confound, some of\nwhich are becoming widely used. The vast majority of these were\npublished after the confound became known (Li et al. 2021). Some of\nthese are unaware of the confound. Others are aware, but dismiss it,\noften based on the argument of Palazzo et al. (2020b, 2021, 2024).\n\nThat argument is what this manuscript refutes."}}, "id": "QagfihByp8", "forum": "qtUw7Wwu0t", "replyto": "qtUw7Wwu0t", "signatures": ["ICLR.cc/2026/Conference/Submission4796/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4796/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4796/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763735732445, "cdate": 1763735732445, "tmdate": 1763735732445, "mdate": 1763735732445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper audits published EEG \"activation maps\" from a prior pipeline by running a blinded expert-judgment study (two forced-choice tasks) to test whether the maps are interpretable to humans. The authors report that experts do not reliably match maps to stimuli and conclude that such maps should not be used to support neuroscience claims about the underlying dataset or model line. The stated contributions are: a focused human-study evaluation of published EEG maps from a prominent prior work, and a critique arguing that these maps, as presented in the literature, do not carry the neuroscientific signal they are often claimed to reflect."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear, concrete target: evaluates what readers actually see in prior publications (published maps), not a moving reimplementation.\n\n2. The overarching question whether these maps are meaningfully interpretable by domain experts is timely and under-tested.\n\n3. Simple, transparent task design that is easy to reproduce."}, "weaknesses": {"value": "1. This is a narrow human-subjects audit with minimal algorithmic or methodological innovation. For an ICLR main track, the CS contribution feels thin; the work reads as a focused methods/ethics evaluation better suited to a special track or journal.\n\n2. The study evaluates maps from a single prior pipeline yet the paper’s narrative gestures toward invalidating a broad literature. The conclusion should be bounded to the artifacts actually tested. This is not a request to \"test all papers\"; it’s a request to either restrict the claim to the evaluated pipeline, or add a small, diverse sample of other pipelines if the broader claim is essential.\n\n3. There is no positive control using a known good EEG paradigm and/or dataset to verify that the interface/task can detect interpretability when it’s expected to be present. Without this, the null finding could be driven by task design, map rendering, or the natural limits of EEG for fine-grained image classes. Likewise, no negative controls (e.g., randomized/permuted or class-swapped maps) are included to calibrate chance-level behavior.\n\n4. Per-class topographies for natural images are an extremely demanding target for EEG (low SNR, coarse spatial resolution, limited coverage). A failure on this task does not uniquely imply \"non-interpretability\" of the underlying neural activity; it may reflect a mismatch between the claim tested and what EEG can plausibly support.\n\n5. \"EEG expert\" is a broad term. The sample mixes backgrounds and seniority, and it’s unclear how many participants have direct experience with cognitive/vision EEG map interpretation versus clinical EEG. This weakens the negative inference.\n\n6. The maps appear heavily smoothed/rasterized rather than recomputed from source. If originals cannot be regenerated, the paper should analyze and justify how extraction/smoothing affects discriminability and discuss robustness to these presentation choices.\n\n7. The tone and structure feel informal for an A* venue (short abstract that under-reports the methods/results, heavy quoting/bullets). The paper would benefit from tighter framing, clearer hypotheses, and more rigorous statistical exposition without going into excessive polemics."}, "questions": {"value": "Will you constrain the central claim to the specific pipeline you evaluated, or add a small sample from other pipelines to justify broader statements?\n\n1. Can you include a positive control (even a compact add-on) using a well-established EEG paradigm to demonstrate that your task/interface can detect interpretability when present?\n\n2. How were \"EEG experts\" screened for relevance to cognitive/vision EEG topographies (as opposed to clinical EEG)? Would results change if limited to that subgroup?\n\n3. If recomputing maps from source is infeasible for the rebuttal, can you document the exact provenance and quantify how rasterization/smoothing might degrade discriminability (e.g., show side-by-side renderings with/without smoothing or different interpolation)?\n\n4. Can you refine the hypothesis to acknowledge EEG modality limits and rephrase conclusions accordingly (e.g., \"these published maps, as rendered and for this task, do not support the claimed interpretability\")?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WJlPpS5xkw", "forum": "qtUw7Wwu0t", "replyto": "qtUw7Wwu0t", "signatures": ["ICLR.cc/2026/Conference/Submission4796/Reviewer_kqv8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4796/Reviewer_kqv8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838116147, "cdate": 1761838116147, "tmdate": 1762917579918, "mdate": 1762917579918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}