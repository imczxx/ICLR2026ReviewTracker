{"id": "sq6Lc4v80W", "number": 10019, "cdate": 1758156505441, "mdate": 1762927330163, "content": {"title": "To Memorize or Not to Memorize: An Analysis of Supervised Fine-Tuning in Large Language Models", "abstract": "Supervised fine-tuning (SFT) is a cornerstone technique for adapting large language models (LLMs) to specific domains and tasks. However, its propensity to induce verbatim memorization of training data poses significant risks to safety, privacy, and generalization. This paper presents an empirical analysis of the mechanisms underlying memorization within LLMs during SFT. Our findings confirm that SFT is a direct driver of memorization, with a clear positive correlation between the number of training epochs and the rate of verbatim data recall. The characteristics of the fine-tuning dataset are a critical determinant of memorization. We demonstrate that models trained on broad, open-domain datasets exhibit substantially more memorization than those trained on narrow, domain-specific ones, highlighting a crucial trade-off between model versatility and data containment. Furthermore, we indicate that verbatim memorization is suppressed when the training data includes inputs with high similarity paired with dissimilar outputs. We posit that this phenomenon is not a desirable mitigation strategy but rather a symptom of the model being exposed to conflicting data signals. These findings underscore the complex trade-offs in SFT and stress the importance of understanding these underlying dynamics to develop LLMs that are both capable and secure.", "tldr": "Memorization of LLM at i'th data in space and j'th terms in sequence can be nicely decomposed and easily predicted.", "keywords": ["Memorization", "Interpretation", "Privacy"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e9f9e7eb60918ed5642202f2bff5920bd30899be.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper performs some analysis on the memorization in LLM fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The experimental setups are described clearly."}, "weaknesses": {"value": "Novelty: The authors conduct some experiments and perform some analysis, but for me these are some common understandings based on deep learning literature (e.g., longer training often leads to zero training loss but a large generalization gap, thus early stopping is a common practice) and others are also intuitive (e.g., if two samples are almost the same but the label is different, then the model is likely to be less memorizing this content). The authors are suggested to provide a more comprehensive literature review. \n\nExperiment design: The experiment in Figure 2 is not rigorous. Different datasets have different sample size.\n\nWriting and layout: The following are not critical issues in the novelty and intellectual merit, but somehow hurt the reading experience.\n\n(1) Abstract: The abstract lists some observations in the experiments, but the last sentence is not clear. \"These findings underscore the complex trade-offs in SFT and stress the importance of understanding these underlying dynamics to develop LLMs that are both capable and secure.\" Why the observations underscore the complex trade-offs in SFT? What are the underlying dynamics?\n\n(2) Figures: the figures can be smaller.\n\n(3) Algorithm 1 can be moved to the appendix."}, "questions": {"value": "Please address me concerns in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hFHJwEYk1a", "forum": "sq6Lc4v80W", "replyto": "sq6Lc4v80W", "signatures": ["ICLR.cc/2026/Conference/Submission10019/Reviewer_oHt9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10019/Reviewer_oHt9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760656130048, "cdate": 1760656130048, "tmdate": 1762921432441, "mdate": 1762921432441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "1Smmd6hCGH", "forum": "sq6Lc4v80W", "replyto": "sq6Lc4v80W", "signatures": ["ICLR.cc/2026/Conference/Submission10019/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10019/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762927328954, "cdate": 1762927328954, "tmdate": 1762927328954, "mdate": 1762927328954, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Supervised fine-tuning directly drives memorization in large language models, with longer training and more diverse datasets increasing verbatim recall, while conflicting data signals only suppress memorization by causing model confusion."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a comprehensive and systematic empirical analysis of memorization dynamics during supervised fine-tuning, filling an important research gap.\n\n2. It introduces quantitative metrics linking dataset diversity, training duration, and conflicting signals to memorization, offering practical insights for safer model fine-tuning."}, "weaknesses": {"value": "1. The paper appears incomplete, with several sections and analyses left unfinished.\n\n2. The experimental results are based on limited data, reducing the reliability of the conclusions.\n\n3. It lacks comparisons across different model architectures, which weakens the generality of the findings."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "IZ4Io8k997", "forum": "sq6Lc4v80W", "replyto": "sq6Lc4v80W", "signatures": ["ICLR.cc/2026/Conference/Submission10019/Reviewer_3Bws"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10019/Reviewer_3Bws"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761611052867, "cdate": 1761611052867, "tmdate": 1762921431933, "mdate": 1762921431933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how supervised fine-tuning (SFT) leads to memorization in large language models. The authors analyze memorization across training epochs, dataset diversity, and conflicting samples, which means similar inputs with different outputs). Using Llama-3.1-8B-Instruct, they find that longer training and higher data diversity increase verbatim memorization, while conflicting examples suppress it due to gradient interference. The work offers clear empirical evidence and practical insights for safe fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear and well-structured empirical study on SFT memorization.\n2. Findings are intuitive and consistent across datasets.\n3. Practical insights like treating epochs as a memorization budget are useful for fine-tuning practice."}, "weaknesses": {"value": "1. The paper shows that samples with similar inputs but different outputs are less likely to be memorized. This is an interesting pattern, but the explanation stays at the observation level and remains speculative. There is no evidence from maybe gradients, attention distributions, or representation dynamics to support the hypothesis. I think adding such analysis will be better.\n2. The evaluation focuses on exact match and prefix continuation, which captures surface-form reproduction but does not cover semantic or entity-level memorization. In practice, language models often retain information through paraphrasing or recalling factual entities rather than copying text verbatim. The paper would be stronger if it discussed or tested whether these deeper forms of leakage follow the same trends.\n3. The study is conducted only on an 8B model with full-parameter SFT, and it is unclear whether models of different sizes would exhibit the same memorization behaviors. Some discussion on model scaling would help define how broadly the conclusions apply."}, "questions": {"value": "The conflict-sample analysis is interesting. Could you elaborate on how those examples were selected and whether their effect remains stable across domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6XeOqVYVnO", "forum": "sq6Lc4v80W", "replyto": "sq6Lc4v80W", "signatures": ["ICLR.cc/2026/Conference/Submission10019/Reviewer_7xy2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10019/Reviewer_7xy2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10019/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969726161, "cdate": 1761969726161, "tmdate": 1762921431599, "mdate": 1762921431599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}