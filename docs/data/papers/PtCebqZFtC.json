{"id": "PtCebqZFtC", "number": 8793, "cdate": 1758098415357, "mdate": 1759897763570, "content": {"title": "On the Expressive Power of Weight Quantization in Deep Neural Networks", "abstract": "In recent years, weight quantization, which encodes the connection weights of neural networks in an $n$-bit format, has garnered significant attention due to its potential for model compression. Many implementation techniques have been developed; however, the theoretical understanding of many aspects, especially the approximation and degradation of expressive power as the number of quantization bits decreases, remains unclear. In this paper, we conduct a theoretical investigation into the expressive capability of deep neural networks relative to the number of quantization bits. We establish the universal approximation property of quantized neural networks with linear width and exponential depth. Additionally, we confirm that weight quantization leads to expressive degradation, in which the expressive capacity of quantized neural networks degrades polynomially as the number of quantization bits decreases. These theoretical findings provide a solid foundation for advancing weight quantization in the context of scaling laws and shed insights for future research in model compression and acceleration.", "tldr": "This paper conducts a theoretical investigation into the expressive capability of deep neural networks relative to the number of quantization bits.", "keywords": ["Deep Neural Networks", "Weight Quantization", "Expressivity", "Universal Approximation", "Expressive Degradation"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/db4a5a1c4f7361b7bd89ba55f4a1ab9730e4893a.pdf", "supplementary_material": "/attachment/cbe9804b06d55ed25a4e130e3c5a4f234118cb63.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes a theoretical framework for analyzing the expressive power of weight-quantized neural networks, proving that networks with two or more bits retain universal approximation while 1-bit networks suffer expressive collapse, and that expressive power degrades polynomially as bit-width decreases, with experimental validation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper establishes a formal mathematical link between quantization bit-width and expressive power, providing a solid theoretical basis for weight quantization.\n2. The paper rigorously formulates universal approximation, expressive collapse, and polynomial degradation with clear and reproducible logic.\n3. The theoretical results are validated through simulation and ImageNet experiments, enhancing the credibility and practical relevance of the work."}, "weaknesses": {"value": "1. The metric ln(accuracy/model complexity) used in the ImageNet experiments appears to be uncommon; have the authors considered providing the raw values to facilitate the evaluation of fitting accuracy?\n2. Would the main theorems still hold if weight quantization were modeled as a stochastic process rather than deterministic rounding?\n3. The paper repeatedly mentions “linear width” and “exponential depth”; could the authors provide a one-sentence explanation of their physical or intuitive meanings?\n4. In Experiment 1, 1000 sample points were generated, but the sampling method (uniform or normal) was not specified—could this affect the results?\n5. The paper contains rich theoretical proofs; it is recommended that the authors include a table of symbols and terminology to help readers better follow the subsequent derivations."}, "questions": {"value": "See the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Zn89luY96r", "forum": "PtCebqZFtC", "replyto": "PtCebqZFtC", "signatures": ["ICLR.cc/2026/Conference/Submission8793/Reviewer_a7St"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8793/Reviewer_a7St"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918685645, "cdate": 1761918685645, "tmdate": 1762920565003, "mdate": 1762920565003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the theoretical expressive power of neural networks with quantized weights. It shows that (1) quantized networks can still be universal approximators with sufficient depth and width, and (2) expressiveness degrades polynomially as the number of bits decreases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Provides a clear analysis of how weight quantization affects expressive power, which is important for model compression research.\n* Establishes universal approximation for quantized networks and quantifies polynomial degradation with bit reduction."}, "weaknesses": {"value": "* The paper does not cite Three Quantization Regimes for ReLU Networks (ca2024), which studies depth-precision trade-offs, minimax approximation error, and identifies under-, over-, and proper quantization regimes. This omission weakens both novelty and completeness of the literature review.\n* While polynomial degradation is shown, the paper does not connect this to minimax approximation error or the mechanisms behind it, limiting practical guidance on bit allocation."}, "questions": {"value": "* How does your polynomial degradation result relate to the three quantization regimes identified in ca2024? Why was this prior work not discussed?\n* Can you provide bounds or rates for the universal approximation property relative to network width and depth, to make the results more actionable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rPWmZDdg4o", "forum": "PtCebqZFtC", "replyto": "PtCebqZFtC", "signatures": ["ICLR.cc/2026/Conference/Submission8793/Reviewer_U3p8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8793/Reviewer_U3p8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000013834, "cdate": 1762000013834, "tmdate": 1762920564500, "mdate": 1762920564500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores how weight quantization influences the expressive power of deep neural networks. It shows that networks using two or more bits can still approximate any continuous function when sufficiently deep, while one-bit networks restricted to {0, 1} weights lose this ability entirely. The authors further derive a quantitative relationship between precision and representational accuracy, demonstrating that the approximation error between quantized and full-precision models grows polynomially as the number of bits decreases. Empirical tests on synthetic and image-classification tasks follow the same general pattern, though the experiments are limited in scope and mainly serve as qualitative support for the theoretical claims."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  The paper provides a unified theoritical framework connecting quantization to expressibity and approximation error. \n2. The constructive proof for universality is mathematically sound and leverages ideas from deep-narrow network theory (Kidger & Lyons, 2020)."}, "weaknesses": {"value": "The main weakness lies in the fact that the paper’s “expressive collapse” theorem for 1-bit neural networks only applies to models whose weights are restricted to the set {0, 1}, rather than the practically relevant signed case {−1, +1}. Because {0, 1} weights can only form non-negative linear combinations, such networks lack the ability to perform subtraction or cancellation, which makes their limited expressiveness somewhat inevitable. This means the negative result demonstrates the weakness of a degenerate, unsigned quantization scheme rather than establishing a general limitation of binary networks. Without extending the analysis to signed weights or reconciling it with prior work that found universal approximation in the {−1, +1} setting, the paper’s main claim risks overstating its generality and practical relevance.\n\nMinor writing error (do not affect score):\nReference list duplicates Courbariaux et al., 2015a/2015b entries. They are identical. keep one."}, "questions": {"value": "Would allowing signed 1-bit weights {−1, +1} restore universal approximation, or does the expressive-collapse phenomenon persist under that setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q5FEoViiFp", "forum": "PtCebqZFtC", "replyto": "PtCebqZFtC", "signatures": ["ICLR.cc/2026/Conference/Submission8793/Reviewer_bWbs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8793/Reviewer_bWbs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762046883902, "cdate": 1762046883902, "tmdate": 1762920564074, "mdate": 1762920564074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}