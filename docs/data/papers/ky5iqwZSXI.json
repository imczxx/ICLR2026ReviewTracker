{"id": "ky5iqwZSXI", "number": 24615, "cdate": 1758358584963, "mdate": 1763637620055, "content": {"title": "Reliable Fine-Grained Evaluation of Natural Language Math Proofs", "abstract": "Recent advances in large language models (LLMs) for mathematical reasoning have largely focused on tasks with easily verifiable final answers while generating and verifying natural language math proofs remains an open challenge. We identify the absence of a reliable, fine-grained evaluator for LLM-generated math proofs as a critical gap.\nTo address this, we propose a systematic methodology for developing and validating evaluators that assign fine-grained scores on a 0-7 scale to model-generated math proofs. \nTo enable this study, we introduce ProofBench, the first expert-annotated dataset of fine-grained proof ratings, spanning 145 problems from six major math competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from Gemini-2.5-Pro, o3, and DeepSeek-R1. \nUsing ProofBench as a testbed, we systematically explore the evaluator design space across key axes: the backbone model, input context, instructions and evaluation workflow.\nOur analysis delivers ProofGrader, an evaluator that combines a strong reasoning backbone LM, rich context from reference solutions and marking schemes, and a simple ensembling method; it achieves a low Mean Absolute Error (MAE) of 0.926 against expert scores, significantly outperforming naive baselines.\nFinally, we demonstrate its practical utility in a best-of-$n$ selection task: at $n=16$, ProofGrader achieves an average score of 4.14/7, closing 78\\% of the gap between a naive binary evaluator (2.48) and the human oracle (4.62), highlighting its potential to advance downstream proof generation.", "tldr": "LLMs lack reliable proof evaluators. We introduce ProofBench and a 0–7 methodology; our ProofGrader (marking schemes + ensembling) hits RMSE 1.093 vs experts and lifts best-of-8 to 4.05/7, closing >90% of the gap to a human oracle.", "keywords": ["automated proof evaluation; LLM-as-a-judge; LLM-generated math proofs; rubric-guided grading; prompt optimization; expert-annotated proof dataset; evaluator reliability; reward modeling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/56b07f400a2a6c79d6c3483e4d9f6655416147f2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper evaluates the feasibility and effectiveness of LLMs as judges for the continuous-scale grading of natural language solutions to mathematical problems. The authors present a pipeline for 1) automatically generating grading rubrics, 2) manually verifying LLM-generated solutions, and 3) investigating various aspects of an LLM-as-a-judge system. This process resulted in the `ProofBench` dataset of 393 expertly graded solutions, used for evaluating the backbone model and prompting style of an LLM judge. The work also presents `ProofGrader`: the best-performing combination of a judgment ensemble using the o3 model, with a marking scheme and potential solutions as additional reference information. As a proof of concept for `ProofGrader`'s utility, the authors show that the system can be used as a solution selector in a best-of-8 setting, achieving a score that approaches the human oracle's performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The topic of the paper is clearly well-motivated, relevant, timely, impactful, and provides opportunities for further developments in the field\n2. The paper is overall well-written and conveys the high-level idea well.\n3. The authors have investigated several factors affecting the performance of an LLM-as-a-judge, identifying the backbone model and a well-defined marking scheme as the most significant factors for its performance. This provides readers with clear guidance on the information necessary for achieving accurate automatic grading.\n4. This work provides the currently largest dataset of expert-annotated solutions graded on a continuous scale, covering a wide range of challenging recent problems suitable for further LLM-as-a-judge validation.\n5. The best evaluated settings for the LLM-as-a-judge are shown to also correlate with a better performance as a best-of-n selector. This is a very desirable result, showing that the comparisons performed in the analysis are valid on downstream tasks.\n6. The authors provide the prompts necessary for running the LLM-as-a-judge, aiding reproducibility.\n7. The annotator calibration phase is a valuable step for minimizing error and noise in the final dataset."}, "weaknesses": {"value": "For this section, I have labelled each comments as either:\n - *Critical*: These weaknesses have significantly impacted the score, and addressing/not addressing them could positively or negatively affect my final recommendation.\n- *Important*: Addressing these weaknesses would, in my opinion, significantly strengthen the paper's quality and/or clarity.\n- *Minor*: These are nitpicks that, while valuable to address, have not impacted the score of this assessment.\n\n1. Reproducibility\n    - (Critical) The authors claim `ProofBench` as one of their core contributions. However, despite promising to open-source the dataset upon publication, I see no reason why this was not done as part of the supplementary material. Not providing a core contribution of the paper for peer review significantly hinders the process. Upon release, the authors should also ensure they include an appropriate license, given the nature of the source materials.\n    - (Important) The annotation pipeline, described in A.3, is missing a substantial amount of details. Further elaboration can be found in the **Questions** section.\n\n2. Methodology\n    - (Critical) The authors have not reported the reliability of their human grading. They describe having performed double-grading on 20% of the solutions but have not reported any inter-annotator agreement statistics. In particular, they mention that they \"adjudicate all flagged disagreements,\" however, it is not clear what threshold for a disagreement constitutes a flag, how often flags occurred, or how they resolved these inconsistencies. The perceived reliability of the remaining 80% of grades is highly dependent on clarifying this process.\n    - (Critical) Generating rubrics is an incredibly challenging task in practice. This paper involves an automated system that is difficult to verify without significant manual intervention. Details about how this process was refined are lacking. Furthermore, no discussion is presented on how the annotators verified that these rubrics adhered to a standard for high-quality marking schemes. For example, the rubric in Appendix A.6 gives a total of 3 points for initial observations that seem relatively trivial compared to the rest of the proof. While my interpretation of this rubric is a personal opinion, I believe the authors should clarify how they ensured that the rubrics adhere to a high standard of quality, reflecting authentic evaluation practices.\n    - (Important) The authors claim that in Section 6 they investigate the judging framework as a reward model. However, the described setting appears to lack a clear, realistic application. In particular:\n\n        * If used within a solver's selection system, as described in the paper, such applications often aim to solve a problem without access to a reference solution. In that case, the best result seems to be `o3`, at around 3 average points, which is considerably lower than the human oracle's 4.21.\n        * If used as a reward model for training, as proposed in Sections 1 and 7, the presented setups are all too expensive to be scalable to a full RL (or other) pipeline.\n \n      The authors should clarify what a potential use case for this result can be.\n    - (Minor) The paper makes no distinction between results on undergraduate- and high-school-level problems, and whether this is a relevant factor.\n\n3. Dataset\n    - (Important) The dataset consists of problems from 2022-2025 from popular competitions (IMO, USAMO, Putnam). The earlier years precede the knowledge cutoff dates for the models tested. The authors should discuss whether test-set contamination has impacted the results and, if so, how significant the effect is.\n    - (Important) The authors claim to have sourced their problems and solutions from official sources. However, to the best of my knowledge, the USAMO and USATST do not publicly release their competition materials. The authors should clarify this point in their rebuttal.\n\n4. Writing and Clarity\n    - (Minor) The work never explicitly states which configuration constitutes `ProofGrader`. While it can be inferred from the results, it would be best if the authors defined this in the earlier sections of the paper.\n    - (Minor) Most models presented in the paper are not cited according to best practices. Standard practice is to cite the official model cards from the providers, if one exist.\n    - (Minor) In Section 5.3, the authors refer to an \"ensemble\" of evaluators. This term is usually reserved for a collection of **different** models or algorithms applied to the same task, rather than for multiple samples from a single algorithm."}, "questions": {"value": "1. Can the authors address all the concerns listed in the **Weaknesses** section?\n2. The authors describe the marking scheme generation methodology in Section 3.1 and later in Appendix A.3. However, the description lacks sufficient detail. Can the authors clarify:\n    - The model families they tested for rubric generation?\n    - What the prompts (with and without examples) entailed?\n    - How the annotators interacted with the initially generated rubrics, e.g., what were their instructions, how was consensus achieved, and what was the extent of disagreement prior to adjudication?\n    - What was the average rubric quality rating in the final iteration?\n    - How was the best configuration selected, was it based solely on the average score?\n3. The authors have measured the bias of the graders with respect to the human grades. However, when breaking down by model, [1] and [2] show a clear positive bias toward a model's own solutions. Can the authors report similar metrics and discuss the implications?\n4. In what realistic setting can the authors' framework from Section 6 be applied (refer to comments in W2.3)?\n5. When evaluating different provers, the authors have constrained themselves to using a score-based selection system. How does this compare to using a tournament-style approach?\n6. The best-of-n evaluation was done on 29 selected problems. How were these problems selected and what was their difficulty distribution?\n7. The authors claim in 3.2.3 that Staged Evaluation is \"particularly effective for improving the performance of weaker backbone models.\" This is a very strong claim, given that this observation is only seen for the `o4-mini` model. Can the authors consider running additional experiments to support this statement or temper the claim to reflect the limited evidence?\n8. For 2/3 models in 5.2.2, the *Strict* setting yields more accurate judgement than the *Norm* one. Do the authors have any qualitative explanations for this?\n\n## Current rating\n\nI have given this paper a score of **4: Borderline Reject**. The contribution is valid, important, and potentially impactful to the field. However, the lack of transparency on some aspects, particularly reproducibility, prevents me from assigning a higher score. I would be happy to raise my score if the authors address the majority of my concerns during the rebuttal and discussion period.\n\n### References\n\n[1] Dekoninck et al. The open proof corpus: A large-scale study of llm-generated mathematical proofs. arXiv preprint arXiv:2506.21621, 2025.\n\n[2] Petrov et al. Proof or bluff? evaluating llms on 2025 usa math olympiad. arXiv preprint arXiv:2503.21934, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KrqxuRAjeH", "forum": "ky5iqwZSXI", "replyto": "ky5iqwZSXI", "signatures": ["ICLR.cc/2026/Conference/Submission24615/Reviewer_EDhE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24615/Reviewer_EDhE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760972677785, "cdate": 1760972677785, "tmdate": 1762943136978, "mdate": 1762943136978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates the feasibility and effectiveness of LLMs as judges for the continuous-scale grading of natural language solutions to mathematical problems. The authors present a pipeline for 1) automatically generating grading rubrics, 2) manually verifying LLM-generated solutions, and 3) investigating various aspects of an LLM-as-a-judge system. This process resulted in the `ProofBench` dataset of 393 expertly graded solutions, used for evaluating the backbone model and prompting style of an LLM judge. The work also presents `ProofGrader`: the best-performing combination of a judgment ensemble using the o3 model, with a marking scheme and potential solutions as additional reference information. As a proof of concept for `ProofGrader`'s utility, the authors show that the system can be used as a solution selector in a best-of-8 setting, achieving a score that approaches the human oracle's performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The topic of the paper is clearly well-motivated, relevant, timely, impactful, and provides opportunities for further developments in the field\n2. The paper is overall well-written and conveys the high-level idea well.\n3. The authors have investigated several factors affecting the performance of an LLM-as-a-judge, identifying the backbone model and a well-defined marking scheme as the most significant factors for its performance. This provides readers with clear guidance on the information necessary for achieving accurate automatic grading.\n4. This work provides the currently largest dataset of expert-annotated solutions graded on a continuous scale, covering a wide range of challenging recent problems suitable for further LLM-as-a-judge validation.\n5. The best evaluated settings for the LLM-as-a-judge are shown to also correlate with a better performance as a best-of-n selector. This is a very desirable result, showing that the comparisons performed in the analysis are valid on downstream tasks.\n6. The authors provide the prompts necessary for running the LLM-as-a-judge, aiding reproducibility.\n7. The annotator calibration phase is a valuable step for minimizing error and noise in the final dataset."}, "weaknesses": {"value": "For this section, I have labelled each comments as either:\n - *Critical*: These weaknesses have significantly impacted the score, and addressing/not addressing them could positively or negatively affect my final recommendation.\n- *Important*: Addressing these weaknesses would, in my opinion, significantly strengthen the paper's quality and/or clarity.\n- *Minor*: These are nitpicks that, while valuable to address, have not impacted the score of this assessment.\n\n1. Reproducibility\n    - (Critical) The authors claim `ProofBench` as one of their core contributions. However, despite promising to open-source the dataset upon publication, I see no reason why this was not done as part of the supplementary material. Not providing a core contribution of the paper for peer review significantly hinders the process. Upon release, the authors should also ensure they include an appropriate license, given the nature of the source materials.\n    - (Important) The annotation pipeline, described in A.3, is missing a substantial amount of details. Further elaboration can be found in the **Questions** section.\n\n2. Methodology\n    - (Critical) The authors have not reported the reliability of their human grading. They describe having performed double-grading on 20% of the solutions but have not reported any inter-annotator agreement statistics. In particular, they mention that they \"adjudicate all flagged disagreements,\" however, it is not clear what threshold for a disagreement constitutes a flag, how often flags occurred, or how they resolved these inconsistencies. The perceived reliability of the remaining 80% of grades is highly dependent on clarifying this process.\n    - (Critical) Generating rubrics is an incredibly challenging task in practice. This paper involves an automated system that is difficult to verify without significant manual intervention. Details about how this process was refined are lacking. Furthermore, no discussion is presented on how the annotators verified that these rubrics adhered to a standard for high-quality marking schemes. For example, the rubric in Appendix A.6 gives a total of 3 points for initial observations that seem relatively trivial compared to the rest of the proof. While my interpretation of this rubric is a personal opinion, I believe the authors should clarify how they ensured that the rubrics adhere to a high standard of quality, reflecting authentic evaluation practices.\n    - (Important) The authors claim that in Section 6 they investigate the judging framework as a reward model. However, the described setting appears to lack a clear, realistic application. In particular:\n\n        * If used within a solver's selection system, as described in the paper, such applications often aim to solve a problem without access to a reference solution. In that case, the best result seems to be `o3`, at around 3 average points, which is considerably lower than the human oracle's 4.21.\n        * If used as a reward model for training, as proposed in Sections 1 and 7, the presented setups are all too expensive to be scalable to a full RL (or other) pipeline.\n \n      The authors should clarify what a potential use case for this result can be.\n    - (Minor) The paper makes no distinction between results on undergraduate- and high-school-level problems, and whether this is a relevant factor.\n\n3. Dataset\n    - (Important) The dataset consists of problems from 2022-2025 from popular competitions (IMO, USAMO, Putnam). The earlier years precede the knowledge cutoff dates for the models tested. The authors should discuss whether test-set contamination has impacted the results and, if so, how significant the effect is.\n    - (Important) The authors claim to have sourced their problems and solutions from official sources. However, to the best of my knowledge, the USAMO and USATST do not publicly release their competition materials. The authors should clarify this point in their rebuttal.\n\n4. Writing and Clarity\n    - (Minor) The work never explicitly states which configuration constitutes `ProofGrader`. While it can be inferred from the results, it would be best if the authors defined this in the earlier sections of the paper.\n    - (Minor) Most models presented in the paper are not cited according to best practices. Standard practice is to cite the official model cards from the providers, if one exist.\n    - (Minor) In Section 5.3, the authors refer to an \"ensemble\" of evaluators. This term is usually reserved for a collection of **different** models or algorithms applied to the same task, rather than for multiple samples from a single algorithm."}, "questions": {"value": "1. Can the authors address all the concerns listed in the **Weaknesses** section?\n2. The authors describe the marking scheme generation methodology in Section 3.1 and later in Appendix A.3. However, the description lacks sufficient detail. Can the authors clarify:\n    - The model families they tested for rubric generation?\n    - What the prompts (with and without examples) entailed?\n    - How the annotators interacted with the initially generated rubrics, e.g., what were their instructions, how was consensus achieved, and what was the extent of disagreement prior to adjudication?\n    - What was the average rubric quality rating in the final iteration?\n    - How was the best configuration selected, was it based solely on the average score?\n3. The authors have measured the bias of the graders with respect to the human grades. However, when breaking down by model, [1] and [2] show a clear positive bias toward a model's own solutions. Can the authors report similar metrics and discuss the implications?\n4. In what realistic setting can the authors' framework from Section 6 be applied (refer to comments in W2.3)?\n5. When evaluating different provers, the authors have constrained themselves to using a score-based selection system. How does this compare to using a tournament-style approach?\n6. The best-of-n evaluation was done on 29 selected problems. How were these problems selected and what was their difficulty distribution?\n7. The authors claim in 3.2.3 that Staged Evaluation is \"particularly effective for improving the performance of weaker backbone models.\" This is a very strong claim, given that this observation is only seen for the `o4-mini` model. Can the authors consider running additional experiments to support this statement or temper the claim to reflect the limited evidence?\n8. For 2/3 models in 5.2.2, the *Strict* setting yields more accurate judgement than the *Norm* one. Do the authors have any qualitative explanations for this?\n\n## Current rating\n\nI have given this paper a score of **4: Borderline Reject**. The contribution is valid, important, and potentially impactful to the field. However, the lack of transparency on some aspects, particularly reproducibility, prevents me from assigning a higher score. I would be happy to raise my score if the authors address the majority of my concerns during the rebuttal and discussion period.\n\n### References\n\n[1] Dekoninck et al. The open proof corpus: A large-scale study of llm-generated mathematical proofs. arXiv preprint arXiv:2506.21621, 2025.\n\n[2] Petrov et al. Proof or bluff? evaluating llms on 2025 usa math olympiad. arXiv preprint arXiv:2503.21934, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KrqxuRAjeH", "forum": "ky5iqwZSXI", "replyto": "ky5iqwZSXI", "signatures": ["ICLR.cc/2026/Conference/Submission24615/Reviewer_EDhE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24615/Reviewer_EDhE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760972677785, "cdate": 1760972677785, "tmdate": 1763728704870, "mdate": 1763728704870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the critical bottleneck of evaluating natural language math proofs from LLMs with PROOFBENCH, a new expert-annotated dataset of LLM-generated proofs graded on a 0–7 scale using detailed marking schemes. Based on a systematic study of evaluator design, they developed PROOFGRADER, an evaluator that achieves high alignment with human experts. Moreover, they show in a Best-of-N task where PROOFGRADER, as a reward model, closes over 90% of the performance gap between a naive binary evaluator and a human oracle."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper tackles the crucial and timely problem of scalable, reliable proof evaluation.\n2. PROOFBENCH represents a substantial and meticulous human annotation effort.\n3. The BoN experiments demonstrate the effectiveness of the methods."}, "weaknesses": {"value": "1. The paper relies heavily on aggregate metrics (RMSE, etc.). It would be better to include some case studies and error analyses.\n2. The paper mentions MathArena but doesn't adequately compare PROOFBENCH to MathArena's manual annotation efforts.\n3. The backbone models are mainly proprietary models. The analysis would be more comprehensive if it evaluated some open-source models (e.g., Llama and Qwen series) to look into their capabilities as evaluators."}, "questions": {"value": "1. Does the evaluator show a \"self-enhancement bias\"? For instance, does the O3-backbone evaluator systematically over-score proofs generated by O3? A heatmap of bias (Generator vs. Evaluator) would be insightful.\n2. The paper shows that strong models (like O3) are strong evaluators. What about the other dynamics? How well do weak models evaluate strong models, and vice versa? Understanding this is important for understanding the robustness of \"LLM-as-a-judge\" and its potential for \"weak to strong generalization\".\n3. Is there a risk that generator-LLMs could hack the evaluator to get a high score?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H0uHn63MJA", "forum": "ky5iqwZSXI", "replyto": "ky5iqwZSXI", "signatures": ["ICLR.cc/2026/Conference/Submission24615/Reviewer_zjS9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24615/Reviewer_zjS9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761561140875, "cdate": 1761561140875, "tmdate": 1762943136726, "mdate": 1762943136726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response (1/n)"}, "comment": {"value": "We thank all the reviewers for the valuable feedback! We are excited that many strengths of our paper are recognized by reviewers: our paper focuses on a very important, timely and impactful problem (HNs6, LGPu, zjS9, EDhE); the systematic methodology is well-motivated and well-designed (HNs6, LGPu, EDhE); the experimental design is comprehensive and thorough with clear ablation studies that provide actionable insights (HNs6, LGPu, EDhE); the best-of-n experiments effectively demonstrate practical utility (HNs6, LGPu,zjS9, EDhE); and ProofBench provides a valuable resource for the community as the largest expert-annotated dataset with continuous-scale grading (LGPu, zjS9, EDhE).\n\n\nTo address the reviewers' concerns, we have submitted a revised paper incorporating several changes, and shared our dataset as supplementary materials. Below, we first summarize the major revisions and address common questions raised by reviewers. We will provide individual responses to each reviewer's specific concerns.\n\n\n## Revisions\n\n\nWe significantly improved the writing of the paper based on reviewers’ feedback, and respectfully ask the reviewers to take a look. Most concerns have been addressed in the revised version.\n\n\nWe have made the following updates to our dataset, with corresponding adjustments throughout the paper. **Importantly, none of the core claims, conclusions, or overall narrative of our work has changed.**\n- Expanded ProofBench dataset: We have expanded ProofBench to 145 problems with 435 LLM-generated mathematical solutions, now covering all problems from the six competitions across the four-year period mentioned in the paper.\n- Enhanced best-of-n experiments: We have expanded the best-of-n datasets to include 16 responses per problem (double the previous size) and implemented Monte Carlo subsampling for each n (replacing the previous approach of using only the first n responses). This enables evaluation across a larger range of n values and provides unbiased results. Figure 3 and descriptions in Section 4 have been updated to reflect these changes.\n- Increased annotation coverage: We have added more expert graders to strengthen the dataset. The complete annotation process now involves 5 experts: in ProofBench, 41% of proofs are double-graded; in the best-of-n datasets, half of the responses (8 out of 16 per problem) are double-graded. Additional details are provided in Section 2.1 and Appendix A.3 of the revised paper.\n\n\nWe slightly adjust the paper structure for better presentation: section 2, 3, 4 now correspond to our three contributions respectively. The tables and figures are all improved for better presentation.\n\nWe provide our dataset artifacts in an anonymous repository [1].\n\n## Common Questions\n\n> **1. What are the inner-annotator agreement statistics? How do you resolve inconsistencies between annotators? (HNs6, LGPu, EDhE)**\n\nAs mentioned in the earlier paragraph, we have refined our dataset to strengthen annotation quality and coverage. The annotation process is now conducted by a team of five experts. In ProofBench, 41% of proofs are double-graded; in the best-of-n datasets, 50% of responses (8 out of 16 per problem) are double-graded. For double-graded data, we achieve within-1-point agreement (WTA≤1) 87.5%, which is higher than the agreement rates between AI graders and human experts reported in our paper. \n\nTo resolve inconsistencies, experts work collaboratively in supervised sessions monitored by a PhD student and a professor (both co-authors). When annotators differ by >1 point, they discuss their rationale to reach consensus. On the 0–7 scale, scores naturally cluster into four bands: incorrect (0), partial progress (1–3), nearly complete (4–6), and fully correct (7). During discussion, annotators must agree on (1) which band a solution belongs to, and (2) the exact score for fully correct (7) and fully incorrect (0) solutions. For partial-credit cases (1–6), minor differences may initially arise due to inherent ambiguities in natural-language proofs and subjective judgment even with marking scheme guidance, but these are resolved through further discussion. All scores in the final dataset reflect this consensus-based adjudication. Please find more details in section A.3 in the revised paper.\n\n---\n[1] Link to the datasets. https://anonymous.4open.science/r/ProofBench-Supplementary-4DD2."}}, "id": "sX1VKXNHGQ", "forum": "ky5iqwZSXI", "replyto": "ky5iqwZSXI", "signatures": ["ICLR.cc/2026/Conference/Submission24615/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24615/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24615/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763637967977, "cdate": 1763637967977, "tmdate": 1763637967977, "mdate": 1763637967977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the critical bottleneck of evaluating natural language math proofs from LLMs with PROOFBENCH, a new expert-annotated dataset of LLM-generated proofs graded on a 0–7 scale using detailed marking schemes. Based on a systematic study of evaluator design, they developed PROOFGRADER, an evaluator that achieves high alignment with human experts. Moreover, they show in a Best-of-N task where PROOFGRADER, as a reward model, closes over 90% of the performance gap between a naive binary evaluator and a human oracle."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper tackles the crucial and timely problem of scalable, reliable proof evaluation.\n2. PROOFBENCH represents a substantial and meticulous human annotation effort.\n3. The BoN experiments demonstrate the effectiveness of the methods."}, "weaknesses": {"value": "1. The paper relies heavily on aggregate metrics (RMSE, etc.). It would be better to include some case studies and error analyses.\n2. The paper mentions MathArena but doesn't adequately compare PROOFBENCH to MathArena's manual annotation efforts.\n3. The backbone models are mainly proprietary models. The analysis would be more comprehensive if it evaluated some open-source models (e.g., Llama and Qwen series) to look into their capabilities as evaluators."}, "questions": {"value": "1. Does the evaluator show a \"self-enhancement bias\"? For instance, does the O3-backbone evaluator systematically over-score proofs generated by O3? A heatmap of bias (Generator vs. Evaluator) would be insightful.\n2. The paper shows that strong models (like O3) are strong evaluators. What about the other dynamics? How well do weak models evaluate strong models, and vice versa? Understanding this is important for understanding the robustness of \"LLM-as-a-judge\" and its potential for \"weak to strong generalization\".\n3. Is there a risk that generator-LLMs could hack the evaluator to get a high score?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H0uHn63MJA", "forum": "ky5iqwZSXI", "replyto": "ky5iqwZSXI", "signatures": ["ICLR.cc/2026/Conference/Submission24615/Reviewer_zjS9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24615/Reviewer_zjS9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761561140875, "cdate": 1761561140875, "tmdate": 1763717623109, "mdate": 1763717623109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of reliably evaluating natural language mathematical proofs generated by large language models. The authors propose a systematic methodology for developing automated evaluators that assign fine-grained scores on a 0-7 scale, rather than binary correct/incorrect judgments. They introduce PROOFBENCH, an expert-annotated dataset containing 393 LLM-generated solutions to 131 competition math problems, along with problem-specific marking schemes. Through systematic experimentation, they develop PROOFGRADER, an ensemble-based evaluator that achieves strong alignment with expert judgments (RMSE of 1.093) and demonstrates practical utility as a reward model for best-of-n selection tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1 The work tackles an important gap in mathematical proof evaluation with a systematic approach. While building on existing LLM-as-a-judge paradigms, the application to fine-grained mathematical proof evaluation with problem-specific marking schemes is novel.\n2. The experimental design is thorough and methodical. The expert annotation process is well-designed with appropriate quality controls. The systematic ablation studies provide clear insights about what factors matter for evaluator performance.\n3. The paper is well-organized and clearly written. The motivation is compelling, methodology is systematic, and results are presented comprehensively with good visualizations.\n4. Addresses a real bottleneck in mathematical reasoning research. PROOFBENCH provides a valuable resource for the community, and the insights about evaluator design will inform future work. The demonstration of practical utility in best-of-n selection shows real-world applicability."}, "weaknesses": {"value": "1. The dataset contains only 393 solutions across 131 problems from competition mathematics. This is relatively small for drawing broad conclusions about evaluator design, and competition problems may not represent the full spectrum of mathematical reasoning tasks.\n2, The approach relies heavily on LLM-generated marking schemes, which could introduce systematic biases or limitations. The quality of these schemes fundamentally constrains the evaluation quality, but this dependency is not thoroughly analyzed.\n3. All experiments use the same expert annotators and marking scheme generation process. It's unclear how well the findings generalize to different mathematical domains, difficulty levels, or evaluation standards.\n4. The paper doesn't compare against other potential evaluation approaches beyond varying LLM configurations. For instance, how does this approach compare to simpler heuristic methods or other structured evaluation frameworks?"}, "questions": {"value": "1. How sensitive are the results to the quality of the automatically generated marking schemes? Have you conducted experiments with human-written marking schemes for comparison?\n2. How well do these evaluator design principles transfer to other mathematical domains beyond competition problems (e.g., research-level proofs, educational contexts)?\n3. Can you provide more detailed analysis of when and why the evaluator fails? What types of mathematical reasoning or proof structures are most challenging?\n4. What are the computational costs of your best evaluator compared to simpler alternatives? How does this scale with problem complexity?\n5.What is the inter-annotator agreement between your experts, and how does this compare to the agreement between experts and your automated evaluator?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8bR6jZs7qY", "forum": "ky5iqwZSXI", "replyto": "ky5iqwZSXI", "signatures": ["ICLR.cc/2026/Conference/Submission24615/Reviewer_LGPu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24615/Reviewer_LGPu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838173499, "cdate": 1761838173499, "tmdate": 1762943136440, "mdate": 1762943136440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ProofBench, a dataset of 131 competition math problems with 393 expert-graded LLM solutions, and systematically studies design choices for automated proof evaluators. The authors propose ProofGrader, which achieves RMSE of 1.093 against expert scores and demonstrates practical utility in best-of-n selection tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1: The paper tackles an important problem for mathematical reasoning research. Reliable proof evaluation is a critical bottleneck for training and assessing LLMs on mathematical reasoning tasks, and the lack of scalable alternatives to human grading or formal verification makes this work timely and valuable.\n\nS2: The systematic methodology using problem-specific marking schemes as an anchor is well-motivated. The two-stage annotation process (generating marking schemes, then grading with them as guidance) provides a principled way to maintain consistency while allowing flexibility for alternative solution approaches.\n\nS3: The experimental design is comprehensive and well-structured. The ablation studies clearly isolate the impact of different design choices (backbone model, context components, instructions, workflows), providing actionable insights about what matters for evaluator performance.\n\nS4: The best-of-n experiment (Section 6) effectively demonstrates practical utility beyond correlation metrics. Showing that ProofGrader closes 90% of the gap between a naive binary evaluator and the human oracle provides evidence of real-world value for downstream applications like RL training.\n\nS5: The decision to use fine-grained (0-7) rather than binary scoring is validated empirically. Figure 2 clearly shows that binary evaluators fail to distinguish among correct solutions, while fine-grained scoring enables effective ranking."}, "weaknesses": {"value": "W1: ProofBench is only shown to show predictive power (via MSE on the benchmark) over whether an evaluator is good at grading outputs of three specific models (o3, Gemini 2.5 Pro, DeepSeek-R1), all of which are precisely the models whose proofs are used as the annotated examples for computing the MSE. There is no evaluation of how well ProofGrader generalizes to solutions from weaker models, different model families, or human-written proofs, which limits our understanding of its robustness. The paper should have tested whether evaluators with low MSE on ProofBench have high agreement with human annotators on grading proofs generated by models that are not one of those three models, in order to demonstrate general utility.\n\nW2: Inter-annotator reliability is insufficiently reported. While the paper mentions that two experts underwent calibration and double-scored 20% of items, the actual agreement metrics (e.g., correlation, exact agreement rate, within-1 agreement) are not provided. Since the significance of the contributions hinges on the correctness of the human annotations, this needs to be emphasized in the main text.\n\nW3: Error analysis is absent. The paper does not systematically examine where ProofGrader fails or succeeds, what types of errors it makes (over-crediting vs. under-crediting), or which problem types are most challenging. Understanding failure modes would be valuable for future work and practical deployment.\n\nW4: Computational costs and efficiency are not discussed. For practical deployment, especially in RL training scenarios where evaluators may need to score thousands of candidate solutions, the cost (in terms of API calls, latency, and financial expense) compared to simpler baselines would be relevant information."}, "questions": {"value": "Q1. Since the best-of-n experiments are supposed to establish the predictive generalization power of ProofBench for use as a reward model, shouldn’t the ground truth annotations for the proofs used in the experiment be annotated by different people than those used to annotate the examples in ProofBench? A difficulty in designing a benchmark for evaluators is that the bias in the annotations themselves need to be accounted for, i.e, the paper needs to demonstrate that the possibly imperfect annotator preferences used in ProofBench are sufficient for generalization to general aesthetic preferences in the math community (regarding competition style proofs).\n\nQ2. Similarly to Q1, in the best-of-n experiments, shouldn't the set of model generations being evaluated by the candidate evaluators be generated by models that are different to the three specific models used to generate the outputs in the benchmark? Could the authors provide justification why the current methodology is sufficient to demonstrate the general utility of ProofBench for selecting evaluators meant to evaluate generations from other models? Currently, it seems like ProofBench is useful for selecting evaluators for proofs generated by o3, Gemini, and DeepSeek-R1, but no indication of general ability.\n\nQ3. This approach seems very committal to a specific frozen mark scheme. Since the labor intensive annotation process is predicated on a particular MS, what happens if the mark scheme needs to be changed? Do we need to re-annotate each time? Could the authors provide justification why using a particular frozen mark scheme for the annotation process is sufficiently general to avoid re-annotation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TAq6T2iuEB", "forum": "ky5iqwZSXI", "replyto": "ky5iqwZSXI", "signatures": ["ICLR.cc/2026/Conference/Submission24615/Reviewer_HNs6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24615/Reviewer_HNs6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994751901, "cdate": 1761994751901, "tmdate": 1762943136189, "mdate": 1762943136189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}