{"id": "0ST7MDmzPL", "number": 3439, "cdate": 1757427106079, "mdate": 1759898089861, "content": {"title": "Manifold Learning via Data Topology Optimization: Gradient Method Application", "abstract": "The study of topological properties in data and their application to machine learning is a growing research area. While most methods operate in Euclidean space, alternative topologies (e.g., hyperbolic embeddings for recommender systems) often yield superior performance. However, real-world data sets lack a known intrinsic topology, which requires manual specification. We propose a novel method for inferring the underlying topological structure through joint optimization of a learnable distance matrix and embedding. Our approach combines the learning of neural networks with a differentiable Isomap implementation, enabling end-to-end optimization of both the metric and mapping. Experiments on synthetic non-Euclidean datasets demonstrate accurate topology recovery, suggesting broader applicability to real-world problems with unknown geometric structure, a claim we preliminarily validate on the MNIST dataset.", "tldr": "Paper propose approach for intrinsic data topolody search with end-to-end differentiable Isomap implementation and obtained manifold usage for out-of sample mapping.", "keywords": ["manifold learning", "gradient optimization", "Isomap", "topology search"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87e663ee3c7ca2a8e5cf19403e4f563381b12361.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a data topology optimized manifold learning method that jointly optimizes a learnable distance matrix via gradient-based techniques to infer the intrinsic topological structure of data. Unlike traditional methods (e.g., Isomap), which rely on predefined geometric assumptions such as Euclidean or hyperbolic spaces, the proposed approach directly optimizes the global geometry of the data through task-driven objective functions (e.g., classification or regression loss), thereby eliminating the need for manually specifying the topology. By integrating neural networks with a differentiable Isomap algorithm, the method achieves end-to-end optimization. Experimental results demonstrate that the proposed approach accurately recovers topological structures on synthetic non-Euclidean datasets and validates its applicability to real-world problems using the MNIST dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This is achieved through differentiable shortest-path calculation (e.g., an improved Floyd-Warshall algorithm) and differentiable MDS, enabling gradient backpropagation.\n2. This work achieves the gradient-based optimization of Isomap, addressing the optimization challenges caused by non-differentiable operations.\n3. A novel framework for end-to-end differentiable topology learning is proposed. It combines the dimensionality estimation with a differentiable Isomap algorithm, enabling joint optimization of neural networks and manifold mapping."}, "weaknesses": {"value": "1. The distance matrix is optimized solely based on labels, which results in a lack of direct association between the distance matrix and the original data.\n2. The paper lacks certain theoretical analysis. Although it mentions that the proposed algorithm can be described through Ricci flow process, it does not provide a mathematical proof of their equivalence. Additionally, there is a lack of theoretical guarantees regarding the quality of the embeddings."}, "questions": {"value": "1. How should the learned distance matrix be interpreted, and how can it be related back to the original data?\n2. In Section B.1, gradients are computed using a custom backpropagation implementation of the Floyd-Warshall algorithm. How can numerical stability of the gradients be ensured? Are there risks of vanishing or exploding gradients?\n3. The proposed method is a neural-network-based optimizable approach, but it has not been compared with methods such as UMAP, VAE, or hyperbolic embeddings. Were these baselines considered? If the performance advantages are limited, how can the unique contribution of this method be justified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FyAqr6PUxS", "forum": "0ST7MDmzPL", "replyto": "0ST7MDmzPL", "signatures": ["ICLR.cc/2026/Conference/Submission3439/Reviewer_E612"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3439/Reviewer_E612"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761292612213, "cdate": 1761292612213, "tmdate": 1762916724241, "mdate": 1762916724241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a differentiable version of Isomap that enables end-to-end learning of a manifold’s\nintrinsic geometry jointly with a downstream objective. The authors introduce a learnable distance\nmatrix, use a differentiable approximation of the Floyd–Warshall shortest-path algorithm and multidimensional\nscaling, and train this jointly with a reconstruction or classification loss. The paper claims\nthis allows recovery of intrinsic manifold structure and better downstream performance. Experiments\nare presented on synthetic datasets and MNIST for reconstruction of manifold geometry."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The problem of non-differentiability in manifold learning is important, and the paper addresses a meaningful\ngap between geometric methods and gradient-based optimization. Unifying manifold learning\nwith differentiable training pipelines is a promising direction, particularly for integrating geometric\npriors into neural architectures. The idea of a differentiable Isomap pipeline is conceptually interesting,\nand if made computationally stable and scalable, it could help bridge geometric learning and deep\nrepresentation learning communities."}, "weaknesses": {"value": "The paper is difficult to follow, not well structured, and contains abrupt logical transitions. The\nconnection between the mathematical formulation and the experimental implementation is unclear.\nThe algorithm’s structure, especially regarding differentiability, is not clearly described. Table 1 and\nFigure 7 appear to present the same results in different forms.\n\nThe paper asserts differentiability through discrete shortest-path computations (via Floyd–Warshall)\nbut provides no derivation or proof that the gradients are correct. Using indicator functions in the\nbackward pass suggests a straight-through approximation that is likely unstable. No gradient-check\nresults are provided. The overall pipeline is not truly “end-to-end differentiable,” since inference relies\non non-differentiable approximators (KNN, Random Forest), breaking the differentiability chain\nclaimed.\n\nThe use of local PCA for dimensionality estimation (in Figure 3 and Section 3.3 for MNIST) contradicts\nthe paper’s core claims. The pipeline cannot be considered fully “automated” or “differentiable”\nif a separate, non-differentiable algorithm is used to determine a key hyperparameter (the intrinsic\ndimension d). Moreover, the estimated intrinsic dimension of 482 for MNIST is unusually high and\nlikely reflects overfitting.\n\nThe experimental setup is weak. Only simple synthetic and MNIST datasets are used, which are\ntoo limited to support the generality claimed. The synthetic experiments are circular: the target\nfor learning is defined as the same intrinsic coordinates used to generate the data, which guarantees\nsuccess by construction and does not demonstrate true geometry discovery. The MNIST experiment\nlacks critical details about what regression and classification tasks were used, making it impossible to\nassess the reported results.\n\nBaseline comparisons are insufficient. The authors only compare against PCA and raw features.\nStronger baselines such as UMAP, t-SNE, or standard neural networks should be included to establish\nwhether the method is competitive. Similarly, there is no qualitative visualization of the learned\nmanifolds, which is standard practice for evaluating topology recovery methods.\n\nWhile the authors claim novelty in learning geometry via a differentiable embedding pipeline, prior\nworks (e.g., Liu et al. (2019); Tong et al. (2021); Lim et al. (2024)) have already explored differentiable\ndistance learning, manifold-aware autoencoders, and geometry-preserving embeddings. The paper does\nnot discuss or cite these related studies, weakening its originality claim.\n\nComputational costs are prohibitive for practical use. Training on 2000 MNIST samples takes 6.5\nhours, and inference on 60 000 samples takes 40 minutes, with costs that scale exponentially with the\nintrinsic dimensionality."}, "questions": {"value": "The training of the method shows clear instability, with “loss spikes” that required them to add “ad\nhoc weights perturbation” to fix. This suggests a fundamental problem with the learning process. How\ndoes the method’s approach to gradient calculation through the Floyd-Warshall algorithm contribute\nto this instability, rather than providing a smooth learning dynamic? Furthermore, what has been\ndone to prove that the gradient is calculated correctly through the custom shortest-path algorithm?\n\nDoes the learned distance matrix satisfy symmetry and triangle inequality? If not, how does that\naffect the geometry and embedding stability?\n\nHow sensitive is the performance to the intrinsic dimension estimate (e.g., d = 482 for MNIST)?\n\nIs the goal to improve reconstruction or downstream performance? The paper sometimes presents\nit as geometry recovery, other times as improvement on downstream task. Please clarify.\n\nThe paper strongly motivates the method as “task-driven” and claims that a good geometry is\none that maximizes downstream performance. What is the support for this claim? And is there an\nexplanation of how the downstream task actually guides the discovery of the geometry?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H0OKaQglQh", "forum": "0ST7MDmzPL", "replyto": "0ST7MDmzPL", "signatures": ["ICLR.cc/2026/Conference/Submission3439/Reviewer_7fPG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3439/Reviewer_7fPG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852242649, "cdate": 1761852242649, "tmdate": 1762916724062, "mdate": 1762916724062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method for inferring data geometry via joint optimization of a parametrized distance matrix and embedding, combining neural networks with a differentiable Isomap-based pipeline."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The studied research problem is timely and interesting\n2. Various synthetic manifolds were generated and tested"}, "weaknesses": {"value": "1. Throughout the manuscript, the terms “manifold learning,” “topology learning,” and “geometry learning” are intertwined and conflated. It undermines the conceptual precision necessary for a technical contribution in this area. Could the authors clarify the correct usage and relationships between these terms (especially since each has distinct literature and associated methodology)?\n2. The authors build their pipeline around Isomap, but do not provide sufficient justification for this choice over other classic and modern manifold learning techniques\n3. The paper reads more like preliminary notes than a complete paper. The writing requires further polishing\n4. All citations are formatted as inline, which does not follow conventional conference style (\\citep should be used non-inline).\n5. The rationale behind citation selection is unclear. For example, Fitz (2022) and Fitz et al. (2024) are unlikely to be established references for the claim that “hyperbolic geometry has proven powerful in representing hierarchical structures.” A much more relevant citation would be Nickel et al.’s work on Poincaré embeddings. Additionally, references to kernel ridge regression should cite core foundational work, not recent sources. \n6. The paper repeatedly highlights hyperbolic geometry in the Introduction and Practical applications and further use, which contradicts the framing as geometry-agnostic. This comes across as overemphasized and potentially misleading.\n7. Distance matrix parameterization is underspecified. How exactly is the distance matrix produced/parameterized? Are distances constrained to satisfy metric axioms (nonnegativity, symmetry, triangle inequality)? \n8. The text claims “various losses” are possible, yet experiments appear to use MSE/RMSE and evaluate convergence.\n9. The current experiments are limited to synthetic data and MNIST, which do not convincingly demonstrate utility on real scientific/practical datasets\n10. There is no reference when first mentioning UMAP\n11. \\phi_k in Eq. (1) is not defined \n12. “Neural network architecture” is referenced repeatedly without specification. \n13. Impossible to follow Figure 3, unclear what message it wants to convey \n14. Missing related work: diffusion maps, Laplacian Eigenmaps, LLE/HLLE, t-SNE/PaCMAP/TriMap\n15. The introduction makes broad claims without citations, e.g., that prior methods “presuppose a specific geometry” or rely on strong priors, and that defining a “good” geometry is traditionally unsupervised via geodesic preservation. Please support these with references.\n16. Figure 1 is unclear and hard to follow. What do the coordinates represent? What do the x-axis and y-axis represent in the polar coordinates figure? What do the values here represent?\n17. The paper does not provide sufficient background and related work introduction"}, "questions": {"value": "1. How is the proposed method compared to \"Is Distance Matrix Enough for Geometric Deep Learning?\n2. What is topological preservation?\n3. How does it perform compared to LDLE: Low Distortion Local Eigenmaps? \n4. In Eq. (1) and (2) - phi^* and f* are obtained by min or argmin? \n5. Training loss at line 231:  could the authors specify MSE between what quantities (pairwise distances vs. geodesic distances vs. reconstruction error)?\n6. For the MNIST dataset, why not use classification loss? \n7. How does the proposed method perform for manifolds with hole(s)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n88B9seHxm", "forum": "0ST7MDmzPL", "replyto": "0ST7MDmzPL", "signatures": ["ICLR.cc/2026/Conference/Submission3439/Reviewer_WHiQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3439/Reviewer_WHiQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919519407, "cdate": 1761919519407, "tmdate": 1762916723876, "mdate": 1762916723876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a differentiable manifold learning framework inspired by the classical Isomap algorithm. The authors aim to “learn” the geometry of data rather than assume a fixed metric, drawing heavily on ideas from Isomap and Multidimensional Scaling (MDS). The proposed method learns a parameterized distance matrix such that, after differentiable geodesic computation and MDS embedding, the resulting distance-preserving representation can be optimized for a downstream task through backpropagation. Experiments are performed mainly on synthetic datasets and MNIST, focusing on convergence behavior and feasibility."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper explores an interesting conceptual direction—recasting a well-known geometric algorithm into a differentiable, learnable pipeline that could, in principle, make manifold learning compatible with modern neural training.\n\n- The experiments provide a proof-of-concept demonstration that the idea is computationally viable and can recover known intrinsic geometries on synthetic data."}, "weaknesses": {"value": "- The empirical evaluation is narrow: there is no comparison to state-of-the-art or realistic benchmarks (e.g., LLM embeddings, image, or shape representations) that would demonstrate the method’s usefulness in challenging settings.\n\n- Conceptually, several aspects remain unclear. The core pipeline on page 3 (the five-step differentiable Isomap) needs better exposition:\n\n- What is the architecture of the optimization module?\n\n- Why is the distance matrix optimized both before and after k-NN graph construction?\n\n- Would it not be simpler or more stable to learn features directly instead of pairwise distances?\n\n- It is also unclear how this formulation relates quantitatively to standard Isomap—e.g., does it yield improved out-of-sample extension accuracy or robustness?\n\n- The authors should discuss whether the method is compatible with known Isomap variants such as Landmark Isomap or Subspace Least Squares MDS , which are crucial for scalability. See De Silva, Vin, and Joshua B. Tenenbaum. Sparse multidimensional scaling using landmark points. Vol. 120. technical report, Stanford University, 2004. and Boyarski, Amit, Alex M. Bronstein, and Michael M. Bronstein. \"Subspace least squares multidimensional scaling.\" International Conference on Scale Space and Variational Methods in Computer Vision. Cham: Springer International Publishing, 2017."}, "questions": {"value": "Overall, this submission revisits a classical algorithm through the lens of differentiable optimization and offers an interesting conceptual experiment. However, the current evaluation remains preliminary and does not convincingly demonstrate either methodological clarity or practical advantage over existing approaches. At present, the contribution seems better suited as an exploratory workshop paper rather than a full ICLR main-track submission."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qFu3aBoK6E", "forum": "0ST7MDmzPL", "replyto": "0ST7MDmzPL", "signatures": ["ICLR.cc/2026/Conference/Submission3439/Reviewer_9CAd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3439/Reviewer_9CAd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3439/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947562700, "cdate": 1761947562700, "tmdate": 1762916723674, "mdate": 1762916723674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}