{"id": "Jti8ZbC7kM", "number": 16507, "cdate": 1758265343059, "mdate": 1759897236431, "content": {"title": "Primary-Dual Diffusion Solver for Quadratic Programming Problems", "abstract": "Quadratic Programming (QP) is an important class of mathematical optimization problems widely used in various fields such as economics, engineering, finance, and machine learning. Recently, with the development of Learning to Optimize , many studies have attempted to solve QP problems using Graph Neural Networks (GNNs), but they suffer from relatively poor performance compared to traditional algorithms. In this paper, we introduce the Primary-Dual Diffusion (PDD) model for solving QP problems. The model uses a diffusion approach to simultaneously learn both primary and dual variables in order to predict an accurate solution. Based on this prediction, only a small number of KKT-based correction and  parallelizable post-processing iterations (e.g, PDHG, ADMM) are needed to ensure that the solution satisfies the constraints and converges to the optimal solution. Notably, our PDDQP is the first QP neural solver capable of obtaining the optimal solution. Additionally, to address the slow convergence issue of the diffusion model, we adopt a consistency distillation method to develop a one-step diffusion solver for QP. Experimental results demonstrate that our approach achieves state-of-the-art performance in learning-based QP solvers while remaining competitive with traditional methods.", "tldr": "A diffusion model simultaneously outputs both the primal and dual variables of a quadratic programming (QP) problem, combined with a posterior refinement algorithm.", "keywords": ["Primary-Dual Diffusion; Quadratic Programming; Consistency Distillation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0eb5f7159f6a32fea2c99de22c7f30bab63e779f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a PDD framework for QP, generating primal and dual variables with a DDPM teacher and distilling a one-step student via consistency training while enforcing feasibility through KKT-guided sampling. The final solution is then obtained by several iterations of classical refinements, such as ADMM or PDHG."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The design of the PDD structure aligns the learned initializer with downstream optimality conditions and provides a principled handle to reduce projection burdens during refinement. \n2. The one-step student offers a latency reduction while preserving numerical priors learned by the teacher. This may help to decrease the number of ADMM/PDHG steps needed in the final step."}, "weaknesses": {"value": "1. The empirical evidence is confined to synthetic QP generators and moderate scales, so the claimed ‚Äúgrowing advantage at higher dimensions‚Äù is insufficiently substantiated for large-scale QP problems. \n2. The method strongly relies on post-refinement, and the statements about ‚Äúobtaining the optimal solution‚Äù are not backed by standalone guarantees for the learned component nor by ablations that separate the contributions of teacher sampling, KKT guidance, and classical iterations. In general, there is no condition where the diffusion initializer alone reliably reaches near-optimality, and no clean ablations separating the learned part from refiners.\n3. No convergence, feasibility, or stability analysis is provided. KKT guidance in this structure seems a heuristic sampling of energy rather than a guarantee. \n4. The authors do not report training cost, memory footprint, inference latency, and required refinement steps. They also do not provide end-to-end elapsed time comparisons at matched accuracy with warm-started solvers. Including training and maintenance may erode any claimed speedups.\n5. There is no systematic breakdown of gains from primal‚Äìdual joint generation, KKT guidance, consistency distillation, and the GNN backbone. Improvements could largely stem from the classical refinements, leaving the diffusion stage‚Äôs true contribution unclear."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nY7AQZ8wvx", "forum": "Jti8ZbC7kM", "replyto": "Jti8ZbC7kM", "signatures": ["ICLR.cc/2026/Conference/Submission16507/Reviewer_ZMb3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16507/Reviewer_ZMb3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907130255, "cdate": 1761907130255, "tmdate": 1762926601468, "mdate": 1762926601468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a diffusion-based neural QP solver that jointly predicts primal and dual variables, distills to a one-step model, and applies KKT-based refinement. Claims include both high accuracy and competitive runtime vs traditional solvers."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Joint primal‚Äìdual learning. Conceptually aligned with KKT structure in QP. \n2. Attempts to combine learning with classical optimization. \n3. Use of diffusion-based generative modeling in a continuous constrained optimization setting may be novel."}, "weaknesses": {"value": "Motivation vs. chosen architecture mismatch. The paper frames speed as a core motivation, yet diffusion inference is inherently multi-step and computationally heavy. Distillation is proposed but lacks rigorous evidence of preserving solution quality under reduced steps. \nDistillation motivation and roles unclear. Paper does not articulate why a teacher diffusion model is needed, nor provide ablations showing student-only training fails. It remains unclear what optimization knowledge is actually distilled."}, "questions": {"value": "Experimental design insufficient to justify claims. Experiments are small-scale toy QPs (‚â§500 variables). Wall-clock timing comparisons are not controlled or fair because performance gaps fall within configuration variability. \n\nProblem formulation and guidance contain non-differentiable terms. The ‚ÄúKKT Guidance‚Äù uses max(¬∑) in (11), making gradients undefined at zero. This contradicts diffusion‚Äôs requirement for smooth gradients. No workaround or theoretical analysis is provided. \n\nNotation errors and undefined symbols. Multiple symbols appear without definition (e.g., PDDDP, \\theta). Several repeated sentences and formatting errors throughout the manuscript indicate immaturity of the submission. \n\nIn Appendix D, the PDHG method of (18) and the ALM method of (19) cannot handle inequality constraints where x>=0 in the proplem (1)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ehboAuFoWS", "forum": "Jti8ZbC7kM", "replyto": "Jti8ZbC7kM", "signatures": ["ICLR.cc/2026/Conference/Submission16507/Reviewer_e9n5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16507/Reviewer_e9n5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915834902, "cdate": 1761915834902, "tmdate": 1762926600963, "mdate": 1762926600963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Primary‚ÄìDual Diffusion (PDD) framework for convex quadratic programs (QP). The authors claim the approach attains near-optimal solutions with only a few correction iterations and scales favorably versus commercial solvers at higher dimensions. Experiments are reported on synthetic QP instances with metrics including relative objective gap, primal/dual residuals, and wall-clock time."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Predicting solutions aligns with KKT structure and gives good warm starts for classical methods. The architectural choice (bipartite GNN over variable/constraint nodes) is well-motivated.\n\nA clear distillation recipe for fast inference; the paper argues why other one-step generative variants underperform for numerically sensitive QP."}, "weaknesses": {"value": "The core ingredients‚Äîdiffusion for solution generation, a GNN over primal/dual factor graphs, and a KKT-residual (feasibility) loss‚Äîare each well-established in adjacent literature. As presented, the paper‚Äôs contribution seems more like a careful composition/tuning of known pieces rather than a clearly new principle. The manuscript would benefit from a sharper positioning of what is genuinely novel (algorithmically or theoretically) versus what is inherited."}, "questions": {"value": "How does the model behave under distribution shift in (Q,A,b,c) especially sparsity patterns and conditioning of ùëÑ"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w9QaBjWh4i", "forum": "Jti8ZbC7kM", "replyto": "Jti8ZbC7kM", "signatures": ["ICLR.cc/2026/Conference/Submission16507/Reviewer_vckv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16507/Reviewer_vckv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056622648, "cdate": 1762056622648, "tmdate": 1762926599694, "mdate": 1762926599694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a diffusion-based model for solving quadratic programming (QP) problems that jointly learns both primal and dual variables. The core method involves training a one-step solver through consistency distillation from a multi-step teacher model, with KKT-based energy guidance to encourage feasibility during sampling. Experimental results show promise compared against GNN baselines and standard optimization solvers, although experiments are limited."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of the paper can be listed as follows: \n\n1. This work studies an important problem with broad impact, as finding fast solutions to QP problems can benefit many application domains. \n\n2. The direction of integrating diffusion models into learning-to-optimize is interesting, and especially the idea of learning both primal and dual variables jointly through diffusion has not been explored before. \n\n3. The KKT guidance is a smart domain-specific application of energy-based guidance that directly incorporates optimality conditions into the diffusion sampling process. \n\n4. Some promising results are shown comparing against GNNs and standard optimization solvers, though experiments are limited."}, "weaknesses": {"value": "My major concerns for this paper are listed as follows: \n\n1. **Inadequate positioning within the learning-to-optimize literature for QP.** The paper's framing in the abstract, introduction and experiments, focuses almost exclusively on GNN-based end-to-end prediction methods, misleading readers about other relevant baselines in learning-to-optimize for QP. For example, the recent work in [R1] unfolds the ADMM-based OSQP solver (state-of-the-art in QP) [R2] and learns its hyperparameters for solving specific classes of problems faster than classical solvers. The DeepQP framework presented in [R3] (cited in present paper, but the authors only refer to the distributed version) goes further by learning open-loop or closed-loop policies for the parameters to enhance their adaptability across different problems. Another example is [R4], which uses MLP-based prediction followed by fixed OSQP iterations in an end-to-end learning fashion.  The authors should either provide experimental comparisons or substantively discuss how their diffusion-based approach relates to this line of work. \n\n2. **Missing recent work on diffusion-based models for constrained optimization.** The paper claims novelty in applying diffusion models to continuous optimization problems with constraints, but recent work [R5] has already explored diffusion-based models for constrained optimization. The authors should better clarify their contribution relative to existing diffusion-based constrained optimization methods. \n\n3. **Inaccurate claim about achieving optimal solutions.** The claim ‚ÄúNotably, our PDDQP is the first QP neural solver capable of obtaining the optimal solution.‚Äù in the abstract and contributions is fundamentally misleading. This doesn‚Äôt follow from any guarantees that accompany the diffusion-based model but only from the fact that corrector steps are simply added after the inference of the diffusion model. This post-hoc refinement strategy is standard practice in learning-to-optimize literature, not a novel contribution. See for example, page 2 of [R1] ‚ÄúIf run for enough iterations, the iterates are guaranteed to converge to an optimal solution due to the inclusion of the steady-state phase in our architecture.‚Äù or then page 6 in the same paper for more details. \n\n4. **Unsupported claim about inferior performance of other generative models.** The authors claim in lines 69-71 and 188-191 that they have ‚Äúexperimented with various models such as self-training consistency models and flow matching, yet their performance is inferior‚Äù. However, no experimental results are presented to substantiate this claim. This omission is problematic because understanding why alternative generative approaches fail is essential for evaluating the necessity and contribution of the proposed method. \n\n5. **Limited problems and scale in experiments.** Results are only shown for randomly generated problems that are not inspired by any real-world application. In addition, the problems are of a relatively small scale. \n\n6. The contributions are written in an overly verbose manner, making it hard for the reader to identify the actual intellectual merit of this paper. \n\nSome additional minor comments: \n\n1. The paper has a significant amount of typos or grammar/syntax errors. Examples include lines 13, 87, 124, 128, 132, 157, 188, 243 (\"your\"), and others. \n\n2. The title and several places in the paper use \"primary\" instead of \"primal\" (the standard term in optimization). \n\n3. Section C is the Appendix is empty (no text). At minimum, there should be text pointing to Algorithm 2 or explaining the iterative KKT corrector method. \n\n4. In section D of the Appendix, there is an indicator function for the constraint $Ax=b$ missing in the augmented Lagrangian. In addition, the phrasing ‚Äúthe augmented Lagrangian for the consensus constraint‚Äù is also a bit inaccurate as this is the AL for problem (17) in general. \n\n[R1] Sambharya, R., & Stellato, B. (2024). Learning algorithm hyperparameters for fast parametric convex optimization. arXiv preprint arXiv:2411.15717. \n\n[R2] Stellato, Bartolomeo, et al. \"OSQP: An operator splitting solver for quadratic programs.\" Mathematical Programming Computation 12.4 (2020): 637-672. \n\n[R3] Saravanos, Augustinos D., et al. \"Deep Distributed Optimization for Large-Scale Quadratic Programming.\" The Thirteenth International Conference on Learning Representations. \n\n[R4] Sambharya, Rajiv, et al. \"Learning to warm-start fixed-point optimization algorithms.\" Journal of Machine Learning Research 25.166 (2024): 1-46. \n\n[R5] Ding, Shutong, et al. \"Exploring the Boundary of Diffusion-based Methods for Solving Constrained Optimization.\" arXiv preprint arXiv:2502.10330 (2025)."}, "questions": {"value": "Some questions for better clarifying the contributions of this work:\n\n1. Can you provide some discussion of what the main novelties of this work are compared to the recent diffusion-based constrained optimization framework in [R5]? \n\n2. Can you provide more extensive experiments on standard QP problems that are relevant to real-world applications (for example, model predictive control, portfolio optimization, etc.)?  \n\n3. Can you provide some ablation study on what is the effect of the KKT guidance? \n\n4. What happens when problem structure changes significantly from the training distribution? For example: different sparsity patterns, larger/smaller scale, different constraint/variable ratios, or different conditioning of Q? \n\n5. Can you provide more insights into how much training time does the proposed framework require? \n\n6. Can you provide experimental evidence for the claim that flow Matching and self-trained consistency models fail for QP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XDN4KEO8M2", "forum": "Jti8ZbC7kM", "replyto": "Jti8ZbC7kM", "signatures": ["ICLR.cc/2026/Conference/Submission16507/Reviewer_hRiP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16507/Reviewer_hRiP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762853784897, "cdate": 1762853784897, "tmdate": 1762926598980, "mdate": 1762926598980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}