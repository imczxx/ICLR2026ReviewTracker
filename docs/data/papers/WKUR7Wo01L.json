{"id": "WKUR7Wo01L", "number": 17883, "cdate": 1758281631802, "mdate": 1759897147835, "content": {"title": "Listening to the Wise Few: Query–Key Alignment Unlocks Latent Correct Answers in Large Language Models", "abstract": "Language models often struggle with multiple-choice question answering (MCQA) tasks when they fail to consistently choose the correct letter corresponding to the right answer. We find that while certain attention heads within large language models (LLMs) identify the right answer internally, this information can be lost before the final decision stage of what to output. To demonstrate and measure this effect, we introduce the QK-score, a metric based on query- and key-vectors alignment, that retrieves the correct answer directly from individual attention heads. This allows us to identify \"select-and-copy\" heads, that consistently focus on the correct option during inference. Across four standard MCQA benchmarks (MMLU, CosmosQA, HellaSwag, and HaluDialogue), QK-score from such heads can be better than the model’s own output by up to 16% in terms of accuracy, especially for smaller models. On a synthetic dataset, these heads may outperform the baseline by as much as 60%. We also find that QK-scores from \"select-and-copy\" heads are robust to option permutations and remain effective in few-shot settings. After analyzing a wide range of models across the LLaMA, Qwen, and other families, with 1.5B to 70B parameters, we observe the select-and-copy phenomenon in all of them. Our findings offer new insights into the inner workings of LLMs and open a principled path toward head-level interventions for controllable and trustworthy LLM reasoning.", "tldr": "Query–Key score reveals “select-and-copy” heads whose strongest Q–K match pinpoints the right choice: boosts MCQA accuracy up to +16 pp (60 pp synthetic) across diverse LLMs—training-free, interpretable, model-agnostic.", "keywords": ["large language models", "attention mechanisms", "interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ed10c90a190a9c07b1124e8fc7427bfcf39918e.pdf", "supplementary_material": "/attachment/7410d87e9f58342c9ce355589bf3df6967004dca.zip"}, "replies": [{"content": {"summary": {"value": "The paper offers a simple, principled readout (QK-score) that surfaces a real and interesting phenomenon: mid-layer “select-and-copy” heads often contain the correct MCQA choice even when the decoder output fails. The evidence spans multiple datasets, models, and ablations, which is a meaningful contribution and likely to spark follow-up work in interpretability and evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Mechanistic evidence: by emphasizing query–key alignment (QK) rather than attention weights or hidden-state probes, the paper introduces a QK-score metric that reveals select-and-copy behavior.\n\n- Cross settings experiments: extensive results across different models, languages, option counts, zero-/few-shot, and several option-representative tokens; includes robustness tests via option permutation, added “I don’t know/None of the above,” and zero ablation.\n\n- Implications for evaluation: QK-score serves both as a diagnostic readout to locate layer-wise information loss and as a practical auxiliary readout for reliable multiple-choice scoring."}, "weaknesses": {"value": "- Insufficient evidence beyond MCQA: core claims center on MCQA; extrapolation to generative QA, long-form reasoning, and tool use remains untested.\n\n- Dependence on option-representative tokens: the method assumes stable anchors for options, automatic discovery or robust definition under freer, unstructured formats is unclear.\n\n- Head selection coupled with the dataset: stability without a validation set or across domains is uncertain, selecting heads on the synthetic set degrades on some real tasks, indicating distribution sensitivity."}, "questions": {"value": "1. Head selection appears post hoc. The selection criteria rely on downstream experimental results rather than an a priori, interpretability-grounded theory, which makes the method largely heuristic. Please pre-register head-selection rules and validate them out-of-domain to rule out selection bias.\n\n2. Exposition and organization: Understanding the core contribution requires frequent back-referencing (e.g., approach in Section 4 depends on results in Section 7). I suggest restructuring so that selection principles, diagnostics, and evidence appear in a single forward-reading pipeline, to minimize cross-references.\n\n3. Model coverage and consistency: I appreciate the many experiments across model sizes, but most results concentrate on the Llama family. In some non-Llama models, the proposed method shows no clear advantage, and coverage is uneven (e.g., some sections report Qwen but omit Phi and other models). I suggest standardizing model selection and ensuring comparable setups and metrics across families.\n\n4. Minor: Missing reference at line 1722—“Figure ?? contains results…”. \n\n5. Add a reproducibility or ethics statement section refer to the author guide."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZFkCr1dH00", "forum": "WKUR7Wo01L", "replyto": "WKUR7Wo01L", "signatures": ["ICLR.cc/2026/Conference/Submission17883/Reviewer_yKm9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17883/Reviewer_yKm9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980064451, "cdate": 1761980064451, "tmdate": 1762927710252, "mdate": 1762927710252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors use an approximation of the attention mechanism (select-and-copy) as the basis of a white box based intervention that improves MCQA accuracy. The main result is based on the observation the select-and-copy heads can be used in place of token outputs, which allows for an alternative means of accessing a model's \"knowledge\""}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "# originality\n\nThe select-and-copy head is a novel idea, but that is impart because it depends on the validity of the approximation (line 140) to be useful. That they find a set of benchmarks where it occurs suggest the approximation may be valid\n\n#quality\n\nThe results are on a limited set of benchmarks and only touch on a single task MCQA. The results are reasonable, although another pass would help improve the clarity.\n\n# clarity\n\nI find the top level introduction unclear, the abstract and how exactly the method improves performance is not discussed much until line 178, so I was wondering how all this maps to the MCQA answers the whole time. I also found figure 3 difficult to read as all the lines overlap. \n# significance\n\nThis relies on a task specific fine tuning like process, so I'm not sure how well the method will generalize. Also, the approximation is nver formally discussed so I'm not sure if it can be used for other tasks."}, "weaknesses": {"value": "My main issue with the work is that it presents the \"select-and-copy\" heads as a major contribution, but does not attempt to quantify how they occur or if they are a significant phenomena in general. Additionally the Query-Key metric appears to be basically a type of fine tuning, in that it produces a non-linear transformation of the model's outputs based on a small training set. Comparing to a baseline fine tune or explaining how this is more data efficient would help"}, "questions": {"value": "Can you explain how this method works when deployed, how do you pick the answer to a MCQ without outputting a token corresponding to it? \"retrieves the correct answer directly from individual attention heads.\" from the abstract\n\nWas removing RoPE based on empirical observations? I'm surprised it was a significant factor\n\nWhat am I supposed to gain from the visualizations of the QK scores? How should I interpret figure 6?\n\nGenerally this comes across as a weak result, I think it should be published, but it is not currently in good enough shape for ICLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T8XCPLIEST", "forum": "WKUR7Wo01L", "replyto": "WKUR7Wo01L", "signatures": ["ICLR.cc/2026/Conference/Submission17883/Reviewer_cQyL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17883/Reviewer_cQyL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982275774, "cdate": 1761982275774, "tmdate": 1762927709188, "mdate": 1762927709188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the inner workings of LLMs in an attempt to identify specific heads that reliably encode correct answers to multiple-choice benchmark questions.  They show that these select-and-copy heads can be identified, that they typically exist in the middle of a model, and that by extracting information from them performance on benchmarks can be improved over standard methods.  This suggests that LLMs \"know more\" than they seem to, or that their outputs are perhaps jumbled by later processing layers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This is an intriguing white-box approach to directly assessing how LLMs represent knowledge and calculate outputs for MCQ tasks.  The identification of select-and-copy heads seems quite interesting, and I found it interesting to learn that LLMs correctly identify answers more often than they are given credit for.\n\nThe proposed method is fairly straightforward, although as a white-box technique, it is only applicable to open models.\n\nThe idea is natural and intriguing.\n\nThe experiments were most reasonable (with some caveats).\n\nThe ablations were helpful."}, "weaknesses": {"value": "While I generally liked the idea of this paper, it fell down in a couple of places:\n\n* The central results in Figure 3 are incredibly difficult to read, because all of the lines are overlapping. I think a bar chart would have been much better, or some alternative visualization that somehow magnified the difference between the various techniques.  As it is, because I cannot tell the difference between the algorithms and there are no error bars, I conclude that this method essentially does not change behavior / performance. That calls into question the entire point of the paper.\n\n* There are a couple of times when the method genuinely seems to help.  However, given the fact that it doesn't help in 50% of the benchmark tasks, (and even then, only in the few-shot regime), it seems that more extensive empirical work is required, combined with better presentation.\n\n* I like the idea of trying to find universal heads with a synthetic task. The heads you found didn't seem optimal, and you state that: \"Comparing head-selection strategies, synthetic-data heads excel on the first two datasets but lag on the latter pair, whereas validation-selected heads deliver robust performance throughout. This suggests that tasks involving longer options, deeper reasoning, or discourse coherence require skills not captured by our simple synthetic task.\"\n\nSo: why didn't you design a better synthetic task?\n\n* The identification of the SAC heads is ultimately unsatisfying -- while it's interesting to see their existence, it's not really clear how they work (and more importantly, why they are inhibited!).\n\nOverall, I think this paper sits uncomfortably straddling two worlds: it combines elements of benchmarking and mechanistic interpretability, but does not seem to make strong contributions to either world."}, "questions": {"value": "See weaknesses section above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UqHLCNWNp0", "forum": "WKUR7Wo01L", "replyto": "WKUR7Wo01L", "signatures": ["ICLR.cc/2026/Conference/Submission17883/Reviewer_8bfx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17883/Reviewer_8bfx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055339544, "cdate": 1762055339544, "tmdate": 1762927708600, "mdate": 1762927708600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The goal of the work is to investigate LLM capabilities to comply with a rigid output format for better application usage - findings show that there are selection bias with multiple choice questions and that the LLMs tend to provide some undesirable formats, complicating automatic evaluation despite their answer being correct. \n\nThe paper shows how the attention heads with Query-Key scores internally identify correct answers to multiple-choice questions. They first freeze the pre-trained LMs and calculate the Q-K scores within the head selected as the best among the sampled datasets. Using the select-and-copy operation, the work shows that the middle attention layer has more answer information for such multiple choice question-answering tasks and proposes to use the selective attention head from the middle layer (e.g., 14th layer) for robust performances in those tasks. The query-key interactions show how attention mechanism interact with model input-output and the internal structures in between."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Permutation Accuracy (PA) shows meaningful results in Table 2\n\nFigs 4 and 5 - show interesting patterns in attention heads.\n\nSSD settings seem to be informative to see QK does great job (Table1) - but it would have been more informative if there were more comparing with other baselines? \n\nExperimental setups are illustrated enough to replicate - e.g., prompt designs (Appendices A, D and H)\n\nTakeaways are clear."}, "weaknesses": {"value": "The empirical implications do not seem to be vivid and conclusive (Figs. 2 and 3) \n- is the authors' implication in Fig. 2 that the QK operates better than Attention for capturing correct answers? If so, the results don't seem to be apparent to me, except for some cases content eol and label cases on Hellaswag?\n- same for Fig. 3? \n\nThe proposed approach does not show robust performances in tasks with longer context, deeper reasoning, or discourse coherence \n\nThe current settings in main are too much leaning towards llama models'.\nIn main page, I cannot see any model families except for Llama model despite the abstract illustrating other model families including Qwen. Isn't it more complete to have them in the main page? Regarding those results in Appendix, \n- Tables 3 and 4 - Qwen2.5-1.5B doesn't seem to work pretty well compared to llama models. (baselines outperform the QK-scores frequently overall)\n- Fig. 21 - Qwen model do seem to work better at later layers than the middle layers unlike LLama models.\n- Fig. 22 - selection bias appear extreme to zero shot settings. Llama and R1-Distill-Qwen-7B seem to show different patterns. Instruct models seem to be robust agains the selection bias than base ones. 5-shot settings mitigate those biased results both for llama and distilled reasoning models. (isn't this base performance important to show how the QK on middle layers do meaningful job on capturing the correct responses?)\n- Figs.23-50 - with other models than llama family, QK scores seem relatively underperforming to the baselines; llama with higher parameters (Figs. 24, 26, 27, 29, 31), Chat, instruct models (figs. 31, 33, 34, 35, 36), models with too tiny paramters (figs. 42, 43).\n\nAppendix G seems lack of explanation in their experimental setups \n\nSome are arranged (Layer, Head) in Table 8, others are (Head, Layer) in Table 52 - better integrate with the one same arrangment? Now am confused with Figure 9 - is it arranged with (Layer, Head) or (Head, Layer)?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PEotLd90ee", "forum": "WKUR7Wo01L", "replyto": "WKUR7Wo01L", "signatures": ["ICLR.cc/2026/Conference/Submission17883/Reviewer_4sms"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17883/Reviewer_4sms"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17883/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088133221, "cdate": 1762088133221, "tmdate": 1762927707984, "mdate": 1762927707984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}