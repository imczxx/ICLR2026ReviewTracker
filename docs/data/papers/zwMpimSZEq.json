{"id": "zwMpimSZEq", "number": 1547, "cdate": 1756891051999, "mdate": 1759898203036, "content": {"title": "Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Method", "abstract": "Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically ref-\nerencing visual regions, just like human “thinking with images”. However, no\nbenchmark exists to evaluate these capabilities holistically. To bridge this gap, we\npropose TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic\nbenchmark built on three principles: (1) focused visual perception of subtle targets\nin complex scenes, (2) traceable evidence via bounding box evaluation, and (3)\nsecond-order reasoning to test object interactions and spatial hierarchies beyond\nsimple object localization. Prioritizing images with dense objects, we initially\nsample 1K high-quality images from SA-1B, and incorporate eight LMM experts\nto manually annotate questions, candidate options, and answers for each image.\nAfter three stages of quality control, TreeBench consists of 405 challenging vi-\nsual question-answering pairs, even the most advanced models struggle with this\nbenchmark, where none of them reach 60% accuracy, e.g., OpenAI-o3 scores only\n54.87. Furthermore, we introduce TreeVGR (Traceable Evidence Enhanced Visual\nGrounded Reasoning), a training paradigm to supervise localization and reasoning\njointly with reinforcement learning, enabling accurate localizations and explainable\nreasoning pathways. Initialized from Qwen2.5-VL-7B, it improves V* Bench\n(+16.8), MME-RealWorld (+12.6), and TreeBench (+13.4), proving traceability is\nkey to advancing vision-grounded reasoning. The code and data will be released.", "tldr": "", "keywords": ["visual reasoning", "benchmark", "thinking with images", "MLLM"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/673a43af7b1918d00885d0fb913df84adfdbc147.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to assess and improve the grounding reasoning of Vision-Language Models. To this end, it initially introduces a small, high-difficulty benchmark targeting detailed visual queries. Furthermore, the authors present a two-stage training method, which is demonstrated to boost the performance of Qwen2.5-VL-7B on their self-constructed benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed benchmark fills a gap in existing open-source benchmarks by specifically testing a model's ability to attend to fine-grained details\n2. The proposed method significantly improves the performance of Qwen2.5-VL-7B on their self-constructed benchmark"}, "weaknesses": {"value": "1. Regarding the ground truth for the intermediate reasoning: 1) How was this data obtained or generated? 2) What measures were taken to ensure its correctness and accuracy? 3) What was the underlying method and rationale for its construction?\n\t2. The authors have collected a 37k-sample dataset for RL, but it is unclear how the contributions of the data and the proposed method are disentangled. The paper would be strengthened by an ablation study that isolates the impact of each component. To provide a clearer picture of the method's efficacy, I suggest including a baseline that applies conventional RL algorithms to the same 37k dataset. This would help clarify whether the performance gains stem from the novel method itself or the curated data.\n\t3. The paper would benefit from a clearer justification for the evaluation dimensions presented in Section 3. Could the authors elaborate on the rationale behind their selection and explain the method used to ensure their comprehensiveness? Furthermore, providing data distribution across these different dimensions would be crucial.\n\t4. The paper only provides results on a self-constructed benchmark that focuses on small regions. Provide results on public benchmarks. Besides, does this specialized training approach compromise the model's generalization ability?\n\t5. Does focusing on a single small point per image create bias? This approach may allow the model to succeed without understanding the global context of the image."}, "questions": {"value": "Please refer to the issues detailed in the Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tmLM0O50fO", "forum": "zwMpimSZEq", "replyto": "zwMpimSZEq", "signatures": ["ICLR.cc/2026/Conference/Submission1547/Reviewer_o6mU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1547/Reviewer_o6mU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909619206, "cdate": 1761909619206, "tmdate": 1762915806844, "mdate": 1762915806844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of making vision language models (VLMs) not only answer visual questions correctly but also show where in the image their reasoning comes from. Current VLMs like Qwen2.5-VL or GPT-4V often produce correct answers without verifiable visual grounding, leading to untraceable or hallucinated reasoning.\n\nTo address this, the authors propose TreeVGR, a two-stage training framework built on Qwen2.5-VL-7B, and introduce TreeBench, a new benchmark for evaluating traceable visual grounded reasoning.\n\nTreeBench contains manually verified image–question–answer triplets with annotated bounding boxes marking the visual evidence, covering ten sub-tasks spanning perception (e.g., color, attribute, OCR) and reasoning (e.g., ordering, contact, spatial containment).\n\nTreeVGR first performs supervised fine-tuning to teach the model to produce structured reasoning with bounding boxes, then applies reinforcement learning with a reward that combines answer correctness, reasoning format, and bounding-box IoU.\n\nThe resulting TreeVGR-7B achieves nice gains over the base Qwen2.5-VL-7B on TreeBench (+13.4 points) and shows moderate improvements on other multimodal reasoning benchmarks such as V*Bench and MME-RealWorld. The authors claim that TreeVGR enables more transparent and verifiable visual reasoning by linking model predictions to explicit image evidence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. TreeBench is carefully constructed and manually verified for correctness and visual traceability, providing one of the first benchmarks that explicitly links reasoning answers to bounding-box evidence.\n\n2. The paper identifies a gap in current multimodal research: the lack of verifiable, evidence-grounded reasoning, and frames the need for “traceable visual reasoning” in a straightforward way.\n\n3. The two-stage TreeVGR pipeline (supervised fine-tuning followed by RL with evidence-based rewards) is simple, reproducible, and effectively demonstrates that incorporating bounding-box supervision can improve visual reasoning performance.\n\n4. The experiments cover diverse perception and reasoning sub-tasks, results are consistently reported, and the writing is clear, making both the dataset and the approach easy to understand and potentially useful for future follow-up work."}, "weaknesses": {"value": "1. TreeBench, though high quality, relies heavily on manual verification of image–question–evidence triplets, making it difficult to scale to larger or more diverse data. The process is partly automated but still human-dependent, which restricts reproducibility and extensibility.\n\n2. The paper equates correct reasoning with the ability to predict accurate bounding boxes, but provides no empirical evidence that models failing to output boxes are not attending to the correct regions internally. This makes the “traceability” assumption more procedural than cognitive, and potentially misleading.\n\n3. The model is trained and tested on tasks with nearly identical structures and output formats (question + bounding-box evidence). As a result, the large reported gains likely reflect adaptation to the curated data and benchmark design rather than a general improvement in reasoning ability.\n\n4. The RL stage yields only minor improvements over supervised fine-tuning and is insufficiently analysed. The paper does not demonstrate that reinforcement learning meaningfully enhances reasoning depth or grounding beyond improving output formatting.\n\n5. The study evaluates only on reasoning-style benchmarks and does not test whether TreeVGR retains the broader multimodal skills of the base Qwen2.5-VL model. This leaves open the possibility of overfitting or catastrophic forgetting of non-reasoning capabilities.\n\n6. Although the method claims to enhance traceable reasoning, the paper never validates whether the predicted evidence regions align with the model’s internal attention patterns or decision process, limiting the interpretability claims it makes."}, "questions": {"value": "1. How do the authors envision scaling TreeBench to larger or more diverse datasets without compromising annotation quality or requiring extensive human effort, since this is one of the reasons the authors claim their dataset to be superior?\n\n2. The paper assumes that correct reasoning must manifest through accurate bounding-box prediction, yet models may still attend to the correct region internally without explicitly outputting coordinates. Have the authors analysed attention maps or other internal signals to verify that bounding-box accuracy genuinely reflects visual grounding?\n\n3. Since both the training data and TreeBench share nearly identical QA structures and output formats, to what extent do the reported gains represent task adaptation rather than generalizable reasoning improvements? Have the authors tested transfer to unseen reasoning styles or datasets?\n\n4. The RL component seems to add only marginal improvements. Can the authors clarify what qualitative or behavioural differences emerge after RL fine-tuning compared to supervised fine-tuning alone, and whether these differences justify the added complexity?\n\n5. Given that TreeVGR fine-tunes the full Qwen2.5-VL model, have the authors evaluated whether general tasks such as captioning or VQA are preserved, or does the model overfit to the traceable reasoning format?\n\n6. The paper claims that the method enhances “traceable reasoning,” but without analyzing the alignment between predicted evidence and model attention. Can the authors provide evidence—such as attention heatmaps or token-level visualization—that the model truly focuses on the localized regions it predicts or what was the before vs after effect of their method on the base model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6Okxj2b9T7", "forum": "zwMpimSZEq", "replyto": "zwMpimSZEq", "signatures": ["ICLR.cc/2026/Conference/Submission1547/Reviewer_qSgY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1547/Reviewer_qSgY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915802093, "cdate": 1761915802093, "tmdate": 1762915806383, "mdate": 1762915806383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TreeBench, a benchmark for evaluating traceable visual grounded reasoning, and TreeVGR, a reinforcement learning framework that jointly supervises reasoning and localization using a dual IoU reward. TreeBench includes 405 expert-curated VQA samples with bounding box annotations to assess perception, reasoning, and evidence traceability. TreeVGR improves visual reasoning through a two-stage training process with supervised initialization followed by reinforcement learning, achieving notable improvements across multiple benchmarks and contributing to more explainable multimodal reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Studying visual grounding in the reasoning process is very meaningful because most models may ignore intermediate results in decision-making and fail to learn the true causal relationships.\n- A new benchmark has been constructed, which allows researchers to consider a wider range of factors."}, "weaknesses": {"value": "- Although the paper emphasizes achieving traceability through bounding boxes, the \"intermediate interpretability\" of the inference chain is still weak if it relies solely on box localization measure (mIoU).\n- Although the paper includes extensive comparisons with open-source multimodal models such as LLaVA-OneVision and Qwen2.5-VL, these models are not specifically designed for reasoning or reinforcement learning–based visual grounded reasoning. As a result, the experimental comparison may not fully demonstrate TreeVGR’s advantage over other reasoning-oriented approaches.\n- In the *Reinforcement Learning with Traceable Evidence* stage, the paper assumes that reasoning chains should explicitly include bounding boxes to ensure traceability. However, the necessity of this design is not fully justified. Reasoning transparency could also be achieved through implicit or attention-based grounding without inserting explicit box tokens. The paper lacks an analysis or ablation to clarify whether explicit box supervision is essential for reasoning quality.\n- It is recommended to discuss work related to the interpretability of visual grounding [1].\n\n[1] Interpreting Object-level Foundation Models via Visual Precision Search. CVPR 2025."}, "questions": {"value": "Please see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3yHpxv81mK", "forum": "zwMpimSZEq", "replyto": "zwMpimSZEq", "signatures": ["ICLR.cc/2026/Conference/Submission1547/Reviewer_rrGV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1547/Reviewer_rrGV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943445472, "cdate": 1761943445472, "tmdate": 1762915806128, "mdate": 1762915806128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to enhance the \"thinking with images\" capability of large multimodal models (LMMs). To this end, we introduce a benchmark named TreeBench and a training pipeline called TreeVGR. Specifically, TreeBench comprises 406 visual question-answer pairs, each accompanied by trace evidence that serves as a verifiable grounding instance. The proposed TreeVGR method extends the reinforcement learning algorithm GRPO to incorporate relevant visual instances (e.g., bounding boxes) as a form of Chain-of-Thought. Extensive experiments on TreeBench and the V* benchmark demonstrate the effectiveness of TreeVGR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The TreeBench Benchmark: They construct a novel VQA benchmark wherein each question is explicitly linked to a groundable instance that serves as traceable evidence, ensuring verifiability.\n\n2. The TreeVGR Method: They propose a GRPO-based training pipeline designed to enhance the groundedness of LMMs in VQA. The TreeVGR method guides the model to identify and utilize relevant evidential instances as a Chain-of-Thought."}, "weaknesses": {"value": "1. The paper does not explicitly measure the quality of the groundable evidence in TreeBench. Given the semi-automatic annotation process (Lines 91-101), how is the correctness of this evidence guaranteed? A compelling way to validate the importance of the evidence would be to observe if masking the critical instances (e.g., the bounding boxes) leads to a significant drop in VQA performance.\n\n2. The overall reward $R=R_{acc} + R_{iou} + R_{format}$ combines terms from different scales. Were these reward components normalized to a common range to prevent any single term from disproportionately dominating the optimization?"}, "questions": {"value": "1. How is the causal relationship between the evidence and the final answer validated? A critical test would be to see if the model's performance drops significantly when the key evidence instances are masked.\n\n2. Should the individual reward components ($R_{acc}, R_{iou}, R_{format}$) be normalized to the same scale? If not, there is a risk that the term with the largest magnitude could dominate the entire training process."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "the paper doesn't need ethic review"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "39YnrUjxwq", "forum": "zwMpimSZEq", "replyto": "zwMpimSZEq", "signatures": ["ICLR.cc/2026/Conference/Submission1547/Reviewer_PkSa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1547/Reviewer_PkSa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984221654, "cdate": 1761984221654, "tmdate": 1762915805696, "mdate": 1762915805696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}