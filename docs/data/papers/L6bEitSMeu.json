{"id": "L6bEitSMeu", "number": 18026, "cdate": 1758283024221, "mdate": 1760120092905, "content": {"title": "InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models", "abstract": "Recent benchmarks and datasets have been proposed to improve spatial reasoning in vision-language models (VLMs), yet existing open resources remain limited in scale, visual diversity, and instruction expressiveness. In this work, we introduce InternSpatial, the largest open-source dataset for spatial reasoning in VLMs, along with InternSpatial-Bench, a corresponding evaluation benchmark designed to assess spatial understanding under diverse instruction formats. InternSpatial comprises 12 million QA pairs spanning both single-view and multi-view settings, drawn from diverse visual environments and supporting 19 instruction formats that reflect varied query styles. For evaluation, we propose InternSpatial-Bench for single-view tasks and expand multi-view reasoning by introducing a novel rotation angle prediction task that has not been explored in prior work. Experimental results show that models trained on InternSpatial achieve 12.1% improvement on InternSpatial-Bench and 10.7% on VSI-Bench, while maintaining strong performance on general-purpose benchmarks. We hope these resources will support the development of spatially capable VLMs in practical applications such as robotics and embodied AI.", "tldr": "", "keywords": ["Vision Language Models", "Spatial Reasoning", "Visual Question Answering"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e576ba7b5d5505e256fddb9432d4d954592a8190.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Authors introduce a 12M QA-pair dataset containing both single and multi view images. This dataset aims to improve spatial reasoning in vision-language models (VLMs). The data is sourced from diverse domains (in-the-wild, indoor, driving, object-centric, embodied) and also contains an evaluation benchmark of 6,000 QAs. \nThe data generation pipeline uses automated annotation (e.g. object masks, depth estimation) with pretrained model followed by template-based QA generation.\nThe authors finetune an InternVL2.5-8B models on their dataset and obtain clear performance gains on spatial reasoning benchmarks including their own, while maintaining performance on general VQA benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear improvement over baseline when trained on new data. \n2. Thorough analysis of dataset statistics.\n3. Large-scale open-source dataset contribution"}, "weaknesses": {"value": "1. Only `InternVL2.5-8B` is used as a baseline. How does this dataset help other models in general? \n2. Only an 8B scale model is used. Will this data improve smaller (e.g. 2B-3B models), large (14B-70B), and MoE models? \n3. Minimal description of data generation process. How do the automated tools (e.g. SAM for bbox / mask) perform? What are the error rates? These are not clearly discussed in the paper. \n4. QA generation: diversity? These is little analysis on the dataset. Maybe calculate diversity metrics on the text and image data. Also, given the template based generation (as opposed to human or LLM), this data diversity concern is amplified. \n5. Test data leakage: The training dataset is created using 3D information of some datasets, \"integrated multi-view data derived from the training splits of the ScanNet/MultiScan/R2R/Objaverse\". At the same time, their benchmark uses some of these same datasets' test splits to evaluate. Even other benchmark (e.g. VSI contains ScanNet data) use this common data. Could the performance improvement be due to this similar domain data being used to create the training dataset?"}, "questions": {"value": "See weaknesses. \n\n1) In Table 1, the authors mention their dataset \"InternSpatial\" as Open-source. Is the dataset already open-source? If not, this claim is false? \n\n2) See related work sections of below papers for 2D spatial reasoning. Consider discussing prior 2D spatial reasoning works in detail?\n  - Ferretv2: https://arxiv.org/abs/2404.07973 \n  - LocVLM: https://arxiv.org/abs/2404.07449 \n  - Shikra: https://arxiv.org/pdf/2306.15195"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ynws0alW8g", "forum": "L6bEitSMeu", "replyto": "L6bEitSMeu", "signatures": ["ICLR.cc/2026/Conference/Submission18026/Reviewer_mDLs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18026/Reviewer_mDLs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850205270, "cdate": 1761850205270, "tmdate": 1762927816303, "mdate": 1762927816303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InternSpatial and the corresponding benchmark for spatial reasoning. The dataset contains 12 million QA pairs spanning both single-view and multi-view scenarios, covering 19 instruction formats (textual and visual). It also proposes InternSpatial-Bench, a new benchmark with a new rotation angle prediction task. Experiments are performed on various benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe proposed dataset is large-scale especially regarding the number of QA pairs.\n2.\tThe data generation pipeline is sound.\n3.\tIt shows promising performance leveraging the curated data."}, "weaknesses": {"value": "1.\tCould the author compare the proposed dataset with previous ones in terms of scenarios, question types, etc.?\n2.\tCould the authors validate the effectiveness of the proposed data on more open-source frameworks?\n3.\tCould the authors explain the performance show limited gain on rotation estimation and object counting?\n4.\tCould the generated QA pairs reflect the complexity or ambiguity of human spatial questions.\n5.\tWill data generation pipeline and data be publicly available?"}, "questions": {"value": "Please refer to Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CPyB5omVNR", "forum": "L6bEitSMeu", "replyto": "L6bEitSMeu", "signatures": ["ICLR.cc/2026/Conference/Submission18026/Reviewer_2hUS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18026/Reviewer_2hUS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124841516, "cdate": 1762124841516, "tmdate": 1762927815784, "mdate": 1762927815784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes InternSpatial, which is claimed to be the largest spatial QA dataset with 12M data. They sourced the data from 2D images as well as 3D datasets of various sources, generating single-view and multi-view QA pairs. They report the scores of various models on the InternSpatial Bench, including InternVL-8B model trained on their datasets. They also report results on VSI-Bench as well as other general benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "In general, I think a12M dataset is a quite significant improvement from previous QA datasets in terms of data quantity. The dataset comes from a wide variety of data, as shown by Figure 4. Results on InternSpatial-Bench, VSI-Bench as well as other general benchmark results show that training on this dataset brings a lot of improvements."}, "weaknesses": {"value": "It would be great to understand how much the image datasets are helping with the training in general. The alignment to view space from 2D images requires depth estimation followed by camera estimation, both of which could potentially introduce significant errors. I wonder if it would be possible to see the improvements based on InternVL-Spatial-8B trained with only 3D datasets and/or only 2D datasets. Also, this would give more insights on whether there are domain gaps within the training dataset itself.\n\nThe paper could also be strengthened by showing more baselines of models specialized in 3D on InternSpatial-Bench (e.g., SpatialMLLM, SpaceR, etc). Currently, the results shown are mainly on general VLMs and not VLMs specifically trained for spatial reasoning. \n\nThe paper would also be improved by showing results of more methods finetuned with InternSpatial dataset to further show the effectiveness of the dataset on different model architectures/training methods."}, "questions": {"value": "Overall, my main questions of the paper are two-fold:\n\n1. Are both dataset sources (2D and 3D images) helpful in contributing to the training when evaluating on other benchmarks such as VSI-Bench? Are there potential ways to filter out results of bad prediction during 2D data preprocessing?\n\n2.Are there results of other methods finetuned on this dataset? Does that bring any further improvements?\n\nOverall, I do see this dataset being beneficial to the community, so I am leaning towards accept. The questions I believe would significantly add to the contribution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uyurqltzBX", "forum": "L6bEitSMeu", "replyto": "L6bEitSMeu", "signatures": ["ICLR.cc/2026/Conference/Submission18026/Reviewer_S4PX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18026/Reviewer_S4PX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762157131340, "cdate": 1762157131340, "tmdate": 1762927815295, "mdate": 1762927815295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents InternSpatial, a large-scale open-source dataset (12M QA pairs) designed to improve spatial reasoning in Vision-Language Models (VLMs). It addresses key limitations in prior works, such as limited scene diversity, narrow instruction formats, and lack of multi-view supervision, by aggregating data from a wide range of sources (COCO, Visual Genome, ScanNet, Cityscapes, Objaverse, R2R, etc.) and generating question-answer pairs with 19 instruction formats spanning both textual and visual variations. The authors also propose InternSpatial-Bench, a benchmark comprising 6,008 QA pairs that evaluate single-view and multi-view spatial reasoning, including a new rotation angle prediction task.\n\nModels trained on InternSpatial (notably, InternVL-Spatial-8B) show large performance gains +12.1% on InternSpatial-Bench and +10.7% on VSI-Bench, while maintaining comparable performance on general VQA tasks, confirming that spatial reasoning gains do not come at the expense of general multimodal ability"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Reasonable Dataset Design\n- The dataset covers diverse visual domains (indoor, outdoor, object-centric, embodied, urban) and both single-view and multi-view reasoning setups.\n- It supports a wide variety of instruction modalities, text, bounding boxes, masks, numeric indicators, coordinate-based prompts, totaling 19 instruction types, a major advancement over prior datasets like SpatialVLM or OSD.\n### Data Generation Process\n- The data pipeline integrates multiple pretrained modules for depth, segmentation, and camera parameter estimation (SAM2, Metric3Dv2, PerspectiveFields, WildCamera) to lift 2D annotations into 3D canonical view space. The pipeline is modular and reproducible, allowing flexible annotation generation and QA synthesis without relying on expensive LLM prompting for each sample.\n\n### Novel Multi-view Reasoning Component\n- The addition of rotation angle prediction is new and well-motivated for embodied AI and robotics. Multi-view QA construction uses geometric consistency (e.g., Alpha Shape–based room estimation, OrientedBoundingBox fitting) to ensure physically grounded reasoning.\n\n### Strong Experimental Results and Benchmarking\n- The evaluation suite is broad: InternSpatial-Bench, VSI-Bench, and five standard multimodal tasks.\n- Results show large, consistent improvements in spatial reasoning, including outperforming commercial VLMs like GPT-4o and Claude 3.7 Sonnet in several spatial tasks.\n- Ablation studies isolate the effects of instruction format diversity and confirm its value for cross-format generalization."}, "weaknesses": {"value": "### The limit of Template-Driven QA Generation. \n\nWhile efficient, the template-based QA generation may lead to limited linguistic diversity and potential overfitting to templated phrasing. The authors acknowledge this, but do not quantify how template rigidity affects generalization to natural human queries.\n\n### Lack of Qualitative Error Analysis\n\nThe evaluation focuses almost exclusively on quantitative metrics. There is little qualitative examination of failure modes (e.g., reasoning about occluded objects, symmetry, or ambiguous rotations).\n\n### Over-Reliance on InternVL2.5 Backbone\n\nExperiments are restricted to fine-tuning InternVL2.5-8B. The generality of the dataset across architectures (e.g., LLaVA, Qwen2.5-VL) is not tested. This limits claims of dataset generalizability.\n\n### Rotation Task Evaluation Unclear\n\nThe “rotation angle prediction” task is introduced as novel but evaluated using classification accuracy, without specifying the label granularity (e.g., 15° bins?). Clarifying this would help interpret improvements."}, "questions": {"value": "### Template Diversity:\nHow many distinct QA templates were used, and how was linguistic diversity ensured across 12M samples? Were any human validation steps introduced beyond filtering for ambiguity?\n\n### Instruction Format Sampling:\nGiven 19 formats, how were the subsets for each training batch sampled? Is there a weighting scheme, or are they uniformly sampled?\n\n### Multi-View Ground Truth Validation:\nFor the rotation estimation task, how were ground-truth rotation angles derived or verified in scenes where camera calibration might be uncertain?\n\n### Cross-Model Evaluation:\nHave the authors tested whether models other than InternVL2.5 (e.g., LLaVA-OneVision, Qwen-VL) benefit similarly from InternSpatial training?\n\n### Human Baseline or Difficulty Assessment:\nHas any human performance baseline been measured on InternSpatial-Bench to contextualize the difficulty of the tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G26d84Ijzf", "forum": "L6bEitSMeu", "replyto": "L6bEitSMeu", "signatures": ["ICLR.cc/2026/Conference/Submission18026/Reviewer_9QLW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18026/Reviewer_9QLW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18026/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762367966856, "cdate": 1762367966856, "tmdate": 1762927814237, "mdate": 1762927814237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}