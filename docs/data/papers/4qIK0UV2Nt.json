{"id": "4qIK0UV2Nt", "number": 3048, "cdate": 1757321608991, "mdate": 1759898111612, "content": {"title": "Human-MME: A Holistic Evaluation Benchmark for Human-Centric Multimodal Large Language Models", "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant advances in visual understanding tasks. \nHowever, their capacity to comprehend human-centric scenes has rarely been explored, primarily due to the absence of comprehensive evaluation benchmarks that take into account both the human-oriented granular level and higher-dimensional causal reasoning ability. \nSuch high-quality evaluation benchmarks face tough obstacles, given the physical complexity of the human body and the difficulty of annotating granular structures.\nIn this paper, we propose Human-MME, a rigorously curated benchmark designed to provide a more holistic evaluation of MLLMs in human-centric scene understanding. Compared with other existing benchmarks, our work provides three key features:\n**(1) Diversity in human scene**, spanning 4 primary visual domains with 15 secondary domains and 43 sub-fields to ensure broad scenario \ncoverage.\n**(2) Progressive and diverse evaluation dimensions**, evaluating the human-based activities progressively from the human-oriented granular perception to the higher-dimensional multi-target and causal reasoning, consisting of eight dimensions with 19,945 real-world image question pairs and an evaluation suite.\n**(3) High-quality annotations with rich data paradigms**, constructing the automated annotation pipeline and human-annotation platform, supporting rigorous manual labeling by expert annotators to facilitate precise and reliable model assessment. Our benchmark extends the single-person and single-image understanding to the multi-person and multi-image mutual understanding by constructing the choice, short-answer, grounding, ranking and judgment question components, and complex question-answer pairs of their combination. The extensive experiments on 17 state-of-the-art MLLMs effectively expose the limitations and guide future MLLMs research toward better human-centric image understanding and reasoning. All data and code will be publicly released.", "tldr": "", "keywords": ["Benchmark", "Multimodal Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0bee3f42e9407b8e58b11f32b54e3e0df106490f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Human-MME, a comprehensive evaluation benchmark for the human-centric scene understanding capabilities of Multimodal Large Language Models (MLLMs). It aims to address the issue that existing benchmarks lack evaluations of fine-grained human perception and high-dimensional causal reasoning. The benchmark secures the quality of annotations by collecting and screening 16,765 high-quality images from free stock image libraries and the HICO-DET dataset, combined with an \"automated annotation (5-step process) + manual adjustment (cluster deduplication and instance correction)\" approach. It designs 19,945 image-question pairs covering 8 evaluation dimensions (such as face understanding and causal discrimination) and 5 types of question-answer paradigms. Experiments on 17 mainstream MLLMs show that GLM-4.5V achieves the best overall performance, while Qwen2.5-VL-72B excels in high-level reasoning. Meanwhile, the experiments reveal the limitations of the models in aspects like left-right discrimination of body parts and precision-recall trade-off in judgment tasks, providing support for research on human-centric scene understanding of MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- To address the issue that existing benchmarks lack comprehensive evaluation of \"fine-grained perception + high-dimensional reasoning\" in human-centric scenarios, this study constructs, for the first time, an 8-dimensional progressive evaluation system ranging from face/body understanding (fine-grained) to intention/causality/emotion discrimination (high-level reasoning). This system covers the core capability dimensions of human scene understanding and can more comprehensively reveal the capability boundaries of Multimodal Large Language Models (MLLMs) in this field.\n- Five types of question-answer paradigms are designed, including multiple-choice questions, bounding box questions, short-answer questions, ranking questions, and judgment questions, to adapt to different evaluation needs. Moreover, each type of paradigm is matched with exclusive and precise metrics (e.g., Intersection over Union (IoU) for bounding box questions, Kendall’s τ for ranking questions, and F1 score for judgment questions). This avoids the flaw of a \"one-size-fits-all\" metric approach and enables a more objective assessment of model performance across different tasks."}, "weaknesses": {"value": "- The annotation of high-level reasoning tasks such as intention, emotion, and causality relies on Qwen3 to generate \"identity-neutral descriptions\". Although manual verification is conducted, there are still certain subjective differences in judgments like \"reasonableness of intention\" and \"emotion matching degree\" (for example, different experts may have different opinions on whether \"a person's intention is 'preparing for an adventure' or 'organizing equipment'\"), which may affect the objectivity of the evaluation for such tasks.\n- The human-centric scenes in the dataset do not explicitly mention cross-cultural diversity (such as differences in clothing and behavioral habits across regions), nor are there specially designed evaluation scenarios for specific populations (such as people with disabilities and groups of different age groups). This makes it difficult for the benchmark to measure issues like \"cultural bias\" or \"insufficient population coverage\" of MLLMs in human-centric scene understanding."}, "questions": {"value": "It is suggested that the authors add a table to comparatively demonstrate the differences and advantages between Human-MME and previous relevant benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P5Mv7XEy2V", "forum": "4qIK0UV2Nt", "replyto": "4qIK0UV2Nt", "signatures": ["ICLR.cc/2026/Conference/Submission3048/Reviewer_ZsgC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3048/Reviewer_ZsgC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727313595, "cdate": 1761727313595, "tmdate": 1762916528291, "mdate": 1762916528291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the gap of insufficient human-centric evaluation benchmarks for Multimodal Large Language Models (MLLMs) by proposing **Human-MME**, a holistic benchmark for human-centric scene understanding. Key features of Human-MME include: (1) Diverse human scenes (4 primary domains, 15 secondary domains, 43 sub-fields) and 19,945 real-world image-question pairs; (2) Progressive evaluation across 8 dimensions (from fine-grained perception like face/body understanding to high-level reasoning like intention/causal discrimination); (3) High-quality annotations via an automated pipeline (YOLOv11, Qwen2.5-VL, Grounding DINO) plus manual refinement (Gradio interface). The paper conducts extensive experiments on 17 state-of-the-art MLLMs (open/closed-source), revealing critical findings (e.g., stronger scaling effects in Choice/Ranking tasks, training data’s greater impact on grounding than model size, left-right body part discrimination challenges). Contributions include the benchmark itself, a scalable annotation pipeline, and insights guiding MLLM development for human-centric understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Human-MME fills a key gap by covering 8 dimensions that progress from fine-grained perception to abstract reasoning (e.g., causal inference), with 21 question types. This design captures the full complexity of human-centric understanding, unlike existing benchmarks that focus on either coarse perception or single-task reasoning.\n\n- The hybrid automated-manual pipeline balances scalability and accuracy. Automated steps extract fine-grained features (13 bounding box types, 42 facial attributes) using state-of-the-art models, while manual review (cluster de-duplication, instance correction) and identity-neutral processing (removing age/gender from emotion/intention labels) reduce bias and errors—critical for reliable human-centric evaluation.\n\n- Evaluating 17 MLLMs reveals nuanced findings and general trends (scaling benefits Choice/Ranking tasks most). These insights directly guide future MLLM research (e.g., prioritizing grounding data over size for spatial tasks)."}, "weaknesses": {"value": "* For the most abstract dimensions (Intention, Causal, Emotion Discrimination), the \"ground truth\" options are narratives generated by Qwen3. Although these are reviewed by humans, it's unclear if these annotations represent genuine human ground truth or simply a \"plausible, model-generated\" narrative. Using model-generated text to benchmark the reasoning of other models in such subjective tasks is a potential methodological weakness that merits more discussion.\n* The authors need to provide more latest MLLMs to support experimental conclusions, such as Claude-4-Sonnet,  GPT-5, and o3.\n* The data and code not be publicly released."}, "questions": {"value": "* The results in Table 2 show that models like Gemini-2.5-Pro and Intern-S1 have very poor grounding (BBox) scores but strong high-level reasoning scores, while GLM-4.5V shows the opposite pattern. Does this suggest a fundamental architectural or training trade-off between fine-grained spatial localization and high-level abstract reasoning, or do you believe this is purely an artifact of their specific training data (as discussed in 3.3)?\n* Did you measure difficulty (e.g., inter-model accuracy variance, human annotation time) across dimensions/sub-fields? Are there imbalances (e.g., over-representation of easy Daily Life questions) that might skew model rankings?\n\nI look forward to an active discussion with the authors during the rebuttal phase and will revise my score accordingly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UsCyjRq21t", "forum": "4qIK0UV2Nt", "replyto": "4qIK0UV2Nt", "signatures": ["ICLR.cc/2026/Conference/Submission3048/Reviewer_rEtP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3048/Reviewer_rEtP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926700745, "cdate": 1761926700745, "tmdate": 1762916526483, "mdate": 1762916526483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Human-MME, a large-scale benchmark for evaluating human-centric multimodal large language models (MLLMs). It includes 19,945 image-question pairs across 43 scenarios, covering 8 core human understanding dimensions and 5 question types. Through both automated and manual annotation, the evaluation of 17 major MLLMs reveals clear weaknesses in tasks like left-right hand recognition, emotion analysis, and hallucination suppression."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Pioneers the construction of a systematic, multi-dimensional, end-to-end benchmark for human-centric scene evaluation, offering comprehensive content coverage.\n\n- The benchmark is data-rich and evaluates a wide range of capabilities: from low-level perception (face, body, interaction) to high-level reasoning (emotion, intent, causality, multi-image/multi-object), forming a progressive evaluation hierarchy with rich annotation dimensions (such as left/right hand, emotion, intent, causality)."}, "weaknesses": {"value": "- Although the benchmark is extensive in both scale and coverage, its methodology (automated annotation plus manual validation, Q&A-style evaluation) is largely similar to existing benchmarks (e.g., MMBench, MME), lacking methodological novelty.\n\n- The current benchmark focuses mainly on static images, without covering video-based continuous scenarios or multimodal temporal reasoning, which falls short of some human-centric video benchmarks in terms of scenario, action, and intent diversity.\n\n- The analysis of model differences and conclusions is not sufficiently in-depth. While the performance disparities among models are reported, the study lacks a thorough exploration of the underlying causes—for example, the influence of training data, visual encoder, or alignment strategies on the results."}, "questions": {"value": "- Has a quantitative analysis been conducted to prevent data leakage (i.e., training/testing overlap)? If models have potentially seen duplicates such as HICO-DET, how might this affect the evaluation conclusions?\n\n- The remaining issues will be further explored after communicating with the other reviewers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vNeoG7scwz", "forum": "4qIK0UV2Nt", "replyto": "4qIK0UV2Nt", "signatures": ["ICLR.cc/2026/Conference/Submission3048/Reviewer_jnXs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3048/Reviewer_jnXs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931290814, "cdate": 1761931290814, "tmdate": 1762916525915, "mdate": 1762916525915, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Human-MME, a large-scale benchmark for evaluating multimodal large language models on human-centric scene understanding, covering eight progressive dimensions from fine-grained perception to high-level reasoning, with 19945 samples, an automated–manual annotation pipeline, and evaluations on 17 models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses the underexplored problem of human-centric multimodal understanding and provides a clear motivation for its importance.\n2. It provides a holistic eight-dimension evaluation that spans fine-grained perception to high-level reasoning, with 19,945 real-world image–question pairs covering 43 diverse scenarios across four human activity domains.\n3. The hybrid automated and expert-reviewed annotation pipeline combines scalability with high annotation quality and reduced bias."}, "weaknesses": {"value": "1. Proprietary models such as GPT-4o perform poorly on face-related or human-identity tasks, which may stem from privacy or policy restrictions, but the paper does not discuss this possible factor.\n2. The evaluation could be improved by including results from newer proprietary models to better reflect the current state of the field.\n3. Lacks deeper error analysis, especially for the best-performing models, to identify detailed failure patterns across tasks and provide clearer insights for future model development."}, "questions": {"value": "1. Could the authors clarify whether the poor performance of proprietary models such as GPT-4o on face-related or human-identity tasks is due to policy or privacy restrictions, or inherent model limitations?\n2. Would it be possible for the authors to include or discuss results from newer proprietary models to better reflect the current performance landscape?\n3. Could the authors provide a more detailed error analysis for the best-performing models, like highlighting representative failure cases across different task types to better understand current limitations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iyVid8cDrC", "forum": "4qIK0UV2Nt", "replyto": "4qIK0UV2Nt", "signatures": ["ICLR.cc/2026/Conference/Submission3048/Reviewer_cQVR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3048/Reviewer_cQVR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974202690, "cdate": 1761974202690, "tmdate": 1762916525682, "mdate": 1762916525682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}