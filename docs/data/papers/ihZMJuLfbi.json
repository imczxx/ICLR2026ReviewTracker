{"id": "ihZMJuLfbi", "number": 892, "cdate": 1756822089292, "mdate": 1759898237225, "content": {"title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language Models", "abstract": "Video large language models (VideoLLMs) have demonstrated the capability to process longer video inputs and enable complex reasoning and analysis. However, due to the thousands of visual tokens from the video frames, the key-value (KV) cache can significantly increase memory requirements, becoming a bottleneck for inference speed and memory usage. KV cache quantization is a widely used approach to address this problem. In this paper, we find that 2-bit KV quantization of VideoLLMs can hardly hurt the model performance, while the limit of KV cache quantization in even lower bits has not been investigated. To bridge this gap, we introduce VidKV, a plug-and-play KV cache quantization method to compress the KV cache to lower than 2 bits. Specifically, (1) for key, we propose a mixed-precision quantization strategy in the channel dimension, where we perform 2-bit quantization for anomalous channels and 1-bit quantization combined with FFT for normal channels; (2) for value, we implement 1.58-bit quantization while selectively filtering semantically salient visual tokens for targeted preservation, for a better trade-off between precision and model performance. Importantly, our findings suggest that the value cache of VideoLLMs should be quantized in a per-channel fashion instead of the per-token fashion proposed by prior KV cache quantization works for LLMs. Empirically, extensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show that VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit precision with almost no performance drop compared to the FP16 counterparts.", "tldr": "", "keywords": ["Video Large Language Model", "Visual KV Cache Quantization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b86fdf2279d74163f9e66116aaa9a6422d64e32f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents VidKV, a study on how to effectively quantize the KV cache for video LLMs. The authors propose to quantize the key cache per channel using FFT-based 1-bit quantization on normal channels and apply 2-bit quantization to abnormal channels. They also claim that, contradict to prior works on text-only LLM, the value cache, instead, should be quantized per-channel, and they choose to apply 1.58-bit quantization to the value cache, along with 2-bit quantization for those critical visual tokens. This mixed precision quantization strategy to some extent preserves the quality of the model's output, as shown in the experiment section."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of this paper, pushing the quantization level to 1.x bit quantization for video LLMs, is clear, and the overall presentation of the paper is fluent and clean.\n\n2. The authors present several ablation studies to justify their design choices for how to quantize key/value cache in a better way, which is informative and well supportive of their methodology."}, "weaknesses": {"value": "1. The claim that the value cache for video LLMs should be quantized per-channel instead of per-token is not well justified. The primary justification for this claim is presented in Table 1. However, in Table 1, when the Bit (K/V) is 1.5/1.58, it is shown that using per-token quantization for the value cache is better. Conversely, when the Bit (K/V) is 2/2, per-channel is better. The results presented here are very confusing. \n\n2. When it comes to the experiment section, I doubt the effectiveness of the proposed methodology, as there are essentially no baseline results for reference when the quantization bits are below 2 bits, and it is very hard to assess and interpret the evaluation results. For example, one naive baseline can be applying 1.58bits to both key/value cache as presented in the original paper for BitNet b1.58 [1]. Also, it is helpful to compare the proposed methodology to existing KV cache quantization works for VLMs, as these models also accept vision input and should potentially be applicable to video input.\n\n3. The authors claim that \"Notably, KV cache quantization in VideoLLMs is essential to mitigate memory and computational bottlenecks.\" around line 105. However, there is no computation comparison presented in this paper. Also, I am curious if the proposed methodology indeed improved the efficiency of video LLM inference, considering the complex quantization scheme the authors used in this paper. Any latency comparison results would be necessary to clarify this.\n\n4. This paper focuses heavily on the methodology and empirical results, but does not present adequate insights into how LLMs with vision input differ from LLMs with only text input. For example, does the vision token behave similarly from the quantization perspective compared to text tokens? The author claims that the key cache should be quantized per channel, but why? Is it because of the specialty of vision tokens? The novelty and contribution of the proposed methodology appear limited, as it primarily combines existing techniques. Nonetheless, deriving new and well-justified insights (if there are any) by applying established methods to new domains could still offer meaningful value to the community. I would be happy to see more discussion on insights in the main paper.\n\n[1] The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits"}, "questions": {"value": "1. The results presented in Table 1 are inconsistent, as K-C, V-C is better when Bit (K/V) is 2 / 2, but worse when Bit (K/V) is 1.5 / 1.58. Why?\n\n2. In Figure 6, the performance drops significantly without FFT as the quantization bit approaches 1-bit. However, the results with FFT are not provided. How does the performance behave when FFT is applied under such extreme quantization (near 1-bit)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "huk7gnIKlm", "forum": "ihZMJuLfbi", "replyto": "ihZMJuLfbi", "signatures": ["ICLR.cc/2026/Conference/Submission892/Reviewer_fUDc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission892/Reviewer_fUDc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572145360, "cdate": 1761572145360, "tmdate": 1762915636831, "mdate": 1762915636831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VidKV, a PTQ framework for Video LLM KV-Cache compression. VidKV combines 2-bit quantization with FFT-based 1-bit quantization for Key-Cache; for the value cache, it uses 1.58-bit quantization with selective token protection. A key finding of the paper is that, unlike plain text LLMs, the value cache of VideoLLMs is more amenable to per-channel rather than per-token quantization. Experiments demonstrate that VidKV can compress the KV cache to approximately 1.5 bits with little performance degradation compared to the FP16 version."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Pioneering Work in Sub-2-Bit Quantization for Video LLM: The paper fills an important gap by exploring sub-2-bit KV cache quantization specifically for VideoLLMs, a domain that has been underexplored compared to textLLMs.\n2. Strong Performance at Near-2-Bit Precision: The experimental results compellingly show that the proposed method achieves nearly lossless performance compared to the FP16 baseline when quantizing the KV cache to approximately 2-bits."}, "weaknesses": {"value": "1. Lack of Essential Efficiency Experiments: The paper's claims are centered on improving inference efficiency, yet it completely lacks empirical data on crucial metrics like end-to-end throughput or latency. All evaluations are focused on model accuracy, which is insufficient for a work positioned to solve an efficiency bottleneck. This is a critical omission.\n2. Incremental Contribution: The primary novel contribution appears to be the observation that the value cache benefits from per-channel quantization. However, the experimental results in Table 2 show that applying 2-bit per-channel quantization to the value cache (VidKV 2-bit) does not offer a significant performance improvement over the KIVI baseline, which uses per-token quantization. Other techniques build heavily upon concepts thoroughly explored in prior work like KIVI. This makes the overall contribution to the work appear incremental."}, "questions": {"value": "1. FFT Overhead on GPUs: The paper claims that the FFT overhead is small (\"less than 5%\"). Could you provide specific latency figures for the FFT/IFFT operations on a GPU? How does this overhead scale with an increasing batch size during the decoding stage?\n2. System Complexity of Mixed-Precision: The introduction of mixed-precision quantization (e.g., 1-bit, 1.58-bit, 2-bit) and token protection adds complexity to KV cache management. How would this affect integration into highly optimized inference systems like vLLM? Does this introduce significant system-level complexity that could offset theoretical memory savings?\n3. Clarification on Figure 3: The legend in Figure 3 appears to be confusing, with multiple legend items sharing the same visual representation (e.g., \"Important Token\" and \"Important Token in V\"). Could you please clarify this visualization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "a0NaSnuEHZ", "forum": "ihZMJuLfbi", "replyto": "ihZMJuLfbi", "signatures": ["ICLR.cc/2026/Conference/Submission892/Reviewer_Jkph"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission892/Reviewer_Jkph"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659961705, "cdate": 1761659961705, "tmdate": 1762915636727, "mdate": 1762915636727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VidKV, a KV cache quantization method for Video Large Language Models (VideoLLMs). VidKV can compress the key-value cache to 1.58-bit precision with minimal performance loss. The authors observe that VideoLLMs exhibit high visual token redundancy, enabling aggressive quantization beyond the typical 2-bit approaches used for text-only LLMs. VidKV employs distinct mixed-precision quantization strategies for keys and values: (1) for key cache, it identifies anomalous channels using range-based evaluation and applies 2-bit quantization to these channels while using FFT-based 1-bit quantization for normal channels; (2) for value cache, it implements 1.58-bit ternary quantization (mapping to {-1, 0, 1}) with an optional semantic token protection mechanism that preserves critical visual tokens at 2-bit precision. Critically, the authors find that value cache in VideoLLMs should use per-channel quantization rather than the per-token approach proposed for text LLMs. Experiments on LLaVA-OV-7B and Qwen2.5-VL-7B across six benchmarks demonstrate that VidKV reduces KV cache size by 80% while maintaining near-baseline performance, offering significant memory savings for long video inference scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper studies <2-bit KV cache quantization specifically for VideoLLMs. The authors provide a distribution analysis showing that VideoLLMs exhibit distinct characteristics compared to text-only LLMs, particularly the high redundancy of visual tokens and different outlier patterns in value caches. \n\n2. The author applies FFT to the key cache for stabilized low-bit quantization. As observed, the key cache contains channels with sharp fluctuations (or outliers) and abnormal variations in the time domain, and this makes 1-bit quantization extremely challenging and leads to large quantization errors. To handle this, they utilize the FFT transform to the key cache from the time domain to the frequency domain.\n\n3. They propose a mixed-precision quantization scheme that applies different bit-widths to different components based on their distribution characteristics: for the key, they use a range-based channel evaluation to identify outlier channels with large magnitude variations and quantize them at 2-bit precision, while normal channels undergo FFT-based 1-bit quantization in the frequency domain to stabilize their distributions; for the value cache, they employ 1.58-bit ternary quantization ({-1, 0, 1}) across all channels using per-channel quantization, with an optional semantic token protection mechanism that selectively preserves critical visual tokens at 2-bit precision to better trade off between compression ratio and model performance.\n\n\n4. The method achieves promising results on two VideoLLM families (LLaVA-OneVision-7B and Qwen2.5-VL-7B) across six diverse benchmarks, compressing KV cache to 1.5-bit and 1.58-bit precision with 80% memory reduction and minimal accuracy loss compared to FP16 baselines, while outperforming KIVI at 2-bit quantization through per-channel value cache quantization, demonstrating the practical effectiveness of their training-free, plug-and-play approach."}, "weaknesses": {"value": "1. The outlier in the key cache is already widely observed and studied through very different approaches. For example, KIVI also addresses this through column-wise (per-channel) quantization on the key cache, and KVQuant also observed this phenomenon. The proposed approach with FFT is not only applicable to VideoLLMs but is a general technique for smoothing distributions that has been widely used for outlier handling in various quantization contexts (as acknowledged by citing Tseng et al., 2024). The novelty of applying FFT to key cache quantization is somewhat incremental, and the paper lacks sufficient comparison with other outlier handling techniques such as rotation-based methods (e.g., QuaRot, RotateKV mentioned in related work) or more sophisticated mixed-precision strategies that could potentially achieve similar or better results without the overhead of FFT/IFFT transformations.\n\n2. The benchmark methods seem not sufficient as there are various approaches on KV cache quantizations such as KVQuant, SKVQ, ZipCache, and CQ mentioned in the related work, yet the paper only compares against KIVI as the primary baseline. The absence of comprehensive comparisons with these recent methods makes it difficult to assess the relative merits of VidKV, especially since some of these methods also employ mixed-precision strategies or handle outliers differently. Furthermore, the paper lacks comparisons with other Video-LLM-specific compression techniques such as token pruning or merging methods (e.g., DyCoKe, HoliTom mentioned in related work), which could provide complementary or alternative solutions to the KV cache memory bottleneck and would help position VidKV's contributions more clearly within the broader landscape of VideoLLM efficiency research.\n\n3. While the paper claims FFT adds \"less than 5%\" computational overhead, there is no detailed analysis of the actual inference latency of FFT in practice. The 1.58-bit ternary quantization is presented as enabling conversion of matrix multiplication to addition operations for computational efficiency, but no actual runtime measurements or throughput comparisons are provided to validate these theoretical benefits. In addition, it would be great to state the hardware support and implementation details, for instance, whether existing hardware accelerators can efficiently handle the mixed-precision scheme, FFT/IFFT operations during inference, and the ternary quantization format, which are critical factors for real-world deployment scenarios.\n\n4. The results show notable performance degradation on certain benchmarks, particularly for the Qwen2.5-VL model on TempCompass and VideoDC when using 1.5-bit key and 1.58-bit value quantization, but the paper does not provide in-depth analysis of why these specific tasks are more sensitive to quantization. The VATEX captioning results (Table 4) show significant drops across all metrics for 1.x-bit configurations, suggesting the method may struggle with fine-grained generation tasks that require precise token representations."}, "questions": {"value": "1. How the proposed method performs compared to additional methods, e.g., KVQuant, SKVQ, ZipCache, or CQ, or otation-based outlier smoothing methods (e.g., QuaRot, RotateKV)?\n\n2. How the VidKV implement 1.58-bit quatization? What is the hardware support status for mixed-precision and ternary quantization formats?\n\n3. What is the actual throughput improvement from converting multiplication to addition in 1.58-bit quantization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wst1VXRrin", "forum": "ihZMJuLfbi", "replyto": "ihZMJuLfbi", "signatures": ["ICLR.cc/2026/Conference/Submission892/Reviewer_uDCi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission892/Reviewer_uDCi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979399168, "cdate": 1761979399168, "tmdate": 1762915636592, "mdate": 1762915636592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}