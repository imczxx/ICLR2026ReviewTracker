{"id": "NbxuP8NIk8", "number": 5659, "cdate": 1757925993510, "mdate": 1759897962420, "content": {"title": "UniMind: Unleashing the Power of LLMs for Unified Multi-Task Brain Decoding", "abstract": "Decoding human brain activity from electroencephalography (EEG) signals is a central challenge at the intersection of neuroscience and artificial intelligence, enabling diverse applications in mental state assessment, clinical monitoring, and human–machine interaction. Recent efforts have extensively investigated building EEG-based pretrained encoders for generalized brain decoding through large-scale training on multiple datasets. However, most of these approaches still struggle to achieve satisfactory performance without task-specific tuning, owing to the pronounced inherent heterogeneity across decoding tasks. To address these challenges, we present UniMind, a general-purpose EEG foundation model for unified multi-task brain decoding by uniquely unleashing the power of LLMs to comprehend complex neural patterns. UniMind enjoys several merits. First, we design a Neuro-Language Connector to transform the spatiotemporal neural patterns of EEG data into LLM-understandable representations. Second, a Task-aware Query Selection module is proposed to inject task-awareness into the cross-modal understanding by dynamically generating task-adaptive query tokens, enabling the learning of task-relevant neural patterns across diverse tasks. Extensive experiments across 10 datasets demonstrate that UniMind substantially outperforms state-of-the-art multi-task decoding models (11% gain on average), while also offering valuable neuroscientific insights into neural functional correlations across tasks. The code will be made publicly available.", "tldr": "", "keywords": ["Brain-computer Interfaces", "Brain Foundation Model", "Large Language Model", "Multi-task Learning"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d0dded13a325544f9b9f32896386a905e82926d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes UniMind, a general-purpose EEG foundation model for unified multi-task brain decoding, leveraging large language models to interpret complex neural patterns. The key idea is to bridge EEG signals and LLMs through a Neuro-Language Connector and a Task-aware Query Selection module. Extensive experiments on 10 EEG datasets spanning 5 domains demonstrate that UniMind achieves state-of-the-art performance in multi-task settings and even matches or surpasses leading single-task models like LaBraM in several cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The manuscript is logically structured, with intuitive figures that clearly illustrate the model pipeline and experimental design.\n2. Evaluation on 10 datasets, along with ablations on NLC, TQS, and query configurations, provides solid empirical support. The visualizations of task-adaptive queries and inter-task correlations are particularly insightful.\n3. UniMind achieves comparable or even superior results to the best single-task model (LaBraM) in a unified multi-task setting, which is a clear technical and conceptual advancement."}, "weaknesses": {"value": "1. The current experiments use only InternLM2.5 as the LLM. Since the LLM plays a central role in cross-modal reasoning, it would be valuable to test the generality of UniMind with other open-source backbones such as Qwen3 or LLaMA 4. This would help assess how much performance depends on the underlying language model.\n2. Figure 4 examines query pool sizes on only five datasets. It would strengthen the claim if this analysis were extended to all 10 datasets or at least one representative dataset per task domain, since optimal query size might vary with task heterogeneity.\n3. In Figure 3, UniMind surpasses LaBraM, and Figure 5 shows that joint training benefits each single task. It remains unclear whether UniMind's improvement mainly stems from joint training synergy or from the TQS/NLC architecture itself. A controlled conparison where UniMind trained independently on each task would clarify this.\n4. The paper mentions constructing 929k instruction–EEG pairs, but does not discuss the diversity or linguistic quality of these prompts. Since task instructions may influence LLM behavior, providing examples and ablations on prompt formulation could strengthen the claims."}, "questions": {"value": "1. How sensitive is UniMind's performance to the choice of the LLM? Could a smaller or instruction-tuned LLM (e.g., Qwen3-0.6B) achieve similar results?\n2. Have you explored whether UniMind can zero-shot transfer to unseen EEG tasks or datasets without retraining, given its multi-task instruction-tuned nature?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jHW79FRd8C", "forum": "NbxuP8NIk8", "replyto": "NbxuP8NIk8", "signatures": ["ICLR.cc/2026/Conference/Submission5659/Reviewer_9xEi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5659/Reviewer_9xEi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760767154930, "cdate": 1760767154930, "tmdate": 1762918180004, "mdate": 1762918180004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed UniMind, a general-purpose EEG foundation model for unified multi-task brain decoding, is quite interesting and attractive. Its  novel idea of Neuro-Language Connector and task-adaptive query scheme directly interface EEG signals with LLMs is new and interesting.\nResults across 10 datasets demonstrate that UniMind outperforms other EEG foundation models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The general EEG foundation model topic itself is emerging and attractive, with many potential applications; \n2. The proposed UniMind model is novel;\n3. Results are promising."}, "weaknesses": {"value": "I noted an earlier version of this paper on arXiv (back to June 2025: https://arxiv.org/abs/2506.18962\nThe content significantly overlaps with the current version. (I would say pretty much the same.)\nI am curious what happened to the earlier version. \n\nFor the performance comparison, only Balanced Accuracy results were reported; while other performance metrics (e.g., F1, Kappa etc) were also reported in related refs. E.g., even the earlier arXiv version reported F1 results. Any reason for this?"}, "questions": {"value": "From table 1, it seems that the model size (e.g., UniMind-1.7B and UniMind-7B) doesn't matter much. Any comment on this? \nAlso, what is the model size of NeuroLM-B, L, and XL? \nWhat are the language models in Uni-Mind and NeuroLM? Are they the same? Could the performance difference be also due to the language models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DEJjPJ5d8p", "forum": "NbxuP8NIk8", "replyto": "NbxuP8NIk8", "signatures": ["ICLR.cc/2026/Conference/Submission5659/Reviewer_rV12"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5659/Reviewer_rV12"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882323978, "cdate": 1761882323978, "tmdate": 1762918179750, "mdate": 1762918179750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents UniMind, a general-purpose foundation model for multi-task EEG decoding. The core innovation is to use a large language model to unify diverse brain decoding tasks within a single framework. The authors propose two key components: first, a Neuro-Language Connector (NLC) to map EEG into representations the LLM can process, and second, a Task-aware Query Selection (TQS) module to select relevant query tokens for the various tasks. The model is trained on a large 'instruction tuning' dataset spanning 10 public EEG benchmarks. The authors report state-of-the-art results for multi-task decoding, with an 11% average gain over the main baseline, NeuroLM."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clearly written, and the proposed architecture is well-explained and is well-reasoned for the purpose of multi-task decoding.\n- The research is high-effort and the presentation is polished, with clear figures and tables.\n- The model achieves a substantial performance improvement over the primary multi-task baseline (NeuroLM), with particularly large gains on difficult datasets like SEED-IV and TUEV."}, "weaknesses": {"value": "- **The paper's core motivation appears contradictory**. The authors rightly state that multi-task learning is challenging and that their model is theoretically at a disadvantage compared to specialized single-task models. This is confirmed by some of the results, where the unified model can be less performant than the finetuned Labram baseline. To me, this begs the main question: what is the use-case? Why would someone adopt a 7B-parameter model that is computationally demanding and less accurate, simply for the benefit of it also performing unrelated tasks? The paper fails to articulate a compelling answer.\n\n- **Missing baselines and architectural justification** 1/2. I'm finding it difficult to assess the work's contribution due to the way it appears to be benchmarked. The work is a direct followup to NeuroLM and integrates parts from LaBraM, two works which share the authors. In case the current work is performed by the same research group, then the primary baselines, LaBraM and NeuroLM, come from the same lab, making the comparisons feel incremental. In this case, the authors do not compare against any external EEG work published in the last ~2 years (since BIOT if I see correctly). A model like e.g. CBraMod would be a valuable addition. \n\n- 2/2 But perhaps more importantly, the paper omits a class of computationally cheaper EEG-language alignment methods, such as those using contrastive learning (e.g., EEG-CLIP https://doi.org/10.48550/arXiv.2503.16531;  ELM-MIL; https://doi.org/10.48550/arXiv.2409.07480). The authors never justify why their complex, generative 7B LLM approach is necessary for what are ultimately classification tasks. A simpler contrastive model, which aligns EEG and text in a latent space, could be a far more efficient and practical solution, but this alternative is not explored or even discussed.\n\n- **Confounding data overlap with task synergy**: The model is trained on datasets from shared corpuses (e.g., TUAB, TUEV, TUSL are all from the TUH EEG corpus; SEED and SEED-IV are from the same lab). The authors interpret query similarity between TUAB and TUEV (Fig 7a) as capturing \"consistent neural activation patterns\". A far simpler and more likely explanation is that the model is just seeing data from the same source, subjects (e.g. TUAB and TUEV may share subjects), and recording setups. This interpretation is further weakened by Figure 6a, where the t-SNE plots show clear separation between datasets, suggesting the TQS module is learning to isolate tasks, not find shared representations."}, "questions": {"value": "1a. Can you justify the generative 7B LLM architecture over a simpler, cheaper contrastive alignment (e.g., EEG-CLIP, ELM-MIL)? \n\n1b. Have you tried a contrastive baseline with your synthetic instruction data?\n\n2. The TUEV benchmark is a 6-class event classification task (e.g., spike, eye movement, artifact). However, the instruction templates provided in Appendix A.6 for TUEV are exclusively focused on \"epileptic events\" and \"epileptic states\". How can the model make significant gains on TUEV and correctly classify non-epileptic events (artifacts, eye movements) when the prompt only asks for \"epileptic states\"?\n\n3.  How can you be sure your \"task synergy\" is not just a data-level artifact of using multiple datasets from the same TUH and SEED corpuses? A baseline (e.g., LaBraM, but ideally another model like Cbramod) fine-tuned on the combined datasets would be needed to disentangle this. Currently, the baselines are inadequate in my opinion.\n\n4. Given that the multi-task model is much, much larger and sometimes less accurate than the single-task model, what is the practical use-case for UniMind over a set of smaller, more accurate specialists?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bhCszeImDv", "forum": "NbxuP8NIk8", "replyto": "NbxuP8NIk8", "signatures": ["ICLR.cc/2026/Conference/Submission5659/Reviewer_jZMk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5659/Reviewer_jZMk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919204821, "cdate": 1761919204821, "tmdate": 1762918179499, "mdate": 1762918179499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UniMind, a general-purpose EEG foundation model designed to perform unified multi-task brain decoding across heterogeneous EEG datasets without task-specific fine-tuning. The core idea is to leverage large language models (LLMs) to interpret neural signals by transforming EEG data into representations compatible with LLM input spaces. The authors propose 1. Neuro-Language Connector: A dual-branch spatiotemporal cross-attention module that projects EEG embeddings into the LLM’s semantic space, effectively bridging the modality gap between neural signals and textual representations. Additionally, a Task-Aware Query Selection routing mechanism is proposed to promot knowledge sharing across related tasks while reducing interference among dissimilar ones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes Neuro-Language Connector to bridge the representational gap between neural and linguistic modalities. It uses a dual-branch spatiotemporal cross-attention mechanism, projecting EEG signals into the semantic embedding space of LLMs.\n\n2. The Task-Aware Query Selection module achieves a balanced integration of shared and specialized representations. This design enhances cross-task generalization while maintaining task discriminability\n\n3. The authors conduct experiments on ten EEG datasets spanning five cognitive domains."}, "weaknesses": {"value": "1. The novelty and necessity of using NLC and TQS are not sufficiently established. The rationale for using a router-based mechanism and for decoupling spatial and temporal aggregation is only superficially discussed. It remains unclear why these specific architectural designs lead to improved EEG–LLM alignment or mitigate data sparsity. \n\n2. The router-style task conditioning has been explored extensively in multi-task and mixture-of-experts literature, so the contribution risk feels incremental without a clearer theoretical or empirical justification.\n\n3. The performance gains over prior models are relatively modest and often within expected statistical variance. In several datasets, the improvements are smaller than 1–2%, which makes it difficult to attribute success to the proposed modules rather than training noise or dataset differences."}, "questions": {"value": "1. Could the authors provide deeper justification for the design choices of the NLC and TQS? Specifically, what motivates the use of a router-based mechanism and the decoupled spatial–temporal aggregation, and how do these contribute to improved EEG–LLM alignment beyond serving as architectural heuristics?\n\n2. How does the proposed router-style conditioning differ in substance from prior multi-task or mixture-of-experts approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lehyOwCp2t", "forum": "NbxuP8NIk8", "replyto": "NbxuP8NIk8", "signatures": ["ICLR.cc/2026/Conference/Submission5659/Reviewer_U2MG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5659/Reviewer_U2MG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5659/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970155037, "cdate": 1761970155037, "tmdate": 1762918179266, "mdate": 1762918179266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}