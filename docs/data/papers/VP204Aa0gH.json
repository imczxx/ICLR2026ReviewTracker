{"id": "VP204Aa0gH", "number": 21476, "cdate": 1758317989516, "mdate": 1759896919886, "content": {"title": "OmniCode: A Benchmark for Evaluating Software Development Agents", "abstract": "LLM-powered coding agents are redefining how real-world software is developed.\nTo drive the research towards better coding agents, we require challenging bench-\nmarks that can rigorously evaluate the ability of such agents to perform various\nsoftware engineering tasks. However, popular coding benchmarks such as Hu-\nmanEval and SWE-Bench focus on narrowly scoped tasks such as competition\nprogramming and patch generation. In reality, software engineers have to handle\na broader set of tasks for real-world software development. To address this gap,\nwe propose OmniCode, a novel software engineering benchmark that contains a\ndiverse set of task categories, including responding to code reviews, test generation,\nfixing style violations, and program repair. Overall, OmniCode contains 2912 tasks\nin Python, 728 tasks per category.\nIn contrast to prior software engineering benchmarks, the tasks in OmniCode are\n(1) manually validated to eliminate ill-defined problems, and (2) synthetically\ncrafted or recently curated to avoid data leakage issues, presenting a new frame-\nwork for synthetically generating diverse software tasks from limited real world\ndata. We evaluate OmniCode with popular agent frameworks such as SWE-Agent,\ndemonstrating shortcomings on tasks that differ from bug-fixing. For instance,\nSWE-Agent with Gemini 2.5 Flash obtains 14.0 % on test generation and 8.1 %\non fixing style issues. With OmniCode, our aim is to spur the development of\nagents that can perform well across a broader spectrum of software development\nprocesses.", "tldr": "A holistic benchmark for lllm-based software engineering agents consisting of bug-fixing, test-generation, style-fixing and addressing code reviews.", "keywords": ["Code Generation", "Benchmarking", "Large Language Models", "Software Engineering", "Test Generation", "Code Review"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9cd442caf528cabea0f902c2b7e4832156cde41f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces OmniCode, a benchmark for evaluating LLM-powered software development agents across four task categories: bug fixing, test generation, code review response, and style fixing. The benchmark comprises 1,794 tasks spanning Python, Java, and C++, derived from 494 base instances. The authors evaluate popular agent scaffolding frameworks (SWE-Agent and Aider) and find significant performance gaps, particularly in test generation and C++ tasks.\n\nThe idea of extending task types beyond bug-fixing or feature development is timely and addresses a real gap in existing benchmarks. The work builds on (Multi-)SWE-Bench and introduces methods to synthetically generate multiple task types from already collected instances. However, the work has several limitations that weaken its contribution: the pipeline's manual curation process limits scalability to other languages; the experimental evaluation is restricted to a single model family (Gemini); validation of synthetically generated components (bad patches, code reviews) is insufficient; statistical analysis of results is missing; and critical reproducibility information (prompts, code, containers) is not provided in the submission."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Adding code review response, test generation, and style-fixing alongside bug repair usefully widens evaluation beyond SWE-Bench-like tasks and represents a step toward more comprehensive software engineering evaluation.\n2. The multi-bad-patch protocol for test evaluation is a meaningful design choice that ensures generated tests are non-trivial.\n3. Using not only SWE-Bench but also Multi-SWE-Bench enables support for popular languages beyond Python.\n4. The approach of bootstrapping multiple task types from base instances is compelling and enables large-scale automation.\n5. The overall structure is easy to follow."}, "weaknesses": {"value": "1. The paper lacks ablations and additional analyses to validate the synthetic data generation pipeline. Since synthetic data is a major component, it may be sensitive to different prompts, introduce leakage, or create unsolvable tasks. Specifically:\n  - The paper states that code reviews are \"informative but do not give away the complete solutions\" but provides no rigorous evaluation of review quality, realism, or usefulness.\n  - No validation is provided for the bad patches, are they realistic failure modes that strong models would produce?\n2. The primary evaluation is quite limited. While SWE-Agent and Aider can be considered representative examples of current SWE scaffolding approaches, evaluating only with Gemini 2.5 Flash is insufficient. Comparison with leading open-source models (Qwen3-Coder/GLM-4.x/Kimi-K2) and ideally frontier models (gpt/sonnet) would provide a more comprehensive understanding of how modern LLMs perform on the proposed benchmark.\n3. SWE agents' performance can vary significantly across runs, especially on small subsets like the 44 C++ instances for test generation. Reporting confidence intervals or standard errors would provide stronger statistical evidence for the performance claims.\n4. The conclusions lack detail. For example, the paper states \"we observe that it struggles at C++ tasks as well as Test-Generation across languages\" but provides minimal investigation into underlying causes. What specific patterns emerge in test generation failures? What types of C++ bugs are most challenging?\n5. The observation that reviews help for Java/C++ but hurt for Python is interesting but remains unexplained. The speculation about \"distraction\" lacks empirical support and appears speculative.\n6. While the appendix is referenced, critical details for reproducibility are missing from the provided excerpt, so it seems that appendix itself is missing.\n  - The actual prompts used for each task type are not included\n  - The bad patch generation prompt is referenced but not shown\n7. [Minor] The paper contains a series of typos, e.g., guage -> gauge, incomplete sentence in Sec. 5"}, "questions": {"value": "1. What specific patterns emerge in test generation failures? Are agents failing to understand the bug, unable to construct proper test syntax, or missing edge cases?\n2. The formula (ΔFiles + Hunks + AddedLines + RemovedLines)/10 lacks theoretical or empirical justification. Could you explain how this formula was derived? Different components seem to have vastly different scales (e.g., ΔFiles typically ranges from 1-10 while AddedLines can be in the hundreds), which means their contributions are not balanced.\n3. Is there a correlation between your complexity metric and task resolution rate? Does the metric actually predict difficulty?\n4. The submission does not specify whether containers, prompts, bad patches, reviews, and evaluation scripts will be publicly released. Reproducibility depends critically on these artifacts. Which components do you plan to release?\n5. How many instances were rejected during manual validation and for what reasons? What is the inter-annotator agreement if multiple annotators were involved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u4fkyuPeAY", "forum": "VP204Aa0gH", "replyto": "VP204Aa0gH", "signatures": ["ICLR.cc/2026/Conference/Submission21476/Reviewer_8Gg7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21476/Reviewer_8Gg7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824697180, "cdate": 1761824697180, "tmdate": 1762941798009, "mdate": 1762941798009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "OmniCode aims to propose a benchmark for evaluating LLM coding agents by focusing on a range of real-world SE tasks. In particular, the authors consider bug fixing, test generation, code review, and style fixing into a single, manually validated benchmark. However, the benchmark itself suffers from some critical weaknesses: the design for some of the tasks is flawed (discussed below). While OmniCode is a valuable prototype, it currently lacks the rigor to fully assess the nuanced capabilities of coding agents across SE tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. With growing interest and evolving design of coding agents, the setup in OmniCode presents a comprehensive way to evaluate LLM capabilities beyond over-engineered, task-specific solutions.\n\n2. OmniBench allows for vital cross-language analyses, providing sufficient scale and diversity across the four SE tasks."}, "weaknesses": {"value": "I am most not convinced by the design for some of the tasks.\n1. Code review: Real-world code reviews often contain discussions centering high-level, systemic reasoning; sometimes performance optimizations; or even code deduplication. By limiting the task to merely \"generate instructions\" to fix bad code, the benchmark is significantly simplified.\n\n2. Code style: An ideal design should challenge the agent's udnerstanding of idiomatic langauge features and style choices that enhance maintainability and readability. The current setup lacks the depth to measure these aspects, and are now a simple measure of an agent's ability to apply automated linting rules.\n\n3. Test generation is brittle: While the \"bad patches\" strategy is interesting, its effectiveness relies entirely on the quality, diversity, and plausibility of those incorrect patches. An ideal test should focus on testing boundary conditions or invariants."}, "questions": {"value": "1. Did the authors assess consistency of agent performance across tasks, i.e., does good performance on bug fixing predict success in code review responses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HGHuREXJsf", "forum": "VP204Aa0gH", "replyto": "VP204Aa0gH", "signatures": ["ICLR.cc/2026/Conference/Submission21476/Reviewer_SNPc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21476/Reviewer_SNPc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958819978, "cdate": 1761958819978, "tmdate": 1762941797757, "mdate": 1762941797757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose OmniCode a code agents benchmark combining instances from SWE-Bench and Multi-SWE-Bench with recently mined new instances. The data sets are enhanced by using LLMs to create new task types from existing instances. More precisely, the authors add three tasks to the standard \"issue resolving\" task: 1) Test generation, 2) responding to code review, and 3) code style application. For 2) and 3) an LLM is used to create bad patches which are related to the ground truth patch but do not solve the task at hand. Any test generated for the test generation task has to fail for the bad patches and pass for the ground truth patch. Bad patches are also used to create \"code reviews\" where an LLM is tasked to generate a review of a bad patch with knowledge about the ground truth. The task is then to \"respond\" to the review to fix it and arrive at the ground truth solution. In the experimental section, the authors compare both the Aider and SWE-agent scaffold with Gemini 2.5 Flash and demonstrate varying performance over the different tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* OmniCode combines multiple important tasks over multiple languages. In particular the latter is important but oftentimes overlooked. I'd love to see the authors to expand the supported languages (e.g., with JavaScript and TypeScript instances from other benchmarks that offer verified splits). \n* The manuscript is well written and easy to understand. Visualizations clearly convey the key results and experimental findings, making it straightforward to follow the authors' analysis and conclusions."}, "weaknesses": {"value": "* Bad patch generation: A bad patch is defined as one that doesn't pass the golden tests. If the golden tests are too narrow or too permissive bad patches may reflect these shortcomings and the code review task would be affected by this as well.\n* Quality and solvability. LLMs are used for patch and review generation. Especially since the latter builds on top of the first LLM results, the risk for decreased quality and potential impacts on solvability multiplies (LLMs on LLMs). In general it is of limited usefulness for the community to develop benchmarks based on LLM-generated inputs. If we want the agents to support humans, inputs should come from humans. Despite being trained on human preference data, typical LLMs will not write the same patches or review messages as an SDE or VibeCoder. On a larger scale this may eventually harm the field as we measure performance of coding agents on inputs that are not from the same distribution in which we'd like to use them."}, "questions": {"value": "* You are already combining instances from SWE-Bench and Multi-SWE-Bench, is there a reason why you don't add instances from another verified multi-language data set like SWE-PolyBench[1]?\n* Did you verify that the test cases for a given could lead to false positives (or even false negatives)?\n\n1. Rashid, M. S., Bock, C., Zhuang, Y., Buchholz, A., Esler, T., Valentin, S., ... & Callot, L. (2025). SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents. arXiv preprint arXiv:2504.08703."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Wfm2fBPaEN", "forum": "VP204Aa0gH", "replyto": "VP204Aa0gH", "signatures": ["ICLR.cc/2026/Conference/Submission21476/Reviewer_Bh5q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21476/Reviewer_Bh5q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972986742, "cdate": 1761972986742, "tmdate": 1762941797344, "mdate": 1762941797344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OmniCode, a new benchmark designed to evaluate LLM-powered software development agents beyond the narrow scope of existing benchmarks like HumanEval and SWE-Bench. The authors argue that real-world software engineering involves a more diverse set of tasks. OmniCode addresses this gap by providing 1,794 tasks across three programming languages (Python, Java, C++) and four key task categories: bug fixing, test generation, responding to code reviews, and fixing style violations.\n\nHowever, the paper's writing looks incomplete. The appendix section still has placeholder text instead of actual content. I also didn't see any specific details in the supplementary materials."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a clear and widely recognized gap in the field. Current benchmarks focus heavily on bug fixing or single-function generation. OmniCode provides a much-needed holistic benchmark that covers a wider, more realistic spectrum of the software development lifecycle, including testing, code review, and style adherence\n\n- The paper provides a strong set of baseline experiments. The results are insightful, such as the clear identification of Test Generation as a major weakness for current agents and the nuanced finding that code reviews help on complex Java/C++ tasks but may hurt performance on simpler Python tasks.\n\n-"}, "weaknesses": {"value": "-  The paper's writing looks incomplete. The appendix section still has placeholder text instead of actual content. I also didn't see any specific details in the supplementary materials.\n\n- The robustness of the \"Test Generation\" and \"Code Review\" tasks hinges on the quality of the synthetic \"bad patches\" and \"review reports.\" The paper details how these are generated (e.g., using weaker agents or LLM-based perturbation), but a more in-depth qualitative analysis of their diversity and realism would strengthen the paper. For instance, how do we know the \"bad patches\" cover a truly diverse set of realistic human errors?\n\n- The evaluation for the \"Code Style\" task focuses on quantifying the reduction of linter-reported issues using a specific score. However, it is not explicitly stated whether the project's functional test suite is run after the style fix. A good style fix should not introduce functional regressions, and this would be a valuable check to include for a more robust evaluation."}, "questions": {"value": "Regarding the \"Code Style\" task: Did the evaluation process involve running the functional test suite after an agent applied a style fix? It seems critical to verify that the agent did not introduce functional regressions while refactoring the code to resolve style violations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fvEtAhcKma", "forum": "VP204Aa0gH", "replyto": "VP204Aa0gH", "signatures": ["ICLR.cc/2026/Conference/Submission21476/Reviewer_S6pD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21476/Reviewer_S6pD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21476/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973340797, "cdate": 1761973340797, "tmdate": 1762941797086, "mdate": 1762941797086, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}