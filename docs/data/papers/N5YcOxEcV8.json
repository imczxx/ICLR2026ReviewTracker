{"id": "N5YcOxEcV8", "number": 1142, "cdate": 1756849729775, "mdate": 1759898225276, "content": {"title": "Benchmarking MLLMs on Topological Reasoning of Chemical Reaction Diagrams", "abstract": "Chemical reaction diagrams are visual representations of complex process graphs, where understanding the overall pathway, including its branches, cycles, and flow, is crucial. While Multimodal Large Language Models (MLLMs) have shown proficiency in recognizing the individual nodes of these graphs, such as molecules and reagents, their ability to perform topological reasoning on the entire structure remains critically underexplored. This creates an urgent need for a targeted evaluation framework to probe this higher-order skill. Fulfilling this need, this paper introduces a systematic benchmark to evaluate this specific capability. We present **ReactBench**, a collection of 1,618 question-answer pairs designed to measure MLLM performance on a hierarchy of tasks, from component recognition to complex topological analysis. Our evaluation of state-of-the-art models reveals a significant deficit: while GPT-4o achieves 79.71% accuracy on node-level identification tasks, its performance plummets to 49.5% on questions that require true topological reasoning about the pathway. By providing the first focused benchmark for this skill, our work establishes a rigorous methodology for diagnosing a key failure mode in MLLMs and guiding the development of models that can comprehend the full, structured processes depicted in scientific diagrams.", "tldr": "", "keywords": ["VQA Benchmark", "Multimodal Large Language Model", "Chemical Reaction Diagram Understanding"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/96ab93e346cd90d0848bcb2df62e7eedcc904e49.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This is a benchmark work evaluating the ability of MLLMs to perceive, understand, and reason over chemical reaction diagrams. \nThe authors introduce ReactBench, a carefully annotated benchmark that encompasses four key aspects of diagram comprehension: element localization, information extraction, connectivity reasoning, and topology analysis.\nBased on experiments, the authors reveal that the reasoning capability (rather than visual perception) is the principal bottleneck limiting current MLLMs’ performance on reaction diagram comprehension."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors clearly define their research scope within the domain of chemical reaction diagrams and provide a well-structured benchmark for systematic evaluation. \n2. The dataset is carefully annotated and covers multiple dimensions of diagram comprehension. \n3. The paper is well written, logically organized, and easy to follow."}, "weaknesses": {"value": "(see questions below)"}, "questions": {"value": "In Section 5, the authors design experiments to demonstrate the bottleneck that limits diagram comprehension. They introduce a JSON-formatted “External Knowledge (EK)” input that includes: (1) a list of bounding boxes indicating the locations of diagram elements, and (2) a list of triples describing reaction relationships. \n\nThe authors describe EK as a form of ground-truth perception. However, I have some concerns here. Most current VLMs have not been trained for explicit visual grounding or object detection, which means they may not actually understand the location information encoded in bounding boxes. \n\nFrom this perspective, I’m not entirely convinced that EK enables “perfect perception” (as claimed around line 473). Instead, introducing clearer semantic cues (such as molecule names or subscript values) might serve as more meaningful external knowledge to support reasoning. \n\nI’d appreciate if the authors could further elaborate on this point. Thanks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A5GwzHdiHq", "forum": "N5YcOxEcV8", "replyto": "N5YcOxEcV8", "signatures": ["ICLR.cc/2026/Conference/Submission1142/Reviewer_QNNg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1142/Reviewer_QNNg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744514878, "cdate": 1761744514878, "tmdate": 1762915688703, "mdate": 1762915688703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ReactBench, roughly 1.6k expert-annotated QA items from about 1.3k reaction diagrams, designed to probe four skills -- spatial localization, information extraction, pathway connectivity, and structural,topological analysis.  Modern MLLMs handle local recognition reasonably well but stumble on global,topological reasoning (e.g., strong on node ID, weak on path,graph structure). CoT and ``external knowledge'' inputs help but don’t close the gap, pointing to a reasoning rather than perception bottleneck.\n\nThe problem is well-motivated and the benchmark could become a useful community resource. Right now, the absence of strong structured baselines, metric robustness, leakage checks, and a clear release,licensing story keep it just below the bar."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1 Clear, timely framing that isolates topological reasoning in chemical diagrams—an under-explored capability gap.  \n\nS2 A hierarchical task design that feels diagnostic rather than one-shot leaderboard gaming.  \n\nS3 Broad model coverage with qualitative failure analyses that are intuitive and instructive.  \n\nS4 Sensible diagnostics (CoT, JSON parsing, “external knowledge” ablations) to separate perception from reasoning."}, "weaknesses": {"value": "W1 Heavy reliance on exact-match scoring likely undercounts semantically correct answers; consider stronger normalization and semantic equivalence.  \n\nW2 Some tasks blur topology with pure extraction; tighter isolation (e.g., graph-only synthetic schematics) would sharpen the causal claims.  \n\nW3 Unclear dataset release,licensing plan given literature,patent sources; impact will be limited without a credible release.  \n\nW4 No analysis of potential pretraining leakage or near-duplicate filtering, which weakens generalization claims.  \n\nW5 Results lack uncertainty estimates (CIs), significance testing, or per-item difficulty analysis.  \n\nW6 Missing strong graph-first baselines (e.g., OCSR → reaction-graph → algorithmic queries) to contextualize MLLM gaps.  \n\nW7 Minor inconsistencies,clarity nits in tables,averages and terminology that make the results harder to parse."}, "questions": {"value": "1. What is the concrete release plan (images, QA, prompts, scoring scripts) and how are IP,licensing risks handled?  \n\n2. What are the annotation protocols and IAA statistics across task families?  \n\n3. How did you check for train,test leakage against common pretraining corpora and patents?  \n\n4. How robust is the metric under answer normalization or partial credit for correctly traced subpaths?  \n\n5. Can you provide a topology-only synthetic subset to fully decouple OCR,chemistry text from structural reasoning?\n\n### Minor comments\n1. Fix minor typos (e.g., Temerature), normalize terms like “Multiple line(s),” and standardize fonts,spacing in figure tables."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6q0gqqaM8d", "forum": "N5YcOxEcV8", "replyto": "N5YcOxEcV8", "signatures": ["ICLR.cc/2026/Conference/Submission1142/Reviewer_TMog"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1142/Reviewer_TMog"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983497951, "cdate": 1761983497951, "tmdate": 1762915688578, "mdate": 1762915688578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ReactBench, a benchmark for evaluating the topological reasoning of Multimodal Large Language Models (MLLMs) on chemical reaction diagrams. It consists of 1,618 question-answer pairs across four aspects. The authors evaluate several state-of-the-art MLLMs to explore their ability to integrate visual and chemical knowledge.  While the benchmark itself is a valuable contribution, the paper needs a substantial revision before it can be accepted."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The released ReactBench benchmark might be extremely useful for future research on capabilities of Multimodal LLMs in understanding chemical reactions.\n\n2. The experimental part of the paper covers a wide range of current Multimodal LLMs, serving as a useful benchmark for future research on chemical tasks.\n\n3. Additional experimental analysis has revealed that even with explicitly provided structured data, MLLMs are not able to achieve accuracy above 65%."}, "weaknesses": {"value": "1. Some of the key premises of the paper are not supported with any evidence. Specifically, no supporting references for the claims on weaknesses of OCSR methods are provided (Lines 87–92).\n\n2. Vague description of the collected dataset hinders the reproducibility of the results as well as the generalization of the proposed data collection methodology. Specifically, no annotation guidelines as well as annotators' qualification are reported. The same omission applies to inter-annotator agreement.\n\n3. The abstract defines the topological reasoning as the primary exploration target. However, the paper lacks a single-modal LLM baseline provided with a ground-truth textual representation of the graph (e.g., in SMILES or SELFIES). If a unimodal LLM (given ground-truth textual representation, e.g., SMILES) outperforms all MLLMs, it would suggest that the bottleneck lies not in chemical reasoning, but in either visual parsing or cross-modal alignment. At this point, the quantitative metrics are hard to interpret as it's not clear whether the provided metrics are high or low compared to uni-modal reasoning. \n\n4. The paper is hard to follow for a broad audience not familiar with chemistry. The examples in Figure 2 require more detailed explanations on why Product 2 does not count towards end products. Poor quality of illustrative figures (e.g., Figures 2, 4) in terms of readability makes the paper even harder to understand.\n\n5. The usage of solely exact strict string matching may underestimate reasoning capabilities. A surface form variation (e.g., \"2\" vs \"two\") or a correct but slightly paraphrased entity name would be penalized as a complete failure, potentially underestimating a model's partial understanding. The inclusion of a soft metric (e.g., ROUGE, BLEU) or LLM-as-a-judge for a subset of free-form answers would give a broader view of the results.\n\n6. The paper does not provide the names of the chemical literature and patent database used for data collection (line 181-183) which strongly reduces the usability and reliability of the proposed dataset."}, "questions": {"value": "* Line 087-092: The claimed weaknesses of OCSR methods are not supported with any references.\n\n* Line 094: \"existing benchmarks\" - Please specify the benchmarks?\n\n* Line 242-242: \"The annotation process involves several iterative rounds of cross-checking\" - Did you measure inter-annotator agreement?\n\n* Line 183-187: - please specify what is implied under incomplete mechanisms, ambiguous visual representations. \n\n* To what extent does the strict exact-match evaluation protocol impact the absolute performance scores, particularly for the \"Structural Topology Analysis\" tasks where answers might be more conceptual (e.g., \"linear chain with a branch\")? Did you experiment with a less strict evaluation protocol?\n\n* In Table 2, the \"Average\" is an arithmetic mean over four tasks with vastly different numbers of questions (e.g., Element Localization has 835, Reasoning has 167 samples). The weighted average would provide a more representative summary statistic, or at least a footnote clarifying the calculation.\n\n* Line 424-425: Provide annotation details and explain what is implied under ground-truth structured data.\n\n* The paper would benefit from a broader discussion on how the released dataset aligns with real-world data and practical applications.\n\n* Line 452-455: The analysis seems to slightly contradict the observed findings: the majority of the models is said to show Analysis performance decline while 2 of 3 models show the accuracy increase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vvysCi1dqF", "forum": "N5YcOxEcV8", "replyto": "N5YcOxEcV8", "signatures": ["ICLR.cc/2026/Conference/Submission1142/Reviewer_eygh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1142/Reviewer_eygh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1142/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993586726, "cdate": 1761993586726, "tmdate": 1762915688472, "mdate": 1762915688472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to Reviewer eygh W5,Q5 & Reviewer TMog W1,Q4:  Evaluation under exact match and LLM-as-a-judge"}, "comment": {"value": "Thank you for these thoughtful comments on our evaluation protocol. We agree that strict exact-match scoring can in principle underestimate a model’s reasoning ability, and we have clarified both its limitations and our mitigation steps.\n\n**(1) Strict exact matching and our current setup, including topology questions.**  \nFirst, for the “Structural Topology Analysis” questions, the answers are multiple choice rather than free-form conceptual descriptions, so strict matching does not penalize paraphrases. A typical example is:\n\n> The diagram illustrates a reaction mechanism. Which term classifies the structure of the reaction pathway?  \n> A) Single line  \n> B) Multiple line  \n> C) Tree  \n> D) Graph  \n>  \n> **Format Rules:**  \n> - Reply only with the letter (A/B/C/D).\n\nIn this setting, the model only needs to output one of {A, B, C, D}. There is no risk that a correct conceptual answer like “linear chain with a branch” is marked wrong because of wording, since the evaluation is on the letter choice, not the phrase itself.\n\nFor many other questions, especially numeric ones, we also explicitly restrict the output format to minimize surface-form variation. For example, for numeric questions we use rules such as:\n\n> Rules: (1) Reply only with the number (e.g., 80); (2) No symbols, units, or words.\n\nUnder this setup, we manually inspected the outputs and did not observe typical cases like `2` vs `two` or clearly correct entities that are only slightly paraphrased but marked wrong. Therefore, in our current experiments the practical impact of exact matching is limited, although we fully acknowledge that exact-match scoring is conservative and may underestimate partial understanding in principle. We thank the reviewers for pointing this out.\n\n**(2) Additional experiment: LLM-as-a-judge with free-form answers.**  \nTo directly probe semantic equivalence, we added a complementary evaluation protocol where we:\n\n- remove the strict answer-format rules and allow free-form answers  \n- use an external LLM-as-a-judge (Gemini-2.5-Flash) to decide correctness based on semantics\n\nFor example:\n\n- **Question:** “What is the percentage yield of the step from 5 to 4 in the diagram?”  \n- **Expected answer:** `98`  \n- **Model answer:** “The percentage yield of the step from 5 to 4 is 98%.”  \n- **Judge verdict:** correct.\n\nIn this setting, semantically correct but non-exact surface forms are counted as correct.\n\n**(3) Effect on quantitative results under both evaluation settings.**  \nWe report results for Qwen2.5-VL-3B and Qwen2.5-VL-7B on the four tasks (Localization, Extraction, Reasoning, Analysis), together with both the simple arithmetic average and a question-count weighted average. \n\nWe compare:\n- exact-match with constrained answers (original evaluation)  \n- LLM-as-a-judge with free-form answers (new evaluation)\n\n| Model          | Eval protocol                        | Localization | Extraction | Reasoning | Analysis | Arithmetic avg | Weighted avg |\n|----------------|--------------------------------------|-------------:|-----------:|----------:|---------:|---------------:|-------------:|\n| Qwen2.5-VL 3B  | Exact match, constrained answers     | 24.43        | 89.61      | 49.70     | 34.65    | 49.60          | 44.99        |\n| Qwen2.5-VL 3B  | LLM-as-judge, free-form answers      | 36.40        | 83.33      | 70.06     | 25.25    | 53.76          | 50.49        |\n| Qwen2.5-VL 7B  | Exact match, constrained answers     | 37.96        | 85.75      | 66.47     | 39.11    | 57.32          | 53.28        |\n| Qwen2.5-VL 7B  | LLM-as-judge, free-form answers      | 54.01        | 87.44      | 87.42     | 42.57    | 67.86          | 64.58        |\n\nHere, the arithmetic average is the simple mean over the four tasks. The weighted average is computed by weighting each task score by its number of questions and dividing by the total number of questions.\n\nWe observe that:\n\n- reasoning scores increase substantially under the LLM-as-a-judge metric (for example, from 49.70 to 70.06 for 3B and from 66.47 to 87.42 for 7B), confirming that a semantic metric recovers some answers that were previously counted as incorrect  \n- both arithmetic and weighted averages increase for both models (for example, the weighted average for Qwen2.5-VL-7B increases from 53.28 to 64.58)  \n- the relative ranking between 3B and 7B remains unchanged, and the performance gaps are still large\n\nThis suggests that our original exact-match evaluation is conservative but does not distort the relative comparison between models. The LLM-as-a-judge results provide a more permissive semantic view that is consistent with our main conclusions."}}, "id": "OOD4royFbi", "forum": "N5YcOxEcV8", "replyto": "N5YcOxEcV8", "signatures": ["ICLR.cc/2026/Conference/Submission1142/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1142/Authors"], "number": 17, "invitations": ["ICLR.cc/2026/Conference/Submission1142/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763700030325, "cdate": 1763700030325, "tmdate": 1763700119179, "mdate": 1763700119179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}