{"id": "rSv6zRyfQf", "number": 11984, "cdate": 1758205011620, "mdate": 1763072107956, "content": {"title": "Learning to Interact in World Latent for Team Coordination", "abstract": "This work presents a novel representation learning framework, interactive world latent (IWoL), to facilitate *team coordination* in multi-agent reinforcement learning (MARL). Building effective representation for team coordination is a challenging problem, due to the intricate dynamics emerging from multi-agent interaction and incomplete information induced by local observations. Our key insight is to construct a learnable representation space that jointly captures inter-agent relations and task-specific world information by directly modeling communication protocols. This representation, we maintain fully decentralized execution with implicit coordination, all while avoiding the inherent drawbacks of explicit message passing, *e.g.*, slower decision-making, vulnerability to malicious attackers, and sensitivity to bandwidth constraints. In practice, our representation can be used not only as an implicit latent for each agent, but also as an explicit message for communication. Across four challenging MARL benchmarks, we evaluate both variants and show that IWoL provides a simple yet powerful key for team coordination. Moreover, we demonstrate that our representation can be combined with existing MARL algorithms to further enhance their performance.", "tldr": "", "keywords": ["Multi-agent Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4dc4a33d567c137fc64859e56b99795248744923.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present IWoL, a unified representation learning and communication framework for cooperative multi-agent RL (MARL). The proposed method jointly learns a latent space that captures inter-agent relations and helps in addressing partial observability in MARL. The method features two variants, one with implicit communication and the other with explicit communication at execution/test time. The authors perform experiments over different scenarios/benchmarks, comparing the performance of the proposed method against other baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper features a good discussion of related work in Sec. 2. The experimental protocol seems solid, with the authors performing multiple runs and reporting confidence intervals. The experimental results seem reproducible. The experiments are also extensive, considering several different environments. The experimental results seem to be in favour of the proposed method. The authors also provide an ablation study."}, "weaknesses": {"value": "I think the clarity of the paper could be slightly improved in some parts, particularly in Sec. 4.2.\n\nThe proposed method seems to feature some limitations (deterministic transitions and access to the underlying state at training time).\n\nThe contributions of the paper appear somewhat incremental, as it builds upon and combines previously proposed techniques in the MARL field. Could the authors further clarify if there are key contributions (in terms of the proposed method) besides combining previously proposed techniques? Nevertheless, I believe this should not be a reason to reject the paper alone, as the paper has its own merits."}, "questions": {"value": "- \"(...) we assume a deterministic MDP throughout this work\" - this means the state transition dynamics are deterministic? This is quite a restrictive assumption.\n- \"(...) we introduce a learnable protocol\" - the learnable protocol function P needs to be trained/applied in a centralized fashion, right? I understand that for the implicit world model this is not a problem, but for the explicit world model it may be.\n- line 205, \"Next, the communication protocol block\" -  this corresponds to the red block in Fig. 2, right? If so, please clarify it in the text. I got a bit lost while reading Sec. 4.2.\n- line 214, \"Lastly, the previously produced vectors, e.g., communication message and intermediate embedding, are used as input to policy and value function networks.\" - is this correct? Looking at Fig. 2 it seems that this is indeed the input to the value function, but the input to the policy is z_i^t. Also, z_i^t is only explained around line 244. I suggest the authors explain it earlier in the text to make the discussion clearer.\n- line 247, \"The world decoder reconstructs the agent’s privileged state s^t_i\" - so, this assumes access to the true state of the Dec-POMDP during training? This is quite a restrictive assumption, right? What about reconstructing joint observations?\n- Why would Im-IWoL outperform Ex-IWoL - E.g., looking at Fig. 7 this seems to be the case. Why? I would expect Ex-IWoL to be in advantage in comparison to Em-IWoL since it features explicit communication between the agents at execution time, right?\n\nMinor comments/typos:\n- sentence in line 17 of the abstract starting with \"This representation\" needs to be revised.\n- sentence in line 61 \"if desired, the same backbone can expose as explicit messages at test time.\" is a bit hard to understand.\n- line 197: \"by a self-attention, (...)\" - \"layer\" missing in the sentence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bR3e7yHm3N", "forum": "rSv6zRyfQf", "replyto": "rSv6zRyfQf", "signatures": ["ICLR.cc/2026/Conference/Submission11984/Reviewer_eTsZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11984/Reviewer_eTsZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761648191495, "cdate": 1761648191495, "tmdate": 1762922979692, "mdate": 1762922979692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper develops a technique to learn inter-agent communication for cooperative MARL tasks with a Transformer-based communication architecture and an encoding-decoding loss. \nThe technique requires inter-agent communication during training (centralized training) and is flexible to turn on (“explicit”) or off (“implicit ”) the communication during execution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Overall, the paper is well-written with clarity.\nThe empirical study is systematic and presented in an organized way."}, "weaknesses": {"value": "1. Some key structural assumptions on top of Dec-POMDPs are not explicitly state:\n    - Section 3 introduces the standard Dec-POMDP formulation but additional structures are needed in Section 4, such as agent position and minimum distance for communication (line 236). \n    - Even, agent \"privileged state” (line 247) is critical for training but not defined.\n\n2. The novelty of the proposed method, IWoL, is fairly limited. \n    - Prior works did the learning of communication topology (e.g.,MAGIC,I2C) and used Transformer-based communication modules (CommFormer, TarMAC).\n    - The DecoderW module helps IWoL a lot according to Figure 8. However, such a module seems to be flexible to be a plug-and-play module for any existing prior method. For example, CommFormer/MAGIC in Figure 8 is much better with DecoderW than without. \n\n3. It is unclear what the paper wants to say about implicit vs explicit communication: \n    - Section 4.1 argues that explicit communication is worse than implicit, but the failure example of explicit communication is caused by bandwidth limit and message-corruption, which should equally affect implicit communication (during training).\n    - In principle, explicit should subsume implicit because explicit agents can choose to send null messages. In Table 1,  Im-IWoL outperforms Ex-IWoL, and the paper does not explain this performance difference."}, "questions": {"value": "All my concerns/questions are in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S6yPmrLDWW", "forum": "rSv6zRyfQf", "replyto": "rSv6zRyfQf", "signatures": ["ICLR.cc/2026/Conference/Submission11984/Reviewer_5Kvq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11984/Reviewer_5Kvq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764784110, "cdate": 1761764784110, "tmdate": 1762922979096, "mdate": 1762922979096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel representation learning framework called Interactive World Latent (IWoL) to enhance team coordination in multi-agent reinforcement learning (MARL), which captures both inter-agent relations and task-specific world information while supporting both implicit (message-free) and explicit (message-rich) coordination modes without additional modules. Across four MARL benchmarks, IWoL variants outperform existing MARL baselines, showing strong robustness under incomplete observations and good scalability with increasing agent numbers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes the IWoL framework that jointly captures inter-agent relations and task-specific world information, supporting both implicit (message-free) and explicit (message-rich) coordination modes, avoiding drawbacks of traditional explicit communication\n2. IWoL outperforms existing MARL baselines across four benchmarks with higher success rates or rewards and also show good scalability."}, "weaknesses": {"value": "1. The implicit mode (Im-IWoL) relies on a well-trained latent representation, and its performance may degrade if the training data fails to cover diverse scenarios.\n2. Compared to the simple MARL baseline (MAPPO), IWoL still has slightly higher training overhead, especially in complex tasks like bimanual dexterous manipulation.\n3. The selected baselines are not comprehensive compared to what the authors listed in the related work."}, "questions": {"value": "1. Can the authors show the performance degradation of Ex-IWoL similar to Figure 1?\n2. Can you explain why implicit world latent is needed if the agents do not exchange messages during execution, and how can it keep generalizable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U7xr7hBafb", "forum": "rSv6zRyfQf", "replyto": "rSv6zRyfQf", "signatures": ["ICLR.cc/2026/Conference/Submission11984/Reviewer_CW1E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11984/Reviewer_CW1E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993298865, "cdate": 1761993298865, "tmdate": 1762922978567, "mdate": 1762922978567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Interactive World Latent (IWoL), a representation learning framework for multi-agent reinforcement learning (MARL) that aims to facilitate team coordination. The key idea is to learn a latent representation that jointly captures inter-agent relations and task-specific world information through a communication protocol based on graph-attention mechanisms. The framework supports both implicit (no messages at test time) and explicit (messages fed to policy) modes. The authors evaluate their approach across four MARL benchmarks: MetaDrive, Robotarium, Bi-DexHands, and multi-agent quadruped environments, demonstrating improved performance over several baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Well-motivated problem**: The paper addresses an important challenge in MARL - building effective representations for team coordination under partial observability. The motivating example (Figure 1) effectively demonstrates the fragility of explicit communication under realistic constraints.\n\n2. **Clear presentation**: The paper is generally well-written with clear diagrams that effectively communicate the architectural design.\n\n3. **Comprehensive experimental evaluation**: The experiments cover 10 tasks across 4 different environments, including autonomous driving, swarm robotics, dexterous manipulation, and quadruped coordination.\n\n4. **Strong empirical results**: IWoL variants consistently achieve top or second-best performance across all tasks, with particularly notable improvements on challenging tasks where baselines struggle (e.g., Go1Sheep, Two Catch Underarm).\n\n5. **Thorough ablation studies**: The paper includes ablations examining the role of world representation, incomplete observations, scalability, and latent dimensionality."}, "weaknesses": {"value": "1. **The title is a bit misleading**: \n   - The title \"Learning to Interact in World Latent\" sounds like model-based RL, which is confusing\n   - The term \"interactive world latent\" is also confusing - what makes it \"interactive\" vs just \"world latent\"?\n\n2. **Some design choices are unexplained**: Several important architectural decisions lack justification:\n   - Why use self-attention in the observation encoder?\n   - Why use additive-attention and GumbelSoftmax in the communication protocol?\n   - Why use decentralized value functions for IWoL when other CTDE methods use centralized critics? This seems to give up an advantage of the CTDE framework\n   - How is the interactive world encoder (for Im-IWoL) designed?\n\n3. **The communication protocol is unclear**: The communication mechanism needs clarification:\n   - Does the system execute L rounds of communication before every action? Line 209 mentions \"This mask is a relationship graph that guides a Transformer block that performs L rounds of attention-based message aggregation and refinement\", I didn't fully get the procedure\n   - What is the relationship between the adjacency mask (line 208 and 233) and the topology graph (line 235)? The adjacency mask is computed from agent features, while the topology graph appears to be based on physical distance (d_comm). Do they both refer to G^t?\n   - Besides, does ∥x^t_i − x^t_j∥ ≤ d_comm mean agents can communicate when they are closer than d_comm, or when they are farther? Line 237 mentions that \"d_comm is a minimum distance for communication.\"\n   - How exactly does \"Ex-IWoL\" work? Since computing the relationship graph requires all agents' features, does this mean broadcasting all agent features and messages for L rounds? The notation P: M^0_1 × ... × M^0_I → M_1 × ... × M_I suggests a function of all agents' initial messages, but the architecture description suggests per-agent processing"}, "questions": {"value": "see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u9iK9mR7ET", "forum": "rSv6zRyfQf", "replyto": "rSv6zRyfQf", "signatures": ["ICLR.cc/2026/Conference/Submission11984/Reviewer_nwwV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11984/Reviewer_nwwV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762252865849, "cdate": 1762252865849, "tmdate": 1762922978182, "mdate": 1762922978182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Comment for paper revision"}, "comment": {"value": "Dear AC and all reviewers,\n\nWe express our gratitude to AC for managing the review process and all four reviewers for their insightful feedback.  We are pleased to present the updates we have made in response to valuable suggestions, as detailed below.\n\n- Retitled the paper to `Unifying Agent Interaction and World Information for Multi-agent Coordination` to prevent potential misinterpretations and enhance conceptual clarity\n- Renamed the algorithm from Interactive World Latent to `Interaction-World Latent` for clearer semantic alignment\n- Updated Section 4 to clarify the structure and roles of each component, and to improve readers’ understanding of the overall framework.\n- Added an explanatory sentence in Section 5.2, RQ1 (Line 395) to provide intuition on why Im-IWoL performs better than Ex-IWoL\n- Added three new paragraphs in Appendix C.2 that detail the implementation of each module, offering clearer guidance for reproducibility.\n- Rephrased one line in the abstract (Line 18) to improve readability and reduce ambiguity\n- Rephrased one line at Line 60 to enhance clarity and improve the flow of the introduction\n- Added a clarifying sentence to the problem formulation section (Line 122) to better explain the Dec-POMDP setup and the assumed communication protocol\n- Moved the discussion section from Appendix B to the last section of the main body to provide our limitations and open challenges\n\nWe have left individual comments to address each reviewer’s questions, concerns, and our misclarification. Please review each individual comment for more details!"}}, "id": "DOBzrSJ0uk", "forum": "rSv6zRyfQf", "replyto": "rSv6zRyfQf", "signatures": ["ICLR.cc/2026/Conference/Submission11984/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11984/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission11984/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763572579882, "cdate": 1763572579882, "tmdate": 1763572918999, "mdate": 1763572918999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}