{"id": "m35gwg11mT", "number": 8463, "cdate": 1758084909669, "mdate": 1759897782487, "content": {"title": "Boosting Large Language Models with Mask Fine-Tuning", "abstract": "The large language model (LLM) is usually kept integral in the mainstream optimization protocol. No works have questioned whether maintaining the integrity of the model is indispensable for promising performance. In this work, we introduce Mask Fine-Tuning (MFT), a brand-new LLM fine-tuning paradigm to show that properly breaking the structural integrity of the model can surprisingly lead to improved performance without model weights update. Specifically, MFT learns and applies a set of binary masks on well-optimized models supervised by the typical LLM fine-tuning objective. Based on full fine-tuned models, MFT uses the same fine-tuning datasets to gain consistent performance boosts across various domains and backbones (e.g., 2.60 / 4.15 average gain in IFEval with LLaMA2-7B / 3.1-8B). Detailed ablations and analyses study the proposed MFT from different perspectives such as sparse ratio, loss surface, etc. Additionally, MFT is compatible for collaborating with other LLM optimization procedures for general model enhancement by deploying it on well-trained models. Further, this study extends the functionality of masking operation from its conventional network pruning context for model compression into a general model capability scope.", "tldr": "", "keywords": ["Sparsity-Aware Training", "Mask-based Fine-tuning", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ae8a75c70876855e1bd5061a03fec99bb594f7c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper points out that existing fine-tuning methods, such as full-model fine-tuning and LoRA, often suffer from issues like overfitting, leading to performance degradation after fine-tuning. This paper proposes a Mask Fine-Tuning (MFT) method, which further improves the model's performance by fine-tuning a binary matrix after the initial model fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written, with clear logic, and it fully conveys the motivation and methodology of the research.\n\n2. The paper conducts extensive experiments, including comparative experiments on data of different types and scales."}, "weaknesses": {"value": "1. First, the paper points out that performing MFT fine-tuning based on \"best fine-tuning\" can improve model performance. However, challenges such as identifying the \"best time point,\" along with the additional computational costs and training data required, hinder the practical application of MFT and increase the overall cost of fine-tuning.\n\n2. Second, MFT is rather heuristic in nature. Currently, we still lack clarity on the rationality of applying MFT after \"best fine-tuning\" and the true reasons behind the resulting model performance improvement. The key factors that influence performance enhancement remain unknown, and it is worth exploring whether an analysis can be conducted from a theoretical or fundamental perspective.\n\n3. Finally, there is the issue of MFT’s generalizability. The models used in existing experiments are limited to those with a scale of less than 8B parameters, and it is questionable whether the experimental conclusions are valid for larger-scale models."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sC42oPNp8n", "forum": "m35gwg11mT", "replyto": "m35gwg11mT", "signatures": ["ICLR.cc/2026/Conference/Submission8463/Reviewer_943M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8463/Reviewer_943M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760604532633, "cdate": 1760604532633, "tmdate": 1762920346378, "mdate": 1762920346378, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **Mask Fine-Tuning (MFT)** — a simple post-training step applied *after* standard supervised fine-tuning (SFT or FFT) of large language models.\nDuring MFT, the model parameters $W$ are **frozen**, and a binary mask $M \\in {0,1}$ is learned using the **straight-through estimator (STE)**.\nThe effective weights are obtained by element-wise multiplication:\n$$\n\\tilde{W} = W \\odot M\n$$\n\nThrough experiments on **LLaMA2-7B** and **LLaMA3.1-8B**, the authors show consistent gains across instruction-following (IF-Eval), math (GSM8K), and code (HumanEval) benchmarks — typically $+2$–$6$ points compared to the best fine-tuned baseline.\nAblations reveal that shallow and late layers benefit most from masking, and visualizations indicate flatter loss landscapes with lower PAC-Bayes bounds, suggesting improved generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* **Elegant simplicity and practicality**\n  The method is conceptually minimal — no new modules or objectives, just a learnable binary mask applied post-SFT. It can be easily integrated into existing fine-tuning pipelines.\n\n* **Systematic ablations**\n  Layer-wise masking, masking ratio, data ratio, and local vs. global masking are all explored. These ablations reveal that masking early and late layers yields the largest gains."}, "weaknesses": {"value": "1. **Limited generality — only LLaMA-based experiments**\n   All experiments are performed exclusively on **LLaMA2-7B** and **LLaMA3.1-8B**.\n   No evidence is provided that the method generalizes to other architectures such as Mistral, Falcon, GPT-NeoX, or encoder–decoder models.\n   The results might exploit architectural features unique to LLaMA (e.g., SwiGLU gating, RMSNorm, rotary embeddings).\n\n2. **Mechanistic opacity**\n   The paper does not clearly explain *why* MFT improves performance.\n\n   * Are the masked connections genuinely redundant or overfitted?\n   * Does masking act as a form of regularization or noise smoothing?\n     The presented PAC-Bayes argument is mathematically valid but does not illuminate the underlying mechanism.\n\n3. **Weak baselines**\n   MFT is compared only to standard fine-tuning and LoRA.\n   There is no comparison to sparsity-based fine-tuning techniques such as Movement Pruning, Diff-Pruning, $L_0$-regularization, or Sparse-FT.\n   Without these, the novelty relative to prior sparsity literature remains unclear.\n\n4. **Lack of statistical robustness**\n   All results are single-seed. Small benchmarks like HumanEval or IF-Eval require multiple seeds or confidence intervals (e.g., bootstrap estimates) to validate significance.\n\n5. **Interpretability missing**\n   There is no visualization or qualitative analysis showing *which* neurons or connections are masked, nor how masking alters attention or activation patterns."}, "questions": {"value": "1. **Cross-architecture validation**\n   Apply MFT to other architectures (e.g., Mistral-7B, Falcon-7B, T5-11B) to verify generality beyond the LLaMA family.\n\n2. **Mechanistic analysis**\n\n   * Visualize layer- and head-level mask distributions.\n   * Measure changes in activation sparsity, gradient norms, or representational similarity before and after MFT.\n   * Distinguish between pruning-like and regularization-like behavior.\n\n3. **Add stronger baselines**\n   Include comparisons to Movement Pruning, Diff-Pruning, and $L_0$-masking under equal compute budgets.\n\n4. **Improve statistical reliability**\n   Report mean ± std over multiple seeds, and possibly provide 95% confidence intervals for HumanEval / GSM8K.\n\n5. **Integrated training variant (future work)**\n   Explore alternating SFT and MFT epochs (e.g., epoch 1 full-SFT, epoch 2 MFT),\n   or a joint objective:\n   $$\n   \\mathcal{L}*{\\text{joint}} = \\mathcal{L}*{\\text{SFT}}(W,M) + \\lambda |M - 1|_1\n   $$\n   to treat masking as a regularization process *during* fine-tuning rather than as a post-hoc step.\n\n6. **Relation to gating, dropout, and LoRA**\n   The authors should explicitly situate MFT within this broader landscape:\n\n   * **Dropout vs. MFT:** both apply multiplicative masks $r$ or $M$, but dropout uses random Bernoulli masks ($r_{ij}!\\sim!\\text{Bernoulli}(p)$) for stochastic regularization during training, whereas MFT learns a *deterministic* binary mask that persists at inference. Hence MFT can be viewed as a “learned deterministic dropout.”\n   * **Gating vs. MFT:** gating mechanisms (e.g., SwiGLU) use *continuous* gates., MFT, in contrast, enforces *hard* selection on edges or neurons, producing structural sparsity rather than soft modulation.\n   * **LoRA vs. MFT:** LoRA modifies the parameter space additively ($W' = W + BA$), introducing low-rank updates that expand the representational subspace. MFT modifies it multiplicatively ($W' = W \\odot M$), effectively contracting the subspace by removing redundant connections. Interestingly, a well-trained LoRA could partially *cancel* certain weight directions ($BA\\approx -W_{\\text{unwanted}}$), producing a masking-like effect. An explicit comparative experiment—measuring cosine similarity between LoRA updates and MFT masks—would clarify whether both methods converge toward complementary adaptation patterns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vxVNf8kywn", "forum": "m35gwg11mT", "replyto": "m35gwg11mT", "signatures": ["ICLR.cc/2026/Conference/Submission8463/Reviewer_uzVQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8463/Reviewer_uzVQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658025606, "cdate": 1761658025606, "tmdate": 1762920345947, "mdate": 1762920345947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Mask Fine-Tuning, a post-training method that improves fully fine-tuned LLMs by learning which parameters to remove rather than updating weights. The key claim is counterintuitive: you can improve a well-trained model by carefully masking out 10% of its parameters in specific layers.  Main results show modest but consistent improvements over the best FFT checkpoint - typically 0.3-6 points depending on the task.  The paper discovers that you can improve models by carefully removing parameters but doesn't explain why this helps, when it will work, or how it differs meaningfully from existing pruning methods. The practical utility is unclear given the modest gains, manual tuning requirements, and cross-domain degradation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The finding that you can improve a converged model by removing parameters (not updating them) challenges common assumptions about neural network training. This is worth investigating.\n\n2. Experimental scope is reasonable, include Two model families (LLaMA2, LLaMA3.1); Three diverse domains (math, code, instruction-following); Multiple training scenarios (domain-specific FFT, mixed-domain FFT)\n\n4. Includes both local and global masking experiments. The global masking results (Table 3) are mostly negative, but including them shows intellectual honesty about where the method fails.\n\n5. Figure 5 suggests MFT moves models to flatter minima, which aligns with better generalization. The visualization is clear and the trend is consistent across domains.\n\n6. Training cost analysis is included. Figure 4 breaks down memory, tokens, and time, making it clear what overhead MFT adds beyond FFT."}, "weaknesses": {"value": "1. The improvements are small and sometimes within noise\n\nLooking at Tables 1-2 with error bars: many gains are 0.3-2 points, and standard deviations often overlap between Best FFT and MFT. For example, LLaMA3.1-8B Math domain shows 77.0±0.88 vs 77.3±0.97 - not convincing. No statistical significance tests are provided to confirm these differences are real.\n\n2. **The distinction from pruning is unconvincing**\n\nThe paper claims to differ from pruning because the goal is \"improvement not compression,\" but technically it's doing the same thing - learning which parameters to remove using training data and gradients. Modern pruning methods like Wanda or SparseGPT also aim to maintain or improve performance while reducing parameters. The conceptual distinction feels forced.\n\nMore damaging: the paper doesn't compare against any actual pruning methods. The baselines are just random masking and L1 magnitude masking (which doesn't even use training). Where's the comparison to gradient-based pruning, lottery tickets, or recent LLM pruning work?\n\n3. Cross-domain results reveal a problem\n\nTables 5-6 show that MFT often hurts performance on non-target domains. For instance, training MFT on math improves GSM8K but degrades HumanEval. This suggests the method may be overfitting to the target domain rather than genuinely improving the model. This contradicts the generalization improvement narrative."}, "questions": {"value": "Q1: Is this just preventing overfitting through capacity reduction? The paper shows continued FFT hurts performance (overfitting) but MFT helps. The obvious explanation: MFT reduces capacity, making overfitting harder. But that's not really \"improvement\" - it's just better regularization than doing nothing. How does MFT compare to :(1) Continued FFT with dropout (2) Continued FFT with stronger weight decay. Including these comparisons would make it clearer whether MFT offers unique benefits beyond standard regularization approaches.\n\nQ2: Why these specific layers? Figure 3 shows different layers work for different settings. What determines this? Is there something about these layers' representations? Their gradient statistics? Their weight magnitudes? The paper identifies which layers work but not *why*, making it hard to apply the method to new models.\n\nQ3: Continued FFT comparison seems unfair. Continued FFT is trained for the full 4 epochs and evaluated at the end (showing degradation). But MFT can choose its best checkpoint within 2 epochs. Wouldn't continued FFT also improve if you picked its best checkpoint from the same epoch range?\n\nQ4: Why does masking ratio vary by domain? Figure 6 shows coding prefers 10% but instruction following prefers lower ratios. What property of the domain determines this? Task complexity? Dataset size? Base model capability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h0bkcjLTRP", "forum": "m35gwg11mT", "replyto": "m35gwg11mT", "signatures": ["ICLR.cc/2026/Conference/Submission8463/Reviewer_B2Fx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8463/Reviewer_B2Fx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710049369, "cdate": 1761710049369, "tmdate": 1762920345505, "mdate": 1762920345505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Mask Fine-Tuning (MFT), a novel post-fine-tuning approach that learns binary masks on already fine-tuned LLMs to further improve performance. The key insight is that removing certain parameters through learned masks can enhance model capability rather than merely maintaining it. The authors validate MFT on LLaMA2-7B and LLaMA3.1-8B across three domains (math, coding, and instruction-following), showing consistent improvements over fully fine-tuned baselines. The method freezes model weights and only learns which parameters to mask out, using the same training objective and datasets as standard fine-tuning. Theoretical analysis via PAC-Bayes bounds and empirical loss landscape visualizations are provided to support the approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A good perspective on model sparsity: The paper presents an interesting conceptual shift by using masking not for compression but for capability enhancement. This counter-intuitive finding that \"subtraction leads to addition\" is thought-provoking and extends the conventional understanding of sparse networks beyond efficiency concerns.\n2. Comprehensive experimental validation: The authors conduct thorough ablations including layer-wise sensitivity analysis (Figure 3), masking ratio studies (Figure 6), and data ratio experiments (Figure 7). The proof-of-concept studies systematically identify which model components benefit most from MFT.\n3. Theoretical grounding: The inclusion of PAC-Bayes generalization bounds (Section 3.3) and Hessian-based loss landscape analysis provides theoretical justification beyond empirical results. The analysis showing that both training loss and model complexity terms decrease is valuable."}, "weaknesses": {"value": "1. **Limited task complexity and diversity:** The evaluation focuses on relatively standard benchmarks (GSM8K, HumanEval, IFEval) that may not fully demonstrate the method's effectiveness on more challenging or specialized tasks. The paper would benefit from:\nMore complex reasoning tasks (e.g., multi-hop reasoning, mathematical proof generation)\nDomain-specific applications (legal document analysis, medical diagnosis, scientific literature understanding)\nLonger-context tasks that stress different model capabilities\n\n2. **Marginal performance gains:** While consistent, the improvements are often modest:\nMany gains are within 1-3 points, raising questions about practical significance\nError bars overlap in several cases, suggesting some improvements may not be statistically significant\nNo discussion of whether these gains justify the additional training phase"}, "questions": {"value": "1. **Generalization to modern training paradigms:** Can you provide any preliminary results or theoretical analysis on how MFT would work with DPO, PPO, or other policy-based training methods? Given the growing importance of RL, this seems critical for practical adoption.\n\n\n2. **Model diversity experiments:** What prevents extending the evaluation to other model families like Qwen? Are there architectural requirements that limit applicability? Results on at least one additional model family would significantly strengthen the claims.\n\n\n3. **Efficiency quantification:** What is the exact wall-clock time overhead of MFT compared to continued FFT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sq50Qkg29K", "forum": "m35gwg11mT", "replyto": "m35gwg11mT", "signatures": ["ICLR.cc/2026/Conference/Submission8463/Reviewer_7LUn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8463/Reviewer_7LUn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8463/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997518346, "cdate": 1761997518346, "tmdate": 1762920344809, "mdate": 1762920344809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}