{"id": "HHx5KFnj8P", "number": 6334, "cdate": 1757969416457, "mdate": 1763610423799, "content": {"title": "Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners", "abstract": "Large Language Models (LLMs) show strong reasoning abilities, often amplified by Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although RL algorithms can substantially improve reasoning, they struggle to expand reasoning boundaries because they learn from their own reasoning trajectories rather than acquiring external knowledge. Supervised fine-tuning (SFT) offers complementary benefits but typically requires large-scale data and risks overfitting. Recent attempts to combine SFT and RL face three main challenges: data inefficiency, algorithm-specific designs, and catastrophic forgetting.\nWe propose a plug-and-play framework that dynamically integrates SFT into RL by selecting challenging examples for SFT. This approach reduces SFT data requirements and remains agnostic to the choice of RL or SFT algorithm. To mitigate catastrophic forgetting of RL-acquired skills during SFT, we select high-entropy tokens for loss calculation and freeze parameters identified as critical for RL. Our method achieves state-of-the-art (SoTA) reasoning performance using only 1.5\\% of the SFT data and 20.4\\% of the RL data used by prior SoTA, providing an efficient and plug-and-play solution for combining SFT and RL in reasoning post-training.", "tldr": "Our method achieves state-of-the-art (SoTA) reasoning performance using only 1.5\\% of the SFT data and 20.4\\% of the RL data used by prior SoTA, providing an efficient and plug-and-play solution for combining SFT and RL in reasoning post-training.", "keywords": ["LLM", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e130c3388f60ec464612380cbeb0b2a557a87d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MIFO (Mitigating Forgetting between SFT and RL), a framework that dynamically integrates supervised fine tuning (SFT) with reinforcement learning (RL) to mitigate high data requirements and overfitting inherent to SFT while expanding reasoning frontiers through judicious use of out of distribution data. A highlight of MIFO is that it overcomes the phenomenon of catastrophic forgetting by-design. Experimental evaluations demonstrate that MIFO, and a variant MIFO$^+$ result in significantly improved data usage compared to state of the art. \n\n**Caveat**: I am not fully familiar with the scope of mathematical reasoning benchmarks and datasets that are used in the evaluation (as mentioned by the authors in Sec. 5.1. My review is based on a presumption that these are adequate; I will defer to the authors/ other reviewers on whether any other benchmark/ dataset can potentially serve as additional datapoints for evaluation of MIFO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(+) Interleaving of SFT with RL is a methodology that is unique relative to prior work. The merits of such an approach are revealed in the fact that it obviates a need for large amounts of data that is typically required for SFT. \n\n(+) MIFO aims to mitigate catastrophic forgetting by design, which is significantly different from approaches in current art. \n\n(+) The results demonstrating the redundancy in SFT relative to RL is particularly insightful, and serves as a strong basis for the working of MIFO in terms of mitigating catastrophic forgetting typical of SFT. \n\n(+) The paper is generally well-written and the logical flow is sound. At the same times, I have some questions about the technical aspects and experiments (please see Weaknesses, below)."}, "weaknesses": {"value": "(-) The central claim by the authors is that MIFO is agnostic to the specific RL or SFT algorithm used (e.g., the claim that an RL algorithm different than GRPO can be used for RL training at the start of Sec. 4.1). However, the experimental evaluations do not seem to suggest that this claim has indeed been tested on using MIFO with multiple RL/ SFT algorithms. \n\n(-) In Fig. 1 right, while the gap does begin to close at around the 110th step as the authors write, it subsequently begins to diverge. The text of the paper does not appear to provide an explanation for this phenomenon. \n\n(-) In Fig. 1, while the gap between the SFT curves closes at the 40th step, and remains close subsequently and the gap between RL curves closes at the 110th step, it is not clear what other factors determine convergence. Also, from the right side of Fig. 1, it is not clear that RL converges even at 350 gradient steps. \n\n(-) The labels on the graph of Fig. 2 do not seem to match with the caption of the figure or the text in Lines 159-161. Perhaps the green curve corresponds to SFT while the blue curve corresponds to RL? \n\n(-) In Tables 1 and 2, it is not clear why MIFO produces shorter length outputs than MIFO$^+$ for the 7B model, while MIFO$^+$ yields significantly shorter length outputs than MIFO for the 1.5B model. Some insight into this result, and intuition about the role of the history parameter $\\alpha$ will help make the interpretation of the results more clear. \n\n(-) Some aspects of the presentation can be improved. For example, in Lines 269-270, the authors write `Entropy describes the uncertainty…’ - the writing will benefit from having a more formal definition of entropy over here. \n\n(-) Minor comment: typo - in Line 194, there is an additional space between ( and Section."}, "questions": {"value": "Please see Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ueNS5GynU4", "forum": "HHx5KFnj8P", "replyto": "HHx5KFnj8P", "signatures": ["ICLR.cc/2026/Conference/Submission6334/Reviewer_Qgmg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6334/Reviewer_Qgmg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761327967833, "cdate": 1761327967833, "tmdate": 1762918627978, "mdate": 1762918627978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MIFO (Mitigating Forgetting between SFT and RL), a framework that dynamically integrates supervised fine tuning (SFT) with reinforcement learning (RL) to mitigate high data requirements and overfitting inherent to SFT while expanding reasoning frontiers through judicious use of out of distribution data. A highlight of MIFO is that it overcomes the phenomenon of catastrophic forgetting by-design. Experimental evaluations demonstrate that MIFO, and a variant MIFO$^+$ result in significantly improved data usage compared to state of the art. \n\n**Caveat**: I am not fully familiar with the scope of mathematical reasoning benchmarks and datasets that are used in the evaluation (as mentioned by the authors in Sec. 5.1. My review is based on a presumption that these are adequate; I will defer to the authors/ other reviewers on whether any other benchmark/ dataset can potentially serve as additional datapoints for evaluation of MIFO.\n\n\n*******\nUpdated overall score after author response.\n*******"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(+) Interleaving of SFT with RL is a methodology that is unique relative to prior work. The merits of such an approach are revealed in the fact that it obviates a need for large amounts of data that is typically required for SFT. \n\n(+) MIFO aims to mitigate catastrophic forgetting by design, which is significantly different from approaches in current art. \n\n(+) The results demonstrating the redundancy in SFT relative to RL is particularly insightful, and serves as a strong basis for the working of MIFO in terms of mitigating catastrophic forgetting typical of SFT. \n\n(+) The paper is generally well-written and the logical flow is sound. At the same times, I have some questions about the technical aspects and experiments (please see Weaknesses, below)."}, "weaknesses": {"value": "(-) The central claim by the authors is that MIFO is agnostic to the specific RL or SFT algorithm used (e.g., the claim that an RL algorithm different than GRPO can be used for RL training at the start of Sec. 4.1). However, the experimental evaluations do not seem to suggest that this claim has indeed been tested on using MIFO with multiple RL/ SFT algorithms. \n\n(-) In Fig. 1 right, while the gap does begin to close at around the 110th step as the authors write, it subsequently begins to diverge. The text of the paper does not appear to provide an explanation for this phenomenon. \n\n(-) In Fig. 1, while the gap between the SFT curves closes at the 40th step, and remains close subsequently and the gap between RL curves closes at the 110th step, it is not clear what other factors determine convergence. Also, from the right side of Fig. 1, it is not clear that RL converges even at 350 gradient steps. \n\n(-) The labels on the graph of Fig. 2 do not seem to match with the caption of the figure or the text in Lines 159-161. Perhaps the green curve corresponds to SFT while the blue curve corresponds to RL? \n\n(-) In Tables 1 and 2, it is not clear why MIFO produces shorter length outputs than MIFO$^+$ for the 7B model, while MIFO$^+$ yields significantly shorter length outputs than MIFO for the 1.5B model. Some insight into this result, and intuition about the role of the history parameter $\\alpha$ will help make the interpretation of the results more clear. \n\n(-) Some aspects of the presentation can be improved. For example, in Lines 269-270, the authors write `Entropy describes the uncertainty…’ - the writing will benefit from having a more formal definition of entropy over here. \n\n(-) Minor comment: typo - in Line 194, there is an additional space between ( and Section."}, "questions": {"value": "Please see Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ueNS5GynU4", "forum": "HHx5KFnj8P", "replyto": "HHx5KFnj8P", "signatures": ["ICLR.cc/2026/Conference/Submission6334/Reviewer_Qgmg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6334/Reviewer_Qgmg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761327967833, "cdate": 1761327967833, "tmdate": 1763746706313, "mdate": 1763746706313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MIFO, a plug-and-play framework to jointly optimize SFT and RL for reasoning post-training of LLMs. The key claim is that SFT introduces redundant and high-magnitude parameter updates that overwrite the more updates of RL, leading to catastrophic forgetting. To address this, MIFO Interleaves SFT into RL, selecting only challenging rollouts and applying loss only on high-entropy tokens. MIFO achieves perfect results on AIME-24/25, AMC, MATH-500, OlympiadBench, and MMLU-Pro, while using only 1.5% of the SFT data and 20.4% of the RL data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Identifies and visualizes the gradient update magnitude between SFT and RL.\n\n2. Consistently gains across different reasoning benchmarks.\n\n3. Solid ablations of complementary effects of entropy-based token selection and parameter freezing."}, "weaknesses": {"value": "1. The freezing and entropy ideas, while effective, are incremental extensions of existing interleaved SFT+RL frameworks (e.g., ReLIFT).\n\n2. All experiments use Qwen-Math models on mathematical reasoning; no evidence of generalization to other domains or other model settings.\n\n3. Limited discussion on compute or runtime overheads."}, "questions": {"value": "1. How sensitive is MIFO to the hyperparameters? \n\n2. Have you tested MIFO with other domains or other non-math models?\n\n3. What is the computational overhead (e.g., GPU hours) of MIFO compared to baselines?\n\n4. Theoretical analysis (Appendix C) is disconnected from practice. The introduced Decision–Redundancy Ratio (DR) is not computed empirically nor related to the actual experiments in main sections. It’s unclear what the analysis truly verifies.\n\n5. Forgetting not quantitatively measured. Figures only visualize parameter update magnitude, not actual forgetting metrics. The claim that MIFO “mitigates forgetting” is weakly supported.\n\n6. The paper lacks any figure showing model performance over training steps (e.g., test dataset performances across training steps). Such curves would clarify whether MIFO actually stabilizes learning rather than just improving final accuracy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GY0F1sf70A", "forum": "HHx5KFnj8P", "replyto": "HHx5KFnj8P", "signatures": ["ICLR.cc/2026/Conference/Submission6334/Reviewer_jiz5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6334/Reviewer_jiz5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761631882364, "cdate": 1761631882364, "tmdate": 1762918627619, "mdate": 1762918627619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MIFO, an interleaved SFT RL post training framework to mitigate forgetting. MIFO mainly consists of two components: data processing to strengthen low accuracy examples for SFT, and parameter freezing to prevent overwriting key parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- MIFO outperforms multiple baselines on math tasks.\n- MIFO improves data efficiency than baselines."}, "weaknesses": {"value": "Weaknesses:\n- The experimental validations mainly focus on math tasks, while other reasoning tasks beyond math are overlooked.\n- Experiments focus on Qwen family, making applicability of MIFO to other model families unclear especially given the observed performance drop under different templates.\n- MIFO relies on experts or a stronger teacher model. The cost associated with it is ignored in validations. \n- Linearized approximation in theoretical analysis in Appendix C needs to be justified.\n- Results in Figure 2 seem contradicting with description in lines 160-161. Please clarify."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rAUuOb1j7u", "forum": "HHx5KFnj8P", "replyto": "HHx5KFnj8P", "signatures": ["ICLR.cc/2026/Conference/Submission6334/Reviewer_p9ox"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6334/Reviewer_p9ox"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803911665, "cdate": 1761803911665, "tmdate": 1762918627206, "mdate": 1762918627206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes MIFO, Mitigating Forgetting Between SFT and RL, a new pipeline to bridge the SFT and RL in post-training of LLM reasoning. The pipeline starts from RL and constructs an SFT data buffer. Then, it uses entropy-based token selection and RL parameter update-based freezing for SFT. The comprehensive experiments on small-scale LLMs show that it can efficiently boost the model's reasoning performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It proposed an interesting view of SFT and RL in post-training of LLM reasoning. The analysis in section 3 provided good motivation for the design of MIFO. And the components of MIFO provide a promising advantage to improve the training of SFT+RL.\n- The experiment as well as ablation study, indicates MIFO is well effective compared to baseline, and provides good data efficiency and token efficiency."}, "weaknesses": {"value": "- The experiment is purely based on qwen 2.5 models and the math domain training dataset. The generalizability of this approach to other domain is tricky. And qwen 2.5 models (even it is the base model) include heavy mid-training data, experiment on these models are more like containing an implicit SFT, which is different from the claimed RL-first-then-SFT paradiam.   \n- Risk of catastrophic forgetting is mentioned as motivation, but not studied/showed how MIFO addressed this.\n- Writing issue: e.g., L323 NuminaMath Li et al. (2024) > NuminaMath (Li et al., 2024)"}, "questions": {"value": "- Though I did not see the code, I think the proposed method will introduce computation overhead due to the (frequent) context switch between SFT and RL. As my question below, the interval matters in this design to balance training performance and training efficiency.  \n- I did not find out MIFO iteration number/interval used for the experiment. And what is the effect of these factor? For example, it can be high high-frequency interval (e.g., every batch), or a low-frequency interval (e.g, every epoch).\n- I am interested in the data buffer dynamics of training. In other words, does the effective buffer get smaller during training, and does the questions get back and forth in the buffer? This information helps the understanding of whether the model learns something new to improve the performance, otherwise it may be more like randomness.  Also, I wonder how often the frozen RL updated parameters overlap with those from high-entropy sft tokens. My intuition is that these are pretty much overlapped, so I did not understand what is updated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SiBvRzn2oK", "forum": "HHx5KFnj8P", "replyto": "HHx5KFnj8P", "signatures": ["ICLR.cc/2026/Conference/Submission6334/Reviewer_RRa6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6334/Reviewer_RRa6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806277189, "cdate": 1761806277189, "tmdate": 1762918626755, "mdate": 1762918626755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}