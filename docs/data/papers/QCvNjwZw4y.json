{"id": "QCvNjwZw4y", "number": 13314, "cdate": 1758216389950, "mdate": 1759897445944, "content": {"title": "GLARE: Towards Graph-less Retrieval for Retrieval Augmented Generation on Million-scale Knowledge Graphs", "abstract": "Retrieval-augmented generation (RAG) has emerged as an effective solution to mitigate hallucinations in Large Language Models (LLMs) by retrieving from an external knowledge base.\nRecent works have explored KG-based RAG, which leverages knowledge graphs (KGs) to incorporate rich relational information. \nHowever, existing methods suffer from high retrieval latency, as the retriever model needs to directly operate over graph space, thereby hindering their scalability to large-scale KGs.\nWe propose GLARE, a scalable KG-based RAG framework that enables fast and accurate information processing over million-scale KGs.\nSpecifically, GLARE compresses large KGs into a compact, knowledge-intensive vector memory enabling efficient retrieval without searching over an exponentially vast graph space. To preserve critical information, we further design a non-parametric, importance-aware graph pooling strategy and a VAE-style projector that reconstructs relational structures from the vector memory. \nAt inference time, GLARE enables linear-time retrieval from the vector memory, significantly accelerating KG-based question-answering (QA) while maintaining high response quality. \nEvaluations on the STaRK benchmark across multiple domains demonstrate that GLARE achieves over $\\times 30,000$ retrieval speedup with improved question-answering performance.", "tldr": "", "keywords": ["RAG", "graph learning", "LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9fd1955053e5e7070bd4f7a46891f278025ac019.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To address high retrieval latency in knowledge-graph-based (KG-based) RAG, this paper introduces GLARE, a KG-based RAG framework that enables fast and accurate information processing over million-scale KGs. Specifically, GLARE compresses large KGs into a vector memory. To preserve critical information, GLARE employs  a non-parametric, importance-aware graph pooling strategy and a VAE-style\nprojector that reconstructs relational structures from the vector memory. Empirical studies demonstrate the effectiveness and efficiency of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**S1.** The paper proposes a novel methodology for KG-based RAG with intuitive motivations, combining ideas from network science, information retrieval, graph neural networks, and VAEs.\n\n**S2.** Empirical studies demonstrate the effectiveness and efficiency of the proposed approach.\n\n**S3.** The paper is overall easy to follow."}, "weaknesses": {"value": "I'm willing to adjust my rating if some of important issues can be addressed.\n\n**W1.** The proposed approach has inherent limitations in its expressive power.\n- The subgraph selection process is fully structure-based, ignoring semantic information. Introducing some clustering-based ideas may help.\n- Only one-hop subgraphs are extracted.\n- The subgraph pooling process also ignores relational semantic information.\n\n**W2.** This paper only considers evaluations on the STaRK benchmark, while many previous KG-based RAG approaches were developed and evaluated on other datasets. It remains unclear whether the proposed approach is generalizable to those widely adopted datasets. While Appendix A.1 presents some comparisons on WebQSP and CWQ, they remain limited with insufficient baselines. In particular, the STaRK graphs have very few numbers of entity and relation types (<20) compared to KGs like FreeBase and Wikipedia. The questions in other KG question answering datasets can also present different supporting graph structures.\n\n**W3.** The paper still misses some highly relevant papers in related work discussion, such as [1], [2], [3], [4]. Expanding the related work discussion can help readers better assess your contributions. E.g., RoG [1] also does not suffer from KG scalability issues. I notice that the paper does compare against [1] in empirical studies (Table 4-6), but the baseline is not mentioned in the text, which is weird. Some discussions and comparisons in terms of methodology design will also be helpful.\n\n[1] Luo et al. Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning. ICLR 2024.\n\n[2] Luo et al. Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models. ICML 2025.\n\n[3] Mavromatis & Karypis. GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning. ACL Findings 2025.\n\n[4] Chen et al. Plan-on-Graph: Self-Correcting Adaptive Planning of Large Language Model on Knowledge Graphs.\n\n**W4.** Figure 1 claims that the time complexity of retrieval in knowledge graph is $O(2^n)$ while in vector memory is $O(n)$. This contrasting difference is not sufficiently discussed and justified other than the figure caption.\n\n**W5.** Equations and notations.\n- Equation (1) is problematic and should take expectations over the sample distributions as you do not construct a vector memory for each question.\n- Equations like (2) can be made more clear by using $\\phi$ or $\\theta$ to denote the parametrized model to be learned.\n\n**W6.** Important experiment details are not sufficiently discussed and justified. E.g., the LLM used for the proposed approach and baselines. The hyperparameters used for the baselines, which can significantly affect the baseline performance, e.g., the number of retrieved triples / paths.\n\n**W7.** The paper misses some ablation studies like the impact of different loss terms in equation (11). E.g., Is VAE really needed compared to AE? To what extent does the structure preserving loss help?"}, "questions": {"value": "**Q1.** For the final $Project_\\phi$, do you project $z$ into the token space or token embedding space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9dOjEkUaMa", "forum": "QCvNjwZw4y", "replyto": "QCvNjwZw4y", "signatures": ["ICLR.cc/2026/Conference/Submission13314/Reviewer_PcSX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13314/Reviewer_PcSX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727769178, "cdate": 1761727769178, "tmdate": 1762923978062, "mdate": 1762923978062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GLARE compresses a million-scale knowledge graph into a dense vector memory, enabling linear-time retrieval without expensive graph traversals. On the STaRK dataset it matches or exceeds KG-based RAG baselines while achieving roughly 100× compression and ~30,000× faster retrieval, highlighting its strong compression and retrieval-speed advantages for large KGs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tTackles a timely, impactful problem relevant to both industry and research.\n2.\tProposes a compact vector-memory representation of the KG to cut retrieval overhead.\n3.\tDemonstrates dramatic retrieval-speed improvements relative to existing RAG and graph-RAG implementations."}, "weaknesses": {"value": "1.\tThe core idea resembles prior work but this similarity is not addressed in the manuscript.\n2.\tEvaluation is limited to the STaRK benchmark, which restricts the generality of the results.\n3.\tThe chosen baselines are not sufficiently representative of related methods."}, "questions": {"value": "1.\tThe approach appears similar to Niu et al. (ECML PKDD 2023), which uses node and graph memory modules. GLARE seems to rely mainly on a graph-level memory—please clarify distinctions and cite/discuss related methods in the main text.\n2.\tSTaRK alone is insufficient to establish robustness. Please add additional benchmarks to better demonstrate comparative performance.\n3.\tInclude more representative baselines (e.g., GraphRAG, LightRAG, RAPTOR, and other relevant methods) to strengthen comparative claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7nUhlASNmk", "forum": "QCvNjwZw4y", "replyto": "QCvNjwZw4y", "signatures": ["ICLR.cc/2026/Conference/Submission13314/Reviewer_V2Za"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13314/Reviewer_V2Za"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922314789, "cdate": 1761922314789, "tmdate": 1762923977791, "mdate": 1762923977791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GLARE for efficient GraphRAG on large-scale KGs. GLARE embeds subgraphs in KGs and builds a vector memory for the embeddings of the subgraphs. During query time, graph traversal is avoided; instead, subgraph embeddings are retrieved according to the query embedding. To let the LLM understand the retrieved subgraph embeddings, a VAE-style projector is trained, which maps the message in vector memory into the token space of the frozen LLM. Experiments on the STaRK benchmark show the effectiveness of GLARE."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is fast due to retrieving in the embedding space, but not directly over graph data.\n- If there are no similar methods previously, the proposed projector is interesting, which can let LLMs understand subgraph embeddings. \n- It shows great capability even with significant compression."}, "weaknesses": {"value": "- The proposed method introduces information loss & locality bias. In some cases, this could improve efficiency while keeping accuracy, but it will also cause issues in cases, e.g., requiring long-hop reasoning. In the meantime, long-tail information in KGs would also be easily lost.\n    - I think this could be the main weakness of the method, so I believe it would be better to evaluate GLARE on long-hop QAs.\n- I might miss this in the paper, but how is the projector trained, what's the exact arch of the projector, and what's the cost to train the projector?\n- How do different anchor policies affect results? If pooling with multiple hop info, how would this affect results?"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5yY58sWsCS", "forum": "QCvNjwZw4y", "replyto": "QCvNjwZw4y", "signatures": ["ICLR.cc/2026/Conference/Submission13314/Reviewer_WhFN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13314/Reviewer_WhFN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967391123, "cdate": 1761967391123, "tmdate": 1762923977421, "mdate": 1762923977421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GLARE, a \"graph-less\" RAG framework intended to address retrieval latency on million-scale KG. The method compresses the KG into a compact vector memory using an importance-aware pooling strategy and a VAE-style projector to reconstruct information. The stated goal is to accelerate retrieval from graph space while maintaining question-answering quality. Experiments on STaRK show a significant latency reduction with moderate accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method achieves a significant retrieval speedup on million-scale KGs compared to graph-traversal baselines like ToG+GPT-4o.\n- The \"graph-less\" approach, which compresses the KG into a vector memory, is a sound contribution for avoiding costly graph traversal.\n- The paper is well-structured and the core idea is clearly presented."}, "weaknesses": {"value": "- The claimed performance gains are not convincing. The method struggles to consistently outperform the standard (and much simpler) RAG baseline, and in some cases, performs worse.\n- Absence of ablation experiments. The paper fails to properly analyze the contribution of its core components (the pooling strategy and the VAE projector) against reasonable alternatives."}, "questions": {"value": "- Q1. As shown in Table 2 and Figure 3, GLARE does not perform significantly better than other models in terms of non-human evaluation metrics. Given the significant added complexity of the VAE-based compression, could you elaborate on the practical justification for this method over  much simpler RAG baselines?\n- Q2. The contributions of the **Vector Memory Construction** and **Information Reconstruction** modules are not quantified. To assess the impact of your specific design choices, please provide ablation experiments. For instance:\n    1. How does the 'importance-aware' pooling (Centrality/PageRank)  compare against both a simple random selection baseline and a more advanced method like SubgraphRAG’s DDE [1]?\n    2. How critical is the VAE-style projector? What is the performance when swapping it with a deterministic projector, such as those used in G-Retriever [2] or GraphLLM [3]?\n\nI will consider raising the final rating score if you resolve the above issues.\n\n[1] Retrieval or reasoning: The roles of graphs and large language models in efficient knowledge-graph-based retrieval-augmented generation\n\n[2] G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering\n\n[3] GraphLLM: Boosting Graph Reasoning Ability of Large Language Model"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fs8TQrmiRg", "forum": "QCvNjwZw4y", "replyto": "QCvNjwZw4y", "signatures": ["ICLR.cc/2026/Conference/Submission13314/Reviewer_pceQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13314/Reviewer_pceQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13314/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977839336, "cdate": 1761977839336, "tmdate": 1762923977007, "mdate": 1762923977007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}