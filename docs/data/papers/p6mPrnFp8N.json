{"id": "p6mPrnFp8N", "number": 8154, "cdate": 1758070673096, "mdate": 1763142079945, "content": {"title": "Beyond the Shot: Rethinking Cinematography Understanding with Foundational Skill Evaluation", "abstract": "Cinematography understanding refers to the ability to recognize not only the visual content of a scene but also the cinematic techniques that shape narrative meaning. This capability is attracting increasing attention, as it enhances multimodal understanding in real-world applications and underpins coherent content creation in film and media. As the most comprehensive benchmark for this task, ShotBench spans a wide range of cinematic concepts and VQA-style evaluations, with ShotVL achieving state-of-the-art results on it. However, our analysis reveals that ambiguous option design in ShotBench and ShotVL’s shortcomings in reasoning consistency and instruction adherence undermine evaluation reliability, limiting fair comparison and hindering future progress. To overcome these issues, we systematically refine ShotBench through consistent option restructuring, conduct the first critical analysis of ShotVL’s reasoning behavior, and introduce an extended evaluation protocol that jointly assesses task accuracy and core model competencies. These efforts lead to ShotBench++, a refined and expanded benchmark that enables more reliable assessment and fosters future advances in cinematography understanding. The benchmark and code will be publicly released.", "tldr": "", "keywords": ["multimodal large language models", "cinematography understanding", "dataset and benchmark"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/fbfcc05f344236270559fd5afb0fd52b7e12c2a5.pdf", "supplementary_material": "/attachment/291790333d83186beccefe2a43ad7b8f4910a517.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the task of cinematography understanding—the ability of multimodal large language models to recognize not only the visual content in video shots but also the underlying cinematic techniques such as lighting, camera movement, composition, and framing. The authors identify two key issues in the existing benchmark ShotBench and its baseline model ShotVL: (1) Ambiguous and inconsistent multiple-choice design; (2) Baseline unreliability, including reasoning unfaithfulness and poor instruction adherence. To address these problems, the paper proposes: (1) ShotBench++, a refined version of ShotBench with consistent and mutually exclusive options; (2) New evaluation protocols introducing Faithful Reasoning Score (FRS) and Instruction Adherence Score (IAS) to jointly measure task accuracy and reliability; Comprehensive re-evaluation of ShotVL and Qwen2.5-VL models, showing that high task accuracy does not necessarily imply reliable reasoning or prompt adherence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors identify concrete flaws in ShotBench and provide a clear taxonomy-based correction procedure, ensuring consistent option granularity and mutual exclusivity.\n\n2. The discovery of reasoning unfaithfulness and instruction non-adherence is an important observation for the community. The use of structured prompts with <think> / <answer> tags is clear and reproducible.\n\n3. These metrics provide valuable quantitative measures beyond accuracy, offering a new lens for evaluating reasoning consistency in MLLMs."}, "weaknesses": {"value": "1. The primary contribution of this paper lies in refining the benchmark and introducing an evaluation protocol, rather than proposing a new modeling technique. The paper mainly focuses on diagnosing and reorganizing an existing dataset, rather than presenting a new dataset, algorithm, or representation framework.\n\n2. Experiments are limited to two model families (ShotVL and Qwen2.5-VL). There is no evaluation on other MLLMs such as LLaVA-OneVision, InternVL3, or Gemini. This raises concerns about generality.\n\n3. The FRS and IAS rely on automated verification using Qwen as a reasoning checker, which may introduce bias. There is no discussion of cross-validation using human judges or other LLMs to confirm consistency.\n\n4. While the authors claim improved granularity and exclusivity, they do not measure inter-annotator agreement or human validation on the refined dataset. Thus, it remains unclear whether the new benchmark truly improves human interpretability or only aligns internally defined taxonomies."}, "questions": {"value": "Please see Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "chBvxc7mjm", "forum": "p6mPrnFp8N", "replyto": "p6mPrnFp8N", "signatures": ["ICLR.cc/2026/Conference/Submission8154/Reviewer_sCg5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8154/Reviewer_sCg5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835924684, "cdate": 1761835924684, "tmdate": 1762920122844, "mdate": 1762920122844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "F9debqJ290", "forum": "p6mPrnFp8N", "replyto": "p6mPrnFp8N", "signatures": ["ICLR.cc/2026/Conference/Submission8154/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8154/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763142078739, "cdate": 1763142078739, "tmdate": 1763142078739, "mdate": 1763142078739, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ShotBench++, a refined and extended benchmark for cinematography understanding that goes beyond visual recognition to include cinematic techniques such as lighting, framing, and camera movement. The authors identify two major issues with the existing ShotBench and its associated model ShotVL: (1) ambiguous and inconsistent multiple-choice options, and (2) unreliable reasoning and instruction adherence in current state-of-the-art models.\nTo address these, they systematically restructure the benchmark options for consistency and introduce new diagnostic protocols, including Faithful Reasoning Score (FRS) and Instruction Adherence Score (IAS), to jointly assess performance and reliability. Experimental analyses on ShotVL and Qwen variants validate the proposed refinements and highlight meaningful behavioral differences between models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Thorough analysis**: The paper systematically diagnoses flaws in both benchmark construction and model behavior, offering a clear motivation for ShotBench++.\n- **Clear writing and structure**: The paper reads well and articulates its motivations and contributions in a logically coherent manner.\n- And I think the proposed FRS and IAS metrics are clearly formulated and conceptually grounded in reliability and interpretability."}, "weaknesses": {"value": "- **Dependence on existing datasets**: Most experiments are re-evaluations on ShotBench; there is limited evidence of generalization to new cinematography datasets or unseen domains.\n- **Evaluation scope**: The study primarily compares two model families (ShotVL and Qwen). Broader evaluation across diverse MLLMs would make the findings more generalizable."}, "questions": {"value": "- How sensitive are the new metrics (FRS, IAS) to the choice of the verifier model (e.g., Qwen-3B)? Would results change if another LLM were used as the judge?\n- In constructing ShotBench++, were human experts involved to verify the correctness and coherence of the revised options?\n- Could the proposed evaluation framework be used in a training-time diagnostic context, e.g., to improve reasoning consistency during fine-tuning?\n- In the conclusion, you said: ''Our in-depth analysis of ShotVL reveals overfitting to dataset **artifacts** and highlights the necessity of evaluating fundamental reasoning alongside benchmark scores.'' And where did you provide evidence of **artifacts**?\n\nPS: I am leaning toward raising my score if the authors can solve the weekness and the question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ajZ7mnI7wX", "forum": "p6mPrnFp8N", "replyto": "p6mPrnFp8N", "signatures": ["ICLR.cc/2026/Conference/Submission8154/Reviewer_a5Uq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8154/Reviewer_a5Uq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919211475, "cdate": 1761919211475, "tmdate": 1762920122416, "mdate": 1762920122416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets at the field of Cinematography and VQA to evaluate how well can VLMs understand the fine-grained Cinematography techniques. The benchmark paper refined an existing dataset, and evaluated on Qwen2.5 and ShotVL two sets of baseline models (3B + 7B each) on the new dataset. The paper analyzed an existing dataset, ShotBench, refined some multiple choices to preserve mutual exclusivity, and randomly shuffled choices. The size of the dataset went from 3500 VQA to 961. The paper then benchmarked two sets of models on this refined dataset, and observed improved accuracy overall. The paper conducted further analysis on reasoning faithfulness, reasoning instruction adherence through qualitative and quantitative experiments, and proposed two scores: Faithful Reasoning Score (FRS) and Instruction Adherence Score (IAS) to reflect the reasoning process of the models. The paper conducted these evaluations using Qwen-3B as an automatic judge and observed that ShotVL lacks reasoning and instruction following capabilities while Qwen models demonstrated better performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and well structured\n2. The paper contributed a refined dataset focusing on fine-grained cinematography terminology\n3. The paper conducted thorough analysis and experimentations benchmarking two sets of VLMs on the new dataset, and revealed their limitations and strengths\n4. The paper provided sufficient qualitative and quantitative analysis, easy-to-understand figures and examples to demonstrate the experiment results."}, "weaknesses": {"value": "1. The paper conducted Qwen-as-judge to auto-evaluate the key reasoning capabilities of the baseline models. It would be great and important if these evaluates are validated through human evaluations.\n2. The motivation of refining an existing dataset is focused around introducing and preserving mutual exclusivity among multiple choices. While it makes it 'cleaner' for humans to understand, it is possible that the refinement step made the task easier (as shown in Table 1). The question for the premise is: is mutual exclusivity actually necessary? Should the VLMs still be able to choose the right answer(s) even if they are not?\n3. The paper could potentially be better suited for a more niche targeted group of audience\n3. Questions below."}, "questions": {"value": "1. Is there any reason that the dataset size drastically reduced from 3500 QAs to 961 QAs?\n2. Table 3: if ShotVL models could achieve higher performance and in shorter time through 'Direct' predictions, compared to Qwen models, is it necessary to go through the reasoning steps, especially in real world applications? \n\nOther minor questions:\n\n3. Section 4.1 Reasoning Faithfulness -- Qualitative Analysis: the think and answer tags '?!'\n4. Table 1 LC ShotVL-3b: why isn't 67.4 bold, compared to 65.7 for ShotVL-7B?\n5. Ln 208: how did you define '16,7% confusion rate'"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0raqLmgF6a", "forum": "p6mPrnFp8N", "replyto": "p6mPrnFp8N", "signatures": ["ICLR.cc/2026/Conference/Submission8154/Reviewer_58d7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8154/Reviewer_58d7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983770791, "cdate": 1761983770791, "tmdate": 1762920121982, "mdate": 1762920121982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RefineShot, a rigorously refined and extended benchmark for evaluating cinematography understanding in MLLMs. Motivated by the observation that the current standard benchmark, ShotBench, suffers from inconsistent option granularity and that its best-performing baseline, ShotVL, exhibits unreliable reasoning and poor instruction adherence, the authors first restructure ShotBench’s multiple-choice sets to enforce mutual exclusivity and dimensional consistency across eight cinematic attributes (e.g., lighting, framing, camera motion). An analysis of ShotVL is conducted, demonstrating that its high accuracy masks frequent mismatches between generated reasoning traces and final answers as well as dramatic performance drops under structured prompting. Finally, they augment the benchmark with an evaluation protocol that jointly measures task accuracy, and FRS/IAS indicator they proposed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A systematic re-engineering of ShotBench that replaces ambiguous, cross-dimensional options with mutually exclusive choices of uniform granularity, yielding 961 revised questions and a demonstrably fairer evaluation set.\n    \n2. An analysis is conducted on the SOTA ShotVL family, exposing systematic unfaithfulness between its reasoning and answers and severe fragility under chain-of-thought or step-by-step prompts.\n    \n3. The introduction of complementary metrics—Faithful Reasoning Score (FRS) and Instruction Adherence Score (IAS)—that jointly assess task performance and core model competencies, enabling nuanced differentiation between superficial accuracy and genuine reasoning reliability."}, "weaknesses": {"value": "1. The paper does not contribute any new data; the so-called ShotBench++ is merely a re-packaging of ShotBench, which considerably dilutes its novelty.\n\n2. No new model architecture or training strategy is proposed. While abstaining from methodology is acceptable for a benchmark paper.\n\n3. However, the absence of an in-house method then obliges the authors to evaluate an extensive suite of existing models (typically 15–20). Instead, only four models are tested.\n\n4. Among these four, all are 3B or 7B parameter variants; 72 B checkpoints and proprietary APIs are ignored. If even these small-scale models already saturate ShotBench++, the task’s continued relevance is called into question.\n\n5. Human is not in the loop. The paper does not report κ-scores or crowd-worker consistency for the 961 re-written options, leaving the community uncertain whether the “refined” labels are actually less ambiguous than the original ones."}, "questions": {"value": "The present contribution is too slender—both in scientific advance and in empirical heft—to meet the bar of a top-tier venue such as ICLR. The authors **neither release new data nor propose a novel architecture, and the evaluation matrix is restricted to four small-scale models** (3B / 7B) while omitting larger checkpoints and commercial APIs. Consequently, ShotBench++ is better viewed as a light-weight extension of ShotBench than as a self-standing benchmark; indeed, **it is less comprehensive than the ShotBench, which at least supplied baseline models.**\n\nNevertheless, the paper does deliver two genuinely fresh insights: (i) the diagnosis that reasoning faithfulness and instruction adherence can collapse even when accuracy is high, and (ii) the technically simple yet powerful idea of “Benchmark Refinement’’—re-engineering option sets to guarantee mutual exclusivity and dimensional purity. This latter concept is ripe for generalisation: applying the same protocol to MMBench, MMMU, or MME could purge analogous ambiguities across the multimodal-evaluation ecosystem. If the authors were to broaden their scope to multiple flagship benchmarks and release a unified refinement toolkit with extensive human re-annotation, the resulting study would amass the critical mass of novelty, utility, and community impact expected of a top-tier publication."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hvaf9wpC2C", "forum": "p6mPrnFp8N", "replyto": "p6mPrnFp8N", "signatures": ["ICLR.cc/2026/Conference/Submission8154/Reviewer_MnGU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8154/Reviewer_MnGU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8154/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998669915, "cdate": 1761998669915, "tmdate": 1762920121593, "mdate": 1762920121593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}