{"id": "QN4WZ5QoqC", "number": 2082, "cdate": 1756986065395, "mdate": 1759898170538, "content": {"title": "Active speech enhancement: beyond passive denoising declipping and dereverberation", "abstract": "We introduce a new paradigm for active sound modification: Active Speech Enhancement (ASE). While Active Noise Cancellation (ANC) algorithms focus on suppressing external interference and traditional speech enhancement passively reconstructs degraded speech signals, ASE goes further by actively shaping the speech signal, both attenuating unwanted noise components and amplifying speech-relevant frequencies to improve intelligibility and perceptual quality. To enable this, we propose a novel Transformer-Mamba-based architecture, along with a task-specific loss function designed to jointly optimize interference suppression and signal enrichment in an acoustic environment. Our method outperforms existing baselines across multiple speech processing tasks, including denoising, dereverberation, and declipping, demonstrating the effectiveness of active, targeted modulation in challenging acoustic environments. A demo page and source code are provided in the Supplementary Materials.", "tldr": "Active speech enhancement and noise suppression", "keywords": ["Active Noise Cancellation", "ANC", "Transformer", "Mamba", "Dereverberation", "Declipping", "NLP", "Speech"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd6c854ee61972557d03d83f5fd2999c83ad442e.pdf", "supplementary_material": "/attachment/132cd6538b5aeb93ec56d7c7b07445bb3c5101c5.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose using the Active Noise Cancellation (ANC) paradigm for speech enhancement. They use Transformer- and Mamba-based architectures, applied in the spectral domain, to model the secondary path signal. The model's output is regressed against the ground-truth clean signal, and the model parameters are optimised using several regression losses. The model is compared against several baselines on the VoiceBank-DEMAND dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I would like to highlight the following strengths that the paper possesses:\n\n1. The paper is well-written and well-structured, which makes it easy to follow and understand the main ideas.\n2. The background and the model description are well-presented and clear.\n3. The experiment results are well-described."}, "weaknesses": {"value": "However, the paper also has several weaknesses:\n\n1. The model is trained using a linear combination of 6 losses. However, no ablation study is conducted to showcase the importance and the effect of each loss. There is no discussion about the benefits of each loss. A discussion about the methods used to choose the $\\gamma_i$ weights in loss functions is also lacking. I believe this discussion is necessary to shed some light on the intricacies of training and provide a clearer picture of how the model behaves.\n2. The paper lacks results for the non-intrusive metrics, such as DNSMOS [1], UTMOS [2], or WV-MOS [3], despite their abundant presence in the literature (see, for example, [4 - 6, inter alia]). Moreover, some previous works [3, 7, 8] indicate their benefits over traditional metrics like PESQ. I would recommend including the results for the aforementioned metrics in the paper or justifying why those metrics should not be used for this particular case. Computing the Mean Opinion Score (MOS) would also strengthen the paper.\n3. It is worth considering applying the model to other benchmarks, as VoiceBank-DEMAND is a relatively simple benchmark with audio samples containing only mild distortions.\n4. Oftentimes, the advantage of the presented model is difficult to perceive while listening to the audio samples given in the supplemental material. Despite outperforming the baselines in PESQ and other metrics, the presented model sounds similar to some of the baselines. Therefore, it is quite hard to appreciate the advantage of the model perceptually. To this end, I would recommend providing samples that clearly showcase the advantage of the presented model, as well as reporting perceptual metrics (MOS, UTMOS, etc) to substantiate the improvements in perceptual quality."}, "questions": {"value": "I have a few questions about the work.\n\n1. It is unclear to me why considering ANC paradigm for speech enhancement is beneficial, while there exist several works [4, 5, 6] that successfully solve the speech enhancement without ANC paradigm.\n2. Some applications of speech enhancement include deploying the models on wearable devices and using the models in a streaming fashion. These applications require high inference speed; moreover, it can be beneficial to make the model autoregressive. I wonder what the inference speed of the model is (in terms of RTF), and if the model can be made autoregressive.\n\nReferences:\n\n[1] Reddy et al., \"DNSMOS P. 835: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors\"\n\n[2] Saeki et al., \"UTMOS: UTokyo-SaruLab system for VoiceMOS challenge 2022\"\n\n[3] Andreev et al., \"HiFi++: a unified framework for bandwidth extension and speech enhancement\"\n\n[4] Su et al., \"HiFi-GAN-2: Studio-quality speech enhancement via generative adversarial networks conditioned on acoustic features.\"\n\n[5] Babaev et al., \"FINALLY: fast and universal speech enhancement with studio-like quality\"\n\n[6] Guimarães et al., \"High-fidelity generative speech enhancement via latent diffusion Transformers\"\n\n[7] Manocha et al., \"Audio similarity is unreliable as a proxy for audio quality.\"\n\n[8] Manjunath T., \"Limitations of perceptual evaluation of speech quality on VoIP systems.\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "94bOYoA3Vf", "forum": "QN4WZ5QoqC", "replyto": "QN4WZ5QoqC", "signatures": ["ICLR.cc/2026/Conference/Submission2082/Reviewer_mgHN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2082/Reviewer_mgHN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761585683468, "cdate": 1761585683468, "tmdate": 1762916013943, "mdate": 1762916013943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel paradigm for speech enhancement \"Active Speech Enhancement\" (ASE) which is inspired by Active Noise Cancellation (ANC). \n\nIt proposes also a transformer-mamba architecture for this novel framework which extends SEMamba proposed firstly for speech enhacement. \n\nThe method is tested on different datasets and distortion types (denoising, and dereverberation + declipping) including the widely used VoiceBank-DEMAND denoising dataset against different baseline systems taken from ANC literature and shows superior performance."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The transformer-mamba architecture seems effective."}, "weaknesses": {"value": "The paper claims a novel approach to ANC called active speech enhancement where the model instead of predicting a cancellation signal predicts an additive signal. \nI think the claim of novel paradigm is not significant. \n\nThe paper seems to present just a standard supervised speech enhancement approach where the target c(n) = P(z) * s_clean(n) simply has an extra acoustic path simulation. \nAs such in the paper the authors should probably compare with SotA techniques in speech enhancement (SEMamba, TFGridNet and so on). \nBut only comparisons with ANC algorithms are made, but these rely on different assumptions. \n\nMoreover, 'These baseline methods were adapted and retrained or configured to the ASE framework across all tested tasks.'\nBut how these where adapted ? \nLooking at the Tables in some cases some models have even worse PESQ (DeepANC in particular) than noisy signal which calls the experimental validation into question or the fairness of this comparison. Have the authors adapted these baselines to the ASE framework (supervised speech enhancement) ? Or kept ANC ? \n\nThe better results are given by the powerful transformer-mamba architecture which is likely much stronger than the other baseline ANC models, and also by the use of the many loss components. \nThis clearly is valuable but I think not enough for this venue and also the claim of new paradigm is unsupported. \n\n\nMinor: \n\nThe NLP keyword is not needed in my opinion."}, "questions": {"value": "Some questions were placed in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3kSWqWa3P2", "forum": "QN4WZ5QoqC", "replyto": "QN4WZ5QoqC", "signatures": ["ICLR.cc/2026/Conference/Submission2082/Reviewer_7zK6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2082/Reviewer_7zK6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871551125, "cdate": 1761871551125, "tmdate": 1762916013824, "mdate": 1762916013824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new paradigm called Active Speech Enhancement (ASE), which aims to go beyond traditional passive speech enhancement and active noise cancellation by actively shaping the speech signal. Unlike conventional approaches that only suppress noise or reverberation, ASE both attenuates interference and amplifies speech-relevant frequency components to improve perceptual quality. The authors propose a Transformer–Mamba–based model (ASE-TM) that generates an active modification signal through time–frequency Mamba blocks and an attention mechanism, coupled with a joint suppression–enrichment loss balancing denoising and signal enhancement. Comprehensive experiments across denoising, dereverberation, and declipping tasks show consistent improvements over classical and deep ANC baselines (e.g., DeepANC, ARN, THF-FxLMS) in PESQ, STOI, and NMSE metrics. The results demonstrate ASE-TM’s robustness under diverse acoustic and nonlinear conditions, suggesting its potential as a unified framework for active and intelligent speech restoration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear writing and decent experimental reproducibility.\n\n2. The idea of linking active control theory with speech enhancement could, in principle, be interesting if more rigorously developed.\n\n3. The adoption of Mamba2 might inspire follow-up studies in sequence modeling for SE."}, "weaknesses": {"value": "1. Conceptual Ambiguity in Task Definition\nThe paper claims to “formalize the ASE task and describe appropriate evaluation metrics that capture both noise suppression and speech enhancement.” However, in Section 3 (Eq. 4), the formulation directly borrows the notation from Active Noise Control (ANC), where the primary signal d(n) and anti-signal a(n) have clear physical meanings. In the proposed ASE setting, a(n) no longer corresponds to an anti-noise component with opposite phase or amplitude, and there is no identifiable physical counterpart in the presented model. This raises fundamental doubts about whether ASE constitutes a truly independent task, rather than merely a reinterpretation of ANC applied to speech enhancement. The authors should clarify what specific theoretical or practical advantage the ANC framework offers over conventional speech enhancement formulations.\n\n2. Limited Novelty in Model Architecture\nThe proposed “Transformer–Mamba–based model” is nearly identical to SE-Mamba, with the main change being the replacement of Mamba with Mamba2. While Mamba2 has demonstrated strong modeling ability in ASR [1], its integration here is a straightforward substitution rather than a conceptually new design. The claimed novelty therefore lies more in model adoption than in architectural innovation.\n\n3. Minimal Contribution in Loss Design\nThe proposed “joint suppression–enrichment loss” combines six components, five of which are identical to those used in MP-SENet. The only addition, L_T, represents a combined time–frequency loss that has already appeared in earlier works (e.g., [2]). Thus, the contribution in objective design is marginal and largely derivative of prior art.\n\n4. Weak Empirical Performance\nAs shown in Table 1, the denoising results of the proposed method fall significantly short of current state-of-the-art systems [3]. This performance gap undermines the paper’s claim that the ASE formulation or the proposed model yields superior practical benefits.\n\n[1] Y. Masuyama, K. Miyazaki and M. Murata, \"Mamba-Based Decoder-Only Approach with Bidirectional Speech Modeling for Speech Recognition,\" 2024 IEEE Spoken Language Technology Workshop (SLT), Macao, 2024, pp. 1-6, doi: 10.1109/SLT61566.2024.10832186.\n[2] Chen, Hang, et al. \"Cross-attention among spectrum, waveform and SSL representations with bidirectional knowledge distillation for speech enhancement.\" Information Fusion (2025): 103218.\n[3] M. S. Khan, M. L. Quatra, K. -H. Hung, S. -W. Fu, S. M. Siniscalchi and Y. Tsao, \"Exploiting Consistency-Preserving Loss and Perceptual Contrast Stretching to Boost SSL-Based Speech Enhancement,\" 2024 IEEE 26th International Workshop on Multimedia Signal Processing (MMSP), West Lafayette, IN, USA, 2024, pp. 1-6, doi: 10.1109/MMSP61759.2024.10743615."}, "questions": {"value": "1. What concrete benefits does the ASE formalization provide compared to conventional SE frameworks?\n\n2. What is the physical or algorithmic interpretation of a(n) in your system, given that it no longer functions as an anti-noise signal?\n\n3. Can you isolate the gain of Mamba2 over SE-Mamba through controlled experiments?\n\n4. Have you compared your method with recent diffusion-based or GAN-based SE systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xnffUohrKh", "forum": "QN4WZ5QoqC", "replyto": "QN4WZ5QoqC", "signatures": ["ICLR.cc/2026/Conference/Submission2082/Reviewer_saHn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2082/Reviewer_saHn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106976952, "cdate": 1762106976952, "tmdate": 1762916013707, "mdate": 1762916013707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}