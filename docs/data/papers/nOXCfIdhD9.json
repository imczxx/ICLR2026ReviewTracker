{"id": "nOXCfIdhD9", "number": 13681, "cdate": 1758220840069, "mdate": 1759897419890, "content": {"title": "Corner Gradient Descent", "abstract": "We consider SGD-type optimization on infinite-dimensional quadratic problems with power law spectral conditions. It is well-known that on such problems deterministic GD has loss convergence rates $L_t=O(t^{-\\zeta})$, which can be improved to $L_t=O(t^{-2\\zeta})$ by using Heavy Ball with a non-stationary Jacobi-based schedule (and the latter rate is optimal among fixed schedules). However, in the mini-batch Stochastic GD setting, the sampling noise causes the Jacobi HB to diverge; accordingly no $O(t^{-2\\zeta})$ algorithm is known. In this paper we show that rates up to $O(t^{-2\\zeta})$ can be achieved by a generalized stationary SGD with infinite memory. We start by identifying  generalized (S)GD algorithms with contours in the complex plane. We then show that contours that have a corner with external angle $\\theta\\pi$ accelerate the plain GD rate $O(t^{-\\zeta})$ to $O(t^{-\\theta\\zeta})$. For deterministic GD, increasing $\\theta$ allows to achieve rates arbitrarily close to $O(t^{-2\\zeta})$. However, in Stochastic GD, increasing $\\theta$ also amplifies the sampling noise, so in general $\\theta$ needs to be optimized by balancing the acceleration and noise effects. We prove that the optimal rate is given by $\\theta_{\\max}=\\min(2,\\nu,\\tfrac{2}{\\zeta+1/\\nu})$, where $\\nu,\\zeta$ are the exponents appearing in the capacity and source spectral conditions. Furthermore, using fast rational approximations of the power functions, we show that ideal corner algorithms can be efficiently approximated by practical finite-memory algorithms.", "tldr": "Generalized (S)GD algorithms = contours in $\\mathbb C$. Contours having a corner with external angle $\\theta\\pi, 1<\\theta<2,$ accelerate loss convergence rates $t^{-\\xi}$ to $t^{-\\theta\\xi}$.", "keywords": ["mini-batch stochastic gradient descent", "momentum", "sampling noise", "convergence rates", "acceleration", "power laws", "phase diagram", "contour integration", "rational approximations", "asymptotic methods", "MNIST", "frequency response function"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c237675a96eb346dcf56f560905fb7d55f124e3.pdf", "supplementary_material": "/attachment/c2514bd7d34c3f980bbaab207f885b2a8d237bda.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposed a novel approach to the acceleration of (S)GD in ill-conditioned quadratic problems through a geometric interpretation of stationary algorithm. The authors identify a view of generalized (S)GD as contours leveraging the characteristic polynomials and loss expansion. A special class of those contours with an external angle is proved to accelerate (S)GD and the corresponding effects are analyzed. The paper further demonstrates that the ideal corner algorithm can be approximated with finite-memory algorithms, and provide experiments on a synthetical problem and MNIST."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "(i) The paper is novel. It introduces a new geometric framework for understanding stationary (S)GD algorithms through complex-plane contours, offering a conceptual unification of gradient descent, momentum. The idea of characterizing acceleration via cornered contours is novel and mathematically elegant.\n\n(ii) The analysis is rigorous, theorems are clearly stated and proved, and the trade-off between acceleration and noise amplification is discussed and quantified. \n\n(iii) The authors offer practical implementation of the algorithm and tested on both synthetical experiments and MNIST dataset, which further validates the effectiveness."}, "weaknesses": {"value": "My primary concern is the experiments part, while being theoretically elegant, the experiments primarily focuses on the 1d synthetic regression task and a small scale MNIST example. The results are consistent with theory but do not establish a broader applicability. Are there any specific reasons that prevents validation on a task of a larger scale? Other than that, I believe the paper is generally well-written, but some parts of it, especially the background would benefits from additional explanations and intuitions."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xb9sWisQAS", "forum": "nOXCfIdhD9", "replyto": "nOXCfIdhD9", "signatures": ["ICLR.cc/2026/Conference/Submission13681/Reviewer_g74e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13681/Reviewer_g74e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760607948037, "cdate": 1760607948037, "tmdate": 1762924243517, "mdate": 1762924243517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the SGD behaviour for quadratic problems. The authors analyze scaling laws and their corollaries for the gradient descent with memory. They propose a connection between SGD-type optimization with memory and holomorphic maps. After deriving the analysis for optimization schemes with infinite memory, the authors propose a finite-memory relaxation, that achieve better results on MNIST dataset."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1)Considering SGD with the instruments of complex analysis is nonstandard and provide new views on well-established theory.\n\n2)Derived theory correlates with the recent works, that analyze the spectral power laws.\n\n3)Analysis of the optimization schemes with infinite memory. This approach is not well-developed and might lead to better finite approximations."}, "weaknesses": {"value": "1)The writing is complicated. Although this might be fixed with an extended number of pages, the authors should consider reorganizing the paper.\n\n2)The errors of finite-memory approximations are not given.\n\n3)Only quadratic losses are considered.\n\n4)The paper lacks comparison of finite-memory approximations of corner SGD with varying $M$.\n\n5)Minor typos ($\\mu^{t-1}$ instead of $\\mu^{k-1}$ on line 293, etc)."}, "questions": {"value": "1)For finite-memory approximations we need to settle the parameter $\\theta$, which affects iterate dynamics as well as convergence rates. While Theorem 4 claims, that there exist an upper bound $\\theta_{max}$, what will happen, if we take $\\theta > \\theta_{max}$ for finite approximation?\n\n2)Following the previous question, how can one estimate $\\theta$ for various datasets?\n\n3)What are the limitations on the batch size $B$? The authors demand $U_\\Sigma < 1$, which depends on the batch size.\n\n4)Why for finite-memory approximations we choose $\\Psi$ specifically as in equation 28? The properties of the optimization scheme depends on $\\Psi$ properties, as declared in Theorem 3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No additional ethical concerns."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fxSXHb20mn", "forum": "nOXCfIdhD9", "replyto": "nOXCfIdhD9", "signatures": ["ICLR.cc/2026/Conference/Submission13681/Reviewer_C97c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13681/Reviewer_C97c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761693121081, "cdate": 1761693121081, "tmdate": 1762924243141, "mdate": 1762924243141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces generalized stationary SGD with infinite memory, a theoretical framework for accelerating stochastic gradient descent on infinite-dimensional quadratic problems with power-law spectral conditions. The authors build upon classical results showing that deterministic Gradient Descent achieves a convergence rate of $O(t^{-\\zeta})$, and that Heavy Ball with a non-stationary Jacobi schedule can accelerate this rate to $O(t^{-2\\zeta})$. However, in stochastic mini-batch settings, no such results are known.\nThe contribution of the paper is a geometric representation of stationary (S)GD algorithms as contours in the complex plane. The authors show that contours with a corner of external angle $\\theta\\pi$ accelerate the plain GD rate from $O(t^{-\\zeta})$ to $O(t^{-\\theta\\zeta})$. In deterministic settings, increasing $\\theta$ up to $2$ gives rates arbitrarily close to $O(t^{-2\\zeta})$. In stochastic setting, increasing $\\theta$ also amplifies noise, leading to a trade-off, with optimal rate $\\theta_{\\max} = \\min(2, \\nu, \\tfrac{2}{\\zeta + 1/\\nu})$, where $\\nu$ and $\\zeta$ are the spectral exponents coming from the capacity and source conditions.\n\nTo make Corner SGD practical, the paper proposes finite-memory approximations of the infinite-memory algorithms using rational approximations of the power function $z^\\theta$. Empirical experiments on a synthetic problem and MNIST show accelerated convergence consistent with the theoretical predictions."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The contour-based geometric representation of stationary (S)GD algorithms is new and interesting, connecting optimization dynamics with complex analysis.\n\n2. The paper unifies acceleration, stability, and noise amplification under a single geometric-spectral framework.\n\n3. Rigorous theoretical results.\n\n4. The finite-memory approximation gives a practical way to approximate the idealized algorithms."}, "weaknesses": {"value": "1. The paper is technically rigorous and presents a new geometric perspective on SGD acceleration, but several assumptions (only quadratic case, SE, infinite-dimensional limit) narrow the practical relevance. It is unclear if the main conclusions hold beyond this idealized regime.\n\n2. Limited experimental validation (one synthetic experiment with MNIST). However, I acknowledge that the paper is of theoretical nature."}, "questions": {"value": "1. Can the authors comment on how the framework might extend beyond quadratic case?\n\n2. How should one choose the memory size $M$ in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1iX8p40D1y", "forum": "nOXCfIdhD9", "replyto": "nOXCfIdhD9", "signatures": ["ICLR.cc/2026/Conference/Submission13681/Reviewer_yRoH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13681/Reviewer_yRoH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764065056, "cdate": 1761764065056, "tmdate": 1762924242853, "mdate": 1762924242853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider the problem of linear regression with random design in a (possibly infinite-dimensional) Hilbert space. They aim to estimate a vector $w^\\star$ minimizing quadratic risk. In order to do so, they apply stochastic gradient descent (SGD) with memory. It is known that in quadratic optimization problems deterministic gradient descent (GD) converges at the rate $O(t^{-\\zeta})$ while the heavy-ball method has a significantly faster rate of convergence $O(t^{-2\\zeta})$. Here $\\zeta$ reflects the signal smoothness. Unfortunately, the question of a similar acceleration for mini-batch SGD is underexplored. In [Varre, Flammarion, 2022] the authors proposed a non-stationary modification of SGD that achieves quadratic acceleration but their bounds include the condition number of the design covariance matrix. As a consequence, they are valid in finite-dimensional setups only. The present submission studies possibility of quadratic acceleration in the infinite-dimensional case. The authors show that stationary SGD algorithms with memory can be characterized by contours on a complex plane. They establish that contours with a corner at $0$ play a crucial role and call the corresponding stochastic algorithms corner gradient descent. Such algorithms can achieve the rate of convergence $O(t^{-\\theta\\zeta})$, where $1 < \\theta < 2$, provided that the external angle at $0$ is equal to $\\theta \\pi$. The only drawback of corner algorithms is that they require infinite memory. Fortunately, the authors suggest an approach how to approximate them with finite-memory SGD."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper studies an interesting and important question, which yet has not got sufficient attention in the literature. The authors suggest a non-trivial geometric interpretation of SGD with memory, which helps to understand the possibility of acceleration compared to the standard rate of convergence $O(t^{-\\zeta})$. In the signal-dominating regime, they provide a very demonstrative figure showing when one can achieve the rate $O(t^{-\\zeta})$ and when only intermediate rates are possible. I am not an expert in convex optimization but I suppose that this contribution will be interesting to the community."}, "weaknesses": {"value": "1. The authors refer to [Velikanov et al., 2023] and [Yarotsky, Velikanov, 2024] for extensive discussion of the SE approximation. I took a quick look at these papers and found no examples with $\\tau_2 = 0$ in eq. (6). At the same time the key theoretical results presented in the main text are obtained under the condition $\\tau_2 = 0$. I think that restrictiveness of this assumption must be discussed.\n\n2. Rigorous theoretical guarantees are obtained in the case $\\tau_2 = 0$ only. When $\\tau_2 \\neq 0$, the authors provide only sketch of the proof."}, "questions": {"value": "Can you discuss how severe the requirement $\\tau_2 = 0$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "caa76sTqkD", "forum": "nOXCfIdhD9", "replyto": "nOXCfIdhD9", "signatures": ["ICLR.cc/2026/Conference/Submission13681/Reviewer_Fw2s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13681/Reviewer_Fw2s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994025731, "cdate": 1761994025731, "tmdate": 1762924242547, "mdate": 1762924242547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}