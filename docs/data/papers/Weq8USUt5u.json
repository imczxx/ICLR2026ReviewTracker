{"id": "Weq8USUt5u", "number": 16998, "cdate": 1758271051648, "mdate": 1759897205210, "content": {"title": "Subgraph Plug-in Boosts up Graph Neural Networks", "abstract": "Message-passing neural networks (MPNNs) often collapse into a one-dimensional subspace because repeated neighborhood aggregation amplifies the dominant eigenvector of the normalized adjacency matrix, erasing local distinctions and limiting graph classification performance. In this paper, we theoretically analyze this phenomenon using perturbation theory to trace the eigenvector amplification process and mutual information bounds to quantify the resulting loss of discriminative signals. Guided by these insights, we propose the Subgraph Plug-in (SP), a lightweight, architecture-agnostic module that selects the top-$\\kappa$ nodes by centrality, extracts their $\\tau$-hop neighborhoods as interpretable subgraphs, and concatenates the resulting subgraph embeddings with the global representation of any base GNN without altering its architecture or incurring significant computational overhead. Across 11 graph-classification benchmarks and 13 GNN variants, we evaluate each backbone with and without SP, yielding 110 model–dataset pairs; SP improves performance in 94 of 110. Beyond these, on ZINC and OGBG-MolHIV, we conduct head-to-head comparisons against 11 methods, including augmentation modules, recent GNNs, and subgraph-based methods. SP achieves the best results among augmentation and subgraph-based approaches and remains competitive with recent GNNs, supporting its role as a widely applicable, cost-effective plug-in that preserves feature diversity and amplifies discriminative substructures. performance.", "tldr": "SP partitions and extracts subgraphs around high-centrality nodes to augment GNNs, theoretically preventing rank collapse from repeated message passing and consistently boosting graph classification and regression accuracy across diverse benchmarks.", "keywords": ["Subgraph partitioning", "mutual information", "perturbation theory", "graph neural network", "plug-in method"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f382271c2fcce7d73c96b13dff5cb32aea7a6ccd.pdf", "supplementary_material": "/attachment/35a3e80eb0a27e059a77b604fcf769e3db200bbe.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates the tendency of deep message-passing neural networks (MPNNs) to collapse toward low-rank representations and attributes this to repeated aggregation amplifying the dominant eigenvector of the normalized adjacency matrix. To address this issue, it introduces a simple, architecture-agnostic component called the Subgraph Plug-in (SP). The method scores nodes by centrality, extracts several seed-centered subgraphs, and concatenates their embeddings with the global graph representation produced by the backbone network. Theoretical analysis suggests that this approach helps preserve information and maintain feature diversity as network depth increases. Experiments across a broad range of graph-classification benchmarks and GNN variants show consistent performance improvements, indicating that the Subgraph Plug-in can enhance the expressiveness of existing architectures without requiring structural changes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s strengths are primarily conceptual and theoretical. In terms of originality, it offers a clean, architecture-agnostic way to inject subgraph information into existing GNNs, combining centrality-guided subgraph selection with late fusion in a way that feels simple yet nontrivial relative to prior subgraph heuristics. The quality of the framing is solid: the collapse phenomenon is analyzed through a spectral lens, the assumptions are explicit, and the main claims are supported by clear propositions and proofs that make the mechanism’s intended effect understandable. Clarity is a notable positive—definitions, algorithmic steps, and notation are laid out in a way that makes the plug-in straightforward to reimplement and adapt. On significance, the work targets a widely acknowledged pain point in deep GNNs (over-mixing and loss of discriminative structure) and proposes a plug-and-play fix that could realistically be adopted across diverse backbones, which gives the idea practical potential even if its novelty is incremental rather than sweeping."}, "weaknesses": {"value": "The empirical evidence is the main weakness: reported gains are small and often within large standard deviations, making it unclear whether the method offers a reliable improvement over strong, well-tuned baselines in practice; this calls for more seeds plus parameter- and compute-matched controls. On novelty, the contribution feels incremental relative to prior subgraph and pooling lines (e.g., substructure-aware GNNs and pooling modules), so a targeted comparison against these families—ideally with ablations that isolate the value of centrality-guided selection vs. generic k-hop crops—would strengthen the case. The paper would also benefit from mechanism-oriented diagnostics that directly test the stated goal of combating rank collapse: report feature-rank/participation ratio, singular-value spectra, representation dispersion, and agreement between global and subgraph embeddings across depth, and show that these quantities move in the hypothesized direction only when the plug-in is active."}, "questions": {"value": "- Please rerun key benchmarks with more seeds, report 95% confidence intervals, and add parameter/compute-matched controls to verify that gains aren’t due to capacity or noise.\n\n- Can you directly validate the anti-collapse mechanism by reporting effective rank, singular-value spectra, dispersion, alignment with the top eigenvector, and global–subgraph embedding similarity across depth, and showing these metrics correlate with performance?\n\n- Please position the plug-in against representative subgraph/pooling methods via head-to-head, budget-matched ablations with sensitivity to k, tau, centrality, and disjointness, and include training/inference overhead and memory scaling."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b2IsiGpEOZ", "forum": "Weq8USUt5u", "replyto": "Weq8USUt5u", "signatures": ["ICLR.cc/2026/Conference/Submission16998/Reviewer_q2hC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16998/Reviewer_q2hC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761083271633, "cdate": 1761083271633, "tmdate": 1762927017917, "mdate": 1762927017917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a lightweight *Subgraph Plug-in (SP)* that (i) ranks nodes with simple centralities, (ii) extracts \\(\\tau\\)-hop neighborhoods around the top-\\(\\kappa\\)  seeds as disjoint subgraphs within each centrality type, and (iii) encodes each subgraph with the same backbone before concatenating the subgraph embeddings with the global graph embedding at readout. The method is motivated as a practical way to mitigate depth-induced rank collapse/oversmoothing without altering the underlying encoder. Theoretical framing argues that, when certain informativeness and contraction conditions hold, the concatenated representation is at least as informative as the global one. Experiments cover many backbones and benchmarks; the paper claims broadly positive gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear modular design. The pipeline is easy to add to existing GNNs and does not require architectural changes, which improves reproducibility and lowers engineering cost.\n\n- Breadth of evaluation. The work tests multiple backbones across common benchmarks, and also includes head-to-head comparisons against recent subgraph-based approaches on representative datasets.\n\n- Ablations and perturbations. The paper includes masking experiments and centrality-choice ablations, which qualitatively support the claim that preserving high-centrality neighborhoods can be beneficial.\n\n- Complexity discussion. The authors explicitly break down costs for centrality computation, τ-hop extraction, and fusion, and discuss which components dominate in practice.\n\n- Candid limitations. The paper acknowledges tie-breaking can harm permutation invariance, that the approach may be less relevant to strongly global tasks, and that centrality computation can be expensive—suggesting approximate proxies as potential drop-ins."}, "weaknesses": {"value": "1. Inconsistent aggregate improvement counts.\n   The abstract reports “improves performance in 94 of 110,” whereas a later section states “improves … in 96 of the 110 experimental configurations.” This discrepancy leaves the headline claim ambiguous and should be reconciled, including a precise definition of what constitutes a “configuration.”  \n\n2. Theory depends on strong, hard-to-verify assumptions.  \n   The central guarantee relies on assumptions A1–A3 and a condition comparing informativeness and contraction factors (e.g., \\(\\gamma \\ge \\eta\\)). The manuscript does not operationalize how these quantities could be estimated or validated on real data, limiting the practical applicability of the theorem and making it difficult to know when the stated improvement condition is likely to hold.  \n\n3. Fixed hyperparameters with limited sensitivity analysis.  \n   Core experiments hold \\(\\kappa\\) and \\(\\tau\\) fixed “for fair comparison,” and the paper also notes cases of underperformance without SP-specific tuning. Without sweeps or principled selection rules, it remains unclear how robust SP is to these choices and whether better settings could systematically reduce the underperforming cases.  \n\n4. Permutation invariance is compromised by tie-breaking.  \n   The method acknowledges that deterministic tie-breaking by node index is not permutation-invariant. However, there is no empirical quantification of how often ties occur or how much this affects reported gains, nor a validated canonicalization remedy.  \n\n5. Disjointness heuristic may discard salient regions.  \n   Algorithmically, “later seeds skip covered regions,” enforcing disjoint \\(\\tau\\)-balls within a centrality type. In graphs where discriminative motifs cluster, this can exclude adjacent high-centrality neighborhoods and reduce coverage; the empirical impact of this design choice is not analyzed.  \n\n6. Cost claims vs. centrality complexity.\n   While the paper emphasizes negligible overhead in the narrative, the complexity section lists betweenness/closeness costs that can scale as \\(O(nm)\\) (or worse when weighted), which can be substantial on large graphs. The experiments do not include wall-clock/runtime studies to reconcile these statements.  \n\n7. Narrow centrality family despite rich literature. \n   The method and ablations restrict attention to three classic centralities (degree, betweenness, closeness). Given the breadth of widely-used measures (e.g., PageRank, eigenvector, Katz, k-core, HITS, flow-based variants), the paper should justify the restriction or include an empirical study demonstrating that the SP effect is not contingent on this particular trio.  \n\n8. Overlap policy across centralities is under-analyzed.\n   Subgraphs are disjoint within each centrality type but can overlap across types. The redundancy/diversity induced by cross-type overlap is not quantified, leaving open whether the concatenated embeddings capture complementary regions or mostly re-encode the same structures."}, "questions": {"value": "1. Win-count discrepancy. Which figure—94/110 or 96/110—is correct, and how exactly are “configurations” defined across Tables 1–3?\n\n2. Sensitivity of \\(\\kappa\\) and \\(\\tau\\). Can you provide controlled sweeps or even a simple adaptive rule for selecting \\((\\kappa,\\tau)\\), and report performance/cost trade-offs across datasets?\n\n3. Permutation-invariant selection.  Could you implement a canonical tie-break (e.g., WL-based hashing or a symmetric learned rule) and show whether the reported gains persist?\n\n4. Beyond three centralities. What motivated the choice of degree/betweenness/closeness? Please include at least one additional practical centrality (PageRank/eigenvector/Katz) in the centrality-choice ablation.\n\n5. Runtime evidence. Please report preprocessing and end-to-end wall-clock time and memory (with/without SP) on the largest datasets to support the “negligible overhead” claim.\n\n6. Effect of disjointness. How often does “skip covered regions” exclude adjacent high-centrality neighborhoods, and does allowing controlled overlap improve performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CfczKQOPP3", "forum": "Weq8USUt5u", "replyto": "Weq8USUt5u", "signatures": ["ICLR.cc/2026/Conference/Submission16998/Reviewer_afsn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16998/Reviewer_afsn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804928756, "cdate": 1761804928756, "tmdate": 1762927017511, "mdate": 1762927017511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the rank collapse problem in deep message-passing neural networks, where repeated neighborhood aggregation causes node embeddings to converge to a one-dimensional subspace dominated by the leading eigenvector of the normalized adjacency matrix. The authors propose the \"Subgraph Plug-in\", which is a lightweight, architecture-agnostic module that does the following: 1) identifies high-centrality nodes using degree, betweenness, and closeness measures, 2) extracts their k-hop neighborhoods as subgraphs, and 3) concatenates these subgraph embeddings with the global graph representation at readout. The approach is evaluated across 11 graph classification benchmarks and 13 GNN variants, showing improvements in 94 out of 110 configurations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper conducts a thorough empirical evaluation, testing across 13 graph classification benchmarks and 13 different GNN architectures (GCN, GIN, GAT, GraphSAGE, various pooling methods). The depth of the evaluation strengthens the claim that “SP is architecture-agnostic and broadly applicable”, though the depth of analysis for each combination is limited.\n2. The plug-in nature of SP is genuinely practical. It requires no modifications to the base GNN architecture and works by augmenting the readout phase only. The implementation involves computing centrality scores once as preprocessing, extracting κ subgraphs of radius τ, encoding them with the same base GNN, and concatenating at readout via Equation 5. This simplicity means existing trained models could potentially be enhanced without retraining from scratch, though the paper doesn't explore this possibility.\n3. The paper uses three complementary centrality measures (degree, betweenness, closeness), with each capturing different structural aspects. The ablation in Table 6 shows that while individual centralities help, combining all three consistently yields the best performance. For disconnected graphs, the authors properly handle the closeness centrality using harmonic closeness. This multi-faceted approach to identifying important nodes is more robust than relying on a single measure.\n4. The centrality computation happens once per graph and can be cached, as stated in Section 3.5. While betweenness has O(nm) complexity, this is a one-time cost. During training, the only additional overhead is encoding κ additional subgraphs through the base GNN, which the authors claim is \"roughly depth-invariant.\" This makes the approach practical for scenarios with repeated training on the same graphs, though the actual timing comparisons promised in Appendices C.4 and C.5 would be crucial for validation."}, "weaknesses": {"value": "1. The theoretical analysis hinges on three assumptions (A1-A3) that could be justified better. A1 (task locality) assumes the label Y is independent of G\\S* given S*, which fails for tasks requiring global graph properties like diameter or connectivity. A2 and A3 involve parameters γ and η that cannot be measured in practice. The authors claim their \"ablations, perturbation tests, and depth-sensitivity studies collectively corroborate A1-A3,\" but the perturbation test (Table 5) only shows centrality masking hurts more than random masking. I am not sure this proves the label is independent of the rest of the graph.\n2. The paper acknowledges in Section 4.5 that \"deterministic tie-breaking by node index can, in principle, violate permutation invariance\", a fundamental property for graph methods. While the authors suggest fixes, these are not implemented or evaluated. \n3. Using centrality-based subgraph selection is well-established in graph analysis. I think the main contribution is applying this at the readout phase rather than during message passing, which is more of an engineering choice than a fundamental insight. The information-theoretic analysis (Theorem 1, Lemma 1) essentially states that adding more information doesn't decrease mutual information with the label, which is not very surprising.\n4. While the paper claims improvements in 94/110 cases, many gains are marginal. In Table 1, for instance, several improvements are within one standard deviation (e.g., GIN on MUTAG: 81.0±10.2 vs 82.5±9.9). The paper admits that 16 configurations show no improvement or degradation but attributes this to \"strict adherence to each model's original hyperparameters without any SP-specific tuning\". Yet this same constraint should apply to the positive results too.\n5. The paper fixes κ=2 and τ=4 across all experiments \"for fair comparison\" (Section 4.1), but this may be unfair since these are new hyperparameters that should be tuned per dataset like any other. The claim of using \"original hyperparameters without any SP-specific tuning\" is misleading when SP introduces its own hyperparameters. Additionally, comparing against methods like GCNII and U-Net that \"exhibit poor performance\" on ZINC/MolHIV (Table 4) inflates the relative performance of SP.\n6. The O(nm) complexity for betweenness centrality becomes difficult for large graphs. While the authors mention \"approximate centralities (e.g. PageRank proxies) are drop-in replacements\" in Section 4.5, they provide no evaluation of how these approximations affect performance. For \"web-scale graphs\" mentioned in the conclusion, even the O(n^2) space for storing all-pairs shortest paths becomes infeasible.\n7. The paper could include systematic studies on crucial design choices. There's no sensitivity analysis for κ and τ beyond the single ablation in Table 6. The fusion strategy is fixed to concatenation+MLP without exploring alternatives like attention-based fusion or learnable weighting. The choice of which centrality measures to use isn't justified beyond empirical performance. The enforced disjointness within centrality types (Algorithm 1) is not ablated to show its necessity."}, "questions": {"value": "1. How does performance vary with different values of κ and τ? Why were these fixed at 2 and 4?\n2. Can you provide statistical significance tests for the claimed improvements?\n3. How does the method perform when task labels depend primarily on global graph properties rather than local substructures?\n4. What is the actual computational overhead in practice for large-scale graphs (>10k nodes)?\n5. How does the disjointness enforcement in Algorithm 1 affect the selected subgraphs when high-centrality nodes are clustered?\n6. Have you considered learnable centrality measures rather than fixed graph-theoretic ones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A56fO16CzI", "forum": "Weq8USUt5u", "replyto": "Weq8USUt5u", "signatures": ["ICLR.cc/2026/Conference/Submission16998/Reviewer_5agY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16998/Reviewer_5agY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926703094, "cdate": 1761926703094, "tmdate": 1762927016975, "mdate": 1762927016975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper theoretically analyze the phenomenon of rank collapse problem in MPNNs, and we propose a lightweight plug-in called SP that allow GNNs to focus on subgraph informations.\nExtensive experiments are conducted over 11 graph benchmarks with 13 GNN variants to validate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and the key ideas are clearly presented.\n2. The authors provide theoretical analysis on the proposed method.\n3. The experiments are comprehensive and the results are convincing."}, "weaknesses": {"value": "Notations need to be carefully defined in the preliminaries and theorem sections. For example, I() is the mutual information? \n\nThe assumptions are too strong, especially A2. Besides the empirical evidence, is there any additional theoretical analysis to support or validate this assumption?\n\nMoreover, theorem 1 seems intuitive, based on the assumption A2 and A3. What is the purpose to provide this theorem?"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0YGqLAtius", "forum": "Weq8USUt5u", "replyto": "Weq8USUt5u", "signatures": ["ICLR.cc/2026/Conference/Submission16998/Reviewer_KhZ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16998/Reviewer_KhZ7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16998/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011628380, "cdate": 1762011628380, "tmdate": 1762927016360, "mdate": 1762927016360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}