{"id": "qPUQMLRBsT", "number": 21216, "cdate": 1758315037638, "mdate": 1759896934178, "content": {"title": "LLE-MORL: Locally Linear Extrapolation of Policies for Efficient Multi-Objective Reinforcement Learning", "abstract": "Multi-objective reinforcement learning (MORL) aims at optimising several, often conflicting goals in order to improve the flexibility and reliability of RL in practical tasks. This can be achieved by finding diverse policies that are optimal for some objective preferences and non-dominated by optimal policies for other preferences so that they form a Pareto front in the multi-objective performance space. The relation between the multi-objective performance space and the parameter space that represents the policies is generally non-unique, and we provide new insights into this by formalising a local parameter-performance relationship. Using a training scheme based on the local parameter-performance relationship, we propose LLE-MORL, a method that directly extrapolates a small set of base policies to efficiently trace out a high-quality Pareto front. Experiments conducted with and without retraining across different domains show that LLE-MORL consistently achieves higher Pareto front quality and efficiency than state-of-the-art approaches.", "tldr": "", "keywords": ["Multi-objective Optimization", "Reinforcement Learning", "Interpretability", "Pareto Front"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dec240d18291ec046441151ef974af3205e823b0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a \"parameter-performance relationship\" (PPE) in multi-objective reinforcement learning (MORL). PPE is an empirical insight that small shifts in the parameter space result in predictable small shifts in the performance space. The paper formalizes this insight and devises an algorithm, called LLE-MORL, that exploits it for more sample-efficient Pareto front exploration in MORL. The algorithm works by first searching an initial set of policies using PPO. Then, these policies are retrained to get directions for extrapolation. Further points on the Pareto front are then obtained using linear inter- and extrapolation of the weight vectors from the initial to retrained ones. A final fine-tuning stage then ensures that the extrapolated policies stay close to the Pareto front.\n\nThe paper poses a theorem that LLE-MORL will faithfully reconstruct the Pareto front up to a certain precision if some assumptions are met. LLE-MORL is then empirically evaluated against multiple baselines on standard MORL environments and found to be consistently superior."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea that parameter space changes are in a structured way related to changes in the space of returns is a fertile one\n- The proposed algorithm enjoys high sample-efficiency due to interpolating the policies on the Pareto front rather than searching for them from scratch\n- A comprehensive set of environments and tracked metrics is selected when evaluating the method1"}, "weaknesses": {"value": "# Theory\nThe theoretical discussion in the paper is not sound. \n\nFirst, the Parameter-Performance Relationship (PPR) is defined in L182-186 in a way that makes it clear it will almost never hold in practice. The key problem is that the value $h(\\Delta\\theta)$ in the definition does not depend on $\\theta$, only on $\\Delta \\theta$. Even if $V(\\theta+\\Delta\\theta)-V(\\theta)=h(\\Delta\\theta)$ only holds when $\\Vert\\Delta\\theta\\Vert<\\delta$, this implies that the function $V(\\theta)$ is _globally affine_. To see that,\n$$h(\\Delta\\theta_1+\\Delta\\theta_2)=V(\\theta+\\Delta\\theta_1+\\Delta\\theta_2)-V(\\theta)=[V(\\theta+\\Delta\\theta_1+\\Delta\\theta_2)-V(\\theta+\\Delta\\theta_1)]+[V(\\theta+\\Delta\\theta_1)-V(\\theta)]=h(\\Delta\\theta_2)+h(\\Delta\\theta_1).$$\nSo $h$, at least in a small ball, satisfies the Cauchy functional equation. Then, under mild regularity conditions, it is linear. We can then tile the entire parameter space with overlapping balls of radius $<\\delta$ to show global linearity. This implies that $V\n(\\theta)=V(\\mathbf{0})+h(\\theta)$ is affine. This in turn implies that straight lines in the parameter space should translate into straight lines in the performance space, which is clearly not true even from Fig. 3 in the paper (the curves on the figure are not straight). Such affinity would also break under reparameterization. Even in the simplest case where a tabular policy is parameterized directly by action probabilities, the value function depends non-linearly on the policy, see for example Fig. 4 in [1]. \n\nWhat the authors could potentially have in mind is that the function $V(\\theta)$ is often smooth, but this would be a much weaker statement that would still require additional scrutiny.\n\nSecond, the theorem on L286-289 does not stand to the standards of mathematical rigor. The assumptions A1-A4 are only defined in sketches. It is not stated what it would mean for the algorithm to reconstruct a part of the Pareto front \"to resolution $\\Delta\\alpha$.\" Furthermore, even if we ignore this omission, we can always set $\\Delta\\alpha$ to be large enough for the theorem to be trivially satisfied. The assumptions 1-4 are also only stated in natural language, without proper mathematical notation. They also seem to contradict each other. In particular, a differentiable manifold is standardly defined through differentiable (hence continuous) maps from Euclidean space, meaning that it would of course contain what the paper refers to as \"cluster points\" beyond some edge cases.  The \"proof,\" deferred to the appendix, turns out to be a three-paragraph proof sketch.\n\n[1] Dadashi, Robert, et al. \"The value function polytope in reinforcement learning.\" International Conference on Machine Learning. PMLR, 2019.\n\n# Missed prior work\nThe idea behind LLE-MORL that we can linearly interpolate in the reward space to get the nearly-Pareto optimal rewards has also been proposed in the literature under the name of \"reward soups\" [2]. The paper regrettably fails to cite this prior work. As it stands, LLE-MORL could be seen as an incremental improvement to the method of reward soups evaluated on different environments ([2] works with text and image generation), but the novelty is highly limited. \n\n[2] Rame, Alexandre, et al. \"Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards.\" Advances in Neural Information Processing Systems 36 (2023): 71095-71134.\n\n# Problems with experimental evaluation\nRL algorithms in general, and especially MORL, are known to be particularly sensitive to hyperparameters. From the experimental sections in the paper and in the appendix, I cannot see any attempts to tune the hyperparameters of the methods. I understand that the computational resources are limited (from table 6), but in such cases it is standard to do hyperparameter tuning in a reduced setup, e.g. when running for fewer total timesteps. I couldn't find the hyperparameters that were selected for any of the baselines either.\n\nI am especially suspicious that the paper fails to tune the hyperparameters correctly in light of the results in Figure 3. It claims to obtain the \"original policy\" weights by running PPO \"to convergence.\" However, Figure 3 (c) reveals that the original policy is highly Pareto-non optimal (regardless of relative reward weights), since there is a better policy on the top right of the figure.\n\nIn this form, the paper cannot be accepted to the conference."}, "questions": {"value": "- What are the x- and y-axes of the heatmap in Figure 2 (a)?\n- When defining Hungarian distance, how do you efficiently search for permutations that make the networks closest?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vP1yXDbZ4r", "forum": "qPUQMLRBsT", "replyto": "qPUQMLRBsT", "signatures": ["ICLR.cc/2026/Conference/Submission21216/Reviewer_Qph7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21216/Reviewer_Qph7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760514402, "cdate": 1761760514402, "tmdate": 1762941626936, "mdate": 1762941626936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-objective reinforcement learning (MORL) algorithm for approximating a Pareto front of conflicting reward functions (objectives). The paper discusses the relationship between the performance space of a policy (its multi-objective expected return) and the parameter space of a policy e.g., the weights of a neural network). Then, the authors propose a population-based MORL method that generates novel policies via Locally Linear Extension (LLE). That is, for each policy, the method computes a vector of the differences between the policy network weights before and after being trained for a different preference over objectives. Next, the method generates novel policies by simply adding this parameter change to the original network weights using different scale factors. All policies generated using this process are then filtered, retaining only the non-dominated solutions. The method is evaluated and compared with existing MORL methods in the multi-objective MuJoCo benchmark from MO-Gymnasium."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem of approximating a Pareto front of policies in RL is of interest to the community, and methods that can tackle this problem efficiently are highly relevant.\n- The proposed method is simple to implement and computationally efficient."}, "weaknesses": {"value": "- The paper misses discussing two very related and similar methods in the MORL literature (see [1] and [2] below). The work in [1] also explores the relationship between the performance and parameter spaces of Pareto optimal policies. However, instead of simply interpolating neural network weights, they train a hypernet to output the parameters of new policies given a preference vector. The general idea of their work is quite similar, and I would expect that their hypernet produces better results than the author’s proposed interpolation method, due to its higher expressiveness.\nMoreover, the work in [2] also proposes to interpolate the weights of multiple neural network policies to generate other Pareto undominated policies efficiently (although in the RLHF setting).\n\n- The proposed method has 8 different parameters that need to be tuned (see Table 5 in Appendix C.5). It seems these parameters were tuned during the sensitivity analysis in Appendix F. However, it is not detailed how the hyperparameters of the competing methods were tuned. It is not possible to infer whether the results are due to misrepresented baselines/lack of hyperparameter tuning, or due to the algorithmic contributions.\n- The authors state they used multiple random seeds. Please state how many random seeds and what the dispersion metric was used in the results in the tables (standard deviation, standard error, etc).\n- It is unclear whether the method can scale or would be easily applicable to domains with more than $d=2$ objectives.\n\n[1] Shu, Tianye, Ke Shang, Cheng Gong, Yang Nan, and Hisao Ishibuchi. ‘Learning Pareto Set for Multi-Objective Continuous Robot Control’. Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, 2024.\n\n[2] Rame, Alexandre, Guillaume Couairon, Corentin Dancette, et al. ‘Rewarded Soups: Towards Pareto-Optimal Alignment by Interpolating Weights Fine-Tuned on Diverse Rewards’. Thirty-seventh Conference on Neural Information Processing Systems, 2023."}, "questions": {"value": "Below, I have some suggestions and questions for the authors:\n\n- Since it is assumed a linear scalarization function, it is useful to define a convex coverage set (CCS) as the set of optimal policies in the convex region of the Pareto front in Section 2.2. It is known that you can not identify points in concave regions of the Pareto front using linear scalarization.\n\n- The quality of the plots in Figure 2 is very low. The authors should re-generate them with higher resolution.\n\n- In Definition 2, the Pareto front is defined using the letter $\\mathcal{F}$, but in Section 3.5, the authors use the letter $P$.\n\n- The assumptions A1 to A4 should be motivated and justified. For instance, I do not think it is trivial to guarantee the validity of A4.\n\n- Hypervolume (HV) and Expected Utility (EU) -> The acronyms are being redefined multiple times in the text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yoXPioR7VQ", "forum": "qPUQMLRBsT", "replyto": "qPUQMLRBsT", "signatures": ["ICLR.cc/2026/Conference/Submission21216/Reviewer_eZe9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21216/Reviewer_eZe9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832528718, "cdate": 1761832528718, "tmdate": 1762941626002, "mdate": 1762941626002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel approach for probing the Pareto-front of multi-objective reinforcement learning problems. The key idea of this approach is to take a pair of near-optimal policies parametrized by similar parameter vectors and evaluate a sequence of \"intermediate policies\" to map the appropriate piece of Pareto-front. Intermediate policies are obtained by sampling parameter vectors on the linear interval between the two original policies in the parameter space."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Novel approach for probing the Pareto front in MORL."}, "weaknesses": {"value": "W1) The notion of Parameter-Performance Relationship (PPR) the authors introduce seems to basically require linearity of V in the region U, which is unlikely to hold for any meaningful region in general. The notion of local PPR is then used in the text without being explicitly defined. Presumably, local PPR is just differentiability in a particular point.\n\nW2) The proposed theorem feels too vague to be useful. The meaning of \"reconstructs this part of the Pareto front up to a resolution\" is not clear.\n\nW3) The meaningfulness of the comparisons presented in evaluation is not clear to me. Different approaches produce different numbers of points in the performance space. Different approaches seem to have different computational budgets. The only thing that is similar between the approaches in the two presented settings is the \"number of training steps\". However, this number only controls training a single base policy, while any re-training steps or the number of base policies are\nnot controlled by this parameter."}, "questions": {"value": "Q1) There are several mentions of \"structurally similar policies\" throughout the text, but the notion is never defined. What structure is implied here and why is it similar between the mentioned pairs of policies?\n\nQ2) Figures 4 and 5 seem to indicate that the base policies themselves perform much better than the competition. Could the overall performance improvement then be attributed to simply having better training routine than what's used in the other approaches? I would love to see an additional ablation experiment, where the original 6 base policies are output as is with no further modifications. Such an experiment would help determine how performant the LLE process is by itself."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q6YIPE2diY", "forum": "qPUQMLRBsT", "replyto": "qPUQMLRBsT", "signatures": ["ICLR.cc/2026/Conference/Submission21216/Reviewer_T2kU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21216/Reviewer_T2kU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762293098748, "cdate": 1762293098748, "tmdate": 1762941625673, "mdate": 1762941625673, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LLE-MORL, a new method for multi-objective reinforcement learning (MORL) that leverages a locally linear parameter-performance relationship (PPR) between policies. The authors show that small, structured weight updates caused by short retraining under new preferences can be used to extrapolate additional candidate policies without full training. The key idea is to:\n1.\tTrain several base policies under different preference vectors.\n2.\tConduct short retraining with neighboring preferences to acquire directional parameter updates.\n3.\tLinearly extrapolate parameters along these directions to produce candidate policies.\n4.\tPerform short fine-tuning to refine each candidate back to the Pareto front.\nExperiments across continuous-control MO-Gym environments demonstrate both sample-efficiency and improved Pareto-front quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Novel conceptual insight: The work introduces and formalizes the parameter-performance relationship (PPR) for MORL and empirically validates it, which is conceptually interesting and bridges representation geometry and RL optimization.\nAblation study demonstrates necessity of retraining vs. extension and the benefit of fine-tuning.\nTheoretical characterization of when the method reconstructs the Pareto front."}, "weaknesses": {"value": "Limited benchmark diversity: Only continuous control environments from MO-Gymnasium are used. Additional settings (e.g. discrete tasks, real-world robotics, offline MORL) would strengthen conclusions.\nComputational trade-offs: While extrapolation is cheap, initial base policy training + retraining overhead may be non-trivial. A fairer wall-clock runtime comparison table would be helpful.\nComparisons with related work on Pareto front analysis: A more thorough comparison against some similar Pareto front analysis for MORL.\nWriting style: The paper is written in a dense form and requires a thorough reading, there are references that are missing names of all authors or incomplete information. Please increase the font sizes of all text in all plots especially in Figures 3, 4 and 5. Also the numbers in the Tables are really small while the figures can be re-positioned so that it allows for additional space and increase the font sizes."}, "questions": {"value": "1.\tHow sensitive is performance to \\alpha step sizes and number of base policies?\nCan adaptive \\alpha or uncertainty-based step-control improve robustness?\n2.\tHave you tested higher-dimensional objectives (d > 2)?\nDoes efficiency degrade in practice following the theoretical M^{n-1}complexity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UobNjsSq9U", "forum": "qPUQMLRBsT", "replyto": "qPUQMLRBsT", "signatures": ["ICLR.cc/2026/Conference/Submission21216/Reviewer_xqCc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21216/Reviewer_xqCc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762321939587, "cdate": 1762321939587, "tmdate": 1762941625350, "mdate": 1762941625350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}