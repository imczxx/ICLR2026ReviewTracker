{"id": "WZWLnivZAj", "number": 8827, "cdate": 1758099453299, "mdate": 1763647481709, "content": {"title": "Adversarial Unlearning of Poisoned Features for Backdoor Defense in Federated Learning", "abstract": "Federated learning (FL) enables collaborative model training without exposing local data but is highly vulnerable to backdoor attacks, particularly under non-IID client distributions and persistent malicious participants. Existing defenses often rely on robust aggregation or auxiliary data, yet their effectiveness diminishes under challenging conditions such as low poisoning ratios and heterogeneous data, and they remain susceptible to adaptive or stealthy adversaries. We propose \\emph{adversarial unlearning of poisoned features} (AUPF), an in-training defense that generates adversarial perturbations on benign clients to expose vulnerable decision boundaries and explicitly regularizes the feature representations of clean and perturbed samples. This feature-level alignment suppresses poisoned associations and ensures that robustness acquired locally propagates to the global model despite dynamic updates and client heterogeneity. We design a bi-level optimization framework that integrates seamlessly with FL training and show that it achieves computational efficiency comparable to lightweight baselines while avoiding the scalability issues of prior defenses. Extensive experiments across diverse datasets, attack strategies, and non-IID scenarios demonstrate that AUPF consistently achieves lower attack success rates while maintaining high clean accuracy, establishing it as an effective and scalable defense for backdoor-resilient federated learning.", "tldr": "", "keywords": ["Federated learning", "backdoor defense"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/19e9c5454c9ffb3442292fead194385f348d5616.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Adversarial Unlearning of Poisoned Features (AUPF), a client-side defense framework for backdoor-resilient federated learning. The method introduces a bi-level optimization process where benign clients generate universal adversarial perturbations to expose fragile decision boundaries and perform feature-level alignment between clean and perturbed samples. This joint adversarial and feature-space unlearning aims to suppress poisoned associations in local models and propagate robustness to the global model through standard aggregation. Extensive experiments across CIFAR-10, CIFAR-100, and FashionMNIST demonstrate that AUPF achieves lower attack success rates and stable performance under non-IID data compared with eleven representative defenses."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a novel client-side defense, AUPF, that integrates universal perturbation and feature-level alignment for backdoor mitigation in FL.\n\n2. The motivation of performing adversarial unlearning to eliminate poisoned features is clear and intuitively appealing.\n\n3. Experiments are comprehensive, including diverse datasets, four attack types, and eleven baseline methods.\n\n4. Results show strong empirical robustness under non-IID data and adaptive attacks, indicating good practical relevance."}, "weaknesses": {"value": "1. Unclear distinction from prior adversarial training works.\nThe manuscript does not sufficiently differentiate AUPF from existing adversarial training or adversarial unlearning approaches (e.g., Wei et al., NeurIPS 2023; Zeng et al., ICLR 2022). It remains unclear what conceptual or algorithmic novelty AUPF introduces beyond combining universal perturbations with feature alignment.\n\n2. Unconvincing discussion on centralized adversarial training.\nThe claim that centralized adversarial training cannot handle dynamic models or data partitioning in FL is not well supported. In practice, centralized approaches can also recompute universal perturbations for each global model update. Moreover, the argument about the lack of full data access should be supported by citations and empirical evidence showing that local-only perturbation generation indeed limits robustness propagation.\n\n3. Lack of theoretical robustness analysis.\nAlthough the feature alignment term is a key contribution, the paper lacks a formal robustness or stability analysis showing how this term guarantees representation consistency under non-IID conditions.\n\n4. Missing sensitivity analysis on attacker ratio.\nThe defense performance of AUPF may degrade as the proportion of malicious clients increases, since the perturbation direction estimated by benign clients can deviate from true poisoned gradients. A quantitative study on this sensitivity would strengthen the work."}, "questions": {"value": "1. What are the key algorithmic differences between AUPF and previous adversarial unlearning methods beyond feature alignment?\n\n2. How does AUPF behave when malicious clients exceed 30%? Is there a sharp or gradual degradation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "40lpwFSmDo", "forum": "WZWLnivZAj", "replyto": "WZWLnivZAj", "signatures": ["ICLR.cc/2026/Conference/Submission8827/Reviewer_2GMK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8827/Reviewer_2GMK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761618851585, "cdate": 1761618851585, "tmdate": 1762920596626, "mdate": 1762920596626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents AUPF, a federated backdoor defense method that uses universal adversarial perturbations and feature-level alignment at benign clients to “unlearn” poisoned representations. The approach relies on bilevel optimization: inner maximization to obtain universal perturbations, and outer minimization to align clean and perturbed representations locally before standard FedAvg aggregation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses an important problem in FL security.\n2. Claims to work without auxiliary clean data and under non-IID settings.\n3. Includes multiple backdoor attacks and several baselines in experiments."}, "weaknesses": {"value": "1. The main pipeline is a direct combination of universal adversarial perturbations, standard adversarial training, and feature alignment loss. These components are well-established. The paper largely re-packages them as “adversarial unlearning” without introducing fundamentally new ideas or theoretical insights.\n\n2. The work repeatedly claims to unlearn poisoned features, but no formal unlearning objective is defined, no theoretical guarantees are offered, and no mechanistic analysis or interpretability evidence is shown. In practice, the method behaves like adversarial regularization, not unlearning.\n\n3. The adaptive threat model is weak. Modern backdoor defenses must evaluate 1) attacker aware of defense, 2) tailored gradient manipulation attacks, and 3) clean-label and semantic backdoors. None of these is rigorously studied. Thus, robustness claims are questionable.\n\n4. Despite claiming practical FL relevance, many scenarios, such as partial participation or model personalization/heterogeneous heads etc, should be considered for practical deployment viability.\n\n5. The method implicitly assumes benign clients can reliably extract poisoned directions by maximizing loss. This only holds when the global model is already reasonably clean and a benign majority dominates. This is a standard assumption in FL defenses, not a new mechanism, and reduces novelty.\n\n6. Important baselines are missing, e.g., recent certified/robust aggregation defenses, advanced trigger-inversion defenses, and clean-label adaptive attacks. The provided baselines appear selected to favor the proposed method.\n\n7. ~2.5× per-round slowdown vs. FedAvg is non-trivial, yet communication and scalability impacts are not measured (e.g., ResNet50, ViT, FL on large models)."}, "questions": {"value": "1. How does this differ fundamentally from standard adversarial training in FL?\n\n2. How does the method perform under partial participation and model personalization?\n\n3. Where are strong adaptive attack results (e.g., semantic triggers, clean-label attacks)?\n\n4. Can the approach scale to foundation-model FL or cross-device real-world settings?\n\n5. Can the authors explain and provide evidence that poisoned features are unlearned, not merely regularized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nJNQdaziDx", "forum": "WZWLnivZAj", "replyto": "WZWLnivZAj", "signatures": ["ICLR.cc/2026/Conference/Submission8827/Reviewer_xjUd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8827/Reviewer_xjUd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795227286, "cdate": 1761795227286, "tmdate": 1762920594946, "mdate": 1762920594946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Adversarial Unlearning of Poisoned Features (AUPF), a backdoor defense mechanism for Federated Learning (FL) that eliminates backdoors by performing adversarial unlearning locally on benign clients. AUPF generates universal adversarial perturbations and trains the model by combining prediction-level unlearning with a key feature-level alignment regularization term. In this way, this method is robust in non-IID settings and can withstand continuous and adaptive attacks. Experimental results show that AUPF achieves lower ASR and higher RPG than those of existing defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The proposed client-side defense paradigm bypasses the Non-IID difficulties associated with server-side aggregation and offers practical advantages such as requiring no auxiliary data and compatibility with standard FedAvg.\n\n2.This paper proposes adversarial unlearning-based feature-level alignment, conceptually addressing the difficulty of transferring local robustness to the global model in non-IID scenarios.\n\n3.The paper is well-written and easy to follow."}, "weaknesses": {"value": "1.Although AUPF achieves the lowest ASR on CIFAR-100, the clean accuracy is notably lower than that of FedAvg. While this is a common trade-off in defense mechanisms, and the RPG metric still favors AUPF, the paper would benefit from a brief discussion on this point. Is this drop due to the inherent complexity of the task, the effectiveness of the defense, or a hyperparameter sensitivity specific to larger class spaces?\n\n2.Key SOTA attack evaluations are missing, particularly A3FL [1] (which targets unlearning mechanisms) and Chameleon [2] (a stealthy attack). In practice, a more sophisticated adaptive attacker might employ a different, perhaps more targeted, adversarial training strategy to reinforce the backdoor.   A discussion on the potential limitations of the current adaptive attack model and how AUPF might fare against other adaptive strategies would be valuable.\n\n3.The evaluation overlooks comparisons with state-of-the-art defenses designed against adaptive attacks, such as AlignIns [3].\n\nReference:\n\n[1]Zhang, Hangfan, et al. A3FL: adversarially adaptive backdoor attacks to federated learning. NeurIPS’ 2023.\n\n[2]Yanbo Dai, Songze Li. Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning. ICML2023.\n\n[3] Jiahao Xu, Zikai Zhang, et al. Detecting Backdoor Attacks in Federated Learning via Direction Alignment Inspection. CVPR 2025."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ha7oJmoqvz", "forum": "WZWLnivZAj", "replyto": "WZWLnivZAj", "signatures": ["ICLR.cc/2026/Conference/Submission8827/Reviewer_ppHF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8827/Reviewer_ppHF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8827/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896598791, "cdate": 1761896598791, "tmdate": 1762920594185, "mdate": 1762920594185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}