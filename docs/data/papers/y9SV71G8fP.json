{"id": "y9SV71G8fP", "number": 3693, "cdate": 1757497738901, "mdate": 1759898074699, "content": {"title": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation", "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities on complex problems using long Chain-of-Thought (CoT) reasoning. However, they often suffer from overthinking, meaning generating unnecessarily lengthy reasoning steps for simpler problems. This issue may degrade the efficiency of the models and make them difficult to adapt the reasoning depth to the complexity of problems. To address this, we introduce a novel metric **T**oken **E**ntropy **C**umulative **A**verage (**TECA**), which measures the extent of exploration throughout the reasoning process. We further propose a novel reasoning paradigm---Explore Briefly, Then Decide---with an associated **C**umulative **E**ntropy **R**egulation (**CER**) mechanism. This paradigm leverages TECA to help the model dynamically determine the optimal point to conclude its thought process and provide a final answer, thus achieving efficient reasoning. Experimental results across diverse mathematical benchmarks show that our approach substantially mitigates overthinking without sacrificing problem-solving ability. With our thinking paradigm, the average response length decreases by up to 71% on simpler datasets, demonstrating the effectiveness of our method in creating a more efficient and adaptive reasoning process.", "tldr": "", "keywords": ["LLM", "LongCoT", "overthinking", "entropy", "nlp"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f426ba7ca381ef74b746100a4155d22e0cbb836.pdf", "supplementary_material": "/attachment/8531511a7451c307adb802efe6c5d423b229c8f3.zip"}, "replies": [{"content": {"summary": {"value": "The paper observes that in overthinking models, the token entropy cumulative average (TECA) continuously increases during the thinking process. Based on this finding, the authors propose an entropy-based reinforcement learning algorithm that rewards the model based on the TECA during inference, encouraging the model—when producing correct answers—to use as few tokens as possible."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper experimentally demonstrates that TECA exhibits an upward trend during the thinking process of overthinking models.  \n2. The proposed TECA-based reward mechanism reduces the average output length without significantly compromising model accuracy."}, "weaknesses": {"value": "1. Prior work has shown that models may generate correct answers despite erroneous reasoning steps. Could the proposed method exacerbate this phenomenon?  \n2. Would reducing TECA negatively impact the model’s exploratory behavior and creativity? Specifically, might the model under-explore and thus underperform on complex problems?  \n3. The proposed algorithm appears to primarily balance performance and output length relative to the baseline. Could the authors plot results from different models on the same graph to demonstrate that their method indeed lies on the Pareto frontier between accuracy and output length?"}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qBu8K4IsbM", "forum": "y9SV71G8fP", "replyto": "y9SV71G8fP", "signatures": ["ICLR.cc/2026/Conference/Submission3693/Reviewer_nNHM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3693/Reviewer_nNHM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547506463, "cdate": 1761547506463, "tmdate": 1762916927077, "mdate": 1762916927077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework to address over-thinking in large language models by defining the new metric Token Entropy Cumulative Average (TECA) to quantify exploration during reasoning. Building on TECA, the authors propose the “Explore Briefly, Then Decide” paradigm alongside a Cumulative Entropy Regulation (CER) mechanism, which guides models to terminate reasoning when sufficient certainty is reached. Empirical results on multiple mathematical benchmarks show substantial reductions in response length while maintaining solving capability. The primary contributions are the TECA metric, the CER-based reasoning paradigm, and strong evidence of improved efficiency in reasoning without sacrificing accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.Introduces a clear, quantitative metric for exploration behavior. TECA provides a novel and interpretable way to monitor when a model is overthinking by measuring uncertainty accumulation during reasoning.\n2.Demonstrates large efficiency gains without major accuracy loss. On arithmetic and mathematics benchmarks the method reduced average reasoning length by up to 71 % while maintaining solving performance, showing strong practical value."}, "weaknesses": {"value": "1.The paper focuses heavily on mathematical reasoning benchmarks, but the generalizability of the proposed TECA metric and CER mechanism to open-ended or conversational tasks is not demonstrated.\n2.There is limited discussion on the potential trade-off between exploration suppression and creativity or robustness in reasoning. Suppressing exploration might hurt performance on novel or adversarial examples, but this risk is not fully evaluated.\n3.In the CER mechanism, the segmentation of reward and the entropy threshold used to determine when to stop exploration or adjust weights appear to require manual tuning. The paper does not include an ablation study examining these hyper-parameters, nor does it describe the detailed workflow for adjusting them.\n4.Although response length reduction is compelling, the accuracy drop or change in reasoning style and explanation detail is less thoroughly analyzed, making it harder to assess practical impacts on downstream use-cases."}, "questions": {"value": "1.While CER has generally reduced output length while maintaining accuracy on multiple mathematical benchmarks, does it still suppress meaningful reasoning steps? Does the model exhibit a tendency to halt reasoning prematurely?\n2.The current experiments are primarily based on mathematical reasoning benchmarks, but I wonder whether this approach can be generalized to more open-ended or non-mathematical reasoning tasks and undergo broader testing?\n3.Based on the paper's description, the Cumulative Entropy Regulation framework appears to involve threshold setting and parameter tuning based on TECA. Could you present a sensitivity analysis demonstrating how different threshold settings impact the results?\n4.The authors indicate that while the average response length was significantly reduced across multiple benchmarks, the accuracy rate only experienced a slight decline. However, can accuracy rates alone in mathematical reasoning benchmarks fully represent the completeness and effectiveness of the reasoning process? Does CER potentially lead to the truncation of certain important reasoning exploration steps?\n5.The paper only provides theoretical analysis regarding the boundary tokens between exploration and decision-making. Could you present concrete examples of reasoning trajectories that illustrate the transition points between exploration and decision-making? Including such examples in the appendix could better support the theory proposed in the paper.\n6.While reasoning efficiency has been improved, does CER training significantly increase training costs? How much additional training cost does it incur compared to the baseline?\n7.We note that the experiments were conducted solely on Qwen3-4B and Qwen3-8B models. Can this methodology be extended to larger-scale models or different architectures? Does TECA's effectiveness remain consistent in larger-scale models?\n8.I remain skeptical that CER can consistently reduce redundant reasoning while maintaining answer completeness for every test sample. Have there been instances where CER led to performance degradation or premature termination? What mechanisms have the authors implemented to mitigate such issues?\n9.The \"Explore Briefly, Then Decide\" concept proposed in the paper is quite compelling. I wonder whether it could be integrated with other early-exit reasoning mechanisms to achieve better utility?\n10.The experimental analysis in the paper appears overly simplistic, lacking ablation studies on components such as the segmented reward mechanism and TECA metric. Furthermore, details regarding the training environment and duration are not provided. Appropriate supplementation in the appendix could substantially improve the completeness of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JetZTD9vMo", "forum": "y9SV71G8fP", "replyto": "y9SV71G8fP", "signatures": ["ICLR.cc/2026/Conference/Submission3693/Reviewer_oyuY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3693/Reviewer_oyuY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916154966, "cdate": 1761916154966, "tmdate": 1762916926208, "mdate": 1762916926208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TECA as a proxy for “exploration” during CoT inference, and introduces a GRPO-based reward shaping scheme (CER) that suppresses high TECA at the tail of the trajectory. Experiments show substantial reductions in average CoT length with minimal accuracy degradation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The TECA metric is intuitively interpretable, simple to compute, and shows reasonable empirical correlation with response length.\n\nThe qualitative case study indeed shows fewer redundant “reflective” tokens in the CER-trained model."}, "weaknesses": {"value": "**W1. Experimental scale is insufficient to support the claimed generality.**\nOnly Qwen3-4B and Qwen3-8B are used. GRPO RL training is notoriously sensitive to backbone initialization and emergent base-model bias. A two-model sample is not enough to justify the claim that the proposed mechanism reflects a general property of “overthinking” and adaptive reasoning. In fact, even for the 8B model, CER only provides a marginal advantage relative to CCoT in some metrics.\n\n**W2. The work should also report Pass@k.**\nLength reduction could come from eliminating alternative chains prematurely, not from eliminating irrelevant exploration. Pass@k is the standard instrument to distinguish “more decisive reasoning” from “shallower sampling.” Without this control, it is unclear whether CER suppresses useless branches or simply cuts off exploration globally.\n\n**W3. Conceptual analysis is shallow.**\nPreliminaries section repeats known GRPO and entropy definitions, while the core insight, *TECA is a principled proxy of exploration*, is not justified beyond curve visualizations. Two major omissions weaken the conceptual claim:\n- The paper only establishes correlation between TECA trajectories and response length. There is no causal diagnostic: no intervention study (injecting entropy spikes), no ablation vs. simpler uncertainty surrogates (e.g. logit margin / variance). Thus, it is not shown that TECA is the right variable. It may simply correlate with the visible surface symptom (redundant tokens).\n- The CER reward construction is not theoretically motivated. Rewarding TECA only on correct samples can encourage lucky short guesses, and the paper does not analyze this incentive misalignment. The design choices (equal weighting with accuracy reward, exponential form, applying TECA only at the end step) are hyperparameter-like knobs but are not justified mathematically."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0JqIoIaQZS", "forum": "y9SV71G8fP", "replyto": "y9SV71G8fP", "signatures": ["ICLR.cc/2026/Conference/Submission3693/Reviewer_6psw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3693/Reviewer_6psw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975555916, "cdate": 1761975555916, "tmdate": 1762916925336, "mdate": 1762916925336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper looks at the problem of overthinking in chain-of-thought reasoning efficiency by regulating token-level entropy. They propose a framework with two key components: TECA, which measures cumulative uncertainty, and CER, which penalizes excessive entropy to encourage concise reasoning. They explore (1) reasoning dynamics, where they analyze entropy changes during exploration and determination, and (2) RL integration, where they embed CER into GRPO to control reasoning depth. They also extend their work to mathematical reasoning benchmarks and show consistent reductions in reasoning length with minimal accuracy loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea of regulating token-level entropy to mitigate overthinking in LLM reasoning is both novel and well-motivated. Most existing work focuses on increasing reasoning accuracy through longer CoT generation or reinforcement learning rewards, but these approaches cannot adaptively control reasoning depth or prevent redundant exploration. This would limit their efficiency and interpretability. This paper introduces a method that improves reasoning efficiency without sacrificing accuracy. This makes the contribution practically impactful and valuable to the community.\n\n\nThe workflow is well-structured, as it integrates entropy-based reasoning control with GRPO to make the reasoning process adaptive and self-regulated. This method allows LLMs to decide when to stop exploring.\n\nThe experiments are extensive, with detailed analysis of the results. These experiments validate the effectiveness across different model sizes and reasoning complexities."}, "weaknesses": {"value": "i. The experiments are now all on math tasks. In math tasks the reasoning process usually follows a clear logical progression: i. the model explores multiple computational paths ii. converges to a definitive conclusion. In this setting, token-level entropy does effectively reflect the model’s degree of cognitive divergence. The cumulative entropy can capture the transition from exploration to determination relatively easily. The assumption may not work well on open-ended and/or textual tasks. E.g., QA tasks lack a clear boundary between exploration and determination. The model simultaneously retrieves facts, interprets semantics, and formulates answers. This could cause the TECA to mix different cognitive phases and produce noisy signals. For open-ended or multi-answer tasks, the evaluation signals are ambiguous and entropy penalties may prematurely constrain generation. I would suggest that the entropy-based works dig deeper into diverse tasks.\n\nii. The contribution of each component is not fully isolated. Need an ablation study.\n\niii. A stability and sensitivity study is needed, as the entropy-based rewards may be sensitive to temperature, model calibration, and/or decoding policy.\n\niv. I would suggest adding a clock time report or computational cost, although the reasoning length is reduced."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BhdZaIbtgD", "forum": "y9SV71G8fP", "replyto": "y9SV71G8fP", "signatures": ["ICLR.cc/2026/Conference/Submission3693/Reviewer_bUgf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3693/Reviewer_bUgf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762302521797, "cdate": 1762302521797, "tmdate": 1762916924644, "mdate": 1762916924644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}