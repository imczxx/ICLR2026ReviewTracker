{"id": "GoZyhk7NKP", "number": 8848, "cdate": 1758099836475, "mdate": 1759897759795, "content": {"title": "GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning", "abstract": "Graph Neural Networks (GNNs) are powerful tools for precessing relational data but often struggle to generalize to unseen graphs, giving rise to the development of Graph Foundational Models (GFMs). However, current GFMs are challenged by the extreme heterogeneity of graph data, where each graph can possess a unique feature space, label set, and topology. To address this, two main paradigms have emerged. The first leverages Large Language Models (LLMs), but is fundamentally text-dependent, thus struggles to handle the numerical features in vast graphs. The second pre-trains a structure-based model, but the adaptation to new tasks typically requires a costly, per-graph tuning stage, creating a critical efficiency bottleneck. In this work, we move beyond these limitations and introduce \\textbf{G}raph \\textbf{I}n-context \\textbf{L}earning \\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free architecture. GILT introduces a novel token-based framework for in-context learning (ICL) on graphs, reframing classification tasks spanning node, edge and graph levels in a unified framework. This mechanism is the key to handling heterogeneity, as it is designed to operate on generic numerical features. Further, its ability to understand class semantics dynamically from the context enables tuning-free adaptation. Comprehensive experiments show that GILT achieves stronger few-shot performance with significantly less time than LLM-based or tuning-based baselines, validating the effectiveness of our approach.", "tldr": "", "keywords": ["Graph Neural Networks", "Graph Foundational Models", "In-context Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6bc328f583d895027d1579013ea0077f1dd76f22.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GILT (Graph In-context Learning Transformer), which aims to be a tuning-free and LLM-free Graph Foundational Model (GFM). The idea is to reframe few-shot graph learning — including node, edge, and graph-level classification — as a token-based in-context reasoning problem. GILT tokenizes graph structures into a unified format, processes them using a Transformer designed for in-context learning, and predicts through a non-parametric prototypical head, avoiding gradient updates or external text encoders altogether."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The inference efficiency comparison (e.g., sub-second vs. minutes for tuning-based methods) is convincing. This aspect gives the work genuine practical value, especially for large-scale or latency-sensitive applications.\n2. The paper is clearly written, with logical flow and professional presentation. It does a good job positioning GILT relative to prior work and articulating its contributions clearly.\n3. The motivation is good — trying to build a graph foundation model that’s both LLM-free and tuning-free addresses two major bottlenecks in the field. The paper’s goal of unifying different graph tasks into a single in-context reasoning framework feels visionary and aligns well with the broader trend of foundation models in other domains."}, "weaknesses": {"value": "1. The improvements over strong baselines like GraphAny or GCOPE are small — often within error margins. While GILT shows consistency, it doesn’t convincingly outperform other methods across all settings. The authors emphasize efficiency and generality, but in real performance terms, the margin is not enough to justify the added architectural complexity.\n2. The claim that GILT performs “in-context learning” is not fully convincing. The mechanism seems closer to a meta-learning framework (learn to classify from support sets) than true in-context reasoning as understood in LLM literature. There’s no evidence that GILT dynamically infers rules beyond standard metric-based few-shot learning. This makes the “Transformer ICL” narrative feel more like rebranding than substance.\n3. Using PCA for feature alignment is elegant but shallow. It’s unclear whether this linear preprocessing truly generalizes across heterogeneous graphs or just works for the small set of benchmarks. A non-learnable feature transformation might actually cap performance — and the authors themselves acknowledge this as a limitation in the conclusion."}, "questions": {"value": "1. How is “in-context learning” here fundamentally different from conventional few-shot meta-learning?\n2. Is PCA alignment fixed across datasets, or recalculated per graph? If the latter, does that leak test information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hNDw6JLUC7", "forum": "GoZyhk7NKP", "replyto": "GoZyhk7NKP", "signatures": ["ICLR.cc/2026/Conference/Submission8848/Reviewer_FVan"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8848/Reviewer_FVan"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644630949, "cdate": 1761644630949, "tmdate": 1762920613563, "mdate": 1762920613563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed GILT, a GFM without LLM or tuning for few-shot ICL on graph tasks. They proposed a tokenization model (PCA+GCN) that aligns different feature spaces and encode to a unified space. A ICL transformer is then used to perform token-based reasoning with causal attention and a prototypical prediction head. The authors pretrained this model with many datasets & tasks, and evaluated on different tasks. The results showed good performance as well as efficiency of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. I think is paper showed a interesting new direction of aligning different graphs without pretrained LLMs, which loosened the constrain of text-attributed graphs of many of GFM works, while still not requiring per-graph tuning for the unseen feature spaces.\n2. The paper is overall clearly written and easy to follow.\n3. The experimental design is thorough, with good focus on practicalness (efficiency)."}, "weaknesses": {"value": "1. While PCA is a handy tool to convert arbitrary feature space into a fixed dimension, I think it's unconvincing to rely on it for feature alignment. PCA's only goal is to preserve variance, and it would create a new feature space for each of the graphs (although they are all $d$-dim), so the feature spaces are still independent. Additionally, PCA would probably throw away lots of useful information that are task relevant, given how it works. I'm surprised on how well this method works with PCA being the foundational feature alignment component. And this also makes me wonder how much does node features actually contribute in this system - what if we just swap all features in all graphs with random $d$-dim vectors?"}, "questions": {"value": "1. The authors highlighted multiple times that the tokenizer is tuning-free for new graphs. But in Appendix B.1 (line 889), the authors mentioned that there is a learnable linear projector for some of the graphs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lYQ4wDcTed", "forum": "GoZyhk7NKP", "replyto": "GoZyhk7NKP", "signatures": ["ICLR.cc/2026/Conference/Submission8848/Reviewer_uE5v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8848/Reviewer_uE5v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761693336596, "cdate": 1761693336596, "tmdate": 1762920612902, "mdate": 1762920612902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GILT, an LLM‑free and tuning‑free graph foundational model for few‑shot adaptation via in‑context learning. The method first converts any N‑way K‑shot graph task into a unified token set. Then, a two‑stage Transformer performs self‑attention over support tokens and cross‑attention from queries to the refined support, with a prototypical head operating in a designated class‑space for cosine‑similarity classification. On several graph benchmarks, GILT reports SOTA few‑shot node accuracy, strong link prediction and competitive graph classification, while achieving sub‑second inference compared to tuning‑based and LLM baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1: This paper focuses on an important research problem, addressing both LLM‑free and tuning‑free generalization in Graph FMs.\n\nS2: This paper proposes a clean tokenization, Transformer ICL, and prototypical head pipeline, with a principled causal attention schedule that mirrors best practices in tabular ICL.\n\nS3: This paper reports strong few‑shot results across tasks, including SOTA averages on 1‑shot/5‑shot node classification and competitive link/graph results."}, "weaknesses": {"value": "W1: This paper's novelty appears incremental relative to well-known prototypical few-shot learning and set-based attention architectures; the work would benefit from deeper comparisons to similar works, such as Prototypical Networks[1] and Set Transformer[2] families.\n\nW2: This paper standardizes feature dimensions with PCA and zero-padding before the structural encoder, but it does not justify why PCA is preferable to other linear bases (e.g., truncated SVD) or to learnable, structure-aware adapters (e.g., GNNs) that could better preserve discriminative signals under heterogeneity. A comparison or ablation (PCA vs. SVD vs. a learnable GNN) would clarify whether PCA contributes uniquely to the reported gains.\n\nW3: This paper's \"deep linear GCN\" removes learned weights and nonlinearities to avoid overfitting, which is similar to SGC or APPNP. The paper should provide appropriate citations and discussions.\n\nW4: This paper applies LayerNorm with learnable affine parameters after each linear aggregation, which is not the prevailing normalization choice in message passing. The paper should justify this design relative to alternatives such as BatchNorm or GraphNorm[3].\n\nW5: This paper presents an attractive pipeline, but many steps are described only at a conceptual level, leaving several crucial mechanisms (e.g., attention masks, token construction, and prototype computation) insufficiently specified.\n\nW6: This paper's link-prediction baselines omit standard strong methods (e.g., SEAL[4] and MaskGAE[5]), so the claim of superiority over fully supervised GNNs is not yet convincing.\n\nW7: This paper evaluates node ICL on the Planetoid family and WikiCS, which are classic but small-scale graphs with known limitations and may not be ideal benchmark choices; evaluation on larger or more diverse node-level datasets would be more convincing.\n\nW8: This paper describes the encoder as \"parameter-free\" yet LayerNorm contains learnable affine parameters and the Transformer itself is learned.\n\n\nMinor points:\n\n+ The results in Table 4 seem unreasonable, as GILT is reported as 63.10 while baselines are reported as decimals around 0.58. The paper should clarify the reason behind this.\n+ This paper's feature-alignment procedure uses per-graph PCA but does not specify whether statistics are computed on the full graph (risking transductive leakage) or only on train/support data, which is essential in strict inductive settings.\n\n+ This paper's cross-task link prediction evaluation appears to reuse the same graphs (Computers, Photo) that are included in the pre-training corpus, which undermines the claim of generalization to unseen graphs and conflates task transfer with graph reuse.\n+ There appears to be a typo in Equation (2), where the class support set $S_c$ is multiplied by a vector $h$.\n\n\n\nReference:\n\n[1] Snell, J., Swersky, K., & Zemel, R. Prototypical Networks for Few‑Shot Learning. NeurIPS 2017.\n\n[2] Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., & Teh, Y. W. Set Transformer: A Framework for Attention‑based Permutation‑Invariant Neural Networks. ICML 2019.\n\n[3] Cai, T., et al. GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training. ICML 2021\n\n[4] Zhang, M., & Chen, Y. Link Prediction Based on Graph Neural Networks. NeurIPS 2018.\n\n[5] Li, J., et al. What's Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders. KDD 2023."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A5zruOgSe4", "forum": "GoZyhk7NKP", "replyto": "GoZyhk7NKP", "signatures": ["ICLR.cc/2026/Conference/Submission8848/Reviewer_h6fb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8848/Reviewer_h6fb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894777478, "cdate": 1761894777478, "tmdate": 1762920612576, "mdate": 1762920612576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GILT, an LLM-free and tuning-free framework for graph foundation modeling. It introduces a token-based in-context learning mechanism that unifies node-, edge-, and graph-level tasks while effectively handling heterogeneous and numerical graph features. Experiments demonstrate that GILT achieves superior few-shot performance."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-written.\n2. The paper identifies critical challenges of existing GFMs which heavily rely on textual features and dataset-specific tuning."}, "weaknesses": {"value": "1. The use of PCA for unifying heterogeneous features across graphs is not well justified. PCA only performs dimensionality reduction and does not align the semantic spaces between different graphs, making the proposed graph encoder conceptually ill-defined.  \n\n2. The novelty of the work is limited. Prior studies have already explored unifying node-, edge-, and graph-level tasks under few-shot settings, and the use of attention mechanisms for fusing support and query representations is also well-established, offering little technical innovation.  \n\n3. The experimental design appears severely biased, with problematic baseline choices and insufficient details on dataset processing.  \n\n   3.1 Table 2 compares GILT with other GFM methods. However, existing GFMs are known to perform poorly on many datasets, regardless of the reported gains in their original papers. It is crucial to include results from traditional baselines such as semi-supervised GNNs and self-supervised GNNs to better assess the true capabilities of GILT.  \n\n   3.2 Table 3(a) compares GILT with GCN and GraphSAGE on link prediction. The data split used in this setting is unclear. Do the baseline methods use the same training and test edges as GILT? If so, training a link predictor on only five edges is highly unfair, as such a small number of samples is insufficient to capture the complex dynamics of link formation. Under this extreme setting, it is unsurprising that in-context learning performs better than trained models. However, this comparison is severely biased and does not reflect the true performance of different models in realistic scenarios. Simple link prediction heuristics (e.g., common neighbors) might even outperform the chosen baselines. To properly validate the method, results under varied shot settings and with stronger baselines should be provided.  \n\n   3.3 Table 3(b) again compares GILT with GFMs for link prediction. Since GFMs are known to perform poorly, GNN baselines should also be included for a fairer comparison. Additionally, prior work [1] suggests that AUC is not an appropriate metric for link prediction; metrics such as HITS@K or MRR should be considered instead.  \n\n   3.4 Table 4 reports AUC scores for graph classification tasks, but both the baselines and the proposed method achieve nearly random performance (AUC around 0.5). Such results are insufficient to demonstrate the claimed in-context learning capability of GILT.  \n\n   3.5 Table 6 compares GILT with GFMs, but the reported results appear unusually low. For example, the 20-shot Cora accuracy of 78% is significantly below commonly reported values in the literature. More details on dataset splits and baseline are needed.  \n\n4. Overall, the experimental section would benefit from a clearer description of the experimental setup and dataset preprocessing, as well as a more comprehensive evaluation including varied shot settings and stronger, more reasonable baselines.\n\n[1] https://arxiv.org/abs/2306.10453"}, "questions": {"value": "1. What is the rationale behind cross-task transfer of GILT? Why would node-level pretraining be beneficial for link-level or graph-level tasks? \n\n2. Is the in-context learning model really making progress, or is it just because the baselines are too bad?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YWuafkuHJi", "forum": "GoZyhk7NKP", "replyto": "GoZyhk7NKP", "signatures": ["ICLR.cc/2026/Conference/Submission8848/Reviewer_B818"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8848/Reviewer_B818"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985845871, "cdate": 1761985845871, "tmdate": 1762920611945, "mdate": 1762920611945, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}