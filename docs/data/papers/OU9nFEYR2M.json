{"id": "OU9nFEYR2M", "number": 12752, "cdate": 1758210080975, "mdate": 1759897489493, "content": {"title": "Learning to Reason without External Rewards", "abstract": "Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision. We explore Reinforcement Learning from Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic signals without external rewards or labeled data. We propose Intuitor, an RLIF method that uses a model's own confidence—termed self-certainty—as its sole reward signal. Intuitor replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, enabling fully unsupervised learning. Experiments demonstrate that Intuitor matches GRPO's performance on mathematical benchmarks while achieving superior generalization to out-of-domain tasks like code generation, without requiring gold solutions or test cases. Our findings show that intrinsic model signals can drive effective learning across domains, offering a scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable. Code is available in the supplementary materials.", "tldr": "", "keywords": ["RL", "Reasoning", "LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d48d3c80b9e56e19d0715c3f2d49ff67ffd537b2.pdf", "supplementary_material": "/attachment/56ff88d6513d977cdcd92c46e055b35861474ffb.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Reinforcement Learning from Internal Feedback (RLIF) as a paradigm for improving reasoning ability in large language models (LLMs) without relying on external, verifiable rewards. The authors introduce Intuitor, an instantiation of RLIF that replaces the external correctness signal used in methods like GRPO with an intrinsic “self-certainty” metric. Self-certainty is operationalized via the KL divergence between the model’s output distribution and a reference distribution, encouraging more confident and consistent reasoning. The authors demonstrate that Intuitor achieves comparable performance to GRPO on mathematical reasoning benchmarks and shows competitive generalization to out-of-distribution tasks such as code generation. Interestingly, models trained with Intuitor tend to produce longer, more detailed chain-of-thought (CoT) responses, which the authors interpret as reinforcing stepwise reasoning coherence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a timely and important question: how can LLMs learn to reason in domains where explicit supervision or reward verification is infeasible? The proposed self-certainty–based RL framework is elegant, computationally tractable, and broadly applicable.\n\n- The conceptual simplicity of optimising for internal confidence instead of external correctness makes the method appealing for scaling reasoning improvement without costly reward models or test suites.\n- The inclusion of multiple model families in the appendix (beyond Qwen) demonstrates the generality of the approach, though this could be highlighted more clearly in the main text.\n- The observation that RLIF-trained models produce longer and more detailed CoTs is particularly interesting. The authors’ explanation—that longer CoTs stabilize reasoning trajectories—is plausible and worth further study.\n\nOverall, the paper contributes to the growing discourse on methods for reasoning elicitation in LLMs."}, "weaknesses": {"value": "While the approach is novel and promising, several aspects of the evaluation and analysis could be strengthened.\n- The evaluation scope is somewhat narrow. Although the method is positioned as applicable to datasets without verifiable answers, all reported experiments are on domains (math, code) where correctness can in fact be automatically checked. Demonstrating effectiveness on tasks without clear reward signals would make the paper more convincing.\n- The mechanistic explanation of how optimizing for self-certainty leads to qualitatively better reasoning remains underspecified. The KL-divergence–based objective effectively sharpens the output distribution, similar to reducing the sampling temperature. It would be helpful for the authors to clarify why RL training with self-certainty yields higher-quality reasoning compared to simply sampling greedily or using low-temperature decoding. It would be helpful to understand in abstract, mathematical terms what is the optimal solution of RL training with self-consistency vs. a greedy sample.\n- Claims of superior out-of-distribution generalisation over GRPO appear overstated. For instance, in the MATH→LCB/CRUX transfer setup, the performance gains are small or inconsistent across models (e.g., Qwen2.5-1.5B and OLMo2-7B-SFT show slightly higher CRUX gains for GRPO).\n- Related works ([2]) have shown that higher entropy in the distribution of the initial tokens encourages better exploration of the answer space, improving performance on more complex tasks. The focus on self-certainty may limit the diversity of generated solutions. It would be interesting to see how this method performs on more complex math or code benchmarks. Alternatively, a qualitative comparison of the diversity of generated code solutions of INTUITOR and MATH could bring extra insights into this matter.\n- The paper would also benefit from a brief discussion of recent follow-up work such as [1], which analyzes why RLIF methods often yield diminishing returns for instruction-tuned models and links internal feedback to changes in token-level entropy. Including this perspective would help contextualize both the strengths and limitations of the proposed approach.\n\n\nRelated works:\n\n[1]  No Free Lunch: Rethinking Internal Feedback for LLM Reasoning https://arxiv.org/abs/2506.17219 \n\n[2] The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models https://arxiv.org/pdf/2505.22617"}, "questions": {"value": "1. The paper notes that the self-certainty objective modifies the generation process beyond simply increasing the probability of the most likely next token. Could the authors elaborate on how this differs from temperature scaling or confidence sharpening, both locally and across reasoning trajectories?\n\n1. Have the authors considered evaluating RLIF with self-certainty on datasets without verifiable answers? This would better substantiate the claim of applicability to unverifiable domains.\n\n1. In Figure 6, is it possible that the same pattern that we observe in offline training would emerge also for online training, but after many more iterations? \n\n---\n\nOverall, I believe this paper is an important contribution to the field. However, for strong acceptance, I would encourage the authors to provide a more thorough investigation of the method’s limitations and underlying mechanisms. In particular, it seems that the observed improvements primarily stem from a) better output formatting b) longer reasoning chains which happen to correlate with higher self-certainty. The additional findings of [1] also indicate that RLIF yields little improvement for instruction-tuned models, indicating diminishing returns of intrinsic feedback once an LLM is already trained to generate well-structured outputs. It remains unclear whether we can observe easy-to-hard generalisation by purely relying on self-certainty. I would be inclined to raise my score if the main text and abstract were revised to present the contributions more cautiously and in proportion to the empirical evidence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wH3AmSbL8c", "forum": "OU9nFEYR2M", "replyto": "OU9nFEYR2M", "signatures": ["ICLR.cc/2026/Conference/Submission12752/Reviewer_gp3y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12752/Reviewer_gp3y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760629745879, "cdate": 1760629745879, "tmdate": 1762923568857, "mdate": 1762923568857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Reinforcement Learning from Internal Feedback (RLIF), a paradigm for fine-tuning LLMs using intrinsic self-certainty measured as average KL divergence from uniform token distributions as the sole reward signal, instantiated via an adaptation of Group Relative Policy Optimization (GRPO) called INTUITOR. It claims this enables unsupervised reasoning improvements on math (MATH, GSM8K) and generalization to code (LiveCodeBench, CRUXEval) without gold answers or verifiers, with ablations exploring KL penalties, scaling, and optimization strategies. The show good empirical evidence of emergent structured reasoning and applicability across models (Qwen, Llama, OLMo), though limited by small-scale experiments and simpler baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper extends RLIF by integrating self-certainty into GRPO for process-aware rewards using online computation to curb hacking, differing from outcome-focused RLVR.\n\n2. The paper presents comprehensive experiments across families, with ablations rigorously contrasting entropy/random baselines to affirm stability,\n\n3. The paper addresses a relevant question: scalable rewards without supervision for RLVR-limited domains, with execution showing INTUITOR's 13.8% OOD gains (e.g., LiveCodeBench) and emergent reasoning, valuable for self-improving agents."}, "weaknesses": {"value": "1. The approach replaces RLVR with self-certainty based reward, however it is questionable if the gains still hold when the models hallucinate, especially when the method is scaled to larger models.\n\n2. The paper's experiments are confined to small models (1.5B-14B) and corpora (7.5k problems), raising doubts on scalability.\n\n3. The novelty is modest: the self-certainty reward builds directly on self-certainty ides proposed in Kang et al. (2025) to replace RLVR in the GRPO formulation from DeepSeek-R1 lacking novel formulation or broad applicability beyond math/code.\n\n4. The gains are not transformative, as they are comparable to baselines on in-domain tasks. I would also expect comparisons with stronger reasoning baselines rather than just GRPO, like STaR (Zelikman et al., 2022) or Quiet-STaR (Zelikman et al., 2024), which incorporate advanced self-consistency or chain-of-thought mechanisms and would better contextualize INTUITOR's relative strengths in unsupervised settings. The paper does mention alternatives like perplexity or entropy-based measures but no such ablations are provided in the paper."}, "questions": {"value": "1.  I would assume that when the models hallucinate, the learnt distribution would sway away from the uniform distribution, increasing the KL divergence and thus the self-certainty. Thus, I am not convinced how self confidence is a good metric to replace RLVR on complex reasoning tasks in such scenarios? \n\n2. The above issue seems to be mitigated using online learning by co-evolving the reward with policy, as shown in an ablation. But I still do not understand how it would be useful when the signal is very hallucinatory. Can the authors explain this?\n\n3. Have the authors experimented with a hybrid approach that uses say a weighted combination of RLVR and self-certainty reward.\n\n3. I would also assume that using self-certainty reward would hurt continual learning too. E.g. say a model is trained on MATH dataset using the self-certainty reward. It may get good at solving Math problems but may be poor at solving Physics questions. Let us say we later try to fine-tune it to solve Physics problems too, however, since the self-certainty is high, it will assign high self-certainty reward to poor solutions for the Physics problem, thus hampering learning. Do the authors have a way to mitigate this?\n\nIn all, I am not fully convinced that self-certainty is a scalable alternative for RLVR for solving reasoning tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JOwGbuEzeg", "forum": "OU9nFEYR2M", "replyto": "OU9nFEYR2M", "signatures": ["ICLR.cc/2026/Conference/Submission12752/Reviewer_Np23"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12752/Reviewer_Np23"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817478585, "cdate": 1761817478585, "tmdate": 1762923568438, "mdate": 1762923568438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces INTUITOR and RLIF, which train LLMs to improve reasoning without any external rewards or supervision.\nInstead of verifiable correctness signals (as in RLVR) or human preferences (as in RLHF), INTUITOR uses the model’s self-certainty as the intrinsic reward.\nImplemented via Group Relative Policy Optimization (GRPO), INTUITOR replaces external verifiable rewards with normalized self-certainty scores to update the policy. Experiments on multiple benchmarks show that INTUITOR achieves comparable reasoning performance to GRPO on in-domain mathematical tasks while generalizing better to unseen domains like code generation and instruction following. The paper includes ablations, scaling studies, and qualitative analyses demonstrating emergent reasoning structure and robustness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Proposes a new RL paradigm (RLIF) that removes the dependency on external or verifiable rewards. It is an elegant and forward-looking idea for autonomous reasoning systems.\n\n+ Using self-certainty as an intrinsic reward is well-motivated, mathematically grounded, and can be integrated into existing policy optimization frameworks.\n\n+ Demonstrates solid performance across both reasoning and code tasks, with consistent improvements in generalization, instruction-following, and early learning speed.\n\n+ Benchmarks against GRPO, entropy-based rewards, and random rewards are thorough, and the reward hacking experiments (online vs. offline self-certainty) convincingly highlight robustness.\n\n+ The paper contributes a meaningful step toward autonomous self-improvement in LLMs, addressing the scalability limits of RLHF/RLVR."}, "weaknesses": {"value": "I have 3 main weakness concerns for this paper:\n* The authors should argue way more to convince the reader on why self-certainty is a stable way of guiding the learning process, and won't lead agents to overconfidently learn shortcut behaviors. Having a random baseline that assigns \"random confidence\" would help a lot asserting the value of self-confidence. Also, providing second order metrics.\n\n* In the experimental evaluation section, providing list of scientific questions at the beginning of the experimental evaluation section (often denoted RQ1, RQ2, ... etc), that are each answered in different paragraphs. For example\n  * (RQ1) Is our framework outperforming existing baselines?\n  * (RQ2) Does self-confidence lead to better selection of ...?\n  * (RQ3) How important is module A ? (conducting an ablation study)\n  \nFigures can drastically be improved:\n+ Captions could be more expressive, clearly explaining what each plot demonstrates and how it supports the paper’s claims. Improving figure layout and adding descriptive captions would significantly enhance readability and interpretability. In details:\n**Each figure should support a claim made by the authors.**\nEach caption should make the figure a standalone component of the paper. Thus, each figure should be built in the following way:\n   * The first sentence (in bold) should highlight the main takeaway of the Figure/Table (e.g. \"*[Our] method improves the ability of agents to do task T.*\"). This is the main message for the reader that supports one of your claim.\n   * The next sentences then explain what is depicted in the Table/Figure. E.g. \"*..., as depicted by the superior mean test accuracy (+/- std) of our method over the different baselines.*\"\n   * Finally, details and references to e.g. appendix can be provided if necessary. E.g. \"*Our method outperforms baseline 1 in 3 out of 4 tasks, ... etc. 5 seeded rerun. Best results highlighted in bold. Further description of the training and testing setups are available in Appendix C.*\"\n\n\nMy remarks can also be addressed by reference to existing literature, to which I might be unfamiliar.\n\n+ The lack of second order metrics (std) prevents from drawing accurate conclusions on the performance difference between the baselines\n\n+ Since self-certainty is derived from the model’s own token probabilities, there is a risk of reinforcing its pre-existing biases or hallucinations; this should be addressed.\n\n+ The distinction between “intrinsic reward,” “self-certainty,” and “entropy minimization” could be made clearer.\n\n\n## Points to Improve\n\n*This is not meant for the rebuttal per se (see below), but general points that would strengthen the paper*\n\n+ Discuss potential failure cases where intrinsic rewards may amplify overconfidence or bias, and propose mitigation strategies.\n\n+ Clarify the relationship between RLIF, RLVR, and RLHF, especially how RLIF could complement them in hybrid setups."}, "questions": {"value": "* How sensitive is performance to the KL penalty and group size hyperparameters?\n\n* Have you observed scenarios in which your intrinsic reward leads to overly verbose or repetitive reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qneZDlEnNg", "forum": "OU9nFEYR2M", "replyto": "OU9nFEYR2M", "signatures": ["ICLR.cc/2026/Conference/Submission12752/Reviewer_9ExH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12752/Reviewer_9ExH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825898409, "cdate": 1761825898409, "tmdate": 1762923568213, "mdate": 1762923568213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for self-improving LLMs that uses self-certainty as a reward signal, where self-certainty is defined as the KL divergence between a uniform distribution over the vocabulary and the model's next-token distribution. They train the model using the MATH dataset and the Codeforces dataset, and evaluate on a range of datasets. They compare against baselines like GRPO with gold labels and GRPO with plurality voting, which is another self-improvement method, and report the results. The results show performance on par with GRPO-PV on the math dataset, and improved performance on coding tasks, such as LCB and CRUX.\n\nThe paper also claims in the experiment section that the method exhibits faster early-stage learning compared to GRPO, demonstrates effective cross-task generalization (training on MATH improves code generation performance), and leads to emergent long-form reasoning behaviors. Additional analyses include comparisons across different model architectures (Llama, OLMo) and an examination of online versus offline self-certainty computation, showing that online computation prevents reward hacking."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Originality: This paper is part of a wave of recent work attempting self-improvement for LLMs. These methods share a similar flavor, focusing on math/code tasks and using some type of heuristic—in this case, a KL divergence metric. The specific approach appears to be novel, though I am not confident about it.\n\nQuality: The idea is clear and the experimental analysis is quite thorough.\n\nClarity: The writing is quite clear.\n\nSignificance: The analysis is good. The insight regarding online versus offline self-certainty computation is particularly interesting."}, "weaknesses": {"value": "The main contribution of this work is the use of KL divergence against a uniform distribution as a proxy reward. I don't think there is enough theoretical justification or empirical evidence showing why this particular proxy reward is better than alternatives. The only comparison provided is with plurality voting, yet several other alternatives exist (missing references are linked below).\n\nThe analysis of different model behaviors is interesting, but it is unclear whether these behaviors are due to the particular proxy reward or are a general consequence of self-improvement methods.\n\nIf the authors could either provide more theoretical explanation for why this proxy is superior, or show more empirical comparisons with alternative approaches, I would increase my score.\n\nSome related work \nHuang, Audrey, et al. \"Self-improvement in language models: The sharpening mechanism.\" arXiv preprint arXiv:2412.01951 (2024).\nSong, Yuda, et al. \"Mind the gap: Examining the self-improvement capabilities of large language models.\" arXiv preprint arXiv:2412.02674 (2024).\nShafayat, Sheikh, et al. \"Can Large Reasoning Models Self-Train?.\" arXiv preprint arXiv:2505.21444 (2025)."}, "questions": {"value": "1. Why do we see an out of distribution improvement for coding on Qwen but not on LLama? \n2. Can you show baseline GRPO-PV for the LLama models too? I think they are missing in the appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FSNYySoPlJ", "forum": "OU9nFEYR2M", "replyto": "OU9nFEYR2M", "signatures": ["ICLR.cc/2026/Conference/Submission12752/Reviewer_cUuw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12752/Reviewer_cUuw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948377346, "cdate": 1761948377346, "tmdate": 1762923567969, "mdate": 1762923567969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}