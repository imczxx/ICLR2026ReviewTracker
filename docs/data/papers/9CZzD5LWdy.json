{"id": "9CZzD5LWdy", "number": 9324, "cdate": 1758118904489, "mdate": 1759897731364, "content": {"title": "Tequila: Deadzone-free Ternary Quantization for Large Language Models", "abstract": "Quantization techniques are essential for the deployment of Large Language Models (LLMs) on edge devices. However, prevailing methods often rely on mixed-precision multiplication that lacks efficient hardware support, making it not feasible. Ternary weight quantization addresses this by constraining weights to {-1, 0, 1}, replacing expensive multiplications with hardware-efficient additions. However, such aggressive compression leads to significant accuracy degradation, even after costly quantization-aware training with massive data.  We identify the core issue as _**deadzone trapping**: a large number of weights are trapped at the deadzone boundary._ This occurs because these weights receive only noisy, uninformative gradients, preventing stable escape from the deadzone and severely impeding model capacity and optimization. To address this issue, we propose **Tequila**, a trapping-free quantization optimization method that reactivates deadzone-trapped weights by repurposing them as dynamic biases. This allows the repurposed weights to provide a continuous signal in the forward pass and, critically, receive direct, meaningful gradient signals during backpropagation, thereby enhancing model capacity and optimization with nearly _zero_ inference overhead. Extensive evaluations demonstrate that Tequila outperforms state-of-the-art (SOTA) ternary quantization methods across five benchmarks. Specifically, on the ARC benchmark, it achieves $>4$% accuracy gain over the SOTA baseline, nearly matching full-precision performance (within $<1$% gap) with an $3.0\\times$ inference speedup. Consequently, Tequila offers a highly practical and efficient implementation for the deployment of advanced LLMs in resource-constrained environments. The code is available at \\url{https://anonymous.4open.science/r/Tequila-2B5F/}", "tldr": "", "keywords": ["Ternary Quantization", "Large Language Models", "Edge Computing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd0b703ee160fc58e4efafa3070ddbbb8a3b5cbc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Tequila, a training-time method to mitigate the “deadzone trapping” problem inherent in ternary quantization of large language models (LLMs). The authors argue that a large number of weights are sparsified to 0 (the “deadzone”) and receive only noisy straight-through estimator (STE) gradients, which leads to ineffective updates and loss of model capacity. Tequila addresses this by reinterpreting these dead weights as dynamic biases, enabling stronger gradients and providing forward signal restoration. The method is nearly overhead-free during inference and is evaluated across multiple benchmarks (ARC, PIQA, HellaSwag, etc.) using LLaMA-based models, outperforming previous state-of-the-art ternary quantization approaches with significantly fewer training tokens and maintaining ternary inference efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a novel and intuitive perspective into the deadzone problem in ternary quantization and proposes a clean and effective approach (Tequila) to solve this challenge.\n\n2. The proposed method is practical and hardware-friendly, as it maintains the computational advantages of ternary quantization and requires negligible inference-time complexity.\n\n3. Convincing empirical results across multiple benchmarks and model sizes (1B and 3B LLaMA) show significant and consistent improvements over both static and learnable ternary quantization baselines, including BitNet and Spectra.\n\n4. The paper is well-written, with clear figures and mathematical exposition. The insight into trapped weights and how reinterpreting them as biases yields optimization benefits is well motivated."}, "weaknesses": {"value": "1. Tequila risks being a niche solution as this method is tightly coupled with ternary quantization, where the 0 weight level — the \"deadzone\" — is prominent. It is unclear how or whether Tequila can be extended to other quantization schemes, especially 2-bit symmetric quantization (4-value) or binary quantization (2-value), where such a deadzone may not exist.\n\n2. Since the current mechanism focuses only on reactivating dead weights (quantized to 0), it is not evident whether a similar strategy can be applied to weights that are quantized to ±1 in ternary quantization, or other levels in higher-bit setups."}, "questions": {"value": "1. Can Tequila be extended to 2-bit or 4-bit quantization scenarios where the weight values are {−1, +1} or {−2, −1, +1, +2} and there is no explicit “deadzone”? If not, can you provide a justification or limitations on generalization?\n\n2. Have the authors considered adding similar learnable correction biases globally or per-group even to non-zero weights in ternary quantization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "C4F1Qg6X05", "forum": "9CZzD5LWdy", "replyto": "9CZzD5LWdy", "signatures": ["ICLR.cc/2026/Conference/Submission9324/Reviewer_N2uB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9324/Reviewer_N2uB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761287715860, "cdate": 1761287715860, "tmdate": 1762920958884, "mdate": 1762920958884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper builds upon the observation that in extreme quantization settings (i.e., weights taking only values -1,0,1) there is a certain area of values that are mapped to zero. The paper then proposes to re-purpose those weights as biases, through which they receive a more stable gradient signal."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Experimental results are very promising. The proposed approach excels in all eval benchmarks for the analyzed 3B model, and in all but one case for the 1B model.\n\nThe paper further provides analysis of loss trajectory (constant improvement) resource requirements (no extra resources needed).\n\nThe visuals of the paper are excellent; very clear and accessible.\n\nA sensitivity analysis of the reactivation threshold is included.\n\nAblation study considering all components of the approach shows consistent gains."}, "weaknesses": {"value": "Results limited to 1B and 3B models. Other research in quantization-aware training (\"The era of 1-bit LLMs\") suggests that – at ascale – performance decrease is marginal anyways.\n\nThere are other approaches that tackle the same (or highly related) issue of deadzones, e.g., AbsMedian (arXiv:2407.09527) – which do not harm the soundness of the paper but could be considered."}, "questions": {"value": "Are the experimental results only based on a single run or have they been repeated with different random seeds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xRRkbk5AUq", "forum": "9CZzD5LWdy", "replyto": "9CZzD5LWdy", "signatures": ["ICLR.cc/2026/Conference/Submission9324/Reviewer_7ZxG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9324/Reviewer_7ZxG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761600566421, "cdate": 1761600566421, "tmdate": 1762920958425, "mdate": 1762920958425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Tequila, a precision enhancement strategy specifically designed for ternary Quantization-Aware Training (QAT) of Large Language Models (LLMs). The paper argues that the primary cause of accuracy degradation in ternary quantization is the phenomenon where many weights are quantized to zero, leading to a \"deadzone trapping\" issue during gradient updates. This trapping prevents these weights from being effectively updated. To address this, the authors propose reutilizing these weights trapped in the deadzone by treating them as dynamic biases, thereby making their gradient updates more effective and meaningful. Through this approach, the authors claim their method can surpass current SOTA techniques for ternary quantization. Furthermore, the authors provide ablation studies in a progressive manner to illustrate the effectiveness of their proposed techniques."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Valuable Research Problem:** Improving the accuracy of ternary quantization is highly beneficial for model acceleration during inference, as it allows replacing complex multiplications with additions. This aligns well with current trends in model quantization research. Furthermore, the paper's experimental results demonstrate a significant accuracy improvement compared to other existing ternary quantization works of similar scale.\n2.  **Sufficient Experiments and Impressive Results:** Although the authors conducted experiments on relatively small model scales (1B/3B) and a token scale of 10B, they compared their method extensively against numerous other ternary quantization works of the same scale and achieved impressive accuracy gains. Additionally, they performed comprehensive ablation studies concerning method design, parameter selection, and quantization granularity, yielding complete results. Their experimental outcomes are exceptionally good; other ternary-quantized models of similar scale, even after training on 100B tokens, often fail to match the performance on most tasks achieved by TequilaLLM trained on only 10B tokens."}, "weaknesses": {"value": "1.  **Theoretical Analysis is Unconvincing and Contains Flaws:** \n    1) In Section 2, the paper primarily states the drawbacks of traditional gradient computation for ternary quantization problems. However, the gradient formula (3) presented in line 132 is difficult to further derive into the authors' statement about the gradient computation flaw (line 152: *\"Because they contribute no information to the prediction loss L, they receive uninformative gradients during backpropagation\"*), and the authors do not provide further theoretical analysis supporting this statement. Formula 3 for gradient calculation seems to originate from the reference (Chen et al., 2024). However, if using a traditional STE estimation scheme, the calculation of the original weight gradient ∂L/∂wᵢ should be identical to the quantized weight gradient ∂L/∂Ŵ, which is (∂L/∂Y) * xᵢ * α. In this case, when the original weight lies within the range (–δ, δ) (the \"deadzone\" scenario claimed in the paper), the weight gradient would be the same as outside the deadzone. Therefore, the paper's derivation of the \"DEADZONE TRAPPING\" scenario based solely on this formula in Section 2.2 is unconvincing. Furthermore, line 161 uses exaggerated terms like \"permanently inactive\" and \"drastically\" to describe the harm of the deadzone, which is unpersuasive. \n    2) Section 3's introduction of the bias-like correction term is even more confusing. Firstly, regarding the first variant, Minima Reactivation, the so-called \"informative\" value mentioned in line 192 is merely sign(x)*ε from a formulaic perspective, and the authors do not provide complete analysis proving this design is effective. In my view, this approach seems more like splitting zero into a small positive and a small negative value to expand capacity, effectively simulating 4-bit quantization, which naturally performs better than 3-bit.\n    3) Section 3.2's proposal of the Tequila method is essentially a relaxation of STE approximation, a concept already present in quantization literature. The step-by-step simplification process from STE -> differentiable reactivation -> activation-free bias (using dead weights as bias) -> letting these weights act as both bias and participate in multiplication (dual role) seems reasonable at each step, but lacks persuasive power and involves strong logical leaps. For example, in line 246, the authors claim \"bias-like term can be precomputed offline, reducing inference overhead to nearly zero,\" but do not explain: Why can this term be completely decoupled from the input? Under different input distributions, is it truly \"approximately constant\"? This is an empirical assumption without mathematical or experimental support. Ultimately, in the derived gradient correction formula, the authors claim in line 165 that this gradient for dead weights is \"superior,\" but provide no quantitative analysis proving that adding a constant bias term xᵢ in formula (8) results in a better gradient estimation than normal STE.\n2.  **Exaggeration is Prevalent in the Text:** For example, line 152 asserts that weights within (–∆, ∆) under STE have gradients that \"contribute **no information** to the precision loss L\" and line 153 uses **'uninformative'**, line 220 directly claims \"**fundamentally** enable the efficient recovery of previously trapped weights, **dramatically** accelerating convergence\", and the advantages given in line 270 seem to be summarizing for the sake of summarizing, with little relation to the actual logical flow of the text. The paper uses a large number of highly positive adjectives to describe their method (fundamentally, informative, superior), but lacks specific data or theoretical support, giving the impression of \"more promotion than argumentation.\"\n3.  **Insufficient Analysis of Deadzone Generality:** According to the paper's logic, the deadzone phenomenon should exist for quantization at any bit-width, as some weights will be quantized to zero. If the authors could provide a more detailed analysis of this deadzone problem across different bit-width quantization scenarios, it would be better.\n4.  **Lack of Specific Details on the Mixed-Precision Framework Design:** It appears that during the quantization training process, apart from the online quantization of weights to 3-bit, other components such as main weights, activations, gradients, and optimizer states should remain at higher precision. The authors should briefly state this in the main text or at least in the appendix, explaining that maintaining these parts at high precision is also necessary to ensure the final ternary-quantized model achieves good results during inference, to avoid misleading readers unfamiliar with this field."}, "questions": {"value": "The main questions focus on the paper's theoretical analysis, as detailed in weakness 1. I'll briefly summarize here to facilitate the author's rebuttal:\n\n1.  Can the authors provide a more reasonable and complete analysis explaining *why* gradient updates within the deadzone are noisy, uninformative, and potentially lead to \"making model's parameters **permanently inactive**\" (line 161)?\n2.  Can the authors provide a more logical reasoning process for Section 3.1, primarily addressing the origin and justification of the bias-like design term?\n3.  Can the authors add more rigorous, step-by-step deductions in Section 3.2 when deriving the final Tequila formula to ensure soundness?\n\nAdditionally, there are a few other minor questions:\n\n4.  In Table 1, were the experiments conducted using exactly the same datasets across all compared methods?\n5.  The results in Table 2 are particularly impressive. It's striking that LLM-QAT, BitNet, and Spectra, trained on 100B tokens, perform worse on most tasks than TequilaLLM trained on only 10B tokens. Have the authors considered and can they discuss the fundamental reasons behind this impressive phenomenon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SR3yFq8dQH", "forum": "9CZzD5LWdy", "replyto": "9CZzD5LWdy", "signatures": ["ICLR.cc/2026/Conference/Submission9324/Reviewer_ckpi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9324/Reviewer_ckpi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707355715, "cdate": 1761707355715, "tmdate": 1762920958120, "mdate": 1762920958120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Tequila, a ternary quantization method for large language models (LLMs) targeting edge deployment. It tackles the “deadzone trapping” problem, where many weights become inactive due to noisy gradients. Tequila reuses these inactive weights as adaptive biases, restoring useful forward and backward signals. Experiments on five benchmarks and two LLaMA sizes show that Tequila achieves higher accuracy and efficiency than existing ternary methods, approaching full-precision performance with much lower hardware and data costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Clear identification of the core problem: The paper accurately pinpoints “deadzone trapping” as a key obstacle to effective ternary quantization, and Figure 8 clearly demonstrates the effectiveness of the proposed Tequila method in addressing this issue.\nPracticality and efficiency: Tequila achieves near–full-precision performance while remaining hardware-friendly, improving accuracy by over 4% compared to SOTA methods and maintaining comparable inference speed to classic ternary quantization approaches such as BitNet.\nHigh-quality and well-presented figures: For example, Figure 1 (“Deadzone Trapping in Ternary Quantization”) and Figure 2 (“Tequila Reactivation Strategy”) intuitively illustrate the problem and solution, while Figure 3 clearly shows faster convergence.\nGood reproducibility and transparency: Experimental details and code implementations are publicly available, enabling community verification and reproduction."}, "weaknesses": {"value": "Limited discussion of related work: As a QAT approach, the paper overlooks several concurrent efficient ternary quantization methods such as OneBit[1], as well as recent PTQ ternary approaches like PTQTP[2].\nLimited model scale and architectural generalization: Experiments focus mainly on smaller LLaMA models (1B and 3B). Validation on larger models (e.g., 7B, 13B) or different architectures is missing, leaving generalization untested.\nInsufficient analysis of λ sensitivity: Although Figure 6 is provided, the paper does not explore how λ interacts with other hyperparameters or training configurations. Like, whether model size affects the optimal λ during training.\n[1]Xu Y, Han X, Yang Z, et al. Onebit: Towards extremely low-bit large language models[J]. Advances in Neural Information Processing Systems, 2024, 37: 66357-66382.\n[2]Xiao H, Yang R, Yang Q, et al. PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models[J]. arXiv preprint arXiv:2509.16989, 2025."}, "questions": {"value": "Q1: In Section 3.2, the paper states, “Given the approximately symmetric distribution of inputs (activations) in transformer architectures.” Is there any prior literature or empirical evidence supporting this assumption?\n\nQ2: Also in Section 3.2, the paper mentions, “In addition to functioning as an adaptive bias.” However, shouldn’t $\\sum_{i\\in D}\\lambda w_i$ be input-independent? Could the authors clarify this reasoning?\n\nQ3: The paper claims “a learnable reactivation parameter λᵢ for each deadzone weight wᵢ,” but in experiments a fixed λ value is used. Could the authors elaborate on this inconsistency?\n\nQ4: It would strengthen the paper if the authors could include additional comparisons between Tequila and PTQ-based ternary quantization algorithms in terms of quantization efficiency and downstream task performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LUETHabqVP", "forum": "9CZzD5LWdy", "replyto": "9CZzD5LWdy", "signatures": ["ICLR.cc/2026/Conference/Submission9324/Reviewer_GbUj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9324/Reviewer_GbUj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995717886, "cdate": 1761995717886, "tmdate": 1762920957878, "mdate": 1762920957878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}