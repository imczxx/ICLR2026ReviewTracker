{"id": "XGECnjDEcS", "number": 19102, "cdate": 1758293577413, "mdate": 1763585563416, "content": {"title": "CoolPrompt: An Automatic Prompt Optimization Framework for Large Language Models", "abstract": "The effectiveness of Large Language Models (LLMs) is highly dependent on the design of input prompts. Manual prompt engineering requires a domain expertise and prompting techniques knowledge that leads to a complex, time-consuming, subjective, and often suboptimal process. We introduce CoolPrompt as a novel framework for automatic prompt optimization. It provides a complete zero-configuration workflow, which includes automatic task and metric selection, also splits the input dataset or generates synthetic data when annotations are missing, and final feedback collection of prompt optimization results. Our framework provides three new prompt optimization algorithms ReflectivePrompt and DistillPrompt that have demonstrated effectiveness compared to similar optimization algorithms, and a flexible meta-prompting approach called HyPE for rapid optimization. Competitive and experimental results demonstrate the effectiveness of CoolPrompt over other solutions.", "tldr": "", "keywords": ["AutoPrompting", "LLM", "NLP", "prompt engineering", "prompt optimization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f174714206c88e963817deb76e4e006076e9a8e.pdf", "supplementary_material": "/attachment/1b95a0562fe49edaa1ef3c350c8b2a3b4917f462.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents CoolPrompt, an automatic prompt optimization framework designed to be \"zero-configuration\". It automates the full optimization pipeline, including automatic task and metric selection, and synthetic data generation for when annotations are missing. The framework introduces three new optimization algorithms: HyPE (a rapid meta-prompting method),"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The primary strength is the \"zero-configuration\" framework that aims to automate the entire autoprompting pipeline. This includes valuable components like a Task Detector, Synthetic Data Generator, and a PromptAssistant for optimization feedback, addressing significant practical barriers for users."}, "weaknesses": {"value": "1. The evaluation is limited which doesn't benchmark against other prompt optimization methods such as OPRO. \n2. I reserves conservation on the novelty of framework. \n3. The proposed three prompt optimization approaches is not thoroughly benchmarked and analyzed if they are the main contribution."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vbCkH9T5hc", "forum": "XGECnjDEcS", "replyto": "XGECnjDEcS", "signatures": ["ICLR.cc/2026/Conference/Submission19102/Reviewer_Z8Xp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19102/Reviewer_Z8Xp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951379565, "cdate": 1761951379565, "tmdate": 1762931131113, "mdate": 1762931131113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Official Comment"}, "comment": {"value": "Thank you for your reviews.\n\nTo Reviewer w4An:\n\nAbout key novelty of proposed methods:\nThe key novelty of the ReflectivePrompt is the application of the reflective evolution method within the framework of the autoprompting task (originally it was used as a part of Language Hyper-Heuristics for different COP solutions and was not considered as a prompt optimization technique). \nWhile HyPE is inspired by the hypothetical-generation idea in HyDE, our contribution is distinct: we apply this paradigm to prompt optimization rather than retrieval. HyPE provides a data-free, single-step enhancement procedure that avoids task-specific exemplars and iterative search, using only one LLM call to produce a refined, task-aligned instruction. \nAbout cost analysis: Check Appendix 5.5. \nAbout the technical limitations of existing methods when using custom LLMs:\nSince current frameworks have a specific list of available models, the limitation is that using a custom model requires additional time and development expertise to adapt the solution's source code. If modified incorrectly, this can also affect the optimization results. Furthermore, accessing models via an API is not the only method available. Our library implements other common native approaches for running local models.\n\nTo Reviewer cWFZ:\nWe provided detailed implementation information in Appendix A 5.3.\n\nAbout key novelty of proposed methods:\nCheck answer 1 to Reviewer w4An\n\n\nTo Reviewer cWFZ and qvU3:\nReminder for the experimental setup:\nThe main settings for the automatic optimization libraries were set to default. Identical initial prompts, data for the optimization process, and model generation parameters were used for the experiment.\nClarification regarding the datasets: The reason for using this small optimization dataset size is to demonstrate the effectiveness of the prompt optimization solution with limited data. The data for optimization was randomly selected and identical for each framework, respectively.\n\nTo Reviewer 5tTe:\n\nClarification about CoT:\nIn this section we were referring to the original formulation of Chain-of-Thought (CoT) prompting introduced by Wei et al. (2022)\nJustification for several proposed methods and computational cost analysis: Check Appendix 5.5. \nAbout relatively old datasets: Noted, thank you, we will keep this in mind for our future research.\nAbout pipeline contribution: The primary contribution to prompt optimization comes from the optimization algorithms themselves (HyPE, DistillPrompt, ReflectivePrompt). Modules such as TaskDetector, Synthetic Data Generator, and others are used to address engineering challenges such as implicit metric or task type parameters, lack of labeled data, and other issues. Therefore, in this case, we can discuss the effectiveness and competitiveness of the automatic prompt optimization algorithms, which are sufficiently described in our paper.\n\nTo reviewer qvU3:\n\nComputational cost and execution time comparison is shown in Appendix 5.5\nIndeed, such an anomaly exists for this dataset: the optimized prompt from Promptify failed to produce summarization results, which consequently caused the BERTScore value to drop.\nAbout LLM Circularity:\nLLM circularity does not arise in our experiments because:\n1. We rely primarily on non-LLM evaluation metrics.\n2. When using an LLM-based metric (e.g., G-Eval), we separate the target and system models.\nAbout synthetic data generator: Appendix A 5.3.\nAbout prompts transferability: please see Appendix 2.4.\nOur framework is capable of handling complex tasks for two reasons:\n1. Support for custom, user-defined evaluation functions (including LLM-based ones such as G-Eval).\n2. The benchmark suite already covers diverse and nontrivial task types in our experiments. The datasets used in our evaluation span several fundamentally different task families.\n\nTo ICLR 2026 Conference Program Chairs and Senior Area Chairs:\nI am writing to formally appeal the decision and assessment of the review for our submission. While we respect the rigorous peer-review process, we have identified several substantive issues in the provided review that we believe warrant your reconsideration. A rejection based on a lack of novelty requires identifying the specific literature that preempts these contributions, which the reviewer has not done. The review states the evaluation is \"limited\" and does not benchmark against \"other prompt optimization methods such as OPRO.\" It does not specify which aspects of our comprehensive evaluation (e.g., on diverse tasks, with synthetic data) are considered \"limited.\" Given these points, we believe the review does not provide a fair or sufficiently expert assessment of our work. We respectfully request that you seek an additional, independent review from a reviewer with demonstrated expertise in automatic prompt optimization to ensure a balanced evaluation. Thank you."}}, "id": "wgRYBQTlJs", "forum": "XGECnjDEcS", "replyto": "XGECnjDEcS", "signatures": ["ICLR.cc/2026/Conference/Submission19102/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19102/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19102/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763585573858, "cdate": 1763585573858, "tmdate": 1763585573858, "mdate": 1763585573858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CoolPrompt, a zero-configuration framework for automatic prompt optimization that combines (1) a rapid meta-prompting method HyPE, and (2) two longer-running autoprompting algorithms ReflectivePrompt and DistillPrompt. The system also includes automatic task/metric detection, an LLM-driven synthetic data generator, and a PromptAssistant that provides feedback; experimental comparisons across several benchmarks show competitive results versus other autoprompting libraries."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper demonstrates an end-to-end system perspective by presenting a complete pipeline that includes task detection, synthetic data generation, optimization strategies, and feedback mechanisms, making it valuable as an applied system.\n2. It introduces diverse optimization strategies—HyPE for meta-prompting, ReflectivePrompt for evolutionary reflection, and DistillPrompt for iterative distillation—that complement each other and are well motivated.\n3. The work incorporates several practical features, such as model-agnostic integration through LangChain, automatic metric selection, and feedback generation, which strengthen its engineering contributions.\n4. The authors conduct thorough ablations on synthetic data and meta-prompt design, evaluating synthetic data quality across models and refining the HyPE meta-prompt, making these analyses appropriate and informative."}, "weaknesses": {"value": "1. The experimental setup is weakly grounded — each task uses only 30 samples and three runs, which is insufficient for statistically reliable conclusions.\n2. The paper does not report standard deviations, confidence intervals, or significance tests, making it difficult to assess robustness. The optimization experiments rely on only 30 samples per task and three runs, which is too small to draw statistically significant conclusions. No confidence intervals, variance estimates, or error bars are provided.\n3. Several baseline results appear inconsistent or potentially misconfigured (e.g., extremely low BertScore values), raising questions about fairness and reproducibility.\n4. The paper lacks human evaluation for generative tasks and does not discuss known failure cases or limitations. All evaluations rely on automated metrics like BertScore or EM, which are insufficient for tasks involving coherence, relevance, or factual consistency. A small-scale human study would have strengthened the evidence.\n5. Conceptual novelty is modest; the work focuses more on system integration than on algorithmic advancement.\n6. Key configuration details are missing - including random seeds, prompt templates for baselines, number of LLM calls, runtime, and cost estimates. Without these, reproducibility is difficult, and results cannot be independently verified.Key configuration details are missing — including random seeds, prompt templates for baselines, number of LLM calls, runtime, and cost estimates. Without these, reproducibility is difficult, and results cannot be independently verified.\n7. The paper does not analyze cases where automatic prompt optimization fails (e.g., tasks with ambiguous metrics or open-ended generation), nor does it provide insights into when human-in-the-loop guidance remains necessary."}, "questions": {"value": "1. How many optimization trials and LLM calls were performed per method, and what were the compute costs?\n2. How consistent are results when the same optimization is repeated with different random seeds?\n3. How were baselines (Promptify, DSPy, etc.) configured, and were identical datasets, metrics, and generation parameters used?\n4. What explains anomalies such as the very low BertScore for some baselines in Table 1?\n5. How is circularity avoided when using GPT-4o or GPT-3.5 both as optimizer and as evaluation model?\n6. What mechanisms ensure that automatically generated synthetic data are accurate and not hallucinated or mislabeled?\n7. Can the authors demonstrate transferability?  Do optimized prompts generalize to unseen LLMs or unseen domains?\n8. How does the proposed system handle tasks where evaluation metrics cannot be automatically inferred (e.g., subjective or multi-objective tasks)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QS7rbUCy8s", "forum": "XGECnjDEcS", "replyto": "XGECnjDEcS", "signatures": ["ICLR.cc/2026/Conference/Submission19102/Reviewer_qvU3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19102/Reviewer_qvU3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996399356, "cdate": 1761996399356, "tmdate": 1762931130662, "mdate": 1762931130662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new prompt optimization framework, CoolPrompt, that receives a generic description of the task, and then uses a series of optimization processes to iteratively refine the prompt using real data, synthetic data, and LLMs. Three central techniques are used: (1) using an LLM to generate new prompts, (2) a genetic algorithm to extend and synthesize prompts, and (3) and an iterative generate and distill approach. The approach is tested on five type of tasks, with prompts from the second technique attaining higher performance on four of five"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Presents multiple options for how to optimize prompts automatically. Methods uses a series of steps that can each potentially contribute to the optimization\n\n- Minor, but I agree with the authors that this paper is filling a key gap in prompt optimization for smaller open-weight models\n\n- Compares performance on five types of tasks"}, "weaknesses": {"value": "- The paper describes a single framework but three approaches are analyzed (Table 2) and it is not clear to me whether the framework picks one of these methods or if their prompts are somehow aggregated. Given the superior performance of the ReflectivePrompt setup, it's not clear why the other approaches are needed (perhaps they do better for some settings?)\n\n- The whole pipeline is relatively complex, which isn't necessarily bad. However, it's not clear what in this pipeline is contributing to a better prompt. Some type of ablation analysis would be very helpful here beyond what's in A.2. It would help to see what impact each stage has on the resulting performance.\n\n- While the models are getting better performance with the prompt, I would be very interested in the timing of the pipeline, given its complexity. Compared to other approaches, how much longer (or shorter) is CoolPrompt?\n\n- The tasks, while diverse, are relatively old with AG News being over a decade. I'm not opposed to older tasks if they still pose a challenge but given the age, most pretrained/large language models have seen this data which adds a potential confound. Also, given the relatively high performance on all tasks, even for the manual zero-shot prompt, it would be useful to see whether these approaches work for more recent and more challenging tasks.\n\n- Minor, but many experiment details are moved to the appendix. however, the paper has nearly a page of extra space. It would make the paper much more readable to have this content in the main part."}, "questions": {"value": "- I was confused by the comment on line 251 that CoT requires task-specific exemplars. I don't think this is true since you could just include the \"Think step by step\" command in the prompt to elicit CoT output. Could you clarify what is meant here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RC0kkPsol4", "forum": "XGECnjDEcS", "replyto": "XGECnjDEcS", "signatures": ["ICLR.cc/2026/Conference/Submission19102/Reviewer_5tTe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19102/Reviewer_5tTe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062926241, "cdate": 1762062926241, "tmdate": 1762931130190, "mdate": 1762931130190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CoolPrompt, an automatic prompt optimization framework for large language models. The framework provides a zero-configuration workflow including automatic task detection, metric selection, synthetic data generation, and optimization feedback. The authors propose three optimization methods: HyPE for rapid optimization, and ReflectivePrompt and DistillPrompt for long-term optimization. Experiments on multiple benchmark datasets demonstrate the framework's effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. This work proposes a relatively complete prompt optimization framework with commendable attention to data aspects (including automatic data generation and task detection), which has significant practical value for real-world applications.\n2. The work introduces several optimization algorithms synthesized from existing approaches (HyPE, ReflectivePrompt, DistillPrompt) and demonstrates improved performance over baselines across multiple tasks."}, "weaknesses": {"value": "1. The paper's presentation requires significant improvement. Figures 1, 2, and 3 use crude flowcharts that rely heavily on text stacking, lacking clear visual hierarchy, making it difficult for readers to quickly grasp key information and inter-module relationships.\n2. While claiming to present a complete framework, the paper omits critical implementation details. Particularly, the synthetic data generation module, listed as a core contribution, is only described with a four-step outline (Appendix A.5.3) without providing specific generation algorithms, prompt templates, or quality control mechanisms, making this contribution difficult to verify and reproduce.\n3. The experimental design has significant flaws. First, the benchmark datasets are dated and the experimental scale is too small (only 30 samples total, with just 6 samples in the training set). Second, the comparison targets are inappropriate, only comparing against engineering frameworks like Promptify and AdalFlow, rather than mainstream optimization algorithms such as OPRO, DSPy, and APE. Finally, the experiments lack statistical significance testing, ablation studies, and cost analysis.\n4. The paper lacks substantial academic novelty. ReflectivePrompt and DistillPrompt are essentially combinations of existing evolutionary algorithms and Tree-of-Thoughts methods. While HyPE shows some originality, it lacks in-depth analysis. The synthetic data generation is not a novel method but merely an engineering integration. Overall, this reads more like a technical report for an engineering project rather than an academic paper with significant methodological contributions."}, "questions": {"value": "Please provide detailed implementations of key modules, particularly the complete algorithmic workflow and data quality control mechanisms for synthetic data generation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zoSz5jlCUK", "forum": "XGECnjDEcS", "replyto": "XGECnjDEcS", "signatures": ["ICLR.cc/2026/Conference/Submission19102/Reviewer_cWFZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19102/Reviewer_cWFZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762079458176, "cdate": 1762079458176, "tmdate": 1762931129660, "mdate": 1762931129660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an automatic prompt optimization system for large language models (LLMs) that includes automatic task and metric selection, and synthetic data generation. The proposed method provides three prompt optimization strategies: ReflectivePrompt, DistillPrompt, and HyPE. The effectiveness of the proposed method is verified through experiments on several tasks, including question answering, mathematical reasoning, and text classification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper provides a zero-configuration framework for prompt optimization, which is useful for users who are not familiar with prompt engineering.\n- In the proposed method, three different prompt optimization strategies are provided for short- and long-term optimization.\n- The experimental results demonstrate that the proposed method exhibits competitive performance on different tasks with existing prompt optimization methods."}, "weaknesses": {"value": "- Although this paper proposes a complete pipeline for prompt optimization, each component of the proposed method is based on existing methods. For instance, the prompt optimizers, HyPE and ReflectivePrompt, are based on prior works of HyDE (Gao et al., 2023) and Reflective Evolution (Ye et al., 2024). The technical novelty of the proposed method is limited, and the core innovation of this paper is not clear.\n- The experimental evaluation is weak:\n    - Only the gpt-3.5-turbo model is considered. It is unclear whether the proposed method works well for other LLMs.\n    - Only the performance of each prompt optimization method is evaluated. However, the cost of each method in terms of LLM API calls or token consumption is not discussed."}, "questions": {"value": "- How is the computational cost of each prompt optimization method in terms of LLM API calls or token consumption?\n- The authors claim that other automatic prompt optimization frameworks have limitations of usage only for proprietary LLMs. However, the reviewer cannot understand the reason for this point. What are the technical limitations and difficulties of existing methods when using custom LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OlSotQ4zap", "forum": "XGECnjDEcS", "replyto": "XGECnjDEcS", "signatures": ["ICLR.cc/2026/Conference/Submission19102/Reviewer_w4An"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19102/Reviewer_w4An"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156692666, "cdate": 1762156692666, "tmdate": 1762931129008, "mdate": 1762931129008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}