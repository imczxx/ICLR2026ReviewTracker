{"id": "XGECnjDEcS", "number": 19102, "cdate": 1758293577413, "mdate": 1759897059373, "content": {"title": "CoolPrompt: An Automatic Prompt Optimization Framework for Large Language Models", "abstract": "The effectiveness of Large Language Models (LLMs) is highly dependent on the design of input prompts. Manual prompt engineering requires a domain expertise and prompting techniques knowledge that leads to a complex, time-consuming, subjective, and often suboptimal process. We introduce CoolPrompt as a novel framework for automatic prompt optimization. It provides a complete zero-configuration workflow, which includes automatic task and metric selection, also splits the input dataset or generates synthetic data when annotations are missing, and final feedback collection of prompt optimization results. Our framework provides three new prompt optimization algorithms ReflectivePrompt and DistillPrompt that have demonstrated effectiveness compared to similar optimization algorithms, and a flexible meta-prompting approach called HyPE for rapid optimization. Competitive and experimental results demonstrate the effectiveness of CoolPrompt over other solutions.", "tldr": "", "keywords": ["AutoPrompting", "LLM", "NLP", "prompt engineering", "prompt optimization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d896eaeb2238957eeb49d2088796ba6b3e1306cb.pdf", "supplementary_material": "/attachment/1b95a0562fe49edaa1ef3c350c8b2a3b4917f462.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents CoolPrompt, an automatic prompt optimization framework designed to be \"zero-configuration\". It automates the full optimization pipeline, including automatic task and metric selection, and synthetic data generation for when annotations are missing. The framework introduces three new optimization algorithms: HyPE (a rapid meta-prompting method),"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The primary strength is the \"zero-configuration\" framework that aims to automate the entire autoprompting pipeline. This includes valuable components like a Task Detector, Synthetic Data Generator, and a PromptAssistant for optimization feedback, addressing significant practical barriers for users."}, "weaknesses": {"value": "1. The evaluation is limited which doesn't benchmark against other prompt optimization methods such as OPRO. \n2. I reserves conservation on the novelty of framework. \n3. The proposed three prompt optimization approaches is not thoroughly benchmarked and analyzed if they are the main contribution."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vbCkH9T5hc", "forum": "XGECnjDEcS", "replyto": "XGECnjDEcS", "signatures": ["ICLR.cc/2026/Conference/Submission19102/Reviewer_Z8Xp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19102/Reviewer_Z8Xp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951379565, "cdate": 1761951379565, "tmdate": 1762931131113, "mdate": 1762931131113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CoolPrompt, a zero-configuration framework for automatic prompt optimization that combines (1) a rapid meta-prompting method HyPE, and (2) two longer-running autoprompting algorithms ReflectivePrompt and DistillPrompt. The system also includes automatic task/metric detection, an LLM-driven synthetic data generator, and a PromptAssistant that provides feedback; experimental comparisons across several benchmarks show competitive results versus other autoprompting libraries."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper demonstrates an end-to-end system perspective by presenting a complete pipeline that includes task detection, synthetic data generation, optimization strategies, and feedback mechanisms, making it valuable as an applied system.\n2. It introduces diverse optimization strategies—HyPE for meta-prompting, ReflectivePrompt for evolutionary reflection, and DistillPrompt for iterative distillation—that complement each other and are well motivated.\n3. The work incorporates several practical features, such as model-agnostic integration through LangChain, automatic metric selection, and feedback generation, which strengthen its engineering contributions.\n4. The authors conduct thorough ablations on synthetic data and meta-prompt design, evaluating synthetic data quality across models and refining the HyPE meta-prompt, making these analyses appropriate and informative."}, "weaknesses": {"value": "1. The experimental setup is weakly grounded — each task uses only 30 samples and three runs, which is insufficient for statistically reliable conclusions.\n2. The paper does not report standard deviations, confidence intervals, or significance tests, making it difficult to assess robustness. The optimization experiments rely on only 30 samples per task and three runs, which is too small to draw statistically significant conclusions. No confidence intervals, variance estimates, or error bars are provided.\n3. Several baseline results appear inconsistent or potentially misconfigured (e.g., extremely low BertScore values), raising questions about fairness and reproducibility.\n4. The paper lacks human evaluation for generative tasks and does not discuss known failure cases or limitations. All evaluations rely on automated metrics like BertScore or EM, which are insufficient for tasks involving coherence, relevance, or factual consistency. A small-scale human study would have strengthened the evidence.\n5. Conceptual novelty is modest; the work focuses more on system integration than on algorithmic advancement.\n6. Key configuration details are missing - including random seeds, prompt templates for baselines, number of LLM calls, runtime, and cost estimates. Without these, reproducibility is difficult, and results cannot be independently verified.Key configuration details are missing — including random seeds, prompt templates for baselines, number of LLM calls, runtime, and cost estimates. Without these, reproducibility is difficult, and results cannot be independently verified.\n7. The paper does not analyze cases where automatic prompt optimization fails (e.g., tasks with ambiguous metrics or open-ended generation), nor does it provide insights into when human-in-the-loop guidance remains necessary."}, "questions": {"value": "1. How many optimization trials and LLM calls were performed per method, and what were the compute costs?\n2. How consistent are results when the same optimization is repeated with different random seeds?\n3. How were baselines (Promptify, DSPy, etc.) configured, and were identical datasets, metrics, and generation parameters used?\n4. What explains anomalies such as the very low BertScore for some baselines in Table 1?\n5. How is circularity avoided when using GPT-4o or GPT-3.5 both as optimizer and as evaluation model?\n6. What mechanisms ensure that automatically generated synthetic data are accurate and not hallucinated or mislabeled?\n7. Can the authors demonstrate transferability?  Do optimized prompts generalize to unseen LLMs or unseen domains?\n8. How does the proposed system handle tasks where evaluation metrics cannot be automatically inferred (e.g., subjective or multi-objective tasks)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QS7rbUCy8s", "forum": "XGECnjDEcS", "replyto": "XGECnjDEcS", "signatures": ["ICLR.cc/2026/Conference/Submission19102/Reviewer_qvU3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19102/Reviewer_qvU3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996399356, "cdate": 1761996399356, "tmdate": 1762931130662, "mdate": 1762931130662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new prompt optimization framework, CoolPrompt, that receives a generic description of the task, and then uses a series of optimization processes to iteratively refine the prompt using real data, synthetic data, and LLMs. Three central techniques are used: (1) using an LLM to generate new prompts, (2) a genetic algorithm to extend and synthesize prompts, and (3) and an iterative generate and distill approach. The approach is tested on five type of tasks, with prompts from the second technique attaining higher performance on four of five"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Presents multiple options for how to optimize prompts automatically. Methods uses a series of steps that can each potentially contribute to the optimization\n\n- Minor, but I agree with the authors that this paper is filling a key gap in prompt optimization for smaller open-weight models\n\n- Compares performance on five types of tasks"}, "weaknesses": {"value": "- The paper describes a single framework but three approaches are analyzed (Table 2) and it is not clear to me whether the framework picks one of these methods or if their prompts are somehow aggregated. Given the superior performance of the ReflectivePrompt setup, it's not clear why the other approaches are needed (perhaps they do better for some settings?)\n\n- The whole pipeline is relatively complex, which isn't necessarily bad. However, it's not clear what in this pipeline is contributing to a better prompt. Some type of ablation analysis would be very helpful here beyond what's in A.2. It would help to see what impact each stage has on the resulting performance.\n\n- While the models are getting better performance with the prompt, I would be very interested in the timing of the pipeline, given its complexity. Compared to other approaches, how much longer (or shorter) is CoolPrompt?\n\n- The tasks, while diverse, are relatively old with AG News being over a decade. I'm not opposed to older tasks if they still pose a challenge but given the age, most pretrained/large language models have seen this data which adds a potential confound. Also, given the relatively high performance on all tasks, even for the manual zero-shot prompt, it would be useful to see whether these approaches work for more recent and more challenging tasks.\n\n- Minor, but many experiment details are moved to the appendix. however, the paper has nearly a page of extra space. It would make the paper much more readable to have this content in the main part."}, "questions": {"value": "- I was confused by the comment on line 251 that CoT requires task-specific exemplars. I don't think this is true since you could just include the \"Think step by step\" command in the prompt to elicit CoT output. Could you clarify what is meant here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RC0kkPsol4", "forum": "XGECnjDEcS", "replyto": "XGECnjDEcS", "signatures": ["ICLR.cc/2026/Conference/Submission19102/Reviewer_5tTe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19102/Reviewer_5tTe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062926241, "cdate": 1762062926241, "tmdate": 1762931130190, "mdate": 1762931130190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CoolPrompt, an automatic prompt optimization framework for large language models. The framework provides a zero-configuration workflow including automatic task detection, metric selection, synthetic data generation, and optimization feedback. The authors propose three optimization methods: HyPE for rapid optimization, and ReflectivePrompt and DistillPrompt for long-term optimization. Experiments on multiple benchmark datasets demonstrate the framework's effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. This work proposes a relatively complete prompt optimization framework with commendable attention to data aspects (including automatic data generation and task detection), which has significant practical value for real-world applications.\n2. The work introduces several optimization algorithms synthesized from existing approaches (HyPE, ReflectivePrompt, DistillPrompt) and demonstrates improved performance over baselines across multiple tasks."}, "weaknesses": {"value": "1. The paper's presentation requires significant improvement. Figures 1, 2, and 3 use crude flowcharts that rely heavily on text stacking, lacking clear visual hierarchy, making it difficult for readers to quickly grasp key information and inter-module relationships.\n2. While claiming to present a complete framework, the paper omits critical implementation details. Particularly, the synthetic data generation module, listed as a core contribution, is only described with a four-step outline (Appendix A.5.3) without providing specific generation algorithms, prompt templates, or quality control mechanisms, making this contribution difficult to verify and reproduce.\n3. The experimental design has significant flaws. First, the benchmark datasets are dated and the experimental scale is too small (only 30 samples total, with just 6 samples in the training set). Second, the comparison targets are inappropriate, only comparing against engineering frameworks like Promptify and AdalFlow, rather than mainstream optimization algorithms such as OPRO, DSPy, and APE. Finally, the experiments lack statistical significance testing, ablation studies, and cost analysis.\n4. The paper lacks substantial academic novelty. ReflectivePrompt and DistillPrompt are essentially combinations of existing evolutionary algorithms and Tree-of-Thoughts methods. While HyPE shows some originality, it lacks in-depth analysis. The synthetic data generation is not a novel method but merely an engineering integration. Overall, this reads more like a technical report for an engineering project rather than an academic paper with significant methodological contributions."}, "questions": {"value": "Please provide detailed implementations of key modules, particularly the complete algorithmic workflow and data quality control mechanisms for synthetic data generation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zoSz5jlCUK", "forum": "XGECnjDEcS", "replyto": "XGECnjDEcS", "signatures": ["ICLR.cc/2026/Conference/Submission19102/Reviewer_cWFZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19102/Reviewer_cWFZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762079458176, "cdate": 1762079458176, "tmdate": 1762931129660, "mdate": 1762931129660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an automatic prompt optimization system for large language models (LLMs) that includes automatic task and metric selection, and synthetic data generation. The proposed method provides three prompt optimization strategies: ReflectivePrompt, DistillPrompt, and HyPE. The effectiveness of the proposed method is verified through experiments on several tasks, including question answering, mathematical reasoning, and text classification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper provides a zero-configuration framework for prompt optimization, which is useful for users who are not familiar with prompt engineering.\n- In the proposed method, three different prompt optimization strategies are provided for short- and long-term optimization.\n- The experimental results demonstrate that the proposed method exhibits competitive performance on different tasks with existing prompt optimization methods."}, "weaknesses": {"value": "- Although this paper proposes a complete pipeline for prompt optimization, each component of the proposed method is based on existing methods. For instance, the prompt optimizers, HyPE and ReflectivePrompt, are based on prior works of HyDE (Gao et al., 2023) and Reflective Evolution (Ye et al., 2024). The technical novelty of the proposed method is limited, and the core innovation of this paper is not clear.\n- The experimental evaluation is weak:\n    - Only the gpt-3.5-turbo model is considered. It is unclear whether the proposed method works well for other LLMs.\n    - Only the performance of each prompt optimization method is evaluated. However, the cost of each method in terms of LLM API calls or token consumption is not discussed."}, "questions": {"value": "- How is the computational cost of each prompt optimization method in terms of LLM API calls or token consumption?\n- The authors claim that other automatic prompt optimization frameworks have limitations of usage only for proprietary LLMs. However, the reviewer cannot understand the reason for this point. What are the technical limitations and difficulties of existing methods when using custom LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OlSotQ4zap", "forum": "XGECnjDEcS", "replyto": "XGECnjDEcS", "signatures": ["ICLR.cc/2026/Conference/Submission19102/Reviewer_w4An"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19102/Reviewer_w4An"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156692666, "cdate": 1762156692666, "tmdate": 1762931129008, "mdate": 1762931129008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}