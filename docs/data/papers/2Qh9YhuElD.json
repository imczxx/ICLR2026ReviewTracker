{"id": "2Qh9YhuElD", "number": 24459, "cdate": 1758357111367, "mdate": 1759896764973, "content": {"title": "ICPRL: Acquiring Physical Intuition from Interactive Control", "abstract": "VLMs excel at static perception but falter in interactive reasoning in dynamic physical environments, which demands planning and adaptation to dynamic outcomes. Existing physical reasoning methods often depend on abstract symbolic inputs or lack the ability to learn and adapt from direct, pixel-based visual interaction in novel scenarios. We introduce **ICPRL** (In-Context Physical Reinforcement Learning), a framework inspired by In-Context Reinforcement Learning (ICRL) that empowers VLMs to acquire physical intuition and adapt their policies in-context. Our approach trains a vision-grounded policy model to internalize a policy-improvement process by learning across diverse multi-episode interaction histories. \nThis enables the agent to adapt its strategy at test time to unseen environments by conditioning on recent interactions, without requiring any weight updates. This adaptive policy works in concert with a separately trained world model that provides explicit physical reasoning by predicting the results of potential actions. \nAt inference, the policy proposes candidate actions, while the world model functions as an in-context planner, guiding a search procedure to select the most promising action. Evaluated on the diverse physics-based puzzle-solving tasks in the DeepPHY benchmark, **ICPRL** demonstrates significant improvements across both its policy-only and world-model-augmented stages. Notably, these gains are retained in unseen physical environments, demonstrating that our framework facilitates genuine in-context acquisition of the environment's physical dynamics from interactive experience.", "tldr": "", "keywords": ["VLM Agents", "Physical Reasoning", "Dynamic Environments"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c0dad954665eea2bef247c6e77bbb1b25149732.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces In-Context Physical Reinforcement Learning (ICPRL), a framework that enables vision-language models (VLMs) to perform interactive physical reasoning in dynamic environments without weight updates. The approach combines an adaptive policy model—trained with Generalized Reinforcement Preference Optimization (GRPO) on multi-episode interaction histories—to learn in-context adaptation, with a separately trained world model that predicts action outcomes and guides planning via a PUCT search. Both models are built on Qwen2.5-VL-3B/7B backbones and trained on PHYRE and I-PHYRE, then evaluated on unseen environments (Kinetix, Pooltool, Angry Birds, Cut the Rope) within the DeepPHY benchmark. Experiments show that ICPRL improves the performance significantly compared to the baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents an in-context reinforcement learning framework applied to vision-language models (VLMs), leveraging VLMs' inherent generalization and in-context reasoning abilities—an idea that could have notable value for the broader learning and embodied AI community.\n\n2. The incorporation of a world model for score prediction and planning is a useful design that enhances reasoning and supports generalization to unseen domains, offering a potentially transferable mechanism for future adaptive agents.\n\n3. The experimental evaluation is comprehensive, covering multiple physics-based environments across the DeepPHY benchmark. The writing and presentation are clear."}, "weaknesses": {"value": "1. The paper lacks clarity regarding the decision horizon and episode length, making it difficult for readers to assess how well the proposed method generalizes to tasks with longer temporal dependencies.\n\n2. The chosen benchmarks are highly structured and have short decision horizons, which diminishes the reinforcement learning aspect. The policy behavior may resemble an enumerative strategy—selecting unseen actions based on past outcomes—rather than genuine long-horizon reasoning. It remains unclear whether the approach would hold in settings like Atari or DMC that require extended sequential control.\n\n3. It is unclear whether ICPRL genuinely enhances in-context learning capability beyond what is already achieved through supervised fine-tuning (SFT) or task-specific adaptation. In tasks where models have been heavily fine-tuned, it remains questionable whether adding the ICPRL stage provides additional adaptive benefit rather than simply reinforcing prior task-specific behavior.\n\n4. The definition and role of \"attempts\" are insufficiently detailed, and since this metric is tightly coupled to the benchmark structure, it limits interpretability and reproducibility."}, "questions": {"value": "1. Could the authors clarify the episode length, number of actions per episode, and the formal definition of “attempts” used across different environments?\n\n2. How do the authors rule out the possibility that the improvements arise primarily from adaptation to task format rather than genuine in-context learning (weakness 3)? Including an additional baseline or diagnostic analysis addressing this would strengthen the paper’s claims."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fubpendf3z", "forum": "2Qh9YhuElD", "replyto": "2Qh9YhuElD", "signatures": ["ICLR.cc/2026/Conference/Submission24459/Reviewer_zPwA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24459/Reviewer_zPwA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811766903, "cdate": 1761811766903, "tmdate": 1762943087975, "mdate": 1762943087975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ICPRL (In-Context Physical Reinforcement Learning), a framework designed to enable Vision-Language Models (VLMs) to perform interactive reasoning and planning in dynamic physical environments. The core idea is to decouple the agent into two components: an adaptive policy model trained via Group Relative Policy Optimization (GRPO) and a world model trained separately and offline to predict the physical outcomes of actions. At inference, the adaptive policy acts as a prior, proposing candidate actions. The world model then evaluates these candidates, and its predictions are used to guide a PUCT search algorithm to select the optimal action. The authors evaluate this framework on the DeepPHY benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Valid and Sound Idea: The paper presents a valid and logical approach. Combining the principles of In-Context Reinforcement Learning (ICRL) with an explicit, learned world model is a sensible strategy for tackling complex, interactive physical reasoning tasks that VLMs currently struggle with.\n* Effective Paradigm Extension: The framework successfully extends the well-established planning and learning paradigm to the VLM domain. The architecture, which integrates a policy prior, a value/outcome model, and a search procedure, is a proven combination, and the paper shows it can be effectively adapted for VLM-based agents.\n* Comprehensive Experiments: The experimental evaluation is a clear strength. The authors test their framework across diverse environments in the DeepPHY benchmark and compare it against strong VLM baselines. The inclusion of thorough ablation studies provides convincing evidence for the contribution of each component of the ICPRL framework."}, "weaknesses": {"value": "* Marginal Novelty over Existing Paradigms: The primary weakness is that the framework's architecture is fundamentally a straightforward extension of the MuZero algorithm, adapted for the VLM setup. MuZero also combines a learned policy prior and a world model to guide a Monte Carlo Tree Search (MCTS, of which PUCT is a variant). While applying this to VLMs is a good engineering contribution, the paper does not offer new scientific insight into the underlying principles of planning or model-based reinforcement learning, as these concepts are already well-proven.\n* Limited Impact Given Prior Work: Related to the first point, the Dreamer series of works has also extensively demonstrated that model-based RL (learning a world model and planning within it) is highly effective. This existing body of work makes the core contribution of this paper feel more marginal and incremental.\n* Questionable Generalization Claims: The tasks in DeepPHY benchmark were proposed to evaluate generalization. The authors claim strong generalization by training on two environments and testing on the other four. However, it has been shown that standard RL agents can achieve good performance on these tasks, which may dilute the significance of the results. More importantly, the paper lacks sufficient detail on the nature of the generalization being tested."}, "questions": {"value": "1. Scientific Contribution: Given the strong architectural parallels to MuZero, could the authors elaborate on what they see as the key new scientific insight from this work, beyond demonstrating a successful application of a known-good algorithm to the VLM domain?\n2. Within-Task Generalization: The paper focuses on cross-task generalization (training on PHYRE/I-PHYRE, testing on others). How does the model perform on within-task generalization? For example, was the model evaluated on new, unseen levels or configurations of the training environments (PHYRE and I-PHYRE) that were held out from the training data?\n3. Generalization Evaluation Protocol: Could you clarify the generalization protocol further?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gq7kB3dK2R", "forum": "2Qh9YhuElD", "replyto": "2Qh9YhuElD", "signatures": ["ICLR.cc/2026/Conference/Submission24459/Reviewer_KzRK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24459/Reviewer_KzRK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883155683, "cdate": 1761883155683, "tmdate": 1762943087716, "mdate": 1762943087716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Train a VLM using GRPO over a large collection of interactions. Then give access to a world model which the agent can preform planning in. Inference is then performed by having the policy propose actions and the model to provide simulations to guide search. The GRPO training uses selective token masking, which masks out tokens not generated with the policy. Inference scores the outputs using the model, which takes in actions and then predicts the success."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "develops a comprehensive system for learning an VLM on discrete action data\n\nThe method appears to create improvement in performance with multiple retries\n\nThe experiemtns are on challenging domains and get decent results"}, "weaknesses": {"value": "The writing is highly informal and makes it hard to follow exactly what is being done\n\nThe claims of the paper are not particularly well supported by either the method or the experiments"}, "questions": {"value": "The abstract could benefit from a greater degree of specificity related to how the method works, since \"internalize a policy-improvement process\" is not something that is well defined, as is \"works in concert\". Ideally, there is a sense from the abstract about how the innovations in this work actually acheive these ends. \n\nIt is not obvious how training a VLM using GRPO rather than learning using RL is supposed to teach the model how to adjust its strategy, since it could still be out of distribution for the history that it trained on---it is just augmenting the state with history. \n\nThe description of the method is quite confusing, because there is no formal description of the components. What are the language inputs? What are Trajectories? What does the policy model map between? Why are the actions discrete? How do the actions differ from tokens or language? While some of these are defined later, they are described before being formally defined, and before it is clear what they are used for in the overall process. \\pi_\\text{ref} is also never defined. \n\nSimilar to the action learning stage, the world model is also not well defined, since it is not clear how the JSON is supposed to be a world model, or what the success probabiliy is over. Is it measured by the LLM? Is it measured based on reaching a goal state? Fruthermore, it is not clear what the prediction is supposed to contain since \"qualitative insights for analysis\" is not well defined. The most important missing component is what it formally means to be successful. Without this, the entire description is ungrounded.\n\nWorld model does not seem like the right terminology for the $\\mathcal M_\\phi$, since a world model typically encodes the dynamics, but this \"world model\" seems to only generate text and success rates. It would probably be more accurate to call this some kind of discriminator or evaluator.\n\nThe term grounding is poorly used, since it is not obvious that the VLM it itself grounded, and yet the text describing the objects and their relationships are grounded. Grounded means to attach the meanings of statements to objects in the real world, and yet the VLM could just as likely hallucinate this \"grounding,\" and there is no measure given for preventing this hallucination.\n\nThe promise of the method to include \"physical intuition\" seems to be faulty at best. The intution seems to just come from having a language model take in images and spit out statements about what it thinks will happen, but this is less physical intution as verbal intuition.\n\nConsidering the benchmarks appear to be RL domains, shouldn't the comparisons be also with RL algorithms, not simply VLMs? It seems like if this was genuinely showing physical intuition, it should be able to outperform or at least equal methods that are trained on real representations. As it is the methods appear to perform quite poorly across the board.\n\nThe claim that the online policy is able to adapt its strategy in-context does not appear to be supproted by evidence, since the only results are an improvement in performance, but this could simply be a consequence of more directed training on the environment, and the fact that having a longer context helps the sequence to be more in-distribution. In general it seems like a lager number of explanations could exist for the changes in performance other than having greater \"physical intuition\" as described by this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3wIP4jlGHz", "forum": "2Qh9YhuElD", "replyto": "2Qh9YhuElD", "signatures": ["ICLR.cc/2026/Conference/Submission24459/Reviewer_Ncvj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24459/Reviewer_Ncvj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973597235, "cdate": 1761973597235, "tmdate": 1762943087432, "mdate": 1762943087432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ICPRL (In-Context Physical Reinforcement Learning), a framework that leverages world models to bootstrap an RL agent’s performance in decision-making tasks through test-time planning. Both the policy and the world model use a pre-trained VLM as the backbone. The policy is trained online using GRPO, while the world model is trained on a curated dataset containing both successful and failed trajectories. Planning is performed via Monte Carlo Tree Search over action candidates proposed by the policy, which are scored using the world model’s predictions of the resulting outcomes. The paper shows extensive results on the DeepPHY benchmark, which comprises six simulated environments designed to evaluate VLMs, ablating through various model backbones, training algorithms and datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The idea of using a world model to enhance the test-time performance of policies is intuitive.\n- The paper provides extensive evaluations across different model backbones and training algorithms and datasets."}, "weaknesses": {"value": "- The novelty of the paper is limited. The core idea of using a world model at test time to enhance policy performance is not new and has been explored in prior work such as [1][2]. \n- Using text as the backbone for embodied tasks, where actions are naturally continuous, seems contrived. The paper lacks discussion on why VLMs are the best backbone for both the policy and the world model. \n- The world model is task-specific as it also predicts success/failure of a specific task. What’s the benefit of having a text-based world model than just learning a task-specific value function commonly done in RL other than interpretability?\n- ICPRL assumes that the action space of the tasks are discrete and finite. It’s unclear how the proposed method (world model and policy training, planning algorithm) can be extended to continuous action spaces.\n- The data curation process for world model training is limiting, requiring both successful and failed trajectories in a 1:1 ratio. This seems infeasible for real-world decision-making tasks or even simulated benchmarks with more complex dynamics.\n- The method also requires environment-specific parameters like the noise radius for perturbed actions for the stability score. How is this picked? It’s unclear from the experiments how this would affect the model’s performance. \n- The policy learning module requires online interaction with the simulator, which is largely infeasible for real-world decision-making tasks.\n- The paper’s writing can be improved by providing more details about the benchmark task setup, such as the model inputs, task horizon, and size of the action space. \n\n[1] Han et al. Strengthening Generative Robot Policies through Predictive World Modeling\n\n[2] Delong et al. Planning with Reasoning using Vision Language World Model"}, "questions": {"value": "- Can the paper provide concrete benchmark task examples? For instance, how is a task like Cut the Rope implemented using text-based actions?\n- What are the time and compute costs for training the world model and for test-time planning? What is the action space for the tasks used in the paper?\n- What are the time and compute costs for training the policy in the interactive environment? How many environment steps are required?\n- What is the benefit of using VLMs as the backbone for both the policy and the world model, given that both components are task-specific?\n- Why is the policy trained online? Since success and failure trajectories need to be generated for world model training, why not use the same data to train the policy? Similarly, why can’t the world model be trained from data collected during online policy learning?\n- In Table 1, all fine-tuned models on the Kinetix benchmark perform worse than the random agent (MOCK). While the paper mentions that Kinetix requires high-precision actions, why do the fine-tuned models significantly underperform the closed-source models without fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YSQhInRQFc", "forum": "2Qh9YhuElD", "replyto": "2Qh9YhuElD", "signatures": ["ICLR.cc/2026/Conference/Submission24459/Reviewer_z9qi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24459/Reviewer_z9qi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24459/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050956998, "cdate": 1762050956998, "tmdate": 1762943087044, "mdate": 1762943087044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}