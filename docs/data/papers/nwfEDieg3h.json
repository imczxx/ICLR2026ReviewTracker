{"id": "nwfEDieg3h", "number": 20036, "cdate": 1758301755115, "mdate": 1759897005060, "content": {"title": "From Perception to Punchline: Empowering VLM with the Art of In-the-Wild Meme", "abstract": "Generating humorous memes is a challenging multimodal task that moves beyond direct image-to-caption supervision. It requires a nuanced reasoning over visual content, contextual cues, and subjective humor. To bridge this gap between visual perception and humorous punchline creation, we propose HUMOR, a novel framework that guides VLMs through hierarchical reasoning and aligns them with group-wise human-like preferences.\nFirst, HUMOR employs a hierarchical, multi-path Chain-of-Thought (CoT): the model begins by identifying a template-level intent, then explores diverse reasoning paths under different contexts, and finally anchors onto a high-quality, context-specific path.\nThis CoT supervision, which traces back from ground-truth captions, enhances reasoning diversity.\nWe further analyze that this multi-path exploration with anchoring maintains a high expected humor quality, under the practical condition that high-quality paths retain significant probability mass.\nSecond, to capture subjective humor, we train a pairwise reward model that operates within groups of memes sharing the same template.\nFollowing established theory, this approach ensures a consistent and robust proxy for human preference, even with noisy labels.\nThe reward model then enables a group-wise reinforcement learning optimization, guaranteeing that the model's humor quality does not degrade beyond a bounded amount. \nExperiments show that HUMOR empowers various base VLMs with superior reasoning diversity, more reliable preference alignment, and higher overall meme quality compared to strong baselines.\nBeyond memes, our work presents a general training paradigm for open-ended, human-aligned multimodal generation, where success is guided by comparative judgment within coherent output groups.", "tldr": "HUMOR trains meme generators by multi-path CoT + rank-consistent pairwise rewards + group-wise RL, yielding human-aligned humor with bounded, theory-backed gains across vision–language base models.", "keywords": ["Vision-language models", "Meme generation", "Hierarchical chain-of-thought (CoT) supervision", "Pairwise reward modeling", "Reinforcement Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0eee65172cea63612a10798106cfa0504c15bb84.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes HUMOR, a framework for training vision-language models to generate in-the-wild memes through hierarchical, multi-path chain-of-thought reasoning and group-wise preference learning. The approach introduces two-stage CoT supervision anchored by human-labeled punchlines, formulates meme generation as a within-template comparison task to handle subjective humor, and employs group-wise pairwise reward modeling with GRPO for policy optimization. The authors provide theoretical analysis on humor quality preservation, ranking consistency, and optimization guarantees, along with experiments demonstrating improvements over baseline VLMs on human-evaluated metrics including humor, readability, relevance, and originality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear and well-motivated problem formulation. The paper compellingly reframes meme generation as a group-wise, open-ended reasoning problem with formal notation, explicitly addressing that humor is subjective and cross-template comparisons are unreliable.\n\n2. Comprehensive theoretical analysis with rigorous proofs. Four propositions cover humor quality preservation under multi-path CoT, ranking consistency and noise robustness of pairwise rewards, and KL-constrained optimization guarantees, with clear and mathematically sound proofs provided in Appendices B and C.\n\n3. Human evaluation prioritized with careful critique of automated metrics. The experimental design appropriately gives primacy to human-annotated evaluations, with HUMOR-CoT and HUMOR-RL consistently outperforming open- and closed-source baselines across humor, readability, relevance, and originality dimensions, while also sharply diagnosing the weaknesses and instability of VLM-based absolute scoring for subjective humor."}, "weaknesses": {"value": "1. Insufficient novelty in methodology beyond multi-path CoT and group-wise modeling. While the hierarchical CoT and group-wise preference modeling are reasonable contributions, the subsequent stages (SFT, preference modeling, GRPO) are direct applications of existing mature methods without task-specific innovations. The paper reads more as an engineering combination rather than a methodological advancement, and the theoretical analysis of SFT and GRPO largely restates results already well-established in prior RLHF/DPO literature without clearly delineating which theoretical contributions are unique to this work versus restatements in a new context.\n\n2. Severely limited baseline comparisons and missing ablation studies. The experiments only compare against pretrained VLMs and CoT-driven fine-tuning variants, completely missing comparisons with specialized meme generation models (e.g., GAN/Diffusion-based approaches, template-matching methods) or VLMs trained with alternative alignment techniques (DPO, IPO, KTO). Critical ablation experiments are entirely absent: removing two-stage CoT SFT, replacing GRPO with DPO/PPO, comparing multi-path versus single-path CoT, removing group-wise constraints for global preference modeling, and systematic comparison of different base models as reward models. This makes it impossible to assess whether improvements stem from the proposed method itself or simply from applying RL/SFT.\n\n3. Complete absence of qualitative case studies and generated examples. The paper presents no actual meme samples generated by HUMOR, which is a critical flaw for a generation task focused on subjective humor. Without visual examples, readers cannot judge the practical quality, verify what \"humor\" and \"relevance\" metrics actually capture, or assess failure modes. Essential missing cases include: success cases with actual image+text outputs, failure cases where the model overestimates unfunny memes, risk cases involving offensive content or stereotypes, adversarial cases demonstrating potential reward hacking, and diversity demonstrations across the same template. This lack of qualitative analysis severely undermines the credibility of quantitative results.\n\n4. Poor presentation with critical details relegated to appendices and missing technical information. Key methodological details including dataset construction (Appendix D), EBC computation (Appendix F), and reward design (Appendix E) should be in the main text, as their absence severely impacts readability. Essential technical details are completely missing: full CoT generation prompts with few-shot examples, human rating generation prompt templates, specific definitions of \"7 basic emotions and intensity levels,\" validation of User Requirements reverse-engineering accuracy via API, justification for hyperparameter choices (β, λ_fmt, λ_cnt), and computational costs. Figure 1 fails to demonstrate how claimed limitations (lack of hierarchy, cross-group phenomena) affect actual generation quality. Additionally, there are writing errors including citation formatting issues in Related Work and redundant use of \"equation\" on line 183.\n\n5. Cross-cultural evaluation completely missing despite acknowledging cultural impact. While the paper mentions that cultural differences significantly affect meme humor, experiments appear limited to English-language memes only, with no exploration across different cultural contexts or user demographics. The demographic characteristics of the rating population (age, gender, cultural background) are not reported, yet would significantly influence the numerical results in Table 1, raising concerns about generalizability of findings."}, "questions": {"value": "1. Can you provide comprehensive ablation studies quantifying the contribution of each component? Specifically: (a) What is the performance impact of removing two-stage CoT SFT entirely? (b) How does replacing GRPO with DPO or PPO affect final results? (c) What is the benefit of multi-path CoT compared to single-path baselines? (d) What happens when you remove group-wise constraints and use global preference modeling instead? Please provide quantitative results for each ablation on the same test set used in Table 1.\n\n2. Can you include actual generated meme examples and detailed case studies? Please provide at least 10-15 visual examples including: (a) 3-5 success cases showing high-quality memes with actual images and text, (b) 3-5 failure cases where generated memes score high but are actually unfunny, (c) 2-3 risk cases involving potential offensive content or stereotypes, (d) 2-3 adversarial examples demonstrating reward hacking, if any exist, and (e) examples showing diversity within the same template. How do these qualitative observations align with your quantitative metrics?\n\n3. How does HUMOR compare against specialized meme generation baselines? Your current baselines only include general VLMs and CoT variants. Can you provide comparisons with: (a) traditional template-based meme generation methods, (b) GAN or Diffusion models trained specifically for meme generation, (c) VLMs trained with other state-of-the-art alignment methods like DPO or IPO on the same dataset? Without these comparisons, how can you claim the superiority is from your method rather than simply applying any fine-tuning?\n\n4. Can you verify the theoretical assumptions hold empirically and clarify group definition? Proposition 1 assumes high-quality paths maintain probability mass α and quality gaps are bounded by δ, but you don't report actual measured values. Can you provide training curves showing α and δ throughout optimization to verify assumptions hold? Additionally, how exactly do you define and operationalize group boundaries in practice—manual annotation, automatic clustering, or predefined templates? How do you handle memes that could belong to multiple groups or use hybrid templates?\n\n5. What are the complete technical details for reproduction? Please provide: (a) full prompt templates for CoT generation including few-shot examples, (b) exact prompts used for Gemini-based human rating generation, (c) specific definitions and annotation guidelines for \"7 basic emotions and intensity levels,\" (d) detailed methodology and accuracy validation for reverse-engineering User Requirements via API, (e) rationale for all hyperparameter choices (β, λ_fmt, λ_cnt), and (f) computational costs including GPU hours and memory requirements for each training stage. Why was Gemini specifically chosen as the rating model over alternatives?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper involves human annotation and rating of memes but does not provide details on annotator compensation, consent procedures, or demographic information of raters. Given that memes can contain culturally sensitive or potentially offensive content, proper documentation of responsible research practices with human subjects is needed. Additionally, while the paper mentions filtering violent content, there is insufficient discussion of safety protocols for handling potentially harmful content including discrimination, stereotypes, or microaggressions that may be more subtle in memes."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MSFIyw3HaV", "forum": "nwfEDieg3h", "replyto": "nwfEDieg3h", "signatures": ["ICLR.cc/2026/Conference/Submission20036/Reviewer_ftT4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20036/Reviewer_ftT4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831634557, "cdate": 1761831634557, "tmdate": 1762932935669, "mdate": 1762932935669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for meme understanding that treats humor recognition as a group-wise comparability problem. The authors argue that current Vision-Language Models fail to capture humor due to reasoning collapse and a lack of structured reasoning. To address this, they theoretically formulates the meme understanding problem by defining group-wise comparability and introduce a Hierarchical Chain-of-Thought (CoT) approach that generates multiple reasoning paths and anchors the most coherent one using group-wise reinforcement learning. The method empirically shows improved reasoning quality and human-perceived humor compared to baseline models."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a solid theoretical foundation and formal modeling for the meme understanding problem in group-wise.\n- The experiments explicitly optimize the reasoning process by finetuning with CoT data and use reinforcement learning to align the model’s outputs with human humor distributions"}, "weaknesses": {"value": "- The Hierarchical CoT framework, while conceptually rich, still depends heavily on VLMs to extract the template intent. If the intent extraction is incorrect (e.g., the model misinterprets the scene or theme), all subsequent reasoning chains may deviate.\n- The paper lacks a detailed statistical analysis of the dataset, such as the content composition, image complexity or cultural diversity of memes.\n- The proposed method is only evaluated on single-panel meme images; it may not generalize to multi-panel or sequential memes where contextual humor is distributed across frames.\n- Apart from human evaluation, there are no strong quantitative metrics (e.g., BERTScore, GPT-eval, or G-Eval) to validate the quality of generated captions.\n\nMinor\n- Please enlarge the image text in figures for readability.\n- The training pipeline and data generation pipeline are less clear to understand."}, "questions": {"value": "- On the choice of Λ: How would using different forms of the aggregation function Λ affect the humor scoring or optimization process? Would alternative formulations (e.g., non-logistic or non-symmetric functions) change the reward dynamics?\n- On multi-CoT diversity: Could you elaborate on how you ensure the diversity of the generated multi-CoT reasoning paths? For example, do you use stochastic decoding (e.g., temperature sampling, nucleus sampling) or any explicit diversity-promoting constraints?\n- On the reliability of the Distance metric: Since the reported Context-Swap Distance remains within the narrow range of 0.5–0.6, could this variation simply reflect random noise rather than meaningful diversity? For instance, differences in caption length or stylistic phrasing (rather than semantic novelty) could also influence the cosine distance. In such cases, do you believe this metric still provides a reliable signal for evaluating diversity?\n- On evaluation metrics: Have you considered employing automated evaluation metrics such as GPT-Eval, BERTScore, or ROUGE to assess the generated captions more objectively, in addition to human evaluation? Or could you please give some insights why you don't use them?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nNnoSU9iYO", "forum": "nwfEDieg3h", "replyto": "nwfEDieg3h", "signatures": ["ICLR.cc/2026/Conference/Submission20036/Reviewer_uciU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20036/Reviewer_uciU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928934502, "cdate": 1761928934502, "tmdate": 1762932935157, "mdate": 1762932935157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HUMOR, a framework for training vision-language models (VLMs) to generate humorous memes through hierarchical reasoning and preference alignment. The approach consists of three main components: (1) a hierarchical, multi-path Chain-of-Thought (CoT) supervision that separates template-level intent from context-specific grounding, (2) a pairwise reward model trained on group-wise comparisons to capture subjective humor preferences, and (3) group-wise reinforcement learning optimization. The authors demonstrate improvements over baseline models in humor quality, readability, and diversity metrics through human evaluation and automated metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Novel problem formulation: The paper addresses the challenging task of meme generation as a group-wise reasoning problem, acknowledging that humor comparability is more reliable within meme templates than across them.\n- Theoretically grounded approach: The framework provides theoretical guarantees including conditional humor lower bounds (Proposition 1), rank consistency (Proposition 2), and bounded degradation under KL control (Proposition 4).\n- Comprehensive evaluation: The paper includes both human evaluation across multiple dimensions and novel metrics like \"Distance under Context Swap\" to measure diversity and adaptability.\n- Hierarchical reasoning design: The two-stage CoT approach that separates template understanding from context-specific realization is intuitive and well-motivated for the meme generation task."}, "weaknesses": {"value": "- Missing critical citations: The paper lacks important references in humor understanding using LLMs and alignment of subjective humor preferences. Notable omissions include work by Hessel et al. (2023), Zhang et al. (2024), Zhou et al. (2025), Kazemi et al. (2025), Liang et al. (2025), Binsted et al. (2006), and Apte et al. The claim about CoT improving VLM reasoning also needs citation support.\n- Insufficient human evaluation details: The paper provides no information about human annotators - recruitment methods, number of annotators, compensation, inter-annotator agreement, or specific instructions given. This is critical for reproducibility and validity of human evaluations.\n\n- Unclear evaluation methodology:\n\n- The Human-Likeness Score methodology is unclear. Is it calculated using Gemini-2.5-flash? What's the rationale for using this specific model as a judge? Has this model been validated as correlating with human preferences?\n- Figure 4b claims the ranking \"aligns more closely with human judgment\" but doesn't provide quantitative evidence for this alignment.\n\n\n- Qualitative vs. quantitative analysis: Figure 6's analysis about Qwen preferring object mentions while Keye prefers human-like states appears purely qualitative. Quantitative metrics supporting these observations would strengthen the claims.\nTechnical presentation issues:\n\n- Typo: \"tives. Concurrently, Ramesh et al. Ramesh et al. (2021)\"\n- Typo: \"Template images of each rannking dataset\" (Figure 8)"}, "questions": {"value": "1. **Human evaluation protocol**: Can you provide detailed information about your human annotation setup? How many annotators participated? What was the inter-rater reliability? How were they compensated? What specific instructions were provided?\n\n2. **VLM-as-judge validation**: Why was Gemini-2.5-flash chosen for the Human-Likeness Score? Have you validated that this model's predictions correlate with actual human judgments? The 91.3% score seems surprisingly high.\n\n3. **Group-wise ranking validation**: How exactly does Figure 4b demonstrate that group-wise ranking aligns better with human judgment? Can you provide correlation coefficients or other quantitative metrics?\n\n4. **Base model analysis**: For the qualitative observations in Figure 6, can you provide quantitative analysis showing the frequency of object-mention preferences vs. state-description preferences across a larger sample?\n\n5. **Generalization**: How well does HUMOR generalize to meme templates not seen during training? Have you tested on completely novel meme formats?\n\n6. **Computational cost**: What is the computational overhead of the multi-path CoT generation compared to direct generation approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yctOq2lAY8", "forum": "nwfEDieg3h", "replyto": "nwfEDieg3h", "signatures": ["ICLR.cc/2026/Conference/Submission20036/Reviewer_VisL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20036/Reviewer_VisL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955225438, "cdate": 1761955225438, "tmdate": 1762932933629, "mdate": 1762932933629, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes a new framework for generating memes, separating the reasoning from \"realization\" (interesting choice of words there). The HUMOR framework introduces a hierarchical multi-path CoT base approach to first infer a meme template's approach, ground it in a scene and finally anchor to one (CoT) path using the ground-truth caption.\n\nTo do this, they collected a large meme dataset, in-painted it to remove the text, used OCR to extract the text.  They also collected preference data within groups - (groups consist of the same template/punchline schema/topic) from human annotators, learn a pairwise reward model, and aggregating pairwise preferences (using EBC, should account for missing pairs I believe), and then finetunes a meme-generator using group-wise RL, trying to prioritize the higher ranked captions, while trying not to diverge too far from the reference policy. \n\nThe experiments and results show better human ranking of the finetuned model outputs over baselines. They also justify the need for group-wise evaluation by showing that using a VLM for evaluation directly is not reliable."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Really good framing of the problem - a hard problem to tackle (subjective, multimodal) but a good formulation. Breaking out the chain of thought to first reason about the template and then about the grounding / text is a great way to approach it, similar to how a human would go about making a meme. I like that the authors headed off questions about necessity of using group-wise ranking by showing that absolute VLM scoring is not as aligned with human ranking as group-wise.\nOverall - I think taking on this open ended, subjective problem statement and showing a reasonably strong mathematical framework for measuring humor + improving on existing approaches makes this paper a strong candidate."}, "weaknesses": {"value": "There aren't as many weaknesses that I caught in this paper. Theoretically, it makes sense that using GRPO would limit the deviation from the supervised policy, but I wonder how much it ensures the improvement in the generated outputs. \n\nOne thing I would note is that a lot of the structures used to improve alignment to human preferences are engineered for this particular scenario of maximizing humor in the generated image. It would be great to show how this could be generalized for other use cases - for example, if we were to use this framework to generate better posters for presentations, we would need different types of pairwise data. \n\nA framework to quantify *how* the types of pairwise data were chosen would be really helpful here - but I also recognize that this comment might be a bit of a nitpick.\n\nThe lack of details about the human annotator demographics is slightly concerning - since the results are mostly empirical, that determines if there is actual improvement in humor, or some sort of overfitting happening."}, "questions": {"value": "I appreciate the amount of experiments run, so I am loathe to suggest more ablations. Several of the metrics reported rely on a single language model as a judge - (gemini/keye). Have you considered evaluating with multiple judges? Or are these the judges that did the best/most consistent?\n\nWhy are we using bge-base-zh for text similarity? Is a large amount of the corpus in Chinese? \n\nOn data collection: 80,000 memes were collected but table 3 in the appendix shows only 3,713? Was this some sort of filtered subset?\n\nOn CoT anchoring: how do you select the anchor path in Stage 2? Are results sensitive to the anchoring?\n\nHow do you account for offensive memes? \n\nHumor is really subjective - so a lot of the human alignment results the paper demonstrates hinges on the annotator demographics. Do we have a varied group of annotators? \n\nAnother thing that your formulation guarantees is that we don't get significantly worse than the supervised policy (by virtue of the KL divergence constraint on the objective). But this does not really show that the performance as judged by humans is getting better, which hinges on having a good human annotator group. Knowing number of raters, how they were asked to rate the images, how strong human-human agreement was would help ground the empirical results better."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No real ethics concerns, but the results may be biased based on the human annotator demographics."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "olYxY7EryU", "forum": "nwfEDieg3h", "replyto": "nwfEDieg3h", "signatures": ["ICLR.cc/2026/Conference/Submission20036/Reviewer_iDgE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20036/Reviewer_iDgE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144987491, "cdate": 1762144987491, "tmdate": 1762932932975, "mdate": 1762932932975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}