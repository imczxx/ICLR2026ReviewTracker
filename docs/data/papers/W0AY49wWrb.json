{"id": "W0AY49wWrb", "number": 9943, "cdate": 1758151277526, "mdate": 1763723885791, "content": {"title": "Learning Primitive Embodied World Models: Towards Scalable Robotic Learning", "abstract": "While video-generation-based embodied world models have gained increasing\nattention, their reliance on large-scale embodied interaction data remains a key\nbottleneck. The scarcity, difficulty of collection, and high dimensionality of em-\nbodied data fundamentally limit the alignment granularity between language and\nactions and exacerbate the challenge of long-horizon video generation—hindering\ngenerative models from achieving a \"GPT moment\" in the embodied domain. There\nis a naive observation: the diversity of embodied data far exceeds the relatively\nsmall space of possible primitive motions. Based on this insight, we propose a novel\nparadigm for world modeling–Primitive Embodied World Models (PEWM). By\nrestricting video generation to fixed short horizons, our approach 1) enables fine-\ngrained alignment between linguistic concepts and visual representations of robotic\nactions, 2) reduces learning complexity, 3) improves data efficiency in embodied\ndata collection, and 4) decreases inference latency. By equipping with a modular\nVision-Language Model (VLM) planner and a Start-Goal heatmap Guidance mech-\nanism (SGG), PEWM further enables flexible closed-loop control and supports\ncompositional generalization of primitive-level policies over extended, complex\ntasks. Our framework leverages the spatiotemporal vision priors in video mod-\nels and the semantic awareness of VLMs to bridge the gap between fine-grained\nphysical interaction and high-level reasoning, paving the way toward scalable,\ninterpretable, and general-purpose embodied intelligence.", "tldr": "PEWM tackles embodied AI’s data bottleneck by generating short clips of primitive actions—enabling precise language-action alignment, better efficiency, and compositional control via VLM + goal guidance. A step toward “scalable robotic learning.”", "keywords": ["World Models", "Video Diffusion Models", "Zero-Shot Policy Learning", "Embodied Intelligence"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8cff1da32888c090a59181649f828357b8cc72ab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "It is very hard to summarize what this paper actually doing. The authors first introduce lots of definations to claim the importance of atomic action unit. Then introduces data collection that (may) follows the previous statement. At the core of technical side is a open-sourced video diffusion model (i.e., DynamiCrafter), and training Self Forcing to distillate the open-sourced diffusion model. If I understand correctly, the key technical contribution may training current approaches on their own data. But I don't regard spliting embodied tasks into sub-tasks as a innovation, which is widely adopted in complex embodied tasks. Another problem is most of key approaches & results are demonstrated in appendix. The main paper should be self-contain, not just describe the concept."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* As far as I can tell, I can't summarize the strength of this work. Maybe data collection, but it seems will not be open-sourced.\n\n* I would strongly encourage the authors to refine this work with an/multiple experienced researchers."}, "weaknesses": {"value": "* The so-called primitive data is not something new. I think most of the researchers have the consensus of spliting complex tasks into sub one or atomic action unit. It is unnecessary to introduce lots of definition for a well-known insights. Also this introduced defination is not highly-related to the following technical contributions. Therefore, Sec 2.1 can be totally removed. It is enough to say the data is annotated with the atomic action unit. By the way, the defination itself is unprecise, what do you mean of 'cannot be meaningfully decomposed into shorter language-grounded sub-instructions without loss of task-level meaning'? I think there are too much ambiguity.\n\n* Technical contribution is very limited. The paper finetunes pretrained video models, e.g., DynamiCrafter. Then distill it using open-sourced code. I don't find any other technical contribution. Further, no experimental results are provided for the motivation and results of distillation. Does it affect visual quality?\n\n* Experimental results are unreasonable. There is only one result shown in the main paper, but it is totally unclear and unfair. Which data do you use? The proposed one? Then what's the contribution of this work? A new dataset? If the author would like to show the value of the proposed dataset. The comparison should be: a same method on the proposed dataset and existing dataset. If the author would like to claim the proposed techniques, then the comparison should be: different method on the same data. It is unclear which part contribute to the final results.\n\n* Too many typos. For example, line 398, 415. The notation in the method part should be defined first. In addition, some descriptions are not preciseness, e.g., in line 338, what do you mean by 'zero-shot 6-DoF pose estimator'? zero-shot typically means generalizing to *unseen* task."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "scbb7bxCvL", "forum": "W0AY49wWrb", "replyto": "W0AY49wWrb", "signatures": ["ICLR.cc/2026/Conference/Submission9943/Reviewer_P5zD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9943/Reviewer_P5zD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920049937, "cdate": 1761920049937, "tmdate": 1762921392396, "mdate": 1762921392396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Primitive Embodied World Models: learn short-horizon, semantically atomic “primitives” (2s clips) and compose them for long-horizon tasks. Primitives are defined with start–goal heatmap guidance and executed by a video world model, a VLM planner sequences primitives for closed-loop control. \n\nAlso, the authors build a multi-view, teleop-segmented primitive dataset, with an automated language-annotation pipeline. A three-stage sim-to-real fine-tuning adapts a pretrained video model, followed by causal distillation to reach real-time 12 FPS inference. \n\nApplications include direct 6-DoF trajectory extraction from generated videos, compositional long-horizon control, and data synthesis. On RLBench, the method achieves higher success rates than previous baselines including Image-BC, UniPi, and 4DWM."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper has a clear and principled reframing for embodied intelligence. It defines primitives as semantically atomic short clips with temporal locality and generative feasibility, offering a tractable interface between language and low-level control.\n\n- The multi-camera capture plus teleop boundary tagging and an LMM-assisted labeling pipeline increase data coverage and collection efficiency.\n\n- The proposed system has higher RLBench success across nine tasks compared to Image-BC, UniPi and 4DWM."}, "weaknesses": {"value": "- The number of primitives is controlled via an RDP threshold (binary search), but there’s no clear sensitivity analysis for density vs. performance/latency trade-offs.\n\n- The approach focuses scenes where observations are captured by a fixed camera, limiting applicability to moving cameras."}, "questions": {"value": "- How would the pipeline adapt to moving cameras?\n\n- Could the authors provide closed-loop cycle latency (planning + generation + pose + actuation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OaMKnhSHmW", "forum": "W0AY49wWrb", "replyto": "W0AY49wWrb", "signatures": ["ICLR.cc/2026/Conference/Submission9943/Reviewer_Nogp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9943/Reviewer_Nogp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980021120, "cdate": 1761980021120, "tmdate": 1762921392068, "mdate": 1762921392068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper advocates using primitive-level, short-horizon video generation as the core unit of embodied world models, rather than pursuing long-horizon prediction. Accordingly, PEWM constrains video generation to fixed short windows and decomposes tasks into primitive trajectories that are semantically atomic, temporally local, and generatively feasible, each paired with a single instruction. PEWM comprises three components: a primitive-level video world model that enables extraction of precise 6-DoF trajectories including end-effector poses and gripper actions from generated videos; a modular VLM planner that decomposes high-level instructions into sequences of primitives to support compositional generalization and flexible closed-loop control; and Start-Goal heatmap Guidance, which provides spatial guidance to improve the controllability and executability of generation. The paradigm is validated in both simulation and real-robot experiments, demonstrating generalization to novel instructions, robustness to domain shifts, and efficient adaptation with limited task-specific data, highlighting its potential toward scalable, interpretable, and general-purpose embodied intelligence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: Positions primitive-level, semantically atomic short-horizon video generation as the core unit of world models, explicitly advocating a paradigm shift of “alignment before scaling.” Integrates a VLM planner and SGG to form a cortex–cerebellum-style hierarchical closed-loop framework.\nQuality: If the experiments are as described, covering both simulation and real robots, domain-shift robustness, and data-efficiency scans, the evidence systematically supports the claims. Short-horizon generation naturally reduces complexity and latency, aligning with practical control needs.\nClarity: The paper clearly motivates the problem, defines primitives along three operational dimensions, and uses figures to elucidate compositional generalization and the execution pipeline.\nSignificance: Charts a scalable path for video-generation-based world models, helping address embodied data sparsity and long-horizon prediction challenges, while improving interpretability and executability."}, "weaknesses": {"value": "The semantic atomicity of primitives lacks an objective, cross-task criterion; sensitivity to primitive duration thresholds and category granularity has not been systematically quantified. Their impact on performance, latency, and compositionality needs verification via matrix-style sweeps and failure case analysis.\nMechanism ablations and controlled comparisons for SGG, the VLM planner, and the short-horizon length are insufficient; a systematic comparison between direct action-space prediction and the “video-first then trajectory extraction” pathway is missing to assess differences in performance, latency, and stability.\nData collection and training cost curves are not sufficiently quantified, including metrics such as sample size versus performance, GPU hours versus performance, memory footprint, and inference latency; the claim of improved data efficiency lacks a direct sample-complexity comparison against end-to-end long-horizon methods."}, "questions": {"value": "Under the same data constraints, how does PEWM compare with end-to-end methods?\nHow are the start and goal heatmaps generated and integrated into the diffusion process, and what is the computational overhead?\nDoes the method degrade on long-horizon tasks?\nWhat is the impact of different VLMs, including open-source versus closed-source and different parameter scales, on decomposition quality and overall performance? How are SGG heatmaps generated, at which layer do they fuse with the video model, and have learning-based or adaptive guidance approaches been attempted? What are the effects of removing SGG or replacing it with point or box constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CWRL6zEogK", "forum": "W0AY49wWrb", "replyto": "W0AY49wWrb", "signatures": ["ICLR.cc/2026/Conference/Submission9943/Reviewer_vf2c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9943/Reviewer_vf2c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997132724, "cdate": 1761997132724, "tmdate": 1762921391705, "mdate": 1762921391705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to tackle the challenge that long-horizon, video-generation-based world models are data-hungry, poorly aligned with language, and slow for closed-loop robotic control.  It proposes PEWM: a primitive-centered paradigm combining a short-horizon video diffusion world model (DynamiCrafter‑based) conditioned by Start‑Goal heatmaps (SGG), a VLM planner (Qwen2.5‑VL with LoRA Planner/Grounder plus Reasoner and Verifier) to decompose instructions into primitives, causal distillation / Self‑Forcing for low‑latency autoregressive rollout, and Gen6D-based 6‑DoF pose extraction for execution.  Empirically PEWM outperforms baselines on RLBench tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) Primitive‑centric short‑horizon decomposition with Start‑Goal heatmaps: reframes long‑horizon control as composition of short, language‑grounded primitives (a pragmatic formulation rather than a radical departure). Presentation is clear.\n\n2)  Integrating a VLM planner (Qwen2.5‑VL + LoRA), a short‑horizon video world model, and Gen6D 6‑DoF extraction into an end‑to‑end pipeline. Experiments and ablations on RLBench and real robots demonstrate robust performance, and low VRAM use, indicating sound engineering."}, "weaknesses": {"value": "1) Weak novelty claim and attribution. The paper’s high‑level idea—using video generation to synthesize VLA data and leverage VLMs for planning/grounding—has prior art (e.g., NVIDIA GR00T, OpenVLA and related VLA/system papers referenced in the manuscript). The manuscript emphasizes DynamiCrafter’s lower VRAM but that engineering efficiency alone is not the central scientific contribution; as written it risks overstating novelty when many system components are re‑combinations of existing techniques.  \n   Suggestion: be explicit about what is new (e.g., the specific primitive formulation + SGG pipeline vs. prior system syntheses) and include baseline comparisons showing that the claimed benefits are not just due to picking a smaller diffusion backbone.\n\n2) Unclear evidence for “fine‑grained alignment” between language and action. The claim that primitive short‑horizon rollouts enable fine‑grained language→action alignment is central, but the paper lacks targeted ablations or quantitative metrics that isolate this effect (e.g., does SGG vs. no SGG produce measurably better instruction grounding or fewer semantic errors?).  \n   Suggestion: add controlled experiments that (a) remove or randomize the language conditioning while keeping SGG, (b) ablate SGG, and (c) report alignment metrics (instruction→primitive classification accuracy, semantic error types, or retrieval‑based grounding scores).\n\n3) Incomplete evidence for primitive execution and compositional zero‑shot generalization. The manuscript claims strong primitive execution and zero‑shot composition, but does not convincingly demonstrate dramatic, human‑level compositional generalization (the “astronaut riding a horse” analogue). Reported examples and task tables are promising but fall short of broad, qualitative zero‑shot compositions that would convince readers the primitives truly recombine in highly novel ways.  \n   Suggestion: include concrete zero‑shot composition experiments with truly out‑of‑distribution pairings (visual examples and success statistics), and show qualitative failure modes; quantify how many novel compositions succeed vs. fail and why.\n\n4) Insufficient justification of the three‑stage training recipe. The paper devotes a lot of space to the three‑stage sim→mix→reality fine‑tuning schedule but does not provide ablations that justify each stage’s necessity, hyperparameter choices, or data ratios. It is therefore unclear whether the gains come from the staged design, the particular data mix, or simply from more training.  \n   Suggestion: run ablations that compare (i) one‑stage training on mixed data, (ii) two‑stage variants (sim→real and sim→mix), and (iii) varying sim:real ratios; report metrics for generation fidelity, pose extraction accuracy, and downstream task success to demonstrate the schedule’s benefit."}, "questions": {"value": "1. Could you clarify which parts of PEWM you consider the primary novel contributions versus engineering/implementation choices (e.g., use of DynamiCrafter, Qwen2.5‑VL + LoRA, Gen6D), and, if possible, provide an ablation that isolates the benefit of the \"primitive + SGG\" formulation from the choice of backbone models?\n\n2. For the Start‑Goal heatmap Guidance (SGG), can you provide quantitative ablations showing task success, instruction→primitive grounding accuracy, or semantic error rates with vs. without SGG (and with randomized/noisy heatmaps)?\n\n3. How do you measure the claimed “fine‑grained alignment between language and action”? Please report a concrete metric (e.g., instruction→primitive classification accuracy or grounding IoU) and an ablation that removes language conditioning while keeping SGG.\n\n\n4. Dataset composition and diversity: could you provide per‑split statistics (counts per primitive type, object diversity, views) and an analysis of how many unique primitive templates $K$ you effectively learn before marginal returns diminish?\n\n5. For Gen6D‑based 6‑DoF extraction from generated videos: please report quantitative pose errors (translation RMSE in cm, rotation error in deg) on a held‑out set, and ablate the postprocessing steps (motion masking, outlier removal, temporal smoothing) to show their individual contributions.\n\n6. Regarding sim–real mixing and the three‑stage fine‑tuning: can you provide controlled experiments comparing (a) one‑stage mixed training, (b) two‑stage variants, and (c) your three‑stage schedule, including the exact sim:real ratios and epochs for each stage and metrics that motivated your choices?\n\n7. On causal distillation / Self‑Forcing: what are the teacher and student denoising step counts, what specific distillation losses were used, and how does reducing steps (e.g., from 50 → 4) quantitatively affect generation quality and downstream task success?\n\n\n8 Zero‑shot compositionality: can you show representative qualitative examples equivalent to the \"astronaut riding a horse\" analogue (i.e., highly novel predicate‑object compositions) and provide success rates across a curated set of truly out‑of‑distribution combinations?\n\n9. Failure modes and robustness: what are the primary failure cases observed on real robots (e.g., EE too distant, small motion, occlusion, object slips), how frequently do they occur, and what recovery strategies (replan, corrective primitives, human intervention) are implemented?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dTl2uHC1nq", "forum": "W0AY49wWrb", "replyto": "W0AY49wWrb", "signatures": ["ICLR.cc/2026/Conference/Submission9943/Reviewer_wkrp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9943/Reviewer_wkrp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762775691072, "cdate": 1762775691072, "tmdate": 1762921391434, "mdate": 1762921391434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}