{"id": "17DNmdQ9aU", "number": 15622, "cdate": 1758253272955, "mdate": 1759897294438, "content": {"title": "StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs", "abstract": "Prevalent semantic speech tokenizers, designed to capture linguistic content, are surprisingly fragile. We find they are not robust to meaning-irrelevant acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech is perfectly intelligible, their output token sequences can change drastically, increasing the learning burden for downstream LLMs. This instability stems from two flaws: a brittle single-path quantization architecture and a distant training signal indifferent to intermediate token stability. To address this, we introduce StableToken, a tokenizer that achieves stability through a consensus-driven mechanism. Its multi-branch architecture processes audio in parallel, and these representations are merged via a powerful bit-wise voting mechanism to form a single, stable token sequence. StableToken sets a new state-of-the-art in token stability, drastically reducing Unit Edit Distance (UED) under diverse noise conditions. This foundational stability translates directly to downstream benefits, significantly improving the robustness of SpeechLLMs on a variety of tasks.", "tldr": "This paper introduces StableToken, a novel speech tokenizer architecture with superior noise robustness and excellent performance.", "keywords": ["speech tokenizer", "noise robustness", "audio", "multi-modality", "speech language modeling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/31e4985eb55406bbdd9a31fa3b94f634282e7f82.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a new Tokenizer (StableToken) to encode speech while being less affected by noises. The proposed architecture is based on a majority voting system with a Lookup-Free-Quantizer, training with perturbed and clean views so the model learns to predict the same tokens with noisy and clean signal.\n\nThe model is compared to a range of SOTA tokenizers, which it outperforms on all evaluated tasks.\nThe model's robustness is measured on automatic speech recognition and speech emotion recognition tasks for a range of noises and SNRs, and show a reduced impact from the raising SNRs compared to other SOTA approaches."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper shows a clear structure and detailed experiments to prove the main contributions: the production of a SOTA tokenizer (see Table 1) with good performances on reconstruction quality (see Table 2) and downstream tasks under noise (see Table 3/Figure 3).\n\nThe ablation study comes to complete the full justification of the architectural choices, while the voter count analysis and the case study on error correction clarify further the intuitions behind the work."}, "weaknesses": {"value": "The main weakness of the article is its framing. The claim is to produce a robust tokenizer for speech LLMs, where the only experiments performed for speech LLMs are for the downstream tasks of SER and ASR. \nThose tasks could have been performed with a range of different models, so no experiment really highlights why this tokenizer is specifically fit for speech LLMs."}, "questions": {"value": "Did you try to apply this tokenizer on a larger set of downstream tasks related more closely to the semantic aspects of the speech (like machine translation, summarization or question answering) or to phonetic and accoustic aspects of the speech (like speaker characterization or accent identification) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CbMePgKiog", "forum": "17DNmdQ9aU", "replyto": "17DNmdQ9aU", "signatures": ["ICLR.cc/2026/Conference/Submission15622/Reviewer_Khaj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15622/Reviewer_Khaj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659877906, "cdate": 1761659877906, "tmdate": 1762925888216, "mdate": 1762925888216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces StableToken, a noise-robust discrete speech tokenizer designed to improve stability and representation consistency under diverse acoustic conditions. The method incorporates Noise-Adaptive Token Alignment (NATA) and Multi-Granular Contextual Quantization (MGCQ) to achieve invariant discrete token representations, enabling better downstream generalization for speech generation and understanding models. Comprehensive evaluations are conducted across noisy and clean conditions, demonstrating improved token stability, representation purity, and semantic retention compared to previous models such as HuBERT, SpeechTokenizer, and AudioTokenizer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The combination of NATA and MGCQ introduces a fine-grained, adaptive quantization strategy that balances stability and expressiveness. The paper effectively frames quantizer instability as a consistency problem between clean and noisy latent spaces.\n- The evaluation was conducted under diverse noise conditions (additive noise, reverb, SNR degradation), including both analysis level (token consistency, MI) and task-level (ASR, LM perplexity, reconstruction) metrics. The improvements are demonstrated in real speech-LM setups, not fixed probing or static benchmarks."}, "weaknesses": {"value": "- The token stability is part of the major claim but insufficiently formalized. The paper describes it as “the percentage of tokens invariant across clean–noisy pairs,” but does not clarify the matching threshold or how it scales with temporal misalignment.\n- The model’s success may partially stem from extensive noise augmentation rather than architectural novelty. There is insufficient ablation isolating the contribution of NATA and MGCQ from data-scale effects.\n- Although StableToken integrates seamlessly, the multi-quantizer architecture and contextual alignment loss may introduce overhead.\n- Experiments focus heavily on additive noise and mild reverberation, but real-world degradation often involves non-linear distortions (e.g., clipping, compression, far-field effects). The robustness of the method in unseen noise cases would be a good extension elaboration of the robustness of the method."}, "questions": {"value": "- Can the authors quantify how much granularity adaptation (e.g., variable codebook resolution) contributes to stability gains vs. standard augmentation?\n- Is the granularity selection deterministic or learned during inference?\n- How is token correspondence established across variable frame rates or time shifts?\n- Could the method generalize to music signals where spectral structures differ substantially?\n- How does StableToken perform under limited training data or unseen noise conditions?\n- Can the model generalize to domains not covered by augmentations?\n- What is the runtime cost compared to standard quantizers like FSQ or RVQ?\n- How scalable is MGCQ to high frame rates or large codebooks? Is there any observed trade-off between speed and robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UDYscsIlWa", "forum": "17DNmdQ9aU", "replyto": "17DNmdQ9aU", "signatures": ["ICLR.cc/2026/Conference/Submission15622/Reviewer_p2V9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15622/Reviewer_p2V9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723218993, "cdate": 1761723218993, "tmdate": 1762925887708, "mdate": 1762925887708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets the brittleness of semantic speech tokenizers under meaning-irrelevant acoustic noise, arguing that instability in discrete token sequences burdens downstream SpeechLLMs. It proposes StableToken, which replaces single-path quantization with a multi-branch Voting-LFQ module that aggregates per-branch binary codes via bit-wise majority voting, coupled with Noise-Aware Consensus Training that aligns clean/noisy branch representations using a consensus loss. StableToken reduces token instability, while matching or improving reconstruction (WER/MOS) and translating to better downstream ASR/SER/TTS robustness. Ablations confirm each component’s contribution."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. A simple but elegant architectural redundancy (multi-branch) with bit-level voting, paired with a clean consensus loss that sidesteps discrete-code gradient issues.\n\n2. Strong tokenizer-level robustness (UED 10.17%), maintained reconstruction (WER/MOS), and clear downstream benefits on ASR/SER/TTS, including OOD noise. Solid ablations and voter-count analysis.\n\n3. Architecture/training described crisply. The evaluation protocol is broad (synthetic and real noise, multiple tasks).\n\n4. Stability is a real deployment pain point. Showing robustness gains that carry to ASR/SER/TTS is valuable for the community."}, "weaknesses": {"value": "# 1. Latency/RTF/throughput and memory footprint\nThe paper claims negligible overhead, but lacks wall-clock numbers (GPU/CPU), batch size effects, and memory usage. This matters given multiple branches and a large backbone.\n\n# 2. Streaming/segment length\nThe encoder is initialized from Whisper Large-v3 (commonly used with 30s segments). The paper doesn’t clarify the maximum context during training/inference, the chunking strategy, or whether the tokenizer itself inherits a 30s limit.\n\n# 3. Fair comparison controls\nRobustness comparisons span models with different frame rates and codebook sizes (e.g., 12.5/25/50 Hz & vocab 4k–16k). Please provide a control where bitrate/frame rate are aligned across tokenizers or report robustness at matched bits-per-second to rule out confounds. Or perhaps using some normalization methods to make the UEDs comparable.\n\n# 4. Ablations\nYou analyze voter count. I would also suggest evaluating the effect of varying the proportion of perturbed branches ($k$), and the placement of the quantizer within the encoder."}, "questions": {"value": "1. Latency/RTF and throughput. Please report RTF, tokens/sec, and memory usage for StableToken vs. the strongest baselines on a common GPU and on CPU. Include per-branch FLOPs and end-to-end wall-clock with batch size effects.\n2. Segment length & streaming. What is the maximum audio duration processed without truncation? Was training/inference chunked to 30s due to Whisper Large-v3 initialization? How does overlapping/chunk stitching affect stability?\n3. Training audio window. Was the tokenizer trained strictly with $\\leq$ 30s segments, or longer via chunking? Does stability degrade across chunk boundaries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fsjod7ON7k", "forum": "17DNmdQ9aU", "replyto": "17DNmdQ9aU", "signatures": ["ICLR.cc/2026/Conference/Submission15622/Reviewer_Shzy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15622/Reviewer_Shzy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771895462, "cdate": 1761771895462, "tmdate": 1762925887267, "mdate": 1762925887267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes StableToken, a noise-robust semantic speech tokenizer aimed at improving the stability of SpeechLLMs under noisy conditions. The design integrates a multi-branch quantizer with bit-wise voting to mitigate token assignment instability, and a Noise-Aware Consensus Training (NACT) objective to enforce consistency between clean and noisy inputs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is novel in combining multi-branch quantization with voting and noise-consensus training.\n2. StableToken outperforms state-of-the-art baselines across tokenizer-level and downstream tasks in noisy conditions.\n3. Comprehensive analysis includes ablations on branch count, noise levels, and downstream performance."}, "weaknesses": {"value": "1. The method emphasizes semantic token consistency but does not address whether acoustic nuances (e.g., prosody, speaker traits) are preserved. This may limit usefulness in tasks like TTS or voice cloning, where fine-grained acoustic details are crucial.\n2. Most experiments rely on additive or SNR-controlled noise (e.g., 0 dB, 5 dB). Real-world noise conditions such as overlapping speech, environmental interference, or channel distortion are underexplored, leaving practical robustness uncertain.\n3. Multi-branch quantization may generate redundant tokens. While voting improves stability, it could reduce the information density of tokens and thus impact efficiency in downstream models.\n4. The paper does not study how multi-branch tokenization affects token distribution, entropy, or scaling laws. Shifts in token distribution could potentially influence pretraining dynamics in large-scale SpeechLLMs."}, "questions": {"value": "1. Do StableTokens still preserve paralinguistic cues such as speaker identity, prosody, and emotion, or does the stability objective reduce these attributes that are important for tasks like TTS or emotion recognition?\n2. Could the multi-branch voting mechanism introduce inefficiencies or biases in multilingual settings, where phonetic distributions differ widely across languages?\n3. Beyond consensus training, could robustness be further improved by combining StableToken with joint pretraining that explicitly incorporates noise augmentation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wFBdREYBfF", "forum": "17DNmdQ9aU", "replyto": "17DNmdQ9aU", "signatures": ["ICLR.cc/2026/Conference/Submission15622/Reviewer_vfbD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15622/Reviewer_vfbD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792907457, "cdate": 1761792907457, "tmdate": 1762925886859, "mdate": 1762925886859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}