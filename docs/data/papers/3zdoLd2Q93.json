{"id": "3zdoLd2Q93", "number": 13660, "cdate": 1758220543183, "mdate": 1759897421662, "content": {"title": "RCStat: A Statistical Framework of Relative Contextualization in Transformers", "abstract": "Estimating the importance of input tokens and their activations in auto-regressive models is a fundamental requirement in many applications, such as key-value (KV) cache compression and attribution. Prior work computes token importance using attention weights, which are obtained by normalizing the raw attention logits (query-key inner products) with a softmax operation. However, the softmax normalization suppresses the rich information within the attention logits. We introduce RCStat, a statistical framework that harnesses the raw attention logits via Relative Contextualization (RC) -- a random variable measuring contextual influence from one subset of tokens to another. We derive computationally efficient bounds on the expected RC and demonstrate its utility in two applications:  (i) KV compression, where RC‐based adaptive thresholding evicts substantial portions of the KV cache with minimal quality loss in token generation; and (ii) Attribution, where attention heads with high expected RC yield accurate span‐level attribution. Across QA, summarization, and attribution benchmarks, RCStat achieves state-of-the-art performance, improving generation quality by 15–40\\% and attribution accuracy by 2–16\\%, all without any model retraining.", "tldr": "RCStat leverages raw, pre-Softmax attention logits to quantify token importance at multiple granularities, enabling adaptive KV-cache eviction and high-fidelity attributions, all without any model retraining.", "keywords": ["Large Language Models", "Pre-Softmax Analysis", "Attention Logits", "Relative Contextualization", "KV-Compression", "Attribution"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b9876d6b2b6c2a51d9fd990e69571905b47f6678.pdf", "supplementary_material": "/attachment/6eee29ca38110c853a2ddf3d268fdb3db484eafc.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces RCStat, a statistical framework that leverages pre-softmax attention logits to quantify contextual influence between token groups through a measure called Relative Contextualization (RC). Unlike prior methods relying on post-softmax attention weights—which lose fine-grained relational information due to normalization—RCStat models raw attention logits as random variables and derives an efficient upper bound on their expected difference to estimate contextual relevance. This statistical formalism enables two key applications: adaptive KV-cache compression and token-level attribution, both achieved without retraining or auxiliary supervision."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The work provides a theoretical justification for their kv cache's importance quantification."}, "weaknesses": {"value": "1. The authors claim \"Despite this potential, the usage of pre-softmax attention remains largely underexplored, primarily due to the lack of statistical tools and frameworks to extract structured insights from unnormalized logits.\" I believe this is not not true. Please check out [1]: a kv cache selection paper that quantifies importance of kv based on pre-softmax scores, or [2].\n2. Missing comparison with SOTA baselines such as [1][2].\n3. The ultimate goal of most KV-cache compression techniques is inference efficiency (faster decoding, lower memory footprint) with minimal performance loss. While the proposed work emphasizes improved performance under compression (which may itself be questioned in competitiveness to SOTA), the work provides no experiments on efficiency metrics (latency, GPU memory, throughput) - thus its practical viability remains unclear.\n\n\n[1] Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference, Tang et. al., ICML 2024.\n[2] InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory, Xiao et. al., ArXiv.\n[3] DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads, Xiao et. al., ICLR 2025."}, "questions": {"value": "1. I'm wondering what is the performance of the proposed method under other long-context tasks such as needle in a haystack?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GeiCbVPpqc", "forum": "3zdoLd2Q93", "replyto": "3zdoLd2Q93", "signatures": ["ICLR.cc/2026/Conference/Submission13660/Reviewer_mgpv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13660/Reviewer_mgpv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761522524639, "cdate": 1761522524639, "tmdate": 1762924230060, "mdate": 1762924230060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a method for quantifying the influence of attention logits by measuring the relative contribution of the prompt part of tokens toward the generation part of the tokens. Basic ideas is to treat the raw attention logits as values for a random variable in a probability density function, and defines two random variables defined for the queries during generation: cross-contextualization which represents the logits for the prompt part of keys and self-contextualization which represents the logits for the generation part of keys. The third variable, relative contextualization, is defined to measure the importance of cross-contextualization over self-contextualization motivated by the focus of prompt during generation. The metric is employed for two tasks: KV cache compression by pruning irrelevant keys and chunk-level prompt attribution, demonstrating superior performance over baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- This work presents a novel view of the raw attention logits in order to represent the influence of two parts, i.e., prompt and generation, by introducing random variables, cross-contextualization, self-contextualization and relative-contextualization. Given this view, this work also shows that the upper-bound of expected relative contextualization could be computed by marginization with an efficient algorithm for the computation.\n- Experiments are designed systematically with solid results. The KV cache experiments clearly show better tradeoff of the compression and qualities. Chunk-level accuracy for attribution also shows gains when compared with other methods."}, "weaknesses": {"value": "- It takes time to understand the manuscript, given that several key explanation for Equations CC (9) and SC (10) is presented in Appendix. Probably a little bit more intuitive explanation will alleviate the issue.\n- Similarly, the chunk-level prompt attribution task is not clearly explained, and thus, readers have to look up the prior studies to understand the setting."}, "questions": {"value": "- $c$ appeared in Equation (SC), but it should be $s$ if my understanding is correct."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T7CNNz29uo", "forum": "3zdoLd2Q93", "replyto": "3zdoLd2Q93", "signatures": ["ICLR.cc/2026/Conference/Submission13660/Reviewer_kwdx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13660/Reviewer_kwdx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981871682, "cdate": 1761981871682, "tmdate": 1762924229702, "mdate": 1762924229702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces RCStat, a method for measuring attribution to prior context by using unnormalized attention weights. It proposes both an exact form and a more efficient approximation of this measure. Then, the paper demonstrates the utility of RCStat by using it to develop both a KV cache eviction method and text attribution method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The idea of RCStat is intriguing-- I like the argument that looking at weights post-softmax loses valuable information. To the best of my knowledge, this is a novel way of measuring influence from prior context.\n\nS2. The formulation of RCStat with separation of context into two contrastable segments is also interesting, and the way that this allows for span-level identification of importance is a nice property.\n\nS3. Attribution is a really hard problem, and one that this paper makes an nice contribution towards. I think the analysis of attribution performance growing worse with adding too many irrelevant attention heads is an interesting one."}, "weaknesses": {"value": "W1. The paper presents a lot of insights, which is nice, but because it takes on three different focuses (deriving RCStat, using RCStat to design a competitive-with-SOTA KV cache eviction strategy, and analyzing RCStat's usefulness for attribution), it sometimes struggles to provide sufficient evidence for each. I organize the rest of the critique into these three subparts. \n\nW2. The initial explanation of RCStat was a bit hard to follow. I found the shapes of density functions drawn in 2(d) to be a bit distracting, and something that it would have been helpful to have clarified earlier-- in particular, why do these density functions overlap only slightly? \n\nW3. To make an argument about applying RCStat as a new KV cache eviction method, you really must address latency in the main text; I think the results in the appendix are somewhat promising, but I don't agree with the statement that this is only \"modestly slower\" than methods that are 4-5x faster per layer in the compression step (I understand the argument that, compared to prefill+decode, the overall time difference is negligible, but if that is the case you want to make, you should additionally report some metrics of overall time!). ROUGE-1 is not a reasonable quality metric to use; even an older neural metric would make a stronger case. And it seems this is missing several KV cache methods that are prominent in the community -- what about H20 or Pyramid KV? Why is this particular set of baselines the most reasonable choice? \n\nW4. The attribution section is interesting, but I think could benefit from some presentation improvements. Can you report standard deviations and/or significance test the differences between scores in Table 1? L3.1-8B's HS baseline is in a different section from the other L3.1-3B results, but for the Llama 8B and Qwen numbers this is in the same grouping. The pre/post comparison for Llama 8B might be better positioned in a separate table. Are the \"least RC\" numbers listed for the other models the pre-softmax ones? I was also wishing for a bit more exploration of which heads were good predictors of attribution-- the relevance of depth is mentioned a few times (and discussed in other work), but it would be interesting to hear more about how this works for RCStat's measure of head relevance in particular."}, "questions": {"value": "Q1. Do you have any intuition for why VeriGran performance drops so much more dramatically than QuoteSum performance with increasing number of heads for Qwen in Fig 8b? \n\nQ2. My main critique is that the paper, in trying to do a lot of different things, is not quite doing all of them to a sufficient level. In your conceptualization of this work, do you see the theoretical framing of RCStat itself as the main contribution (with RCStat-based KV cache eviction + RCStat-based attribution being two example applications), or would you place all three on equal footing? \n\nQ3. Like most attention-based methods, this requires explicitly instantiating the attention matrix in memory, instead of calling an efficient kernelized implementation, right? Can you discuss the VRAM requirements imposed by this? \n\nQ4. various specific questions/feedback on the sections, as detailed in the weaknesses above. \n\nSmall typos:\n- line 463: generalizability misspelled\n- line 100: this citation for the phrase mechanistic interpretability seems strange to me -- not clear why this must be cited or that this is the right thing to cite? I'm not as familiar with this literature though so if there is a real reason for this please disregard.\n- line 112: \"up to 84%\" is what that work observed, but this phrasing makes it seem like this is a hard-and-fast maximum, which of course is not true\n- line 419: \"the an\"; more generally I don't really understand what this sentence is trying to say"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QgreKlTgVd", "forum": "3zdoLd2Q93", "replyto": "3zdoLd2Q93", "signatures": ["ICLR.cc/2026/Conference/Submission13660/Reviewer_kCPH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13660/Reviewer_kCPH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146469167, "cdate": 1762146469167, "tmdate": 1762924229287, "mdate": 1762924229287, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The attention module in Transformer introduces structural bias by sharpening attention toward dominant tokens while flattening the others. It may discard potentially meaningful contextual signals. Corresponding to this phenomenon, the authors posit that logits encode not only what the current layer attends to but also preserve upstream interactions, offering a richer statistical substrate for analysis. Based on this behavior, the authors quantify how different attention heads behave in KV-cache compression and token attribution. By using a newly proposed method, relative contextualization (RC), the authors investigate that most heads have compressible KV-caches, while the few resistant ones provide useful attribution signals. The experimental results on large language models (LLMs) show that the proposed RC-based framework, RCSTAT, can support the improvement of prompt attribution and KV cache reduction in summarization and QA tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The targeting issue of the information loss in attention is fundamental for Transformer-based models, including large language models (LLMs).\n- The assumption of the relative contextualization (RC) is based on the actually observed cases.\n- The authors provide the computational complexity of the expected RC.\n- The experimental results show the effectiveness of RC-based KV cache reduction and prompt attribution on various tasks.\n- The authors compared their RC-based method with strong baselines, including state-of-the-art methods."}, "weaknesses": {"value": "- The paper is not self-contained because reading the main text part requires accessing content in appendices like Equations (9) and (10). This is a presentation issue.\n- The used models are restricted to small language models that have less than 10B of parameters.\n- Runtime comparison is not reported."}, "questions": {"value": "In the KV cache reduction, norms of value vectors are important as well as attention weights shown in the following papers. Could you explain the potential of combining your approach with such methods?\n- Alessio Devoto, Yu Zhao, Simone Scardapane, and Pasquale Minervini. 2024. A Simple and Effective L_2 Norm-Based Strategy for KV Cache Compression. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 18476–18499, Miami, Florida, USA. Association for Computational Linguistics.\n- Zhiyu Guo, Hidetaka Kamigaito, and Taro Watanabe. 2024. Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 21158–21166, Miami, Florida, USA. Association for Computational Linguistics."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K4wSBDldZa", "forum": "3zdoLd2Q93", "replyto": "3zdoLd2Q93", "signatures": ["ICLR.cc/2026/Conference/Submission13660/Reviewer_fdAX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13660/Reviewer_fdAX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762172763709, "cdate": 1762172763709, "tmdate": 1762924228876, "mdate": 1762924228876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}