{"id": "LgZg5D4EHt", "number": 23588, "cdate": 1758346020757, "mdate": 1759896806268, "content": {"title": "Adaptive High-Dimensional Subspace Evolution Based on Broad Learning System and Error-Correcting Output Codes", "abstract": "High-dimensional data (HDD) commonly exhibit complex hierarchical structural characteristics; however, existing approaches typically employ fixed subspace evolution strategies that fail to adapt to the inherent hierarchical diversity across different datasets, resulting in suboptimal revelation of underlying discriminative patterns. Considering this critical limitation, we propose an adaptive high-dimensional subspace evolution algorithm (AHSE) featuring a dual-branch collaborative architecture: the series branch leverages Cholesky decomposition-based incremental Broad Learning System (BLS) to efficiently evolve cascaded subspaces tailored to distinct types of high-dimensional hierarchies; the parallel branch, built on multiple subspace evolution bases, utilizes post-hoc error-correcting output codes (ECOCs) for robust spatial encoding and evolutionary optimization. Both branches converge into a lightweight circuit, forming a closed evolutionary loop. Owing to the hierarchy-tailored evolution strategy, AHSE excels in various HDD tasks such as image pattern recognition, speech emotion recognition, and few-shot learning. Moreover, we offer a rigorous theoretical analysis of the mechanism and robustness guarantee of ECOCs on BLS, further promoting the integrity of AHSE.", "tldr": "", "keywords": ["high-dimensional data", "subspace evolution", "broad learning system", "error-correcting output codes"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/219601f79edcb50bef1103043a8fd1acb16fa0e2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Aiming at the complex hierarchical structure of high-dimensional data (HDD) and the core problem that the existing methods can't adapt to the diversity of data hierarchy by using fixed subspace evolution strategy, this paper proposes an adaptive high-dimensional subspace evolution algorithm (AHSE). AHSE adopts a double-branch collaborative architecture: the serial branch is an incremental breadth learning system (BLS) based on Cholesky decomposition, which customizes the evolution path of cascaded subspaces for high-dimensional data of different hierarchical types through feature priority sorting, subspace alignment and sample weight evolution; Parallel branch (PATH) is based on multi-subspace evolution basis, combined with post-error correction output code (ECOCs) to realize robust spatial coding, and the target space is purified by Flame optimization mechanism. The two branches are dynamically fused by lightweight SPOT circuit to form a closed-loop evolutionary system. This paper provides a rigorous theoretical analysis and robustness proof of ECOCs on BLS. Experiments verify the superiority of AHSE in many high-dimensional tasks such as image pattern recognition, speech emotion recognition, and small sample learning, giving consideration to performance and computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Adaptive subspace evolution with hierarchical adaptation is proposed, which breaks through the limitation of traditional fixed evolution strategy, and designs differentiated evolution modes for different structural data such as MNIST and Fashion-MNIST to accurately match the internal characteristics of the data.\n2. SEED branch inherits the lightweight advantage of BLS and realizes efficient subspace iteration through incremental Cholesky decomposition. PATH branch uses the error correction characteristics of ECOCs to improve the anti-noise ability, Flame mechanism effectively solves the compatibility problem between ECOC codebook and BLS latent space, and the two branches cooperate to achieve the balance between efficiency and robustness.\n3. SOTA or Top-2 performance is achieved in images (MNIST to TinyImageNet), speech emotion recognition (5 benchmark data sets) and small sample learning (5 high-dimensional small sample data sets), and the training time and reasoning FLOPs are significantly lower than those of ResNet and ViT models, giving consideration to generalization and deployment feasibility."}, "weaknesses": {"value": "1. The evolution modes (exponential, cosine, linear) of different data sets need to be preset manually, and there is no mechanism to automatically identify data hierarchy types and dynamically select evolution strategies, which limits the applicability of the method to high-dimensional data with unknown structure.\n2. The performance of extremely high-dimensional small sample scenes whose feature dimensions far exceed the number of samples (such as ten thousand-dimensional features and thousands of samples) has not been tested, which only verifies the robustness under weighted noise, and does not explore the influence of input data noise (such as Gaussian noise and feature missing) on the evolution process, so its practicability is limited.\n3. The length and coding strategy of ECOC codebook depend on manual setting and fixed pool selection, and lack of adaptive optimization.\n4. Compared with the mainstream high-dimensional data processing methods in recent years (such as self-supervised subspace learning and adaptive kernel method)."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rpuLMHXT9I", "forum": "LgZg5D4EHt", "replyto": "LgZg5D4EHt", "signatures": ["ICLR.cc/2026/Conference/Submission23588/Reviewer_BJWZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23588/Reviewer_BJWZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551925788, "cdate": 1761551925788, "tmdate": 1762942723081, "mdate": 1762942723081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AHSE (Adaptive High-Dimensional Subspace Evolution), an algorithm that aims to adaptively evolve feature subspaces in high-dimensional data. The model integrates three components: SEED: a series subspace evolution mechanism using Cholesky-decomposition-based incremental Broad Learning Systems (BLS); PATH: a parallel subspace evolution using Error-Correcting Output Codes (ECOCs); SPOT: a circuit that fuses SEED and PATH outputs dynamically. The paper claims that this dual-branch design enables hierarchical adaptation to different data structures, achieving superior results on diverse tasks including image recognition, speech emotion recognition, and few-shot learning.\n\nWhile the paper presents an interesting and ambitious approach to adaptive subspace learning, the contribution lacks theoretical depth, conceptual clarity, and fair experimental validation for acceptance at a top-tier venue. A significantly revised and more focused version - with clear ablations, theoretical integration, and modern baselines - could be competitive in the future."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The attempt to address “adaptive subspace evolution” in high-dimensional data is an underexplored and potentially meaningful problem.\n2. Comprehensive experiments: The paper includes a wide range of datasets and tasks.\n3. The manuscript provides equations, pseudocode references, and hyperparameters, showing effort toward reproducibility."}, "weaknesses": {"value": "1. The core ideas are extremely difficult to follow. Terms such as “evolutionary Cholesky decomposition,” “Flame optimization,” “hierarchy-tailored evolution,” and “closed-loop subspace evolution” are introduced with little intuition or justification. It remains unclear what adaptive subspace evolution concretely means in learning-theoretic or algorithmic terms. The method reads as a collection of loosely related heuristics rather than a coherent learning principle.\nThe presentation is overly verbose and obfuscatory. Many mathematical formulations restate standard operations (e.g., weighted least squares, feature ranking, or error-correcting output codes encoding) in unnecessarily complex notation.\n \n2. Despite heavy terminology, the technical content is incremental:\n- The SEED component largely combines feature selection, incremental regression, and Cholesky updates, which are all well-established.\n- PATH builds on existing ECOC formulations with a “Flame” heuristic that seems ad hoc and lacks justification.\n- SPOT’s fusion mechanism is simply a weighted combination of two outputs using validation loss ratios.\nThere is no fundamental new learning algorithm here — only an engineering combination of BLS, ECOC, and feature-ranking procedures.\n \n3. The paper repeatedly asserts that it provides “rigorous theoretical analysis” and “robustness guarantees” for ECOCs on BLS. However, there are no theorems, proofs, or meaningful derivations in the main text. The “theoretical” results are said to appear in the appendix but are not summarized, contextualized, or experimentally verified. This undermines the scientific validity of the claimed contributions.\n \n4. While many datasets are used, the evaluation does not meet top-tier standards:\n- Unfair comparisons: BLS-based models are trained on pretrained ResNet or MoCo features, while deep baselines train end-to-end, making comparisons misleading.\n- Weak baselines: Many chosen baselines (e.g., MLP, VGG-16, ResNet-34) are outdated. Modern efficient architectures (ConvNeXt, Swin, EfficientNet, ViT-L variants, or transformer hybrids) are absent.\n- No variance reporting: All results are single-run accuracies, with no confidence intervals or standard deviations.\n- Questionable scalability: The method is described as efficient, but the numerous steps (FPFE, SA, SWE, Flame, etc.) appear computationally heavy, and there are no FLOP or runtime analyses beyond summary tables.\nThe presented results cannot be confidently interpreted as evidence of superiority.\n \n5.  The manuscript is bloated, overtechnical, and poorly structured. The authors use jargon-heavy language throughout, making it exhausting to read and nearly impossible to extract the core insight. Figures are decorative rather than explanatory; algorithms are fragmented across appendices; and critical details are buried in complex pseudocode.\nOverall, the paper fails the clarity standard required for top tier-level publication."}, "questions": {"value": "1. Can you explicitly define what is new in AHSE compared to prior BLS or ECOC-based frameworks?\n2. How exactly does “adaptive evolution” differ from conventional feature selection or boosting?\n3. What are the computational requirements? The architecture seems complex; please provide memory and runtime breakdowns.\n4. Is there any theoretical justification (beyond empirical heuristics) for the “Flame” optimization mechanism?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mIcySNNjTo", "forum": "LgZg5D4EHt", "replyto": "LgZg5D4EHt", "signatures": ["ICLR.cc/2026/Conference/Submission23588/Reviewer_Ypaf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23588/Reviewer_Ypaf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946447048, "cdate": 1761946447048, "tmdate": 1762942722742, "mdate": 1762942722742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is a well-written and technically substantial paper proposing AHSE, an adaptive high-dimensional subspace evolution framework that integrates a serial evolution branch (SEED), a parallel ECOC-based branch (PATH), and a SPOT fusion circuit. The paper tackles the long-standing problem of fixed subspace evolution in high-dimensional data and introduces an architecture that dynamically tailors its evolution path based on data hierarchy.\n\nThe methodology is technically solid, with clear mathematical formulation and well-structured theoretical analysis—especially the derivation of ECOC robustness guarantees within the Broad Learning System (BLS) context. The experiments are broad and compelling, covering both small-scale and large-scale image datasets, speech emotion recognition, and few-shot learning. Ablation results (Table 3) and visualization of evolution modes (Figures 1–3) convincingly validate the adaptive design.\n\nDespite its quality, several aspects could be refined. Some comparisons lack strict fairness (e.g., pretrained feature reliance for BLS-based models), and runtime efficiency would be clearer with standardized hardware benchmarks. The methodology’s generality beyond BLS-based frameworks also warrants further discussion. These are minor but relevant issues in an otherwise strong submission."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "• The dual-branch SEED–PATH design, combined through SPOT, provides a principled mechanism for adaptive subspace evolution that generalizes across hierarchically diverse datasets.\n• Theoretical analysis of ECOCs in BLS is rigorous and novel, with clear proofs and practical robustness implications.\n• Experiments are comprehensive and multi-domain, demonstrating consistent superiority across image, speech, and few-shot settings.\n• Ablation studies are detailed and provide clear evidence of each module’s contribution, particularly FPFE and Flame.\n• The paper is very well written, conceptually coherent, and easy to follow despite its technical depth."}, "weaknesses": {"value": "• Fairness in experimental comparisons could be improved, as AHSE benefits from pretrained features while DNN baselines are trained end-to-end.\n• Computational efficiency claims rely on FLOPs and CPU time rather than uniform wall-clock measurements across hardware setups.\n• Hyperparameter tuning criteria and seed variance are not reported, limiting reproducibility before code release.\n• The adaptive evolution principle is framed specifically for BLS; its applicability to other model families (e.g., deep ensembles) remains untested.\n• The theoretical analysis, while elegant, focuses on bounded-noise scenarios and does not consider stronger adversarial perturbations."}, "questions": {"value": "Can you clarify how feature extraction fairness was ensured in comparisons against deep baselines? For instance, would AHSE’s advantage remain if all methods used the same frozen backbone (e.g., ResNet-34)?\n\nThe evolution schedules (γ, T, α) for different hierarchies are central to your approach. How are these parameters selected in practice—heuristically or via validation search—and how sensitive are results to them?\n\nCould you provide wall-clock runtime comparisons on a shared hardware configuration to substantiate the efficiency claim beyond FLOPs and CPU-only metrics?\n\nHave you evaluated how AHSE behaves under non-Gaussian or structured noise perturbations to validate robustness beyond the assumptions of Theorem 1?\n\nWhile your framework is formulated around BLS, could the adaptive subspace evolution principle extend to deep or transformer-based encoders? If not, what are the main obstacles to doing so?\n\nFinally, can you provide variance or confidence intervals (e.g., standard deviations across runs) for key results in Tables 1 and 2 to better understand reproducibility and statistical significance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SxeFAplXHA", "forum": "LgZg5D4EHt", "replyto": "LgZg5D4EHt", "signatures": ["ICLR.cc/2026/Conference/Submission23588/Reviewer_ibE5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23588/Reviewer_ibE5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762052891751, "cdate": 1762052891751, "tmdate": 1762942722540, "mdate": 1762942722540, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an Adaptive High-dimensional Subspace Evolution algorithm (AHSE) to address the limitation of static subspace evolution strategies in existing methods. AHSE is able to adapt to the inherent hierarchical diversity across different high-dimensional datasets. The architecture of AHSE contains a SEED branch that evolves subspaces using a Cholesky decomposition-based incremental Broad Learning System, a PATH branch that evolves multiple subspaces in parallel based on post-hoc Error-Correcting Output Codes for robust spatial encoding and evolutionary optimization, and a SPOT circuit that dynamically fuses the outputs of both branches to form a closed-loop evolutionary system. Extensive experiments on image classification, speech emotion recognition, and few-shot learning demonstrate that AHSE achieves comparable performance to state-of-the-art methods while maintaining high efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper's major contribution is formulating the subspace evolution problem as an adaptive evolution procedure. It is discussed that a fixed evolution strategy is suboptimal, and the subspace should be dynamically adapted according to the data's intrinsic hierarchical structure. The proposed AHSE is built upon a solid mathematical foundation on the basis of broad learning systems and error-correcting output codes. A rigorous theoretical analysis is presented in the appendix, providing mechanism and robustness guarantees for ECOCs on BLS under mild assumptions."}, "weaknesses": {"value": "The content organization of this paper needs to be substantially improved, and the experiment section also needs improvements to be more convincing. \n1. The problem of subspace evolution is not properly introduced in this paper. Firstly, the problem itself is not a widely studied topic in literature, so a potential reader is not likely to have a good understanding of its basic concept. Secondly, the introduction section only discusses the flaws of existing static evolution strategies, but have not introduced the subspace evolution problem itself. Finally, the preliminary section has not formally define the problem, but only introduces BLS and ECOC. I have not understood the problem until reading the methodology section.\n\n2. It is a bad idea to put a figure right underneath the title. And the contents of the figure is also confusing. The notion of data hierarchy and evolution pattern are hard to interpret.\n\n3. Many contents in the paper contains references to the appendixes, and these contents are not self-contained. So the paper itself is incomplete without its appendix. If the page limits is too short for this paper, perhaps it should be better to submit the paper to a journal where the manuscript can be longer.\n\n4. The experiments section compares AHSE with many widely-used models, but the details of the compared models is not clear. For example, VGG and Resnet have multiple variations, each of which have different number of layers and parameters. Besides, the results of ViT is obviously worse than a well-trained ViT model could get. Considering these aspects, the experimental results are not conviencing enough."}, "questions": {"value": "Please see the weakness section above, and explain the concern about the experimental results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Hb0csrljp0", "forum": "LgZg5D4EHt", "replyto": "LgZg5D4EHt", "signatures": ["ICLR.cc/2026/Conference/Submission23588/Reviewer_1QGY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23588/Reviewer_1QGY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23588/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762290165463, "cdate": 1762290165463, "tmdate": 1762942722302, "mdate": 1762942722302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}