{"id": "dJTMvkuhMr", "number": 21241, "cdate": 1758315297967, "mdate": 1759896932662, "content": {"title": "Persuade Me if You Can: A Framework for Evaluating Persuasion Effectiveness and Susceptibility Among Large Language Models", "abstract": "Large Language Models (LLMs) demonstrate persuasive capabilities that rival human-level persuasion. While these capabilities can be used for social good, they also present risks of potential misuse. Beyond the concern of how LLMs persuade others, their own susceptibility to persuasion poses a critical alignment challenge, raising questions about robustness, safety, and adherence to ethical principles. To study these dynamics, we introduce Persuade Me If You Can (PMIYC), an automated framework for evaluating persuasiveness and susceptibility to persuasion in multi-agent interactions. Our framework offers a scalable alternative to the costly and time-intensive human annotation process typically used to study persuasion in LLMs. PMIYC automatically conducts multi-turn conversations between Persuader and Persuadee agents, measuring both the effectiveness of and susceptibility to persuasion. Our comprehensive evaluation spans a diverse set of LLMs and persuasion settings (e.g., subjective and misinformation scenarios). We validate the efficacy of our framework through human evaluations and demonstrate alignment with human assessments from prior studies. Through PMIYC, we find that Llama-3.3-70B and GPT-4o exhibit similar persuasive effectiveness, outperforming Claude 3 Haiku by 30%. However, GPT-4o demonstrates over 50% greater resistance to persuasion for misinformation compared to Llama-3.3-70B. These findings provide empirical insights into the persuasive dynamics of LLMs and contribute to the development of safer AI systems.", "tldr": "An automatic evaluation framework to assess the persuasive effectiveness of LLMs and their susceptibility to persuasion in multi-turn conversational settings, in subjective and misinformation contexts, to enable safer and ethical AI development.", "keywords": ["persuasion and language models", "persuasive effectiveness", "susceptibility to persuasion", "ethical considerations in NLP applications", "evaluation and metrics"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cccea7a1d30f8d4634a2f8a07d32cc970956cafb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces an automated framework for measuring both persuasiveness and susceptibility to persuasion in large language models (LLMs). The framework runs multi-turn simulated dialogues between a PERSUADER and a PERSUADEE, quantifying (i) how effectively models change others’ opinions and (ii) how resistant they are to persuasion. The work targets scalability by replacing costly human annotation with automated LLM evaluations, while still claiming human-aligned validity.\n\nAcross tested models, Llama-3.3-70B and GPT-4o achieve similar persuasive strength, both outperforming Claude 3 Haiku by almst 30 %."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Good problem statement: Persuasion and susceptibility are central to LLM safety and alignment; a unified framework for both is a meaningful step forward.\n\nThe dual-role simulation (persuader/persuadee) is clearly structured and allows exploration of both offensive and defensive persuasion capabilities. The evaluation spans multiple foundation models and includes both subjective and misinformation scenarios, providing useful descriptive insights.\n\nEven limited, the correlation study between automated scores and human judgments increases credibility relative to purely synthetic setups"}, "weaknesses": {"value": "Persuasiveness and susceptibility are measured entirely using LLM audiences. Prior studies Singh et al. (2024), Anthropic (2024), Hackenburg et al. (2024) demonstrate that LLMs are poor proxies for human persuasion, and that effectiveness scales logarithmically with model size.\nThe study does not evaluate on real-world persuasion datasets (e.g., ChangeMyView, PersuasionArena), leaving open whether the simulated results translate to authentic human discourse.\nIt is a promising framework conceptually, offering a scalable lens on persuasion dynamics. However, its empirical claims rest heavily on simulated LLM interactions,"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qO0iQmI2bF", "forum": "dJTMvkuhMr", "replyto": "dJTMvkuhMr", "signatures": ["ICLR.cc/2026/Conference/Submission21241/Reviewer_ADkH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21241/Reviewer_ADkH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996169233, "cdate": 1761996169233, "tmdate": 1762941650923, "mdate": 1762941650923, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PMIYC (Persuade Me If You Can), an automated framework to evaluate both persuasive effectiveness and susceptibility to persuasion in LLMs through simulated multi-turn conversations between two agents, a persuader and a persuadee. The framework measures opinion shifts using a normalized change in agreement (NCA) score, and experiments span subjective debates and misinformation contexts across multiple state-of-the-art models. The experimental results illustrate that models like Llama-3.3-70B and GPT-4o are strong persuaders, but GPT-4o resists misinformation far better, showing over 50% greater robustness. PMIYC also validates its self-reporting approach with human annotations and multiple consistency checks. In conclusion, it is a large-scale, scalable, and empirically grounded attempt to systematically study persuasion dynamics in LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a well-structured, scalable, and reproducible framework that replaces costly human evaluation while maintaining strong alignment with human judgments.\n\nIt investigates both sides of persuasion, i.e., how models persuade others and how they can be persuaded, addressing a critical but underexplored angle in AI safety and robustness.\n\nThe experimental design is comprehensive, covering single- vs multi-turn setups and subjective vs misinformation contexts, showing good methodological maturity.\n\nValidation through human annotation and multiple consistency checks (self-reports, MCQs, LLM-as-judge) adds credibility to the findings."}, "weaknesses": {"value": "The framework still relies on LLM self-reports as the basis for measuring belief change, which, even though partially validated, remains an imperfect and indirect proxy for true persuasion; models might simulate agreement rather than genuinely “change” stance.\n\nThe persuasion domains (subjective and misinformation claims) are limited; adding more diverse or higher-stakes contexts like moral dilemmas, political reasoning, or multi-agent negotiations could strengthen generality.\n\nThe turn-based setup is rigid and scripted, lacking elements of conversational flow or adaptive context that occur in human persuasion; this limits ecological realism.\n\nAlthough human annotations are used for validation, the sample size (125 conversations) and annotator pool (12 graduate students) are relatively small compared to the scale of automated experiments.\n\nThe paper highlights ethical risks but does not deeply discuss dual-use implications; for instance, how this framework could unintentionally help design more manipulative persuasion systems if misused."}, "questions": {"value": "How “agreement shifts” in the misinformation setting distinguish between semantic clarification (e.g., revising a claim) and genuine persuasion toward falsehoods.\n\nThe normalization formula for NCA is mathematically clean, but it’s unclear how sensitive it is to initial scores near neutrality (e.g., starting at 3).\n\nWhether the persuadee’s “final decision” prompt creates anchoring or framing effects that bias final agreement scores.\n\nHow conversation failures (models refusing to answer or deviating from roles) were handled beyond generation success rates—did these get excluded or repaired?\n\nThe relationship between persuasion effectiveness and linguistic strategies (e.g., emotional appeals, evidence use) is mentioned but not analyzed; including such qualitative analysis would make the results more interpretable."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "This work studies LLMs’ persuasive and susceptibility behaviors, including in misinformation contexts, which carries dual-use and safety risks. The framework could potentially be misused to develop manipulative AI systems. It also involves human annotators, warranting a quick check for responsible research and data-handling practices."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B74cD46mQ2", "forum": "dJTMvkuhMr", "replyto": "dJTMvkuhMr", "signatures": ["ICLR.cc/2026/Conference/Submission21241/Reviewer_WcXZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21241/Reviewer_WcXZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045421347, "cdate": 1762045421347, "tmdate": 1762941650648, "mdate": 1762941650648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PMIYC (Persuade Me If You Can), an automated framework for evaluating both persuasive effectiveness and susceptibility to persuasion in large language models through multi-agent conversational interactions. The framework simulates multi-turn dialogues between persuader and persuadee agents, tracking opinion changes via a normalized change in agreement (NCA) metric across diverse settings, including subjective claims and misinformation scenarios. The authors show several findings through experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes an automated framework to evaluate the persuasiveness and susceptibility to persuasion.\n- The paper provides comprehensive experimental settings that span multiple dimensions, including single-turn vs. multi-turn, subjective vs. misinformation, different model families and sizes."}, "weaknesses": {"value": "- The paper lacks solid technical contributions, as well as interesting insights and findings. There are already well-established works that measure the persuasiveness of language models [1]. This work seems to be an incremental extension of prior works.\n- It’s not realistic to only use LLMs to simulate persuasion conversations, as there could be significant gaps in the behaviors in persuasion between humans and LLMs.\n- The paper did not extensively discuss the sycophancy biases in LLMs [2]. The LLMs are prone to following and accepting requests.\n\n[1] Measuring the Persuasiveness of Language Models. Anthropic. 2024.\n\n[2] Towards Understanding Sycophancy in Language Models. Anthropic. 2023."}, "questions": {"value": "What are the significant implications or takeaways from the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wlberh6srJ", "forum": "dJTMvkuhMr", "replyto": "dJTMvkuhMr", "signatures": ["ICLR.cc/2026/Conference/Submission21241/Reviewer_YFJz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21241/Reviewer_YFJz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762127850882, "cdate": 1762127850882, "tmdate": 1762941650295, "mdate": 1762941650295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Persuade Me If You Can (PMIYC), a framework for evaluating persuasion effectiveness and susceptibility among large language models (LLMs). It simulates multi-turn dialogues between a persuader and persuadee LLM to measure how effectively one can convince the other and how resistant models are to persuasion. The framework introduces a Normalized Change in Agreement (NCA) metric to quantify persuasion outcomes and validates self-reported results through human annotations and action-based tests. Experiments across various models (GPT-4o, Llama-3.3-70B, Claude-3 Haiku) show that multi-turn interactions increase persuasion success, larger models are more persuasive, and susceptibility varies by context, especially under misinformation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. PMIYC is the first scalable framework to evaluate both persuasion and susceptibility in LLMs using fully automated, multi-turn conversations—an advancement over prior one-shot or human-only methods.\n\n2. The methodology is solid, featuring the novel NCA metric and strong validation through human annotations (>75% alignment) and behavioral consistency (>90%).\n\n3. The paper is well-structured, with clearly explained roles, metrics, and experimental design."}, "weaknesses": {"value": "Limited Causal Analysis of Model Behavior\nWhile the paper provides clear empirical findings (e.g., that Llama-3.3-70B and GPT-4o exhibit similar persuasive effectiveness), it does not explore why models behave this way as persuaders or persuadees. The discussion remains descriptive rather than analytical. Readers are left without insight into what underlying factor, such as architectural differences, pretraining data quality, or post-training methods like RLHF, might explain variations in persuasive ability or resistance. A more diagnostic analysis could connect observed behaviors to known model design or alignment techniques, helping contextualize the findings beyond surface-level comparisons.\n\nThe paper successfully measures persuasion and susceptibility but stops short of deriving actionable implications for safer LLM development. It does not clarify how insights from PMIYC could inform strategies to reduce susceptibility to misinformation or balance persuasiveness with ethical safety. For instance, should future LLMs be optimized for higher factual resistance, calibrated confidence, or improved self-consistency? Including a discussion of how PMIYC results can guide alignment, training objectives, or dataset design would significantly enhance the work’s applied impact."}, "questions": {"value": "Can you try to answer my questions in the weakness section?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "47PPuhYlWP", "forum": "dJTMvkuhMr", "replyto": "dJTMvkuhMr", "signatures": ["ICLR.cc/2026/Conference/Submission21241/Reviewer_aUsk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21241/Reviewer_aUsk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762401993136, "cdate": 1762401993136, "tmdate": 1762941650048, "mdate": 1762941650048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}