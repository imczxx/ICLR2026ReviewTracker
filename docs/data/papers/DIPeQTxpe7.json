{"id": "DIPeQTxpe7", "number": 25066, "cdate": 1758363660302, "mdate": 1759896735545, "content": {"title": "Animating the Uncaptured: Humanoid Mesh Animation with Video Diffusion Models", "abstract": "Animation of humanoid characters is essential in various graphics applications, but require significant time and cost to create realistic animations. We propose an approach to synthesize 4D animated sequences of input static 3D humanoid meshes, leveraging strong generalized motion priors from generative video models -- as such video models contain powerful motion information covering a wide variety of human motions. From an input static 3D humanoid mesh and a text prompt describing the desired animation, we synthesize a corresponding video conditioned on a rendered image of the 3D mesh. We then employ an underlying SMPL representation to animate the corresponding 3D mesh according to the video-generated motion, based on our motion optimization. This enables a cost-effective and accessible solution to enable the synthesis of diverse and realistic 4D animations", "tldr": "A method to animate humanoid meshes from a text prompt by transferring motion generated by video diffusion models to the mesh.", "keywords": ["Motion generation", "Motion Tracking & Transfer"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91aeab7ca30d6de38c1e8bc5f53e2e9dd3f4133c.pdf", "supplementary_material": "/attachment/01a878c97b03dbfd9d7ca796420c59fe2c9b5118.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method for animating static 3D humanoid meshes using video diffusion models. Instead of relying on costly motion-capture datasets, proposed approach leverages motion priors learned by large-scale text-to-video diffusion models, which inherently capture diverse human movements. Given a 3D mesh and a text prompt, the model generates a synthetic video of the mesh performing the motion, then reconstructs 3D motion by fitting and optimizing a SMPL body model to track motion cues such as 2D landmarks, silhouettes, and dense DINOv2 features. The optimized SMPL parameters are transferred to the mesh. Experiments on the CAPE dataset show that this method outperforms existing baselines in motion tracking accuracy and smoothness, and a perceptual study confirms that users find the generated animations more realistic and better aligned with textual descriptions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This introduces a simple yet effective framework that leverages the motion priors of existing video diffusion models to animate static 3D humanoid meshes without relying on expensive motion-capture data. Its integration of generative video priors with SMPL-based optimization enables realistic, temporally coherent, and diverse motion synthesis from simple text prompts. This combination, including feature-based optimization, registration, and reparameterization, makes the approach both scalable and generalizable, offering a practical and accessible solution for creating 4D humanoid animations. Also, its performance on pose fitting outperforms the previous works."}, "weaknesses": {"value": "A main concern is that its novelty and contribution are somewhat limited, as the overall concept of using video diffusion models for motion generation has already been explored in prior works such as MotionDreamer [1] and AnyMoLe [2]. Similar to MotionDreamer and AnyMoLe, the proposed approach extracts dense features and generated videos to guide motion reconstruction. Although this paper integrates these components into a clean and unified framework for humanoid, it primarily extends existing ideas rather than introducing a fundamentally new mechanism for motion extraction or representation. Furthermore, while the concepts overlap with [2], this work is not referenced in the paper.\n\n\n[1] MotionDreamer: Exploring Semantic Video Diffusion Features for Zero‑Shot 3D Mesh Animation. Uzolas et al., 3DV 2025\n\n[2] AnyMoLe: Any Character Motion In‑betweening Leveraging Video Diffusion Models. Yun et al., CVPR 2025"}, "questions": {"value": "Why are untextured meshes rendered for video diffusion model inference? It seems that leveraging video diffusion models with textured meshes could yield better performance due to a smaller domain gap with the models’ training distribution. This untextured setting could also effect largely to the pose fitting performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YBu5VKbTCC", "forum": "DIPeQTxpe7", "replyto": "DIPeQTxpe7", "signatures": ["ICLR.cc/2026/Conference/Submission25066/Reviewer_xUv5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25066/Reviewer_xUv5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760952917164, "cdate": 1760952917164, "tmdate": 1762943312282, "mdate": 1762943312282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper described a method to generate 3D humanoid motion using 2D video generation models. Video models have seen significantly more data compared to motion generation models (that are restricted to motion-capture data typically), and hence seem to be less expressive. The method is well engineered, with individual steps that make perfect sense, and produces convincing results."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- excellent results.\n- clear and reasonable method.\n- some nice steps, such as the combined modalities of the tracking.\n- seems to surpass the SOTA even when compared to dedicated motion generation models."}, "weaknesses": {"value": "- Applies to humanoid characters only.\n- Depth is not explicitly addressed."}, "questions": {"value": "- How do you not get motions that are too flat in terms of depth?\n- Why use VPoser, which is a rather old prior instead of newer ones?\n- Why not use the texture of the mesh as well, wouldn't that help the video model to be more expressive?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JF6nYgQ5df", "forum": "DIPeQTxpe7", "replyto": "DIPeQTxpe7", "signatures": ["ICLR.cc/2026/Conference/Submission25066/Reviewer_xL5i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25066/Reviewer_xL5i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829691199, "cdate": 1761829691199, "tmdate": 1762943311997, "mdate": 1762943311997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel method to animate a static 3D humanoid mesh from a text prompt, addressing the limitations of costly MoCap datasets by instead leveraging the rich, generalized motion priors from large-scale Video Diffusion Models (VDMs). The pipeline first generates a video by conditioning a VDM on a rendered image of the mesh and the text prompt. Then, it performs a robust motion transfer by first registering a SMPL body model to the input mesh as a \"deformation proxy\" and re-parameterizing the mesh vertices. A tracker then optimizes the time-varying SMPL parameters to match the generated video, guided by a combination of sparse 2D landmarks, dense silhouettes, and DINOv2 features. To ensure temporal smoothness, these motion parameters are predicted by shallow MLPs, and experiments show this approach outperforms baselines in tracking and is significantly preferred by users over traditional MoCap-based methods for realism and prompt alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper's primary strength is its hypothesis that generative video models, trained on massive, diverse, \"in-the-wild\" video data, contain superior and more generalizable motion priors than the small, clean MoCap datasets currently used by most text-to-motion methods. The strong user study results (Fig. 4) convincingly validate this hypothesis.\n- The method for transferring 2D video motion to the 3D mesh is very well-designed. It correctly identifies that regression-based pose estimators (like HMR) would fail on synthetic VDM-generated videos, and thus wisely opts for a more robust optimization-based tracking approach. The use of multiple, complementary tracking cues (sparse landmarks, dense silhouettes, and semantic DINOv2 features) provides strong guidance for the optimization.\n- The paper does an excellent job of evaluating its claims. The authors wisely isolate and evaluate their tracking component on a controlled task (recovering GT motion from the CAPE dataset). The results show it is not only more accurate (lower MPJPE/PVE) but significantly smoother (much lower \"Accel\" error) than strong baselines. The perceptual user study is the main payoff. The fact that users overwhelmingly preferred this method's animations to those from a strong MoCap-based model (MDM) on realism, prompt alignment, and overall quality is a very strong result.\n- While the method combines several existing tools (VDMs, SMPL, DINO), it does so in a novel pipeline that solves a practical problem. The \"SMPL-as-proxy-rig\" approach makes the method applicable to a wide range of static humanoid meshes that lack their own skeletons or rigs, which is a common use case."}, "weaknesses": {"value": "- The method is a \"Garbage In, Garbage Out\" system that places full trust in the VDM's output. The paper acknowledges that VDMs can produce artifacts or \"morphing effects,\" but does not fully address how the tracker would handle them. If the VDM generates a physically impossible motion, a distorted body part, or a character that morphs into the background, the optimization-based tracker will likely fail or produce an equally nonsensical 3D animation. (I am personally a believer of VDM as world simulators, so hopefully this will be less of an issue over time).\n- The tracker works from a single 2D video, which is an inherently ill-posed 2D-to-3D problem. While the SMPL/VPoser prior helps, the method is still susceptible to ambiguities in depth and self-occlusion. The (very specific) prompt engineering in Appendix A (e.g., \"Wide angle shot,\" \"Fixed camera,\" \"No zoom in\") suggests that the VDM output must be carefully constrained to be \"trackable,\" which limits the range of dynamic camera motions that can be animated."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3jP7oKPItU", "forum": "DIPeQTxpe7", "replyto": "DIPeQTxpe7", "signatures": ["ICLR.cc/2026/Conference/Submission25066/Reviewer_FtqR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25066/Reviewer_FtqR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984672760, "cdate": 1761984672760, "tmdate": 1762943311080, "mdate": 1762943311080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Animating the Uncaptured, a method for animating 3D humanoid meshes from text prompts. Given an input mesh and textual description, the approach first generates motion videos via a text-to-video (T2V) diffusion model, then leverages the SMPL parameterized human model as a deformation proxy to track and reconstruct character motion from the generated video, which is subsequently transferred back to the 3D mesh.\nTo enhance reconstruction quality, the authors integrate multiple cues of body keypoints, silhouettes, and dense DINOv2 features as optimization constraints. Experiments on the CAPE dataset show that the method outperforms SMPLify-X, WHAM, and Multi-HMR baselines. A user study further indicates that the generated animations achieve higher perceived realism and better text-motion consistency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novelty: \nThe paper explores a promising direction by introducing a generalized motion prior from large-scale video diffusion models (VDMs) to animate static 3D meshes. The proposed “generation–tracking–deformation” pipeline bridges generative video synthesis and 3D motion reconstruction, leveraging the strong expressive capacity of modern VDMs. This cross-domain integration offers an extensible framework for text-driven 3D animation.\n\n2. Experimental Evidence: \nThe paper provides abundant qualitative examples, and quantitative results in Table 1 demonstrate clear improvement on CAPE sequences, particularly in the Accel metric, compared with existing registration and reconstruction baselines such as SMPLify-X, WHAM, and Multi-HMR."}, "weaknesses": {"value": "1. Unclear motivation: \nWhile the paper claims to focus on animating humanoid meshes, the methods and experiments seem more centered on registration and tracking from images or videos.\nThe distinction between animation generation and motion fitting is not clearly articulated, making the actual novelty somewhat ambiguous.\n\n2. Method clarity: Several parts of the method are under-explained.\n- line 159 mentions \"we use the encoding $Z \\in R^32$ of the variational autoencoder VPoser\", \nbut no further elaboration is given. \n- lines 256–269 are vague: it is unclear whether $v_i^SMPL$ refers to the corresponding face or vertex on the SMPL template.\nThe function $\\Psi$ is introduced without a precise definition or computational description.\nalso lacks a clear definition or computational description. \n- Equation (3) refers to L1, but the written form corresponds to an L2 prior, indicating inconsistency between text and formulation.\n\n3. Resource requirements: \nThe paper mentions 1,000 iterations for registration and over 4,000 for video tracking, yet omits device type, runtime, or memory requirements. Without such details, the practical feasibility and scalability of the method remain unclear.\n\n4. Lack of ablation on MLP optimization: \nThe implicit MLP for temporal modeling appears to be a design simplification rather than a fundamental requirement. Since the MLP is optimized per sequence and does not generalize across meshes, it limits both efficiency and scalability. A shared temporal model (e.g., RNN, Transformer, or motion prior) might offer better generalization and faster inference.\n\n5. Strong Dependence on the Video Diffusion Model: \nThe presented animations rely heavily on the pretrained Kling AI VDM. The authors neither fine-tune the VDM for animation-related content nor test robustness across different video generators. This raises concerns about reproducibility and generalization to varied video outputs."}, "questions": {"value": "1. Limited quantitative evaluation:\nThe CAPE dataset provides only narrow evaluation scenarios on untextured meshes. \nWhy not assess performance on broader human pose and shape benchmarks such as 3DPW, RICH, or EMDB?  Additionally, would textured meshes influence generation quality or tracking efficiency?\n\n2. Tracking accuracy:\nHas the tracking accuracy been quantitatively evaluated on editable or rigged humanoid meshes to validate applicability in real animation pipelines?\n\n3. Use of SMPL motion generation:\nSince the method already performs mesh-to-SMPL registration, why not leverage existing SMPL-based motion generation techniques to animate the mesh directly, instead of relying on text-to-video tracking? This would seem to be a more straightforward way to drive the mesh using well-established motion priors.\n\n4. Failure cases: \nWhat are the observed failure scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The method uses large text-to-video models trained on web-scraped content, which raises data-source and copyright concerns. It could also produce realistic human motion that may be misused for deepfake or impersonation purposes. Ethical safeguards and data transparency are probably insufficiently discussed."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X1d9pEYjdP", "forum": "DIPeQTxpe7", "replyto": "DIPeQTxpe7", "signatures": ["ICLR.cc/2026/Conference/Submission25066/Reviewer_eAnh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25066/Reviewer_eAnh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762195397731, "cdate": 1762195397731, "tmdate": 1762943310662, "mdate": 1762943310662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}