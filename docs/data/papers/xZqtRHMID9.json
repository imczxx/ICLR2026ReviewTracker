{"id": "xZqtRHMID9", "number": 4408, "cdate": 1757675677090, "mdate": 1759898034070, "content": {"title": "APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training", "abstract": "With the rapid development of LLM-based agents, there is a growing trend to incorporate agent-specific data into the pre-training stage of LLMs, aiming to better align LLMs with real-world autonomous task execution. \nHowever, current pre-training benchmarks primarily focus on isolated and static skills, e.g., common knowledge or mathematical/code reasoning, and fail to reflect model’s agentic capabilities. On the other hand, agent benchmarks are typically designed for post-trained models, requiring multi-turn task execution abilities that base models struggle to support. Thus, there is a compelling need for a benchmark that can evaluate agentic potentials during pre-training and guide the model training more effectively.\nTo address this gap, we propose APTBench, a framework that converts real-world agent tasks and successful trajectories into multiple-choice or text completion questions tailored for base models. \nIt focuses on core agentic abilities, e.g., planning and action, and covers key agent scenarios, software engineering and deep research. \nCompared to existing general-purpose benchmarks, APTBench offers a more predictive signal of a model’s downstream performance as an agent, while remaining significantly more lightweight and cost-effective than full-scale, end-to-end agent evaluations after post-training.", "tldr": "Benchmark of pre-trained base LLMs (not post-trained models) for agentic potentials", "keywords": ["Agent Benchmark", "Pre-Training Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa97895efe82f344fbdb6b3f15c17b9f1e5c02fe.pdf", "supplementary_material": "/attachment/c0d1781b8b20332a78c0c070738551c7770912a0.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces APTBench, a new benchmark designed to evaluate the \"agentic potential\" of base LLMs during the pre-training stage.\n\nThe authors identify a gap where current pre-training benchmarks focus on static skills (like knowledge or reasoning) rather than agentic capabilities, while existing agent benchmarks are mostly end to end, requiring the model to be able to following multi-turn instructions and thus unsuitable for evaluating base models that only undergoes pre-training.\n\nAPTBench's main contribution is a novel methodology for converting agent trajectory logs generated from existing agent benchmark into multiple-choice or text completion questions that are more suitable for evaluating base models, and target specifically on the planning, action and atomic abilities essential for powering agents. The authors demonstrate the method by applying it to two different domains, coding and deep research. Results show that the score is closely correlated with final agentic capabilities of the model."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a clear, important, and timely problem. As the field moves to integrate agent-specific data into pre-training, there is a critical need for a benchmark to measure \"agentic potential\" before the costly post-training stage. The paper correctly identifies this significant gap.\n- Instead of just creating another end-to-end agent task, the work proposes a novel and creative framework to convert existing, complex agent trajectories into a static, multiple-choice/text-completion format. This makes evaluation on base models, which struggle with multi-turn interactions, feasible."}, "weaknesses": {"value": "- The main weakness is around experiment design and results. The experimental results are mixed and inconclusive (e.g., as seen in Table 2, LLAMA3.2-3B has 14 for planning on EnvSetup, but 30 for IssueFix, LLAMA4 has 56 on EnvSetup and 50 on IssueFix, both seem to be good and bad  at planning depending on the split). This seem to suggest that the benchmark may be measuring domain-specific knowledge (e.g., knowledge of coding syntax, familiarity with research topics) rather than the intended general-purpose reasoning and planning abilities for agent. Similarly I don't think use of instruction tuned model results (in correlation analysis) is a fair comparison, as that is heavily impacted by the post-training, and not just base model potential.\n\n- The benchmark's design of \"atomic\" and \"general\" actions feels overly specific to the domains of coding and research, rather than capturing general agentic behavior. True agentic potential involves more abstract capabilities, such as the ability to use new tools on the fly, adapt to unseen environments, or reason over novel information structures. The current tasks seem to equate \"agentic potential\" with \"potential for this specific task,\" which deviates significantly from the paper's stated motivation.\n\n- A primary contribution is presented as the method for converting datasets. However, this method appears tightly coupled to the specific structures of the two source datasets. This lack of generality limits the impact of the proposed methodology.\n\n- The current benchmark appears to be derived from successful or \"golden\" trajectories. A large, non-trivial part of agentic intelligence is the ability to handle errors, recover from failed actions, and re-plan from unexpected states. By omitting error trajectories, the benchmark fails to evaluate a model's robustness and adaptability, which are critical components of true agentic potential."}, "questions": {"value": "- The paper's core motivation is to evaluate base models, but the correlation analysis in Figure 1 and other experiments seem to use instruction-tuned models (e.g., Llama-3-Chat). Could you clarify on this? Since post-training heavily influences a model's ability to plan and follow instructions, how can these results be used to support claims about the base model's inherent potential?\n\n- I found the results in Table 2 difficult to interpret as a clear signal of general planning. For instance, Llama-3.2 3B has a planning score of 14 on \"EnvSetup\" but 30 on \"IssueFix,\" while Llama 4 is high on both (56 and 50). This high variance suggests the benchmark might be testing domain-specific knowledge for that task rather than a general, transferable planning ability. Could you provide an explanation for this variance and provide further support for the claim that this measures general planning? Also there seem to be outliers in result, e.g, GLM4.5 has 0 for action on IssueFix, why is that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MejnoXuyQ5", "forum": "xZqtRHMID9", "replyto": "xZqtRHMID9", "signatures": ["ICLR.cc/2026/Conference/Submission4408/Reviewer_Ggtk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4408/Reviewer_Ggtk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781680330, "cdate": 1761781680330, "tmdate": 1762917349147, "mdate": 1762917349147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new benchmark, APTBench, that can be used to assess the agentic capabilities of pre-trained base models. Standard text benchmarks like MMLU, GSM8K, EvalPlus etc. are not good indicators for agentic tasks like software engineering (SWE-like) or Deep Research kind of setups and benchmarks like SWE-Bench, Terminal Bench, etc. The idea is to leverage multi-turn agentic traces and successful agent trajectories into single-turn MCQ format questions or text completion questions. And then these can be evaluated using standard metrics like exact-match and accuracy, etc. The paper shows correlations scores of base models on standard text benchmarks to be low with final post-trained model capabilities on agentic tasks, but APTBench on the other hand, exhibits higher correlation scores."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important problem of assessing base model capabilities for agentic purposes and real-world coding scenarios. A lot of post-trained benchmarks exist like SWE-Bench, MLE-Bench, Terminal-Bench, etc. but they can be only be evaluated on post-trained models due to their multi-turn and iterative setup. No such benchmarks exist for pre-trained models.\n- Instead of just a benchmark, the authors detail the generation pipeline for building pre-training benchmarks for agentic capabilities, which can be used in the future to refresh/update the benchmark.\n- The evaluation framework focuses on real-world use-cases like SWE and DeepResearch, which are representative of complex multi-step tasks an AI agent might tackle.\n- High correlation of APTBench with downstream post-trained benchmarks shows the efficacy of measuring the potential of models during pre-training on agentic use-cases."}, "weaknesses": {"value": "- I might've missed this but the paper does not mention clearly what models are used to perturb the correct ground truth to generate the incorrect options for the LLMs.\n- How do different models affect the benchmark construction quality and correlation scores if different models are used to generate the traces/MCQ options. That might be an important indicator for the usability of this benchmark for pre-training.\n- EM and Rouge scores are not reliable indicators of performance. I would like to see the correlation between negative log-likelihood (NLL) of the appended text completion vs the final downstream performance on tasks like SWE-Bench. Lower the NLL, higher the post-trained performance should be.\n- Not really a weakness, but there are some typos here and there like GSM8K is incorrectly referenced as GPM8K at various places and opening quotes are not used correctly (\" instead of `` in latex leading to quote mismatch)."}, "questions": {"value": "I have listed some of my questions in the weakness section above, but here are a couple more:\n\n1. Can the authors show an equivalent figure similar to Figure 8 for APTBench (SWE and DR) vs Terminal-Bench and Tua as well. The main paper only shows the correlation b/w APTBench and SWE-Bench in Figure 4.\n2. Instead of EM/Rouge scores, I would like to see LLM-as-a-judge and NLL comparison as well.\n\nI would be happy to increase my score to 8 if the authors are able to address the weaknesses/questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DwpUFenZve", "forum": "xZqtRHMID9", "replyto": "xZqtRHMID9", "signatures": ["ICLR.cc/2026/Conference/Submission4408/Reviewer_Cm9U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4408/Reviewer_Cm9U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856667452, "cdate": 1761856667452, "tmdate": 1762917348894, "mdate": 1762917348894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces APTBench, a new benchmark designed to evaluate the \"agentic potential\" of base LLMs at the pre-training stage. The authors first argue that existing benchmarks (e.g., MMLU) correlate poorly with downstream agent performance. They then propose a new methodology: they define a taxonomy of core agentic skills (planning, action, and atomic abilities), convert multi-turn agent trajectories into single-turn multiple-choice and text-completion questions based on this taxonomy, and finally, validate the benchmark by demonstrating a strong correlation between its total score and performance on a downstream agent task (SWE-bench Verified)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Significant Problem & Interesting Entry Point: The paper correctly identifies a critical gap: the need to evaluate base models for agentic skills before the costly post-training stage. Its core technical idea—converting interactive trajectories into static, single-turn formats (MCQ/TC)—is a clever and pragmatic entry point to solving this challenging problem.\n\nUseful Base Model Analysis: The experimental results, regardless of the benchmark's validation, provide valuable insights for the community. The observations on skill emergence with model scale (e.g., Qwen3-1.7B vs 4B) and the significant impact of agent-focused pre-training data are useful, independent contributions."}, "weaknesses": {"value": "Fundamentally Unjustified Taxonomy and Thin Validation: This is the paper's primary methodological failure. For a paper whose topic is proposing a new benchmark, its main method and focus should be the rigorous selection and validation of what is being measured. This critical part is almost entirely omitted. The paper simply proposes a taxonomy (\"Planning,\" \"Action,\" \"Atomic Abilities\") based on intuition, with no formal analysis, theoretical grounding, or ablation studies to prove these metrics are necessary, sufficient, or comprehensive for measuring \"agentic potential.\" Having skipped the step of validating its own metrics, the paper attempts to validate the entire benchmark by showing a holistic correlation with a single downstream task (SWE-bench). This validation is exceptionally thin and insufficient.\n\nPotential for Benchmark Contamination & Weak Proxies: The benchmark's construction has other methodological issues. It is built from public datasets (SWE-Bench-Lite, GitHub repos) where the risk of data contamination for modern models is high and not deeply analyzed. Furthermore, the use of idealized README files as a proxy for a problem-solving trajectory in the EnvSetup task is a weak proxy that does not reflect real-world, feedback-driven interactions."}, "questions": {"value": "The paper's central claim of measuring \"agentic potential\" relies on an intuitively chosen taxonomy (Planning, Action, Atomic Abilities) that lacks rigorous justification. Can the authors provide the empirical or theoretical evidence that these specific metrics are both necessary and sufficient? If not, should the paper's claim be significantly moderated to something more specific, such as \"a static proxy for SWE-bench performance\".\n\nThe benchmark's validation is based on a correlation with only one downstream task: SWE-bench. Why was this single, domain-specific task deemed sufficient? Were correlations against other agent domains (e.g., web browsing, research-focused agents) attempted? This single-task validation is not robust enough to support the paper's general claims.\n\nThe benchmark is constructed from public data (GitHub, SWE-Bench-Lite) that are likely present in the pre-training corpora of the models being tested. What steps were taken to quantify and mitigate this potential data contamination?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9tWaa2HFNd", "forum": "xZqtRHMID9", "replyto": "xZqtRHMID9", "signatures": ["ICLR.cc/2026/Conference/Submission4408/Reviewer_E5na"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4408/Reviewer_E5na"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4408/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996812223, "cdate": 1761996812223, "tmdate": 1762917348670, "mdate": 1762917348670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}