{"id": "UDo71Vy3jL", "number": 1236, "cdate": 1756866816032, "mdate": 1763649830273, "content": {"title": "Eliciting Diverse Thinking Schemata for Large Reasoning Models", "abstract": "Large reasoning models (LRMs) have attracted increasing attention for their ability to solve complex mathematical problems by generating extended reasoning chains. In this work, we highlight a critical yet underexplored aspect of their reasoning process—thinking schemata, which we define as the distinct transitions between reasoning steps and the variety of solution paths the model produces. We observe a correlation between the diversity of thinking schemata and model performance, which motivates us to enhance diversity as a means to further improve reasoning potential and generalization ability. To this end, we propose Diverse Schemata Policy Optimization (DiScO), a method to elicit diverse thinking schemata by first endowing the model with the capabilities to be aware of the thinking schemata in its reasoning chain and then encouraging their diversity through reinforcement learning. Experiments on multiple mathematical reasoning benchmarks demonstrate that DiScO consistently outperforms standard group relative policy optimization, with particularly pronounced gains on challenging datasets such as AIME, where our 7B and 32B DiScO models surpass the closed-source frontier LRMs by 15\\%-30\\%. Overall, our work suggests the important role that diversity of the reasoning procedure plays and points to scaling along the diversity dimension as a promising research direction.", "tldr": "", "keywords": ["Large Language Model", "Reasoning", "Diversity"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7cbe844d2f0695340644c7d8d5a87f2346e11d01.pdf", "supplementary_material": "/attachment/01e49ba64d9b3970b7fec6c8776c3ddfb41ea0e1.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DiScO (Diverse Schemata Policy Optimization), a reinforcement learning framework for eliciting diverse thinking schemata in large reasoning models (LRMs). The key idea is that the diversity of reasoning transitions and answer candidates—termed “thinking schemata”—correlates strongly with reasoning accuracy and generalization. DiScO augments GRPO with an explicit diversity-based reward that encourages multiple reasoning paths and richer transitions, combined with simple inference-time interventions (initial truncation, repetition filtering)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel conceptual framing – “Thinking Schemata.”: Bridging schema theory and RL diversity gives a fresh conceptual handle on LRM reasoning.\n2. Simple but effective inference strategies. Truncation and repetition elimination are lightweight yet consistently beneficial.\n3. Interpretability. The schema-based view provides a more human-interpretable account of reasoning dynamics than token-level RL signals."}, "weaknesses": {"value": "1. Ambiguity in annotation process.\nThe “annotation ability” section relies on distillation from Qwen-max for annotating reasoning transitions and answers, but the quality and consistency of these annotations are not quantified. How sensitive is DiScO to annotation noise or errors?\n2. Unclear requirement for base model capacity.\nAll experiments use DeepSeek-R1-Distill-Qwen-7B/32B—a model already trained for structured reasoning. It remains unclear whether DiScO offers gains for weaker pretrained LMs or models without prior reasoning distillation. This limitation obscures whether the method truly teaches reasoning diversity or merely amplifies capabilities that already exist.\n3. Marginal improvements at 32B scale.\nAccording to Table 6 (and corresponding main results), the improvement of DiScO-32B over its base model or distilled model is small, sometimes within the margin of noise. For 7B, performance is nearly identical in some benchmarks. The paper should clarify whether these gains are statistically significant and isolate which component (diversity reward vs. truncation) contributes most.\n4. Limited domain coverage.\nEvaluation is restricted to mathematical reasoning. Demonstrating benefits on other reasoning tasks would strengthen generality claims."}, "questions": {"value": "What's the relation to High-Entropy Forking Tokens (Wang et al., 2025)?\n\nThinking schemata align conceptually with the high-entropy forking tokens proposed in Beyond the 80/20 Rule, which identify points where the model’s next-token distribution branches into distinct continuations. DiScO’s reasoning transitions correspond to these high-entropy decision points at a higher semantic level—moments when the model shifts reasoning modes or revises hypotheses. However, the paper does not quantify token-level entropy or verify this correspondence; analyzing entropy maps alongside annotated transitions would clarify and potentially unify the two views.\n\n\nWang, Shenzhi, et al. \"Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning.\" arXiv preprint arXiv:2506.01939 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xETbv9oRoE", "forum": "UDo71Vy3jL", "replyto": "UDo71Vy3jL", "signatures": ["ICLR.cc/2026/Conference/Submission1236/Reviewer_bF43"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1236/Reviewer_bF43"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761559061684, "cdate": 1761559061684, "tmdate": 1762915713832, "mdate": 1762915713832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "The updates in our paper."}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful feedback and for recognizing the potential of our novel conceptual framing, “Thinking Schemata”. We believe this perspective offers a promising direction for modeling divergent reasoning in LLMs. \n\nIn the revised version of the submission, all modifications and newly added text are highlighted in blue.\n\n### Summary of main changes\n\n- The misleading claim in the abstract has been rectified.\n- In Section 4.2, an additional analysis has been added at the end of the “main result” paragraph.\n- In Section 4.3, a more detailed analysis has been added.\n- In Section 6, the “Related Work” section has been refined, incorporating additional comparisons and connections to relevant prior work."}}, "id": "ECdHYXjtXZ", "forum": "UDo71Vy3jL", "replyto": "UDo71Vy3jL", "signatures": ["ICLR.cc/2026/Conference/Submission1236/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1236/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1236/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763647265589, "cdate": 1763647265589, "tmdate": 1763647265589, "mdate": 1763647265589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"thinking schemata“, which indicate the diversity of reasoning transitions and answer candidates, and observes a correlation between this diversity and the performance of Large Reasoning Models. To enhance this, the paper proposes Diverse Schemata Policy Optimization (DiScO), an RL method that rewards schemata diversity and improves on multiple math benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction of concepts from cognitive science is interesting and potentially useful.\n2. The problem this paper studies is important for the LLM reasoning community."}, "weaknesses": {"value": "1. The core concepts of \"thinking schemata,\" \"Reasoning Transition,\" and \"Answer Candidate\" are abstractly defined. It is hard to understand these abstract concepts. As mentioned under \"Soundness,\" the paper fails to explain how these are identified beyond stating it's a distilled ability.\n2. The paper strongly overclaims the paper's results. The abstract claims that DiScO models \"surpass the closed-source frontier LRMs by 15%-30%\". This claim is true only for the AIME benchmarks. On the GPQA-Diamond benchmark, the paper's DiScO-32B model significantly underperforms its own base model. In line 375, the paper claims large benefits, which are contrary to the experimental results.\n3. The definition of metrics is unclear. The paper lacks the mathematical formulation of \"reasoning transitions\", \"answer candidates\", \"unique reasoning answer\", and so on. In addition, the experiments shown in Table 4 demonstrate that the diversity of DiScO is inferior to or about the same as Distill-7B (The base model), which is contradictory to the main claim of the paper.\n4. The method depends on the model learning to annotate “thought patterns,” but this ability was distilled from Qwen-Max using only 840 samples. Such a small, potentially biased dataset likely leads the model to imitate Qwen Max’s reasoning style rather than truly grasp the underlying cognitive structure.\n5. The paper does not position and compare the existing methods in Section 6.1, which makes it unclear what the contribution is to the RL training in Large Reasoning Models.\n6. The paper does not discuss the unique contribution compared to other diversity-seeking RL papers for reasoning, such as [1, 2, 3, 4, 5]\n\n[1] Hu E J, Jain M, Elmoznino E, et al. Amortizing intractable inference in large language models[C]//The Twelfth International Conference on Learning Representations.\n\n[2] Yu F, Jiang L, Kang H, et al. Flow of Reasoning: Training LLMs for Divergent Reasoning with Minimal Examples[C]//Forty-second International Conference on Machine Learning.\n\n[3] Wang S, Yu L, Gao C, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning[J]. arXiv preprint arXiv:2506.01939, 2025.\n\n[4] Younsi, Adam, et al. \"Accurate and diverse LLM mathematical reasoning via automated PRM-guided GFlowNets.\" arXiv preprint arXiv:2504.19981 (2025).\n\n[5] Nair L, Trase I, Kim M. Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options[J]. arXiv preprint arXiv:2502.12929, 2025."}, "questions": {"value": "1. In Table 4, why does it show your SOTA DiScO-7B model reduce #RT and #AC on the AIME datasets compared to the baseline?\n2. Can you please provide concrete details on the \"annotation ability\" distilled from Qwen-max? How does it identify \"Reasoning Transitions\"?\n3. See other questions in the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KhN0ISlpaN", "forum": "UDo71Vy3jL", "replyto": "UDo71Vy3jL", "signatures": ["ICLR.cc/2026/Conference/Submission1236/Reviewer_nWWj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1236/Reviewer_nWWj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761669155536, "cdate": 1761669155536, "tmdate": 1762915713669, "mdate": 1762915713669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the concept of \"thinking schemata\" to characterize reasoning diversity in reasoning models. They focus on two schemata: reasoning transitions and answer candidates. They propose to add additional reward terms to GRPO based on statistics of these two schemata derived from a separate LLM. They achieve strong empirical results on math reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The experimental results are quite strong, especially at the 7b model scale. However, I have concerns about whether the gain in performance are from the proposed method."}, "weaknesses": {"value": "- My main complaint is that the motivation is quite weak: it's based on a correlation with R^2 values < 0.6. I don't follow the logic behind the reward. Isn't the optimal behavior under this diversity reward to come up with a bunch of (unrelated) answer candidates and reasoning transitions before arriving at the final answer? Isn't the desired behavior to directly arrive at the final answer, and in very hard problems where that isn't possible, to explore?\n- The paper doesn't comment on the possibility of reverse causality or confounders. For example, higher-capability models may naturally produce diverse reasoning as a consequence of better internal representations, not because diversity drives performance.\n- The abstract's claims feel a bit disengenous: \"particularly pronounced gains on challenging datasets such as AIME\", \"surpass the closed-source frontier LRMs by 15%-30%\". The strongest model in your frontier LLM list is over a year old (o1-mini), and there are many frontier models that outperform it.\n- The source of the performance gains is unclear. Table 3 shows that removing most of the novel components from DiScO-7B results in a model that would still outperform all other points of comparison at the 7b scale and even most models in the frontier category."}, "questions": {"value": "- How did you get the reasoning traces for o1-mini? My understanding was that the OpenAI API doesn't provide reasoning tokens."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1gPr30z2jQ", "forum": "UDo71Vy3jL", "replyto": "UDo71Vy3jL", "signatures": ["ICLR.cc/2026/Conference/Submission1236/Reviewer_Cgmp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1236/Reviewer_Cgmp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762244622999, "cdate": 1762244622999, "tmdate": 1762915713551, "mdate": 1762915713551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces “thinking schemata” to describe transitions between reasoning steps and diverse solution paths, proposing DiScO, a reinforcement learning framework that explicitly promotes such diversity. The approach is well-motivated and effectively integrates diversity modeling into the RL objective, offering interpretable insights into how reasoning diversity shapes model behavior. It provides comprehensive ablation studies and fine-grained analyses of reasoning processes, demonstrating that encouraging diverse thinking can lead to more coherent, flexible, and human-like reasoning patterns across multiple benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper shows that improving reasoning diversity not only boosts accuracy but also enhances interpretability, reduces repetitive reasoning, enriches the model’s reasoning structure, and improves robustness by encouraging more flexible, human-like thought processes.\n2. The paper presents detailed ablation studies isolating the contributions of each reward component and inference-time strategy. The inclusion of metrics like #RT-avg, #AC-avg, RC, UA, and TAR provides an unusually rich and interpretable analysis of model behavior."}, "weaknesses": {"value": "1. The paper reports only Pass@1 accuracy, without multi-pass (Pass@k) evaluation that would more fully capture the model’s reasoning diversity and robustness. It would be valuable to see the performance numbers and trends as the number of passes increases.\n2. While the current results on mathematical reasoning are impressive, it would be valuable to test DiScO on non-mathematical, open-ended, or multi-hop reasoning benchmarks to assess whether its diversity-based reward generalizes beyond deterministic math domains.\n3. The authors claim that richer thinking schemata lead to improved reasoning robustness, but no experiments are provided to substantiate this claim."}, "questions": {"value": "1. Beyond improving accuracy, can DiScO actually discover more distinct valid solutions or demonstrate better out-of-distribution (OOD) generalization? Since the paper claims that diverse thinking schemata enhance robustness, it would be helpful to show whether this diversity translates into discovering novel or unseen reasoning paths on shifted or harder distributions.\n2. In Table 4, DiScO’s average reasoning transitions (RT-avg) and answer candidates (AC-avg) are often lower than those of Distill-7B. Given that diversity is explicitly encouraged in the reward, why does DiScO not consistently yield higher RT and AC counts?\n3. Is there any deeper analysis explaining why more diverse thinking schemata improve Pass@1 success rate? The paper seems to assume a positive link, but doesn’t discuss the potential trade-off between exploration and exploitation—for example, could excessive diversity harm focused reasoning or answer consistency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WJ0bobrhO0", "forum": "UDo71Vy3jL", "replyto": "UDo71Vy3jL", "signatures": ["ICLR.cc/2026/Conference/Submission1236/Reviewer_bhDJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1236/Reviewer_bhDJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1236/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762548257777, "cdate": 1762548257777, "tmdate": 1762915713436, "mdate": 1762915713436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}