{"id": "6f8qlK7wN4", "number": 20169, "cdate": 1758303264272, "mdate": 1759896996980, "content": {"title": "RedAHD: Toward End-to-End LLM-Based Automatic Heuristic Design using Reductions", "abstract": "Solving NP-hard combinatorial optimization problems (COPs) (e.g., traveling salesman problems (TSPs) and capacitated vehicle routing problems (CVRPs)) in practice traditionally involves handcrafting heuristics or specifying a search space for finding effective heuristics. The main challenges from these approaches, however, are the sheer amount of domain knowledge and implementation efforts required from human experts. Recently, significant progress has been made to address these challenges, particularly by using large language models (LLMs) to design heuristics within some predetermined generalized algorithmic framework (GAF, e.g., ant colony optimization and guided local search) for building key functions/components (e.g., a priori information on how promising it is to include each edge in a solution for TSP and CVRP). Although existing methods leveraging this idea have shown to yield impressive optimization performance, they are far from being end-to-end and still require considerable manual interventions. In this paper, we propose a novel framework, named RedAHD, that enables these LLM-based heuristic design methods to operate without the need of GAFs. More specifically, RedAHD employs LLMs to automate the process of reduction, i.e., transforming the COP at hand into similar COPs that are better-understood, from which LLM-based heuristic design methods can design effective heuristics for directly solving the transformed COPs and, in turn, indirectly solving the original COP. Our experimental results, evaluated on six COPs, show that RedAHD is capable of designing heuristics with competitive or improved results over the state-of-the-art methods with minimal human involvement.", "tldr": "We propose a novel framework toward end-to-end automatic design of heuristics with LLMs for solving NP-hard combinatorial optimization problems.", "keywords": ["combinatorial optimization", "large language model", "evolutionary algorithm", "automatic heuristic design"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8938c9e8ad89cb56af745c45498ac3591a7da40c.pdf", "supplementary_material": "/attachment/28d8c694efb248d9ad7fef6951d80decc75d4c6f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces RedAHD, a novel framework for automatic heuristic design (AHD) that aims to make the process more end-to-end by leveraging Large Language Models (LLMs). The central idea is to address a key limitation of prior LLM-based evolutionary program search (LLM-EPS) methods, which rely on manually specified generalized algorithmic frameworks (GAFs) like Ant Colony Optimization or Guided Local Search. RedAHD automates this by using an LLM to perform reductions: transforming the combinatorial optimization problem (COP) at hand into a similar, better-understood problem. This allows an underlying LLM-EPS method to design heuristics for the transformed problem directly, thereby indirectly solving the original one. The framework includes a \"multi-problem\" evolutionary search, where ideas can be exchanged between heuristics for different reductions, and a \"reduction refinement\" mechanism to improve reductions when the search stagnates. The authors demonstrate through extensive experiments on six COPs that RedAHD, without needing a GAF, can design heuristics that achieve competitive or state-of-the-art performance compared to methods that do."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea of using an LLM to learn problem reductions is a paradigm shift for LLM-based AHD, moving beyond heuristic generation to problem transformation.\n2. The work directly and effectively tackles the reliance on manually-designed GAFs, a major bottleneck in previous state-of-the-art LLM-EPS methods.\n3. The framework is rigorously tested on six different COPs, consistently demonstrating competitive or superior performance against strong baselines on both synthetic and real-world (TSPLib) benchmarks.\n4. The paper is well-written and well-organized, making complex ideas accessible. The figures, tables, and extensive appendices contribute to a high-quality and reproducible research artifact."}, "weaknesses": {"value": "1. While RedAHD removes the need for a GAF, it introduces its own set of hyperparameters (e.g., $M, M_{init}, l, T$). The paper lacks a sensitivity analysis for these parameters, making it unclear how crucial their specific tuning is.\n2. Limited interpretability of generated reductionsâ€”no systematic analysis of what reductions the LLM tends to produce.\n3. Some experimental setups and the choice of hyperparameters need to be further discussed and explained. For instance, the decision to remove three of the five original variation operators from EoH is justified empirically.\n4. The success of the reduction phase hinges on the LLM having relevant knowledge about related COPs. For truly novel or niche problems, the LLM may fail to generate meaningful reductions. This is a critical failure mode, but only briefly touched upon in the limitations."}, "questions": {"value": "1. What was the rationale for choosing $M_{init}=10$ and $M=3$? The article would benefit from an explanation of the hyperparameter choices.\n2. The \"reduction refinement\" step is vital. It is recommended to provide statistics from your experiments on its activation frequency. For instance, in a typical 20-generation run for TSP, how many times was a reduction refined on average?\n3. What kinds of changes does the reduction refinement step typically produce? Are they minor tweaks to the implementation of $(f, g)$, or do they represent fundamentally different reduction strategies? An example would be very helpful.\n4. The author removed the E1 operator (generate a completely new heuristic) from EoH. This seems counterintuitive, as RedAHD is built on exploration. It is recommended to provide a clearer explanation to this question.\n5. For the CVRP results in Table 7, the performance jump when using 03-mini is dramatic. \nIt is suggested to provide a qualitative comparison of the reductions and/or heuristics generated by GPT-40-mini versus 03-mini for this problem. What specifically did the more capable model do better?\n6. For the highly constrained VRPTW, noted that a high percentage of invalid solutions. Is this because the LLM struggles to generate valid reduction functions $(f, g)$, or because the subsequent LLM-EPS search fails to produce heuristics that respect the constraints of the reduced problem B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oZyeWG6I9l", "forum": "6f8qlK7wN4", "replyto": "6f8qlK7wN4", "signatures": ["ICLR.cc/2026/Conference/Submission20169/Reviewer_3HNV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20169/Reviewer_3HNV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738453830, "cdate": 1761738453830, "tmdate": 1762933685064, "mdate": 1762933685064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a key limitation in current LLM-based Automatic Heuristic Design (AHD) methods: they are not truly end-to-end and rely on human experts to specify a Generalized Algorithmic Framework (GAF), such as ACO or IC, which reintroduces significant domain knowledge and implementation effort. To address this, the authors propose RedAHD, a novel framework that enables LLM-AHD methods to operate without a predefined GAF. The core idea is to use an LLM to automate problem reduction: the LLM is prompted to transform the target CO problem A into a similar CO problem B. The LLM generates the functions to map instances from A to B and solutions from B back to A. Existing LLM-EPS methods are then used to design heuristics for directly solving problem B, thereby indirectly solving problem A. The RedAHD framework consists of three main components: 1) Reduction Initialization: An LLM generates a set of candidate Language Reductions (LRs) and a set of heuristics; 2) Multi-Problem LLM-EPS: A novel evolutionary search where heuristics from different LRs (i.e., for different \"Problem B's\") can be used as references to create new heuristics for other LRs, facilitating the discovery of novel algorithmic ideas; 3) Reduction Refinement: An LLM refines the reduction functions if the search for an LR stagnates, helping to avoid local optima. The authors evaluate RedAHD on six COPs (TSP, CVRP, KP, MKP, OBPP, BPP) and show that it achieves competitive or improved results compared to SOTA LLM-AHD methods that rely on manually specified GAFs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper's core premise is sound. Instead of automating heuristic design within a fixed framework, this work attempts to automate the framework itself by re-framing it as a problem-reduction task.\n\n2. The paper is well-written and clearly motivated. The problem statement is easy to understand, and the high-level schematic in Figure 2 effectively communicates the method's workflow.\n\n3. The proposed RedAHD framework is well-structured and thoughtfully designed with its three components (initialization, multi-problem search, refinement)."}, "weaknesses": {"value": "1. It appears the framework has simply shifted the manual-effort burden. Experts must still manually design detailed, COP-specific prompt components (Tables S10, S11) and, most critically, manual solution checkers. The paper's own experiment on VRPTW demonstrates this new burden is a critical point of failure. The authors state that the designed heuristics \"are not consistently valid\" and violate constraints in over 40% of test instances.\n\n2. The experiments set $M_{init}=10$ and $M=3$, but provide no analysis on the quality of this initial pool. How sensitive is the final performance to this initialization step? This critical component seems under-analyzed.\n\n3. The ablation in Table S12 shows that the multi-problem component ($M=3$) is significantly better than the single-problem component ($M=1$). This suggests much of the performance gain might come from the multi-problem search. However, the current baselines (e.g., EoH on ACO) are single-problem (or single-GAF)."}, "questions": {"value": "1. The VRPTW experiment highlights that designing the prompts and solution checks is a new, expert-level burden and a critical failure point. How do you quantify this new manual effort against the effort of implementing a GAF? Given that the method can produce invalid solutions for complex COPs, how can the claim of \"enhanced automation\" be justified?\n\n2. How sensitive is RedAHD to the quality of the initial $M_{init}=10$ LRs? In your experiments, what percentage of these initial LRs were trivial or invalid? What happens if the LLM fails to generate any non-trivial strategies for a new problem?\n\n3. How much of the performance gain over GAF-based baselines is attributable only to the GAF-free reduction aspect, versus the multi-problem search strategy? \n\n4. The problems solved are limited to the vehicle routing and packing problems. This problem seems relatively easy to reduce. Can the proposed method be extended to handle more complex problems such as the flow shop scheduling problem (which is solved in EoH)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jU7RUXU1mZ", "forum": "6f8qlK7wN4", "replyto": "6f8qlK7wN4", "signatures": ["ICLR.cc/2026/Conference/Submission20169/Reviewer_EzGv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20169/Reviewer_EzGv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878890924, "cdate": 1761878890924, "tmdate": 1762933684446, "mdate": 1762933684446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an augmentation method called reduction for building LLM-based AHD methods. Generally, RedAHD can achieve impressive results on TSP. I served as the reviewer for this paper at previous conferences. After comparing the changes, I tend to barely retain my previous review"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The introduction part is well-written. With clear evidence and good logic, clear improvements compared to the previous manuscripts.\n\n2. The proposed RedAHD shows impressive results on TSP and some other COPs."}, "weaknesses": {"value": "See Questions"}, "questions": {"value": "1. RedAHD seems to have significant differences in performance on different issues. According to Figure 3, RedAHD is able to design a 2-opt operator in the TSP problem, which seems to be something that the IC framework cannot achieve. Is this the only reason why RedAHD performs well on TSP? Can performing 2-opt post-processing on the solution of EoH artificially achieve similar performances to RedAHD?\n\n2. Can RedAHD show potential on some problems highly requiring an end-to-end heuristic (e.g., maybe TSPTW, which has no feasibility guarantee for the IC framework, and there are no current implementations with ACO)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mZJpqoroDk", "forum": "6f8qlK7wN4", "replyto": "6f8qlK7wN4", "signatures": ["ICLR.cc/2026/Conference/Submission20169/Reviewer_2KLT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20169/Reviewer_2KLT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970291284, "cdate": 1761970291284, "tmdate": 1762933683897, "mdate": 1762933683897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}