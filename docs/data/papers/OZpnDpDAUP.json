{"id": "OZpnDpDAUP", "number": 23876, "cdate": 1758349678341, "mdate": 1759896792970, "content": {"title": "Overcoming Redundant Context in Auto-Regressive LLMs with Dynamic Draft Refinement", "abstract": "Large language models (LLMs) have demonstrated strong reasoning abilities, driven in part by reinforcement learning–based optimization methods such as Reinforcement Learning with Verifiable Rewards (RLVR). These methods encourage a slow thinking paradigm, where models produce detailed intermediate steps between designated “thinking tokens.” Models employing such strategies are commonly referred to as Large Reasoning Models (LRMs). Despite notable progress, LRMs lack selective retention—the ability to discard redundant reasoning while preserving only the intermediate results that are useful for subsequent steps. In contrast, humans dynamically maintain a structured ''working state'', continuously filtering out unproductive thoughts and retaining essential ones. To examine this limitation, we introduce a lightweight arithmetic benchmark designed to isolate reasoning behaviors. Through systematic evaluation, we show that redundant intermediate traces—specifically those that are low-quality or irrelevant—significantly degrade performance.. Analyses of task accuracy, behavioral patterns, and attention allocation confirm that LRMs often rely on the entire reasoning context, including outdated or unhelpful information. To address this issue, we propose Dynamic Context Refinement (D-Refine), an inference-time mechanism that selectively organizes and condenses reasoning steps as they are generated. Experiments on diverse benchmarks demonstrate consistent performance gains, highlighting the importance of maintaining a well-structured working state for accurate and efficient reasoning. This work establishes selective retention as a key principle for improving LRM reasoning.", "tldr": "", "keywords": ["Large reasoning model", "draft–summary paradigm", "context refinement"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/356a6d423c7c77581a105462c5211d62df32a01e.pdf", "supplementary_material": "/attachment/f53e3959cc72f48e0fc0e28b7a87f00f7bd1b419.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the issue of redundant context in auto-regressive Large Reasoning Models. The authors propose ​​Dynamic Context Refinement (D-Refine)​​, an inference-time mechanism that dynamically filters and condenses intermediate reasoning steps. Through a designed toy arithmetic benchmark, the study systematically analyzes the negative impact of redundant context on reasoning performance and validates D-Refine’s effectiveness across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "​​Clear Problem Identification​​: The paper highlights a critical limitation in LRMs: the lack of selective retention—the ability to discard redundant or low-quality intermediate reasoning traces while preserving essential information. This observation is insightful and empirically validated through controlled experiments (e.g., UQ/SQ+Arith tasks).\n\n​​Simple yet Effective Method​​: D-Refine is a lightweight inference-time optimization that requires no retraining. It dynamically summarizes and prunes intermediate reasoning steps, improving context quality without disrupting the reasoning flow. The approach is easy to implement and compatible with existing reasoning frameworks."}, "weaknesses": {"value": "Limited Novelty​​: D-Refine shares similarities with existing context-compression or memory-augmentation techniques. The authors should more explicitly contrast their approach with related work in Section 2, emphasizing its unique focus on dynamic refinement during single-episode reasoning.\n\n​​Narrow Benchmark Scope​​: Experiments primarily rely on arithmetic and mathematical reasoning tasks. While effective for controlled analysis, broader validation on diverse reasoning benchmarks (e.g., logical reasoning, commonsense QA) would strengthen the claims of generalizability.\n\n​​Heuristic Hyperparameter Tuning​​: The performance of D-Refine is sensitive to trigger length and refinement steps, but no adaptive strategy is proposed. Future work could explore dynamic triggering mechanisms (e.g., based on perplexity or attention entropy).\n\n​​Missing Strong Baselines​​: Comparisons are limited to \"direct reasoning.\" Including state-of-the-art context-management methods and efficient reasoning methods, would better demonstrate D-Refine’s advantages."}, "questions": {"value": "As discussed in the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Rg0CanFyJo", "forum": "OZpnDpDAUP", "replyto": "OZpnDpDAUP", "signatures": ["ICLR.cc/2026/Conference/Submission23876/Reviewer_dCz9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23876/Reviewer_dCz9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729572745, "cdate": 1761729572745, "tmdate": 1762942838016, "mdate": 1762942838016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dynamic Context Refinement (D-Refine), an inference-time method that improves reasoning in Large Reasoning Models (LRMs) by summarizing and pruning intermediate drafts. To enable fair evaluation, the authors also develop a simple arithmetic benchmark, since existing math benchmarks may have been contaminated during pretraining.\n\nUsing this benchmark, the authors present an attention-weight analysis showing that LRMs allocate substantial attention to unrelated context. In their setup, the model is prompted with two problems in sequence: a graph theory problem followed by an arithmetic problem. Accuracy is measured only on the second problem, while the first serves as an unrelated context. The analysis reveals that even while solving the second problem, the model continues to focus on the first, demonstrating that LRMs do not reliably suppress redundant or low-quality context, which leads to lower reasoning accuracy.\n\nTo address this limitation, the authors propose D-Refine, which periodically interrupts generation during inference. At each interruption, the LLM is prompted to produce a concise, note-style summary of the reasoning so far, replacing verbose drafts with a condensed version. The LLM then resumes reasoning conditioned on this refined summary.\n\nOn the arithmetic benchmark, D-Refine yields substantial improvements when an explicit unrelated problem is injected into the prompt. In more realistic settings, where no external problems are added, the method still provides modest gains across the arithmetic benchmark and three additional math reasoning datasets (AIME 24, AIME 25, and MATH500).\n\nOverall, the paper highlights selective context retention as a promising direction for robust multi-step reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper explores an underexplored direction in LLM reasoning by focusing purely on context compression through redundancy removal, without allowing correction, validation, or knowledge distillation. This framing offers a distinct perspective from prior summarization-based methods.\n\n- To ensure fair evaluation, the paper introduces a new arithmetic benchmark to address potential data contamination in existing benchmarks.\n\n- The paper provides empirical evidence, using attention maps, that models continue to focus on low-quality context, showing that large language models cannot naturally filter out all irrelevant information.\n\n- The prompt used for summarization is carefully crafted with clear constraints to ensure the method focuses on context compression rather than correction. It explicitly instructs the model to remove redundancy and repetition but not to fix or verify errors unless the draft itself has already rejected them. This design reflects the authors’ deliberate effort to ensure that the evaluation results genuinely support their hypothesis that improvements in reasoning are possible using only redundancy elimination without fixing the reasoning errors.\n\n- The proposed method operates purely at inference time. So, it can be integrated into existing LRM deployments without retraining."}, "weaknesses": {"value": "- The authors claim that refinement through summarization prunes redundant reasoning, but they provide no quantitative evidence that the summaries are actually shorter or more concise. Since no statistics are presented on token length reduction before and after refinement, it remains unclear whether pruning consistently reduces context size or if the refined drafts sometimes remain equally long or even longer.\n\n- The paper does not include any latency or computational cost analysis. Since D-Refine introduces additional refinement steps during inference, it is unclear how much overhead this adds in practice. Without such analysis, it is unclear whether the performance gains justify the additional latency.\n\n- The effectiveness of D-Refine depends heavily on two hyperparameters: the number of refinement steps and the trigger length. Figure 4 shows that performance is highly sensitive to these choices and varies across models and benchmarks. Although the authors propose 3 refinement steps and a trigger length of 9000 as defaults, the results do not consistently support these as reliable. The paper leaves hyperparameter selection as future work, yet this raises concern that D-Refine may not be robust across different tasks. In realistic setups, an LLM may face diverse types of tasks, and without a common, well-performing hyperparameter configuration, it is unclear how D-Refine would perform in such scenarios.\n\n- The paper evaluates D-Refine only with temperature = 0, which makes the model’s outputs fully deterministic and reduces randomness, diversity, and verbosity in reasoning traces. As a result, it remains unclear whether D-Refine would remain robust in more diverse and verbose reasoning scenarios produced at higher temperatures.\n\n- The evaluation is limited to math datasets, with no experiments on non-math reasoning tasks such as commonsense, QA, or planning. As a result, the generality of D-Refine beyond mathematical reasoning is not demonstrated.\n\n- The paper does not evaluate whether interruptions disrupt fluency, leaving it unclear whether the generated summaries negatively affect coherence in natural language outputs."}, "questions": {"value": "- From my understanding of the paper, the same LLM is used for both summarization and reasoning, as no other model is mentioned for the summarization step. If this is correct, the authors should state it clearly in Section 4.1 that the self-LLM is used for summarization.\n\n- How much context is reduced in terms of token counts or length? Reporting average tokens before and after refinement would make the impact of redundancy removal more concrete and provide better insight into the efficiency of D-Refine.\n\n- Could you share whether you measured the inference time of the experimented LLMs with and without D-Refine? Since the method introduces additional refinement steps, such a comparison would be helpful to understand the computational overhead and the trade-off between efficiency and performance gains.\n\n- D-Refine underperforms compared to direct decoding on two of the three math benchmarks when applied to QwQ-32B. Could you provide more insight into this result? Possible factors might include the model’s reasoning style, sensitivity to pruning, or misaligned hyperparameter choices. A deeper analysis would help clarify why D-Refine is less effective for this model and provide insight into whether the method generalizes reliably across architectures.\n- Did you perform any manual inspection of the refined contexts? It would be useful to see examples showing how the summaries look after refinement and whether they effectively reduce redundancy without losing important reasoning steps. Such analysis could also clarify whether all types of redundancy are being removed or only certain patterns.\n\n- Typos:\n  - Line 25: Remove the extra period after “degrade performance..” \n  - Quotation marks: Fix inconsistent opening quotation marks around terms such as “working state”, “slow thinking”, and “thinking tokens”. Issues appear at lines 20, 45, 46, 49, and 50, and may occur elsewhere. Ensure quotation marks are consistent throughout the paper.\n  - Table 4 caption: The caption does not match the table content. It should be corrected to accurately describe the table."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "51GVnMO9ma", "forum": "OZpnDpDAUP", "replyto": "OZpnDpDAUP", "signatures": ["ICLR.cc/2026/Conference/Submission23876/Reviewer_MVtd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23876/Reviewer_MVtd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750182320, "cdate": 1761750182320, "tmdate": 1762942837683, "mdate": 1762942837683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how redundant or low-quality reasoning context affects Large Reasoning Models (LRMs) that use the \"draft-summary\" paradigm (e.g., O1, DeepSeek-R1, QwQ). Through controlled experiments using a novel two-decimal arithmetic benchmark, the authors demonstrate that LRMs lack selective retention—the ability to filter out irrelevant or flawed intermediate reasoning. When presented with low-quality upstream context, model accuracy drops by up to 20%. The paper proposes Dynamic Context Refinement (D-Refine), an inference-time mechanism that periodically summarizes and prunes reasoning traces via prompted LLM calls. Results show substantial improvements (6-25%) in controlled adversarial settings where bad context is explicit, but mixed results on standard benchmarks (MATH500, AIME), with gains primarily on smaller models like Llama-8B."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies a genuine and timely limitation: LRMs' inability to selectively filter redundant context, unlike human working memory. Section 3 provides excellent multi-perspective analysis (accuracy, behavioral patterns, attention allocation) that systematically demonstrates the problem. The contrast with human \"working state\" provides strong intuitive grounding.\n\nThe arithmetic benchmark is well-designed to minimize memorization effects and isolate reasoning behavior. The two-question setup (SQ/UQ + Arith) effectively creates controlled conditions to demonstrate causal effects of context quality. Length-controlled ablations (padding/truncation) help disentangle length from semantic quality. \n\nComprehensive empirical evidence in controlled setting: Table 3 shows large, consistent gains (6-25%) in the UQ+Arith scenario, validating the core hypothesis. The asymmetry (large gains for bad context, minimal for good context) strengthens credibility. Behavioral analysis (reduced \"alternatively\" tokens) and attention analysis provide supporting mechanistic evidence."}, "weaknesses": {"value": "Critical Missing Analysis: Computational Cost. The paper claims \"efficient reasoning\" but provides zero quantitative analysis of inference overhead. D-Refine requires: (a) pausing generation, (b) up to 3 refinement LLM calls processing up to 32k tokens each, (c) resuming generation. Without wall-clock time, total token counts, or FLOPs comparisons, we cannot evaluate practical utility. Which model executes f_refine? If it's the base model, there's substantial added latency. If it's a stronger external model, the comparison to \"Direct Reasoning\" is unfair\n\nLimited Technical Novelty. The core contribution: periodic prompt-based summarization (Equation 3), is straightforward engineering rather than methodological innovation. No comparison to simpler baselines: (a) single-pass \"summarize above reasoning\" before answering, (b) keyword/heuristic filtering, (c) KV-cache truncation etc., The method is applying summarization techniques to LRM traces. Missing connections to existing work on context compression, test-time CoT optimization, and long-context efficiency\n\nInconsistent Empirical Results on Real Benchmarks. Table 4 shows mixed results: QwQ-32B: 0% gain on AIME'24, -3.3% on AIME'25. Qwen3-4B: Minimal average gain (0.850→0.852, within noise). Only Llama-8B shows substantial improvements (~10%).\n\nArtificial Experimental Design. The two-question setup (solve unrelated Problem 1, then Problem 2) is not representative of real reasoning scenarios. The arithmetic benchmark, while novel, is extremely narrow (only nested two-decimal operations), doesn't validate broader claims about \"reasoning\". No evaluation on other domains: code generation, commonsense reasoning, multi-turn dialogue, agent tasks."}, "questions": {"value": "1. Computational Cost: What is the wall-clock time and total token count (including all refinement calls) for D-Refine vs. Direct Reasoning on each benchmark? Can you provide a cost-benefit analysis showing when the accuracy gains justify the computational overhead?\n\n2. f_refine Implementation: Which specific model(s) execute the refinement step (Listing 1)? Is it the base LRM itself (e.g., QwQ-32B) or a separate, potentially stronger model? This is crucial for fair comparison.\n\n3. Failure Analysis: For cases where D-Refine hurts performance (e.g., QwQ-32B on AIME'25: 0.700→0.667), what went wrong? Can you provide examples of harmful refinement? Is this over-pruning of useful context?\n\n4. Ablation Studies: Can you separate the contributions of: Summarization (condensing long traces), Pruning (removing explicitly rejected content), and Redundancy removal?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1WHECd9tet", "forum": "OZpnDpDAUP", "replyto": "OZpnDpDAUP", "signatures": ["ICLR.cc/2026/Conference/Submission23876/Reviewer_6nMq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23876/Reviewer_6nMq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870854297, "cdate": 1761870854297, "tmdate": 1762942837361, "mdate": 1762942837361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a prompting based approach for constructing intermediate reasoning drafts"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Evaluation on a broad set of Math Reasoning datasets with ablations (e.g, Analysis of attention matrices for UQ and sensitivity of hyperparameters)"}, "weaknesses": {"value": "- Results are mixed on the domains that they evaluate with (e.g, AIME), where they match direct reasoning, making it unclear how effective their proposed approach is\n- Summarization has been explored in prior work for handling long-context tasks (e.g, in MemGPT[1]), making it unclear what the contribution of the work is\n\n\n[1] MemGPT: Towards LLMs as Operating Systems"}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4gXjnpYUbe", "forum": "OZpnDpDAUP", "replyto": "OZpnDpDAUP", "signatures": ["ICLR.cc/2026/Conference/Submission23876/Reviewer_UfoZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23876/Reviewer_UfoZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101111751, "cdate": 1762101111751, "tmdate": 1762942837101, "mdate": 1762942837101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}