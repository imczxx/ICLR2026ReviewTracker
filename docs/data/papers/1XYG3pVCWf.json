{"id": "1XYG3pVCWf", "number": 4031, "cdate": 1757586981294, "mdate": 1763482880435, "content": {"title": "SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation Involving How, When and Where", "abstract": "The accompanying actions and gestures in dialogue are often closely linked to interactions with the environment, such as looking toward the interlocutor or using gestures to point to the described target at appropriate moments. \nSpeech and semantics guide the production of gestures by determining their timing (WHEN) and style (HOW), while the spatial locations of interactive objects dictate their directional execution (WHERE). Existing approaches either rely solely on descriptive language to generate motions or utilize audio to produce non-interactive gestures, thereby lacking the characterization of interactive timing and spatial intent. This significantly limits the applicability of conversational gesture generation, whether in robotics or in the fields of game and animation production. To address this gap, we present a full-stack solution. We first established a unique data collection method to simultaneously capture high-precision human motion and spatial intent. We then developed a generation model driven by audio, language, and spatial data, alongside dedicated metrics for evaluating interaction timing and spatial accuracy. Finally, we deployed the solution on a humanoid robot, enabling rich, context-aware physical interactions. Our data, models, and deployment solutions will be fully released.", "tldr": "", "keywords": ["co-speech gesture generation", "conversational gesture generation", "spatially interactive gesture"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/246f2123ddd112e87a50af991c5c74f8fa0c018f.pdf", "supplementary_material": "/attachment/f173af07a231efbe2706f6088212418ba8991a4a.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a new dataset for spatial context–aware gesture generation. The dataset comprises over 7,000 clips (approximately 9 hours) collected using a motion capture system. The authors also propose a baseline method that incorporates an intention-aware encoder. In addition, they introduce new evaluation metrics, and the model with intention awareness demonstrates clear benefits, as further supported by user studies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Incorporating spatial awareness into gesture generation is a promising direction. This paper presents a dataset and a corresponding benchmark to support research in this area. The dataset will be beneficial for the gesture generation community."}, "weaknesses": {"value": "There are several major weaknesses:\n\n1) The data collection process lacks transparency. It is not clear what kind of instructions were given to the participants. Moreover, the data collection is not grounded in the relevant literature, such as behavioural science, which raises concerns regarding potential bias.\n\n2) Although the dataset contains over 7,000 clips, it was collected from only six participants, which significantly limits its generalisability. Gesture generation, in particular, is influenced by many personal factors such as culture and personality.\n\n3) The scenarios appear more appropriate for two-person interactions, involving elements such as gaze and pointing. It is not clear why only a single person was recorded. (On a related note, Figure 2 depicts two people, which makes this aspect confusing.)\n\n4) The motivation for including pointing gestures and gaze, as described in the paper, is not well justified, especially with regard to their transferability to humanoid robots. Similarly, squatting and lying down do not seem suitable for robotics applications. The paper lacks a convincing practical application. For example, it would have been more realistic if human–robot collaboration had been considered, where the robot could share gaze with a human or perform pointing gestures to achieve shared goals.\n\n5) As acknowledged by the authors in Section 5.3, the dataset is biased.\n\nTaken together, although this dataset may be beneficial, it focuses on a very niche problem with limited evidence of generalisability and insufficient grounding in the existing literature. Moreover, its practical applicability remains unclear."}, "questions": {"value": "1) Please elaborate on the generalisability of this dataset. Why were only six participants included in the data collection process?\n2) Was the data collection approved by an ethics committee? If so, please provide details of the approval process.\n3) Please justify the choices made during data collection. How were the gaze and pointing gestures determined? What instructions were given to participants? Additionally, why was a single person recorded rather than an interactive or multi-person setting?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "The paper does not specify whether the data collection process was approved by an ethics committee, which raises concerns. It is not even clear how these 6 participants were selected, whether diversity was taken into account in the recruitment."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "axzIpo2EwM", "forum": "1XYG3pVCWf", "replyto": "1XYG3pVCWf", "signatures": ["ICLR.cc/2026/Conference/Submission4031/Reviewer_mP3H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4031/Reviewer_mP3H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761420575916, "cdate": 1761420575916, "tmdate": 1762917142278, "mdate": 1762917142278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "GRCmuSC0el", "forum": "1XYG3pVCWf", "replyto": "1XYG3pVCWf", "signatures": ["ICLR.cc/2026/Conference/Submission4031/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4031/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763482879415, "cdate": 1763482879415, "tmdate": 1763482879415, "mdate": 1763482879415, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes a generation process of gestures used in dialogue.\nThe provided definitions make the description self-consistent.\nThe related work section contains a lot of references with  brief descriptions.\nAdditionally, the principal quantities for the metrics are introduced. The global architecture is described in figure 3. The blocks of the elaboration are shortly described. Probably a longer description would make the architecture easier to reproduce.\nThe experiments show quantitative results with defined metrics.\n\nIn Table 2, the metrics for human interactions are reported. They are also called ground-truth. This label is misleading, and a label just referring to human performance should be used.\nAlso, in Table 3, the results are compared with the Ground Truth.  It is strange that the proposed method shows performances that are better than the GT.\n\n\nIn section 6.4, the last sentence is “Thank you again for helping us improve the paper’s adequacy of assessment. “\nThe paper presents an interesting dataset in the field of human-robot interaction. Some portions of the denoising network are shown but are not deeply described. \nAs a minor issue, for all the used software and hardware, a reference should be indicated for their description (e.g., Xsens, HTC Vive, MVN software). The more information are provided the more replicable is the experimental setup."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper describes a dataset of gestures used in dialogue. \n\nGenerally, content is clear, and the provided definitions make the description self-consistent."}, "weaknesses": {"value": "Some portion of the architecture, used as denoising network are not described in detail. Also, if the network components are trained from scratch or a fine tuning has been used was not declared.\nThe comparisons with other models are missing"}, "questions": {"value": "How the network in figure 3 has been trained?\n\nCan your approach be compared, in a qualitive and quantitive way with other similar techniques?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1Vefjei6DT", "forum": "1XYG3pVCWf", "replyto": "1XYG3pVCWf", "signatures": ["ICLR.cc/2026/Conference/Submission4031/Reviewer_jZrg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4031/Reviewer_jZrg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910619962, "cdate": 1761910619962, "tmdate": 1762917142095, "mdate": 1762917142095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SIG-Chat, a multimodal speech–gesture dataset with explicit intent annotations and 3D spatial metadata of intent targets to support intent-aware conversational gesture synthesis; presents a baseline model that fuses audio, transcript, initial posture description, and the target’s trajectory to generate spatially interactive co-speech gestures; and establishes a benchmark with three metrics that explicitly evaluate temporal alignment and spatial interaction accuracy driven by multimodal inputs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a dataset for modeling conversational human behavior that, unlike prior datasets, integrates additional modalities—including spatial intent—to better capture interactive dynamics."}, "weaknesses": {"value": "- The dataset adds additional modalities, but it is unclear whether its scale are sufficient for a benchmark. Although it expands to four modalities (where prior work often uses one or two), the overall data volume appears smaller than some single-modality datasets, raising overfitting concerns. The benchmark design may also introduce bias and hinder fair evaluation.\n\n- Data quality is not well substantiated: there are no qualitative or quantitative comparisons to existing datasets or models. Beyond annotation/caption quality, it is unclear whether the scenarios genuinely reflect typical, real-world conversational behavior.\n\n- Facial features are a core modality for understanding conversational behavior; while prior work includes them, it is unclear whether the proposed dataset and model incorporate this modality."}, "questions": {"value": "Is facial feature also included in this dataset? If it is, please explain the process of data collection and quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "unwMbFklAc", "forum": "1XYG3pVCWf", "replyto": "1XYG3pVCWf", "signatures": ["ICLR.cc/2026/Conference/Submission4031/Reviewer_c9jB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4031/Reviewer_c9jB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936251284, "cdate": 1761936251284, "tmdate": 1762917141909, "mdate": 1762917141909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a main limitation in co-speech gesture generation: the inability of current models to control when and where a gesture interacts with the environment. To solve this, the authors introduce a new large-scale motion capture dataset (SIG-Chat) with synchronized 3D spatial intent data, novel metrics to evaluate the spatial and temporal accuracy of these gestures, and a Diffusion Transformer baseline model that generates gestures guided by audio, text, and 3D target locations. The work is validated with quantitative results, a user study, and a proof-of-concept deployment on a humanoid robot."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles the important and previously overlooked problem of generating spatially-aware conversational gestures.\n- The model significantly outperforms its ablated baseline (\"w/o intent\") in both quantitative evaluations and a user study."}, "weaknesses": {"value": "- The model is given the entire 3D target trajectory as input. This means it doesn't need to infer where or when to point; this information is provided. This limits its practical use in dynamic, unscripted scenarios.\n- The model processes the entire audio and trajectory sequence at once to generate the full gesture clip. This offline approach is not suitable for real-time, interactive applications, and the robot demo appears to be a playback of a pre-computed motion.\n- The primary comparison is an ablation of their own model. The paper does not adapt other state-of-the-art gesture generation models (e.g., winner models from the GENEA challenge) to this new task, which would have provided a stronger context.\n\nMinors:\n- Typo: Figure 3, Sacle -> scale\n- Line 476. Text from previous rebuttal."}, "questions": {"value": "- How is the dataset compared and related to the GENEA challenge?\n- Could you elaborate on the data collection process? Was the dialogue scripted for the actors? Were they given specific instructions like \"now say your line and point to the target,\" or was it more improvisational? This would help in understanding the naturalness and spontaneity of the collected gestures and speech."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qwKL5G5CEv", "forum": "1XYG3pVCWf", "replyto": "1XYG3pVCWf", "signatures": ["ICLR.cc/2026/Conference/Submission4031/Reviewer_bcnn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4031/Reviewer_bcnn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984087435, "cdate": 1761984087435, "tmdate": 1762917141740, "mdate": 1762917141740, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}