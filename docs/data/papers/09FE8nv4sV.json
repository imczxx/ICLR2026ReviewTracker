{"id": "09FE8nv4sV", "number": 12955, "cdate": 1758211996865, "mdate": 1763659779554, "content": {"title": "Targeted MILP Instance Generation via Formulation Code Retrieval", "abstract": "Efficient and controllable data generation is critical for improving the performance of data-driven Mixed-Integer Linear Programming (MILP) solvers, especially in applications facing data scarcity. However, existing MILP instance generation methods typically require training a separate model for each problem class, which can be computationally intensive and does not allow for the generation of instances with varying sizes and solution difficulties. To address these challenges, we introduce MILP-Retrieval, a framework for targeted MILP instance generation via formulation code retrieval. We first build a diverse MILP library that includes multiple modalities and use it to pretrain an MILP embedding model. Based on the output of this embedding model, we propose a novel similarity metric that accurately measures the similarity between instances of different sizes within the same problem class. MILP-Retrieval leverages this new metric to retrieve the formulation code of a target instance and further tune it. Experimental results demonstrate the effectiveness of generating MILP instances through formulation code retrieval, with the ability to control both the scale and difficulty of the generated instances. This approach provides a novel perspective on MILP instance generation and opens up new possibilities for learning-based solvers.", "tldr": "", "keywords": ["Mixed-Integer Linear Programming", "Combinatorial Optimization", "MILP Instance Generation"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f858ce71636ad0e8fa8273973bb18f4fc7c4b95.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces \"MILP-Retrieval,\" a novel framework to address the critical problem of data scarcity for training data-driven Mixed-Integer Linear Programming (MILP) solvers. The authors argue that existing generative methods (e.g., VAEs, diffusion models) are highly inefficient, as they require training a separate, complex model for each distinct problem class and offer poor control over the generated instance's properties.\n\nMILP-Retrieval proposes a paradigm shift, changing the problem from \"generation\" to \"retrieval and tuning.\" The framework consists of several key components:\n\nMILP Library, MILP Embedding Model, Embedding Metric, Retrieval and Tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel and Highly Practical Paradigm: The core idea is the paper's greatest strength. It astutely reframes a very difficult \"generation\" problem into a much more tractable \"retrieval-then-tuning\" problem. This approach is computationally far more efficient, as the expensive pre-training of the embedding model is a one-time, amortized cost. It avoids the need to train a new generative model for every new problem class.\n\nExcellent Controllability: A significant advantage over all other generative methods. By retrieving the underlying code, the method gains direct, interpretable control over the generation process. The \"Targeted Tuning\" using Bayesian optimization (Figures 7 and 8) is particularly powerful, demonstrating the ability to generate instances that match a specific difficulty (e.g., target solve time), which is extremely valuable for solver testing and training.\n\nStrong Contribution in MILP Similarity: The paper makes a valuable standalone contribution by proposing the \"embedding metric.\" The comparison in Figure 4 is very clear and convincing. It shows the embedding metric captures the semantic class of an instance (Fig 4c) even when scale varies, whereas the statistical metric (Fig 4d) is completely confounded by scale."}, "weaknesses": {"value": "High Dependency on Library Quality: The entire framework's performance is fundamentally capped by the quality and comprehensiveness of the MILP library. If a target instance belongs to a novel problem class that is not well-represented in the library, the retrieval will fail to find a good match, and the \"tuning\" step will be useless. The paper acknowledges this, but the risk of \"out-of-distribution\" failure is significant.\n\nLimitations of \"Tuning\": The \"tuning\" mechanism only adjusts parameters (e.g., $N, M$, cost ranges) within a fixed formulation code structure. This is a limitation. If a target instance has a slightly different structural property (e.g., an extra set of constraints) not present in the retrieved code, parameter tuning alone can never reproduce it. This limits the \"fineness\" of the generation."}, "questions": {"value": "Out-of-Distribution Behavior: What is the method's failure mode? If given a target instance from a problem class that is truly novel and not in the library (e.g., from a completely different domain), what formulation code does it retrieve? Does it retrieve a \"least-bad\" match that produces nonsensical instances?\n\nLibrary Sensitivity: Figure 10 shows robustness to library size, but what is the minimum viable library required for this approach to be practical? How much human effort is needed to curate a \"good enough\" library to cover a broad range of real-world problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ceqvjhqNjr", "forum": "09FE8nv4sV", "replyto": "09FE8nv4sV", "signatures": ["ICLR.cc/2026/Conference/Submission12955/Reviewer_5Bjq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12955/Reviewer_5Bjq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761505069646, "cdate": 1761505069646, "tmdate": 1762923712888, "mdate": 1762923712888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MILP-Retrieval, a retrieval-and-tune framework for targeted MILP instance generation. Instead of reconstructing instance structures with a class-specific generative model, the method builds a multi-modal MILP library (instances, formulation code, bipartite graphs, textual descriptions), trains a graph–text contrastive embedding model, uses an embedding-based similarity metric to retrieve the closest formulation code, and then tunes code parameters (randomized or Bayesian/SMAC) to control scale and difficulty before executing the code to synthesize instances. Experiments show higher semantic similarity under the proposed embedding metric, controllable hardness, and downstream gains for Neural Diving across four classes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper investigates a new generation paradigm. Instead of directly generating problem instances, it retrieves formulation code and then produces new instances by executing and tuning that code. This approach enhances controllability and interpretability while avoiding per-class training required by generative models.\n\n1. The ability to generate meaningful instances on MIPLIB is impressive. Prior methods like VAE-based generators typically focus on synthetic or homogeneous datasets. Demonstrating that retrieval-based synthesis can operate on real-world MIPLIB formulations marks a step forward in practicality.\n\n1. The downstream task improvements are important. The authors test Neural Diving on four datasets (FCNF, TSP, GA, VRP) and show consistent improvement when trained with instances generated by MILP-Retrieval."}, "weaknesses": {"value": "1. The paper is closely related to MILP-Evolve, and much of the techniques and even code implementation seems built upon the prior framework. However, the distinction between the two methods is not clearly discussed. In my view, MILP-Retrieval differs mainly in application: MILP-Evolve focuses on constructing diverse datasets for training foundation models, while this work targets generating instances similar to a given dataset for solver improvement. Nevertheless, MILP-Evolve seems to represent a broader and more promising direction, while this work feels like a narrower instantiation. The authors should explicitly clarify this relationship and ideally compare the two works.\n\n1. The proposed embedding-based similarity metric lacks interpretability. The embedding is trained by the authors, but the meaning of the similarity scores is unclear. From Fig. 4(a)(b), the embedding captures some cross-class relations, yet it is uncertain whether those “semantic similarities” are genuinely meaningful. In Fig. 4(c)(d), embeddings recognize similarity across TSP instances of different sizes, which however aldo suggests that the model may fail to encode scale differences. And if scale-related factors were removed from the statistical metric, would results align? The paper would benefit from deeper analysis or case studies, for example, but not limited to, cases showing when and why problems from different classes appear similar.\n\n1. It would strengthen the paper to include more advanced downstream benchmarks, such as Predict & Search or hyperparameter tuning."}, "questions": {"value": "1. Minor typos. For exapmle, P3 Line 161: \"$P$ and $Q’$\" shoud be \"$Q$ and $Q’$\"? In Eqs. (3)(4) the variable notation \"$xu$\" likely should be $x_u$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xU9FN4jMns", "forum": "09FE8nv4sV", "replyto": "09FE8nv4sV", "signatures": ["ICLR.cc/2026/Conference/Submission12955/Reviewer_x6NP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12955/Reviewer_x6NP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708566165, "cdate": 1761708566165, "tmdate": 1762923712166, "mdate": 1762923712166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a MILP instance generation framework centered on formulation code retrieval. The core workflow involves constructing a multi-modal MILP library encompassing diverse problem instances, their corresponding formulation codes, bipartite graph representations, and textual descriptions. For a given set of target instances, the framework first computes their embeddings using a pre-trained model, then retrieves the most semantically similar formulation codes from the library. Extensive experiments validate the framework’s effectiveness across multiple tasks and benchmark datasets, demonstrating strong performance in generating high-quality, target-aligned MILP instances."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed formulation code retrieval paradigm for MILP instance generation is innovative and differentiates itself from existing class-specific training or structure-reconstruction methods.\n\n2. Generating instances via tunable formulation codes inherently guarantees the feasibility and well-defined mathematical properties of the output. \n\n3. Unlike methods relying solely on graph structures, this multi-modal design captures both structural and semantic characteristics of MILP problems."}, "weaknesses": {"value": "1. The framework incurs substantial upfront training costs. \n\n2. The contrastive learning paradigm (inspired by CLIP) requires aligning bipartite graph representations with natural language descriptions, yet many MILP instances lack explicit or consistent semantic connections between these two modalities. This misalignment may render the training process fragile and reduce the reliability of learned embeddings. I doubt the effectiveness and applicability of the CLIP algorithm used in this setting. \n\n3. Textual descriptions of MILP problems are inherently context-dependent. Even for instances with identical underlying mathematical models, their natural language descriptions can vary drastically across application backgrounds (e.g., scheduling vs. logistics). This variability introduces noise into the contrastive training process, potentially degrading the performance of the embedding model and retrieval accuracy.\n\n4. The framework suffers from poor generalization to unseen problem classes. If a target MILP problem has no semantically similar entries in the pre-constructed library, the retrieval step will fail to identify valid formulation codes—limiting its utility for rare or newly emerging combinatorial optimization tasks.\n\n5. The pre-trained embedding model may lack robustness in distinguishing \"foldable\" or structurally equivalent MILP instances. The inherent combinatorial complexity of MILP problems means that distinct instances can exhibit similar surface-level features (e.g., variable-constraint counts) while being mathematically non-equivalent, or vice versa. This ambiguity leads to imprecise similarity matching and undermines the reliability of code retrieval."}, "questions": {"value": "Have the authors evaluated the retrieval accuracy of the framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Hquwcpsxtc", "forum": "09FE8nv4sV", "replyto": "09FE8nv4sV", "signatures": ["ICLR.cc/2026/Conference/Submission12955/Reviewer_iMGz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12955/Reviewer_iMGz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828268700, "cdate": 1761828268700, "tmdate": 1762923711808, "mdate": 1762923711808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MILP-Retrieval, a framework for targeted MILP instance generation via formulation-code retrieval. Built on top of a multi-modal MILP library, the approach first trains a MILP embedding model by contrastively aligning graph and text representations. Given a target instance, the model embeds it, retrieves the most relevant formulation code from the library, and then adjusts the code’s exposed parameters to synthesize new instances with controllable size and difficulty. Experiments demonstrate that MILP-Retrieval can generate coherent instance families across various difficulty levels, and that the synthesized data further enhances Neural Diving when used for downstream training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper leverages formulation code to support MILP instance generation, which makes it possible to flexibly control the scale and difficulty of the generated problems through parameter tuning.\n\n2.\tThe experiments show that the proposed embedding model can recognize semantic similarity among instances generated at different scales/difficulties, and that the generated data is useful for a downstream solver."}, "weaknesses": {"value": "1.\tThe idea of using an embedding-based similarity score is not entirely new. Earlier graph models embed an input graph and then use that embedding to decide the correlation between instance and expert, e.g., the routing module in AnyGraph[1]. The authors may want to clarify the novelty of the proposed embedding metric.\n2.\tThe downstream evaluation only reports improvements in objective value. It would be more convincing to also report efficiency-oriented metrics (solve time or primal–dual integral) or to test on additional downstream tasks to show broader usefulness of the generated data.\n3.\tThe diversity and hardness of the generated MILPs seem to be largely bounded by the coverage and quality of the formulation-code library. For problem classes not represented in the library, the method is naturally limited. It would be helpful to discuss whether cross-evolving or recombining existing formulation codes could expand the library’s structural coverage and alleviate this dependence.\n\n[1] Xia, Lianghao, and Chao Huang. \"Anygraph: Graph foundation model in the wild.\" arXiv preprint arXiv:2408.10700 (2024)."}, "questions": {"value": "1.\tCan you design a “scale-insensitive” variant of the stat metric (e.g., removing statistics that mostly encode problem size) to demonstrate that your embedding metric indeed captures similarity beyond scale/difficulty?\n2.\tIn a real setting where no template exists for a given domain, how would the proposed framework obtain or construct the initial formulation code? \n3.\tCould you describe in more detail how you help ensure the correctness and feasibility of MILPs generated after parameter edits to the formulation code?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EzqNzLSDQj", "forum": "09FE8nv4sV", "replyto": "09FE8nv4sV", "signatures": ["ICLR.cc/2026/Conference/Submission12955/Reviewer_PiGJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12955/Reviewer_PiGJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830135186, "cdate": 1761830135186, "tmdate": 1762923711457, "mdate": 1762923711457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all reviewers for their constructive and insightful feedback. We summarize below the main clarifications and updates made in the revised manuscript (highlighted in blue).\n\n**Additional Downstream Task Evaluation**\n\nFollowing reviewer suggestions, we added downstream tasks using Predict-and-Search framework [1] (Table 17) and Learn2Branch [2] (Table 18). Across these tasks, MILP-Retrieval consistently improves downstream solver performance, demonstrating its broader applicability.\n\n**Clarification of Dependency on Library Quality**\n\nWe clarify that the retrieve-then-tune paradigm (the core contribution of MILP-Retrieval) is *decoupled* from the formulation-code library. A richer or more diverse library can be plugged in without modifying the method.\n\nOur current 4,000-code library is automatically synthesized via cross-evolving and recombination, requires little human effort, and can scale further. Despite not containing any MIPLIB formulations, our method achieves an average embedding similarity of 0.701 on 361 MIPLIB instances, indicating strong real-world coverage.\n\n**Summary of Contribution/Novelty**\n\nMILP-Retrieval introduces a new direction for MILP instance generation:\n\n- **Retrieval-then-tuning paradigm**: Avoids per-class generative training, improves interpretability, and enables strong controllability over problem scale/difficulty.\n\n- **MILP Embedding similarity metric**: Scale-invariant, robust to unseen classes, and substantially more expressive than existing statistical metrics.\n\n- **Formulation Code Tuning**: Supports both diverse and targeted generation with guaranteed feasibility through randomized search and Bayesian optimization.\n- **Practical and efficient pipeline**: Requires training only one embedding model for all problem classes; generates new instances in ~100 seconds and directly benefits existing ML-based solvers.\n\n---\n\nIf you have any questions, we are happy to discuss them further and resolve your concerns.\n\n\n\n[1] Qingyu Han et al., A Gnn-Guided Predict-and-Search Framework for Mixed-Integer Linear Programming, ICLR'23\n\n[2] Maxime Gasse et al., Exact Combinatorial Optimization with Graph Convolutional Neural Networks, NeurIPS'19"}}, "id": "0MhD4aY7dg", "forum": "09FE8nv4sV", "replyto": "09FE8nv4sV", "signatures": ["ICLR.cc/2026/Conference/Submission12955/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12955/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission12955/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763659747716, "cdate": 1763659747716, "tmdate": 1763659747716, "mdate": 1763659747716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}