{"id": "mW3pn67O8Y", "number": 8863, "cdate": 1758100303518, "mdate": 1763653851063, "content": {"title": "Reasoning BO: Enhancing Bayesian Optimization with the Long-Context Reasoning Power of LLMs", "abstract": "Many real-world scientific and industrial applications require the optimization of expensive black-box functions. Bayesian Optimization (BO) provides an effective framework for such problems. However, traditional BO methods are prone to get trapped in local optima and often lack interpretable insights. To address this issue, this paper designs Reasoning BO, a novel framework that leverages reasoning models to guide the sampling process in BO while incorporating multi-agent systems and knowledge graphs for online knowledge accumulation. We systematically evaluate our approach across 10 diverse tasks encompassing synthetic mathematical functions and complex real-world applications. The framework demonstrates its capability to progressively refine sampling strategies through real-time insights and hypothesis evolution, effectively identifying higher-performing regions of the search space for focused exploration. This process highlights the powerful reasoning and context-learning abilities of LLMs in optimization scenarios. For example, in the Direct Arylation task(a chemical reaction yield optimization problem), our method increased the yield to 60.7%, whereas traditional BO achieved only a 25.2% yield. Furthermore, our investigation reveals that smaller LLMs, after post-training, can attain comparable performance to their larger counterparts.", "tldr": "", "keywords": ["Large Language Models", "Bayesian Optimization"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6925e0d6d683e5b8332872076acd496caa797b3a.pdf", "supplementary_material": "/attachment/4a06407fabb356b4ac8eac5a0e970eaf093ce7c1.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Reasoning BO, a Bayesian optimization framework that incorporates large language models (LLMs) as reasoning agents to guide hypothesis generation and decision-making during optimization. The approach combines a reasoning model, a knowledge management module, and a confidence-based filtering mechanism to iteratively refine candidate proposals based on experimental feedback.\n\nThe method is tested on several real-world chemical reaction yield optimization tasks as well as synthetic benchmarks. Across these settings, Reasoning BO consistently outperforms baselines such as Vanilla BO, CMA-ES, and Analytic EI, showing faster early-stage improvements and stronger overall performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\nThis paper introduces a novel idea: using an agentic reasoning model to augment Bayesian optimization. The approach moves beyond typical surrogate-model or acquisition-function improvements and explores how autonomous reasoning agents can guide scientific experimentation. The integration of natural-language reasoning into the BO loop is novel, conceptually exciting, and timely given current trends in AI-for-science and LLM-based experimental design.\n\n\nQuality:\nThe framework is clearly described and experimentally well supported. The authors evaluate on several chemical and synthetic benchmarks, include relevant baselines, and perform ablations to separate the effects of reasoning, initialization, and optimization. The results consistently favor the proposed method.\n\n\nClarity:\nThe paper is easy to follow and well structured. The motivation is clear, the figures are informative, and the system components are explained at an appropriate level of detail. Implementation details and code availability add to transparency.\n\n\nSignificance:\nThis work represents an important step toward reasoning-enhanced optimization, where autonomous agents use language-based reasoning to guide experimental design. By showing that hypothesis-driven reasoning can improve both BO initialization and search efficiency, the paper opens a promising direction for combining symbolic reasoning, natural-language understanding, and statistical optimization. The approach has clear potential to advance AI-driven scientific discovery and automated experimentation."}, "weaknesses": {"value": "1 (Main Weakness): For the so-called “high-dimensional benchmarks,” such as Ackley (15D) and Lunar Lander (12D), the paper does not include comparisons to state-of-the-art Bayesian optimization methods designed specifically for high-dimensional search spaces. It is well known that Vanilla BO often performs poorly in high dimensions, so including at least one such baseline would be important for a fair evaluation. For example, comparisons to either (a) Vanilla BO with the length-scale priors proposed in “Vanilla Bayesian Optimization Performs Great in High Dimensions” or (b) the trust-region BO method (TuRBO) from “Scalable Global Optimization via Local Bayesian Optimization” would provide a more complete empirical assessment. Ideally, both methods could be included for all tasks with search space dimensionality greater than ~10.\n\n\n2: The empirical improvements, while consistent, are somewhat marginal on certain tasks. It would the strengthen the paper if the authors provided discussion or insight into why Reasoning BO achieves larger gains on some tasks and smaller gains on others. \n\n\n3 (Minor): The color palette in Figure 3 uses multiple similar shades, which makes it harder to distinguish curves. Adjusting the colors for better contrast would improve readability."}, "questions": {"value": "1: Could the authors provide an explanation or hypothesis for why Reasoning BO achieves larger improvements on certain tasks but smaller gains on others?\n\n\n\n2: For the “high-dimensional benchmarks,” is there a reason the paper does not include comparisons to high-dimensional BO methods such as the trust-region BO approach (TuRBO, \"Scalable Global Optimization via Local Bayesian Optimization\") or Vanilla BO with length-scale priors (\"Vanilla Bayesian Optimization Performs Great in High Dimensions\")? If such methods were omitted intentionally, additional context would help clarify the scope of evaluation.\n\n\n3: On optimization trajectory plots, it's typically to plot the BEST objective value obtained during the entire optimization run so far on the y-axis, and the number of observations done so far on the x-axis. This leads to plots where the curve is strictly increasing (or strictly decreasing for minimization tasks) since we always plot the BEST thing observed so far. Plots in this paper instead have lines that go up and down (rather than strictly increasing or strictly decreasing) and I'm not sure why that is? Can the authors clarify this and explain why curves go up and down and what exactly is being plotted on the y-axis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LuBs33Up28", "forum": "mW3pn67O8Y", "replyto": "mW3pn67O8Y", "signatures": ["ICLR.cc/2026/Conference/Submission8863/Reviewer_FLYi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8863/Reviewer_FLYi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761061198201, "cdate": 1761061198201, "tmdate": 1762920627033, "mdate": 1762920627033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Reasoning BO, which combines LLM-capabilities with Bayesian optimization. The method uses reasoning models to improve BO sampling by taking advantage of additional information and descriptions provided by the user. This is done by allowing BO to propose multiple points, after which the LLM evaluates and selects a subset of these points based on the additional information provided and its knowledge base. The authors test various iterations of their methodology, including adding a knowledge database and fine-tuned models to perform optimization. They test on various optimization problems, showing improvements beyond the baselines they present in some cases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The methodology shows improvement beyond traditional baselines on a variety of benchmarks. There any many interesting ideas here and I think there is a lot of potential in further exploring or expanding on these ideas. \n- The ablation on initializations vs. reasoning steps provides important insights into the significance of different stages of the model. In particular, it shows that reasoning models are capable of improving all stages of the optimization process.\n- The method is adaptable across many different optimization tasks and appears to have a robust structure that is capable of high-quality optimization."}, "weaknesses": {"value": "- The paper’s clarity could be significantly improved. There are many possible implementations discussed and the paper does not include details on many of these systems. Improving the readability of the paper would significantly improve the understanding for the reader. \n- The ablations presented jump around various test benchmarks and do not provide consistent baselines. I think the clarity would be improved by providing more consistency to aid interpretability here. In particular, the RL enhanced test appears to show the RL-14B model performs the best. However, it appears the methodology suggests the QWQ-plus model is selected without any RL enhancement? These inconstancies and the lack of clarity on the actual implementation shown in the results make the conclusions difficult to understand.\n- The results indicate that reasoning BO appears to have much better results compared to baselines on analytic functions such as Ackley and Hartmann. However, I do not see what relevant context would be fair to provide the model with for these benchmarks. This is concerning as I would expect the model to be able to take advantage of a real-world experiment such as reaction-coupling. However, it does not appear this is the case and I am concerned the data leakage is instead motivating the results."}, "questions": {"value": "- The paper claims that allowing BO to generate the point suggestions eliminates the possibility of data leakage from pre-training. While this may limit it, this knowledge could still be used in the selection of points. Can you further justify this claim?\n- How is the supervised fine-tuning dataset generated? It appears to be generated from previous experiments. What are these experiments and should we be concerned about data leakage?\n- How does the initial prompt description impact the results? Specifically, does providing more or less detail improve or degrade results How does the model perform without any additional specified information.\n- Can you expand your results to a broader range of benchmarks? I would be interested if the method can compare to benchmarks such as BBOB [1] or HPO-B [2]. Including this at scale alongside more specifications about which data is included in would help negate my concerns about data leakage. \n- What are the limitations of this method? Are there circumstances where the model fails/performs poorly?\n------\n- [1] BBOB: https://numbbo.github.io/coco/testsuites/bbob\n- [2] HPO-B: https://arxiv.org/abs/2106.06257"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ub5AHBLoXM", "forum": "mW3pn67O8Y", "replyto": "mW3pn67O8Y", "signatures": ["ICLR.cc/2026/Conference/Submission8863/Reviewer_6mjx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8863/Reviewer_6mjx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540553450, "cdate": 1761540553450, "tmdate": 1762920626573, "mdate": 1762920626573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper proposes Reasoning BO, an LLM-based BO framework that integrates reasoning LLMs, a multi-agent system, and knowledge storage + RAG\n- The LLM participates in evaluating candidate points/assigning confidence scores. LLM-based agents are also employed to store reasoning information from previous trials\n- The authors also include a RL finetuning stage (on GSM8K), showing that it improves BO performance"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This falls in line with recent efforts that try to integrate LLMs with optimization frameworks\n- AFAIK, this is the first work to explicitly investigate whether long-form CoT reasoning can improve BO performance"}, "weaknesses": {"value": "- [P1] If my understanding is correct, the LLM is utilized more as an 'acquisition function', scoring proposed hypotheses (from a Gaussian Process), and filtering by confidence. This does not seem to contain any mechanism to encourage exploration, as one would expect underexplored regions to be reflected in lower confidence.\n- [P2] It is not clear why LLMs are not directly used to propose the hypotheses (as in LLAMBO, BORA that investigate this specifically for BO, but also EUREKA, ALPHA-EVOLVE that are more general). By using a conventional GP to propose candidates, the method still faces the same scalability (L87-88) issues and high-dimensionality challenges.\n- [P3] Perhaps most importantly, it is unclear whether the knowledge graph is useful. It appears the database of <entity, relation, entity> is distilled from LLM reasoning traces. This seems redundant since it is from the LLM itself. The practical benefits of the knowledge system (vector DB + graph DB) are also not shown through targeted ablation experiments.\n- [P4] The RL post-training stage uses GSM8K (math dataset); it is unclear how this transfers to optimization reasoning. If the authors believe that general reasoning training is useful, it would be better to employ a 'generic' reasoning LLM directly. In other words, it is unclear what the purpose or value of finetuning on GSM8K brings.\n- [P5] I am worried about contamination risk; the real-world benchmarks are well-known, and although the LLMs are not involved in proposing candidates, they still act as a selection function, introducing indirect contamination risk\n- [P6] The paper is missing comparisons to stronger or recent LLM-based or advanced BO baselines (e.g., HEBO, kernel BO, LLAMBO, OPRO), and more traditional baselines (e.g., tree-based)\n- [P7] Fig 3 suggests different initialization across methods, which confounds results. I would recommend comparing baselines with the same initialization to understand the advantages purely through optimization. This is especially so as the initialization seems to have a disproportionately large impact, which might be due to contamination.\n- [P8] In general, the experiments (with 10 tasks) are fairly limited to establish robustness for an optimizer paper. Please also report the std dev/CI for the results in Table 1.\n\nOverall, the framework feels over-engineered and is a mixture of components without clear evidence of why and which components matter."}, "questions": {"value": "- How is exploration impacted if LLM-assessed confidence is used to acquire new points.\n- What is the 'Experiment Compass'\n- Have the authors considered RL post-training with more relevant tasks (e.g., related to BO).\n- Can the authors clarify the LogEI was used for all baselines?\n- What is the performance gain from knowledge graph and MA subsystems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I14SjrCWXx", "forum": "mW3pn67O8Y", "replyto": "mW3pn67O8Y", "signatures": ["ICLR.cc/2026/Conference/Submission8863/Reviewer_ZUuJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8863/Reviewer_ZUuJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761570164576, "cdate": 1761570164576, "tmdate": 1762920626274, "mdate": 1762920626274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Reasoning Bayesian Optimization (Reasoning BO), a framework that integrates LLMs with Bayesian optimization to enhance sample selection. The idea is to leverage the reasoning capabilities and contextual knowledge of LLMs to guide BO. Specifically, the BO process generates a set of candidate points, and the LLM selects a subset of promising candidates for evaluation. The LLM is aided by additional textual descriptions and domain knowledge. The authors explore several variants of this approach, including versions augmented with external knowledge bases and fine-tuned reasoning models. Experiments across a range of optimization problems demonstrate that the proposed method can outperform standard BO baselines in certain settings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The approach presents a novel integration of LLM reasoning with classical BO, showing measurable improvements on multiple benchmarks. The underlying concept is interesting and opens up many promising directions for future research and further improvements.\nThe proposed framework is flexible and general enough to be applied across diverse optimization domains, suggesting potential for scalability and cross-domain applicability. This is beneficial as this is a weakness of similar methods, which may not adapt to different circumstances in the same manner.\nThe authors provide many ablations on the model structure and design. There are many components that go into reasoning BO and comparing the effects of these changes is an important aspect of understanding the design."}, "weaknesses": {"value": "The clarity and flow of the paper is poor. Several implementation choices are mentioned but not detailed, making it difficult to reproduce or fully understand the system. The paper mentions SFT on an unspecified dataset and GRPO on GSMK. It is not clear if this went into the final model. The lack of clarity here is a big weakness. A clearer, more structured presentation would strengthen the work substantially.\n\nThe experimental section lacks consistency. The benchmarks and baselines vary across tables and do not provide a complete picture of the results, complicating interpretation. \n\nThe paper discussed data leakage as a concern but does not sufficiently address this aspect. It is unclear how much the results are motivated by data leakage. I think further analysis of the additional data inputted and the data used for fine-tuning would improve the paper."}, "questions": {"value": "The paper does not mention any limitations of reasoning BO. Are there limitations on data types, number of evaluations, evaluations that fail? Does reasoning BO perform worse than classical methods in some cases?\n\nCan you provide an ablation on the additional information provided to the model? How does it perform without additional information or in scenarios where there is not additional information.\n\nThe results on Ackley, Rosenbrock, and Hartmann are strong. However, I do not see information about what additional data is provided to the model for these cases or what relevant information would be justifiable to provide to the model. Can you justify why the model would perform so well without additional information?\n\nCan you provide more tests on more function spaces? The limited evaluation and lack of baselines makes the results less convincing. Providing a larger-scale test would help negate concerns that there is too much data leakage or unfair additional information passed to the model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZkvodJ2IDi", "forum": "mW3pn67O8Y", "replyto": "mW3pn67O8Y", "signatures": ["ICLR.cc/2026/Conference/Submission8863/Reviewer_uEhn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8863/Reviewer_uEhn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933021233, "cdate": 1761933021233, "tmdate": 1762920625983, "mdate": 1762920625983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for integrating knowledge into Bayesian optimization.\nThe main idea is to use a large language model (with reasoning capabilities after post-training) to (a) generate initialization and (b) score the generated candidates by the acquisition function and pick the most promising candidates."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": ". The empirical results on three biology datasets seems to be very competitive, which shows that BO successfully integrates the knowledge in LLMs."}, "weaknesses": {"value": "1. The method description needs to improve.\nThe main issue is that the description of the proposed method, particularly Algorithm 1, is extremely vague and confusing due to the lack of proper definitions.\n    - What are compass \\\\(C\\\\), overview \\\\(O\\\\), insights \\\\(I_t\\\\), and reasoning data \\\\(D_t\\\\)?\n    No context is provided.\n    There is no way to understand this part of the main paper without looking at the examples in Appendix C.\n    Even so, the examples in the appendix are excessive long to fully grasp.\n    - It is also not clear what is exactly stored in the database and/or knowledge graph.\n    Again, it is largely due to the lack of proper definitions.\n    What data get stored in the database, and what data get stored in the knowledge graph?\n\n1. Also, after skimming through the example prompts in the appendix, I feel like the proposed method (with so many complicated components and prompts) is specifically designed for the biology tasks.\nIt is not entire clear if the methodology in this paper is general enough to be transferred to other domains.\n(I don't see example prompts and example LLM outputs for the synthetic benchmarks used in the experiments.)\nI am not surprised that the strongest empirical results are obtained on three biology datasets (Table 1), which the empirical results on other benchmarks are a bit mixed.\nAlso, using LLMs on these biology datasets probably makes more sense because biology knowledge is rich during LLM pretraining.\n\n1. Plots in the experiments do not entirely make sense.\n    - E.g., the caption in Figure 3 says the y-axis is the best observed objective value.\n    Thus, the curves are supposed to be monotonic.\n    Why do the curves oscillate so much?\n    - Why does reasoning BO converge much faster on 15D Ackley than 2D Ackley?\n    For 2D Ackley, reasoning BO takes around 20 iterations to first reach the global optimum; see Figure 3.\n    However, reasoning BO finds the global optimum in fewer than 10 iterations on 15D Ackley; see Figure 5."}, "questions": {"value": "1. In each BO iteration, the acquisition function generates \\\\(5\\\\) candidates, and then the LLM scores the candidates to pick the best \\\\(3\\\\).\nI find these numbers very arbitrary.\nIt would be good to include some ablation studies on this, e.g., change the number of candidate proposals."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RJFbeyby8H", "forum": "mW3pn67O8Y", "replyto": "mW3pn67O8Y", "signatures": ["ICLR.cc/2026/Conference/Submission8863/Reviewer_RRPq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8863/Reviewer_RRPq"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983551669, "cdate": 1761983551669, "tmdate": 1762920625597, "mdate": 1762920625597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}