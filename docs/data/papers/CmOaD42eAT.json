{"id": "CmOaD42eAT", "number": 20247, "cdate": 1758304152738, "mdate": 1759896988536, "content": {"title": "Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs", "abstract": "We investigated Agentic RL with large language models on the TravelPlanner benchmark. Our approach, Planner-R1, achieved a 56.9% final-pass rate with only 180 training queries, a 2.7× improvement over GPT-5’s 21.2% baseline and the strongest agentic result on the public leaderboard. A central finding was that smaller models (8B) were highly responsive to reward shaping: with dense process-level signals, they reached competitive performance while being 3.5× more compute-efficient and 1.5× more memory-efficient than 32B models. Larger models were more robust under sparse rewards but exhibited smaller relative gains from shaping and higher variance across runs. While curriculum learning offered no significant benefit, shaped rewards consistently amplified learning dynamics, making 8B models the most efficient setting for agentic RL. Crucially, these gains did not come at the cost of overfitting: fine-tuned models mostly maintained or exceeded baseline performance on out-of-domain tasks, including Multi-IF, NaturalPlan, and Tau-Bench. These results establish reward shaping as a decisive lever for scaling agentic RL, highlight the competitive strength of smaller models, and demonstrate that efficiency can be achieved without sacrificing generalization.", "tldr": "Planner-R1 hits 56.9 % on TravelPlanner with 180 queries (2.7× GPT-5), sets the strongest open-weight agentic result, and shows reward shaping makes 8B models 3.5× more compute-efficient while still generalizing beyond training.", "keywords": ["Agentic RL", "LLM Planning", "Curriculum Learning", "Multi-step tool use", "SLM as Agent"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/185c502975dc4ce59f0a9157c794afa315203d92.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper targets an important and timely problem — how to use reward shaping to improve the reinforcement learning (RL) process of large language models (LLMs) on long-horizon, tool-based reasoning tasks such as TravelPlanner. The core objective is to **enable smaller LLMs (e.g., 8B models) to achieve competitive performance through efficient post-training**. This aligns well with current practical needs for LLM deployment, where lightweight and efficient models are increasingly desirable.\n\nThe authors propose a lightweight combination of multi-stage reward shaping and curriculum learning to make small-scale LLMs learn effectively without depending on large compute budgets. This is a highly meaningful and practically relevant goal.\n\nHowever, in my view, the current version still feels incomplete as a scientific contribution. While it demonstrates strong performance, it lacks theoretical depth and sufficient justification of generality, making it read more like an engineering case study rather than a full research paper. Of course, a well-executed engineering case study can still be valuable, but in this paper, the experiments are limited to a single domain (TravelPlanner), which **makes it difficult to evaluate whether the proposed method is truly general or simply tailored for this specific environment**."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "**Strong empirical results**: The performance on the TravelPlanner benchmark is impressive relative to existing methods. The experimental setup is robust, with well-documented configurations, comprehensive ablation studies, and both in-domain and out-of-domain evaluations.\n\n**Practical and relevant direction** : The goal of combining small models with efficient post-training is aligned with current industry deployment constraints (where vast compute is infeasible). The motivation is clear, and the paper addresses an important angle of efficient “agentic” fine-tuning."}, "weaknesses": {"value": "1. **Not clearly distinguishing from existing work** : Reward shaping is a long-established technique in RL, and it’s unclear what fundamentally differentiates the proposed method from prior works. The authors should explicitly explain how their reward scheme differs from previous methods like ORSO: Accelerating Reward Design via Online Reward Selection and Policy Optimization, which also adapts reward functions across training stages. (Of course, you target on a different task, as they just pay attention on the traditional RL setting, you focus on LLM post-training). As currently written, this work largely reuses existing ideas (multi-stage shaping, curriculum reward schedules) without introducing new algorithmic or theoretical insights.\n\n2. **Missing theoretical justification**: The paper introduces a multi-stage reward design that spans from fine-grained micro-level reward components to sparse terminal rewards. However, there is no theoretical guarantee that the shaped reward preserves optimal-policy consistency. Alternatively, the authors could have shown that the proposed reward achieves provably better convergence speed or asymptotic performance than sparse rewards. Without such evidence, this work resembles an empirical best-practice report, not a complete academic study. (An empirical best-practice report can also be valuable if the method demonstrates general applicability across multiple tasks. However, since the proposed approach is evaluated only on the TravelPlanner benchmark, its generalizability remains unclear -- which I discuss in weakness 3)\n\n3. **Generalization and scope are insufficient**: While demonstrating strong results on TravelPlanner, the method is validated on only one domain. This makes it hard to claim generality. From an engineering standpoint, designing a handcrafted reward function for a specific domain can always yield improvement; what makes it academically meaningful is showing that the reward design principle generalizes. The paper does not currently establish that generality."}, "questions": {"value": "I have mentioned the major weakness and related questions in the weakness. Here is some additional small but also important questions.\n\n1. **Reward specification lacks clarity**: In Section 2.2 (“Multi-stage reward”), the authors define several reward components (e.g., \\(r^{micro}_{hard}\\): fraction of satisfied hard constraints), but they never specify *which constraints* these rewards cover, nor how the constraint satisfaction is computed.  \n\n2. **Confusion in stage definitions**: While Section 2.2 defines six distinct reward components, the training procedure includes only three stages (dense, category-level, and sparse). The rationale for maintaining six separate sub-rewards is unclear—for instance, if $(r_{cs}^{micro}\\)$ and $(r_{hard}^{micro}\\)$ are consistently enabled or disabled together, defining them separately seems redundant.\n\n3. **Confusion in training**: Moreover, Section 2.3 (Optimization) never explains how or when the model transitions between these stages during training. The details of this curriculum scheduling (trigger condition, number of steps, etc.) should be explicitly clarified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bRKLsIOBOP", "forum": "CmOaD42eAT", "replyto": "CmOaD42eAT", "signatures": ["ICLR.cc/2026/Conference/Submission20247/Reviewer_auEt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20247/Reviewer_auEt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760777352982, "cdate": 1760777352982, "tmdate": 1762933735998, "mdate": 1762933735998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates agentic reinforcement learning (RL) with large language models (LLMs) on the TravelPlanner benchmark. The proposed approach, Planner-R1, achieves a 56.9% final-pass rate with only 180 training queries, representing a 2.7× improvement over GPT-5’s baseline. A key finding is that smaller models (8B) perform competitively with dense reward shaping, achieving better compute and memory efficiency compared to larger models (32B). While larger models are more robust under sparse rewards, smaller models show greater responsiveness to dense shaping. Shaped rewards enhance learning dynamics without compromising generalization, as evidenced by strong out-of-domain performance. The paper concludes that reward shaping is a decisive factor for scaling agentic RL efficiently."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The study provides a new empirical finding regarding reward shaping’s role in improving the efficiency of smaller models, which could inspire future work in agentic RL.\n2. Despite being empirical in nature, the paper conducts a significant amount of experimentation, including out-of-domain evaluations, to support its conclusions."}, "weaknesses": {"value": "1. The paper primarily focuses on empirical observations without proposing fundamentally new methods or theoretical insights, making it more of an empirical study than a groundbreaking contribution.\n2. The findings, while interesting, lack sufficient explanation or justification for why smaller models benefit more from reward shaping, why larger models show higher variance, and why curriculum learning offers no benefit. These gaps leave the conclusions less convincing and potentially unstable.\n3. While the paper claims that fine-tuned models generalize well, the results could still reflect subtle overfitting tendencies, especially given the high variance in larger models and the seemingly counterintuitive behavior compared to broader trends in scaling laws for LLMs.\n4. Only 8B and 32B are tested in detail while larger size models are not included. Only two sizes may not be enough to get the conclusion."}, "questions": {"value": "1. How stable are the findings across different seeds, training runs, or benchmarks? Given the high variance in larger models, what steps were taken to ensure the observed improvements are not the result of chance?\n2. The observation that smaller models (8B) are more efficient and competitive under dense rewards seems to contradict the general trend of larger models excelling in complex tasks. Did you observe any limitations of smaller models in other settings, or do these results suggest a broader pattern?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fP8VHa4I4J", "forum": "CmOaD42eAT", "replyto": "CmOaD42eAT", "signatures": ["ICLR.cc/2026/Conference/Submission20247/Reviewer_zmo9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20247/Reviewer_zmo9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904168027, "cdate": 1761904168027, "tmdate": 1762933735761, "mdate": 1762933735761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Overall, I'm quite positive on this paper. The authors tackle a very timely problem: the trade-off between model size, compute efficiency, and performance in agentic RL. The core claim is fascinating: they show that a smaller 8B model (PLANNER-R1-8B), when trained with dense process-level rewards, can achieve performance (56.4%) on par with a much larger 32B model (56.9%) on the difficult TRAVELPLANNER benchmark, while being 3.5x more compute-efficient. This provides some of the first strong empirical evidence for the \"SLM-First\" hypothesis in agentic AI.\n\nThe paper's strengths are its high significance, strong experimental quality (especially the inclusion of OOD generalization tests), and a very novel analysis of the interaction between model scale and reward density.\n\nHowever, the paper is held back by two major flaws: (1) It completely fails to cite or discuss several critical, highly relevant prior works that use the exact same algorithms and models, which calls its novelty claims into question. (2) It presents fascinating but negative results (e.g., the failure of curriculum learning and the total collapse of the 8B model on sparse rewards) and then fails to analyze or even discuss them. This second point is a deep intellectual flaw, as it ignores a central contradiction in the paper's own findings.\n\nI am \"on the fence\" and my final recommendation will depend entirely on how the authors address the major weaknesses and questions below in their rebuttal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Significance:** This is the paper's greatest strength. The field has been debating the role of SLMs vs. LLMs for agentic tasks. This debate was largely hypothetical, kicked off by position papers like the one from NVIDIA (Belcak et al., 2025) which argued that SLMs were the future. This paper, \"PLANNER-R1,\" provides what is, to my knowledge, the first strong empirical evidence that this hypothesis is correct, at least on a complex, constraint-driven task. Figure 3, showing the 8B model achieving 90% of the 32B's peak performance for a fraction of the FLOPs, is a hugely important result for the community. It moves the \"SLM-First\" idea from a \"position\" to an \"empirically-backed strategy.\" This is a really big deal.\n\n2. **Quality:** The experimental quality here is strong. The authors report means and 95% CIs over 5 runs, which is respectable. What I especially appreciate is the inclusion of out-of-domain (OOD) generalization tests in Table 2. This is a critical step that many RL-finetuning papers omit. By showing that their models don't suffer catastrophic overfitting and maintain performance on tasks like NATURALPLAN and T-BENCH, they proactively address the most common criticism of this kind of task-specific tuning. The qualitative analysis in Appendix A is also insightful.\n\n3. **Originality:** The originality here is nuanced, but significant. To be clear, the components themselves are not new. The field was already using GRPO and VERL, and the idea of using process-based rewards over outcome-based rewards was already the consensus. The real original contribution, in my opinion, is the paper's analysis of the interaction between model scale (8B vs 32B) and reward density (Stage 1 vs Stage 3).\n\n   The key new piece of knowledge from this paper is the discovery of an asymmetric response to reward shaping. Look at Table 1:\n\n   - The 8B model is extremely sensitive. It gets 39.9% on the dense Stage 1 reward but completely collapses to 0.0% on the sparse Stage 3 reward.\n   - The 32B model is robust. It gets 42.3% on Stage 1 and still gets 44.3% on Stage 3.\n      This finding—that SLMs have this asymmetric, sensitive response to rewards—is novel and, frankly, very valuable. It tells us how to train SLMs effectively.\n\n4. **Clarity:** The paper is just very well-written. The problem formulation as an MDP in 2.1 is clean. The multi-stage reward function in 2.2 is well-motivated and easy to understand. The figures are clear and directly support the main claims."}, "weaknesses": {"value": "But this is where my major problems start. The paper has two significant flaws that prevent me from recommending acceptance in its current state.\n\n1. **Missing Key Citations & Discussion (Major Flaw):** The paper has a massive blind spot regarding related work, to the point where it seriously undermines the claims of novelty. The authors must address this.\n\n   - First, 'ToolRL' (Qian et al., 2025): This paper is not cited. \"ToolRL\" is explicitly self-described as the \"first comprehensive study on reward design for tool selection... within the RL paradigm\". It also uses GRPO and also concludes that \"fine-grained reward decomposition leads to more stable and effective learning\". This is so similar to this paper's premise that not citing it is a serious omission.\n   - Second, 'PilotRL' (Sheng et al., 2025): This paper is also not cited. \"PilotRL\" uses almost the exact same technical stack: it explicitly trains a Qwen3-8B model using GRPO on the VERL framework for planning tasks.\n\n   **Constructive Suggestion:** The authors must add a paragraph to their Related Work (Section 5) that discusses these papers. They need to clearly differentiate their work. The most obvious differentiator is that \"PLANNER-R1\" is the first to do a rigorous comparative analysis of reward-density-vs-model-scale (8B vs 32B) on a hard benchmark like TRAVELPLANNER, whereas the other papers focused on either the reward design itself (ToolRL) or the application of the stack (PilotRL). But as written, the paper implies it was the first to do any of this, which is incorrect.\n\n2. **Severe Lack of Analysis on Negative Results (Major Flaw):** My other major, major flaw with this paper is its complete failure to analyze its own negative results. The authors present two fascinating findings in Table 1, and then... just ignore them. This is not good scientific practice.\n\n   - 1. The Failure of Curriculum Learning: The paper states, \"Curriculum learning... provided no measurable benefit\". This is an understatement. Table 1 shows the 8B-Curriculum model got 27.1%, which is dramatically worse than the simple 8B-Stage1 model's 39.9%. This is highly counter-intuitive; curriculum strategies are supposed to help! Why did it fail so badly? Did the model experience catastrophic forgetting? Was the transition between stages too abrupt? The authors offer zero hypotheses. This is a fascinating negative result, and they just walk past it.\n   - 1. The Fragility of the 8B Model: This is the most important one. As I said in 'Strengths', the 8B model's performance completely collapses (0.0% success) when trained on the sparse Stage 3 reward, while the 32B model is fine (44.3%). This reveals a deep and critical limitation to the paper's entire thesis. The authors' \"SLM-First\" efficiency argument completely depends on having access to expensive, dense, process-level rewards.\n\n   This is a core contradiction that is never discussed. The authors portray SLMs as the cheap option, but their results prove SLMs depend on the most expensive type of reward signal. How do we get these dense rewards for new tasks? That's the billion-dollar question! This limitation is central to the paper's claim and must be discussed in the Limitations section.\n\n3. **Slight Overstatement of Generalization (Minor Flaw):** This is a more minor point, but the abstract and discussion claim the gains came \"without sacrificing generalization\". However, Table 2 itself shows this isn't quite true. The `Planner-R1-8B (3000 steps)` model shows a statistically significant drop in performance on two OOD tasks: `NATURALPLAN (Trip)` (12.9 down to 10.7) and `NATURALPLAN (Meeting)` (22.7 down to 20.1). This suggests that while moderate finetuning is fine, \"excessive fine-tuning\" (as the authors call it) does begin to cause overfitting. This is a slight exaggeration and should be toned down to be more precise."}, "questions": {"value": "My questions are aimed at getting the authors to address the weaknesses I've listed.\n\n- **Q1 (On Novelty):** Can you please clarify the exact novel contribution of your work given that 'ToolRL' (Qian et al., 2025) already did a comprehensive study of reward design for tools with GRPO, and 'PilotRL' (Sheng et al., 2025) already applied the Qwen3-8B+GRPO+VERL stack? Am I correct in thinking your core novelty is specifically the 8B-vs-32B comparative analysis on TRAVELPLANNER?\n- **Q2 (On Curriculum Learning):** I was fascinated by the failure of your curriculum learning strategy (Table 1), where the 8B-Curriculum (27.1%) performed much worse than the 8B-Stage1 (39.9%). This is a very interesting negative result. Why do you think this happened? Do you have any hypotheses (e.g., catastrophic forgetting)?\n- **Q3 (On SLM Reward Dependence):** This is my biggest question. Your results show the 8B model collapses (0.0% performance) on sparse rewards, while the 32B model is robust (44.3%). This implies your impressive efficiency gains for SLMs are entirely dependent on having access to dense, process-level rewards, which are notoriously expensive and difficult to create for new tasks. How do you reconcile this? Doesn't this limit the practical utility of your \"SLM-First\" approach to only tasks where we can already afford to create dense process rewards?\n- **Q4 (On 32B Variance):** You note that the 32B model exhibited \"higher variance\", and the CIs in Table 1 confirm this (e.g., 44.3 ± 14.1). Why do you think the larger model was less stable during training? That seems counter-intuitive.\n- **Q5 (On the Solver Gap):** You correctly position your 56.9% as SOTA for end-to-end agents. But you also note that the hybrid \"solver\" method (Hao et al., 2025) gets ~94-97% on this same benchmark. Do you believe that this massive ~37% gap can ever be closed by pure RL agent methods (e.g., with more data or better algorithms), or does this gap suggest that for hard-constraint tasks, hybrid neuro-symbolic methods will always be superior?\n- **Q6 (On Disabling \"Thinking\"):** You mention you disabled the Qwen3 `<think>` token due to memory/context constraints. Since this \"thinking\" is a form of internal monologue (like Chain-of-Thought), is it possible this crippled the 32B model's ability to use the process-level rewards? In other words, perhaps the 32B model needed that \"thinking\" scratchpad to properly internalize the dense reward signals, and by removing it, you inadvertently made it look less responsive to reward shaping than it truly is?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fm39ci2jJi", "forum": "CmOaD42eAT", "replyto": "CmOaD42eAT", "signatures": ["ICLR.cc/2026/Conference/Submission20247/Reviewer_SV5k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20247/Reviewer_SV5k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967547872, "cdate": 1761967547872, "tmdate": 1762933735316, "mdate": 1762933735316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PLANNER-R1 studies RL for tool-augmented planning with LLMs on the TRAVELPLANNER benchmark. The authors show that dense reward shaping enables stable and efficient RL training, especially for smaller models like Qwen3-8B, which become competitive with much larger models at a fraction of the compute. They analyze hallucinations, tool sequencing, and constraint violations, and evaluate generalization across several out-of-domain tasks. The final PLANNER-R1 models substantially outperform GPT-5 in final-pass accuracy and do not exhibit overfitting. The paper argues that careful reward design is key for scalable, sample-efficient agentic RL and effective tool-use planning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. PLANNER-R1 achieves a notably higher final-pass rate compared to GPT-5, reaching 56.9 percent versus 21.2 percent. This is a considerable improvement in a challenging tool-augmented planning setting. The magnitude of the gain highlights the effectiveness of the proposed RL approach and positions the method as a meaningful advancement for agentic LLM planning.\n2. The paper provides detailed examinations of common failure modes, including hallucinations, tool-use sequencing errors, and violations of hard or commonsense constraints, and tracks how these evolve throughout RL training. These analyses help clarify not only why the RL signal improves policy behavior but also how different model scales respond during learning. This level of insight strengthens the paper by making the empirical results more interpretable and actionable.\n3. The evaluation on external benchmarks demonstrates that RL fine-tuning does not lead to overfitting to TRAVELPLANNER’s schema or task structure. Instead, the fine-tuned models often match or exceed their pretrained baselines across multiple out-of-domain tasks. This contrasts with conclusions from earlier RLVR-style work, where RL often harmed transferability. The results here suggest that, with well-designed rewards, RL can generalize more broadly and support effective planning and tool-use behaviors beyond the training environment, which is an encouraging direction for future research."}, "weaknesses": {"value": "1. The paper reports several intriguing qualitative observations, such as the 8B model being highly sensitive to sparse rewards while the 32B model remains stable, and the 8B model achieving significantly better FLOPs efficiency. However, the authors stop at reporting these patterns without attempting to analyze or explain the underlying causes. These trends are not intuitively obvious, and without further discussion it is unclear whether they arise from architectural properties, optimization dynamics, or idiosyncrasies of the TRAVELPLANNER environment. This makes it difficult for readers to assess whether these findings would generalize to other domains or remain benchmark-specific.\n2. Reward shaping is central to the paper’s claims, yet the discussion of the reward function is limited. The paper would benefit from ablations isolating the contribution of each reward component, or at least a more detailed discussion of how the shaping coefficients were chosen and tuned. Since reward design is critical to reproducing the reported improvements, more transparency would provide valuable guidance for practitioners looking to transfer these ideas to new tasks or environments.\n3. igure 4 is difficult to interpret. The 32B model appears to exhibit higher average failure rates across several categories, yet achieves a higher final-pass rate. This seemingly contradictory behavior raises questions: which failure types meaningfully affect the final evaluation metric, and which do not? Are some hallucinations or constraint violations “benign” in the sense that they do not influence schema validity or final constraints? The paper should clarify how these failure metrics relate to task success, otherwise the figure may confuse readers and undermine the interpretability of the qualitative analysis.\n4. Although the paper lists the RL benchmark formulation as a key contribution, it offers limited detail beyond high-level framing. The work would benefit from clearer articulation how the environment proposed by the paper differ from the original TRAVELPLANNER setup."}, "questions": {"value": "1. What underlying factors explain why the 8B model is highly sensitive to sparse rewards while the 32B model remains stable, and why the smaller model achieves superior FLOPs efficiency? Are these effects specific to TRAVELPLANNER, or should we expect similar patterns in other domains?\n2. How were the reward coefficients selected and tuned, and what is the contribution of each reward component? Can the authors provide ablations or design principles that would help practitioners adapt the reward function to new environments?\n3. Why does the 32B model show higher average failure rates yet obtain higher final-pass performance? Which types of failures meaningfully impact the final metric, and which ones are effectively ignored or compensated for during evaluation?\n4. In what specific ways does the proposed RL benchmark formulation differ from the original TRAVELPLANNER environment, and what elements of it constitute the claimed contribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nyvGw8GlqF", "forum": "CmOaD42eAT", "replyto": "CmOaD42eAT", "signatures": ["ICLR.cc/2026/Conference/Submission20247/Reviewer_1Vmv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20247/Reviewer_1Vmv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20247/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762664333351, "cdate": 1762664333351, "tmdate": 1762933735035, "mdate": 1762933735035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}