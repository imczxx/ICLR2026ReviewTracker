{"id": "Ae4eZpkXBX", "number": 17100, "cdate": 1758272110025, "mdate": 1763466661811, "content": {"title": "Fast Escape, Slow Convergence: Learning Dynamics of Phase Retrieval under Power-Law Data", "abstract": "Scaling laws describe how learning performance improves with data, compute, or training time, and have become a central theme in modern deep learning. We study this phenomenon in a canonical nonlinear model: phase retrieval with anisotropic Gaussian inputs whose covariance spectrum follows a power law. Unlike the isotropic case, where dynamics collapse to a two-dimensional system, anisotropy yields a qualitatively new regime in which an infinite hierarchy of coupled equations governs the evolution of the summary statistics. We develop a tractable reduction that reveals a three-phase trajectory: (i) fast escape from low alignment, (ii) slow convergence of the summary statistics, and (iii) spectral-tail learning in low-variance directions. From this decomposition, we derive explicit scaling laws for the mean-squared error, showing how spectral decay dictates convergence times and error curves. Experiments confirm the predicted phases and exponents. These results provide the first rigorous characterization of scaling laws in nonlinear regression with anisotropic data, highlighting how anisotropy reshapes learning dynamics.", "tldr": "", "keywords": ["scaling laws", "gradient flow", "power-law spectrum", "phase retrieval", "anisotropic data", "learning dynamics"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9e01b7faf48f72b5f84de53394579f585435ba41.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors rigorously study and characterize the gradient flow dynamics of the population loss for phase retrieval in a setting where the data are drawn from a normal distribution whose covariance matrix has eigenvalues following a power-law distribution. \nDue to the combined effects of nonlinearity and anisotropy across learning directions, they show that the dynamics are qualitatively different from cases where only one of these aspects is present.\nAnalytically, they find that the dynamics are governed by an infinite hierarchy of coupled ODEs for the order parameters. Using these equations, they are able to describe the entire learning trajectory and rigorously distinguish three distinct phases, in agreement with numerical experiments. In particular, they reveal that the dynamics are generally slower than in the isotropic case, and that the final phase is controlled by the smallest eigenvalues of the covariance matrix, corresponding to the hardest directions to learn. In this final regime,\nthey obtain an analytical scaling law describing how the mean-squared error decays with training time, showing that the decay exponent is determined by the power-law distribution of the covariance spectrum."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The setting studied in this work allows for a complete and rigorous analysis of the gradient flow of the population loss in a case where both a nonlinearity in the regression problem and a power-law distribution of the data are present.\nTo my best knowledge, this is the first rigorous paper to do so. \nThe analytical description is very thorough and matches very well the numerical simulations, allowing one to build a general intuition about the interplay between these two elements."}, "weaknesses": {"value": "The setting is intentionally specific to enable a rigorous analysis since the authors use both the generalization risk (whose landscape is devoid of spurious minima) and the gradient flow approximation, which does not directly capture discrete-time effects of an actual gradient descent. \nMoreover, while a scaling law is analytically found in the third regime, it is not directly linked to the classical scaling laws that connect generalization error with the number of data points (here assumed infinite in the population loss) and the number of parameters (here equal to the input dimension and also tending to infinity). \nThe technical sections are quite dense, making the paper more difficult to read in that part."}, "questions": {"value": "While the works of Hestness, Kaplan, and Hoffmann are cited as motivation, those scaling laws relate error to number of data, number of parameters and discrete FLOPs. How do you see your dynamical scaling laws connecting conceptually or quantitatively to the empirical ones?\n\nIn ”Escaping mediocrity” (Arnaboldi, Krzakala, Loureiro, Stephan, 2023a), the authors analyze how online SGD benefits from over-parametrization (among other factors) to accelerate escape. Do you expect similar over-parametrization gains to influence the phase structure you observe here, for example, by modifying the escape from mediocrity stage or the later spectral-tail phase?\n\nDo you have insights on the qualitative differences there would be there would be if one studied the dynamics where the student norm (the Frobenius norm) is kept fixed to the same one of the teacher, so that the dynamics only aligns the vector w to w*?\n\nThere is a minor graphical bug in Figure 2 that causes the labels in the x axis to overlap."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b6BHEb9wJF", "forum": "Ae4eZpkXBX", "replyto": "Ae4eZpkXBX", "signatures": ["ICLR.cc/2026/Conference/Submission17100/Reviewer_8Pxo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17100/Reviewer_8Pxo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17100/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820162705, "cdate": 1761820162705, "tmdate": 1762927103605, "mdate": 1762927103605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the learning dynamics of gradient descent (GD) in the context of phase retrieval with anisotropic gaussian inputs. The main technical approach is based on reducing the coupled dynamics of an infinite hierarchy of moments to a Volterra integral equation, and then obtaining solution bounds and asymptotics via Laplace-type analysis. \n\nThe main conceptual result, supported by theory and numerical experiments, is that learning roughly exhibits 3 phases: \n- Escape: The weights slowly escape the small gradient regime near \\$w=0\\$ and acquire nontrivial overlap with the ground truth weights. \n- Convergence: The relevant summary statistics converge toward their correct population values while the loss converges toward 0. The authors subdivide this phase into two qualitatively distinct subphases. \n- Spectral tail learning: The authors finally consider a second metric of learning, the MSE of the learned weights. At the beginning of phase 3, the MSE remains high despite low fitting loss. The MSE decreases through phase 3 at a rate that depends on the spectral decay of the data covariance; as one might expect, flatter spectra - i.e. smaller decay exponents - give rise to faster learning in this phase. \n\nAnother conceptual result that is discussed only very briefly is a tradeoff between fast initial learning and fast tail learning. Roughly speaking: low rank data drives fast initial alignment but leads to slow learning of small-eigenvalue directions. \n\nThe bulk of the paper is devoted to describing the analytical results and outlining the proof strategy, though the authors provide empirical support for their results through limited numerical experiments (2 main figures and several supplemental figure panels)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper studies an important issue in learning theory. It considers a nontrivial, currently poorly understood, setting combining model nonlinearity and data anisotropy - both of which are expected to affect the detailed behavior of neural scaling laws. The technical analysis of the paper is interesting and rigorous, and is likely to be applicable in future work. The main theoretical conclusions of the paper are intuitive and appealing, and are supported by limited experimental verification/exploration. Finally, the paper is well written, and the derivations are relatively easy to follow."}, "weaknesses": {"value": "**Robustness of conclusions and applicability to more general settings**. The paper obtains satisfying conclusions about the qualitative behavior of GD dynamics in phase retrieval. However, the neat 3-phase picture appears to depend somewhat on the loss structure of the specific problem under consideration. Currently, the paper does not appear to give any reason - even hand waving - to suspect that more complex models will exhibit escape, convergence, and/or tail learning phases resembling those of phase retrieval. The extent to which these specific results generalize is therefore unclear. This issue is especially concerning given that the results, while theoretically interesting, are likely to be of limited practical relevance in the specific context of phase retrieval, given the wealth of published fitting algorithms that significantly outperform (S)GD. \n\nRelatedly, I have a question/concern about the dependence of the results on the particular initialization scheme studied in the paper (see detailed questions). \n\n**Impact of anisotropy**. Data anisotropy is the main novel ingredient of the analysis. While this introduces novel and interesting technical challenges for the analysis, it appears to have a relatively simple effect on the learning curves. In particular, I don’t feel convinced that anisotropy “leads to a completely different picture from the isotropic intuition” as stated in the paper. The paper would benefit from more explicit discussion of the impact of anisotropy, especially relative to the escape/convergence tradeoff referenced in the title. (Right now, discussion of this result appears to be limited to two sentences in the Contributions section.) See detailed comments below."}, "questions": {"value": "**Major comments/questions**\n\n1. Clarify/amplify role of anisotropy\n    - The title and contributions section make reference to a tradeoff in escape and tail learning speeds. This tradeoff is not discussed further in the main text, and is not illustrated in any of the main figures. Given the prominence of this result, I feel it deserves more attention. \n    - It took a careful reading to understand what was being traded off - ie. “escape from mediocrity” and “convergence to low risk” (line 091). These terms ought to be defined clearly up front. This confusion was related to the fact that the paper studies two distinct measures of performance. See comment 2. \n    - In light of the results, I feel that the description of the anisotropic setting as a “completely different picture from the isotropic intuition” (line 087) is too strong. It seems more consistent with the results to say that anisotropy leaves the dynamics qualitatively unchanged while determining the precise scaling exponents and phase transition times. \n2. Up until Section 4.2, it is not obvious on first reading that the paper studies two distinct measures of performance - population loss and MSE. Several points of confusion could be avoided by making this clear early on. I think the MSE should also be defined alongside the population loss in Section 2 - Problem Setup, and that the text should state explicitly that the analysis studies both quantities, which become relevant in different phases of learning. \n3. Escape phase\n    - The concept of an escape phase in phase retrieval is apparently well known (eg. cited paper Ben Arous et al., 2025). I feel this is worth mentioning when describing the escape phase on line 256. \n    - I am somewhat confused by the initialization scheme. In particular, I don’t understand why the authors have $|w^\\star| = O(\\sqrt{d})$ but initialize the weights with $|w|=1$. This leads to issues later on: Remark 1 seems to suggest that the escape phase occurs because the initial gradient is small, in turn because $u(0)\\sim d^{-1/2}$. However, the third term of 3.1 has both $u$ and $w^\\star$, which has norm $d^{1/2}$. It’s not clear that the alignment term of the gradient is actually small relative to others. A very different explanation is given on line 256, which points out that since $w(0)\\approx 0$, the weights start out near the local maximum at $0$, and thus require long times to escape. Please clarify. (To my mind, the simplest initialization has both $w,w^\\star$ with norm $\\sqrt{d}$. Then $u(0)\\approx d^{-1/2}$ implies the alignment term in the gradient is small relative to the first two “norm terms”. Gradient descent quickly sends $s\\to 1/3$, and then one has a slow “alignment phase” wherein $u$ increases toward 1.)\n4. The authors should briefly describe the approximation scheme used in the phase analysis, perhaps around the statement of the ODEs in eqs 3.3-3.5. In particular, the analysis proceeds by assuming simple forms for $s(t),u(t)$, obtaining expressions for the simplified dynamics, and then bounding the approximation error. The initial approximation $\\Theta(t)\\approx t$ corresponds to $s(t)=0$ - ie. initially small weights, while the phase 3 approximation corresponds to $s(t)=u(t)=1$ - ie. perfect convergence of macroscopic overlaps. \n5. The authors may want to comment on the meaning, relevance, and broader applicability of the division of Phase II into two subphases. Is there any broader conclusion to be drawn, or is this merely a peculiarity of phase retrieval?\n\n\n**Minor**\n1. Line 083-084 uses uppercase $T$ for time while later equations use $t$. \n2. Line 323 typo (growth -> grows)\n3. Line 413 / eq. 5.4 - I don’t see a definition for $\\alpha$ anywhere in the main text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vsbUUFMEwJ", "forum": "Ae4eZpkXBX", "replyto": "Ae4eZpkXBX", "signatures": ["ICLR.cc/2026/Conference/Submission17100/Reviewer_aHGn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17100/Reviewer_aHGn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17100/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828332442, "cdate": 1761828332442, "tmdate": 1762927103078, "mdate": 1762927103078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We thank the reviewers for their thoughtful feedback. Several recurring themes emerged, which we address below and have incorporated into the revised manuscript. Individual comments are answered in detail in the point-by-point rebuttal.\n\nMore than one reviewer noted the simplicity of the model studied. While phase retrieval on anisotropic, power-law data is a specific setting, it captures the essential theoretical challenges that arise in broader problems of interest:\n\n- **Non-convexity:** Classical optimization primarily focuses on convex objectives or provides only local guarantees. Phase retrieval is a fundamentally non-convex problem that arises naturally in signal processing (Dong et al., 2023) and has become a standard testbed in theoretical ML for understanding non-convex optimization (Ma et al., 2021; Candès et al., 2015; Tan & Vershynin, 2019; 2023; Davis et al., 2020; Sarao Mannelli et al., 2020; Arnaboldi et al., 2023a; Martin et al., 2024; Erba et al., 2025; Ben Arous et al., 2025). Our work provides a global convergence rate for the convergence of GF.\n\n- **Anisotropy:** A central simplifying assumption in much of the literature above is isotropy of the data distribution. Rotational invariance reduces the SGD dynamics to a small set of concentrated variables, dramatically simplifying high-dimensional analysis (Ben Arous et al., 2022). Anisotropy breaks this structure and introduces substantial technical difficulties. Our work develops a new analytical approach to anisotropic, non-convex dynamics, which we believe can be extended to richer models such as two-layer neural networks and more general link functions.\n\n- **Scaling laws:** Understanding SGD convergence under anisotropic power-law data has recently gained traction as a model for neural scaling laws (Hestness et al., 2017; Kaplan et al., 2020; Hoffmann et al., 2022). Existing results, however, focus primarily on linear or isotropic settings. To our knowledge, our work is the first to analyze this question in a non-convex and anisotropic covariate model, uncovering a non-trivial trade-off between weak and strong learning that is specific to the nonlinear setting.\n\nIn summary, the setting we study is both relevant to the ICLR theoretical community and exhibits rich, nontrivial phenomena. The technical challenges addressed here are not straightforward extensions of prior work, and we believe the methods introduced are of independent value for analyzing more complex problems of interest."}}, "id": "lmwZnV4Lkh", "forum": "Ae4eZpkXBX", "replyto": "Ae4eZpkXBX", "signatures": ["ICLR.cc/2026/Conference/Submission17100/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17100/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17100/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763466687008, "cdate": 1763466687008, "tmdate": 1763466687008, "mdate": 1763466687008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper uses phase retrieval as a simple setting to study non- linear dynamics, i.e., learning a single-neuron with quadratic activation in the presence of noise. This is a non-convex problem with two global minimizers, a local max at 0 and a saddle manifold. As a result, the landscape is nice in the sense of no spurious minima, but anisotropy (power-law decay eigenvalues of the covariance Q) makes convergence slow: in particular, the parameter trajectory escapes quickly from low alignment but then converges slowly because of small eigenvalues."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "I think the key originality this paper brings is the analysis under anisotropy. I also think on the whole that the paper is well written and clearly explained. In terms of significance I think the identification of slow convergence with the tail of the distribution of the data is nice, particularly around the emergence of this third phase not present when $a=0$. I was also think the fact that a larger $a$ implies faster decay of the loss in the first phase but slower convergence in the third phase is interesting and a nice characterization. Finally the fact that for isotropic data you can just track the magnitude and inner product of the weights versus the target weights, but for anisotropic data this no longer holds and you must track an infinite tower of weighted overlaps instead (proposition 3), I also liked. The analysis appears to involve a nice combination of techniques."}, "weaknesses": {"value": "My only critique perhaps concerns the idealized setting, namely a quadratically activated neuron trained under full batch gradient flow on Gaussian data with power law decay. It is not clear to me how much of the narrative here applies elsewhere to other non-linear optimization problem settings of interest. As a result, I think the scope and general applicability of the results are not clear."}, "questions": {"value": "- On the necessity of Gaussian data: would the same dynamics hold if the inputs were drawn from a non-Gaussian but rotationally symmetric distribution say? Which parts of the proof genuinely require Gaussianity?\n\n- Can one compute the scaling of the loss explicitly in the tail phase as a function of  $a$?\n\n- How does this multiphase trajectory analysis connect to other problems in machine learning. Do you think you can gain any insight say into training of e.g., deep linear networks from this work and the observed phases of training there?\n\n- Your work focuses on gradient flow under anisotropy which is what preconditioning approaches like natural gradient or Adam try to fix. Have you considered or thought about performing a similar analysis with methods like this to see if they can ameliorate the issues of slow escape from a saddle and slow convergence to a global minimum? In short, this setup seems like a nice testbed for studying these two problems, escaping saddles and slow convergence along flat directions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Not applicable."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UBGxDIKsRU", "forum": "Ae4eZpkXBX", "replyto": "Ae4eZpkXBX", "signatures": ["ICLR.cc/2026/Conference/Submission17100/Reviewer_KHcf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17100/Reviewer_KHcf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17100/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929719402, "cdate": 1761929719402, "tmdate": 1762927102844, "mdate": 1762927102844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the theory of scaling laws of phase retrieval, a classical nonlinear regression problem. The authors start by asking \"How does the input spectrum govern finite-time convergence in nonlinear regression, and can we predict the learning curve from the spectral decay?\".\n\nTo answer the question, they consider the gradient flow of optimizing the MSE loss, which can be formulated as infinite-dimensional, continuous dynamical systems. The dynamics is further analyzed through different phases in Section 4, including escape, approximate convergence, and spectral-tail learning. In Section 5 the authors provide a sketch of their proof, and then they wrap up the work with some discussions on limitations and future directions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper gives a solid theoretical study of scaling laws of phase retrieval problems. \n\n2. The authors reveal that the problem can be viewed as infinite-dimensional dynamical systems, and can be analyzed by some math tools such as ODE, dynamical systems, etc."}, "weaknesses": {"value": "1. It is unclear how the dynamics of gradient flow of the phase retrieval problems can inspire the design of new algorithms for solving real-world, large-scale problems. Although this paper mainly focuses on theory, it might be beneficial to include some discussions on the connections between this toy problem and real-world ones.\n\n2. Most of the results include the big-O notations, meaning that the conclusions can only hold when certain numbers are sufficiently large/small. This might indicate that the theorems and propositions are a bit limited.\n\n3. The mathematical tools and conclusions are standard in theory lliterature."}, "questions": {"value": "1. Could the authors provide some discussions on the connections between the problem studied and real-world problems, such as how the reuslts would inspire new algorithms design for large-scale real-data problems.\n\n2. Are there any challenges of removing the big-O notations and asymptotics in theorems? For example theorem 1 requires d to be sufficiently large, how about the cases when d does not satisfy this?\n\n3. The analysis is mainly on the gradient flow. Would it be possible to analyze the discrete cases, i.e., gradient descent or even stochastic gradient descent algorithms, which might align better with real-world scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CXi0iTppYD", "forum": "Ae4eZpkXBX", "replyto": "Ae4eZpkXBX", "signatures": ["ICLR.cc/2026/Conference/Submission17100/Reviewer_gLFC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17100/Reviewer_gLFC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17100/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987371633, "cdate": 1761987371633, "tmdate": 1762927102562, "mdate": 1762927102562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}