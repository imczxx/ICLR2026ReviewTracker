{"id": "AcnCQR2ElW", "number": 6713, "cdate": 1757993213773, "mdate": 1759897899328, "content": {"title": "Quicksviewer: An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes", "abstract": "Large Multimodal Models (LMMs) uniformly perceive video frames, creating computational inefficiency for videos with inherently varying temporal information density. This paper present Quicksviewer, an LMM with new perceiving paradigm that partitions a video of nonuniform density into varying cubes using Gumbel Softmax, followed by a unified resampling for each cube to achieve efficient video understanding. This simple and intuitive approach dynamically compress video online based on its temporal density, significantly reducing spatiotemporal redundancy (overall 45$\\times$ compression rate), while enabling efficient training with large receptive field. We train the model from a language backbone through three progressive stages, each incorporating lengthy videos on average of 420s/1fps thanks to the perceiving efficiency. With only 0.8M total video-text samples for training, our model outperforms the direct baseline employing a fixed partitioning strategy by a maximum of 8.72 in accuracy, demonstrating the effectiveness in performance. On Video-MME, Quicksviewer achieves SOTA under modest sequence lengths using just up to 5% of tokens per frame required by baselines. With this paradigm, scaling up the number of input frames reveals a clear power law of the model capabilities. It is also empirically verified that the segments generated by the cubing network can help for analyzing continuous events in videos.", "tldr": "", "keywords": ["Video Understanding", "Large Multimodal Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b7807cdf19e52dbe6561cbf0adccdd0ff30629b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Quicksviewer, an innovative large multimodal model (LMM) designed for efficient video understanding by addressing the issue of nonuniform temporal information density in videos. Using a novel cubing approach based on Gumbel Softmax, the model dynamically partitions video frames and resamples them, enabling significant spatiotemporal compression (45×) while maintaining a large receptive field. Trained with only 0.8M video-text samples, Quicksviewer outperforms baseline models by a notable margin (up to 8.72 in accuracy) and achieves state-of-the-art performance on Video-MME with reduced token usage. Additionally, the model demonstrates scalable improvements with increased input frames and provides insights into analyzing continuous events in videos."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, with the methodology and experimental results clearly and systematically presented.\n2. The exploration of video token compression in this paper is highly meaningful, as it contributes to both improving long-video understanding and accelerating video processing.\n3. The proposed method demonstrates notable improvements over the baseline, achieving up to 8.72 gains in accuracy."}, "weaknesses": {"value": "1. While the paper introduces a visual token compression approach for videos based on LMMs, it lacks comparisons with other compression methods (e.g., SlowFast, token merging) under equivalent conditions, such as using the same training dataset, to better validate the advantages of the proposed strategy.\n2. Although the paper focuses on token compression, it only discusses token counts without analyzing key metrics like latency and memory usage. In practice, even with the same token count, a more complex method could lead to different performance in terms of latency and memory efficiency.\n3. The paper claims that Quicksviewer is a video understanding model, but most of the evaluations are based on video QA tasks, which typically have lower token requirements. To provide a more comprehensive assessment of its video understanding capabilities, the authors should include comparisons on other tasks like video captioning.\n4. The paper claims that Quicksviewer achieves state-of-the-art performance on several benchmarks, but the baselines used for comparison are relatively weak. For instance, on Video-MME, many existing models (e.g., Qwen VL, Intern VL) have already achieved scores of 60 or even 65+, whereas Quicksviewer only achieves 56.9. While differences in conditions such as training data and token counts exist, the claim itself is not rigorous and does not convincingly demonstrate that the proposed approach achieves high accuracy."}, "questions": {"value": "Please see the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zsTVSr6iF3", "forum": "AcnCQR2ElW", "replyto": "AcnCQR2ElW", "signatures": ["ICLR.cc/2026/Conference/Submission6713/Reviewer_6NpJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6713/Reviewer_6NpJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6713/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760692726343, "cdate": 1760692726343, "tmdate": 1762919002794, "mdate": 1762919002794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Quicksviewer, a method for efficient video understanding. The core idea is to dynamically and non-uniformly partition videos into variable-length cubes, enabling adaptive spatiotemporal token compression."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The use of a learnable cube network to dynamically and non-uniformly segment videos is both reasonable and novel.\n\n2. The adoption of Gumbel-Softmax allows for unsupervised, end-to-end boundary learning, avoiding the need for manual annotation."}, "weaknesses": {"value": "1. The claimed SoTA performance may be misleading, as the baseline methods used for comparison are relatively outdated given the rapid progress in long video understanding. While Table 2 reports Quicksviewer as SoTA, recent models on the Video-MME leaderboard (e.g., VideoChat-Flash, VideoLLaMA-3) achieve scores above 65 with comparable or even smaller model sizes, whereas Quicksviewer only achieves 56.9.\n\n2. The dynamic partitioning is content-aware but not query-aware. For queries requiring fine-grained details, pre-partitioning may result in the loss of critical information that cannot be recovered.\n\n3. More comprehensive comparisons with recent token compression and long video understanding methods are needed, along with deeper analysis."}, "questions": {"value": "The main concern is the potentially misleading reporting of SoTA results. For a top-tier conference like ICLR, the authors should compare against the latest baselines, at least those released in the first half of 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sQJKYjkh8w", "forum": "AcnCQR2ElW", "replyto": "AcnCQR2ElW", "signatures": ["ICLR.cc/2026/Conference/Submission6713/Reviewer_LeaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6713/Reviewer_LeaC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6713/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915087153, "cdate": 1761915087153, "tmdate": 1762919002327, "mdate": 1762919002327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Quicksviewer, an efficient video understanding model designed to address the issues of low computational efficiency and temporal-spatial redundancy caused by uniform frame perception in large-scale multimodal models (LMMs). Quicksviewer proposes a new non-uniform perception paradigm: it first uses a \"cubing network\" and Gumbel Softmax to dynamically split the video into non-uniform \"video blocks\" based on the temporal density of the video content. Then, a unified resampler compresses each variable-length block into a fixed number of visual tokens. It achieves significant compression rates, allowing the model to efficiently process long videos at all training stages."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The model uses the Gumbel Softmax trick to integrate discrete \"block networks\" into end-to-end training.\n2. The cubing that tracks semantic changes in momentum, making it applicable \"online\" to video stream scenarios."}, "weaknesses": {"value": "1. The resampler forces the content of 5 frames and 50 frames into 64 tokens, which is a \"distortion\" with highly uneven information density. The paper also mentions \"misalignment between the model's visual positional encoding and the actual temporal sequence,\" which undermines the model's ability to perform accurate temporal reasoning during inference. Is it possible to use dynamic token numbers, allowing information-rich blocks (e.g., 50-frame blocks) to be compressed into more tokens (e.g., 128), while information-poor blocks (e.g., 5-frame blocks) are compressed into fewer tokens (e.g., 32), thereby maintaining relatively constant information density.\n2. The paper does not provide a direct comparison of speed with the original baseline (for example, processing a 10-minute video, Quicksviewer takes 30 seconds, while the baseline model takes 60 seconds), making it difficult for readers to intuitively understand whether Quicksviewer has additional computational overhead.\n3. Methods may sacrifice some performance in fine-grained target understanding, which is also mentioned in the paper.\n4. The paper focuses on video token compression but lacks comparison with other token compression works, such as visionzip, videoxl, etc."}, "questions": {"value": "1. Have there been tests on temporal grounding tasks for long videos? Can this compression method be applied?\n2. What is the maximum number of frames or length of video the model can support?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "n3UQV4VUqG", "forum": "AcnCQR2ElW", "replyto": "AcnCQR2ElW", "signatures": ["ICLR.cc/2026/Conference/Submission6713/Reviewer_iMvC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6713/Reviewer_iMvC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6713/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991301845, "cdate": 1761991301845, "tmdate": 1762919001965, "mdate": 1762919001965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}