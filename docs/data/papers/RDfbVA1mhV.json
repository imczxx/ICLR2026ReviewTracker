{"id": "RDfbVA1mhV", "number": 16012, "cdate": 1758258616192, "mdate": 1759897267525, "content": {"title": "Blade: A Derivative-free Bayesian Inversion Method using Diffusion Prior", "abstract": "Derivative-free Bayesian inversion is an important task in many science and engineering applications, particularly when computing the forward model derivative is computationally and practically challenging. \nIn this paper, we introduce Blade, which can produce accurate and well-calibrated posteriors for Bayesian inversion using an ensemble of interacting particles.  Blade leverages powerful data-driven priors based on diffusion models, and can handle nonlinear forward models that permit only black-box access (i.e., derivative-free).\nTheoretically, we establish a non-asymptotic convergence analysis to characterize the effects of forward model and prior estimation errors. Empirically, Blade achieves superior performance compared to existing derivative-free Bayesian inversion methods on various inverse problems, including challenging highly nonlinear fluid dynamics.", "tldr": "We propose Blade, a derivative-free Bayesian inversion method with diffusion prior that delivers accurate and well-calibrated posterior.", "keywords": ["Bayesian inversion", "diffusion model", "inverse problem", "derivative-free"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68dc209cf757b7776cd9fb49bf16f0b9442d2b91.pdf", "supplementary_material": "/attachment/8478dd787fc0f65339eb082a754a809f8a8ad7c7.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Blade, a derivative-free Bayesian inversion method that uses a pretrained diffusion model as a data-driven prior. It builds on the split Gibbs sampler, alternating between (i) a derivative-free likelihood step realized via an ensemble-based statistical linearization of the black-box forward model, and (ii) a prior step implemented as the reverse denoising diffusion process. The authors provide non-asymptotic convergence/error bounds that separate the contributions of the forward-model linearization and the diffusion-prior score approximation. Empirically, Blade is evaluated on linear–Gaussian (and Gaussian-mixture) settings and on a challenging Navier–Stokes fluid-dynamics inverse problem"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Diffusion models continue to gain popularity, yet conditional sampling with diffusion priors remains a challenging and important problem, especially when they are used as prior distributions in Bayesian inference. This paper addresses that gap by proposing an ensemble-based particle sampling approach that effectively combines two active research directions: particle-based Bayesian inference and diffusion-model priors.\n\nWhile prior work on diffusion-based split Gibbs sampling has primarily focused on improving the prior step, the authors make a novel contribution by developing a more efficient and theoretically grounded likelihood step. They further strengthen their proposal by quantifying the effects of key approximation errors, those arising from statistical linearization of the forward model and score approximation in the diffusion prior.\n\nFinally, the paper offers an extensive empirical evaluation across both controlled and complex nonlinear inverse problems, providing convincing evidence for the effectiveness and robustness of the proposed algorithm."}, "weaknesses": {"value": "- My main concern lies with the error bound in Theorem 2. The authors bound the discrepancy between systems (9) and (6), but both are assumed to share the same sample covariance $C_t$ computed from the particles of system (9). This assumption greatly simplifies the analysis and, strictly speaking, does not capture the true approximation error between the two dynamics. Instead, it effectively introduces an auxiliary process (6.5) that depends on $C_t$ from (9). The setup is further confusing because system (6) is initialized from its stationary distribution, while (9) is not. A more rigorous comparison would require analyzing how the respective covariance matrices evolve over time.\n\n- The convergence proofs offer only limited novelty and appear to be incremental extensions of known results. Lemma 1 reproduces existing results (as acknowledged), Theorem 1 follows standard arguments under the assumption that $C_t$ remains strictly positive definite, and Lemma 2 closely mirrors Lemma A.4 from Wu et al. (2024). Similarly, the proof structure of Theorem 2 largely parallels Theorem 3.1 in Wu et al. (2024). Lemma 3 is again adapted from previous work. Overall, while the analysis is sound, its theoretical originality is limited.\n\n- The assumptions underlying Theorem 2 are not adequately discussed in the main text, and the appendix treatment is brief. In particular, Assumption 3 is not mentioned outside the appendix, and Assumption 2 seems quite restrictive. Typically, the linearization error can be bounded at a fixed time via a Taylor expansion, yielding an error term dependent on the ensemble covariance norm. However, it is unclear whether such a bound remains uniformly valid over time. A more detailed justification, or an explicit derivation for the proposed algorithm, would strengthen the theoretical section considerably.\n\n- The paper does not include a comparison to the EKS or ALDI variants that incorporate localization (see, e.g., Fokker-Planck Particle Systems for Bayesian Inference: Computational Approaches, Reich & Weissmann, 2021). Localization has been shown to substantially improve the performance of EKS/ALDI in multimodal settings, and it can be implemented efficiently using particle clustering (see, e.g., The Ensemble Kalman Filter for Rare Event Estimation, Wagner et al., 2022). Including such comparisons would be important, especially since EKS appears to outperform Blade in the linear–Gaussian case (Table 4) while also having a lower runtime."}, "questions": {"value": "Below I list my questions and suggestions for the authors:\n\n- How does Blade perform compared to advanced variants of ALDI or EKS that use localized covariance preconditioners?\n- How exactly are ALDI and EKS implemented when incorporating diffusion priors? Are you using the estimated score from the diffusion model as an approximation of the gradient of $g$?\n- In the evaluation, the paper reports only statistics from the final iteration. How does performance evolve over the number of iterations? How do you assess convergence?\n- I recommend discussing Assumptions 1-3 directly in the main text rather than deferring them entirely to the appendix.\n- The discussion of theoretical novelty (e.g., in Remark 2) could be expanded. While similarities to prior work are acknowledged, the current phrasing is somewhat vague. Please clarify the specific conceptual or technical differences relative to earlier analyses (e.g., Wu et al., 2024).\n- Line 48: Derivative-free or zeroth-order gradient approximations also tend to scale poorly in high dimensions. Do you have theoretical or empirical evidence suggesting that statistical linearization scales more favorably?\n- Line 321: Doesn’t the convergence bound require the assumption that both $\\lambda^\\ast$ and $\\delta$ are strictly positive?\n- Figure 11: It is somewhat surprising that increasing the ensemble size beyond a certain point does not improve performance. Are other error sources dominating at that stage? Would the observed plateau decrease if you retuned hyperparameters (e.g., step size, noise level) for larger ensembles?\n\nTypos and minor comments:\n\n- Line 138: we follow[] \n- Line 209: distributio[n] \n- Line 231: to run [the] algorithm\n- Line 333: strictly speaking the statistical linearization error would be avoidable by computing derivatives (if available). \n- Line 332: Theorem 1 show[s]\n- Line 345: This statement is a bit vague, whether the posterior distribution yields a good solution for the ill-posed inverse problem depends on multiple choices. I think it is clear, that you evaluate your algorithm Blade on statistics of the posterior samples since you are working in a Bayesian setup. \n- Line 475: descent should be decent\n- Section 4.2: U-Net vs UNet\n- Line 1013: „… on on …“ \n- Line 1020: „…, [a]s shown in…“\n- Line 1107: Missing space „…performance. []We denote…“"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y8CnTOxvnn", "forum": "RDfbVA1mhV", "replyto": "RDfbVA1mhV", "signatures": ["ICLR.cc/2026/Conference/Submission16012/Reviewer_jPCJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16012/Reviewer_jPCJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761316889047, "cdate": 1761316889047, "tmdate": 1762926217154, "mdate": 1762926217154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Blade, a method for Bayesian inversion based on Diffusion Split Gibbs Sampling and statistical linearization.\nBlade comes in two variants: main and diag with different estimations of the covariance. The authors give a convergence analysis with explicit error bounds. \nBlade is evaluated on several benchmark tasks; in the main paper on Gaussian/Gaussian mixture models, and a Navier-Stokes problem; \nand compared to both optimization-based and other ensemble methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is very well written and easy to follow. \n- The statistical linearization step is well motivated and a sample-efficient solution for non-differentiable forward models. \n- Explicit error bound that includes both errors from the diffusion and forward model derivative approximation\n- Both variants diag and main are well motivated and target different scenarios (point-estimation vs. callibrated posterior). This is shown convincingly in the Navier Stokes experiment\n- Comprehensive set of experiments and ablations in the appendix"}, "weaknesses": {"value": "- Incremental improvement to the diffusion-based split Gibbs sampling\n- No comparisons to sequential MCMC approaches; e.g., Feynman-Kac diffusion steering [1,2] which are also based on interacting particles. This would strengthen the results.\n- I am not really convinced the CDM model is implemented in the most optimal way; just concatenating the observation as an additional channel is simpler and likely to give improved results. \n- One downside of the likelihood step is that the forward model is evaluated on noisy samples. If the forward model is robust to noise this still works, but many applications have highly non-linear forward models where the statistical linearization might not work very well. \n\nMinor typos:\n- L1020 \", As shown\"\n- L209 \"distributio\"\n\n\n[1] https://arxiv.org/pdf/2409.09650v1\n\n[2] https://arxiv.org/pdf/2501.06848"}, "questions": {"value": "- If the forward model G was differentiable, can we use the gradient information directly in eq (6)? How does the statistical linearization compare in this case? Are inference times comparable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Ni5qcvZ8sD", "forum": "RDfbVA1mhV", "replyto": "RDfbVA1mhV", "signatures": ["ICLR.cc/2026/Conference/Submission16012/Reviewer_wxbH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16012/Reviewer_wxbH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933535865, "cdate": 1761933535865, "tmdate": 1762926216698, "mdate": 1762926216698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors established a non-asymptotic convergence analysis to characterize the impact of forward modeling and prior estimation errors. Experimental results show that Blade outperforms existing derivative-free Bayesian inversion methods on various inverse problems, including highly challenging highly nonlinear hydrodynamic problems."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Both the theoretical and experimental parts are excellent."}, "weaknesses": {"value": "I think the experiment could be more thorough."}, "questions": {"value": "Are you considering experiments on a larger scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Id1JIb4Gg3", "forum": "RDfbVA1mhV", "replyto": "RDfbVA1mhV", "signatures": ["ICLR.cc/2026/Conference/Submission16012/Reviewer_vs1A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16012/Reviewer_vs1A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966945663, "cdate": 1761966945663, "tmdate": 1762926216298, "mdate": 1762926216298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The authors present a method for posterior sampling that leverages denoising diffusion models as priors.\n- The proposed framework addresses settings where only pointwise evaluations of the likelihood are available.\n- For that, the authors introduce a Split-Gibbs sampling scheme that alternates between sampling from the diffusion prior and a likelihood update.\n- To circumvent the need for likelihood gradients, they employ a covariance-preconditioned Langevin Dynamics approach, in which the drift term is approximated via statistical linearization using an ensemble of particles.\n- The proposed method is validated on a range of inverse problems, including both synthetic and real setup"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Introducing a method that combines statistical linearization and Split-Gibbs sampling to solve inverse problems with diffusion priors without requiring a gradient of the likelihood"}, "weaknesses": {"value": "**Motivation of the method**\nThe introduction of the covariance-preconditioned Langevin Dynamics appears somewhat arbitrary and insufficiently justified. Its use seems primarily motivated by convenience to avoid directly inverting the covariance matrix $C_t$, rather than by a clear theoretical or empirical rationale.\n\nOn the other hand, equation (9) involves the square root of $C_t$, which as such is difficulty to handle in practice, and the proposed method to handle it is ambiguous (see the point below on Correctness).\n\n\n**Evaluation of the method**\nThe experimental validation is limited and does not support the method’s intended use case.\nThe approach is motivated by scenarios where the likelihood gradient is unavailable or costly to compute, yet all considered experiments (toy problems, Navier–Stokes, and black hole imaging) involve forward models for which gradients can be readily obtained; see [1] for Navier–Stokes and [2] for black hole imaging.\nConsequently, the evaluation does not demonstrate the method’s effectiveness in the truly derivative-free regime.\nFurthermore, the reported runtime in Table 14 is very computationally heavy (around 1 hour per reconstruction )and as such the method does not offer clear performance benefits over gradient-based alternatives.\n\n**Correctness**\nThe use of the square-root covariance matrix approximation in Line 269 is incorrect, the approximation has shape $n \\times$ the number of particles $J$, but is intended to condition the noise $dw_t \\in \\mathbb{R}^n$. In particular, for the single-particle case, $\\sqrt{C_t}$ becomes a column matrix, making the matrix–vector product undefined.\n\n**Typos and minor issues**\n* Equation (2) introduces a nonstandard $\\beta(t)$ coefficient that does not align with conventional diffusion model formulations, and I cannot find it in the cited reference [5]\n* Line 86: replace \"sampling for posterior inference\" ---> “or sampling for inference.”\n* Line 209: \"distributio\" ---> \"distribution\"\n* In Theorem 1 and 2 the term \"horizon\" is ambiguous\n\n**Missing references and related work**\n- The statement in Lines 90–91 overlooks prior sampling-based approaches that handle nonlinear setups, such as [3].\n- The discussion on Gibbs sampling and diffusion priors should cite [4], which provides a relevant treatment of Gibbs-sampling and inverse problems with diffusion models.\n\n\n---\n\n... [1] Rozet, François, and Gilles Louppe. \"Score-based data assimilation.\" Advances in Neural Information Processing Systems 36 (2023): 40521-40541.\n\n... [2] Wu, Zihui, et al. \"Principled probabilistic imaging using diffusion models as plug-and-play priors.\" Advances in Neural Information Processing Systems 37 (2024): 118389-118427.\n\n... [3] Achituve, Idan, et al. \"Inverse problem sampling in latent space using sequential Monte Carlo.\" arXiv preprint arXiv:2502.05908 (2025).\n\n... [4] Janati, Yazid, et al. \"A Mixture-Based Framework for Guiding Diffusion Models.\" Forty-second International Conference on Machine Learning. 2025.\n\n... [5] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022."}, "questions": {"value": "- Figure 1: can the authors clarify how the sample are being generated? Namely, is it one run of the algorithm and what is being plotted are the ensemble of particles?\n- Lines 234–236: The claim that each particle in Blade has its own associated target distribution is unclear. Why would this not also apply to other ensemble-based samplers such as EKS or ALDI?\n- Figure 3: How is the maximum rank defined in this context? What explains the apparent linear relationship between the rank and the step size? The figure caption mentions \"accumulated rank\" but its purpose and interpretation are unclear; what insight does plotting the accumulated rank provide about the algorithm’s behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ToskqVEbmA", "forum": "RDfbVA1mhV", "replyto": "RDfbVA1mhV", "signatures": ["ICLR.cc/2026/Conference/Submission16012/Reviewer_NR9p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16012/Reviewer_NR9p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993623255, "cdate": 1761993623255, "tmdate": 1762926215794, "mdate": 1762926215794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}