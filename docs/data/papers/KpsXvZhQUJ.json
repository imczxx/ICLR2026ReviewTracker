{"id": "KpsXvZhQUJ", "number": 18848, "cdate": 1758291428515, "mdate": 1759897077917, "content": {"title": "ADiff4TPP: Asynchronous Diffusion Models for Temporal Point Processes", "abstract": "This work introduces a novel approach to modeling temporal point processes using diffusion models with an asynchronous noise schedule. At each step of the diffusion process, the noise schedule injects noise of varying scales into different parts of the data. With a careful design of the noise schedules, earlier events are generated faster than later ones, thus providing stronger conditioning for forecasting the more distant future. Our method models the joint distribution of the latent representation of events in a sequence and achieves state-of-the-art results in predicting both the next inter-event time and event type on benchmark datasets. Additionally, it flexibly accommodates varying lengths of observation and prediction windows in different forecasting settings by adjusting the starting and ending points of the generation process. Finally, our method shows superior performance in long-horizon prediction tasks, outperforming existing baseline methods.", "tldr": "", "keywords": ["Temporal point process", "latent diffusion models"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/226100fad9b75dd82073c660ee4f351e235f7c90.pdf", "supplementary_material": "/attachment/394f9381d28bceaa023cce010e05d5e58d7ade5a.zip"}, "replies": [{"content": {"summary": {"value": "ADiff4TPP is a flow-matching model for event sequences with a per-event piecewise linear noise schedule. The noise schedule allows for partial denoising to forecast event sequences with fewer events."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong experimental results\n1. Asynchronous schedule mixes autoregressive forecasting and full-sequence generation."}, "weaknesses": {"value": "1. The paper mixes up flow matching and diffusion in multiple places. These are two different methods and cannot be used interchangeably. The title says diffusion model but my impression is that this is actually a flow matching model.\n1. Figure 1 has a label Event Duration, but events in TPPs do not have a duration.\n1. The paper talks about windows, e.g. in Section 3.4, but this is misleading as it implies that the model forecasts events in a window $[t, t']$ when it actually generates a fixed number of events regardless of their times.\n1. In my opinion, the variable-length generation is not really variable-length, because it has a fixed upper bound."}, "questions": {"value": "1. Why is $A(s)$ a matrix instead of a vector?\n1. How many steps did you use?\n1. Does your asynchronous schedule require the number of steps to be larger than the number of events to be generated?\n1. How are the results if you solve from 1 to 0 instead of $s_{start}$ to $s_{end}$?\n1. Have you tried any smooth alternatives to your piecewise linear asynchronous schedule?\n1. How do other diffusion-like models perform in the long horizon prediction task such as Add-Thin?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nzoQyhGJjv", "forum": "KpsXvZhQUJ", "replyto": "KpsXvZhQUJ", "signatures": ["ICLR.cc/2026/Conference/Submission18848/Reviewer_YuPo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18848/Reviewer_YuPo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760909922101, "cdate": 1760909922101, "tmdate": 1762930817513, "mdate": 1762930817513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Thank you for your engagement and contribution clarifications"}, "comment": {"value": "Dear Reviewers and Area Chair,\n\nFirst, we sincerely thank you for your time and effort in reviewing our submission. We recognize that the review period is short and the workload is overwhelming, and therefore, some misunderstandings of our paper are to be expected. We will answer some common misconceptions in this post.\n\n**Diffusion Models or Flow Matching**\n\nWe thank Reviewers 7Hz1 and YuPo for the suggestion to change our paper title. Flow matching is considered part of the broader family of diffusion models in the literature (see the blog post [diffusionflow.github.io](diffusionflow.github.io)). Indeed, the exact derivations of the training objective differ along DDPM, score matching, flow matching, etc. But all classes of diffusion models are consistent in the sense that a model observes a forward process where data degrades to noise (either via a stochastic differential equation or an interpolation) and learns to reverse the process to generate realistic data.\n\nOur method is based on asynchronous flow matching, which extends flow matching and aligns with this characterization of diffusion models, so we believe changing the title is unnecessary. We will clarify this connection between flow matching and diffusion in Section 2.2. Our motivation for using flow-matching is that it is conceptually the simplest model to enforce asynchronous noise schedules since the noise schedule of every component of the data is linear.\n\n**Motivation for using Diffusion Models**\n\nWe thank the reviewers for questioning our motivation for using DMs, and we agree that the introduction can benefit from more explanation.  The motivation for applying DMs to TPPs stems from their core strength: *learning complex distributions that characterize a dataset without relying on strong parametric assumptions* (i.e., Poisson/intensity or IFTPP - mixture of Gaussians), allowing greater model flexibility. This aligns naturally with the TPP objective of modeling the conditional distribution $p(\\tau_i, k_i | H_i)$.  In the flow matching and diffusion literature, it is established that learning a distribution is equivalent to learning a *vector field that transports a simple base distribution* (e.g. Gaussian distribution) to a more complicated target distribution. In our paper, this corresponds to the distribution of entire event sequences. Samples are drawn by initializing Gaussian noise $\\epsilon$ and solving the proposed ODE (Equation 6) in latent space. DMs also achieve state-of-the-art results in image/video/audio generation tasks, showing their effectiveness in learning complex distributions. Furthermore, unlike prior TPP models that adopt an autoregressive formulation, our model enables *joint generation of multiple events while flexibly conditioning on arbitrary past windows*. This is enabled by our asynchronous noise schedule, which generalizes beyond synchronous or autoregressive conditioning and allows arbitrary observation/prediction splits. To the best of our knowledge, ADiff4TPP is the first method to have this property in the TPP literature.\n\n**Motivation for Modelling in Continuous Latent Space**\n\nWe thank Reviewer mdrw for correctly identifying our motivation behind mapping events (which consists of a numerical time and categorical mark) into a continuous latent vector where diffusion models operate most effectively. However, we acknowledge that this design choice can benefit from more justifications.\n\nWe adopt a latent diffusion approach to cleanly separate event representation (via VAE) and sequence modeling (via DiT). Defering temporal dependencies to the DiT allows for better control and flexibility over arbitrary observation/prediction windows.\n\nThe following papers (particularly the first paper) explain the benefits of performing diffusion in latent space, namely synthetic speed, expressivity, and tailoring in the presence of discrete data.\n\n* Arash Vahdat et al. Score-based generative modeling in latent space. NeurIPS, 2021.\n* Zhang, H. et al. Mixed-type tabular data synthesis with score-based diffusion in latent space. ICLR, 2024.\n* Rombach, R. et al. High-resolution image synthesis with latent diffusion models. CVPR, 2022\n\nWhile prior works (Austin et al., 2021; Inoue et al., 2023) have explored discrete diffusion, models for mixed data types like event sequences remain underexplored. Our method balances efficiency and interpretability, as the VAE ensures structured encoding of heterogeneous data, while the decoder faithfully reconstructs categorical event types. \n\nThat said, to the best of our knowledge, our paper is the first to use DMs to predict event time and category rather than only event time as in EventFlow and Add-Thin.\n\nThank you once again for your careful consideration and helpful feedback. If any questions remain, or if any clarification would be useful, we would be very grateful to hear from you.\n\nSincerely,\n\nAuthors of Paper ID 18848"}}, "id": "oyAgybjLJF", "forum": "KpsXvZhQUJ", "replyto": "KpsXvZhQUJ", "signatures": ["ICLR.cc/2026/Conference/Submission18848/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18848/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18848/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763488868884, "cdate": 1763488868884, "tmdate": 1763488868884, "mdate": 1763488868884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new diffusion-based model for TPPs. It first learns the latent representation of the events using an autoencoder. Then, it learns the diffusion with an asynchronous noise schedule which adds noise to events at different speeds, based on their temporal ordering. Authors theoretically motivate and justify the choices. The paper is clear and easy to follow. The results are good, model is outperforming established competitors on common benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "It makes sense to map an event which consists of a real time and categorical mark into a single vector. This transforms everything into a nice latent space and one can use any kind of generative model.\n\nThe proposed noise schedule is sound and authors show in section 4 some theoretical results.\n\nThe empirical results are good, showing improvement over previous models. Ablations studies justify the choices made in the paper."}, "weaknesses": {"value": "Converting the events to a latent space is a convenient solution that avoids dealing with the specifics of TPP data. After this step is done, any model can be fit to capture the distribution of the latent space. Using diffusion after VAE step is not thoroughly tested. Both using asynchronous noise diffusion without VAE, and some other generative model on latent representations is a possible ablation study.\n\nThere are additional hyperparameters for beta-VAE and asynchronous diffusion that have to be picked and tuned. The method is limited by the maximum length of a sequence N. Sampling is slow for shorter sequences. The method is a combination of known techniques so novelty is limited."}, "questions": {"value": "- Can you comment on the performance of non-diffusion models on latent representations?\n- Why didn't you encode the full sequence to a latent representation using an encoder-decoder?\n- Do you expect that beta-VAE will work for any TPP dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1e3K9lfTrw", "forum": "KpsXvZhQUJ", "replyto": "KpsXvZhQUJ", "signatures": ["ICLR.cc/2026/Conference/Submission18848/Reviewer_mdrw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18848/Reviewer_mdrw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603935273, "cdate": 1761603935273, "tmdate": 1762930816683, "mdate": 1762930816683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ADIFF4TPP proposes an asynchronous noise schedule for diffusion (flow matching to be correct) models for forecasting the next N events of a TPP. Furthermore, they propose to use a VAE to better encode Events to then model them in a latent space with the flow model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Derive asynchronous noise schedule for Flow matching.\n- Asynchronous time schedule is interesting conceptually."}, "weaknesses": {"value": "Even though proposing an asynchronous noise schedule could be interesting, the motivating claims are at times factually incorrect and the claims are not sufficiently justified by theory or experiment.\n\nMotivation:\n- Actual motivation for the asynchronous noise schedule is not backed-up. There is no theoretic reason why even when presented with noisier (teacher forced) past, the model should be limited.\n- Shorter solving of ode for forecasting does not incorporate that later events also get less update steps, i.e. same trade-off between number of steps and expressivity. Furthermore, it could be that one actually needs more steps overall, since events are not updated at each step.\n\n\nMethod:\n- TPPs are generative models, but this method only learns to forecast the next fixed number of points. \n- Usage of VAE compared to other common encodings is unmotivated and not properly showed beyond limited ablation. Why would we want to model in the latent space?\n\nExperiments:\n- Limited evaluation: It proposes a new noise schedule, but only presents limited ablation of the schedule. \n- Claiming efficiency, but do not provide wall clock comparisons.\n- Limited number of benchmark datasets.\n- Do not evaluate against AddThin, EventFlow, Ludke et al. 2024 for long-lange forecasting, even though they have shown Sota results. I know that none of them model marks, but could still be compared against, especially within their evaluation metrics.\n\nMinor:\n- Lacking command of related work, e.g., AddThin does not model the intensity, but instead directly the joint distribution; Ludke et al. (2024) does not allow direct likelihood evaluation; EventFlow is not a diffusion model and a lot of related work on TPPs is missing.\n- Flow matching is claimed to be a variant of DMs, which is not true. \n\n\n\n\nOverall, proposing an asynchronous noise schedule is interesting but is improperly motivated and supported. Furthermore, to me the contribution of an asynchronous noise schedule is limited and in my honest opinion only warrants a workshop paper, i.e., is not a main track contribution. Furthermore, the \"generative\" model does not capture one defining and essential property of TPPs, the distribution over the number of events."}, "questions": {"value": "- Why would one use a VAE plus a Diffusion/Flow model?\n- Why didn't you show the usage of the asynchronous schedule on existing models, but also propose an all together new architecture?\n- Why do you use a Flow model instead of just applying your schedule to a diffusion models for which you would not have to restrict the schedule as much, since there are no invertibility constraints?\n- Why don't you model the number of events within the diffusion/flow-matching framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0LgyXSb0BD", "forum": "KpsXvZhQUJ", "replyto": "KpsXvZhQUJ", "signatures": ["ICLR.cc/2026/Conference/Submission18848/Reviewer_7Hz1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18848/Reviewer_7Hz1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659613372, "cdate": 1761659613372, "tmdate": 1762930815779, "mdate": 1762930815779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ADIFF4TPP, a novel diffusion model framework for temporal point processes that employs an asynchronous noise schedule to handle event sequence generation. Main contributions include: (1) A matrix-valued noise schedule A(s) enabling different diffusion speeds for different events; (2) A $\\beta$-VAE-based latent space for handling heterogeneous event data; (3) A conditional flow matching objective extended to asynchronous schedules. The method supports both next-event and long-horizon prediction by adjusting the observation and prediction windows in the denoising process and achieves superior results on multiple TPP benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1). The work addresses some challenges in TPP modeling (e.g., data heterogeneity, variable sequence length, long-horizon prediction) and demonstrates the potential of asynchronous diffusion in sequential data modeling. The asynchronous diffusion concept for TPPs is genuinely innovative.\n\n(2). The extension of flow matching to matrix-valued noise schedules is well-grounded."}, "weaknesses": {"value": "(1). The specific parameterization of  A(s) in Equation (5) lacks theoretical or empirical justification.\n\n(2). Add analysis about the trade-off between reconstruction quality and generative performance"}, "questions": {"value": "(1). How does the method handle very long sequences where Lâ‰«N? Algorithm 3 uses traditional sliding windows, but is there an evaluation of its effectiveness for capturing long-range temporal dependencies?\n\n(2). How sensitive are the results to the choice of ODE solver and step size? Is there numerical error accumulation in long-horizon prediction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fCt1BnBViZ", "forum": "KpsXvZhQUJ", "replyto": "KpsXvZhQUJ", "signatures": ["ICLR.cc/2026/Conference/Submission18848/Reviewer_DnEh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18848/Reviewer_DnEh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829629859, "cdate": 1761829629859, "tmdate": 1762930814803, "mdate": 1762930814803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}