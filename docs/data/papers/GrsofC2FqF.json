{"id": "GrsofC2FqF", "number": 24864, "cdate": 1758361324909, "mdate": 1759896744823, "content": {"title": "Detection of unknown unknowns in autonomous systems", "abstract": "Unknown unknowns (U2s) are deployment-time scenarios absent from development/testing. Unlike conventional anomalies, U2s are not out-of-distribution (OOD); they stem from changes in underlying system dynamics without a distribution shift from normal data. Thus, existing multi-variate time series anomaly detection (MTAD) methods—which rely on distribution-shift cues—are ill-suited for U2 detection. Specifically: (i) we show most anomaly datasets exhibit distribution shift between normal and anomalous data and therefore are not representative of U2s; (ii) we introduce eight U2 benchmarks where training data contain OOD anomalies but no U2s, while test sets contain both OOD anomalies and U2s; (iii) we demonstrate that state-of-the-art (SOTA) MTAD results often depend on impractical enhancements: point adjustment (PA) (uses ground truth to flip false negatives to true positives, inflating precision) and threshold learning with data leakage (TL) (tuning thresholds on test data and labels); (iv) with PA+TL, even untrained deterministic methods can match or surpass MTAD baselines; (v) without PA/TL, existing MTAD methods degrade sharply on U2 benchmarks. Finally, we present sparse model identification–enhanced anomaly detection (SPIE-AD), a model-recovery-and-conformance, zero-shot MTAD approach that outperforms baselines on all eight U2 benchmarks and on six additional real-world MTAD datasets—without PA or TL.", "tldr": "We formalize U2 (non-OOD dynamic changes without distribution shift), release 8 U2 benchmarks, and propose SPIE-AD—a zero-shot U2 detection method.", "keywords": ["unknown unknowns", "autonomous systems", "conformal bounds"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ae888cc8a84d10a6f39e70b6fd964b4b93cccec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the emerging problem of unknown-unknowns (U2s) in time-series anomaly detection—cases where there is no obvious out-of-distribution shift from normal data, making traditional detection methods ineffective. The authors identify three key challenges: the absence of distributional deviation, the impractical enhancement of existing models, and data leakage in threshold calibration. To address these, they propose a sparse model identification–enhanced anomaly detection framework tailored to handle U2 scenarios. Extensive experiments are conducted on synthetic U2 benchmarks and multiple real-world multivariate anomaly detection datasets, showing that the proposed method outperforms a range of baselines."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper explores an interesting and relatively new direction in anomaly detection by formally framing and tackling the concept of unknown-unknowns. The writing is generally clear, and the empirical section is comprehensive and convincing. The authors provide strong evidence that their method can distinguish subtle anomalies even when there is no visible distribution shift. The experiments are rigorous, covering both synthetic U2 datasets and widely used multivariate benchmarks, with appropriate metrics and detailed comparisons. Overall, the work is solid, timely, and contributes a valuable perspective to the time-series anomaly detection literature."}, "weaknesses": {"value": "1. While the technical contribution and empirical results are strong, the paper could be significantly improved in presentation and positioning. The current layout is too compact: equations appear small and dense, while several tables (e.g., Tables 1, 3, and 4) occupy disproportionate space, making the interleaved text uneven and hard to read. Improving figure/table sizing, rounding consistency, and overall margin balance would greatly enhance readability. Adding visual highlights or color emphasis in key tables could also help.\n\n2. The related-work discussion is incomplete and occasionally overstated. The statement “To the best of our knowledge, there is only one solution for zero-shot MTAD Audibert et al. (2020)” is inaccurate. Other relevant works, such as ITF-TAD, UniTS, should be properly discussed and contrasted. Also, the paper only mentions SIGLLM but omits relevant methods such as VLM4TS, which also address zero-shot and training-free detection in related settings. A fairer and more complete comparison would strengthen the contribution.\n\n3. Beyond metrics, it would also be valuable to include additional qualitative analyses or visual case studies. For instance, Figure 2 could be expanded to show how different methods (including baselines) behave under normal versus anomalous regimes, visually demonstrating how the proposed model differentiates subtle shifts that are not captured by these metrics.\n\nNamura, Nobuo, and Yuma Ichikawa. \"Training-free time-series anomaly detection: Leveraging image foundation models.\" arXiv preprint arXiv:2408.14756 (2024).\nGao, Shanghua, et al. \"Units: A unified multi-task time series model.\" Advances in Neural Information Processing Systems 37 (2024): 140589-140631.\nAlnegheimish, Sarah, et al. \"Large language models can be zero-shot anomaly detectors for time series?.\" arXiv preprint arXiv:2405.14755 (2024).\nHe, Zelin, Sarah Alnegheimish, and Matthew Reimherr. \"Harnessing Vision-Language Models for Time Series Anomaly Detection.\" arXiv preprint arXiv:2506.06836 (2025)."}, "questions": {"value": "Could the authors provide quantitative or qualitative evidence illustrating how their method distinguishes U2 anomalies?\n\nAs the authors note that SPIE-AD struggles with short-lived anomalies, could the authors elaborate on potential approaches to alleviate it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u4Eqo93Uob", "forum": "GrsofC2FqF", "replyto": "GrsofC2FqF", "signatures": ["ICLR.cc/2026/Conference/Submission24864/Reviewer_16mj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24864/Reviewer_16mj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760767561049, "cdate": 1760767561049, "tmdate": 1762943226049, "mdate": 1762943226049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work discusses the problem of operating in the presence of unknown unknowns (U2) data from multivariate time series, based on the hypothesis that state-of-the-art anomaly detection mechanisms are not well-suited to identify this type of data, when any of three assumptions are violated:\n\n1- There is a shift in the data distribution due to an anomaly \\\n2- Data leakage is used to fine-tune the anomaly threshold hyperparameter \\\n3- The evaluation setup takes place in an unrealistic setup (i.e, anomalies are continuous)\n\nThe paper presents the following contributions:\n1) It introduces SPIE-AD, a methodology to detect U2 when the above mentioned assumptions do not hold. At its core, SPIE-AD consists of a sparse non-linear dynamical model recovery strategy, and model robustness interval extraction through conformal prediction. \\\n2) Six synthetic benchmarks s derived from U2 scenarios occurring in three different real-world system I am running a few minutes late; my previous meeting is running over.\\\n3) Experimental demonstration of the disadvantages of using point adjustment (PA)"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem of dealing with unknown unknowns is highly relevant in many applications, still challenging, and actively researched.\n- The paper gathers a good number of baselines and datasets for their experiments"}, "weaknesses": {"value": "- The explanation/definition of U2 is based on a couple of examples illustrated in the paper. A formal (mathematical?) definition is highly desirable to clearly establish when a sample should be denoted as a U2 or an anomaly.\n- Conformal prediction as per [19] is about estimating prediction intervals even when the distribution of the training and test sets deviate, under the condition that the training data is i.i.d.. This, however, does not hold strictly for time series as subsequent points are not independent. The paper does not specify under which circumstances the assumption can be relaxed in this work \n- Some details about the multiple hyperparameters, terms involved, and precise function definitions (i.e., $\\rho$) are not specified."}, "questions": {"value": "At the core of the assumptions and hypotheses that drive this work is the definition of what constitutes an unknown unknown. This is not entirely clear from the paper, and in particular from the datasets used. Hence:\n\n1. For the synthetic datasets: how exactly are the abnormal scenarios generated? Following the definitions in the paper, what makes them a U2 and not an anomaly?\n2. For the real-world datasets: From the description, it seems that this work deals simply with anomalies. However, the introduction discusses that an anomaly is not a U2. What is your position here?\n\nOverall, from the paper, it is not clear why the proposed method should be consider as an U2 detector, rather than a \"classical\" MTAD. Please develop."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fMAizI415k", "forum": "GrsofC2FqF", "replyto": "GrsofC2FqF", "signatures": ["ICLR.cc/2026/Conference/Submission24864/Reviewer_H3Ew"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24864/Reviewer_H3Ew"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927050771, "cdate": 1761927050771, "tmdate": 1762943225806, "mdate": 1762943225806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles unknown unknowns (U2) in autonomous systems-deployment-time failures that do not show input-distribution shift: learn sparse governing equations for normal operation and flag windows whose recovered parameters deviate from the nominal set. The proposed SPIE-AD pipeline continuously extracts sparse nonlinear dynamics from multivariate time series (via SINDy-MPC), refines them with liquid time-constant neural networks for robustness under low sampling and noise, and applies a conformal-inference robustness score to test whether the window’s model remains consistent with normal; in short, it checks whether \\(x_{t+1}\\!\\approx\\!f_\\theta(x_t)\\) has changed even when \\(p(x)\\) has not. Beyond methodology, the paper builds eight U2 benchmarks, diagnoses common evaluation inflations (point-adjustment and test-set threshold leakage), and reports that SPIE-AD outperforms strong MTAD baselines on all U2 benchmarks and on six additional real-world datasets under leakage-free evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Reframes anomaly detection as spotting dynamics/relationship shifts (U2) instead of input-distribution or class shifts, and makes it concrete by learning sparse governing equations and checking their stability over time.\n\n- Uses a principled, modular pipeline, SINDy for sparse model discovery, optional liquid time-constant RNN refinement, and conformal testing, and evaluates without point-adjustment or test-set threshold leakage for fair comparisons.\n\n- Figures crisply contrast the standard MTAD pipeline with the proposed U2 detector and anchor the idea with concrete scenarios; the rule is stated cleanly at the level \\(x_{t+1}\\!\\approx\\!f_\\theta(x_t)\\).\n\n- Targets safety-critical multivariate time series (e.g., UAVs, insulin delivery, EEG) and shows consistent gains on curated U2 benchmarks under strict, leakage-free settings, pointing to a realistic path to deployment."}, "weaknesses": {"value": "- The detector hinges on recovering a correct sparse dynamics model; errors from derivative estimation, library choice, noise, or unobserved states can flip decisions. Add robust SINDy variants and report sensitivity to library/sampling/noise.\n\n- The LTC refinement (ODE-solved RNN) can be heavy for short windows or high-rate streams. Provide wall-clock profiles vs. SINDy-only baselines and consider lighter continuous-time alternatives (Neural ODE/CDE) or pruning.\n\n- Window-F1 (no PA) is a step forward, but operators care about time-to-detect, event coverage, and false-alarm rate. Include NAB-style timeliness scoring and precision–recall vs. latency curves under fixed alarm budgets.\n\n- Several U2 cases are synthetic or parameter flips that sparse polynomial models capture well. Stress-test partial observability, exogenous inputs (use SINDy-with-control), multi-timescale effects, and non-polynomial terms; document known failure modes.\n\n- Performance is sensitive to window length, library size, sparsity thresholds, and miscoverage level. Provide a compact sensitivity study and rules-of-thumb, plus robustness to mis-specification."}, "questions": {"value": "- In settings with hidden states or unmeasured actuations, do you consider SINDy-with-control (or DMDc) and, if not, how would you adapt SPIE-AD to avoid spurious alarms from unmodeled inputs? \n- Beyond window-F1 (no PA), can you report time-to-detect, event detection rate, and false-alarms per hour to reflect operational constraints? \n- What are wall-clock costs per window on your real datasets, and how does the LTC-enhanced variant compare to SINDy-only or light continuous-time alternatives (Neural ODE/CDE) for real-time use?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PEwS41DiLv", "forum": "GrsofC2FqF", "replyto": "GrsofC2FqF", "signatures": ["ICLR.cc/2026/Conference/Submission24864/Reviewer_u3Fj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24864/Reviewer_u3Fj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24864/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762803899976, "cdate": 1762803899976, "tmdate": 1762943225392, "mdate": 1762943225392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}