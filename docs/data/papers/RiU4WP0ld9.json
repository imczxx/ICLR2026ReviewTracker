{"id": "RiU4WP0ld9", "number": 13990, "cdate": 1758226549681, "mdate": 1759897397846, "content": {"title": "Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability", "abstract": "Recent findings suggest that misaligned models may exhibit deceptive behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT) is a promising tool for alignment monitoring: when models articulate their reasoning faithfully, monitors can detect and mitigate harmful behaviors before undesirable outcomes occur. However, a key uncertainty is: \\textbf{Can models obfuscate their CoT in order to pursue hidden adversarial objectives while evading detection?}\nTo answer this question and thus stress test CoT monitorability, we develop a composable and quantifiable taxonomy of prompt-level obfuscation cues to systematically elicit CoT obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT (prompted reasoning in outputs) using toy tasks and more realistic environments in SHADE-Arena.\nWe show that: (i) CoT monitoring performs accurately and efficiently without obfuscation pressure. (ii) Under strong obfuscation pressure, some models successfully complete adversarial tasks while evading detection, though this effect is model-dependent. (iii) Internal CoT is more resilient to obfuscation pressure than external CoT. \nThese results suggest that while CoT provides valuable oversight in benign settings, robust deployment requires model-specific stress-testing of monitorability.", "tldr": "We stress test chain‑of‑thought (CoT) monitorability with a composable obfuscation‑pressure taxonomy and show some reasoning models can complete hidden objectives while evading detection.", "keywords": ["AI Safety", "CoT Monitoring", "AI Control", "Steganography", "Evaluations", "LLMs", "Reasoning Models", "Monitorability", "Obfuscation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/32281f41e224c19aecf7b91c176d31e145cd3036.pdf", "supplementary_material": "/attachment/30ee91e9980ebe8e83e67dc8a6c4189fcf6ed051.zip"}, "replies": [{"content": {"summary": {"value": "The paper measures whether models can obfuscate their CoT against monitors under varying degrees of obfuscation pressure (e.g. being told the CoT is monitored). The main evaluation is performed in an environment where LLMs have to complete legitimate main- and secret side tasks. The paper shows that strong obfuscation pressure allows some models to evade monitors. Additionally, the paper notices that external CoT (CoT within output) is obfuscated more successfully, though the effect might be due to instruction following differences.\n\nMandatory disclosure of LLM usage by the reviewer: LLMs were utilized to structure the review content into prose paragraphs and numbered lists."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. CoT monitoring is an important area of AI safety, and CoT obfuscation a timely issue.\n\n2. The paper considers different levels of obfuscation pressure (small clues to detailed obfuscation instructions).\n\n3. The SHADE arena tasks resemble realistic risks (one could imagine the money task happening in reality via a prompt injection on an AI agent email assistant)."}, "weaknesses": {"value": "1. The most important weakness is that prompted obfuscation \"pressure\" may not be analogous to misalignment and scheming which the paper discusses in the introduction. As the discussion section acknowledges, more realistic settings (e.g. fine-tuning, maybe reinforcement learning against the CoT) could produce vastly different outcomes.\n\n2. The paper's results on internal vs external CoT seem to be largely explained by the observation that internal CoT follows reasoning instructions less reliably (section 5.2), I am confused why point (iii) is still stated as a contribution in the abstract.\n\n3. It's not clear how robust the results are to variations in the obfuscation pressure prompts. The appendix mentions different variants of the prompts, but it is not clear from the paper how much the results depend on the particular choices of prompts. This is especially important because the main experiment tests just two obfuscation levels (\"low\"/\"high\"). The paper would be improved by studying how much different choices of prompts change the results.\n\n4. Finally, the result that CoT-only monitoring is more accurate than full (CoT+output) monitoring (lines 368 - 370) is surprising and makes me suspicious of the statistical power of the results. The paper could be improved by establishing more clearly that the results are robust.\n\n5. (Minor) The paper uses GPT5-mini as grader, it would be nice to know whether the error rate of the grader significantly impacts the results (or is it basically perfect?).\n\nNitpicks (not affecting my score):\n\n* In the sentence \"some models successfully complete adversarial tasks while evading detection, though this effect is model-dependent\", \"some models\" and \"model-dependent\" is maybe repetitive.\n\n* In section 3.2 it is not clear why the dose response experiments are conducted; this becomes clear only later in the paper (computational limitation to \"low\"/\"high\" settings). Explaining this earlier would improve the flow."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ahfd9gxWG8", "forum": "RiU4WP0ld9", "replyto": "RiU4WP0ld9", "signatures": ["ICLR.cc/2026/Conference/Submission13990/Reviewer_gUWE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13990/Reviewer_gUWE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859920994, "cdate": 1761859920994, "tmdate": 1762924487106, "mdate": 1762924487106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores how can reasoning models obfuscate their reasoning traces, an important question for chain-of-thought (CoT) monitorability. The authors perform this task via a series (and combinations) of prompt cues encouraging the language model to obfuscate the CoT. They then evaluate how successful they are on both internal and external CoT traces in SHADE-arena, targeting tasks involving financial transactions, social media recommendations and aggregating school election results while adding hidden side-tasks. The main results highlight that (a) the Claude family of models are more prone to CoT obfuscation when prompted to do so, (b) obfuscation pressure generally reduces the side-task success rate, (c) in the absence of obfuscation cues, CoT monitoring is effective and (d) internal CoT traces are more robust than external ones with respect to the obfuscation cues. The authors finally tie this to how effective are the classes of models used in following instructions, as the obfuscation is encouraged via specific prompt instructions, showing that generally internal CoT follows instructions less reliably, partially explaining the prior findings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work provides practical insights on the relevant problem of CoT monitor evasion via adversarial prompting\n- Relatively realistic experimental setting, involving multi-turn interactions on SHADE-arena. \n- The separate analysis of internal and external CoT was interesting\n- The paper was generally well written and easy to follow"}, "weaknesses": {"value": "- Some of the results and observations can be seen as direct consequences of the works of (Emmons et al., 2025) and (Baker et al., 2025), limiting the novelty. More specifically, Baker et al., 2025 already demonstrated that training against a monitor can lead to obfuscation (and prompting can be seen as a simple way to \"train\" a model to obfuscate without changing the model parameters) and Emmons et al., 2025 demonstrated that when the CoT is necessary for the task, it is harder to obfuscate (and thus the results of section 4.2.2 are a direct consequence where the task is replaced with a side-task)\n- No actionable recommendations provided in order to mitigate the adversarial prompt effects\n- Obfuscation via prompting seems to mostly be seen in the Claude family of models but not as much on, e.g., GPT models and Deepseek,   so the generality of the insights is unclear"}, "questions": {"value": "Some comments and questions for the authors\n\n- How is the main task and side-task success rate of the models shown in Figure 2? I believe that grouping the models on Figure 2 by main task performance would better highlight what the models are doing in practice (e.g., is GPT-5 mostly focusing on the main task and successfully solves it whereas it ignores the side-task and thus does not trigger the conditional obfuscation check?)\n- On section 4.2.2 the authors claim that these results suggest that obfuscation suppresses secondary behavior more severely than primary task completion; isn't that straightforward, given that the obfuscation pressure is only for the side-task?\n- Furthermore, on the same section, the authors mention that a CoT monitor does better than an action or CoT + action monitor. Why is this the case? Couldn't the CoT + action monitor just ignore the action if it is detrimental? \n- On the internal vs external CoT discussed on section 5.2; do the authors have specific intuitions into why do the models have higher instruction following rate in the external part than the internal part? At the moment, the authors present just raw results without much discussion about why they manifest.\n\nOverall, based on all of my prior comments, I would say that this work is borderline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OL8O8v2xNk", "forum": "RiU4WP0ld9", "replyto": "RiU4WP0ld9", "signatures": ["ICLR.cc/2026/Conference/Submission13990/Reviewer_Jjtg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13990/Reviewer_Jjtg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901413916, "cdate": 1761901413916, "tmdate": 1762924486737, "mdate": 1762924486737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether reasoning models can obfuscate their chain-of-thought (CoT) to evade monitoring while pursuing hidden objectives. It introduces a taxonomy of prompt-level obfuscation cues, compares internal vs. external CoT robustness under escalating pressure, and evaluates models in toy tasks and SHADE-Arena. Key findings: CoT monitoring is highly effective without obfuscation (~96–98% detection) but collapses under strong pressure (<10% for some models); external CoT is more brittle than internal CoT; vulnerabilities are model-dependent."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses a highly relevant safety question: Can reasoning models deliberately obfuscate their CoT to evade monitoring? This is timely given the growing reliance on CoT for alignment oversight. The introduction frames the problem well and situates it within recent literature on deception and monitorability. \n* Metrics like conditional obfuscation and obfuscated success are well-defined and isolate deliberate concealment from trivial failures.\n* Strong evidence that external CoT is more brittle than internal CoT under obfuscation pressure.\n* Systematic experimental setup combining toy tasks and realistic multi-turn environments.\n* Clear metrics (conditional obfuscation, obfuscated success) that isolate deliberate concealment."}, "weaknesses": {"value": "* Novelty is narrower than claimed:\n    * Fragility of CoT monitoring and model-dependent vulnerabilities were shown in prior work (Emmons et al., Arnav et al., Hubinger et al.).\n    * The fourth contribution (“pressure-dependent vulnerability”) is not new (Baker et al 2025, OpenAI 2025).\n* Most interesting result is the internal vs. external CoT gap; other findings largely confirm known fragility.\n* Composable taxonomy contribution is under-supported in the main text:\n    * Results are buried in Figures 3, 8, 9 and Appendix B.3; no clear analysis of which cues matter most.\n    * No consolidated table or marginal effect discussion.\n* Related work section is dense and does not clearly separate prior gaps from this paper's contributions.\n* Limited scope: only prompt-level obfuscation; ignores RL or fine-tuning strategies.\n* Capability trade-off (stealth vs. side-task success) is noted but not deeply analyzed."}, "questions": {"value": "1. Where are the main results for the composable taxonomy? The contribution is emphasized, but evidence seems buried in figures and appendix without clear discussion of cue importance.\n2. How do you justify the ordering of cues in the pressure scale? Is there any theoretical or empirical rationale beyond convenience?\n3. Given that prior work already showed CoT fragility and model-dependent vulnerabilities, what is fundamentally new here beyond the internal vs. external comparison?\n4. Did you analyze marginal effects of individual cues (e.g., which ones drive most obfuscation)?\n5. Could the observed capability drop under obfuscation be leveraged as a detection signal? Any thoughts on this as a defense strategy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5JsG1eVA8y", "forum": "RiU4WP0ld9", "replyto": "RiU4WP0ld9", "signatures": ["ICLR.cc/2026/Conference/Submission13990/Reviewer_tyVE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13990/Reviewer_tyVE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995152527, "cdate": 1761995152527, "tmdate": 1762924486352, "mdate": 1762924486352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}