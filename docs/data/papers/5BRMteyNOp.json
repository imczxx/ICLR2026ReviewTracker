{"id": "5BRMteyNOp", "number": 1592, "cdate": 1756895087028, "mdate": 1763721699518, "content": {"title": "Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism", "abstract": "The foundation of reproducible science lies in protocols that are precise, logically ordered, and executable. The autonomous generation of these protocols through natural language queries could greatly improve the efficiency of the reproduction process. However, current leading large language models (LLMs) often generate incomplete or inconsistent protocols, limiting their utility. To address this limitation, we first introduce SciRecipe, a large-scale dataset of over 12K structured protocols spanning 27 biological subfields and encompassing both comprehension and problem-solving tasks. To further improve protocol generation, we propose the \"Sketch-and-Fill\" paradigm, which separates analysis, structuring, and expression to ensure each step is explicit and verifiable. Complementing this, the structured component-based reward mechanism evaluates step granularity, action order, and semantic fidelity, aligning model optimization with experimental reliability. Building on these components, we develop Thoth, trained through a staged Knowledge-to-Action process that progresses from knowledge acquisition to operational reasoning and ultimately to robust, executable protocol generation. Across multiple benchmarks, Thoth consistently surpasses both proprietary and open-source LLMs, achieving significant improvements in step alignment, logical sequencing, and semantic accuracy. Our approach paves the way for reliable scientific assistants that bridge knowledge with experimental execution.", "tldr": "We introduce SciRecipe, a large-scale protocol dataset, and the SCORE mechanism with the “Sketch-and-Fill” paradigm, enabling Thoth to generate precise, executable scientific protocols that surpass existing LLMs.", "keywords": ["AI for biology", "Protocol Generation", "Scientific Reasoning", "Large Language Model", "Reinforcement Learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7854a9ea4121742138fc708af8c175a8d59be6f0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "### Overview\nThis paper proposes a new dataset (SciRecipe) and metric (SCORE) to evaluate the quality of scientific experimental protocols generated by language models. They introduce a formalization of the protocol generation process (Sketch-and-Fill) and use it in conjunction with the SCORE reward to train Qwen3-8B via supervised fine-tuning and GRPO on the collected datasets and reward function. The main contribution is the development of a formal specification for protocol generation that enables systematic rule-based validation of the generated protocols. The effectiveness of this formalization is demonstrated by a suite of experiments in which a language model trained to generate protocols by following this spec and reward achieve higher performance than baselines on the SciRecipe benchmark and some public scientific QA benchmarks. \n\nWhile the motivation for the benchmark and formalization are well introduced, I am leaning towards rejection because (1) it is not clear to me that the tasks in SciRecipe-Eval are truly benchmarking protocol generation, (2) the choices made in the SAF formalization can limit the expressiveness of possible protocols, (3) the terms in the SCORE reward function are not sufficiently justified, nor are they shown to capture meaningful fallacies / weaknesses in existing protocols, (4) the metrics used to compare models are either too simple (ROUGE, BLEU) or novel metrics introduced by the authors (Step-M, Order-S, etc.) that are not introduced in the main text (hidden in appendix)\n\n### Main argument for reject\nMany of the choices made by the authors for both the formalization framework (SAF) and the reward function (SCORE) have weaknesses and lack justification. The SAF framework seems overly restrictive to me. For instance, how does one encode branching logic in this `(Action, Object, Parameters)` format? Given the importance of this formalization for the rest of the paper, it would be nice if the authors could provide some quantitative way of measuring how well this formalization captures the semantics of an existing diverse corpus of high quality protocols. On the other hand, the SCORE reward function can only be applied to models that adopt the SAF generation framework, limiting its generality. It is not evident to me that the combination of order and semantic consistency are sufficient to capture the critical fallacies of existing protocols. An important experiment here would be to show quantitatively that the proposed reward function is sensitive to critical issues in existing protocols that scientific professionals would be able to identify. Finally, the evaluation metrics and the benchmark (SciRecipe-Eval) used for protocol generation (Section 4.2) are not clearly measuring what it claims. As far as I can tell, the SciRecipe-Eval tasks are not actually protocol generation tasks, but rather a collection of adjacent tasks, more so related to protocol comprehension and manipulation. Can the authors clarify why they claim that these tasks (in appendix B) are benchmarking generation rather than understanding and reasoning? Moreover, the evaluation metrics used to compare different models in section 4.2 are either too simple (ROUGE, BLEU, etc.), which the authors themselves have acknowledged, or are novel metrics (Semantic-A, Order-LCS, Order-S, Order-Tau, Step-M) that are not introduced in the main text (Appendix F) and seem to correspond to terms that were actually used the in SCORE reward function to optimize the author's own method (Thoth). This does not seem like a fair comparison, nor does it convincingly capture the quality of the generated protocol as mentioned earlier.\n\n### Things to improve the paper that did not impact the score:\n- there should be citations for some of the claims made in the introduction such as:\n  - \"\"\"*As a result, researchers often find that models offer fragmented recommendations on experimental procedures but fall short of producing concise, logically ordered protocols that can be directly implemented in laboratory workflows*\"\"\"\n  - \"\"\"*the generated protocols often contain unordered steps, redundant operations, factual inconsistencies, or hallucinated actions, undermining both reproducibility and scientific credibility.*\"\"\"\n- the introduction of key evaluation metrics should be made in the main text"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation of the paper is well articulated\n- To my knowledge, the SAF formalization and the SCORE reward are original\n- The curation of the dataset is a meaningful contribution\n- The curation process is well documented in the appendix\n- I appreciate the thoroughness of the experiments and ablations"}, "weaknesses": {"value": "- The tasks in the benchmark are not actually benchmarking protocol generation\n- The design choices in the formalization and reward function lack justification and validation\n- The evaluation metrics used to compare models in the experimental section are either too simple or unfairly recapitulate the design choices in the reward function used to train the model"}, "questions": {"value": "See summary section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z1pgQspfkP", "forum": "5BRMteyNOp", "replyto": "5BRMteyNOp", "signatures": ["ICLR.cc/2026/Conference/Submission1592/Reviewer_ULSZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1592/Reviewer_ULSZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761420909318, "cdate": 1761420909318, "tmdate": 1762915828562, "mdate": 1762915828562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the capability of modern large language models (LLMs) to perform scientific protocol design, focusing on the generation of correct and sense-making wet-lab protocols. The authors curate a large-scale benchmark of biological protocols and propose Thoth, an RL-finetuned variant of Qwen3-8B. Thoth is trained with the proposed Knowledge-to-Action process and a novel SCORE reward mechanism that encourages logical ordering, completeness, and fidelity of the generated protocol. Experimental results suggest that Thoth improves over strong LLM baselines regarding the tasks of interests."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper offers an end-to-end pipeline for studying scientific reasoning in LLMs regarding the biological protocol design, including dataset construction, structured prompting, and RL-based finetuning. This integration is well thought out and can serve as a good example for future research in ai4sci.\n- The curated biological protocol dataset , SciRecipe fills an important gap between free-form scientific text and machine-executable protocols. It provides valuable supervision for procedural reasoning tasks.\n- The authors conduct comprehensive benchmarking against a wide variety of strong models (gpt-4, deepseek-R1, etc.) and perform multiple ablations, showing consistent improvements in structural accuracy and executability."}, "weaknesses": {"value": "- The study of Thoth model relies primarily on the Qwen3-8B backbone. Including additional base models (larger parameters to 32B, or other pre-trained such as deepseek-v3) could strengthen the claim that Thoth’s improvements convincingly stem from the training framework.\n- While the SCORE metric captures fidelity, it remains unclear how well it correlates with real experimental executability (act like some heuristics). Incorporating partial human or domain-expert evaluation and examine the correlation between the SCORE reward and real \"fidelity\", even on a small subset, would better improve the practical credibility of the results."}, "questions": {"value": "- Could the authors further clarify the motivation behind step-level alignment? Is reproducing the exact number of steps in the reference protocol necessary for practical success, or would multiple granularities (eg., merged or decomposed steps) be acceptable as long as the outcome is correct, similar to math problems?\n- The SCORE mechanism (Fig 2) seems to encourage near-exact replication of ground-truth protocols, which could risk overfitting like SFT. Yet in Fig 5, Thoth (Stage 2 + 3) shows stronger gains compared to its non-RL counterpart (ie. SFT only). Could the authors elaborate how SCORE avoids over-constraining the model and still enables genuine reasoning improvements, if this happens?\n- How well would the Thoth framework generalize to other scientific domains (eg. chemistry or material synthesis protocols)? Are there any domain-specific assumptions baked into the data preprocessing or reward computation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MtFFI5BAm9", "forum": "5BRMteyNOp", "replyto": "5BRMteyNOp", "signatures": ["ICLR.cc/2026/Conference/Submission1592/Reviewer_RwGZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1592/Reviewer_RwGZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707826135, "cdate": 1761707826135, "tmdate": 1762915828391, "mdate": 1762915828391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "&nbsp;\n\nThe authors introduce Thoth, a model trained for biological protocol generation. To train Thoth, the authors introduce a dataset of 12K protocols termed SciRecipe. The main technical innovation of the paper is the SCORE mechanism that enables both RL training and evaluation of protocol generation. The authors rigorously evaluate the components of Thoth through multiple ablation studies and sensitivity analyses. Overall, the work is clearly impactful, the empirical analysis is extensive, and the paper's writing and presentation are excellent. My sole concern lies in assessing the reproducibility of the authors' results given that the codebase is not provided (though the authors have described their approach in meticulous detail). If the codebase is released during the review process to address this concern, I will upgrade my score and champion the paper to be considered as a spotlight/oral contribution.\n\n&nbsp;"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "&nbsp;\n\nThe work undertaken in this paper is highly impactful. The authors have contributed concrete artifacts such as a dataset of 12K structured scientific protocols, as well as technical innovations such as the SCORE mechanism which significantly advances RL training for protocol generation. Furthermore, the empirical results of the authors are extensive and highly informative for researchers in the broader field.\n\n&nbsp;"}, "weaknesses": {"value": "&nbsp;\n\nI describe major and minor points below that the authors may wish to consider.\n\n&nbsp;\n\n**__MAJOR POINTS__**\n\n&nbsp;\n\n1. For the SciRecipe dataset introduced by the authors, it would be useful to include a datasheet for datasets [2] to fully describe the curation process and detail the authors' plans for maintenance of the dataset.\n\n2. As mentioned in the summary, if the authors can release their codebase during the review process, I will be inclined to increase my score.\n\n&nbsp;\n\n**__MINOR POINTS__**\n\n&nbsp;\n\n1. There are missing capitalizations in the references e.g. \"PubMedQA\", \"Kendall\", \"GPT\", \"LLMs\", \"AI\" etc.\n\n2. There are missing conference references e.g. Ahmadian et al. 2024 was published at ACL.\n\n3. In the section on related work, it would be worth the authors discussing the relationship between their work and BioPlanner [1].\n\n4. There is a missing comma at the end of Equations 1, 2, 3, and 5.\n\n5. Reference [3] should be cited as the originating paper for GRPO in place of the DeepSeek-r1 paper in the main body of the text. The correct reference is given by the authors in Section D of the appendix.\n\n6. Line 1093, typo in $a^hat_{i}$.\n\n7. When mentioning PPO in Appendix D, the authors should cite [4].\n\n&nbsp;\n\n**__REFERENCES__**\n\n&nbsp;\n\n[1] O’Donoghue, O., Shtedritski, A., Ginger, J., Abboud, R., Ghareeb, A. and Rodriques, S., 2023, December. [BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology](https://aclanthology.org/2023.emnlp-main.162/). In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (pp. 2676-2694).\n\n[2] Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J.W., Wallach, H., Iii, H.D. and Crawford, K., 2021. [Datasheets for datasets](https://dl.acm.org/doi/fullHtml/10.1145/3458723). Communications of the ACM, 64(12), pp.86-92.\n\n[3] Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y.K., Wu, Y. and Guo, D., 2024. [DeepSeekMath: Pushing the limits of mathematical reasoning in open language models](https://arxiv.org/abs/2402.03300). arXiv preprint arXiv:2402.03300.\n\n[4] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. [Proximal policy optimization algorithms](https://arxiv.org/abs/1707.06347). arXiv preprint arXiv:1707.06347, 2017.\n\n&nbsp;"}, "questions": {"value": "&nbsp;\n\n1. In Figure 2, Rollout 3, there is a tickmark in the order box despite the rollout having one fewer action relative to the ground truth? Does this still incur a positive reward (as indicated by the checkmark)? **Update**: This would appear to correspond to the strict subsequence order consistency. I would have thought that the addition of an extra step in the protocol would be damaging to the experiment?\n\n2. In Equation 2, there is an expectation over $y$, the protocol generated from the model $\\pi_{\\theta}$. Why is there a distribution over $y$. I would have assumed that a context $x$ denotes a specific experimental task and so this would imply that the distribution is over repeatedly sampled protocols for the same specific experimental task $x$?\n\n3. In Table 3, each row is the SCORE mechanism evaluated without a single component only?\n\n4. In Section D of the appendix, Equations 13 and 14, the authors do not appear to give a KL penalty for the GRPO definition. Was the KL penalty omitted for the authors' experiments?\n\n&nbsp;"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "&nbsp;\n\nNo ethical concerns identified.\n\n&nbsp;"}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XHObecNrML", "forum": "5BRMteyNOp", "replyto": "5BRMteyNOp", "signatures": ["ICLR.cc/2026/Conference/Submission1592/Reviewer_btq2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1592/Reviewer_btq2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961752202, "cdate": 1761961752202, "tmdate": 1762915828238, "mdate": 1762915828238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a large scale dataset of protocols across many domains in biology. Authors also propose a “Sketch-and-Fill” reasoning paradigm, a structured approach to protocol generation to organize the output into reasoning, structuring and expression steps. This structured output allows them to define a multi gated reward function for formatting, consistency, order of steps etc.  Using these components, they present a trained model called Thoth which achieves improvement in performance compared to base LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Authors presented comprehensive benchmarks across a wide range of LLMs. Authors also considered real-world context in experimental protocol creation in the way the framework and rewards are designed. The paper is also written well and the authors included a comprehensive appendix.\n- The trained Thoth model (including the mini) demonstrates strong performance across all evaluation metrics in Table 1, highlighting both the model’s abilities and the value of the dataset introduced by the authors."}, "weaknesses": {"value": "See questions below"}, "questions": {"value": "- Authors should compare the sketch and fill paradigm and the multi stage generation to performance of frameworks like chain of thought prompting to clarify the added value of sketch-and-fill for generating structured output. \n- Evaluating models ability to do multi step mathematical reasoning is also another idea for benchmarking LLM models for protocol generation,  for tasks like adjusting reagent concentrations or dilution rations which could influence quantitative changes in the downstream steps."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GKyvVGMkHF", "forum": "5BRMteyNOp", "replyto": "5BRMteyNOp", "signatures": ["ICLR.cc/2026/Conference/Submission1592/Reviewer_WL6o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1592/Reviewer_WL6o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1592/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997321236, "cdate": 1761997321236, "tmdate": 1762915828088, "mdate": 1762915828088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}