{"id": "nCgMpMDgR2", "number": 13321, "cdate": 1758216464540, "mdate": 1759897445518, "content": {"title": "ReFIne: A Framework for Trustworthy Large Reasoning Models with Reliability, Faithfulness, and Interpretability", "abstract": "Recent advances in long chain-of-thought (CoT) reasoning have largely prioritized answer accuracy and token efficiency, while overlooking aspects critical to trustworthiness. We argue that usable reasoning systems must be trustworthy, characterized by three properties: interpretability, faithfulness, and reliability. To this end, we propose $\\textbf{\\texttt{ReFIne}}$, a new training framework that integrates supervised fine-tuning with GRPO to encourage models to: (i) improve interpretability by producing structured, tag-based traces with high-level planning that are easier for humans to follow; (ii) enhance faithfulness by explicitly disclosing the decisive information guiding each solution, with consistent cross-section references; and (iii) promote reliability by providing self-assessments of both the derivation’s soundness and the confidence of the final answer. We apply $\\textbf{\\texttt{ReFIne}}$ to the Qwen3 models at multiple scales (1.7B/4B/8B) and evaluate across mathematical benchmarks of varying difficulty. Our experimental results show that $\\textbf{\\texttt{ReFIne}}$ models generate clearer and better-structured reasoning traces (interpretability +44.0\\%), more faithfully expose their underlying decision process (faithfulness +18.8\\%), and offer informative confidence estimates (reliability +42.4\\%). These findings highlight an overlooked but important direction: reasoning models should be optimized not only for accuracy, but also for broader dimensions of trustworthiness.", "tldr": "", "keywords": ["Large Reasoning Models", "Trustworthy Machine Learning", "Reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e8b93a4ef8eae5a1be8ec7560b0e44c6463401b4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes ReFIne, a two-stage training framework (SFT + GRPO) designed to improve Large Reasoning Models along three dimensions: interpretability, faithfulness, and reliability. The authors separate reasoning into distinct phases (understanding, facts, plan, think, self-assessment).  They also perform RL with specifically designed rewards to enhance such behavior. They evaluate ReFIne on Qwen3 models (1.7B–8B) across four mathematical benchmarks, reporting substantial improvements in all three trustworthiness metrics while maintaining competitive accuracy and even improving token efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "a. The introduction of interpretability, faithfulness, and reliability is well-motivated and addresses real gaps in language model evaluation.\n\nb. The paper includes multiple quantitative metrics for quantitative evaluation of trustworthy language reasoning models such as cross-section referencing rates, disclosure faithfulness.\n\nc. RL with rewards designed for cross-section reference and confidence estimation can largely boost the performance in interpretability, faithfulness and reliability.\n\nd. The structured reasoning leads to shorter reasoning traces with competitive accuracy with Plain-Qwen in this study."}, "weaknesses": {"value": "a.Readability and faithfulness are judged by QwQ-32B, no human evaluation is reported in this study.\n\nb. The Plain-Qwen model is a weak baseline for comparison, as it is only trained on the thinking processes of the SFT dataset. This does not provide a robust comparison against a strong, general-purpose CoT or reasoning model. The paper should ideally report the performance of a more capable model, such as Qwen3-8B-Thinking, on the novel metrics (interpretability, faithfulness, and reliability) to provide a more meaningful context.\n\nc. There appears to be a discrepancy between the reported results and the qualitative evidence. Specifically, Figure 4 suggests that ReFine achieves shorter reasoning traces, yet the examples provided in the Appendix demonstrate traces that are noticeably much longer than those generated by Plain-Qwen. This contradiction requires clarification or re-evaluation.\n\nd. The proposed trustworthiness metrics appear highly coupled with the specific templates and reasoning structure used in this study. It is unclear if these metrics generalize to models trained with different reasoning formats, potentially limiting their broader applicability in the field."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2LLPTQdZuP", "forum": "nCgMpMDgR2", "replyto": "nCgMpMDgR2", "signatures": ["ICLR.cc/2026/Conference/Submission13321/Reviewer_Npia"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13321/Reviewer_Npia"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811251669, "cdate": 1761811251669, "tmdate": 1762923982467, "mdate": 1762923982467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on the trustworthiness of LRMs.\nThe authors define trustworthiness along three dimensions: interpretability, faithfulness, and reliability.\nThey propose ReFIne, a two-stage training framework to instill these properties, including SFT and GRPO. Experiments on Qwen3 models across four mathematical benchmarks show that ReFIne significantly improves the trustworthiness while maintaining comparable accuracy and slightly improving token efficiency compared to baseline models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a clear, three-part definition for trustworthy reasoning, moving beyond the standard metrics of accuracy and efficiency. This framework addresses a critical and often-overlooked gap in LRM development.\n2. The ReFIne framework is well-designed, combining SFT to teach the desired structure and GRPO to enforce the target behaviors.\n3. The framework demonstrates significant quantitative improvements across all three target dimensions."}, "weaknesses": {"value": "1. Several key metrics rely on another large model for evaluation. This introduces potential evaluation bias, and the results would be stronger if supplemented with human evaluation to confirm that the model-judged improvements translate to a better human-user experience.\n2. While generally comparable, the ReFIne models show a slight decrease in accuracy on the AIME-2024. Although minor, this suggests a potential trade-off between enforcing a strict, trustworthy structure and achieving peak accuracy on highly complex problems."}, "questions": {"value": "1. The ReFIne framework proves highly effective for structured, step-by-step mathematical problems. How could this framework be adapted for tasks that are inherently less structured, such as commonsense reasoning, where some of the commonsense cannot be verified?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GEmV4QVceP", "forum": "nCgMpMDgR2", "replyto": "nCgMpMDgR2", "signatures": ["ICLR.cc/2026/Conference/Submission13321/Reviewer_RVD7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13321/Reviewer_RVD7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883772100, "cdate": 1761883772100, "tmdate": 1762923982156, "mdate": 1762923982156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ReFIne training framework that aims to enhance the trustworthiness of large reasoning models from three dimensions: interpretability, faithfulness, and reliability. ReFIne integrates supervised fine-tuning with GRPO optimization and adopts a structured, tag-based reasoning format (e.g., understanding, facts, plan, thinking, final answer, and self-assessment). It further incorporates cross-section references to highlight decisive information and generates calibrated confidence estimates for model outputs. The ReFIne is evaluated on reasoning models with different parameter scales using mathematical benchmarks of varying difficulty. Experimental results show that ReFIne improves the clarity of reasoning traces, the accuracy of causal attribution, and the calibration of confidence estimates, while maintaining accuracy and modestly improving reasoning efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses the important challenge of enhancing the trustworthiness of reasoning models through a structured training framework that jointly optimizes interpretability, faithfulness, and reliability.\n\n2. The integration of supervised fine-tuning and GRPO within the structured tag-based reasoning format is coherent and purpose-driven, effectively aligning training objectives with model trustworthiness.\n\n3. The experimental results show improvements across multiple evaluation metrics, indicating the effectiveness of the proposed framework."}, "weaknesses": {"value": "1. The distinction between ReFIne and existing reasoning training paradigms is not clearly articulated.\n\n2. The proposed definitions of interpretability, faithfulness, and reliability lack theoretical justification and formal grounding.\n\n3. The paper lacks ablation and sensitivity analyses to verify the contribution and stability of each component.\n\n4. The experiments are limited to mathematical reasoning tasks, which restricts the generalization of the framework.\n\n5. The paper does not analyze computational cost or the trade-off between efficiency and performance."}, "questions": {"value": "1. The paper should provide a more thorough discussion of existing reasoning training paradigms and clearly distinguish them from the proposed ReFIne framework. The authors are advised to identify the remaining challenges in prior work and explicitly show how ReFIne addresses these gaps to better emphasize its novelty and contribution.\n\n2. The paper introduces new definitions of interpretability, faithfulness, and reliability as dimensions of model trustworthiness and proposes corresponding metrics for their evaluation. However, these concepts currently lack theoretical justification or formal grounding, which undermines their credibility. The authors are encouraged to include theoretical analysis or cite relevant studies to strengthen the conceptual foundation of these definitions.\n\n3. The paper should include ablation experiments on reward weighting, confidence calibration, and reference-linking incentives to verify the contribution of each component. It is also advisable to analyze the sensitivity of the framework to random seeds and data order to ensure that the reported improvements stem from specific design choices rather than training instability. In addition, the authors should test ReFIne under conditions with noisy or missing verification cues to examine whether it maintains reliable attribution and calibrated confidence in more challenging scenarios.\n\n4. The generalizability of the proposed framework is central to its practical value. However, the experiments are confined to mathematical reasoning tasks, which limits the evidence for broader applicability. The authors should extend the evaluation to other reasoning domains, such as commonsense reasoning, scientific question answering, and code analysis, and assess performance on additional model families beyond the Qwen series.\n\n5. The paper should include a concise analysis of computational cost and efficiency by reporting token overhead, reasoning latency, and memory usage. It is also recommended to investigate whether ReFIne can retain comparable improvements when using fewer or simplified reasoning sections, thereby clarifying the balance between efficiency and effectiveness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns are identified."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O8EMEReRkk", "forum": "nCgMpMDgR2", "replyto": "nCgMpMDgR2", "signatures": ["ICLR.cc/2026/Conference/Submission13321/Reviewer_BTMG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13321/Reviewer_BTMG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911382617, "cdate": 1761911382617, "tmdate": 1762923981894, "mdate": 1762923981894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose ReFIne, a training pipeline to improve the reliability, faithfulness, and interpretability of chain-of-thoughts. The key contribution lies in defining a structured data generation pipeline which prompts an LLM to organize CoTs in a certain format (e.g., with problem understanding, listing facts, planning, etc) which are then used to do SFT and further improved upon with GRPO with different format rewards. Having trained models to generate such structured thoughts, the authors show that these thoughts are more readable, faithful, and interpretable and the models also maintain comparable accuracy to those trained without such structured CoTs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Getting LLMs to generate thoughts that are faithful and interpretable is potentially a useful problem to study (although a large body of past work has concluded that CoTs are generally unfaithful and improving that at scale has led to limited success)."}, "weaknesses": {"value": "* I'm largely of the opinion that CoTs should primarily be seen as a mechanism of leveraging additional compute for LLMs to improve performance and while improving the readability of thoughts is also potentially useful, they should not come at the cost of reduction in accuracy. Hence, in this case, Figure 3 is the most important result where the authors compare the final downstream performance. AIME, which is arguably the hardest benchmark, shows a drop in performance at all three scales which is not desirable. I think it also suggests that hand-designing the structure of CoTs is potentially not a scalable solution for harder reasoning domains and eventually wont be useful for frontier models or as a general method for building reasoning models.\n\n* I also think that the claims around models being more faithful and interpretable aren't well supported. For example, the observation that the trained models generate thoughts that point to the facts listed before, etc don't necessarily make them interpretable. Rather, I'd think of it as just imitating the training data where thoughts were curated in that format. \n\n* Some details on the experimental setup are unclear/missing. Plots dont accurately reveal the actual change in performance. Please see my questions below."}, "questions": {"value": "* Are the Plain and ReFIne models trained starting from the Qwen3 base models or the instruct models? Either way, can you show a table (not a plot), with the accuracy numbers of (1) Qwen3-1.7B/4B/8B, (2) Plain-Qwen3 (trained with standard GRPO on the same training data), and (3) ReFIne-Qwe3?\n\n* Since Qwen3 models are heavily tuned on these math benchmarks and math500 and gsm8k are already saturated, can you experiment on some other harder math benchmarks like AIME25, HMMT24, HMMT25, etc?\n\n* How do you think this method can improve a frontier reasoning model where you do not have access to a stronger model to prompt and get these structured traces?\n\n* Why do you think pointing to earlier parts of the thoughts (e.g., facts, problem understanding, etc) makes the model more interpretable or faithful? It could point to it and still not use it (causally) to derive the next steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0xPt1aRr54", "forum": "nCgMpMDgR2", "replyto": "nCgMpMDgR2", "signatures": ["ICLR.cc/2026/Conference/Submission13321/Reviewer_UCM1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13321/Reviewer_UCM1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142922535, "cdate": 1762142922535, "tmdate": 1762923981607, "mdate": 1762923981607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}