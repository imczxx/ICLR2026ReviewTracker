{"id": "Zga6TPc0aT", "number": 9978, "cdate": 1758153920386, "mdate": 1759897682245, "content": {"title": "Balanced Actor Initialization: Stable RLHF Training of Distillation Reasoning Models", "abstract": "The development of alignment and reasoning capabilities in large language models has seen remarkable progress through two paradigms: instruction tuning and reinforcement learning from human feedback (RLHF) alignment paradigm, and distillation-based reasoning fine-tuning paradigm. While both approaches prove effective independently, the third paradigm of applying RLHF to distillation-trained models presents significant challenges.  Our investigation reveals two critical phenomena that emerge in this paradigm: Sequence Length Collapse, where language generation dramatically reduces during early RLHF training, and the Reward Hockey Stick Curve, featuring severe reward score drops followed by gradual recovery. These instabilities fundamentally compromise the model's alignment and reasoning capabilities. To address these challenges, we propose Balanced Actor Initialization (BAI), a two-stage weighted model merging approach. BAI first merges instruction-following and distillation-based reasoning fine-tuned models, then further combines this intermediate model with the pretrained model to preserve foundational knowledge. Through comprehensive experiments across diverse benchmarks and detailed analysis of training experiments, we demonstrate that BAI resolves Sequence Length Collapse, mitigates the Reward Hockey Stick Curve, and enables continuous sequence length improvement during training. Our analysis reveals that balanced merging ratios achieve optimal trade-offs between training stability and reasoning capability preservation. Our work provides the effective solution for stable training in this third paradigm, enabling more capable reasoning models that combine distillation efficiency with RLHF alignment.", "tldr": "", "keywords": ["Large Language Model", "Posttrain", "Natural Language Processing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3b63833469c5b4168981ac1daadab3c944bbf0a8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on the instability encountered when applying Reinforcement Learning from Human Feedback (RLHF) to language models during fine-tuning. The authors identify two key phenomena — Sequence Length Collapse and the Reward Hockey Stick Curve — and propose a two-stage weighted model merging method, termed Balanced Actor Initialization (BAI), to mitigate these issues. Experiments across multiple benchmarks demonstrate improved stability and overall performance.\n\nKey Reasons:\n1. The contribution is incremental and lacks theoretical depth. \n2. The limited generalization and modest novelty further constrain its impact.\n\nSupporting Arguments\n\nWhile the paper clearly outlines two empirical phenomena and offers a practical workaround, the overall contribution remains incremental. BAI is essentially a static weighted average of existing models—a technique that has been extensively explored in model merging literature. All experiments are confined to one MoE model family at modest scale; without evidence that findings generalize to denser or larger models, the practical impact of BAI for the broader community is uncertain."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a practical and relevant problem in the intersection of RLHF and model distillation, which is a critical aspect of post-training pipelines.\nThe proposed method is simple, interpretable, and supported by extensive ablations on merging ratios."}, "weaknesses": {"value": "While the problem is meaningful, the proposed weighted model merging approach is relatively straightforward and lacks substantial algorithmic innovation.\nThe paper provides limited theoretical grounding — there is no formal analysis or modeling to explain why the identified phenomena occur or why BAI effectively mitigates them.\nAll experiments are conducted on a single model family (OLMoE). It remains unclear whether the proposed approach generalizes to other architectures or larger-scale models."}, "questions": {"value": "Can you evaluate the proposed method on other model architectures or larger-scale language models to assess its generality?\nCan you provide a theoretical explanation or empirical analysis to clarify why the proposed merging strategy improves training stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FkFiedrlvS", "forum": "Zga6TPc0aT", "replyto": "Zga6TPc0aT", "signatures": ["ICLR.cc/2026/Conference/Submission9978/Reviewer_8GyN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9978/Reviewer_8GyN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761532567778, "cdate": 1761532567778, "tmdate": 1762921414391, "mdate": 1762921414391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of stabilising reinforcement‐learning‐from‐human‐feedback (RLHF) when applied to reasoning capability models (i.e., models fine‐tuned via distillation for reasoning tasks). It identifies two major failure modes: sequence‐length collapse (outputs becoming suddenly much shorter during training) and the reward “hockey‐stick” curve (sharp drop then rise in reward values). To address these, the authors propose Balanced Actor Initialization (BAI): a two-stage model merging approach that first merges an instruction-following SFT model with a reasoning-distilled SFT model, and then further merges that result with the pretrained base model under controllable weights. Empirical results across multiple reasoning, math and coding benchmarks show that BAI alleviates the instability issues and delivers superior performance compared to existing baselines (instruction-tuning + RLHF; distillation alone) in the “distill + RLHF” setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple yet effective method: The proposed Balanced Actor Initialization (BAI) is conceptually simple, easy to implement, and does not require additional data or complex architectural changes, yet achieves substantial stability and performance gains.\n2. Well-motivated problem identification: The paper clearly articulates the instability issues that arise when combining reasoning fine-tuning with RLHF—specifically sequence length collapse and reward hockey-stick curves. The motivation is strong and timely, addressing a crucial bottleneck in post-training reasoning models.\n3. Comprehensive experiments and analysis: The authors conduct extensive evaluations across multiple benchmarks (knowledge, math, code) and provide detailed analyses, including training dynamics (reward, KL, length evolution). The results consistently demonstrate the effectiveness and robustness of BAI.\n4. Clarity and presentation: The paper is well written, logically structured, and easy to follow. Figures and tables effectively support the main claims, making the overall contribution accessible and convincing."}, "weaknesses": {"value": "1.While the stated goal is to improve both reasoning capability and instruction-following / human alignment, the current evaluation focuses primarily on reasoning and code tasks. It lacks comprehensive benchmarks that directly measure instruction-following or alignment quality (e.g., preference or helpfulness evaluations). This leaves it unclear whether the proposed BAI method genuinely enhances alignment behavior or mainly acts as a form of checkpoint ensembling. Moreover, since the reported gains may result from combining multiple fine-tuned models, it would be important to de-couple and quantify the respective contributions versus the merging process itself. Finally, the evaluation includes only a single model family, which limits the strength of the empirical justification for the broader claims.\n2. Missing implementation and experimental details. Some important training details are under-specified. The paper references experiments with MoE-2.5B and MoE-25B models, but only presents one set of results—making it unclear which base model and exact configuration were used. Similarly, the specifics of the instruction fine-tuning (IFT), reasoning distillation, and RLHF checkpoints or datasets employed for post-training are not fully described. These omissions limit the reproducibility and make it difficult to interpret the generality of the findings."}, "questions": {"value": "1. Goal of RLHF on distilled reasoning models:\nWhat is the main motivation for applying RLHF to the distilled reasoning SFT models? Is the intended outcome to further enhance reasoning ability, to improve alignment and instruction-following, or to balance both? Clarifying this goal would help contextualize the proposed method and evaluation choices.\n\n2. Interpretation of Figure 2 (sequence length trends):\nFigure 2 presents the training dynamics “according to sequence length.” Could the authors explain the rationale behind using sequence length as the central metric? How does it relate quantitatively to stability or reasoning performance?\n\n3. Model checkpoints and datasets:\nCould the authors provide more details on the model configurations and datasets used? Specifically:\n Which base model(s) were used for MoE-2.5B and MoE-25B experiments?\n What instruction-tuning, reasoning distillation, and RLHF checkpoints were used for initialization?\n Which datasets were involved in each of these post-training stages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aBdUOMZUc6", "forum": "Zga6TPc0aT", "replyto": "Zga6TPc0aT", "signatures": ["ICLR.cc/2026/Conference/Submission9978/Reviewer_TtYT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9978/Reviewer_TtYT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761545614987, "cdate": 1761545614987, "tmdate": 1762921414053, "mdate": 1762921414053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates instability when applying RLHF to reasoning models trained via distillation. It identifies Sequence Length Collapse and the Reward Hockey Stick Curve as key issues and proposes Balanced Actor Initialization (BAI), a two-stage weighted model merging method. BAI stabilizes RLHF training and improves reasoning consistency and alignment performance across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper identifies two previously underexplored training instabilities, including Sequence Length Collapse and the Reward Hockey Stick Curve, that emerge when combining reasoning distillation with RLHF, offering new empirical insights into this third training paradigm.\n- The proposed BAI is simple, elegant, and effective: it leverages a two-stage weighted model merging scheme to stabilize training without requiring extra data or architecture changes.\n- Extensive experiments on diverse benchmarks (e.g., MMLU, SuperGPQA, AIME, ArenaHard) show consistent and interpretable improvements, highlighting the generality of the approach."}, "weaknesses": {"value": "- As for MixEval-Hard (Table 1), Paradigm 3 is the best\n- While the paper clearly identifies Sequence Length Collapse and Reward Hockey Stick Curve, the mechanistic explanation of why these phenomena emerge remains descriptive rather than causal. It would be stronger to connect them to PPO dynamics or gradient-level behavior.\n- The BAI merging strategy is empirically effective but largely heuristic; the paper does not provide a theoretical justification or convergence analysis explaining why certain α–β ratios stabilize training. The method’s improvements may depend on carefully chosen weight ratios (e.g., α=0.1, β=0.9), but the paper lacks guidelines or an adaptive scheme for selecting these ratios across tasks."}, "questions": {"value": "- Could the authors provide a more detailed analysis of why Sequence Length Collapse and Reward Hockey Stick Curve emerge, perhaps through gradient or reward distribution visualization during PPO training?\n- Have the authors explored whether different reward model architectures or reward scaling strategies affect these instabilities?\n- Since the optimal BAI ratio (e.g., α=0.1, β=0.9) seems task-dependent, could the authors discuss whether an adaptive or data-driven merging scheme might generalize better across domains?\n- The results on MixEval-Hard (Table 1) show that Paradigm 3 without BAI slightly outperforms BAI. Could the authors explain this discrepancy and clarify under what conditions BAI might underperform?\n- Could the authors analyze the computational overhead or efficiency trade-off of the two-stage model merging compared to conventional initialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XZkQWti1Nk", "forum": "Zga6TPc0aT", "replyto": "Zga6TPc0aT", "signatures": ["ICLR.cc/2026/Conference/Submission9978/Reviewer_1GVs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9978/Reviewer_1GVs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974366411, "cdate": 1761974366411, "tmdate": 1762921413753, "mdate": 1762921413753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies two training instabilities when applying RLHF to distillation-based reasoning models Sequence Length Collapse and the Reward Hockey Stick Curve and proposes Balanced Actor Initialization (BAI), a two-stage linear model-merging approach. Experiments show that BAI stabilizes RLHF training, avoids length collapse, and slightly improves performance across diverse benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses a practical and relevant problem in reasoning-oriented RLHF.\n\n2. Clearly identifies and visualizes two important training pathologies.\n\n3. The proposed method is simple and computationally inexpensive.\n\n4. Ablations across merging ratios are thorough, and diagnostics are informative.\n\n5. Results show consistent, albeit small, improvements across several benchmarks."}, "weaknesses": {"value": "#### **1. Limited conceptual novelty**\nBAI reduces to linear interpolation between checkpoints. Model merging is well-studied, and the method does not demonstrate novelty beyond existing approaches. Furthermore, the paper does not compare against stronger or more recent model-merging baselines (e.g., TIES-Merging, DARE, Fisher-weighted averaging), making it difficult to assess the contribution relative to the state of the art.\n\n#### **2. Unequal training lengths confound fairness**\nIn several comparisons, Paradigm 3 + BAI is trained for 3000 steps while baselines use only 1600 steps. This discrepancy can inflate observed improvements. Training budgets should be the same to ensure fairness.\n\n#### **3. No repetitions / variance / seeds reported**\nRLHF training is inherently noisy. The paper reports only single-run numbers without standard deviations, confidence intervals, or statistical significance tests. As a result, it is unclear whether the reported improvements are robust or simply due to randomness.\n\n#### **4. No experimental Diversity**\nExperiments focus solely on PPO and a single model family. It remains unclear whether the observed training instabilities generalize to other RLHF algorithms (e.g., DPO/GRPO variants) or to different LLM sizes and/or architectures. Without broader coverage, claims about “fundamental” phenomena are difficult to validate.\n\n#### **5. Insufficient explanation of “flexible”**\nSection 3.1 asserts that BAI remains applicable when only the reasoning-SFT model is available, but the paper does not explain how instruction-following capability is preserved in that scenario. When one model type is missing, the method risks becoming obsolete.\n\n#### **6. Missing important entries in Table 1**\nThe paper references a two-stage merging procedure, yet Table 1 lacks:\n\n* BAI using only Stage 1,\n\n* BAI using only Stage 2,\n\n* Full two-stage BAI.\n\nThese ablations are necessary to isolate gains, otherwise the reader cannot attribute improvements to either stage.\n\n#### **7. Missing important entries in Table 2**\n* A “Total / Overall” row would improve readability and comparison.\n\n* $\\alpha = 0$ (no pretrained mixing) is missing; should be included to isolate Stage 1’s effect.\n\n#### **8. Incomplete literature review**\nRelevant model-merging literature is not discussed in the main text. Although a brief discussion exists in the appendix, it is **never referenced** in the main text and may lead readers of the main text to miss that. Related work (e.g., MergeKit, fisher merging, DARE, TIES) should be highlighted and placed in the main text.\n\n#### **9. Writing clarity and terminology issues**\n\nThe manuscript is difficult to follow due to overly long paragraphs, excessive repetition (e.g., frequent use of “paradigm”), inconsistent terminology usage (e.g., RL vs. reinforcement learning, RLHF vs. reinforcement learning from human feedback), and undefined technical terms such as GAE. Several claims rely on qualitative descriptions (“healthy optimization dynamics,” “balanced gradients”) without quantitative support.\n\n#### **10. Potentially misleading citation placement**\n\nThe related work section places GRPO citations immediately after discussing distillation-based reasoning (Paradigm 2), which may incorrectly imply that Paradigm 2 uses GRPO. This attribution should be clarified.\n\n#### **11. (minor) Inconsistent guidance on $\\alpha$  selection**\nThe paper highlights $\\alpha = 0.6$ (line 25) in text but $\\alpha = 0.1$ was actually fixed for the key results of BAI of Table 1, which indeed yields stronger benchmark results.\n\n#### **12. (minor) Redundant notation**\nIn Equation (2), $\\beta$ is defined but always equal to $1−\\alpha$. This additional parameter increases notation complexity without providing conceptual utility.\n\n#### **13. (minor) Unused general formulation**\nEquation (1) generalizes to arbitrary N models, but experiments always use N = 2. This creates an impression of generality that is never explored.\n\n#### **14. (minor) Outdated claim on PPO**\nLine 139 claims PPO remains the most widely used for LLM RL. In practice, recent deployments increasingly favor preference-based objectives such as DPO/GRPO variants. This weakens the paper’s motivation for focusing exclusively on PPO."}, "questions": {"value": "1. What is the conceptual novelty of BAI beyond linear checkpoint interpolation and existing model-merging methods?\n\n2. Why is Paradigm 3 + BAI trained for 3000 steps while baselines use 1600? Do improvements hold under equal training budgets?\n\n3. How many seeds were run, and can you report standard deviations or confidence intervals to assess variance?\n\n4. Do the described training instabilities (sequence length collapse, reward hockey stick) occur under other RLHF algorithms (e.g., DPO/GRPO) or other model architectures/sizes?\n\n5. How BAI remains “flexible” when only the reasoning-SFT model exists?\n\n6. Can you provide ablations for: (i) Stage 1 only, (ii) Stage 2 only, and (iii) full BAI?\n\n7. Why is $\\alpha = 0$ missing from Table 2, and can you add an overall/total row for readability?\n\n8. Can you move or cross-reference model-merging related work from the appendix to the main text to properly contextualize novelty?\n\n9. Can you define technical terms and stick to standardize terminology (RL vs. reinforcement learning, etc.)?\n\n10. Why does the paper mention $\\alpha$ = 0.6 in text but use $\\alpha$ = 0.1 for key results in Table 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZDw1ioJzJo", "forum": "Zga6TPc0aT", "replyto": "Zga6TPc0aT", "signatures": ["ICLR.cc/2026/Conference/Submission9978/Reviewer_LFNK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9978/Reviewer_LFNK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984699677, "cdate": 1761984699677, "tmdate": 1762921413403, "mdate": 1762921413403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}