{"id": "iTaQmRWa7Y", "number": 20851, "cdate": 1758310928088, "mdate": 1759896955330, "content": {"title": "KLAS: Using Similarity to Stitch Neural Networks for an Improved Accuracy-Efficiency Tradeoff", "abstract": "Given the wide range of deployment targets, flexible model selection is essential for optimizing performance within a given compute budget.\nRecent work demonstrates that stitching pretrained models within a model family enables cost-effective interpolation of the accuracy-efficiency tradeoff space.\nStitching transforms intermediate activations from one pretrained model into another, producing a new interpolated stitched network.\nSuch networks provide a pool of deployment options along the accuracy-efficiency spectrum.\nHowever, existing stitching approaches often yield suboptimal tradeoffs and lack generalizability, as they primarily rely on heuristics to select stitch configurations.\nWe argue that constructing improved accuracy-efficiency tradeoffs requires explicitly capturing and leveraging the _similarity_ between pretrained models being stitched.\nTo this end, we introduce KLAS, a novel stitch selection framework that automates and generalizes stitch selection across model families by leveraging KL divergence between intermediate representations.\nKLAS identifies the most promising stitches from the $\\mathcal{O}(n^k)$ possibilities for $k$ pretrained models of depth $n$.\nThrough comprehensive experiments, we demonstrate that KLAS produces improved accuracy-efficiency curve of stitched models at the same cost as baselines.\nKLAS achieves up to $1.21\\%$ higher ImageNet-1K top-1 accuracy at the same computational cost, or maintains accuracy with a $1.33\\times$ reduction in FLOPs.", "tldr": "KLAS is a stitch selection algorithm that improves accuracy-efficiency curves by leveraging KL divergence to identify stitching points between pretrained models, without any additional training cost.", "keywords": ["representation learning", "neural networks", "deep learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/709043a2100e82ac1436a5935b297495138783c3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes KL-divergence–based Anchor Stitching (KLAS), a framework for selecting where and how to “stitch” pretrained networks so as to interpolate accuracy–efficiency trade-offs more effectively than heuristic stitching (e.g., SN-Net). KLAS (i) chooses anchor pairs by the last-block KL divergence of their predictive distributions and (ii) ranks block pairs with a stitch score \\\\(\\\\Gamma(i,j)\\\\) that combines cross-anchor activation distance and the target block’s “capacity” (consecutive-block KL). To obtain the distributions, the authors train lightweight linear probes (ProbeNet) on each block; probes converge in ≈4 epochs and add a small one-time cost. Across DeiT, Swin, LeViT, and ResNet (ImageNet-1K, CIFAR-100/10), KLAS improves the AUC of the accuracy–FLOPs trade-off curves over SN-Net; gains include up to +1.21% top-1 accuracy at equal FLOPs or 1.33× FLOPs reduction at equal accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Principled selection vs. heuristics.** The paper clearly articulates why nearest/paired stitching can be suboptimal and replaces it with a similarity-driven criterion grounded in KL divergence, with explicit formulas for \\\\(\\\\Theta\\\\) and \\\\(\\\\Gamma\\\\).\n2. **Dual notion of similarity.** The KL criterion is argued (and operationalized) to reflect both epresentational alignment and functional compatibility, addressing shortcomings of CKA/MSE/CE/DM for choosing stitch points.\n3. **Efficient probing.** ProbeNet trains one set of blockwise probes efficiently (≈0.25 GPU-days for Swin-B) and shows fast convergence, keeping selection overhead modest.\n4. **Broad empirical coverage.** Results span multiple families (ViTs, CNNs) and even cross-architecture stitches (e.g., ResNet↔Swin), consistently improving trade-off AUC vs. SN-Net.\n5. **Anchor selection that adapts by family.** Last-block KL correctly prefers far stitching (Ti→B) for Swin but nearest (Ti↔S, S↔B) for DeiT, demonstrating generality beyond a fixed heuristic."}, "weaknesses": {"value": "1. **Supervision dependence for “similarity.”** KL is computed on softmax outputs of supervised probes; thus, selection intrinsically depends on labels and probe training. This limits claims for unsupervised/self-supervised representation learning and may bias choices toward the probe’s training distribution.\n2. **Asymmetry & calibration sensitivity.** KL’s asymmetry and dependence on calibration/temperature may distort distances across blocks/models; the method normalizes by an intra-anchor term, but robustness to temperature, class imbalance, or label smoothing is underexplored.\n3. **Metric/selection design choices.** The final selection uses bucketized FLOPs and a threshold \\\\(\\\\tau\\\\) (5% of the minimum in each bucket). The AUC metric and bucket granularity can influence conclusions; more sensitivity studies would strengthen the case.\n4. **Scale of gains in some regimes is small.** Several settings show marginal improvements (e.g., cross-architecture ΔAUC≈+0.002), raising questions about practical significance across all families.\n5. **Representation-learning scope.** Experiments are largely supervised classification; there is no evaluation with self-supervised anchors (e.g., DINO/MAE) nor transfer via frozen-backbone linear probes on diverse tasks.\n6. **Limited evidence for dense prediction/generalization.** Dense-task adaptation is left as future work with only preliminary results; rigorous detection/segmentation studies are missing.\n7. **Compute accounting.** While probe training is “negligible,” it is still an added search cost versus purely heuristic SN-Net; a wall-clock comparison for the full pipeline (probes + stitch fine-tuning) would help."}, "questions": {"value": "1. **Un/SSL compatibility.** Can KLAS be made label-free, e.g., by using self-supervised probes (DINO-style heads) or pseudo-labels? How does anchor/block selection change when anchors are self-supervised?\n2. **Calibration sensitivity.** How sensitive are \\\\(\\\\Theta\\\\) and \\\\(\\\\Gamma\\\\) to softmax temperature, label smoothing, and class imbalance? Could you report ablations and perhaps use temperature-scaled KL?\n3. **Search cost accounting.** What is the end-to-end overhead (wall-clock/GPU-days) of ProbeNet + KLAS vs. SN-Net’s heuristics at equal stitch fine-tuning budgets? Please include variability across families.\n4. **Ranking validity.** Beyond the Min-KL vs. \\(\\Gamma\\) comparison, can you provide rank correlation between KLAS scores and post-fine-tuning accuracy across candidates (per bucket and overall)?\n5. **Cross-family stitches.** For cases with very small ΔAUC, what failure modes arise (capacity mismatch, optimization instability, probe mis-ranking)? Any diagnostics (e.g., KL heatmaps) that predict such cases?\n6. **Dense tasks.** Could you include full experiments on detection/segmentation (COCO/ADE20K) where stitching locations affect multi-scale features, and compare to dynamic routing/pruning baselines?\n\n**If the author can address my questions, I am willing to improve my rating.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2Qf5jCv6vF", "forum": "iTaQmRWa7Y", "replyto": "iTaQmRWa7Y", "signatures": ["ICLR.cc/2026/Conference/Submission20851/Reviewer_8KnS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20851/Reviewer_8KnS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761186836255, "cdate": 1761186836255, "tmdate": 1762999985075, "mdate": 1762999985075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the use of inserted linear probes and KL divergence between them to find layers compatible for stitching between different networks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- clear language and well written\n\n- the idea of using linear probes + KL divergence for stitching is sound and well supported by experiments"}, "weaknesses": {"value": "- The declared goal of the KLAS approach is to use models from a pre-trained zoo to construct models that provide new accuracy / cost trade-offs. This is presented as an alternative to NAS (a somewhat unfair comparison as NAS enriches to base-model pool). However, the standard method of addressing this problem are model cascades / committees (e.g. \"WISDOM OF COMMITTEES: AN OVERLOOKED APPROACH TO FASTER AND MORE ACCURATE MODELS\" or \"Efficient Inference With Model Cascades\"). Model cascades are not discussed at all and not compared against in this paper. A comparison against a strong cascade baseline is essential to be able to claim utility of the proposed method for the suggested purpose. In addition to the direct comparison to a model cascade, please also explain whether there are situations where stitching is preferrable over cascading for structural reasons (maybe average case complexity vs. worst case complexity?). A 1) worst-case and 2) average-case acc/cost tradeoff curve comparing stitching and cascading would be meaningful.\n\n- \"KL divergence uniquely satisfies the dual objectives\" this claim is not substantiated. Probably the authors mean ~ 'uniquely among the few measures we consider here'; if yes, please rephrase, if not, please provide a proof that the KL divergence is indeed unique in this regard.\n\n- it is unclear whether the method is specific to image classifiers or works in other domains as well. The impact of the paper would be much broader, if there was some indication that it works for regression tasks (--> how to substitute the KL div??) and for non-vision classification (e.g. also token prediction in LLMs). Currently the method is of limited interest as it only applies to a niche task."}, "questions": {"value": "In what scenarios is stitching preferrable to a model cascade? Why?\n\nDo linear probes work for regression problems? For very large classifiers (think LLMs)? (the discussion mentions this very shallowly, there is no need to show improvement both for prefill and decode, just pick the easier case and show some improvement)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UDB6IaFEk2", "forum": "iTaQmRWa7Y", "replyto": "iTaQmRWa7Y", "signatures": ["ICLR.cc/2026/Conference/Submission20851/Reviewer_cTKy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20851/Reviewer_cTKy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817696925, "cdate": 1761817696925, "tmdate": 1762999985145, "mdate": 1762999985145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes stitching blocks of neural network by computing similarity between blocks by taking the KL-Divergence of linear probe probability distributions. The paper suggests that \"far-stitching\" where models with significant difference in complexity or performance may also be worthy of stitching. The paper proposes to automatically identify block pairs to stich opposed to SN-Net where they use defined constraints to choose the pairs. The paper compares the proposed KLAS stitching framework with SN-Net on DeiT and Swin model architectures trained on CIFAR-100 and imagenet-1k datasets. The results show marginal gain in performance compared to SN-Net."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper explores functional similarity along with representation similarity as a metric while choosing to stitch networks which is logical. If two networks produce similar same functional output, then the models are better suited for selecting stitching pairs, when considered with representation similarity.\n2. The paper also shows that far stitching where two networks may not have similar performance still can be stitched together is an important contribution.\n3. The experimentation on DeiT and Swin family of models to compare with SN-Net shows similar performance but seemingly less finetuning cost."}, "weaknesses": {"value": "1. In line 232 the paper states \"As a representational similarity metric, KL divergence captures distributional differences between intermediate activations, indicating whether two blocks generate patterns that\ncan be mapped via lightweight transformations.\" which is not supported in my opinion as KL divergence compares the probabilistic score distribution not the representation itself. (Think different features can be used to reach same conclusion with similar confidence).\n\n2. There is only marginal improvement when comparing stitched models of similar flops compared to SN-Net, while individual anchor-block stitches give an edge to KLAS, collectively the gain is minimal.\n\n3. The lack of representational similarity is a concern (as mentioned in point 1 KL divergence doesnt directly compare representations), if there is difference in representational similarity there would be more finetuning cost.\n \n4. The details on stitched model finetuning could be added for better assessment. This is important to assess the improvement in AUC."}, "questions": {"value": "1.  For the stitch finetuning step is the number of training steps per stitched model constant or does it vary depending on when the stitch converges?\n2. I did not get if the the finetuned stitched model is same for both SN-NET and KLAS if the stitch pairs overlap."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iEOFwDCvhn", "forum": "iTaQmRWa7Y", "replyto": "iTaQmRWa7Y", "signatures": ["ICLR.cc/2026/Conference/Submission20851/Reviewer_XiFF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20851/Reviewer_XiFF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943681973, "cdate": 1761943681973, "tmdate": 1762999985330, "mdate": 1762999985330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Stitching pretrained models offers a cost-effective way to explore accuracy-efficiency tradeoffs. Current stitching methods rely on heuristics, leading to sub-optimal results. This paper improves this by using KL-divergence to automate stitch selection and generalize across architectures. KLAS outperforms baselines, achieving higher accuracy at the same computational cost or reducing FLOPs while maintaining accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Technical contribution: This paper addresses a fundamental challenge in current model stitching methods: these methods rely on heuristic-based stitch selection, fix anchors and blocks, and thus yield suboptimal accuracy-efficiency tradeoffs. Specifically, it proposes a coarse-grained anchor selection strategy that leverages the KL divergence of the last block for anchor identification, and employs block-level KL divergence for fine-grained selection. The metric for candidate set selection is well-justified, as it considers both sampling coverage during fine-tuning and the quality of anchor filtering. \n\n* Experiments effectively demonstrate the advantages of the proposed KL-divergence-based anchor stitching approach."}, "weaknesses": {"value": "* Why MSE, CE, CKA, DM can have significantly lower percentage of stitch configurations than KL divergence? Mathematically, it’s not that straightforward.\n\n* The method’s applicability to large language models and multimodal LLMs has yet to be explored—extending it to these models would further enhance the paper’s impact.\n\n* What performance can KLAS achieve on dense prediction tasks, including object detection, semantic segmentation, and depth estimation?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PclOY4kk3B", "forum": "iTaQmRWa7Y", "replyto": "iTaQmRWa7Y", "signatures": ["ICLR.cc/2026/Conference/Submission20851/Reviewer_B36v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20851/Reviewer_B36v"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20851/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963967016, "cdate": 1761963967016, "tmdate": 1762999985608, "mdate": 1762999985608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}