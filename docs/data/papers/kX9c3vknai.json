{"id": "kX9c3vknai", "number": 20695, "cdate": 1758309108413, "mdate": 1759896963570, "content": {"title": "Diffusion-based Behavior Cloning in Multi-Agent Games via Dynamic Guidance", "abstract": "In offline multi-agent imitation learning, agents are constrained to learn from static datasets without interaction, which poses challenges in generalizing across diverse behaviors. Behavior Cloning (BC), a widely used approach, models conditional actions from local observations but lacks robustness under behavioral variability. Recent diffusion-based policies have been introduced to capture diverse action distributions. However, in multi-agent environments, their iterative denoising process can accumulate errors in interactive settings, degrading performance under shifting opponent behaviors. To address these challenges, we propose Diffusion Dynamic Guidance Imitation Learning (DDGIL), a diffusion-based framework built on classifier-free guidance (CFG), which balances conditional and unconditional denoising predictions. Unlike prior methods with fixed weighting, DDGIL introduces a dynamic guidance mechanism that adaptively adjusts the weight at each denoising step, enhancing stability across different agent strategies. Empirical evaluations on competitive and cooperative benchmarks show that DDGIL achieves reliable performance. In high-fidelity sports simulations, it reproduces action strategies that closely resemble expert demonstrations while maintaining robustness against diverse opponents.", "tldr": "", "keywords": ["imitation learning", "multi- agent task", "diffusion", "behavior cloning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/43f97de6e826a43456277fc1e18c77c84c61b725.pdf", "supplementary_material": "/attachment/ba8120602d426e411206814393cb919842d77d8a.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem of limited adaptability in offline multi-agent imitation learning caused by using fixed diffusion guidance weights. To address this, it introduces a Dynamic Diffusion-Guided Imitation Learning method (DDGIL). The idea is to train separate diffusion models for the primary and opponent agents, and then compute a dynamic confidence weight based on the difference between their noise predictions during the reverse diffusion process. This allows the model to adaptively adjust the guidance strength, improving the policy’s robustness and generalization. Experiments on benchmarks like MPE and Atari show that DDGIL outperforms traditional behavior cloning methods (BC, DBC) and fixed-guidance diffusion approaches (DP, DD) on complex interactive tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper analyzes the problem of poor adaptability caused by fixed diffusion guidance weights in multi-agent environments, which has clear research significance.\n2. The experiments cover multiple environments and compare the proposed method with BC, DBC, DP, DD, and offline RL approaches such as OMAR and IDQL. The results are stable and demonstrate the effectiveness of the method.\n3. The method is simple to implement and can be easily integrated into existing diffusion policy frameworks."}, "weaknesses": {"value": "1. The methodological novelty is limited, as the design of dynamic guidance is mainly a heuristic extension of the existing classifier-free guidance approach.\n2. The multi-agent setting requires training a separate diffusion model for each agent, causing computational and memory costs to grow linearly with the number of agents, which limits scalability.\n3. DDGIL remains a pure imitation learning framework without a defined optimality criterion; it relies solely on reconstruction loss and thus cannot learn or improve from failed trajectories.\n4. Opponent modeling is only superficial, since each opponent is assigned its own diffusion model, but these models neither make decisions nor perform any game-theoretic reasoning. They only produce noise predictions during inference, which are then used as guidance signals.\n5. The paper introduces a noise consistency regularization term that assumes the primary and opponent agents should predict similar noise under the same input. However, it does not explain from a theoretical perspective why enforcing such similarity would help improve policy learning. In multi-agent environments, the primary and opponent agents usually exhibit distinct or even conflicting strategies, so enforcing noise similarity might actually blur these differences and weaken opponent modeling. Moreover, the paper does not include an ablation study to verify the effect or necessity of this regularization term."}, "questions": {"value": "1. The paper presents the method as “offline multi-agent imitation learning,” yet the inference procedure appears to control only the primary agent while opponent models provide guidance signals only. Do you execute policies for all agents during inference? If only a single agent is controlled, is this description accurate?\n2. The method also fits the offline data setting and is compared against offline RL baselines. Why not implement and evaluate it within an offline RL framework to assess feasibility, rather than restricting to pure imitation learning?\n3. Line 471 states: “DD underperforms, likely due to long-horizon prediction errors.” Conceptually, diffusion-based planning (DD) should be better suited to long-horizon trajectory generation, whereas the proposed dynamic guidance is stepwise and relies on current state information. Why does DD underperform on long horizons while your method improves?\nMethods like DD are offline RL algorithms whose objectives depend on returns/values. After adapting them to a reward-free imitation setting, is the comparison fair? Does this modification alter their core optimization objective and thus affect the validity of the comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FrhWupUIwJ", "forum": "kX9c3vknai", "replyto": "kX9c3vknai", "signatures": ["ICLR.cc/2026/Conference/Submission20695/Reviewer_QH66"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20695/Reviewer_QH66"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888150206, "cdate": 1761888150206, "tmdate": 1762934072608, "mdate": 1762934072608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DDGIL, an offline multi-agent imitation learning method that extends diffusion-based behavior cloning with dynamic classifier-free guidance (CFG).  Instead of using a fixed guidance weight (as in Diffusion Policy or Decision Diffuser), DDGIL computes a step-wise adaptive weight based on the disagreement between the primary agent’s conditional prediction and the predictions from auxiliary opponent diffusion models.  The approach modifies only inference, keeping training identical to standard diffusion behavior cloning.  Experiments span multiple domains—MPE, Atari, Classic games, and a real-world badminton simulation—showing that DDGIL typically improves team reward or win rate over strong baselines such as BC, DBC, Diffusion Policy (DP), and Decision Diffuser (DD). Ablations further analyze dataset size, fixed vs. adaptive weights, and scalability in the number of opponents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The dynamic guidance rule seems lightweight and practical\n- The method is tested on diverse benchmarks and real-world badminton data, showing consistent performance gains in many settings\n- The paper systematically studies fixed vs. adaptive weights, scalability with opponent count, and shared vs. separate diffusion models, providing good ablation about the trade-offs."}, "weaknesses": {"value": "- The novelty is incremental. The core contribution—a dynamic weighting of classifier-free guidance (CFG)—is conceptually straightforward and builds directly on existing diffusion policy frameworks and opponent modeling ideas already explored in the literature.  \n\n- The baseline comparison is limited. The authors only compare against a few adapted single-agent methods, overlooking several established multi-agent imitation learning algorithms such as MAGAIL [1] and MFIQ [2], which would provide a more comprehensive evaluation.  \n\n- The theoretical analysis justifies the adaptive weight only locally (through score disagreement) but does not establish any global convergence or stability guarantees. In multi-agent reinforcement and imitation learning, the notion of **local-global consistency**—ensuring alignment between local and global policies in cooperative games—is crucial. The paper lacks a discussion of how DDGIL relates to or preserves such consistency.  \n\n- The environments considered in the paper are relatively small in scale. More complex and realistic multi-agent reinforcement learning (MARL) benchmarks, such as **SMACv2**, have recently become standard for evaluating MARL and MAIL algorithms. Including experiments on such environments would strengthen the empirical validation and demonstrate the scalability of the proposed method.  \n\n\n[1] Jiaming Song, Hongyu Ren, Dorsa Sadigh, Stefano Ermon.\n“Multi-Agent Generative Adversarial Imitation Learning.” arXiv preprint, Jul 2018. DOI: 10.48550/arXiv.1807.09936.\n\n[2] The Viet Bui, Tien Mai, Thanh H. Nguyen.\n“Inverse Factorized Soft Q-Learning for Cooperative Multi-agent Imitation Learning.” In Proceedings of NeurIPS, 2024."}, "questions": {"value": "please address the concerns in the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "- There are no major ethical concerns associated with this paper. The work focuses purely on algorithmic development and evaluation using simulated or anonymized datasets.  \n- No human subjects, personal data, or sensitive information are involved in the experiments.  \n- The proposed method does not raise foreseeable safety or misuse risks beyond standard consideratio"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6KKG6qZF7H", "forum": "kX9c3vknai", "replyto": "kX9c3vknai", "signatures": ["ICLR.cc/2026/Conference/Submission20695/Reviewer_PJmh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20695/Reviewer_PJmh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940532866, "cdate": 1761940532866, "tmdate": 1762934072096, "mdate": 1762934072096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies behavioral cloning in multi-agent settings. The paper's main idea is to apply diffusion models to generate actions based on observations of agents. Given the presence of multiple agents, the paper selects an agent as a primary agent and the others as opponents, diffusion-based policy learning is then developed accordingly to learn local policies for individual agents. The paper then introduces a dynamic weight adjustment based on the alignment between the primary agent and its opponents to interpolate between conditional and unconditional noises in diffusion models. Experiments on various tasks including MPE and Atari games show the proposed BC-based multi-agent imitation learning performs better than baselines such as BC, diffusion BC, diffusion policy, and decision diffusion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Experiments show promising results of the proposed method."}, "weaknesses": {"value": "***The justifications for the proposed model’s design choices are insufficient, particularly regarding how diffusion models are utilized for multi-agent action generation:\n\n1. Choice of Primary Agent: The paper does not explain how this agent is selected or how such a choice impacts imitation performance, especially in environments with heterogeneous agent roles.\n\n2. Opponent Diffusion Modeling: Opponent actions are conditioned on the primary agent’s state, which lacks rationale. In standard multi-agent settings, each agent’s actions should depend on its own observations and policies, not solely on the primary agent.\n\n3. Shared Noise Across Agents: The method enforces identical noise inputs for all agents’ diffusion models, yet the paper does not justify why this condition is necessary or meaningful.\n\n4. Lack of Multi-Agent Interaction Modeling: The approach does not clarify how cooperative or competitive interactions are captured. Understanding joint behavior dynamics is crucial in multi-agent learning\n\n*** Experimental Evaluation\n1. Limited Baselines: Several established single-agent offline imitation learning methods could be used as baselines by training policies per agent individually. Including such baselines would provide a more comprehensive and convincing comparison."}, "questions": {"value": "1. Please address my points on weaknesses.\n\n2. Can your method apply for complex domains such as MaMujoco and SMAC? If so, can you please show some empirical results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i5l6OfY8y2", "forum": "kX9c3vknai", "replyto": "kX9c3vknai", "signatures": ["ICLR.cc/2026/Conference/Submission20695/Reviewer_Ydk8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20695/Reviewer_Ydk8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958841017, "cdate": 1761958841017, "tmdate": 1762934071380, "mdate": 1762934071380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}