{"id": "EbbCRpnOaC", "number": 1451, "cdate": 1756883596189, "mdate": 1759898208401, "content": {"title": "UniMMVSR: A Unified Multi-Modal Framework for Cascaded Video Super-Resolution", "abstract": "Cascaded video super-resolution has emerged as a promising technique for decoupling the computational burden associated with generating high-resolution videos using large foundation models. Existing studies, however, are largely confined to text-to-video tasks and fail to leverage additional generative conditions beyond text, which are crucial for ensuring fidelity in multi-modal video generation. We address this limitation by presenting UniMMVSR, the first unified generative video super-resolution framework to incorporate hybrid-modal conditions, including text, images, and videos. We conduct a comprehensive exploration of condition injection strategies, training schemes, and data mixture techniques within a latent video diffusion model. A key challenge was designing distinct data construction and condition utilization methods to enable the model to precisely utilize all condition types, given their varied correlations with the target video. Our experiments demonstrate that UniMMVSR significantly outperforms existing methods, producing videos with superior detail and a higher degree of conformity to multi-modal conditions. We also validate the feasibility of combining UniMMVSR with a base model to achieve multi-modal guided generation of 4K videos—a feat previously unattainable with existing techniques.", "tldr": "", "keywords": ["Multi-modal video super-resolution", "high-resolution controllable video generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4481a95e8a038df9deb4d9d35e0f607da6c6755e.pdf", "supplementary_material": "/attachment/2166674f2f2163b2f35673eb437277a13b660377.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces UniMMVSR, a unified multi-modal generative video super-resolution framework designed to upscale AI-generated low-resolution videos to ultra-high-resolution (e.g., 4K), supporting multi-model input as guidance, including text, multi-ID images, and reference videos.\nThe proposed solution includes the following parts:\n- A unified conditioning strategy combining channel concatenation (for low-res video) and token concatenation (for reference modalities), with separated conditional RoPE for positional embedding.\n- A SDEdit-based degradation pipeline that simulates realistic generative degradations of base models, combining with the conventional synthetic degradation baseline.\n- A difficult-to-easy curriculum training strategy to align sub-tasks of varying complexity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper introduces multi-modal guidance into diffusion-based video super-resolution (VSR). The qualitative results demonstrate the effectiveness of the guidance in guiding the content generation with appealing performance.\n- The ablation study shows the effectiveness of the proposed sdedit degradation as well as the the conditioning strategy for multi-modal guidance VSR"}, "weaknesses": {"value": "- The contributions of the paper seem to weigh more on the engineering side rather than the technical novelty. This paper seems to be built on lots of existing successful practices, like DiT, RoPE, cascaded modeling, etc. The improvements upon the above parts, e.g., conditioning strategy, degradation pipeline and training order seem kind of trival, though maybe effective.\n- The fidelity of the model seems poor compared with previous baselines like SeedVR and STAR, which is obvious in nearly all the qualitative results. It is unclear if this is caused by the introduced multi-model guidance."}, "questions": {"value": "My main concerns of this paper are as follows:\n\n1. The two major weaknesses above, i.e., technical contributions and fidelity.\n\n2. Some of the paper details are vague: 1) Achieving 4K SR is very VRAM expensive. The details of how to achieve such high-resolution video need further explanation, e.g., how many frames can be processed in one inference? how to process long videos? How much VRAM is required for each inference, etc? 2) The details of image-related transformations for reference alignment are vague in Line 299-300. What detailed transformations are used for training?\n\n3. The complexity comparison is missing. There should be FLOPs, parameters and inference time comparison. The training time should also be provided.\n\n4. The details of the evaluation data seem to be missing. What data is used for quantitative and qualitative comparison? The author may also consider making comparisons on commonly used synthetic and real-world benchmarks, following previous works such as Upsale-A-Video, STAR, and SeedVR, with standard metrics such as PSNR, SSIM, CLIP-IQA, MUSIQ, etc. This ensures a more straightforward comparison with previous methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tRD9BwUx2K", "forum": "EbbCRpnOaC", "replyto": "EbbCRpnOaC", "signatures": ["ICLR.cc/2026/Conference/Submission1451/Reviewer_YT7R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1451/Reviewer_YT7R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1451/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760684608555, "cdate": 1760684608555, "tmdate": 1762915773273, "mdate": 1762915773273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a cascaded video super-resolution framework that can support unified inputs of text, images, and videos. To unify different conditions together, the paper explores options such as channel concatenation of low-resolution video, token concatenation of visual references, and separated conditional RoPE for multi-ID images and reference videos. The paper also designs a degradation framework to simulate the artifacts and distortion generated by the base model. The contributions of this work are the following. The proposed framework is the first unified one that can handle multimodal conditions. The paper also provides some useful insights on how to effectively utilize diverse inputs—including low-resolution video, text, multiple ID images, and reference videos. The experimental demonstrations also look solid as it provides sufficient visualizations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of this work are outlined below. \n\nThe proposed framework was claimed to be the first one that can handle multimodal conditions simultaneously for the video super-resolution task. \n\nThe paper provides some useful insights on how to effectively utilize diverse inputs—including low-resolution video, text, multiple ID images, and reference videos. Considering the difference in modalities, the paper explores options such as channel/token concatenation and separated RoPE. \n\nThe experimental demonstrations also look solid as they provide sufficient visualizations, where the ablation shows effectiveness for different components."}, "weaknesses": {"value": "Though the paper offers the above strengths, it still demonstrates a few critical weaknesses, which need to be further clarified. \n\n1. The entire framework looks a bit complicated. For different modalities, it chooses different options such as channel/token concatenation and separated RoPE.  Can we also process them all as tokens and then use token concatenation? After all, the paper shows that the token concatenation performs the best on multi-modal condition injection when incorporating multiple ID images and reference videos. In this case, the motivation work regarding this part could be further clarified. \n\n2. The framework also relies on the choice of different training orders, basically on how to reorganize the order for different tasks of text-to-video, multi-ID image-guided, and video editing. This also depends on how to set up the probability for choosing different tasks. This makes the proposed framework not generalizable and could be difficult to train. It is unclear whether the distribution of these probability values will vary again if datasets and or some base architectures change. \n\n3. The paper should provide some computational complexity analysis on the proposed framework, as it consists of many different components. The most consuming part would lie in the training phase. But it will still be good to list the inference time and the used parameters, and so on. \n\n4. The reason for using multi-ID images is not well clarified. It is unknown which synthesis output relies on these multi-ID images. And among the provided multi-ID images, which image is placed with more emphasis? \n\n5. The proposed method does not seem to achieve the best among the compared methods. For some listed methods such as VEnhancer, STAR and SeedVR, they can all handle the tasks of Text-to-video Generation, Text-guided Video Editing, and Multi-ID Image-guided Text-to-video Generation. What are the advantages of the proposed framework compared to these methods from this perspective?"}, "questions": {"value": "1. When describing the technical details of low-resolution video via channel concatenation, it appears that \"Upsampler\" was directly applied to the latent code, other than in the pixel space. There could be some confusion here. \n\n2. The role of using multi-ID images needs to be better clarified. Based on the illustrations from Figure 3, why does the super-resolution on the text image rely on the input face image, e.g., the third row case? It appears that super-resolution output has nothing to do with the face image.\n\n3. What is the computational complexity of the proposed method, and how reproducible is the proposed approach?\n\n4. What is the motivation for not using the all tokens process for text, image, and videos? Has this been explored in this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w7XjYnz4K2", "forum": "EbbCRpnOaC", "replyto": "EbbCRpnOaC", "signatures": ["ICLR.cc/2026/Conference/Submission1451/Reviewer_jTBQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1451/Reviewer_jTBQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1451/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761114542249, "cdate": 1761114542249, "tmdate": 1762915773094, "mdate": 1762915773094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new model, UniMMVSR, that upscales AIGC videos while also respects hybrid conditions including low-res input, text, ID images and other videos. With multi-modal inputs, it is able to upscale a video up to 4K. It also shows a degradation pipeline based on SDEdit. It shows outstanding performance among multiple video generation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Motivation\n- UniMMVSR is designed for AIGC video super-resolution with a cascaded model after the base model. It shares the same latent space with the low-res videos.\n- To obtain high-fidelity results, UniMMVSR uses multiple inputs as conditions. It unifies multi-modalities including texts, images and videos.  \n\n### Method\n- Token-wise concatenation makes sense for reference input, given that they are not pixel-aligned with the low-res input video.\n- Separated conditional RoPE also makes sense for reference tokens. \n- To simulate AIGC artifacts, authors use SDEdit pipeline to \"degrade\" the input videos. \n- Reference augmentation is used to better adopt references in different scenarios. \n\n### Experimental results\n- Qualitative results show UniMMVSR can preserve details from reference inputs in Fig. 3.\n- UniMMVSR shows competitive qualitative results in three tasks on different no-reference metrics, especially in Multi-ID reference video generation. \n\n### Writing\nThe paper is easy to follow and well-written."}, "weaknesses": {"value": "### Motivation\n- The title sounds a little misunderstanding to me. UniMMVSR is designed to super-resolve AIGC videos, but authors use the term \"Cascaded Video Super-Resolution\", which is more often used for multiple, cascaded networks for video super-resolution in my opinion. Maybe use \"Reference-based Video Super-Resolution\" or other terms would be better than \"Cascaded Video Super-Resolution\".\n\n### Method\n- Since most of the VAEs are lossy, will \"decoding LR latent -> pixel upscaling -> VAE encode\" further increase the information loss? (Sec 3.2, Low-resolution video via channel concatenation).\n- The token-wise concatenation is not well documented. What strategies are used if the number of reference tokens are not the same, i.e. variable token length? Are there \"NULL\" tokens for padding during the training/inference? \n\n### Experimental results\n- 4K results have some limitations. (a) Too short, only 1s (or 21 frames). (b) Color shift (e.g., 41397180.mp4 in text-to-video generation. The first frame and the last frame look so different).\n- The qualitative results are mainly compared on no-reference metrics such as MUSIQ, DOVER, etc, which are infamous for their bias towards their training data. Can authors show a user study as a supplement? \n\n### Writing\n- Typos: \"our UniMMVSR model also need to\" -> \"our UniMMVSR model also needs to\"."}, "questions": {"value": "- There is no very detailed information about the base model, such as model parameters, VAE parameters, etc. Is the base model a private model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VhVPcGuzdJ", "forum": "EbbCRpnOaC", "replyto": "EbbCRpnOaC", "signatures": ["ICLR.cc/2026/Conference/Submission1451/Reviewer_vfEy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1451/Reviewer_vfEy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1451/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761613031238, "cdate": 1761613031238, "tmdate": 1762915772950, "mdate": 1762915772950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a method to unify generative video super resolution framework to include multi-modalities, including text, images and videos. It helps to synthesize more details and achieve high fidelity outputs for various conditional inputs. Specifically, it first upscale and channel concatenate the low-res video latents with high-res latents. Then it treats the multi-ID images and reference videos as visual tokens and token-wise concatenate them with previous high-res latents. Such tokens are performed separate 2D self-attention by themselves and finally jointly performed 3D self-attention together with high-res tokens. For PE, it applies separate RoPE for each reference videos and the target video. The author also proposed SDEdit degradation which adds k steps noise to latents and decode back into RGB space and then add normal degradation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- introduced additional information, including text, reference images, videos during video super resolution process, which makes the high-res detailed generated with guidance.\n- Difficult-to-easy training order makes the multiple task performance better.\n- Reference augmentation makes the output more robust."}, "weaknesses": {"value": "- limited novelty of super resolution and reference-based generation task since LR channel concatenation and reference token-wise concatenation is typical in each domain of research. Using separate PE for reference tokens is also used in other papers."}, "questions": {"value": "- The goal of SDEdit degradation is to mimic the real generated low-res video fidelity distribution. It would be helpful to has discussion about how close is it and propose any metrics to measure it.\n- For individual RoPE assignment for multiple reference tokens, i.e., n_i to n_i + k_i, how are they selected? Does the order of reference content (images/videos) matter? Would the PE with smaller i biased during training?\n- The output quality of base model might differs. How does the super resolution model handle difference quality inputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cD1UnbXH9s", "forum": "EbbCRpnOaC", "replyto": "EbbCRpnOaC", "signatures": ["ICLR.cc/2026/Conference/Submission1451/Reviewer_Sjsc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1451/Reviewer_Sjsc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1451/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798429596, "cdate": 1761798429596, "tmdate": 1762915772829, "mdate": 1762915772829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}