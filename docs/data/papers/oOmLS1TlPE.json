{"id": "oOmLS1TlPE", "number": 16904, "cdate": 1758270139903, "mdate": 1759897211118, "content": {"title": "Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback", "abstract": "Hypothesis ranking is a crucial component of automated scientific discovery, particularly in natural sciences where wet--lab experiments are costly and throughput-limited. Existing approaches focus on pre-experiment ranking, relying solely on a language model’s internal reasoning without incorporating empirical outcomes. We introduce the task of experiment-guided ranking, which prioritizes hypotheses based on feedback from previously tested ones. However, developing such strategies in natural science domains is challenging due to the impractical requirement of repeatedly conducting real experiments.\nTo address this, we revisit the core purpose of real experiments: to provide feedback on both the groundtruth hypothesis and the surrounding hypotheses that form the path toward it. This motivates our alternative: a simulator grounded in three domain-informed conceptual foundations, modeling hypothesis performance as a function of similarity to a known ground truth, perturbed by noise. While the groundtruth is pre-specified, it remains hidden from the ranking agent, enabling faithful evaluation of policies that navigate toward it. Validated against 124 hypotheses with experimentally reported outcomes, the simulator approximates real experimental results with consistent trend alignment. Though not perfectly accurate, its deviations resemble wet-lab noise and can foster more robust ranking strategies.\nWe formulate experiment-guided ranking as a sequential decision-making problem and propose an in-context reinforcement learning (ICRL) framework. Within this framework, we introduce an LLM-based agentic policy that decomposes hypotheses into functional elements, clusters them by shared mechanistic roles, and prioritizes recombinations of promising elements based on feedback.\nExperiments show that our method significantly outperforms pre-experiment baselines and strong ablations. Our toolkit—comprising the simulator and ICRL framework—enables systematic research on experiment-guided ranking, with our policy serving as a strong proof of concept.", "tldr": "", "keywords": ["Experiment-Guided Ranking", "Large Language Models (LLMs)"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9cc79bc122770f53617af8527e1cb2ec3bbabfa3.pdf", "supplementary_material": "/attachment/64c0eb376a20c57c6138f736188b5f4d7ef597c1.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the problem of ranking hypotheses generated by AI models without experimental labels. To tackle the issue of missing evaluation, the paper proposes a simulator based on three universal natural-science principles, which models hypothesis performance as a function of distance to a hidden ground-truth hypothesis with noise. This method is shown to align with true experimental findings and outperform existing methods. The authors then propose an ICRL framework for experiment-guided hypothesis ranking, where experimental feedback is used to prioritize hypotheses. The framework is shown to outperform other baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper studies the important problem of automated hypothesis discovery when real-world feedback is limited.\n2. The proposed benchmark and hypotheses may be of use for future studies in the field.\n3. The paper is clearly structured."}, "weaknesses": {"value": "1. The discussion on the setup is sometimes a bit abstract and it is unclear whether the setup fits beyond chemistry settings (which are most discussed in the paper). For example, a `hypothesis' could mean different things in different context. It may be helpful to refer to examples to see what a hypothesis is and what a question is from time to time. \n2. While the proposed simulator has interesting ideas, it is unclear how general the idea of extracting key components and assessing the importance of components can apply beyond the chemistry context. \n3. Since the experiments focus on existing literature, it might be insufficient to demonstrate the exploration of novel ideas."}, "questions": {"value": "1. Could you explain what a question could be and what a hypothesis could be?\n2. How general might the simulator design and the RL technique apply, e.g., beyond chemistry settings?\n3. Would it be a concern that the mechanism does well due to internal knowledge (since all evaluations are based on known, available knowledge in the literature) but may fail to perform well for novel hypotheses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xeDg0pCUpo", "forum": "oOmLS1TlPE", "replyto": "oOmLS1TlPE", "signatures": ["ICLR.cc/2026/Conference/Submission16904/Reviewer_2keb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16904/Reviewer_2keb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877409778, "cdate": 1761877409778, "tmdate": 1762926934188, "mdate": 1762926934188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a simulator (CSX-Sim) and an in-context RL policy (CSX-Rank) to study \"experiment-guided\" hypothesis ranking in chemistry with simulated wet-lab feedback, reporting strong trend alignment to 124 real experiments and fewer trials to find the ground truth than pre-experiment baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Strong, well-motivated design of the simulator (CSX-Sim) and in-context RL policy (CSX-Rank).\n\n(2) Demonstrates high correlation with 124 real experiments and clear efficiency gains over baselines.\n\n(3) The paper is generally well-written and well-structured, with clear figures and examples, though the introduction could be tightened to avoid redundancy and improve flow."}, "weaknesses": {"value": "* The study focuses mainly on chemistry; the framework still needs evaluation in other component-based natural science domains such as physics, materials science, and biology to test generality.\n* The evaluation uses published experiments rather than new wet-lab tests. This gives real-world grounding but may affect results if the LLM has seen similar data during training.\n* Both the simulator and the policy rely on LLMs, so their good agreement might come from shared model knowledge rather than true reflection of real experiments."}, "questions": {"value": "* How do the authors ensure that the published experiments used for validation were not part of the LLM's training data?\n* What steps were taken to confirm that CSX-Sim’s behavior reflects real experimental dynamics rather than learned text correlations?\n* How is ground-truth information isolated to prevent accidental exposure to the policy during simulation or prompting?\n* How sensitive are the results to the choice of LLM (e.g., GPT-4o-mini vs other models) and to changes in the simulator’s noise assumptions?\n* Can the authors provide examples where CSX-Rank fails to identify the ground truth or diverges from real trends, and analyze the reasons behind those cases? Adding such analysis would clarify limitations and strengthen the paper.\n* The framework assumes that hypotheses can be decomposed into functional components and compared through additive similarity. How does it handle cases where scientific mechanisms are emergent or not easily decomposable? A discussion or experiment on such non-compositional cases would strengthen the paper’s generality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H77z4tXm4o", "forum": "oOmLS1TlPE", "replyto": "oOmLS1TlPE", "signatures": ["ICLR.cc/2026/Conference/Submission16904/Reviewer_HGjD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16904/Reviewer_HGjD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942999541, "cdate": 1761942999541, "tmdate": 1762926933745, "mdate": 1762926933745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Goal:** improve ability of AI systems to rank hypotheses in the natural sciences\n\n**Challenge:** running experiments to develop hypothesis ranking strategies is expensive / not always feasible\n\n**Proposed solution:**\n1. develop a simulator that models hypothesis \"performance\" based on similarity to known experiments\n2. use this simulator to benchmark an inference strategy for hypothesis ranking (\"In Context RL\")\n\n**Results:**\n1. Simulator outperforms a \"matched score\" baseline in terms of hypothesis ranking based on correlation with observed hypothesis performance\n2. Ranking strategy outperforms randomly selected hypotheses or selection only using base model with no experiment feedback"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The authors focus on important challenges in automated science and present interesting intuitions for how to generalize knowledge from observed experiments using LLMs based on similarity."}, "weaknesses": {"value": "1. The paper fails to acknowledge or build upon rich prior research in this area, including the fields of active learning and \"sim2real\" for learning policies from simulators in scientific settings. The paper claims to introduce \"experiment-guided ranking\" which is more commonly referred to as active learning\n2. The simulator is insufficiently validated to support the claim that can be used to meaningfully develop or benchmark hypothesis ranking strategies. Experiments focus on \"30 research questions and 124 hypotheses\" and assess correlation, which does not provide a basis for robust statements about the utility of this simulator or policies learned upon it. This could also use stronger baselines.\n3. Baselines are too weak to make claims about how useful the \"in context RL\" strategy is. It also doesn't make much sense to refer to this as RL, since the policy is fixed. Overall this experiment could be strengthened substantially: benchmark based on real experiment outcomes, use stronger baselines, and show that the proposed strategy based on decomposing hypotheses and comparing similarity of key components is effective for hypotheses across more settings."}, "questions": {"value": "Overall, there are interesting ideas but the experiments are insufficient to support claims about the value of the proposed simulation and hypothesis ranking strategy. For the simulator, it is not clear that having a better correlation than the matched score baseline is sufficient to show that 1) a policy that performs better with the simulator will be better on real data and 2) that this simulator is good enough to learn a policy that will achieve \"sim2real\" transfer. For the hypothesis ranking strategy, it would be much stronger to show that this approach achieves better hypothesis ranking that strong baselines across multiple settings with real data, not the simulator."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zySTA5jWwn", "forum": "oOmLS1TlPE", "replyto": "oOmLS1TlPE", "signatures": ["ICLR.cc/2026/Conference/Submission16904/Reviewer_MP1z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16904/Reviewer_MP1z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965587119, "cdate": 1761965587119, "tmdate": 1762926933284, "mdate": 1762926933284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}