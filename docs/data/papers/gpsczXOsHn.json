{"id": "gpsczXOsHn", "number": 14404, "cdate": 1758234555605, "mdate": 1763591586032, "content": {"title": "Global Resolution: Optimal Multi-Draft Speculative Sampling via Convex Optimization", "abstract": "Speculative sampling reduces the latency of autoregressive decoding for target model LLMs without sacrificing inference quality, by using a cheap draft model to suggest a candidate token and a verification criterion to accept or resample this token. To improve acceptance and decoding efficiency, recent work has explored the multi-draft extension, where at each step $n$ draft tokens are generated, and the verification criterion is a distribution conditioned on these. When this criterion maximizes the probability of accepting some draft token, it is called the optimal transport (OT). However, finding the OT is difficult, as it is the solution of a linear program (OTLP) in over $V^n$ variables, with $V$ being the vocabulary size. Two recent theoretical works have reframed the OTLP in terms of importance sampling or subset selection. In this work, we prove that these formulations are equivalent to an exponentially large relaxed OTLP, so it remains infeasible to solve. Then, we reverse engineer subset selection to formulate the OTLP as a max-flow problem. With a novel application of polymatroid theory, we reduce the exponentially large OTLP to a convex optimization problem in at most $V$ variables. This allows us to devise an algorithm for optimal $n$-draft speculative sampling when the $n$ tokens are chosen i.i.d. from a single draft model, which can be tuned to arbitrary accuracy. Finally, we measure acceptance rates and algorithm runtimes for various $n$ and top-$k$ draft sampling settings. Our findings give the first multi-draft algorithm with 90\\% acceptance and under 100 ms of overhead per generated token with negligible deviation from the target model distribution.", "tldr": "We reduce optimal multi-draft speculative sampling to a convex minimization problem, and solve a truncated version to achieve state-of-the-art acceptance with negligible performance degradation.", "keywords": ["LLMs", "Inference", "Optimal Transport", "Speculative Decoding"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f62cad01613243993a1a2e71918631ea5e000b85.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies multi-draft speculative sampling for autoregressive decoding. It (i) proves that recent importance-sampling and subset-selection formulations are equivalent to an exponentially large relaxed OTLP, which is still intractable, (ii) reverse-engineer subset selection to a max-flow formulation, and (iii) apply polymatroid theory to reduce the problem to a convex program for i.i.d. drafts from a single draft model. Theoretical guarantees are given and empirical results demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper turns an exponential OTLP into a convex program in tolerable number of variables (under i.i.d. drafts) is a substantial conceptual and practical advance.\n- It also provides approximation error guarantees, which helps to determine the time-precision trade-off.\n- The experiments demonstrated the strong performance of the proposed method."}, "weaknesses": {"value": "- The main convex reduction requires i.i.d. drafts from a single $q$. There are some works adopt mixture drafts across experts. The iid configuration may be one concern of the proposed method.\n- The analysis focuses on solver runtime rather than full end-to-end throughput."}, "questions": {"value": "- When extending to the multi-step case, can the authors comment on how the compounding error is expected to scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NhgZTELXkE", "forum": "gpsczXOsHn", "replyto": "gpsczXOsHn", "signatures": ["ICLR.cc/2026/Conference/Submission14404/Reviewer_TpbB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14404/Reviewer_TpbB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811392921, "cdate": 1761811392921, "tmdate": 1762924814311, "mdate": 1762924814311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the computational intractability of optimal multi-draft speculative sampling. Previous work either computed optimal acceptance analytically (for restricted cases) or estimated it without recovering the optimal transport (OT) itself. The authors prove the equivalence of prior formulations (subset selection and canonical decomposition) and introduce Global Resolution, a convex-optimization-based solver that achieves near-optimal OT in the i.i.d. draft setting. The method reduces an exponentially large LP to a convex problem, with guaranteed deviation and practical runtimes (< 100 ms/token)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Important problem. Tackles a key bottleneck in speculative decoding efficiency for LLM inference.\n\nTheoretical soundness. Clear equivalence proofs (Sec. 4), a max‑flow reduction with complementary slackness (Sec. 5), and convex programs for inner/outer systems with explicit error bounds (Theorems 6.4–6.5, Lemma 6.6). \n\nPractical algorithm. Global Resolution achieves < 100 ms/token OT‑solve time while maintaining near‑optimal acceptance. This is a substantial empirical improvement over generic LP/max‑flow baselines in the tested regime."}, "weaknesses": {"value": "Not a major weakness, but the current method is limited to i.i.d. drafts. In practice, sampling without replacement (i.e., enforcing distinct drafts) typically yields better performance. Extending the approach to that regime, as well as to multi‑step setups, is left for future work. However, overall, the paper already makes great progress.\n\nTypos:\nTheorem 4.6, Equation (17): missing a summation on the LHS.\nEquations (22), (23): the conditional distributions are omitted in the notation."}, "questions": {"value": "Do Global Resolution extends (with guarantees) to independent but non‑identical drafts or sampling without replacement drafts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hM9OTIXLLP", "forum": "gpsczXOsHn", "replyto": "gpsczXOsHn", "signatures": ["ICLR.cc/2026/Conference/Submission14404/Reviewer_YzHd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14404/Reviewer_YzHd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994446362, "cdate": 1761994446362, "tmdate": 1762924813834, "mdate": 1762924813834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the inefficiency of *multi-draft speculative decoding* for large language models (LLMs), where computing the optimal transport (OT) verification criterion requires solving an exponentially large linear program (OTLP) over \\(V^n\\) variables. The authors first unify two prior theoretical formulations—importance sampling and subset selection—showing that both are equivalent to a relaxed exponential OTLP. They then reverse-engineer the subset-selection view and reformulate the problem as a max-flow optimization, which, via a novel application of polymatroid theory, is further reduced to a convex minimization problem with at most \\(V\\) variables.  This reduction yields a new algorithm called Global Resolution, which achieves provably optimal acceptance rates in the i.i.d. single-distribution setting where \\(n\\) draft tokens are sampled from the same draft model. Empirically, the paper measures acceptance rates and solver runtimes across different numbers of drafts \\(n\\) and top-\\(k\\) sampling configurations. Results show that the proposed solver can achieve over 90% acceptance with less than 100 ms overhead per token, and negligible deviation from the true target distribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The theoretical contribution of this paper is substantial. It successfully unifies two previously disjoint theoretical perspectives on speculative decoding—importance sampling and subset selection—into a single coherent framework. Building upon this, the authors further derive a novel convex minimization formulation via polymatroid theory, which drastically reduces the exponential complexity of the original OT linear program to a problem in at most \\(V\\) variables."}, "weaknesses": {"value": "This paper does not discuss the robustness of the proposed algorithm with respect to temperature. Intuitively, different temperature values shape different draft and target distributions, which could significantly affect the optimal acceptance rate. It remains unclear how the proposed approach performs under varying temperature settings. Moreovere, this paper does not intergrate their algorithm in real-world speculative decoding systems to show how the algorithm/theory can improve the latency of LLM decoding."}, "questions": {"value": "1. Can you give more details about how you construct the distribution in your experiments, e.g., tempearture. \n2. Can you add some temperature experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e0WAJpTEqq", "forum": "gpsczXOsHn", "replyto": "gpsczXOsHn", "signatures": ["ICLR.cc/2026/Conference/Submission14404/Reviewer_ZwDM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14404/Reviewer_ZwDM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762186335154, "cdate": 1762186335154, "tmdate": 1762924813257, "mdate": 1762924813257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Added Real-World Latency Experiments"}, "comment": {"value": "Thank you to all the reviewers for their considered feedback on our work. A majority of them have suggested that we implement global resolution in a realistic framework to measure end-to-end latency improvements in LLM decoding.\n\nTo this end, we have now implemented global resolution in SpecTr (https://arxiv.org/abs/2310.15141) to measure end-to-end latency in multi-step speculative decoding systems. SpecTr first drafts $K$ i.i.d. paths of length $L$ autoregressively from the draft model to form a draft tree, and then incrementally uses any OTLP solver (with $n$ being set to the number of child nodes) to advance from the root of the tree to child nodes, until it lands off the tree. We use global resolution for our OTLP solver (which is near-optimal) with $k=100$ for top-$k$ draft sampling, and when it fails, we resort to standard sampling from the target distribution $p$ (an approximate OTLP solver). We run our experiments on Gemma-27B/2B with temperature $1.0$ target sampling on the same dataset as our solve time experiments. Our results for $K=2,3,4$ and $L=8$ are shown below.\n\n\n| K | Block efficiency (tokens/target call) | Walltime speedup (relative to vanilla) |\n|-----|--------------------------------------|----------------------------------------|\n| 2   | 2.23                                 | 1.84                                   |\n| 3   | 2.54                                 | 1.89                                   |\n| 4   | 2.65                                 | 1.98                                   |\n\n\nOur walltime speedups in the best setting ($K=4$) are nearly $2\\times$ better than vanilla decoding. **We also emphasize that these numbers are highly conservative estimates.** Unlike previous OTLP solvers used in the SpecTr framework, global resolution is situational: it is provably nearly optimal, but cannot always be used. This means the walltime and efficiency numbers above can be improved significantly by using other approximate OTLP solvers (like K-SEQ, the one used in the original SpecTr paper) when global resolution fails, rather than our simple $p$-sampling heuristic (which is a valid OTLP solver, but has poor acceptances). Furthermore, by modifying the choice of $k$ in top-$k$ draft sampling, one might be able to use global resolution to get high acceptances in failure scenarios for other $k$. We leave such selection of OTLP solvers and their parameters to future work.\n\nThank you again to all the reviewers for suggesting the above point, which we have now additionally incorporated into our draft in Appendix R."}}, "id": "a56AUrvD1D", "forum": "gpsczXOsHn", "replyto": "gpsczXOsHn", "signatures": ["ICLR.cc/2026/Conference/Submission14404/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14404/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14404/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763585122770, "cdate": 1763585122770, "tmdate": 1763588457631, "mdate": 1763588457631, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "First, I apologize for the late review; I was invited to join late.\n\nThe paper studies the problem of speculative sampling with a draft sequence length of one (single-step), which can be formulated mathematically as an optimal transport (OT) problem. The paper studies when the optimal transport problem can be efficiently solved (which lets one hence compute the optimal acceptance ratio). In particular cases, the authors derive an efficient algorithm to solve the OT problem. Several experiments demonstrate the empirical efficacy of the proposed method in solving the OT problem vs other approaches, and also demonstrate the effect of changing various parameters on the resulting acceptance rates."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper establishes an algorithm for solving the optimal transport (OT) problem arising from speculative sampling. The algorithm has some compelling theoretical properties and has interesting connections with seemingly unrelated topics like max flow and linear programming. Experimental results comparing the resulting proposed algorithm vs other methods of solving the OT problem are convincing of the algorithm's efficacy in solving the OT problem compared to off-the-shelf solving methods. While I had some issues with the theory (see below), the experimental improvements combined with the proposed theory for solving the OT are convincing."}, "weaknesses": {"value": "The main weakness of the work is as follows:\n1. It would be interesting to see that the proposed algorithm improves on one way to demonstrate this improvement would be to demonstrate that the proposed algorithm is better empirically. However there is no empirical comparison to Hu et al (2025) or other papers that have studied this problem. \n\n2. The proposed algorithm seems to require enumerating over $(H^{\\star} \\cup T)^n - (H^{\\star})^n$ (see Theorem 6.4) (e.g. to compute gradients of $\\Phi_T$). Yes $T$ can be chosen, but as $|T| \\ge 1$ this still takes time at least $|H^{\\star}|^{n-1} \\cdot |T| \\cdot n \\ge |H^{\\star}|^{n-1}$. As such, how is the proposed algorithms computationally efficient (which is the point of the paper), am I missing something? It would be great if the authors could clarify this point.\n\n3. The writing could be significantly improved. It seems more conventional to present Theorems 6.4 and 6.5 to begin with, describe (at least at a high level) the algorithm, and then argue for its correctness. The current presentation does so backwards. It also took several reads for me to understand some of the key points of the paper, like how $H^{\\star}$ can be obtained efficiently in $O(V \\log V)$ time from Hu et al 2025 so therefore it is reasonable to assume knowledge of $H^{\\star}$, and that the crucial point is to solve the OT rather than compute $\\alpha^{\\star}$. Overall the paper has interesting ideas but it was hard to read."}, "questions": {"value": "Could the authors please clarify weakness 2 above, on the computational efficiency of the proposed algorithm?\n\nIt would be also nice if the authors could add some more references and discussion on why solving the single-step speculative sampling problem is important in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pmLjYEr8nf", "forum": "gpsczXOsHn", "replyto": "gpsczXOsHn", "signatures": ["ICLR.cc/2026/Conference/Submission14404/Reviewer_UvVU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14404/Reviewer_UvVU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762909688425, "cdate": 1762909688425, "tmdate": 1762924812924, "mdate": 1762924812924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}