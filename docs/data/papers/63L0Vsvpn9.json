{"id": "63L0Vsvpn9", "number": 9702, "cdate": 1758135432324, "mdate": 1759897703389, "content": {"title": "BlindSight: Harnessing Sparsity for Efficient Vision-Language Models", "abstract": "Large vision-language models (VLMs) enable joint processing of text and images. However, the inclusion of vision data significantly increases the prompt length, resulting in a longer time to first token (TTFT). This bottleneck can be mitigated by leveraging the inherent sparsity in the attention computation. Analyzing these attention patterns in VLMs when processing a series of images, we observe the absence of cross-image attention in a substantial portion of layers. Based on this, we propose BlindSight: a training-free approach to optimize multi-image VLM inference using an input-template-aware attention sparsity mask.  We utilize a dataset to derive a prompt-agnostic categorization for attention heads: Dense, Sink, Intra-Image, and Intra-Image+Sink. We further develop a Triton-based GPU kernel to leverage this sparsity. BlindSight achieves a 1.8-3.2X speedup in the attention computation (prompt length 36K-300K). We evaluate BlindSight using VLMs such Qwen2-VL, Qwen2.5-VL and Gemma 3; observing only a 1.15% accuracy degradation on average for the evaluated multi-image understanding benchmarks.", "tldr": "Attention layers in VLMs were found to have minimal image-to-image attention in many layers. This sparsity can be leveraged to accelerate inference..", "keywords": ["VLMs", "Sparse Attention", "Multimodal Models", "Prefill Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/280124dd32d9f84b65d1601147b3c0310ee29539.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper’s motivation is to enable efficient prefilling (reduce time to first token) for vision-language models that have several visual tokens in their context corresponding to many images. The paper’s approach is to study attention patterns, particularly inter-image attention patterns. They categorize attention patterns into four categories: dense, sink, intra-image, and sink + intra-image. This is done empirically by visualizing the attention matrix for some trained VLMs on a dataset. The paper then proposes BlindSIGHT, a post-training algorithm that limits each attention head to one of the four templates. For a given model, this is done through calibration stages, where the model is run on a dataset (the paper used MMIU) to identify the dominant sparsity pattern of each attention head and assign it to one of the four templates. The paper shows that a Triton implementation of their post-training method results in more than a 2× improvement in TTFT when the context length is as long as 300k tokens, with about a 1% accuracy loss across different multi-frame VLM benchmarks. Finally, the paper provides intuition and empirical evidence on when attention sinks occur in VLMs, arguing that they mainly appear for image boundary delimiter tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The research direction pursued in the paper (addressing the efficiency challenge of VLMs when processing multiple images due to long context) is important and has clear real-world benefits.\n- The paper’s presentation is easy to follow.\n- The analysis of sparsity patterns in VLM heads for multi-image input is interesting and valuable for future research in the field.\n- The proposed method (BlindSIGHT) is simple yet effective: it converts as many attention heads as possible to more efficient sparsity patterns. In addition, the Triton-based implementation shows significant speedup, achieving more than a 2× reduction in TTFT. Although the proposed method is post-hoc, the paper motivates future work to incorporate such sparse templates during training, which is a logical next step."}, "weaknesses": {"value": "- One major issue with the proposed method is the need to adjust several hyper-parameters: $\\alpha_{layer}$, $\\gamma_d$, $\\gamma_s$, $\\gamma_i$, and the choice of 10% for the sink sparsity template. The paper suggests finding reasonable values through a “calibration” process on a given model and dataset (e.g., MMIU). However, it remains unclear to what extent these optimal values transfer to queries or domains that differ significantly from the calibration dataset in real world use cases.\n- The paper mentions that the degree of enforced sparsity (through the chosen hyper-parameters) determines the trade-off with accuracy. For the settings reported, an average accuracy drop of about 1% is observed. However, it remains unclear how this trade-off behaves more broadly. The authors could provide an accuracy–TTFT plot showing how performance changes as the hyper-parameters vary.\n- A key contribution of the paper is replacing dense inter-image attention with sparse templates. However, there is no discussion on why inter-image attention could be useful. What types of queries require inter-image attention? In Section 6.2, results on various multi-image benchmarks are presented, but the paper should discuss what each evaluation measures, what aspects might require inter-image attention, and how the proposed sparse templates handle such cases.\n- The paper mainly presents results for QwenVL and Gemma. It is unclear whether the observed sparsity patterns would remain relevant for other VLMs, especially those designed to produce fewer visual tokens (for example, FastVLM [1] that generates 16× fewer visual tokens than regular ViTs).\n\n[1] Vasu, Pavan Kumar Anasosalu, et al. “FastVLM: Efficient Vision Encoding for Vision Language Models.” Proceedings of the Computer Vision and Pattern Recognition Conference. 2025."}, "questions": {"value": "- In line 357 (Infrastructure), which VLM are you referring to?\n- For the results shown in Fig. 4, what version of FlashAttention (FA) was used as the baseline? Would you still observe improvements when using the proposed Triton-based kernel compared to more recent variants of FA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "p70sT2Gdvt", "forum": "63L0Vsvpn9", "replyto": "63L0Vsvpn9", "signatures": ["ICLR.cc/2026/Conference/Submission9702/Reviewer_T2Dp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9702/Reviewer_T2Dp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761507838460, "cdate": 1761507838460, "tmdate": 1762921209069, "mdate": 1762921209069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a training-free framework to accelerate VLMs under multi-image inference scenarios. It presents a prompt-template-aware sparsity mask derived from analyzing attention maps, categorizing attention heads into four structural types: Dense, Sink, Intra-Image, and Intra-Image+Sink. The method further employs a dataset-level aggregation scheme to convert prompt-dependent sparsity patterns into a prompt-agnostic configuration. Experiments were conducted on Qwen2-VL, Qwen2.5-VL, and Gemma 3 models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This study empirically characterizes four recurring head-level attention patterns across major VLM families such as Qwen and Gemma, providing insights into modality-aware sparsity in multimodal transformers.\n- This study presents a Triton-based attention kernel tailored for the proposed method, achieving performance gains in realistic long-context inference tasks."}, "weaknesses": {"value": "- I think this work should be compared with previous vision token pruning and token merging methods for VLMs, such as FastV, LLaVA-PruMerge, DivPrune, and DART, in terms of reducing the computational cost of vision tokens. Currently, only a comparison with the original model as the baseline is provided, and the experimental section therefore feels relatively weak.\n  * [FastV] https://arxiv.org/abs/2403.06764\n  * [LLaVA-PruMerge] https://arxiv.org/abs/2403.15388\n  * [DivPrune] https://arxiv.org/abs/2503.02175\n  * [DART] https://arxiv.org/abs/2502.11494\n\n- The choice of algorithm parameters (alpha_layer, gamma_d, gamma_s, gamma_i) seems largely heuristic. Providing rationale, sensitivity analysis, or a principled selection rule for these values would improve the reproducibility and robustness of the approach.\n\n- The accuracy results in Table 1 are reported without specifying the corresponding sparsity levels or inference gains, making it hard to interpret the trade-offs. \n\n- The prompt-level characterization seems to rely primarily on the MMIU benchmark. It remains unclear how well the identified sparsity patterns generalize to other datasets. An analysis of cross-dataset generalizability would be valuable.\n\n- The prompt-level characterization process does not seem computationally lightweight. It would be helpful if the authors could provide information about the associated computational time.\n\n- The analysis presented in this work is interesting; however, it remains unclear whether the proposed sparsity characterization generalizes to temporally correlated multi-image settings such as video frames. In my experience, video frames can also be treated as separate images with distinct delimiter tokens, often yielding comparable performance in video-based VLM inference. In such cases, certain layers are likely to exhibit inter-frame cross-attention patterns due to the high similarity among visual features. It would be valuable to analyze these correlations and discuss how the proposed method behaves when applied to video-like inputs."}, "questions": {"value": "Please find the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QxpLPvPNnx", "forum": "63L0Vsvpn9", "replyto": "63L0Vsvpn9", "signatures": ["ICLR.cc/2026/Conference/Submission9702/Reviewer_7HSk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9702/Reviewer_7HSk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567297257, "cdate": 1761567297257, "tmdate": 1762921208663, "mdate": 1762921208663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of long Time To First Token (TTFT) in large vision-language models (VLMs), which is caused by long prompt lengths resulting from the inclusion of images. The authors observe that the attention computation in VLMs processing multi-image inputs is inherently sparse, specifically noting the absence of cross-image attention in many layers. Based on this, the paper proposes \"BlindSight,\" a training-free optimization approach. This method utilizes an input-template-aware attention sparsity mask to optimize VLM inference. Specifically, the authors use a dataset to derive a prompt-agnostic categorization for attention heads (Dense, Sink, Intra-Image, Intra-Image+Sink). Furthermore, a Triton-based GPU kernel is developed to leverage this sparsity. Experimental results show that BlindSight achieves a 1.8-3.2x speedup in attention computation for long prompts (36K-300K) with a reported average accuracy degradation of only 1.15% on the evaluated multi-image understanding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's categorization of specific sparsity patterns (Intra-Image, Sink) in multi-image VLMs and linking this sparsity to modality boundary tokens (e.g., <image_start>) is a valuable insight.\n\n2. BlindSight is a training-free method, which means it can be readily applied to existing pre-trained models without costly retraining, making it highly practical.\n\n3. The paper goes beyond theoretical analysis by developing a custom Triton GPU kernel, demonstrating a clear path to translating this sparsity into real-world performance gains."}, "weaknesses": {"value": "1. The paper's claim of an \"average accuracy degradation of only 1.15%\" is misleading. A closer look at Table 1 reveals significant performance drops on certain benchmarks. For example: On Qwen2.5-VL (32B), the MMIU benchmark drops from 44.67 to 41.49 (an absolute drop of 3.18 points, or ~7.1% relative degradation). On Gemma 3 (12B), the MUIRBench benchmark drops from 50.64 to 46.62 (an absolute drop of 4.02 points, or ~7.9% relative degradation). These are substantial performance hits that cannot be considered \"minimal,\" suggesting the method is not robust in preserving performance.\n\n2. The method relies heavily on per-image delimiter tokens in multi-image inputs. The authors admit this makes it inapplicable to current video processing schemes (which typically use only one pair of delimiters for the entire video). This is a major limitation, as video is a primary use case for long-context VLMs.\n\n3. The method introduces several hyperparameters ($\\alpha_{layer}$, $\\gamma_{d}$, $\\gamma_{s}$, $\\gamma_{i}$) that require careful, model-specific tuning. For instance, Qwen and Gemma models require different (fixed vs. linear) $\\alpha_{layer}$ strategies. This diminishes the \"training-free\" claim, as it necessitates a sensitive, model-specific tuning process on a representative dataset.\n\n4. While the categorization of VLM sparsity patterns is useful, it builds heavily on existing work on attention sinks and static/dynamic sparsity patterns. Given the significant accuracy trade-off, this incremental novelty may not be sufficient."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ABPEeUchsJ", "forum": "63L0Vsvpn9", "replyto": "63L0Vsvpn9", "signatures": ["ICLR.cc/2026/Conference/Submission9702/Reviewer_JbAX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9702/Reviewer_JbAX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911735591, "cdate": 1761911735591, "tmdate": 1762921208346, "mdate": 1762921208346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "BlindSight is a training-free method that accelerates multi-image vision-language model inference by leveraging the inherent sparsity in attention computation, constructing input-template-aware sparse masks without modifying model architecture. It delivers 1.8–3.2× attention speedup and only about 1.15% accuracy degradation on major benchmarks for Qwen2-VL, Qwen2.5-VL, and Gemma 3 models"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "BlindSight offers significant inference acceleration for multi-image vision-language models by exploiting attention sparsity without requiring extra training or changes to model architecture.\n\nIt maintains almost the same accuracy as dense attention, showing an average accuracy degradation of only about 1.15% across major benchmarks."}, "weaknesses": {"value": "BlindSight relies on predefined sparsity patterns, so it may not capture context-dependent attention dynamics that could be important for some prompts or tasks.\n\nThe minimal accuracy drop is measured only on major benchmarks; specific cases or other domains might experience higher accuracy degradation.\n\nIntegration requires careful attention boundary detection, and underlying model changes (e.g., image tokenization strategy) may affect its effectiveness or compatibility."}, "questions": {"value": "Here are some possible questions for the paper “BlindSight: Harnessing Sparsity for Efficient Vision-Language Models”:\n\nHow well does the BlindSight approach generalize to vision-language models beyond those tested, such as proprietary or non-transformer architectures?\n\nCould the predefined sparse mask templates miss specialized cross-modal or long-range interactions in exceptional tasks, and how can adaptability be improved?\n\nWhat are the trade-offs when tuning the sparsity and accuracy thresholds, and how robust are these methods across varying image resolutions and prompt structures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GRhs7dmThT", "forum": "63L0Vsvpn9", "replyto": "63L0Vsvpn9", "signatures": ["ICLR.cc/2026/Conference/Submission9702/Reviewer_3Um8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9702/Reviewer_3Um8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762176796828, "cdate": 1762176796828, "tmdate": 1762921208064, "mdate": 1762921208064, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}