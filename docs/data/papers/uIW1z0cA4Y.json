{"id": "uIW1z0cA4Y", "number": 6603, "cdate": 1757990230458, "mdate": 1759897905686, "content": {"title": "OpenGPT-4o-Image: A Comprehensive Dataset for Advanced  Image Generation and Editing", "abstract": "The performance of unified multimodal models for image generation and editing is fundamentally constrained by the quality and comprehensiveness of their training data. While existing datasets have covered basic tasks like style transfer and simple object manipulation, they often lack the systematic structure and challenging scenarios required for real-world applications. To address this bottleneck, we introduce \\textbf{OpenGPT-4o-Image}, a large-scale dataset constructed using a novel methodology that combines hierarchical task taxonomy with automated data generation. Our taxonomy not only includes fundamental capabilities such as {text rendering} and {style control} but also introduces highly practical yet challenging categories like \\textbf{scientific imagery} for physics/chemistry illustrations and \\textbf{complex instruction editing} requiring simultaneous execution of multiple operations. Through an automated pipeline leveraging structured resource pools and GPT-4o, we generate 80k high-quality instruction-image pairs with controlled diversity, covering 11 major domains and 51 subtasks. Extensive experiments show that fine-tuning leading models on our dataset achieves significant performance gains across multiple benchmarks, with improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench) and 13\\% on generation tasks (Harmon on GenEval). Our work demonstrates that systematic data construction is key to advancing multimodal AI capabilities.", "tldr": "", "keywords": ["Generation", "Editing", "Dataset", "Unify MLLMs"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f75b140631ddee7a71fbae1350b3f557593d0e6a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the limitations of current multimodal models in image generation and editing tasks due to insufficient data quality and coverage by proposing the OpenGPT-4o-Image dataset. Its core contributions include a hierarchical task taxonomy (covering multiple domains and subtasks, such as style control, scientific image generation, and complex instruction editing) and a GPT-4o-driven automated pipeline, which generates 80k high-quality instruction-image pairs, ensuring data diversity, controllable difficulty, and semantic accuracy. Experimental validation shows that the dataset significantly improves model performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work provides a systematic, data-centric pipeline for advancing generative AI, addressing the critical bottleneck of training data for real-world applications in specialized domains. \n\n2. The proposed dataset is a valuable, as it partly addresses the scarcity of high-quality data for image generation and editing.\n\n3. The paper is well-written and easy to follow. It features a logical flow, precise definitions, and effective use of figures to illustrate the dataset's scope and the automated pipeline's workflow."}, "weaknesses": {"value": "1. The core method heavily relies on GPT-4o-Image as a data generation engine, which introduces the risk of model bias. Although the paper acknowledges this limitation, it does not delve into or quantify the specific manifestations of this bias. This constrains the long-term evolvability of the dataset, as its quality is intrinsically tied to the capabilities of a closed-source model.\n\n2. The paper emphasizes proactive quality control through hierarchical taxonomy. However, this approach remains a priori and rule-based design, rather than an outcome-based validation. A critical gap is the lack of a rigorous human or automated verification step to systematically assess the semantic fidelity of each generated sample.\n\n3. The paper omits a crucial ablation study to validate its central claim—that the hierarchical taxonomy enhances data quality. For instance, an experiment comparing against fine-tuning on an unsystematized subset of the data would be necessary to demonstrate the value of the proposed method."}, "questions": {"value": "Please see Weaknesses Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9P7ESb6LVr", "forum": "uIW1z0cA4Y", "replyto": "uIW1z0cA4Y", "signatures": ["ICLR.cc/2026/Conference/Submission6603/Reviewer_r4vy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6603/Reviewer_r4vy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381742137, "cdate": 1761381742137, "tmdate": 1762918927448, "mdate": 1762918927448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the data bottleneck in training multimodal models for image generation and editing tasks. OpenGPT-4o-Image (80k) is introduced as a large-scale dataset, systematically constructed entirely from GPT-4o-generated instruction–image pairs, with the goal of enhancing multimodal model performance. Experimental results indicate that mainstream models (e.g., UniWorld-V1, Harmon), when fine-tuned on this GPT-4o-generated dataset, achieve up to 18% improvement in editing tasks (ImgEdit-Bench) and up to 13% improvement in generation tasks (GenEval), surpassing contemporaneous works such as ShareGPT-4o-Image."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Comprehensive multi-dimensional validation: performance variation is assessed across different models (four mainstream models), different tasks (generation + editing), and different data scales (20K / 30K / 40K). Significant improvements are observed across most tasks, verifying the dataset’s effectiveness.\n2. Fine-tuned models show notable gains in tasks such as in-image text rendering (e.g., menu generation, calligraphic text) and spatial reasoning (e.g., object-relative positioning, symmetry analysis), making them readily applicable to real-world scenarios in office and design domains.\n3. Introduction of scientific imagery as an independent module, covering specialized domains such as mathematics, physics, and ecology. This effectively addresses the scarcity of professional technical illustrations in existing datasets."}, "weaknesses": {"value": "The dataset is entirely generated via the GPT-4o API , with mitigation of GPT-4o’s inherent biases (e.g., semantic skew, stylistic preferences). Such biases may be propagated into fine-tuned models. For instance, GPT-4o-generated editing data is known to have weaker ID consistency and background preservation, which can degrade ID retention in fine-tuned models, as illustrated by the “dog” case in Figure 6."}, "questions": {"value": "1. Since the editing dataset is generated with GPT-4o and the generated outputs are in principle not perfect ground-truth references (for example, ID consistency may degrade), advances in editing technology may raise concerns about the dataset’s long-term relevance. If stronger editing models Edit become available, could this dataset still provide measurable performance gains when used for fine-tuning these more capable models? Can you test it on the recent Qwen-Image Edit Model?\n2. Given the observed issue that GPT-4o-generated editing data may impair ID retention capability, what strategies could be adopted to alleviate this problem and enhance ID consistency in fine-tuned models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K4k2LqsjmW", "forum": "uIW1z0cA4Y", "replyto": "uIW1z0cA4Y", "signatures": ["ICLR.cc/2026/Conference/Submission6603/Reviewer_WuG5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6603/Reviewer_WuG5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756325828, "cdate": 1761756325828, "tmdate": 1762918927134, "mdate": 1762918927134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OpenGPT-4o-Image, a dataset of 80k instruction–image pairs for image generation and image editing tasks. It claims to offer a hierarchical taxonomy covering 11 domains and 51 subtasks, and to improve fine-tuning performance for several multimodal models such as OmniGen, OmniGen2, UniWorld-V1, and Harmon."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important bottleneck in multimodal research, which is the lack of systematically structured and high-quality datasets for unified image generation and editing. The motivation is clearly aligned with the broader trend of building large, instruction-based multimodal datasets.\n\nThe proposed hierarchical task taxonomy, which spans 11 domains and 51 subtasks, is conceptually interesting and comprehensive.\n\nThe methodology of using GPT-4o to generate structured prompts and synthetic instruction–image pairs is well-described and could be useful for future data curation work."}, "weaknesses": {"value": "Table 1 makes me very confused. The table duplicates MagicBrush, UniWorld-V1, OmniGen and OmniGen2 entries multiple times, sometimes with symbols (†, ‡) that are inconsistently explained. It appears that the results in the upper half of the table were directly copied from Table 5 in OmniGen2 [1] rather than reproduced by the authors. Moreover, the results reproduced by the authors show a substantial discrepancy compared with those reported in the original paper. The row MagicBrush† and the row UniWorld-V1†, what are their differences compared to MagicBrush and UniWorld-V1 respectively, and † represents results without fine-tuning, so why are these results included in the fine-tuning section?\n\nSimilar issues are also present in Table 2.\n\nThe results in Table 2 are almost identical across different models and settings, which makes the evaluation unconvincing.\n\n[1] OmniGen2: Exploration to Advanced Multimodal Generation"}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OaACETNNRy", "forum": "uIW1z0cA4Y", "replyto": "uIW1z0cA4Y", "signatures": ["ICLR.cc/2026/Conference/Submission6603/Reviewer_Ksuf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6603/Reviewer_Ksuf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956890455, "cdate": 1761956890455, "tmdate": 1762918926735, "mdate": 1762918926735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OpenGPT-4o-Image, a dataset of about 80k instruction-image pairs organized by a hierarchical taxonomy that spans 11 major domains and 51 subtasks for both generation and editing. The generation side covers five modules, including Style Control, Complex Instruction Following, In-Image Text Rendering, Spatial Reasoning, and Scientific Imagery. The editing side defines six categories with 21 subtasks such as Subject Manipulation, Text Editing, Complex Instruction Editing, Multi-Turn Editing, Global Editing, and other challenging edits. The data are produced through automated pipelines that use structured resource pools plus GPT-4o and gpt-image-1 for prompt and image synthesis. Fine-tuning several recent models on this dataset yields consistent gains on GenEval, DPG-Bench, GEdit-Bench, and ImgEdit-Bench, for example up to about 18 percent on editing and about 13 percent on generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The generation modules are explicitly enumerated with examples, and the editing taxonomy covers 21 subtasks, which gives useful coverage for targeted training and diagnosis.    \n\nS2: The paper specifies sample counts for several sub-areas, for example Style Control at 13k, In-Image Text Rendering at 3k, Spatial Reasoning at 8k, and Scientific Imagery at 10k, which clarifies dataset balance.        \n\nS3: For generation, the authors define resource pools and template-based prompt construction. For editing, they integrate multiple sources and use GPT-4o for instruction creation plus inpainting, which is well documented.      \n\nS4: The paper reports consistent improvements after fine-tuning, including Harmon’s increase on GenEval and strong boosts on ImgEdit-Bench and GEdit-Bench for several systems."}, "weaknesses": {"value": "W1: The pipeline relies on GPT-4o for prompt generation and on gpt-image-1 for image synthesis and inpainting. This may imprint model-specific priors or artifacts on the dataset and can indirectly entangle training with the same ecosystem used in evaluation. The conclusion itself notes possible GPT-4o bias and the focus on existing benchmarks. A more formal bias audit or cross-ecosystem sanity checks would help.    \n\nW2: The paper emphasizes automatic construction and benchmark-based evaluation, but I did not find a human study verifying that improvements correspond to perceived visual quality or edit faithfulness. This risks over-optimizing to the metrics used by GenEval, DPG-Bench, GEdit-Bench, and ImgEdit-Bench rather than to human preferences.  \n\nW3: The editing data incorporate curated high-resolution images and outputs from multiple datasets and generators. The paper does not detail licensing, filtering for sensitive content, or subject consent for portrait-like material, which is important for release.  \n\nW4: The dataset provides strong coverage in specific capability clusters, yet many real-world cases involve open-world composition or rare styles. Although the taxonomy is broad, the paper does not quantify how balanced the final 80k set is across the 51 subtasks, nor whether harder long-tail prompts are sufficiently represented.    \n\nW5: Data scaling uses 20k to 40k subsets and picks 40k after observing a small delta. It would be helpful to extend the curve and to separate generation versus editing contributions per module."}, "questions": {"value": "How do you mitigate bias from using GPT-4o and gpt-image-1 throughout the pipeline? Can you show cross-assessor validation using independent VLMs or human audits and report agreement with benchmark gains?  \n\nWhat is the distribution across the 51 subtasks and 11 domains, and how do per-subtask gains correlate with data volume? Please provide a stratified breakdown with confidence intervals.  \n\nFor editing, how are licensing and safety handled for curated high-resolution images and for content from external corpora such as OmniEdit or ImgEdit? Do you filter portraits or sensitive scenes before release?  \n\nCan you include a small human evaluation verifying that fine-tuned models improve perceived edit faithfulness and visual quality, not only metric scores on the four benchmarks?  \n\nCould you add ablations isolating each generation module and each editing category to identify which components most drive GenEval and ImgEdit-Bench improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "inhMdaXBbI", "forum": "uIW1z0cA4Y", "replyto": "uIW1z0cA4Y", "signatures": ["ICLR.cc/2026/Conference/Submission6603/Reviewer_Gujd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6603/Reviewer_Gujd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6603/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980898205, "cdate": 1761980898205, "tmdate": 1762918926305, "mdate": 1762918926305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}