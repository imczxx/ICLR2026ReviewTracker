{"id": "sIdhLTLqXn", "number": 22736, "cdate": 1758334876574, "mdate": 1759896849794, "content": {"title": "Transformer Is Inherently a Causal Learner", "abstract": "We reveal that transformers trained in an autoregressive manner  naturally encode time-delayed causal structures in their learned representations. When predicting future values in multivariate time series, the gradient sensitivities of transformer outputs with respect to past inputs directly recover the underlying causal graph, without any explicit causal objectives or structural constraints. We prove this connection theoretically under standard identifiability conditions and develop a practical extraction method using aggregated gradient attributions. On challenging cases such as nonlinear dynamics, long-term dependencies and non-stationary systems, we see this approach greatly surpass the performance of state-of-the-art discovery algorithms, especially as data heterogeneity increases, exhibiting scaling potential where causal accuracy improves with data volume, a property traditional methods lack. This unifying view opens a new paradigm where causal discovery operates through the lens of foundation models, and foundation models gain interpretability and enhancement through the lens of causality.", "tldr": "", "keywords": ["Autoregressive transformers", "Time-series causal discovery"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1cacb6330f082c29749fcbd4540a4a7983458f1b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to provide a theoretical framework that the gradient sensitivities of transformer outputs w.r.t. past inputs directly recover the underlying causal graph.  They provided experimental results to support their conclusions and compared them with many causal discovery baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors proved that decoder-only transformers trained autoregressively admit causal identifiability.\n2. They offered clear and plausible theoretical motivation for their DOT algorithm.\n3. The authors tested DOT's performance and compared it with previous methods."}, "weaknesses": {"value": "1. The proposed method is not well-validated experimentally. How does the causal graph obtained using DOT compare to the ground truth? Are there any visualizations of the results, as well as more evaluation metrics such as SHD?\n\n2. Some problems arose during the literature review. The causal relationship mentioned in the article is basically based on Granger causality, but DYNOTEARS does not deal with Granger causality, so it should not be discussed in a unified manner.\n\n3. There are already some methods that use gradients for Granger causality detection, such as JNRGC, but DOT does not compare them or fully discuss the differences between them. In fact, JNRGC can also be flexibly applied in the Transformer framework.\n\n4. The LRP attribution method used does not seem to be the author's original invention, but a direct reference to other algorithms. I suspect this is not novel enough.\n\n[1] Zhou, W., Bai, S., Yu, S., Zhao, Q., & Chen, B. (2024). Jacobian regularizer-based neural granger causality. arXiv preprint arXiv:2405.08779."}, "questions": {"value": "1. What is the approximate range of the sample size required for DOT? Is there any relationship between it and the number of nodes and causal structure?\n\n2. Can DOT handle dynamic causal graphs? Is there a specific case study?\n\n3. Figure 5 reports DOT’s ability to cope with noise sequences. Does different noise intensities have a big impact on the algorithm?\n\n4. What is the maximum number of nodes that DOT can handle? Are there any experiments and visualizations on real-world datasets, such as CausalTime?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5iqwo9t1rB", "forum": "sIdhLTLqXn", "replyto": "sIdhLTLqXn", "signatures": ["ICLR.cc/2026/Conference/Submission22736/Reviewer_bY8W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22736/Reviewer_bY8W"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708854989, "cdate": 1761708854989, "tmdate": 1762942363987, "mdate": 1762942363987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new identifiability result concerning causal DAGs and transformers: under standard assumptions, they show (1) the DAG implied by the generating structural causal model (in the common sense of Pearl, Spirtes et al., and Peters et al.) is equivalent to the Granger-causal DAG and is identifiable from the conditional independence relations and (2) the conditional independence relations are identifiable from the gradients in a standard transformer.\nI find (2) and it's application in (1) quite interesting, but I'm afraid (1) is false, or at least imprecisely stated---but I think the authors can either fix it or help clear up my misunderstanding during the rebuttal."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- ground-breaking connection (if the theory holds up) between causal models and transformers\n- clearly written\n- good breadth of experimental settings (other than DAG sparsity) and baselines from the literature"}, "weaknesses": {"value": "- conflates the usual notion of causality with Granger-causality, especially in the title and abstract, but also elsewhere in the paper\n- missing important causality context, background, and related work\n- illegible figures\n- limited experiments: only 3 replicates, very sparse generating DAGs, and potentially misleading metrics"}, "questions": {"value": "I'm happy to increase my score depending on how these questions are answered.\n1. The paper uses just \"causal\" in the title and abstract and then later seems to qualify that it's only Granger-causal, and then alternates between the two notions in Section 3.1---which is it? If it really is only Granger-causal, this needs to be made much more clear, especially in the title/abstract, to avoid exaggerated claims; or if Theorem 1 really is about (nonGranger) causal DAG identifiability, it should be made more clear, because this is an important result, even without the connection to gradients. In any case, there should be more explicit discussion on the difference between the two notions.\n2. How does Theorem 1 related to existing results in the literature? There should be more discussion about this, including references. As far as I know, the closest related work is [1], which suggests that Theorem 1 actually requires the stronger assumption of *conditional exogeneity* rather than the weaker causal sufficiency in A1---have I misunderstood something?\n3. A4 should be discussed more carefully; since the work in [2], faithfulness isn't considered to be as mild as the discussion around L196 suggests.\n4. L207: this should be \"Granger-causes\", shouldn't it?\n5. L019: causal accuracy *does* improve with data volume for traditional methods (e.g., provably consistent algorithms, like GES and PC); are the authors rather just trying to say here that traditional methods don't scale?\n6. on L247, how does complexity control in score-based methods (which is commonly, e.g., the L1 norm of the adjacency matrix) limit scalability? and maybe the word \"regularization\" is missing after sparsity on the sentence before?\n7. L121: what does \"horizons\" mean here?\n8. L279: precision or F1 score?\n9. Fig 2 (B): \"F1 averaged across\"...?\n10. L452: it's only \"coherent\" for a linear additive noise model, right? so not for transformers?\n11. L492 explicitly recommends interventional data, but the method isn't at all developed for interventional data, is it?\n12. L869: I wouldn't call fixed in-degree \"comparable sparsity\": for fixed in-degree, as the number of nodes increases, the probability of an edge existing goes to 0. For this reason, along with the unsymmetric F1 score, it's essential to include some sparse random guessing baseline in the experiments, in the experiments, to help contextualize the reported results.\n\n[1] White, H., & Lu, X. (2010). Granger causality and dynamic structural systems. Journal of Financial Econometrics, 8(2), 193-243.\n\n[2] Uhler, C., Raskutti, G., Bühlmann, P., & Yu, B. (2013). Geometry of the faithfulness assumption in causal inference. The Annals of Statistics, 436-463."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wRrrjHvpyo", "forum": "sIdhLTLqXn", "replyto": "sIdhLTLqXn", "signatures": ["ICLR.cc/2026/Conference/Submission22736/Reviewer_Lj6d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22736/Reviewer_Lj6d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947456947, "cdate": 1761947456947, "tmdate": 1762942363641, "mdate": 1762942363641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper claims that transformers are inherently causal learners. The argument proceeds in two parts: first, by pointing out that under causal sufficiency, faithfulness, and no instantaneous effects, causal graphs are identifiable via standard autoregression. It's then argued that transformers are well-suited, with layer-wise relevance propagation, to exploiting this. Numerical experiments are undertaken to compare to various time series structure learning algorithms, finding competitive or favorable performance in transformers according to the F1 metric. One additional contribution is experiments and arguments as to why gradient aggregation is better for graph extraction than attention weights."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- It's certainly of interest to the causality community to obtain scalable and efficient structure learning algorithms, and equally interesting to the more general ML community to understand causal properties of commonly-used architectures. \n- The results are impressive, with transformers outperforming several existing causal discovery methods in various settings.\n- The paper is well-written and well-motivated, which is particularly positive given the broad audience targeted."}, "weaknesses": {"value": "The primary weakness in my opinion is the framing of the paper. To me, the paper essentially makes two separate arguments: (1) that under assumptions A1-A4, causal structure learning reduces to standard statistical learning; and (2), that transformers are particularly well-aligned to causal learning. Both of these arguments appear sound, but I would consider (1) to be well-known, and (2) to fall short of the claim that transformers are \"inherently causal.\" Indeed, the authors concede that Theorem 1 applies to \"any model that could perfectly fit the data, which means it can be any neural network architecture.\" While I appreciate the arguments set forth that transformers are well-aligned to (1), I believe the claim that (decoder-only, autoregressive) transformers are \"inherently causal\" is strong and subject to misinterpretation. \n\nSome of the benchmarking can also be improved. For example, one could make some similar arguments that 1d-CNNs (as used in, e.g., NTS-NOTEARS) are well-suited to causal discovery (though there is, of course, evidence that transformers handle long-scale dependencies better). Including 1d-CNNs that do not enforce acyclicity constraints on instantaneous causes, or author autoregressive architectures, would help improve breadth. Including other benchmarks (for example, similar to CausalTime [1]) would improve the paper. Alternative metrics may also be considered, such as SHD.\n\nA related weakness is the plausibility of assumptions. In particular, the lack of instantaneous edges. Indeed, in NTS-NOTEARS as an example, a significant amount of work is spent on acyclicity constraints for instantaneous causes, and much work and difficult optimization could be avoided via (A2). A potential remedy is also proposed for (A2), but unless I missed it, explicit experiments are not present.\n\nThere is also some prior work regarding the use of population gradients for graph extraction in differentiable causal discovery. NOTEARS+ [2] theoretically argues for using gradients to extract graphs, and [3] makes similar arguments to the present paper about potential pitfalls of using gradient proxies.\n\n[1] Cheng, Y., Wang, Z., Xiao, T., Zhong, Q., Suo, J., & He, K. CausalTime: Realistically Generated Time-series for Benchmarking of Causal Discovery. In The Twelfth International Conference on Learning Representations.\n\n[2] Zheng, X., Dan, C., Aragam, B., Ravikumar, P., & Xing, E. (2020). Learning sparse nonparametric DAGs. In International Conference on Artificial Intelligence and Statistics (pp. 3414-3425). PMLR.\n\n[3] Waxman, D., Butler, K., & Djurić, P. M. (2024). Dagma-DCE: Interpretable, non-parametric differentiable causal discovery. IEEE Open Journal of Signal Processing, 5, 393-401."}, "questions": {"value": "1. How do alternative architectures perform under the predictive causality framework identified via (A1-A4)?\n\n2. How do transformers perform on more realistically-generated time series (e.g., from CausalTime)?\n\n3. What are precision and recall rates for each method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AfUYfk3Gxy", "forum": "sIdhLTLqXn", "replyto": "sIdhLTLqXn", "signatures": ["ICLR.cc/2026/Conference/Submission22736/Reviewer_h6Bw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22736/Reviewer_h6Bw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973638099, "cdate": 1761973638099, "tmdate": 1762942363254, "mdate": 1762942363254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that standard autoregressive transformers, when trained for time-series forecasting, implicitly learn the underlying time‐lagged causal structure. The authors build a theoretical argument—under common identifiability assumptions—that the true causal parents can be identified by examining how the model’s predictions respond to perturbations in past inputs. They propose a practical extraction method based on Layer-wise Relevance Propagation (LRP) and show that it performs competitively, often outperforming state-of-the-art causal discovery approaches, particularly in high-dimensional, nonlinear, and non-stationary settings."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The main claim—that a regular forecasting transformer effectively learns a causal graph without any explicit causal objective—is both elegant and conceptually appealing. It links large-scale predictive modeling with causal structure learning in a way that feels natural and potentially impactful.\n\n- The paper doesn’t rely solely on empirical results. Theorem 1 provides a clear connection between the forecasting task and recoverability of causal parents, assuming standard conditions.\n\n- The evaluation is thorough and well thought out. The proposed method (DOT) consistently outperforms strong baselines such as PCMCI, DYNOTEARS, and VAR-LiNGAM, especially in settings that are typically most challenging: many variables, long-range dependencies, and changing distributions.\n\n- The authors carefully examine how to extract causal structure from transformers and convincingly show why raw attention weights are not reliable, while gradient-based attribution is more informative."}, "weaknesses": {"value": "- The method seems to require more data compared to some specialized approaches in low-dimensional or simpler linear systems. For example, VAR-LiNGAM requires far less data when the dynamics are mostly linear (Figure 3).\n\n- (minor)  Figure 2’s caption labels two subplots as (C), which needs correction.\n\nMissing Citations / Context\n\n\nThe paper compares well against classical baselines, but recent transformer-based causal discovery approaches are not discussed. For example, CausalFormer (Liu et al., 2024) also uses transformer models with attribution-based interpretation. It would be helpful to clarify how this work differs, especially given that the authors emphasize not modifying the transformer architecture.\n\nAdditionally, other recent methods aimed at high-dimensional scalability (e.g., approaches based on dynamical community partitioning or low-rank structure, constrain satisfaction with answer set programming) could provide more context for the scalability claims. Since scalability is a key selling point here, a brief discussion would help readers see the contribution in a broader landscape."}, "questions": {"value": "You mention that the current framework (due to assumption A2) does not handle instantaneous effects but could be combined with other algorithms. Could you elaborate on how you see this working? Would you first use the transformer to get the lagged graph and then apply a method like LiNGAM on the transformer's prediction residuals, as hinted at in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FUyQloVPLm", "forum": "sIdhLTLqXn", "replyto": "sIdhLTLqXn", "signatures": ["ICLR.cc/2026/Conference/Submission22736/Reviewer_iJgh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22736/Reviewer_iJgh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762896876785, "cdate": 1762896876785, "tmdate": 1762942362854, "mdate": 1762942362854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes that decoder-only transformers trained autoregressively can inherently recover lagged causal structure in time-series data. The authors theoretically show that under standard assumptions (causal sufficiency, no instantaneous effects, adequate lag coverage, and faithfulness), the gradient sensitivities of a transformer’s predictions with respect to past inputs correspond to causal dependencies. They operationalize this approach with Layer-wise Relevance Propagation (LRP) and demonstrate empirically that it outperforms classical and neural causal discovery methods on synthetic datasets spanning nonlinear, nonstationary, and high-dimensional regimes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well motivated, connecting two areas - causal discovery and transformer-based autoregressive modeling. The authors clearly argue that transformers, trained with standard forecasting objectives, may implicitly learn lagged causal structures.\n\n2. The experiments cover a wide range of synthetic regimes, including high-dimensional, long-range, nonlinear, and non-stationary systems.\n\n3. The transformer-based approach scales efficiently to higher dimensions and larger lag windows, where traditional methods tend to time out or degrade in accuracy. The model shows monotonic improvement with sample size, indicating favorable scaling behavior."}, "weaknesses": {"value": "1. While the paper establishes a theoretical link between population gradients and causal identifiability, it does not provide empirical evidence that gradient-or LRP-based attributions reliably correspond to true causal effects. All experiments evaluate graph recovery accuracy but do not include diagnostic analyses verifying whether gradient magnitudes align with known interventional or conditional-independence causal measures. As a result, it remains unclear whether the model’s attributions capture genuine causal mechanisms or merely reflect predictive correlations that the transformer has learned. Demonstrating empirical alignment between gradient signals and interventional effects (e.g., via controlled ablation or counterfactual perturbation tests) would be essential to substantiate the causal claims.\n\n2. The assumptions (no confounders, no instantaneous effects) are strong and only briefly discussed in terms of practical violations.\n\n3. The reliance on LRP is heuristic; attribution stability under noise, scaling, or initialization is not explored.\n\n4. Sensitivity to hyperparameter top-k binarization is missing. \n\n5. The identifiability result relies on causal sufficiency and the absence of instantaneous effects, conditions rarely satisfied in realistic systems. While these are discussed qualitatively, there is no empirical study of robustness when assumptions are violated (e.g., latent confounders, contemporaneous interactions)."}, "questions": {"value": "1. How sensitive are the recovered graphs to model depth, attention heads, or normalization layers?\n\n2. Can pretrained forecasting transformers be reused for causal extraction without retraining?\n\n3. Could you discuss whether gradient-based attributions correlate with intervention-based effects?\n\n4. The proposed Top-k and uniform-threshold binarization rules control graph density differently. How is k selected in practice, and how sensitive are the F1 results to this choice?\n\n5. Since all experiments are synthetic, to what extent do you expect the observed scaling behavior and robustness to generalize to real-world nonstationary data, where causal mechanisms may change over time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qUbyuBvsoc", "forum": "sIdhLTLqXn", "replyto": "sIdhLTLqXn", "signatures": ["ICLR.cc/2026/Conference/Submission22736/Reviewer_xm8b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22736/Reviewer_xm8b"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22736/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762973949406, "cdate": 1762973949406, "tmdate": 1762974071228, "mdate": 1762974071228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}