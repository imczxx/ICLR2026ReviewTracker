{"id": "W4FAenIrQ2", "number": 16792, "cdate": 1758268722661, "mdate": 1759897219136, "content": {"title": "RedSage: A Cybersecurity Generalist LLM", "abstract": "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools.\nBuilding on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training.\nTo rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q\\&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code will be released to advance reproducibility and open cybersecurity LLM research.", "tldr": "RedSage leverages domain-aware pretraining and agentic augmentation for post-training to build an open cybersecurity LLM. Evaluated with RedSage-Bench (30K), it improves both security-specific and general LLM performance.", "keywords": ["cybersecurity", "large language models", "data-driven", "dataset curation", "agentic augmentation", "continual pretraining", "supervised fine-tuning", "benchmark evaluation", "open-source"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03df87e79639e41c7c86ae5b134c1a2bbb27d327.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces RedSage, an open-source cybersecurity-oriented LLM trained via domain-aware continual pretraining and agentic post-training augmentation. This work also proposes RedSage-Bench, a 30K-question benchmark spanning a diverse range of tasks. Authors of this paper claim the model outperforms state-of-the-art specialized baselines on both cybersecurity benchmarks and general LLM tasks, demonstrating that cybersecurity specialization can enhance both domain-specific and general reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Clear presentation: This work integrates all stages (CPT, SFT, DPO) with substantial data at each phase, which is clearly shown in Fig. 1. Readers can have a very straightforward view on how RedSage was trained on the dataset selected.\n\nComprehensive Training data coverage: The integration of CyberFineWeb, curated RedSage-Seed, and agentic augmentation provides strong coverage across cybersecurity subfields.\n\nProposed New Benchmark: What I am interested in is that this work expands prior benchmarks by incorporating tool proficiency and qualitative evaluation, from reviewerâ€™s point of view, this innovation can close a key gap in cybersecurity LLM assessment."}, "weaknesses": {"value": "May need computational cost analysis: While \"compute constraints\" are briefly mentioned in page 4 (CyberFineWeb section), there's no breakdown of training time, GPU-hours, or carbon footprint across stages. Adding some analysis on computational cost would help readers to form a general impression on the scale of RedSage training process.\n\nLimited human validation: One of my concern on this work is the data part is heavily reliance on LLM-based verification, which could introduce subtle self-reinforcing biases.\n\nTeacher model (verifier) analysis: Augmentation uses only Llama-3.3-70B and Qwen2.5-72B. Analysis on teacher model can make this paper more solid, such as examining sensitivity to teacher model choice, temperature settings, or comparison with smaller/different teacher models.\n\nMinor issue: Font size on Fig. 3 and Fig. 5 can be larger which I felt difficulty to read"}, "questions": {"value": "As mentioned in the weakness, how do you ensure LLM-generated augmentation does not reinforce factual errors or tool misuse patterns?\n\nHow would RedSage perform in interactive cybersecurity reasoning task such as CTF agentic settings on benchmarks like CyBench?\n\nDoes the author consider data contamination for the proposed datasets and benchmark?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "I think it would be a minor issue as authors of this paper already stated in the ethics statement section."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ADM3vdthut", "forum": "W4FAenIrQ2", "replyto": "W4FAenIrQ2", "signatures": ["ICLR.cc/2026/Conference/Submission16792/Reviewer_QvXR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16792/Reviewer_QvXR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761261950728, "cdate": 1761261950728, "tmdate": 1762926829649, "mdate": 1762926829649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Thank you for the valuable feedback"}, "comment": {"value": "We sincerely thank all reviewers (mCDa, f3PV, QvXR) for their positive feedback and insightful comments. The paper was recognized as well-motivated, clearly written, and easy to follow (mCDa); it demonstrates that cybersecurity-curated pre- and post-training significantly improves performance without catastrophic forgetting, and that the curated dataset is valuable for future research (f3PV); and it offers a clear presentation, comprehensive data coverage through CyberFineWeb, RedSage-Seed, and agentic augmentation, with a benchmark that fills a key gap in cybersecurity LLM assessment (QvXR). We greatly appreciate these encouraging remarks and will publicly release all code, datasets, and our pretrained RedSage models to support further research. We address all the reviewers' concerns in the official comments."}}, "id": "zuOBR5saUi", "forum": "W4FAenIrQ2", "replyto": "W4FAenIrQ2", "signatures": ["ICLR.cc/2026/Conference/Submission16792/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16792/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16792/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763720041941, "cdate": 1763720041941, "tmdate": 1763721266053, "mdate": 1763721266053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RedSage, a cybersecurity-focused LLM that is trained with curated pre-training and post-training for cybersecurity knowledge. It presents an elaborate cybersecurity data curation pipeline that uses LLMs for isolating cybersecurity relevant data and augmenting multi-turn conversational data for post-training. The results demonstrate how both pre-training and post-training can help improve performance on cybersecurity tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper clearly demonstrates that cybersecurity curated pre- and post-training improves performance significantly without catastrophic forgetting of general knowledge\n- The cybersecurity-specific curated dataset is incredibly useful for future training tasks"}, "weaknesses": {"value": "- The method for open-ended Q&A evaluation is not adequately described. Line 317 mentions \"prefix exact match or regex matching\" and points to Appendix C.1 however neither the text nor the appendix provide sufficient details or references to clearly understand this evaluation.\n- The method for generating the instruction-tuned variant is not explained or referenced. A diagramatic view of how each of the variants was derived would be helpful."}, "questions": {"value": "- Why is there a large imbalance between MCQ and open-ended Q&A (30K vs 240)? Is it because of human verification of open-ended benchmark? Please provide details of the human verification.\n- Some estimates of training time would help future research aiming to replicate this work\n- Were other LLMs tried for the agentic augmentation and why were Llama-3.3-70B and Qwen2.5-72B chosen?\n- It would be interesting to see how RedSage fairs against models of higher size and commercial models atleast on the general benchmarks. For instance, RedSage-8B-DPO catches up with GPT4 performance on CTIBench-MCQ."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kGl1g79Efp", "forum": "W4FAenIrQ2", "replyto": "W4FAenIrQ2", "signatures": ["ICLR.cc/2026/Conference/Submission16792/Reviewer_f3PV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16792/Reviewer_f3PV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971368687, "cdate": 1761971368687, "tmdate": 1762926829189, "mdate": 1762926829189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper curates a corpus of data (11.8B tokens) by filtering FineWeb and perform continued pre-training from Qwen3-8B-Base. The resulting models improve cybersecurity multiple-choice benchmarks. Along the way, the authors also introduce a new benchmark, RedSage-Bench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-motivated: to improve a domain-specific capability (cybersecurity), go and curate data for it and fine-tune an existing model.\nThe paper is fairly well-written and easy to follow."}, "weaknesses": {"value": "There is not really any methodological novelty, given this is similar to other methods like FineWeb-Edu.\nI would have liked to see evaluation of larger models (at least 32B) to see how well the methodology transfers to stronger models (one might worry that the gap will shrink).\nIt would also be good to get a closed model (e.g., GPT-5 or Claude) to get a ceiling for the new benchmark.\nThe abstract claims that the fine-tuned model improves on OpenLLM leaderboard tasks, but looking at Table 6, it seems like except on GSM8K, the RedSage is worse as would be expected."}, "questions": {"value": "How were the labels for the ModernBERT-base clasisifer obtained? Were there an LLMs that were used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k6gFKiEJO2", "forum": "W4FAenIrQ2", "replyto": "W4FAenIrQ2", "signatures": ["ICLR.cc/2026/Conference/Submission16792/Reviewer_mCDa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16792/Reviewer_mCDa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129519339, "cdate": 1762129519339, "tmdate": 1762926828856, "mdate": 1762926828856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}