{"id": "t5BFYQLCby", "number": 11475, "cdate": 1758199988508, "mdate": 1763763526200, "content": {"title": "Learning More by Seeing Less: Structure-First Learning for Efficient, Transferable, and Human-Aligned Vision", "abstract": "Despite remarkable progress in computer vision, modern recognition systems remain fundamentally limited by their dependence on rich, redundant visual inputs. In contrast, humans can effortlessly understand sparse, minimal representations like line drawings, suggesting that structure, rather than appearance, underlies efficient visual understanding. In this work, we propose a novel structure-first learning paradigm that uses line drawings as an initial training modality to induce more compact and generalizable visual representations. We demonstrate that models trained with this approach develop a stronger shape bias, more focused attention, and greater data efficiency across classification, detection, and segmentation tasks. Notably, these models also exhibit lower intrinsic dimensionality, requiring significantly fewer principal components to capture representational variance, which mirrors observations of low-dimensional, efficient representations in the human brain. Beyond performance improvements, structure-first learning produces more compressible representations, enabling better distillation into lightweight student models. Students distilled from teachers trained on line drawings consistently outperform those trained from color-supervised teachers, highlighting the benefits of structurally compact knowledge. Together, our results support the view that structure-first visual learning fosters efficiency, generalization, and human-aligned inductive biases, offering a simple yet powerful strategy for building more robust and adaptable vision systems.", "tldr": "By training vision models on structure before color photographs, we create more efficient, robust, and human-aligned systems that prioritize structural understanding over superficial appearance.", "keywords": ["Structure-first learning", "Line drawings", "Data efficiency", "Generalization", "Human-aligned vision", "Shape bias", "Sparse representations", "Knowledge distillation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68f6371dbe0ba69e517f7e8bbf941afcd586b624.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents a structure-first learning paradigm that uses line drawing as the initial training modality to induce more compact and generalizable visual representations. The approach is motivated from the perspective of how humans learn, and the paper argues that structure is a key element that should be focused on instead of relying on unstable color or texture. The paper investigates several task where a model is first trained on line drawing and later trained on the color images. Experiments investigate the attention of the model, data efficiency, and the intrinsic dimensionality of the representations."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A nicely structured and cleanly written manuscript.\n2. Nicely presented figures and tables.\n3. An interesting topic with high relevance to the computer vision domain."}, "weaknesses": {"value": "1. The experimental evaluation is limited.\n\n(a) The proposed approach of training on line drawings before training the color images is compared with training solely on the color images. But it is widely established that pretraining is highly beneficial in computer vision [1, 2, 6], and a range of method exists to do this kind of pretraining. Most notably, self-supervised learning methods have shown great performance and is almost the standard choice to perform pretraining in computer vision [1, 2, 3]. And if the focus here is more on curriculum learning, that is also a field with a wide range of established methods [5, 6]. The core issue here is that without any comparison to relevant baselines, it is unclear if the propose structure-first learning scheme that is beneficial or just pretraining.\n\n(b) Furthermore, the evaluation of the attention of the model is poorly motivated and unclear. The qualitative experiments in Figure 5 is not enough to support the claims in the paper, and the quantitative results in the supplementary are also unclear. There are already established protocols for evaluating such heatmaps [8], which would give much more insight into the potential benefit on the proposed training paradigm.\n\n(c) Then, the description of the experimental setup is not detailed and it is unclear how many of the experiments are setup. My understanding of the experiments is that a model is first trained on the line drawings and the then trained further on the color images. That is compared to a model trained from scratch on the color images. If that understanding is correct, the baselines will have trained for a shorter time than the proposed methods. Looking at Figure 8, it seems like performance is still improving, and it is unclear what will happen if all models are left to complete the training. In light of [7], this seems to resemble many of the findings on curriculum learning and its potential benefits.\n\n(d) Lastly, performance should be evaluated across numerous training runs and statistical analysis for significance should be performed.\n\n2. Related work does not focus on relevant literature. The 3 paragraphs in the related work described prior works related to \"Line Drawing Synthesis and Structural Representation\", \"Methods to Improve Shape Bias\", and \"Sparsity in Neural Representations\". But the core idea of this paper is representation learning a method for pretraining. In my opinion, the related work should be focused on all the recent advances in representation learning through e.g. self-supervised learning. That would also allow the reader to understand the position of the paper and what would be natural baselines to compare it with. Also, curriculum learning should be discussed and described in the related work.\n\n- [1] Chen et al., A Simple Framework for Contrastive Learning of Visual Representations, ICML 2020\n- [2] He et al., Masked Autoencoders Are Scalable Vision Learners, CVPR, 2022\n- [3] Assran et al., Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture, CVPR, 2023\n- [4] Vincent et al., Extracting and composing robust features with denoising autoencoders, ICML 2008\n- [5] Wang et al., EfficientTrain++: Generalized Curriculum Learning for Efficient Visual Backbone Training, TMPAMI, 2024.\n- [6] Soviany et al., Curriculum Learning: A Survey, IJCV 2022\n- [7] Wu et al., WHEN DO CURRICULA WORK?, ICLR 2021\n- [8] Hedström et al., Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond, JMLR 2023"}, "questions": {"value": "1. How does the structure-first paradigm compare to standard self-supervised pretraining methods such as contrastive learning [1], masked modeling [2], joint predictive architectures [3], or just simply autoencoder pretraining [4]?\n2. How does the structure-first paradigm compare to established curriculum learning algorithms [6], for example like [5].\n3. Explain how the findings in this work should be interpreted in light of [7]?\n4. How does a quantitative comparison scores of the attention maps look with more established metrics like presented in [8]?\n\n- [1] Chen et al., A Simple Framework for Contrastive Learning of Visual Representations, ICML 2020\n- [2] He et al., Masked Autoencoders Are Scalable Vision Learners, CVPR, 2022\n- [3] Assran et al., Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture, CVPR, 2023\n- [4] Vincent et al., Extracting and composing robust features with denoising autoencoders, ICML 2008\n- [5] Wang et al., EfficientTrain++: Generalized Curriculum Learning for Efficient Visual Backbone Training, TMPAMI, 2024.\n- [6] Soviany et al., Curriculum Learning: A Survey, IJCV 2022\n- [7] Wu et al., WHEN DO CURRICULA WORK?, ICLR 2021\n- [8] Hedström et al., Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond, JMLR 2023"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nwMGXj0Ol7", "forum": "t5BFYQLCby", "replyto": "t5BFYQLCby", "signatures": ["ICLR.cc/2026/Conference/Submission11475/Reviewer_4TRq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11475/Reviewer_4TRq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761246399247, "cdate": 1761246399247, "tmdate": 1762922582256, "mdate": 1762922582256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a supervised training strategy for computer vision backbones, where a two-stage curriculum is employed: models are first trained on line drawings (and combined with other artistic-style augmented images) and then fine-tuned on natural color images. The exploration of training strategies for deep learning models is a valuable and worthwhile research direction.\n\nHowever, the paper provides limited insight. The experimental evaluation focuses mainly on a single dataset (STL-10) and a few model architectures, offering no precise guidance for applying the approach to other datasets or multi-dataset scenarios. It also remains unclear what would happen if line drawings, various artistic-style augmented images, and natural color images were all mixed from the start of training, or whether this could outperform the staged curriculum proposed in the paper."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The exploration of training strategies for deep learning models is a valuable and worthwhile research direction. The paper presents a supervised, two-stage training strategy in which models are first trained on line drawings (optionally combined with other artistic-style augmented images) and then fine-tuned on natural color images, aiming to contribute to the study of effective training strategies for deep learning models."}, "weaknesses": {"value": "1. The study evaluates only a small number of model architectures and datasets, primarily focusing on STL-10. There is no comprehensive analysis or testing across larger-scale datasets, multi-dataset scenarios, or more different backbones.\n\n2. The paper does not provide concrete recommendations on how to optimally set the proportion, mode, or usage of line drawings for different datasets or tasks. While the authors mainly focus on the relatively simple STL-10 or ImageNet-1K dataset, which has much lower resolution and scale compared to modern annotated datasets, the proposed training strategies are unlikely to directly generalize to more complex real-world scenarios. Practitioners are left without actionable guidance for applying this approach beyond the tested settings.\n\n3. Since the main idea of the paper is to train models first using line drawings, the method relies on converting RGB images to sketches. However, only a limited set of sketch generation styles (fewer than five) is tested, and the paper does not provide a clear or thorough analysis of how different line drawing styles, ranging from highly abstract and simplified sketches to realistic sketches with shading, affect training performance, nor does it explore the reasons behind these differences.\n\n4. Modern training pipelines typically employ extensive data augmentation. The paper does not investigate the potential impact of training models directly with a mixture of line drawings, various artistic-style augmented images, and natural color images throughout the entire training process, leaving it unclear whether such an approach could outperform the proposed staged curriculum. Specifically, Tables 1, 2, and 3 are limited, as they do not examine the simpler and more practical scenario of training directly with a mixture of line drawings and color images. As a result, these experiments do not justify the conclusion that the staged training strategy proposed by the authors is the optimal approach."}, "questions": {"value": "1. Analyze the proposed training strategy on larger-scale datasets, multi-dataset scenarios, or other model architectures.\n\n2. Analyze how different sketch generation styles, from highly abstract to realistic sketches with shading, affect training outcomes, and explain the reasons behind these effects.\n\n3. What are the results of directly training with a mixture of line drawings, artistic-style augmentations, and natural color images throughout the entire training process?\n\n4. It is recommended that the authors provide concrete, generalizable guidance for applying the proposed “structure-first learning paradigm” in future versions, for example, specifying how many and which sketch styles are likely to yield the best results.\n\nMinor issue: The GradCAM visualization examples (e.g. Figure 5) provided in the paper (only two) are insufficient."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RDrkbfKA9u", "forum": "t5BFYQLCby", "replyto": "t5BFYQLCby", "signatures": ["ICLR.cc/2026/Conference/Submission11475/Reviewer_hFtH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11475/Reviewer_hFtH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569486469, "cdate": 1761569486469, "tmdate": 1762922581509, "mdate": 1762922581509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a structure-first learning paradigm inspired by the idea that humans can recognize objects from sparse representations such as line drawings.\nThe method is extremely simple:\n\t1.\tPretrain a vision model on line drawings derived from natural images (using Chan et al., 2022),\n\t2.\tThen fine-tune it on color photographs.\nThe authors claim this curriculum fosters shape-biased, data-efficient, low-dimensional, and “human-aligned” visual representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Simple, reproducible idea with consistently positive effects across diverse tasks.\n- Broad experimental coverage: classification, detection, segmentation, distillation, and attention analysis.\n- Clean presentation and statistical rigor (Wilcoxon tests, multiple architectures).\n- Paper is well-written with strong visualizations of representational effects."}, "weaknesses": {"value": "- Small-scale Training: Most experiments use STL-10 or ADE20K with small backbones (ResNet-18, MobileNetV1, VGG-8).\n- No comparison with alternative regularization strategies (e.g., self-supervised pretraining, low-pass filters, sparse autoencoders) to isolate the “structure-first” effect.\n- Overinterpretation of analysis. Claims such as “lower intrinsic dimensionality implies brain-like efficiency” or “focused Grad-CAMs mean human-like attention” are speculative and unconvincing."}, "questions": {"value": "- I think the major concern I had is this paper demonstrates potential usefulness of structural data on small architecture and well-known dataset, but the true learning of human vison is more self-supervised rather than supervised. Therefore, it is not convincing to me to discuss any human-inspired insights. Is it possible for author to pretrain a MAE over structural data and finetune on RGB images?\n- How do you disentangle curriculum effects from simply adding a different augmentation phase?\n- Can the gains be reproduced at scale (e.g., with ImageNet-21K or COCO)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JSFuIlVW8r", "forum": "t5BFYQLCby", "replyto": "t5BFYQLCby", "signatures": ["ICLR.cc/2026/Conference/Submission11475/Reviewer_FQxU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11475/Reviewer_FQxU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884837364, "cdate": 1761884837364, "tmdate": 1762922580908, "mdate": 1762922580908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the effectiveness model pre-training on line drawings in improving performance on downstream tasks such as image classification, semantic segmentation and object detection as well as its influence on the model's shape bias, feature attention and data efficiency. Instead of training the model directly on color images, the paper proposes first pre-training the model on line drawings of the same images obtained by a drawing synthesis method (Chan et al.) before fine-tuning it on color images. \n\nThrough experiments on STL-10 dataset, the paper shows that the two-phase training pipeline brings performance gains in image classification as well as tranfer learning improvements in semantic segmentation and object detection. The paper also provides analysis showing that the obtainded model has more shape bias and produces sparser, more compact features."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides concrete evidences showing that pre-training models on line drawings could bring performance gains in several downstream tasks.\n\nThe paper provides some analysis on the feature space of models pre-trained line drawings, showing some difference in terms of feature sparsity and compactness compared to baselines."}, "weaknesses": {"value": "The paper does not bring much in terms of novelty, either in the methodological approach or the obtained insights. The paper confirms that pre-training on line drawings helps but this is hardly a surprising result, given much evidence in the literature on the benefits of pre-training and the work of Geirhos et al. that shows the gains when training on texture-modified images. \n\nThe experimental results provided in the paper are limited. Most experiments are done on the STL-10 dataset, which is quite small and does not a large enough set of visual concepts. The analysis on feature sparsity in Figure 6 and Figure 10 give some interesting statistics but appear weak. \n\nMany claims in the paper are only loosely reflected by the evidences:\n- L.176 \"This suggests that features derived from line drawings are more generalizable\": This statement is not reflected by the results. From the first two rows in Table 1, the model trained on color images gives better performance on average, and the last two rows suggest that the model needs to be trained on color images in the final stage to yield good performance. Results from Table 1 simply shows that pre-training on line drawings is helpful.\n- L. 290 \"This pattern indicates that strong responses are concentrated among a small subset of neurons\": The average activation of all channels is at least half of the max activation. It is a stretch to say that these channels are not activated.\n- Sec 4.8: The stronger distilled student could simply come from the fact that the teacher is stronger. The link between stronger students and feature compactness is very loose."}, "questions": {"value": "In Figures 3, 4 and 9, when discussing data effiency of \"color only\" and \"line-color\" pipelines, is it guaranteed that the \"color only\" pipeline has the same compute budget has the \"line-color\" pipeline? Since \"line-color\" pipeline has a pre-training phase, it is not clear if it trains models for more iterations than the \"color only \" pipeline.\n\nA similar question for Table 1 when comparing the two pipelines, I wonder if better performance of \"line-color\" simply comes from more traning time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vK5LFOMfFm", "forum": "t5BFYQLCby", "replyto": "t5BFYQLCby", "signatures": ["ICLR.cc/2026/Conference/Submission11475/Reviewer_m8Db"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11475/Reviewer_m8Db"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916088597, "cdate": 1761916088597, "tmdate": 1762922578924, "mdate": 1762922578924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}