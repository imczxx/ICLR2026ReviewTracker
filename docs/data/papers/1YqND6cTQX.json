{"id": "1YqND6cTQX", "number": 19175, "cdate": 1758294127407, "mdate": 1759897054134, "content": {"title": "Personalization Under Value Conflict: Resolving Contradictory Preferences with Paired Fine-Tuning", "abstract": "Large language models (LLMs) are increasingly expected to capture not only broadly shared human universal values but also the diverse and often contradictory preferences of individual users. Existing alignment approaches typically optimize for a single preference direction, making them unsuitable when users switch between opposing values. We propose \\textbf{Preference-Paired Fine-Tuning (PFT)}, a framework that trains models on paired contradictory preferences, enabling a single model to align with both sides simultaneously. Beyond handling one preference pair, PFT generalizes to multiple mutually exclusive preference dimensions, capturing shared structures across conflicts. With only a few in-context examples from user history, PFT further enables rapid and data-efficient customization, yielding stronger alignment to individual preferences. Experiments show that PFT achieves up to $\\textbf{96.7\\% }$classification accuracy, improves open-ended generation scores by $\\textbf{up to 20.05\\%}$, and reduces data requirements by about $\\textbf{40\\%}$ compared to single-preference fine-tuning. These results highlight a scalable path toward conflict-aware and personalized LLMs.", "tldr": "We present a new dataset and training paradigm for aligning LLMs with diverse and even contradictory individual preferences, providing a step toward one model that can adapt to all preferences under value conflict.", "keywords": ["Preference confict", "Personalization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/710abef7863807e1eab75e59527cc552ea54a9a7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Humans often adjust their preferences when facing different situations, which causes models trained on a single preference to underperform. In this work, the authors propose **Preference-Paired Fine-Tuning (PFT)**, which optimizes the models to align contradictory preferences under the same scenario simultaneously. They present a dataset called **Value Conflict Dilemma (VCD)** that consists of contradictory paired examples, and apply supervised fine-tuning on it. The experimental results indicate that PFT performs better than all single-preference baselines for both positive and negative preferences."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation for aligning the models for both contradictory preferences at the same time is well-defined.\n- The inclusion of human studies is appreciated, as it strengthens the evaluation."}, "weaknesses": {"value": "**About the method**\n\n- In Section 3.5, the authors mention that the PFT-trained model can quickly adapt to an ICL scenario, but do not explain why. The training process itself does not appear to include any ICL-related design.\n\n**About the experiments**\n\n- The ICL experiments in Section 4.3 are not clearly explained. For example, the source of the user history data is unclear, and it is not specified which model was used. Without these details, the results are difficult to interpret.\n- In line 418, the authors claim that single-preference trained models cannot generalize well to other preference types without giving the supporting evidence.\n\n**About the writing**\n\n- The use of the synchronous update method for the main approach is mentioned only in the captions of Tables 1 and 2. This should be clearly stated in the main text to distinguish it from the baselines.\n- Human evaluation is mentioned in line 387 but is only described in the appendix, making it hard to follow.\n- Figure 5 appears to contain duplicated bars."}, "questions": {"value": "- Does the improvement mainly come from using contradictory preferences, or simply from sampling multiple preferences within the same scenario? Have the authors tested a dataset built with independent preferences and fine-tuned the models on it?\n- For the single-preference baselines, do they use the same number of training examples (1000), or does the main method actually use 1000 *pairs* of examples?\n- The negative preference results seem consistently lower than the positive ones. Since both types of preferences should be neutral with respect to human values, what might explain this difference?\n- This framework appears adaptable to RLHF-based methods such as PPO or GRPO. Have the authors explored or considered experiments in that direction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RjrzVlOJDw", "forum": "1YqND6cTQX", "replyto": "1YqND6cTQX", "signatures": ["ICLR.cc/2026/Conference/Submission19175/Reviewer_HNWA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19175/Reviewer_HNWA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640423012, "cdate": 1761640423012, "tmdate": 1762931180960, "mdate": 1762931180960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies personalization when a single user can toggle between contradictory preferences. It introduces Preference-Paired Fine-Tuning (PFT): train with both sides of each pair via either alternating updates or a summed loss, then steer at inference with ~3 in-context examples. The paper also releases a synthetic Value Conflict Dilemma (VCD) dataset and evaluates on VCD and selected tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper presents a clear problem framing of value conflict alignment and a simple recipe that plugs into SFT/DPO pipelines.\n\n2. VCD dataset could be potentially useful as it defines explicit contradictory pairs and is human-checked for label quality."}, "weaknesses": {"value": "1. **Baselines for multi-objective control are missing:** Several directly comparable multi-objective alignment/controlled generation methods, e.g., [1,2,3], are not compared. These methods also aim to train one steerable policy that trades off conflicting objectives during inference.\n\n2. **Limited technical contribution:** PFT is essentially cross-entropy on both sides of a pair (either alternated or summed), with standard gradients and weights. The proposed fast personalization path is simply a user context-conditioned generation by adding user history to the prompt, which has already been explored in several inference-time steering methods, e.g., [2,3].\n\n3. **Scalability and robustness:** The paper studies k=2 (binary contradictory), but many user preferences are multi-dimensional and non-exclusive. The method’s effectiveness to scale to >2 dimensions or interactively changing preferences is unknown.\n\n[1] Zhou, Zhanhui, et al. \"Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization.\" Findings of the Association for Computational Linguistics ACL 2024. 2024.\n\n[2] Wang, Kaiwen, et al. \"Conditional Language Policy: A General Framework For Steerable Multi-Objective Finetuning.\" Findings of the Association for Computational Linguistics: EMNLP 2024. 2024.\n\n[3] Guo, Yiju, et al. \"Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment.\" Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 2024."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jKdw3c9hYg", "forum": "1YqND6cTQX", "replyto": "1YqND6cTQX", "signatures": ["ICLR.cc/2026/Conference/Submission19175/Reviewer_8vgH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19175/Reviewer_8vgH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960151759, "cdate": 1761960151759, "tmdate": 1762931180310, "mdate": 1762931180310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Preference-Paired Fine-Tuning (PFT), a method that trains a single LLM on paired demonstrations of contradictory preferences to enable dynamic personalization under value conflict. It introduces the Value Conflict Dilemma (VCD) dataset and shows that PFT outperforms baselines in both classification and open-ended generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe work tackles the important and underexplored challenge of personalizing LLMs when user preferences conflict.\n2.\tIt introduces VCD, a high-quality, human-validated dataset that supports future research on value conflicts.\n3.\tThe paper is well-written and easy to read."}, "weaknesses": {"value": "1.\tDespite introducing the terms “asynchronous” and “synchronous” update strategies, the method is essentially standard supervised fine-tuning on preference-conditioned paired data and offers limited novelty.\n2.\tThe paper models preferences as strict binary opposites, whereas real-world preferences often exist on a spectrum or are contextually blended, limiting the framework’s applicability to nuanced user behaviors.\n3.\tThe evaluation primarily focuses on the single-dimensional VCD benchmark, lacking assessment in multi-dimensional or finer-grained preference settings, which limits the validation of the method’s generalizability.\n4.\tThe paper primarily compares against general alignment methods (e.g., SFT, DPO, CAA) but omits comparisons with recent specialized personalization techniques. This limits the assessment of PFT’s relative advantages in the broader landscape of personalized LLMs."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k8r9yaH6ix", "forum": "1YqND6cTQX", "replyto": "1YqND6cTQX", "signatures": ["ICLR.cc/2026/Conference/Submission19175/Reviewer_Jmge"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19175/Reviewer_Jmge"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986869604, "cdate": 1761986869604, "tmdate": 1762931179938, "mdate": 1762931179938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge of aligning LLMs with heterogeneous and contradictory user preferences. The paper proposes Preference-Paired Fine-Tuning (PFT), a method that fine-tunes a single model on both sides of a contradictory preference pair, enabling the model to handle opposing preferences without requiring separate models for each preference direction. The paper additionally introduces the Value Conflict Dilemma dataset and show that PFT can combine paired training with lightweight in-context adaptation to better match individual preference histories."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Well-motivated problem. The paper addresses an important limitation in current LLM alignment approaches: most methods optimize for universal preferences rather than handling individual-level preference diversity and conflicts.\n- Comprehensive experiments. Tests on multiple model sizes and families (Qwen, LLaMA), multiple baselines (SFT, DPO, CAA), multi-format evaluation (multi-choice classification with “pick-one” and “select-all-that-apply” protocols, and open-ended generation scored by GPT-4o-mini), as well as ablations on dataset size and preference-pair combinations.\n- New Dataset. VCD focuses specifically on value-conflict scenarios and includes human validation. Even if synthetic, the attention to contradictory labeling could be useful."}, "weaknesses": {"value": "- Methodological Novelty. The core idea is essentially training on both sides of a preference pair simultaneously. This is a relatively incremental modification to standard SFT. The mathematical formulation (especially the synchronous update in Eq. 5-7) is just standard multi-task learning with weighted losses\n- Strength of Claims vs. Results. Some narrative framing seems overstated relative to the reported improvements. For example, several gains in the tables are modest, and certain baselines (e.g., DPO in single-preference directions) outperform PFT in their own setting. The claim that PFT “significantly” improves open-ended generation would benefit from a more tempered interpretation.\n- Missing Critical Operational Detail. Several important methodological details are missing or insufficiently described in the main text. For example, regarding multi-choice evaluation, the main text does not explain how model outputs are converted into selected choices, nor how generation is constrained for the “All” setting. Additionally, given that VCD is positioned as one of the principal contributions, the main text provides only high-level construction details. \n- Unclear How Explanations are Used or How Important They Are. Although explanations are repeatedly emphasized as part of the single-choice training data (“triplet of <scenario, preference, explanation>”), the paper does not run any experiments isolating the impact of these generated explanations. Their actual contribution remains unclear."}, "questions": {"value": "- What is the empirical impact of including generated explanations during training or inference? Since explanations appear prominently in the data pipeline, an ablation (e.g., training with vs. without explanations) seems necessary to understand their influence.\n- How exactly is the user-history context constructed for ICL? Are histories sampled directly from the training distribution, synthesized in a principled way, or drawn from held-out samples?\n- Why are there duplicate entries in Figure 5? \n- The abstract mentions a ~40% reduction in data requirements compared to single-preference fine-tuning, and the conclusion similarly claims that the method is more “data-efficient than SFT and DPO.” However, none of the reported results in the main text seem to justify these numbers. Could you clarify how this number was computed and how the experiments support this conclusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "62ByEGV2Ls", "forum": "1YqND6cTQX", "replyto": "1YqND6cTQX", "signatures": ["ICLR.cc/2026/Conference/Submission19175/Reviewer_dRzQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19175/Reviewer_dRzQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762737822344, "cdate": 1762737822344, "tmdate": 1762931178489, "mdate": 1762931178489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}