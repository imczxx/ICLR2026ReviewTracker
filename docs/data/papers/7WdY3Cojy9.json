{"id": "7WdY3Cojy9", "number": 7668, "cdate": 1758031275323, "mdate": 1763699354215, "content": {"title": "FRABench and UFEval: Unified Fine-grained Evaluation with Task and Aspect Generalization", "abstract": "Evaluating open-ended outputs of Multimodal Large Language Models has become a bottleneck as model capabilities, task diversity, and modality rapidly expand. Existing ``MLLM-as-a-Judge'' evaluators, though promising, remain constrained to specific tasks and aspects (i.e., specific evaluation criteria such as fluency for text and image quality for images). In this paper, we argue that, on one hand, based on the interconnected nature of criteria, learning specific aspects can generalize to unseen aspects; on the other hand, jointly learning to assess multiple visual criteria and tasks may foster a synergistic effect. To this end, we propose UFEval, the first unified fine-grained evaluator with task and aspect generalization for four evaluation tasks --- Natural Language Generation, Image Understanding, Image Generation, and Interleaved Text-and-Image Generation. However, training such a unified evaluator is hindered by the lack of a large-scale, multi-modal, and aspect-level resource. To address this gap, we introduce FRABench, a comprehensive fine-grained evaluation dataset. Specifically, (1) We first construct a hierarchical aspect taxonomy encompassing 112 distinct aspects across the aforementioned four tasks. (2) Based on this taxonomy, we create FRABench, comprising 60.4k pairwise samples with 325k evaluation labels obtained from a combination of human and GPT-4o annotations. (3) Finally, leveraging FRABench, we develop UFEval, a unified fine-grained evaluator. Experiments show that learning on specific aspects enables UFEval to generalize to unseen aspects, and joint learning to assess diverse visual tasks and aspects can lead to substantial mutual benefits.", "tldr": "", "keywords": ["Aspect-level Evaluation Dataset", "Unified Fine-grained Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a9ba3f89b59a508a2cebdc2d69b7ff9df8c9e4e1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes UFEval, a unified fine-grained evaluator designed to achieve task and aspect generalization for MLLMs. To support this, the authors build a hierarchical taxonomy with 112 distinct aspects across four task types, and based on this, construct FRABench, a large-scale, multi-modal, aspect-level evaluation benchmark, obtained through a mix of human and GPT-4o annotations. They then train UFEval on FRABench and evaluate its generalization ability through in-domain and out-of-domain experiments, demonstrate UFEval’s strong performance as an evaluator compared with baselines, and explore its application in preference alignment for both image understanding and image generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work introduces UFEval, the first unified fine-grained evaluator designed with both task and aspect generalization capabilities. It covers four diverse evaluation tasks: Natural Language Generation (NLG), Image Understanding (IU), Image Generation (IG), and Interleaved Text-Image Generation (ITIG). Complementary to this, the paper constructs FRABench, a large-scale, multi-modal, and aspect-level comprehensive evaluation dataset, which directly addresses the critical lack of resources required for training such a universal evaluator.\n\n2. The trained UFEval evaluator demonstrates good performance across various benchmarks.\n\n3. The study shows a substantial amount of work and extensive experiments."}, "weaknesses": {"value": "1. While the paper proposes an ambitious taxonomy covering 112 distinct aspects, there are potential weaknesses in how this taxonomy is constructed. The authors note that they directly adopted existing aspect tree structures from previous studies and incorporated additional aspects \"based on their definitions\". However, the methodology for reconciling differences or conflicts between these source taxonomies is not clearly articulated. Moreover, the paper provides no evidence of expert validation, inter-rater agreement, or any form of consistency checking to support the robustness of the taxonomy.\n\n2. The inclusion of 112 aspects also raises concerns about semantic redundancy. Many aspects seem to overlap substantially in meaning, making them difficult to distinguish in practice. For example:\n\n   - Accuracy and Instruction Following are strongly correlated: when a model accurately completes a task, it typically also follows instructions, and deviations from instructions often result in inaccuracy.\n\n   - Creativity-related aspects such as Appeal, Engagingness, and Creativity appear to measure very similar qualities and may not represent truly distinct evaluative dimensions.\n\n   The paper argues that the FRA-OOD test set contains \"unseen task-specific aspects\" and uses this as evidence of UFEval's generalization ability. However, if these \"unseen\" aspects are semantically overlapping or near-synonymous with training aspects, the evaluator may not be demonstrating genuine generalization. Instead, it may simply be transferring learned evaluation criteria to synonymous labels, which is conceptually weaker than true out-of-domain generalization. This issue undermines the validity of the claimed aspect-level generalization. \n\n3. The paper’s core claim is that jointly learning to assess multiple visual aspects and tasks leads to synergistic effects. To support this, the authors compare models trained on a single task (e.g., “w/ IU”) with the multi-task model (\"Ours\"), and report performance improvements in Tables 4 and 5. However, the \"Ours\" model is trained on a larger and more diverse dataset than the other ablations. This makes it difficult to disentangle whether the observed gains actually stem from cross-task synergy or simply from exposure to more varied training data. Without a controlled experiment where the total training data volume is held constant across conditions, the evidence provided does not conclusively support the claimed synergistic effect. A data-controlled ablation would strengthen the argument for true multi-task synergy."}, "questions": {"value": "1. What specific methodology was used to reconcile conflicts between the source taxonomies?\n\n2. What empirical evidence (e.g., expert validation) can be provided to support the robustness and distinctness of the 112 aspects?\n\n3. Given the high semantic overlap among many aspects, how can the authors prove that the performance on \"unseen\" aspects represents true generalization to novel concepts, and not just inference on synonymous labels?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K3H93DBIWD", "forum": "7WdY3Cojy9", "replyto": "7WdY3Cojy9", "signatures": ["ICLR.cc/2026/Conference/Submission7668/Reviewer_vDBt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7668/Reviewer_vDBt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761379757026, "cdate": 1761379757026, "tmdate": 1762919735076, "mdate": 1762919735076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenge of evaluating open-ended outputs from MLLMs. The authors argue that existing \"MLLM-as-a-Judge\" evaluators are too narrowly focused on specific tasks and aspects, limiting their generalizability. To overcome this, they propose two main contributions: 1) FRABench, a large-scale, fine-grained evaluation benchmark covering four major multimodal tasks (NLG, IU, IG, ITIG) across 28 sub-tasks, and 2) UFEval, a unified evaluator model trained on FRABench. The core hypotheses are that learning to evaluate diverse tasks and aspects fosters synergistic benefits, and that evaluators can generalize to unseen tasks and aspects due to their inherent semantic connections. The paper presents a comprehensive hierarchical taxonomy of 112 evaluation aspects, which forms the foundation of FRABench's 60.4k pairwise samples. Experiments demonstrate that UFEval achieves strong performance on out-of-domain tasks and aspects, is competitive with specialized evaluators on public benchmarks, and can be used to generate high-quality preference data for downstream model alignment via DPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a well-recognized bottleneck in MLLM research. As model capabilities expand, the need for scalable, reliable, and nuanced evaluation frameworks is paramount. The authors correctly identify the limitations of current approaches and propose a compelling, unified vision for evaluation. \n2. The development of FRABench is a major contribution in its own right. Creating a benchmark of this scale (60.4k pairs, 325k labels) that is multi-task, multi-modal, and fine-grained is a significant undertaking. The hierarchical aspect taxonomy is also a valuable conceptual contribution, providing a structured and comprehensive framework for thinking about MLLM quality that could be widely adopted.\n3. The paper presents a strong set of results. UFEval consistently outperforms strong baselines on out-of-domain generalization, particularly on the human-annotated sets. Furthermore, it achieves competitive, and sometimes superior, performance against state-of-the-art, specialized evaluators (e.g., Themis, LLaVA-Critic) on established public benchmarks, despite being a single, unified model."}, "weaknesses": {"value": "1. This is the most significant concern regarding the methodology. A substantial portion of the 325k training labels are generated by GPT-4o. While the authors use human-annotated data for testing, the training data's quality is fundamental to the final model's performance. The paper's validity rests on the assumption that GPT-4o is a sufficiently reliable and unbiased annotator for 112 diverse aspects.\n2. While the resulting aspect tree is a strength, the process of its creation is described somewhat briefly. The paper mentions adapting existing structures and manually organizing the rest based on definitions. This process can be highly subjective.\n3. The paper excels at presenting strong aggregate performance metrics. However, a deeper understanding would be gained from analyzing where UFEval fails. A qualitative analysis comparing UFEval's judgments to human judgments, especially in cases of disagreement, would be highly insightful.\n4. There are few baseline comparisons, and qwen2-vl is already a year-old model. Authors should compare more recent large multimodal models and experiment with newer models"}, "questions": {"value": "1. In Table 1, the number of aspects for each evaluator is listed. For instance, AUTO-J is listed with 332. Are these aspects comparable in scope and granularity to the 112 aspects in your taxonomy? A direct numerical comparison might be misleading if the definition of an \"aspect\" differs significantly across studies. A clarifying footnote could be helpful.\n2. The base model for UFEval is Qwen2-VL-7B. How much of the observed performance gain is attributable to the FRABench data versus the inherent strengths of this particular base model? A useful baseline might be to fine-tune Qwen2-VL-7B on a single-task, single-aspect dataset (e.g., the data used for Themis) and compare its performance on that specific task against UFEval. This would help further isolate the benefits of the unified training approach.\n3. Regarding the GPT-4o annotations for UAs , you mention providing only the response to avoid bias from query-correctness. This is a very thoughtful design choice. Did you explore or observe any other biases in the GPT-4o annotations, and did you employ any other strategies to mitigate them during data creation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JLCHuzu7zM", "forum": "7WdY3Cojy9", "replyto": "7WdY3Cojy9", "signatures": ["ICLR.cc/2026/Conference/Submission7668/Reviewer_B3KA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7668/Reviewer_B3KA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551433551, "cdate": 1761551433551, "tmdate": 1762919734638, "mdate": 1762919734638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced a benchmark / evaluation dataset called FRABench for measuring so-called \"aspects\" for text, image generation and image understanding. Such aspects can be coherence, grammaticality, semantic consistency, etc. Also, it introduces UFEval, an multimodal LLM as a judge for such tasks and aspects, built by fine-tuning Qwen2-VL-7B-Instruct on the new FRABench dataset."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "* FRABench is a substantial contribution: 60.4k pairwise samples and 325k aspect-level labels covering 112 distinct aspects organized in a clear hierarchical taxonomy."}, "weaknesses": {"value": "* Vague terminology and presentation: The paper repeatedly uses the term “aspects” in the abstract and introduction without defining it or providing concrete examples. This makes it difficult for the reader to understand what exactly is being evaluated until much later (around page 4). Introducing a brief definition or illustrative examples early on would substantially improve readability and accessibility.\n\n* Although the dataset includes some human labels, most of the 325k aspect-level annotations are GPT-4o-generated which raises the question of bias dna weakness propagation.\n\n* UFEval is built only on Qwen2-VL-7B-Instruct, which raises concerns about how much the findings depend on that particular architecture. How do the findings generalise for other backbones, such as LlaVA, mplug-owl-3, etc?\n\n* it would have been useful to have a qualitative analysis of failure modes.\n\n* Generalisation tests: While UFEval is evaluated on several external benchmarks (e.g., Winoground, Pick-a-Pic), all of its training and main “aspect generalization” analyses are conducted within the FRABench framework. Since both training and out-of-domain splits rely on the same hierarchical taxonomy of aspects, it remains somewhat unclear how well UFEval’s claimed aspect-level generalization would hold under a completely independent fine-grained evaluation scheme."}, "questions": {"value": "L016-017 there is so much repetitions of the word \"aspects\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lTzKtnp11V", "forum": "7WdY3Cojy9", "replyto": "7WdY3Cojy9", "signatures": ["ICLR.cc/2026/Conference/Submission7668/Reviewer_CEqP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7668/Reviewer_CEqP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836313750, "cdate": 1761836313750, "tmdate": 1762919734371, "mdate": 1762919734371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors have developed a new evaluation dataset, FRABench, and used it to train the UFEval model, a unified evaluator of multimodal LLMs (MLLMs) open ended responses.\n\nTheir FRABench evaluation dataset contains 112 hierarchical, universal and task-specific eval aspects related to major MLLM tasks. The dataset contains 60,400 sample pairs and 325,000 evaluation labels, created using both human reviewers and GPT-4.0.\n\nThe authors argue that because these evaluation aspects are interconnected, it leads to better performance and enable the model to generalize to unseen aspects."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- They created a comprehensive, fine-grained dataset for MLLM tasks.\n- The paper is well written, with detailed experiments, ablation studies, and comparisons on several public benchmarks.\n- They show that FRABench can be used to fine-tune smaller 7B models, improving their performance to match larger 72B+ models for MLLMs tasks\n- They show that UFEval can be used to automatically generate high-quality preference data."}, "weaknesses": {"value": "- Heavy reliance on GPT-4o annotations. It is not shown how much they correlate with human labels. \n- UFEval still underperform relative to the larger models."}, "questions": {"value": "Could FRABench be used for retrieval, providing few-shot examples to improve the performance of bigger models via in-context learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v5YaX9EFH3", "forum": "7WdY3Cojy9", "replyto": "7WdY3Cojy9", "signatures": ["ICLR.cc/2026/Conference/Submission7668/Reviewer_1uwD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7668/Reviewer_1uwD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7668/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954257457, "cdate": 1761954257457, "tmdate": 1762919733993, "mdate": 1762919733993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Major Changes"}, "comment": {"value": "The following change log summarizes the key modifications made during the author-reviewer discussion period.\n\n**Nov. 21th**\n\nThe main differences are summarized as follows:\n\n**1. Elaboration on Aspect Tree Construction**\n\nIn Section 3.1.2, we have added detailed information explaining the construction process of the aspect tree. (Addressing Reviewers **vDBt** & **B3KA**)\n\n**2. Distinction of Generalization Categories**\n\nIn Section 4.2, we have explicitly distinguished between \"Contextual Generalization\" and \"Novel Aspect Generalization\" to clarify our evaluation logic. (Addressing Reviewer **vDBt**)\n\n**3. Empirical Evidence for Aspects (Robustness & Distinctness)**\n\nIn Appendix E, we have provided specific IAA scores and correlation heatmaps to validate the robustness and distinctness of our taxonomy. (Addressing Reviewer **vDBt**)\n\n**4. Integration of Failure Case Analysis**\n\nIn Section 4.2, we have moved the failure case analysis from the Appendix to the main text to provide deeper insights into model limitations. (Addressing Reviewers **B3KA** & **CEqP**)\n\n**5. Inclusion of New Baselines**\n\nIn Section 4.3, we have incorporated the latest baselines, including Qwen3-VL, CIGEval, and Q-Eval, for a more comprehensive comparison. (Addressing Reviewer **B3KA**)\n\n**6. Clarification on Table 1 Comparability**\n\nIn Section 1 (Table 1), we have added a note clarifying the data comparability across different evaluators. (Addressing Reviewer **B3KA**)\n\n**7. Definition of 'Aspect'**\n\nIn the Abstract, we have provided a clear definition of the term \"Aspect.\" (Addressing Reviewer **CEqP**)\n\n**8. Scope of Findings**\n\nIn the Limitation, we have explicitly stated that our findings are primarily verified within the Qwen series models. (Addressing Reviewer **CEqP**)"}}, "id": "VWLtTu19b0", "forum": "7WdY3Cojy9", "replyto": "7WdY3Cojy9", "signatures": ["ICLR.cc/2026/Conference/Submission7668/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7668/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission7668/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763705771812, "cdate": 1763705771812, "tmdate": 1763705771812, "mdate": 1763705771812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}