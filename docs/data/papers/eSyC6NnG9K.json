{"id": "eSyC6NnG9K", "number": 267, "cdate": 1756732916974, "mdate": 1759898269908, "content": {"title": "LG-Bench: A Graph-Structured Evaluation Benchmark for Life Sciences", "abstract": "Traditional evaluation benchmarks reduce inherently interconnected scientific knowledge in life sciences into flat lists of questions, disregarding the underlying topological structure of the knowledge. We introduce LG-Bench, the first graph-structured benchmark for life sciences, featuring over 10,000 high-quality multiple-choice questions across medicine, biology, and chemistry. Our approach constructs a weighted evaluation graph using bidirectional matching and semantic similarity algorithms, where nodes represent questions and edge weights capture their semantic relationships. Leveraging this graph topology, we design two novel evaluation metrics. The Global Coherence Score (GCS) measures a model’s consistency within semantically related neighborhoods, while Knowledge Balance Score (KBS) analyzes how model errors are distributed across the graph to reveal conceptual blind spots. LG-Bench facilitates fine-grained comparison of LLMs by surfacing differences in conceptual coherence and patterns of knowledge organization across models. Our framework shifts the evaluation paradigm from flat accuracy metrics to structure-aware analysis, offering a new lens for diagnosing and improving LLM performance in the life sciences domain.", "tldr": "", "keywords": ["Large Language Models", "Benchmark", "Life Sciences", "Graph"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/adbec14fcc1f691c1507bb5ac21f5d5391a8261b.pdf", "supplementary_material": "/attachment/e8bcef18a51c8c5756d712b37aaebb3fed4999f4.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces LG-Bench, that contains over 10,000 high quality multi-choice questions capturing topoplogical structure of knowledge in the domain of biology, chemistry and medicine. The authors further introduce GCS and KBS to track performance across knoweldge clusters, providing deeper insights of performance on the knowledge distribution rather than plainly reporting accuracy by treating each individual questions as flat and independent entities. Evaluation and Results demonstrate that large-sized closed-sourced models perform quite well in preserving knowlegde within a single and across different domains. Further insights reveal that fine-tuning alters the knowledge landscape by re-adjusting the weights."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well written with proper flow and clear/readable diagrams.\n- Experiments conducted are well-rounded and covered all bases. Interesting plots have been demonstrated highlighting the key impact of the evaluation metric. Overall the need for the proposed evaluation metric is motivated well.\n- The convolution of evaluation over a knowledge graph is novel and can inspire future research to gauge models on different levels of topology."}, "weaknesses": {"value": "- The weighted accuracy w(v,u) in line 269 has not been explained properly.\n- The motivation behind the design choices made towards stage-2 of the benchmark construction pipeline (Graph construction) is not provided. Were there other keyword representation you tried? How does the breakdown and computing semantic similarity over the keyword beneficial? How many keywords were extracted per question and what was their granularity? Moreover, what was the reason for choosing a weighted combination of bi-directional similarity and core-similarity? It would be great if you can demonstrate some ablation studies to justify the design choices made."}, "questions": {"value": "1) What speciaized LLM was used for Guided Question Generation (line 154)?\n2) How were the hyper-parameters decided? Was it a theoritical choice or empirically based?\n3) Can you share the prompts used for Guided Question Generation and Multi-Model Committee Assessment?\n4) How many questions were passed on from Multi-Model Assessment to expert human verification? What was the portion of questions were retained from the committee check and what portion was introduce after expert verification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z6uu4Lr1zC", "forum": "eSyC6NnG9K", "replyto": "eSyC6NnG9K", "signatures": ["ICLR.cc/2026/Conference/Submission267/Reviewer_f3N5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission267/Reviewer_f3N5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760956228650, "cdate": 1760956228650, "tmdate": 1762915481464, "mdate": 1762915481464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LG-Bench, a novel, large-scale, graph-structured evaluation benchmark for LLMs in the life sciences. It comprises over 10,000 multiple-choice questions derived from recent scientific literature. The benchmark transforms this corpus into a weighted evaluation graph and introduces two new graph-based metrics—the Global Coherence Score (GCS) and Knowledge Balance Score (KBS), designed to measure structured understanding and conceptual balance beyond standard accuracy metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "## 1. High Novelty and Vision\n\n- The move from flat-list evaluation to a graph-structured framework is original and timely, addressing a known limitation in LLM evaluation.\n\n## 2. Sophisticated Data Pipeline\n\n- The multi-stage generation and filtering pipeline—combining multiple LLMs and expert review—is impressive, though the scope of human validation remains unclear.\n\n## 3. Potential Diagnostic Insight\n\n- **GCS** and **KBS** are conceptually appealing and could become useful diagnostic measures if shown to align with human judgments of coherence."}, "weaknesses": {"value": "## 1. Graph Construction Sensitivity and Excessive Density \n\n- Average degree of ~586 suggests over-connectedness, risking that GCS/KBS reflect lexical similarity rather than meaningful conceptual coherence.\n- Sensitivity to hyperparameters (γ, θ, κ) is not shown.\n- **Require sensitivity analysis and justification for graph density.**\n\n## 2. Lack of Human Validation for Metrics \n\n- No human evaluation demonstrates that GCS/KBS correlate with expert assessments of model coherence.\n- **A correlation study with expert judgment is essential.**\n\n## 3. KBS Justification and Baseline Comparison \n\n- The KBS amplification factor (α = 100) is arbitrary and unvalidated.\n- Missing comparisons to simpler graph-aware baselines or knowledge-graph (KG) reasoning benchmarks.\n- **Include ablation and KG-based comparisons (e.g., “\"A scalable llm framework for therapeutic biomarker discovery: Grounding q/a generation in knowledge graphs and literature.\" ICLR 2025 Workshop on Machine Learning for Genomics Explorations. 2025.”).**\n\n## 4. Circularity, Reproducibility,\n\n- The same or similar LLMs are used for graph construction and evaluation, risking architectural bias.\n- Missing implementation details (γ, θ, κ, embeddings, prompts).\n- **Include full hyperparameters, model details, responsible release plan, and a formal ethics/biosecurity statement.**"}, "questions": {"value": "## 1. Validation Scope\n\n- Did all 10,000 questions receive human validation, or was this a subset?\n- What proportion were verified by experts versus filtered automatically by LLMs?\n\n## 2. Expert Agreement\n\n- Was inter-rater agreement measured among the four PhD reviewers?\n- If not, how consistent were their judgments?\n\n## 3. Graph Density Justification\n\n- Why is such a high connectivity (avg. degree ≈ 586) appropriate for a domain-knowledge graph?\n- What happens to GCS/KBS if the threshold θ or parameter γ is adjusted to produce sparser graphs?\n\n## 4. Metric Interpretation\n\n- How do **GCS** and **KBS** correlate with accuracy or with human-perceived coherence?\n- Is there evidence that a higher GCS reflects better reasoning rather than answer clustering?\n\n## 5. KBS Amplification (α)\n\n- Why was α = 100 chosen?\n- How sensitive are results to this factor?\n- Would normalization or unamplified variance produce comparable results?\n\n## 6. Knowledge Graph Baseline\n\n- How does LG-Bench differ empirically from a traditional knowledge-graph-based evaluation (e.g., using DrugBank or Hetionet relations)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ki4avCo8OU", "forum": "eSyC6NnG9K", "replyto": "eSyC6NnG9K", "signatures": ["ICLR.cc/2026/Conference/Submission267/Reviewer_WF9d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission267/Reviewer_WF9d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761039937099, "cdate": 1761039937099, "tmdate": 1762915481354, "mdate": 1762915481354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response (1)"}, "comment": {"value": "All three reviewers had questions about the density comparison and parameter selection of the graph structure. To address these concerns, we conducted ablation experiments with different parameter ($\\theta$ and $\\gamma$) to demonstrate the robustness of our approach. \n\nWe observe that GCS remains essentially stable across settings, indicating that global coherence is not an artifact of density. KBS magnitudes change almost multiplicatively with sparsity, which is expected given its variance-like nature, reducing edge counts inflates the variance term, but the rank ordering of models is consistent across nearly all subdomains and in the overall benchmark. Crucially, all conclusions about domain-specific fine-tuning remain unchanged. These results suggest that our graph is appropriate for diagnosing both local and global competencies, while our metrics are robust to reasonable changes in connectivity. \n\nEach table is set with different $\\theta$. For each $\\theta$, the parameters GCS, KBS, and node degree are reported using $\\gamma$ = 0.4 ($\\gamma$ = 0.6) to indicate the comparison between $\\gamma$ values. For example, GCS 14.04 (14.37) means that GCS is 14.04 when $\\gamma$ = 0.4 and 14.37 when $\\gamma$ = 0.6.\n\n1. When $\\theta = 0.45$, Avg. Degree = 249.61 (257.42).\n\n|      MODEL      |       |   MEDICINE    |               |       |    BIOLOGY    |               |       |   CHEMISTRY   |               |       |    OVERALL    |               |\n| :-------------: | :---: | :-----------: | :-----------: | :---: | :-----------: | :-----------: | :---: | :-----------: | :-----------: | :---: | :-----------: | :-----------: |\n|                 |  Acc  |      GCS      |      KBS      |  Acc  |      GCS      |      KBS      |  Acc  |      GCS      |      KBS      |  Acc  |      GCS      |      KBS      |\n|   Qwen2.5-7B    | 78.38 | 62.59 (62.05) | 60.38 (61.25) | 80.03 | 64.29 (64.72) | 40.12 (40.89) | 76.76 | 59.29 (59.81) | 48.07 (48.94) | 78.43 | 62.20 (62.68) | 29.83 (30.56) |\n|   Llama-3-8B    | 72.77 | 54.41 (54.86) | 69.05 (70.01) | 75.67 | 57.68 (58.15) | 32.90 (33.72) | 72.53 | 53.21 (53.74) | 52.91 (53.85) | 73.73 | 55.25 (55.78) | 29.14 (29.96) |\n| Llama3-Med42-8B | 78.10 | 61.79 (62.31) | 55.59 (55.52) | 77.38 | 60.17 (60.64) | 35.06 (35.91) | 75.00 | 56.82 (57.38) | 55.66 (56.73) | 76.82 | 59.59 (60.12) | 27.72 (28.45) |\n|   Qwen2.5-14B   | 83.02 | 69.84 (70.29) | 43.47 (44.38) | 85.26 | 72.75 (73.21) | 36.85 (37.69) | 82.62 | 68.81 (69.35) | 38.89 (39.82) | 83.69 | 70.56 (71.09) | 22.18 (22.91) |\n|   Qwen2.5-32B   | 84.70 | 72.79 (73.26) | 44.72 (45.67) | 86.06 | 74.41 (74.94) | 29.19 (30.03) | 83.75 | 70.58 (71.13) | 38.19 (39.14) | 84.87 | 72.78 (73.32) | 21.94 (22.68) |\n|  Llama-3.1-70B  | 82.77 | 69.74 (70.18) | 48.59 (49.51) | 83.14 | 69.25 (69.76) | 38.64 (39.58) | 81.67 | 66.95 (67.49) | 45.48 (46.42) | 82.54 | 68.75 (69.28) | 25.85 (26.67) |\n|   Qwen2.5-72B   | 86.26 | 75.37 (75.83) | 38.76 (39.68) | 86.97 | 75.76 (76.28) | 29.24 (30.16) | 84.85 | 72.67 (73.21) | 41.59 (42.54) | 86.05 | 74.73 (75.27) | 20.73 (21.55) |\n\n2. When  $\\theta = 0.5$, Avg. Degree = 110.75 (119.17).\n\n|      MODEL      |       |   MEDICINE    |                 |       |    BIOLOGY    |                |       |   CHEMISTRY   |                 |       |    OVERALL    |               |\n| :-------------: | :---: | :-----------: | :-------------: | :---: | :-----------: | :------------: | :---: | :-----------: | :-------------: | :---: | :-----------: | :-----------: |\n|                 |  Acc  |      GCS      |       KBS       |  Acc  |      GCS      |      KBS       |  Acc  |      GCS      |       KBS       |  Acc  |      GCS      |      KBS      |\n|   Qwen2.5-7B    | 78.38 | 62.63 (63.12) | 124.92 (127.43) | 80.03 | 64.11 (64.58) | 77.40 (79.62)  | 76.78 | 59.29 (59.78) |  89.46 (91.77)  | 78.43 | 62.23 (62.71) | 64.70 (66.21) |\n|   Llama-3-8B    | 72.77 | 54.40 (54.89) | 142.21 (145.09) | 75.67 | 57.50 (57.96) | 61.07 (63.29)  | 72.55 | 53.16 (53.67) | 99.42 (101.82)  | 73.73 | 55.32 (55.83) | 63.26 (65.35) |\n| Llama3-Med42-8B | 78.10 | 61.78 (62.28) | 117.57 (119.93) | 77.38 | 60.14 (60.62) | 63.90 (66.27)  | 75.02 | 56.87 (57.38) | 104.30 (106.81) | 76.82 | 59.64 (60.15) | 58.67 (60.47) |\n|   Qwen2.5-14B   | 83.02 | 69.64 (70.14) |  95.62 (97.81)  | 85.26 | 72.70 (73.18) | 60.96 (63.11)  | 82.64 | 68.92 (69.44) |  71.85 (74.07)  | 83.69 | 70.60 (71.11) | 44.11 (46.00) |\n|   Qwen2.5-32B   | 84.70 | 72.59 (73.09) | 101.22 (103.50) | 86.06 | 74.37 (74.87) | 57.84 (59.92)  | 83.77 | 70.63 (71.15) |  71.71 (71.70)  | 84.87 | 72.81 (73.33) | 45.99 (47.81) |\n|  Llama-3.1-70B  | 82.77 | 69.62 (70.11) | 106.73 (109.12) | 83.14 | 69.20 (69.71) | 77.25 (79.43)  | 81.69 | 67.01 (67.54) |  83.49 (85.32)  | 82.54 | 68.81 (69.34) | 53.04 (55.41) |\n|   Qwen2.5-72B   | 86.26 | 75.14 (75.64) |  89.02 (91.17)  | 86.97 | 75.71 (76.21) | 57.89 (60.03)  | 84.88 | 72.77 (73.30) |  77.55 (79.23)  | 86.05 | 74.78 (75.31) | 43.04 (45.21) |"}}, "id": "aISt576SJv", "forum": "eSyC6NnG9K", "replyto": "eSyC6NnG9K", "signatures": ["ICLR.cc/2026/Conference/Submission267/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission267/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission267/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763137382326, "cdate": 1763137382326, "tmdate": 1763137382326, "mdate": 1763137382326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LG-Bench, a novel graph-structured benchmark for evaluating large language models in the life sciences. The authors argue that traditional \"flat list\" benchmarks fail to test an LLM's understanding of interconnected scientific concepts.\n\nThey construct the benchmark questions by creating questions from peer-reviewed scientific papers, which serve as the graph's nodes, and then connecting these nodes with weighted edges that represent the semantic similarity between the questions.\n\nThis graph structure is then used for evaluation. The paper introduces two new metrics that go beyond simple accuracy:\n\n-Global Coherence Score (GCS): Measures a model's consistency by assessing whether it correctly answers clusters of related questions.\n\n-Knowledge Balance Score (KBS): Analyzes the distribution of a model's errors to identify \"conceptual blind spots\" or uneven knowledge.\n\nThe authors show that these metrics can reveal insights into model performance, such as the trade-offs of domain-specific fine-tuning, that traditional accuracy scores cannot."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clear thesis and focus, as well as well-articulated findings\n* As far as I am aware, their graph proposal is a novel concept for a life sciences LLM benchmark, in using graphs to identify more fine-grained nuances in LLM life sciences \n* Logical argument for testing the quality of data"}, "weaknesses": {"value": "* Not thorough in assessing substitute and alternative approaches\n* The authors do not describe the detailed effects (quantitatively, fine-grained breakdown) of the expert human review. There can be valuable insights here for the community (for example regarding patterns in LLM hallucination in question'"}, "questions": {"value": "Can you study alternatives more deeply?\n\nWhat is the percentage of questions that were rejected or rewritten by the human experts? What are the total number of questions generated by the AI before the human validation stage?\n\nWhile this appears to be a useful, unique benchmark, it is often the follow-up questions (and follow-ups to those follow-ups etc) that more akin to real life science work. Could you extend your method to evaluate this in any way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zkc1iZ6C8x", "forum": "eSyC6NnG9K", "replyto": "eSyC6NnG9K", "signatures": ["ICLR.cc/2026/Conference/Submission267/Reviewer_zDqV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission267/Reviewer_zDqV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963650373, "cdate": 1761963650373, "tmdate": 1762915481035, "mdate": 1762915481035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}