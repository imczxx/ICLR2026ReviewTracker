{"id": "ei1bRG971A", "number": 8392, "cdate": 1758081244141, "mdate": 1763558266834, "content": {"title": "DND: Boosting Large Language Models with Dynamic Nested Depth", "abstract": "We introduce Dynamic Nested Depth (DND), a novel method that improves performance for off-the-shelf LLMs by selecting critical tokens to reprocess in a nested depth manner. Specifically, at the end of the given transformer layer, DND identifies more critical tokens with a router and feeds them back for an extra round of processing, effectively \"reviewing\" difficult tokens while avoiding redundant computation for easier ones. The dynamic selection mechanism is tailored for precise control via two novel strategies: a router controlling loss to enhance token selection distinguishability, and a threshold control scheme to ensure selection stability. We demonstrate the effectiveness of DND by directly integrating it into pre-trained dense and MoE models during a post-training phase. On diverse benchmarks, DND boosts the performances of the dense Qwen3-1.7B, Llama3.2-1B, and Gemma3-1B by 1.88%, 2.61%, and 2.50% and the MoE Qwen3-30B-A3B by 0.87%, all with a minimal parameter and computing increase.", "tldr": "We introduce Dynamic Nested Depth (DND), an efficient paradigm that adaptively identifies critical tokens and selectively deepens their computation via nested re-processing.", "keywords": ["Large Language Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/86ecb965b4ab6854f6aabf9755ed18b7c3daa6ea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a new heuristic method using token-conditional dynamic depth, applied post-training for improving accuracy at a low additional training and inference cost. The paper is aimed at LLM system engineers and inference optimization researchers, people who modify or extend pretrained transformer architectures to trade off accuracy and compute efficiency.\n\n- Introduces Dynamic Nested Depth (DND): a router selects “hard” tokens, which are reprocessed once through the same transformer block; the outputs are fused with the original states via a normalized gate.\n- Adds a router-control loss (to keep scores separable yet non-saturated) and a threshold-control loop (buffer proportional control + EMA) to stabilize the selected-token ratio.\n- Applies DND to mid-layers of Qwen3-1.7B (dense) and Qwen3-30B-A3B (MoE) models; reports modest average accuracy gains with ~6 % FLOPs overhead and negligible parameter growth (< 0.1 M)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Simple, reproducible architecture with minimal parameter overhead.\n- Clear ablation analysis separating router and threshold controls.\n- Good methodological transparency (layer range, selection ratios, FLOPs estimate)."}, "weaknesses": {"value": "- Small performance improvements, both in an absolute sense and relative to known model variants. Even fine tuning or just random variation might yield performance improvements similar to those shown in the paper.\n- Weak experimental support: only within-model deltas, no compute-matched baselines, and no wall-clock profiling.\n- Incremental and heuristic: conceptually similar to existing adaptive-depth ideas (MoD, MoR, ITT, early-exit) and lacks deeper theoretical insight."}, "questions": {"value": "- Clarify whether base model weights were frozen or lightly fine-tuned during DND training.\n- Include compute-matched comparisons with other adaptive-compute methods.\n- Discuss failure modes—cases where DND reduces accuracy or increases instability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8CIRDgMWvB", "forum": "ei1bRG971A", "replyto": "ei1bRG971A", "signatures": ["ICLR.cc/2026/Conference/Submission8392/Reviewer_i7d3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8392/Reviewer_i7d3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932216734, "cdate": 1761932216734, "tmdate": 1762920296215, "mdate": 1762920296215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method called Dynamic Nested Depth (DND) which adaptively identifies “difficult” tokens for layer in an LLM and allocates extra computation to those tokens by re‑processing them through the same layer in a nested way, while easier tokens receive standard processing. This token‑level routing is managed by a lightweight router that assigns each token a probability of being selected using hidden state of each token in the seqeunce; if the score exceeds a threshold, the token undergoes a “nested” pass. Once processed, the outputs of the first pass and the subsequent passes are merged together before moving to the next layer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem formulation is compelling, as allocating additional computation to difficult tokens could improve model accuracy at the cost of additional compute.\n - Experiments are adequate and includes evaluations across diverse benchmarks (knowledge, reasoning, coding).\n - Analysis is insightful, with clear plots and ablations that illustrate how token-level nested depth impacts performance."}, "weaknesses": {"value": "- The reported accuracy improvement over regular SFT is only 0.87 on average in table 1, which appears minimal. The proposed method involves processing the input multiple times for some tokens in the given layers. Such process would increase the compute overhead. But this table does not include the computational overhead introduced to bring the 0.87 avg accuracy improvement. This makes the experimental results incomplete.\n -  Experiments are conducted on only a single LLM (Qwen), raising concerns about the generalizability of the method to other architectures.\n\n-  Based on the method description and experiments, it is unclear whether DND fine-tunes all components of the transformer blocks in the selected layers (L_s to L_e) or if it fine-tunes all model parameters like standard SFT. \n\n- The router-level losses and their underlying motivation are not clearly explained. Moreover, their overall contribution appears minimal, as reflected by the similar values in Table 2 (60.54, 60.58, 60.64) where these losses are not included.\n\n - Overall, the approach seems largely heuristic and does not demonstrate a substantial improvement in accuracy."}, "questions": {"value": "Based on the method description and experiments, it is unclear whether DND fine-tunes all components of the transformer blocks in the selected layers (L_s to L_e) or if it fine-tunes all model parameters like standard SFT."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qqBIyQDVWn", "forum": "ei1bRG971A", "replyto": "ei1bRG971A", "signatures": ["ICLR.cc/2026/Conference/Submission8392/Reviewer_o89C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8392/Reviewer_o89C"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953128799, "cdate": 1761953128799, "tmdate": 1762920295360, "mdate": 1762920295360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dynamic Nested Depth (DND) — an efficient method to enhance pre-trained LLMs by selectively deepening computation. Instead of uniformly processing all tokens, DND identifies critical tokens that require more reasoning effort. Within each transformer layer, a lightweight router marks these important tokens after a standard forward pass. Only those tokens undergo an additional \"nested\" pass through the same layer, and their outputs are then fused with the initial results. DND can be applied in post-training (e.g., fine-tuning), making it compatible with existing models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: While adaptive computation is an established research area, DND mechanism is a novel implementation. Instead of skipping layers or routing to different experts, the idea of re-processing a selected subset of tokens through the same transformer layer (\"nested pass\") is an elegant formulation, reminding me of recent looped Transformers. Furthermore, the decision to apply this technique during a post-training (SFT) phase is a highly original and pragmatic choice.\n\nS2: Thorough Empirical Evaluation: The experiments are comprehensive and convincing. The method is validated on two different model scales and architectures (a dense 1.7B and a sparse MoE 30B model), and tested across 17 diverse benchmarks. The performance gains, especially in complex reasoning domains like coding and mathematics, are significant and consistently positive.\n\nS3: The significance of this work is substantial, as it addresses one of the most pressing challenges in the development of LLMs: the trade-off between performance and computational cost."}, "weaknesses": {"value": "W1: While this paper presents a promising direction,the paper does not quantitatively demonstrate why the selected tokens are \"critical.\" The visualizations could be the result of cherry-picking or confirmation bias. It is unclear if the router has learned a meaningful selection strategy or is simply responding to surface-level statistical patterns. To strengthen this claim, the authors should perform a quantitative analysis correlating token selection with intrinsic properties of the tokens or the model's state. \n\nFor instance: Correlation with Model Uncertainty: Are the selected tokens ones for which the model has high prediction entropy or low softmax probability in the initial pass? This would suggest DND is focusing computation on \"hard\" decisions.\n\nW2: The paper claims a \"minimal parameter and computing increase,\" basing its efficiency analysis on a theoretical calculation of FLOPs.  FLOPs are a poor proxy for actual inference speed (latency and throughput) on GPUs. The DND architecture, with its Pack and Unpack operations, breaks the massive parallelism that GPUs are optimized for, potentially leading to significant latency increases that are not captured by FLOP counts. They should report: Wall-clock latency (ms per generated token) and throughput (tokens per second) on a standard benchmark GPU (e.g., A100 or H100)."}, "questions": {"value": "Q1: All experiments are conducted on a single family of models (Qwen3). While the results are positive, this makes it impossible to know if the DND approach is a general-purpose technique or if its effectiveness relies on specific, undocumented properties of the Qwen architecture."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VBvN9Z90Wj", "forum": "ei1bRG971A", "replyto": "ei1bRG971A", "signatures": ["ICLR.cc/2026/Conference/Submission8392/Reviewer_pkVU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8392/Reviewer_pkVU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995779555, "cdate": 1761995779555, "tmdate": 1762920294811, "mdate": 1762920294811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}