{"id": "S1Z5YmN4mV", "number": 20374, "cdate": 1758305265287, "mdate": 1759896981254, "content": {"title": "Metanetworks as Regulatory Operators: Learning to Edit for Requirement Compliance", "abstract": "As machine learning models are increasingly deployed in high-stakes settings, e.g. as decision support systems  in various societal sectors or in critical infrastructure, designers and auditors are facing the need to ensure that models satisfy a wider variety of requirements (e.g. compliance with regulations, fairness, computational constraints) beyond performance. Although most of them are the subject of ongoing studies, typical approaches face critical challenges:  post-processing methods tend to compromise performance, which is often counteracted by fine-tuning or, worse, training from scratch, an often time-consuming or even unavailable strategy. This raises the following question: \"Can we efficiently edit models to satisfy requirements, without sacrificing their utility?\" In this work, we approach this with a unifying framework, in a data-driven manner, i.e. we learn to edit neural networks (NNs), where the editor is an NN itself - a graph metanetwork - and editing amounts to a single inference step. In particular, the metanetwork is trained on NN populations to minimise an objective consisting of two terms: the requirement to be enforced and the preservation of the NN's utility. We experiment with diverse tasks (the data minimisation principle, bias mitigation and weight pruning) improving the trade-offs between performance, requirement satisfaction and time efficiency compared to popular post-processing or re-training alternatives.", "tldr": "A method for learned neural network post-hoc editing using metanetworks, for compliance with sociotechnical and other requirements.", "keywords": ["weight space learning", "neural network editing", "sociotechnical ai"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9e9b07170486a3a33eaf905afa1501cd30d1067f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Paper proposes a graph metanetwork that edits trained MLPs in one forward pass to satisfy requirements (data minimization, equalized-odds fairness, pruning) while preserving behavior. It outputs masks and residuals, trained over model families with JSD + requirement losses, yielding faster, better Pareto trade-offs on Adult/Bank."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Motivation is well-argued and the paper is clearly written, with a crisp problem setup and method description.\n\n2. Covers three compliance needs—data minimization, equalized-odds fairness, and pruning—with promising Pareto trade-offs and extremely fast inference (~0.03s per edit).\n\n3. Accommodates different parameter shapes by operating on a parameter graph, rather than assuming a fixed MLP architecture."}, "weaknesses": {"value": "1. For each dataset, each requirement, and each weighting coefficient, a separate model must be designed and trained.\n\n2. Given the data-hungry training (needing many trainings to construct weight samples) and poor cross-dataset generalization, the claim of “no costly retraining” seems hard to stand.\n\n3. The datasets are extremely limited and very low-dimensional, which makes me worry about applicability to higher-dimensional real-world settings."}, "questions": {"value": "I’d like to know how it performs on higher-dimensional datasets, and how much the final results depend on the number of pre-trained weight samples (weight snapshots) used for training？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8zk1x0oip4", "forum": "S1Z5YmN4mV", "replyto": "S1Z5YmN4mV", "signatures": ["ICLR.cc/2026/Conference/Submission20374/Reviewer_eYmo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20374/Reviewer_eYmo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761403532628, "cdate": 1761403532628, "tmdate": 1762933826884, "mdate": 1762933826884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of ensuring ML models comply with evolving requirements such as fairness, privacy, and efficiency without retraining or performance degradation. It identifies that existing post-processing and fine-tuning approaches are inefficient or limited to specific applications, highlighting a gap in flexible, general-purpose model editing. \nTo address this, the authors propose a graph metanetwork that learns to edit other neural networks’ parameters directly, performing requirement-compliant modifications in a single inference step. The framework formulates requirement compliance as a multi-objective optimization problem balancing performance preservation and regulatory constraints, trained in a data-driven and symmetry-aware manner. Experiments across data minimization, bias mitigation, and pruning show the method achieves better trade-offs between performance, compliance, and computational efficiency than retraining or post-processing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe motivation is strong and well-timed, the paper tackles a pressing issue in trustworthy AI, how to make models meet regulatory and ethical requirements post-deployment. \n2.\tThe metanetwork architecture is innovative: using an equivariant GNN to operate in weight space is a cutting-edge idea that leverages recent advances in meta-learning and symmetry-preserving design.\n3.\tExperiments are thoughtfully structured across diverse requirements (fairness, data minimization, pruning), which convincingly demonstrate versatility rather than one task."}, "weaknesses": {"value": "1.\tThe scope of experiments is limited to MLPs and tabular data. This makes it unclear how well the method scales to large models (e.g., Transformers, diffusion, or other large models) or structured modalities such as text and vision.\n2.\tThe assumption of white-box access to model weights may restrict applicability in auditing or proprietary systems, where only API access is available.\n3. The training cost of the metanetwork itself, though amortized at inference, is not deeply analyzed. It remains uncertain how expensive or data-hungry the pre-training process is compared to fine-tuning.\n4. It would be beneficial if further interpretability analysis and error analysis could be involved."}, "questions": {"value": "1. The paper mainly focuses on differentiable objectives, how to consider other requirement types like safety constraints, robustness?\n2. How realistic is this in common auditing or deployment settings, where many models are only available via APIs?\n3. Can the metanetwork generalize across tasks?\n4. How sensitive is performance to the diversity and size of this NN population?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qsiWe8YauA", "forum": "S1Z5YmN4mV", "replyto": "S1Z5YmN4mV", "signatures": ["ICLR.cc/2026/Conference/Submission20374/Reviewer_4ehz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20374/Reviewer_4ehz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895373508, "cdate": 1761895373508, "tmdate": 1762933826441, "mdate": 1762933826441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present an ambitious and, at first glance, quite elegant technical approach. They propose a framework in which a message-passing graph neural network learns to edit other neural networks, making them compliant with new requirements without retraining them from scratch. This is a neat trick: it’s like giving neural networks a \"regulatory layer\" that can adapt them to new goals on the fly.\n\nI’m generally pleased with the conceptual elegance. It’s a clever piece of engineering and a compelling idea. However, there are a few important caveats that need to be highlighted.\n\nFirst, let’s talk about the question of guarantees. The authors imply that their method can be used to achieve various goodmaking properties fairness, efficiency, data minimization, and so on but they sidestep the hardest part of the problem. The guarantees don’t come from the method itself. In fact, the method doesn’t really offer any inherent guarantees at all. It’s up to whoever is translating those goodmaking properties into mathematical form to ensure that the objectives make sense and are actually achievable. The authors effectively offload the burden of proof to someone else: they say, \"Once you’ve done the hard work of turning your requirement into math, then our method applies.\" This is a notable limitation that's worth being explicit about: it’s a tool that requires someone else to do the heavy conceptual lifting, and it's worth avoiding the implication that mathematics alone can buy you these solutions for free.\n\nSecond, the method’s applicability is limited by its data and architecture dependence. It works for fixed architectures, which means if you want to generalize it, you have to retrain it for each new type of network. That’s a practical hurdle that limits the broader usability of the technique. It’s a nice trick, but it’s not a universal one-size-fits-all solution.\n\nFinally, the authors bury certain limitations in footnotes, such as the assumption of a convex Pareto front. In the messy real world, multi-objective optimization often doesn’t yield such neat, convex trade-offs. This is a real limitation that should be more transparently discussed.\n\nIn conclusion, this is a clever and promising piece of work, but it comes with caveats. It’s a tool that needs careful handling and further demonstration of its generality and robustness. The authors have given us a neat technical insight, but they haven’t solved the hardest parts of the problem. That’s fine, just means there’s more work to be done."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Elegant formulation of model editing as a learnable mapping in weight space.\n- Uses graph metanetworks to perform edits in a single inference step: computationally efficient in a sense.\n- Unifies multiple compliance tasks (fairness, pruning, data minimisation) under one objective.\n- Demonstrates consistent Pareto improvements over post-hoc baselines."}, "weaknesses": {"value": "- Depends on fixed architectures; generalization across model families untested.\n- Offloads hardest work theoretical work (formalising requirements) onto the user (may not be a weakness per se)\n- Assumes convex Pareto fronts and differentiable requirement objectives; necessary move perhaps, no shame in just saying \"here is a nice technical trick, let's not worry about guarantees.\"\n- “Regulatory compliance” rhetoric overstates what is actually a tuning heuristic."}, "questions": {"value": "- How sensitive is the learned editor to the distribution of training networks? Does the universe conspire in our favour to make all editors behave roughly the same?\n- Can metanetworks be stacked or composed for multiple simultaneous requirements? Sure there are no theoretical guarantees but does it work in the wild?\n- What safeguards prevent catastrophic edits or hidden performance degradation? Is there a meta-meta-network for that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xmKz8Vsf0K", "forum": "S1Z5YmN4mV", "replyto": "S1Z5YmN4mV", "signatures": ["ICLR.cc/2026/Conference/Submission20374/Reviewer_g7zS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20374/Reviewer_g7zS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918590114, "cdate": 1761918590114, "tmdate": 1762933825659, "mdate": 1762933825659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a metanetwork that edits trained models in a single pass to meet requirements (data minimization, fairness, sparsity), replacing intensive optimization loops. Framing compliance as a learned weight-space mapping is well-motivated."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "It's good to see the evaluation of how robust the edited models remain when the underlying data distribution (pd) shifts."}, "weaknesses": {"value": "The regulatory positioning could be more precise by linking edits to specific AI Act or ISO/IEC risk-management processes. The relation to unlearning is better reframed as complementary rather than alternative, since this method edits parameter rather than removing data influence."}, "questions": {"value": "The paper doesn’t explain how λ is chosen or how sensitive results are to it. Specify the sampling strategy for the Dd subset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vFnIUkkorp", "forum": "S1Z5YmN4mV", "replyto": "S1Z5YmN4mV", "signatures": ["ICLR.cc/2026/Conference/Submission20374/Reviewer_RyXd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20374/Reviewer_RyXd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926039552, "cdate": 1761926039552, "tmdate": 1762933824926, "mdate": 1762933824926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}