{"id": "AB3jM3iMaW", "number": 5791, "cdate": 1757935378157, "mdate": 1759897953045, "content": {"title": "Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning", "abstract": "Forecasting future links is a central task in temporal graph (TG) reasoning, requiring models to leverage historical interactions to predict upcoming ones. Traditional neural approaches, such as temporal graph neural networks, achieve strong performance but lack explainability and cannot be applied to unseen graphs without retraining. Recent studies have begun to explore using large language models (LLMs) for graph reasoning, but most of them are constrained to static graphs or small synthetic TGs and lack the evaluation of the quality of reasoning traces generated by LLMs. In this work, we present Reasoning-Enhanced Learning for Temporal Graphs (ReaL-TG), a reinforcement learning framework that fine-tunes LLMs to perform explainable link forecasting on real-world TGs. ReaL-TG uses outcome-based reward to encourage models to self-explore reasoning strategies from graph structure and to produce explanations that directly justify their predictions. To enable evaluation on LLM-generated reasoning traces, we propose a new evaluation protocol combining ranking metrics with an LLM-as-a-Judge system that assesses both the quality of reasoning and the impact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuning Qwen3-4B under our framework, show that it outperforms much larger frontier LLMs, including GPT-5 mini, on ranking metrics, while producing high-quality explanations confirmed by both the LLM judge and human evaluation.", "tldr": "We present Reasoning-Enhanced Learning for Temporal Graphs (ReaL-TG), a reinforcement learning framework that fine-tunes LLMs to perform explainable link forecasting on real-world TGs", "keywords": ["Temporal Graphs", "Large Language Model", "Reinforcement Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76aaa6c2ca27238d26e9722f167a182a02bdd826.pdf", "supplementary_material": "/attachment/82caeb6301c84b891e752a3d97231867cd8b8fd5.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes an approach for future link prediction in temporal graphs (called link forecasting). It finetunes an LLM (Qwen3-4B) for this task via reinforcement learning. All textual data is removed from the graphs and replaced by IDs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Original idea of using LLMs for link forecasting\n- Strong results compared both to other LLMs and compared to TGNNs\n- Explanation traces are provided and evaluated in a user study\n- Paper is well-structured"}, "weaknesses": {"value": "- The motivation for using an LLM remains unclear (apart from some success in preliminary work)"}, "questions": {"value": "- What is the motivation of using an LLM if all textual data is removed?\n- Do you have a hypothesis why your approach works so well despite removing all textual data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gPDe6J7xe3", "forum": "AB3jM3iMaW", "replyto": "AB3jM3iMaW", "signatures": ["ICLR.cc/2026/Conference/Submission5791/Reviewer_xRzV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5791/Reviewer_xRzV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760992883214, "cdate": 1760992883214, "tmdate": 1762918263953, "mdate": 1762918263953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper constructs temporal graphs in natural language and finetunes LLMs with reinforcement learning on the dataset. The finetuned model outperforms larger LLMs under the proposed pMRR and LLM-as-a-Judge metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents that the reasoning ability of LLMs on temporal graphs can be enhanced by RL training, which is not explored by previous works of LLM on temporal graphs, which mainly focus on evaluation and ICL. The full pipeline, from data construction to training and evaluation, is reasonable and easy to follow."}, "weaknesses": {"value": "The main weakness is a lack of innovation and further insights. The idea that the graph reasoning ability of LLMs can be incentivized through reinforcement learning has been illustrated in previous work. Although the authors claim that previous works focus on static graphs while they target temporal graphs, they don't show what difference it will bring to the reasoning or further insights (e.g., what is special for temporal graphs but not static graphs). In the whole pipeline, the innovation is quite limited. For example, the algorithm is the well-established GRPO, and the LLM-as-Judge evaluation is also commonly used."}, "questions": {"value": "1. The base models only include Qwen3-4B and Qwen3-0.6B. What's the performance of the model finetuned from a larger Qwen3-8B?\n\n2. What's the performance of the ReaL-TG models on OOD benchmarks, e.g., unseen temporal graph benchmarks?\n\n3. Does the training harm or enhance the general reasoning ability of base models, e.g., the math benchmarks?\n\n4. How does the RL training increase the reasoning of LLMs on temporal graphs? Can you show some intuitive examples?\n\n5. The reason for proposing pMRR is to evaluate the over-generation of LLMs. Can the recall in F1 play a similar role?\n\n6. Why don't you use the proposed metrics as rewards during RL training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "usyrZhFgF0", "forum": "AB3jM3iMaW", "replyto": "AB3jM3iMaW", "signatures": ["ICLR.cc/2026/Conference/Submission5791/Reviewer_VHSj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5791/Reviewer_VHSj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761211798465, "cdate": 1761211798465, "tmdate": 1762918263632, "mdate": 1762918263632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Real-TG, which employs reinforcement learning with verified rewards (LRVR) like GRPO to fine-tune an LLM for more effective explainable link forecasting on anonymized temporal graphs. It formulates the link forecasting as the QA tasks and designs a specific prompt to instruct LLMs to identify the destination node. The authors also propose a metric (i.e., pMRR) to evaluate the prediction capability of LLMs (as LLMs can provide discouraging over-generation). Extensive experiments show that Real-TG-4B achieves good results across different vanilla LLMs and traditional link forecasting methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The idea of introducing LRVR to LLM-based temporal graph models is interesting.\n* The proposed fine-tuning paradigm, Real-TG, is simple and effective.\n* The paper is well-written and easy to follow."}, "weaknesses": {"value": "* The novelty of the proposed Real-TG is limited and lacks technical innovation. In Real-TG, the QA formulation [1], the GRPO pipeline [2], the $\\alpha$-temporal random walk [3], and the LLM-as-a-Judge evaluation [4] are introduced from existing works. All these mechanisms resemble existing works and do not introduce principled advances in LLM-based temporal graph learning or RLVR.\n* The motivation of the model component design is unclear. \n\t* Why is RL chosen instead of SFT? In the literature on large reasoning models, RL is typically used to optimize preferences or human-aligned objectives; however, for tasks with a single correct answer, SFT may be more appropriate. The authors should clarify why RL is necessary and what concrete benefits it confers in this setting.\n\t* Why adopt $\\alpha$-temporal random walks rather than simpler k-hop neighborhoods?\n\t* Why use F1 as the reward metric rather than other ranking/IR metrics (e.g., MRR)? In equation 1, do the authors compute this F1 over a single pair of samples, since F1 fundamentally requires aggregation over multiple samples? An illustrative example may help with a better understanding of the paper.\n* The GRPO procedure is known to be sensitive to hyperparameters. Does Real-TG also suffer from such instability?\n* Plots showing reward vs. training step would greatly strengthen confidence in the RL training procedure.\n* Why not conduct experiments in the recently proposed benchmark [5] and compare the models that also utilize LLMs for temporal graph learning [6]?\n\n[1] Are Large Language Models Good Temporal Graph Learners?\n\n[2] DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n\n[3] Zebra: When temporal graph neural networks meet temporal personalized pagerank\n\n[4] Judging Llm-as-a-judge with Mt-bench and Chatbot Arena\n\n[5] DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs\n\n[6] Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models"}, "questions": {"value": "No"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wrn8quIL6N", "forum": "AB3jM3iMaW", "replyto": "AB3jM3iMaW", "signatures": ["ICLR.cc/2026/Conference/Submission5791/Reviewer_W9DC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5791/Reviewer_W9DC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883187684, "cdate": 1761883187684, "tmdate": 1762918263370, "mdate": 1762918263370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles link forecasting on temporal graphs, emphasizing explainable predictions using large language models that generalize to unseen graphs without retraining. It introduces ReaL-TG, a reinforcement learning framework that fine-tunes LLMs with Grouped Regularized Policy Optimization and an outcome-based F1 reward to encourage self-exploration of reasoning strategies from graph structures. Core contributions include the Temporal Context Graph Selection algorithm for relevant subgraph extraction, a QA formulation for the task, and a new evaluation protocol featuring penalized mean reciprocal rank alongside an LLM-as-a-Judge system assessing faithfulness, logical consistency, and answer-explanation alignment. Main results demonstrate that the fine-tuned ReaL-TG-4B model outperforms larger frontier LLMs and traditional temporal graph neural networks on prediction metrics and reasoning quality across seen and unseen datasets from the Temporal Graph Benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces ReaL-TG, a complete RL-based framework designed to fine-tune LLMs for explainable link forecasting on temporal graphs. This is a novel approach to a task where prior methods lacked explainability.\n- The experimental results are impressive. The fine-tuned 4B model (ReaL-TG-4B) significantly outperforms its base model (Qwen3-4B) and, notably, much larger frontier LLMs like Llama3.3-70B and GPT-5 mini on overall prediction accuracy.\n- The framework demonstrates excellent generalization. ReaL-TG-4B achieves the highest MRR and PMRR on the two \"unseen\" TGB datasets, outperforming all baselines.\n- The paper successfully demonstrates that ReaL-TG not only improves prediction accuracy but also reasoning quality."}, "weaknesses": {"value": "- The paper provides an insightful observation of \"reward hacking\" in the smaller ReaL-TG-0.6B model, which \"justifies its predictions by claiming '(uq, vq, tq) has already been seen'\". This is a critical finding for outcome-based RL methods. Despite its importance, this phenomenon is not systematically measured. The paper provides no evidence or quantification to demonstrate it.\n- The approach injects graph context purely as text; while explainable, it may drop important structural cues vs. learned embeddings.\n- Relies solely on six TGB datasets, which may share similar characteristics like social or transaction networks, limiting generalization claims.\n- Training data sampled from only four datasets with 1,000 queries, potentially insufficient for diverse temporal patterns.\n- Unseen datasets (uci, enron) are smaller, with fewer involved nodes, possibly underrepresenting complex scenarios."}, "questions": {"value": "- Your definition in Section 4 states PMRR penalizes over-generation, resulting in a lower score than MRR. However, in Table 2, several large baseline models (e.g., Llama3.3-70B, GPT-5 mini) show a PMRR score that is higher than their MRR score. Could you please confirm if the description of PMRR in Section 4 is correct, or explain the mechanism by which over-generation leads to a higher PMRR score.\n- Could you please briefly elaborate on the path that makes $(e_2, t_2)$ a 3-hop neighbor in the context of Figure 2? \n- Can you clarify how the termination probability $\\alpha$ and the decay factor $\\beta$ interact in the random walk?\n- Why was the F1 score chosen as the reward function? Did F1 prove uniquely robust to this hacking behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kdOvy0FtPO", "forum": "AB3jM3iMaW", "replyto": "AB3jM3iMaW", "signatures": ["ICLR.cc/2026/Conference/Submission5791/Reviewer_N4Ej"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5791/Reviewer_N4Ej"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762788178092, "cdate": 1762788178092, "tmdate": 1762918263056, "mdate": 1762918263056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}