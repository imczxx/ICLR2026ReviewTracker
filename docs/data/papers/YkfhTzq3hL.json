{"id": "YkfhTzq3hL", "number": 25034, "cdate": 1758363378855, "mdate": 1759896737396, "content": {"title": "Hallucination Benchmark for Speech Foundation Models", "abstract": "Hallucinations in automatic speech recognition (ASR) systems refer to fluent and coherent transcriptions produced by neural ASR models that are completely unrelated to the underlying acoustic input (i.e., the speech signal). While similar to conventional decoding errors in potentially compromising the usability of transcriptions for downstream applications, hallucinations can be more detrimental due to their preservation of syntactically and semantically plausible structure. This apparent coherence can mislead subsequent processing stages and introduce serious risks, particularly in critical domains such as healthcare and law. Conventional evaluation metrics are primarily centered on error-based metrics and fail to distinguish between phonetic inaccuracies and hallucinations. Consequently, there is a critical need for new evaluation frameworks that can effectively identify and assess models with a heightened propensity for generating hallucinated content. To this end, we introduce SHALLOW, the first benchmark framework that systematically categorizes and quantifies hallucination phenomena in ASR along four complementary axes: lexical, phonetic, morphological, and semantic. We define targeted metrics within each category to produce interpretable profiles of model behavior. Through evaluation across various architectures and speech domains, we have found that SHALLOW metrics correlate strongly with word error rate (WER) when recognition quality is high (i.e., low WER). \nStill, this correlation weakens substantially as WER increases. SHALLOW, therefore, captures fine-grained error patterns that WER fails to distinguish under degraded and challenging conditions. Our framework supports specific diagnosis of model weaknesses and provides feedback for model improvement beyond what aggregate error rates can offer.", "tldr": "This paper introduces a framework that categorizes ASR hallucinations into 4 categories, namely lexical, phonetic, morphological, and semantic hallucinations, to provide more detailed error analysis beyond standard WER.", "keywords": ["Hallucination", "Automatic Speech Recognition", "SpeechLLM", "Speech Foundation Model", "Benchmark"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eff6b9578c8a7d76e0306e9b6ccfca58dc401dc3.pdf", "supplementary_material": "/attachment/be05564f42cfec91179eb82a0f1a144ea003a266.zip"}, "replies": [{"content": {"summary": {"value": "This paper identifies a lack of standardized methods for systematically categorizing and measuring hallucinations in existing ASR research. To address this gap, it introduces SHALLOW, a benchmark framework that categorizes ASR errors into four complementary dimensions. Through systematic analysis, the authors demonstrate that the proposed four measurements are distinct and mutually complementary. Furthermore, evaluations based on the proposed metric provide a detailed analysis of the critical weaknesses in existing ASR models. Given that previous ASR hallucination studies primarily relied on WER as a key metric, this research offers significant value to the field."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The study effectively identifies and critiques the reliance on aggregate metrics like WER in current evaluation practices, highlighting significant issues in existing ASR hallucination research. The motivation for the experiments is well-structured and the authors address this problem with precision and timeliness.\n- The authors propose the SHALLOW benchmark framework, categorizing and quantifying hallucination phenomena in ASR across four complementary axes: lexical, phonetic, morphological, and semantic. Through several experiments, the authors demonstrated distinct and complementary nature of these axes, validating the proposed classification system.\n- The study conducted robust experiments across a diverse range of models and datasets. The four proposed axes enable a more detailed analysis of ASR models, previously reliant only on WER, allowing for a comprehensive examination of their strengths and weaknesses. The study reveals hallucinations undetectable by WER alone and empirically demonstrates the ability of SHALLOW to uncover them."}, "weaknesses": {"value": "- Some necessary details appear to be omitted in the design of the metric.\n    - The weights in most of the proposed metrics seem to be chosen arbitrarily. Although the authors claim to describe these details in Appendix B, I can hardly find such details. For the proposed metric to serve as a standard for measuring hallucination, I believe the rationale for the coefficients in Section 3 must be clarified.\n    - This issue appears in all four proposed metrics, and clearer justification would be needed.\n        - e.g., Why choose 0.5 for ( r_i ), 0.3 for ( r_s ), and 0.2 for ( r_d ) in Equation 1? How were these weights determined?\n        - e.g., Why use a naive average for calculating PF in Equation 2, while applying weights of 0.4 for SD and 0.6 for GE in Equation 4?\n    - (Section 3.3) How are the errors in grammar (E_Gr), spelling (E_Sp), and punctuation (E_Pu) measured?\n    - (Section 3.3) Which dependency parser was used to measure morphological errors?\n    \n- (Sections 3.3-3.4) When using neural models as metrics, it is crucial to justify their reliability.\n    - For instance, the reliability of an NLI model's accuracy on out-of-distribution test sets must be addressed. The study should explore how this can be trusted.\n    - The reliability of methods like BERTScore also requires comprehensive validation. The authors mentioned using BERT-based models in line 282, but the specific models utilized are not specified. The authors should clarify which models were used and how their reliability was validated.\n    - Moreover, encoder-based models typically assess sentence-level similarity, yet SHALLOW apply them to measure unigram, bigram, and trigram alignment. While not inherently incorrect, this application demands performance validation.\n    - Without such analysis, the measured semantic error might not constitute a meaningful measure."}, "questions": {"value": "- How do the authors define the SHALLOW benchmark framework?\n    - Does the dataset covered by SHALLOW refer to the datasets experimented on in Section 4, or the verification dataset constructed in Section 1?\n    - Without a clearly defined dataset range, SHALLOW might be more appropriately termed a metric rather than a benchmark.\n- Upon examining each metric, the low correlation between WER and the four metrics is well-acknowledged. However, Table 2 shows that models with generally low WER also tend to exhibit low hallucination tendencies. Could WER, therefore, be seen as a comprehensive metric for measuring hallucination? What are the authors' thoughts on this?\n- Why does the SALMONN language model show a WER of 99.92 in Table 2? This appears to differ significantly from results reported in previous studies. Can the authors provide examples of SALMONN's generated outputs and justify such results?\n- Are the performances reported in Table 2 averaged over all datasets evaluated in Table 4? Did the authors use a combined dataset for evaluation?\n- Regarding phonetic fabrication, is the Philips (2000) method for metaphone transformations sufficiently reliable? I acknowledge that several neural models have been proposed for metaphone transformations, surpassing traditional algorithms. How do the authors address potential transcription errors when using this algorithm-based approach and metric?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZdbkcyT27y", "forum": "YkfhTzq3hL", "replyto": "YkfhTzq3hL", "signatures": ["ICLR.cc/2026/Conference/Submission25034/Reviewer_vMKW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25034/Reviewer_vMKW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630144022, "cdate": 1761630144022, "tmdate": 1762943290860, "mdate": 1762943290860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark for ASR evaluation by accounting for lexical, phonetic, morphological and semantic hallucinations. Along these four directions, the authors mathematically formulate quantifications which is called the SHALLOW benchmark. A synthetic dataset is also curated keeping in mind the above four types of errors which further facilitates fine-grained evaluation of ASR systems. Comprehensive evaluations of various ASR models on various datasets using the SHALLOW metrics show the effectiveness of the proposed approach."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The problem tackled is an important one.\n2. The authors have carefully considered how to best evaluate the ASR systems and come up with four dimensions in their benchmark: lexical, phonetic, morphological and semantic.\n3. Novel evaluation metrics are introduced.\n4. Comprehensive evaluations done on different datasets and ASR models."}, "weaknesses": {"value": "1. A lot of important details (like the methodology for the creation of the synthetic dataset) has been delegated to the appendix.\n2. The paper sometimes has clarity issues which should be fixable (see next section)."}, "questions": {"value": "1. The authors mention the creation of the synthetic dataset. It would be good to have some quantitative evaluations of different ASR models on this dataset in the main text.\n2. What is meant by \"metric vectors\" mentioned in line 139-140? Please provide more details in the main text on how these are computed to plot the t-SNE.\n3. In equation (1), the weights on $r_i, r_s$ and $r_d$  should sum to $3.0$ instead of $1.0$ as an analogy to $WER = \\frac{N_i + N_s + N_d}{N} = r_i + r_s + r_d$.\n4. Please provide more details about Hamming distance, Levenshtein distance and Jaro-Winkler similarity in the main text.\n5. How do you automate the computation of $E_{Gr}$, $E_{Sp}$ and $E_{Pu}$ while computing the Morphological errors.\n6. Please explain the difference between semantic distance and semantic coherence. \n7. How are the weights chosen for aggregated semantic error score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6WLW6oWTAE", "forum": "YkfhTzq3hL", "replyto": "YkfhTzq3hL", "signatures": ["ICLR.cc/2026/Conference/Submission25034/Reviewer_iPx2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25034/Reviewer_iPx2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858604832, "cdate": 1761858604832, "tmdate": 1762943290547, "mdate": 1762943290547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SHALLOW, a set of new evaluation metrics for speech recognition beyond word error rate (WER) with a goal of measuring hallucination. The authors define hallucination to be \"fluent and coherent transcriptions produced by neural ASR models that are completely unrelated to the underlying acoustic input\" (line 11-13). They propose to measure hallucination in 4 dimensions: lexical fabrication, phonetic fabrication, morphological errors, semantic errors. By comparing 12 neural ASR models across several datasets, the authors found that SHALLOW correlate closely when WER is low, but the correlation is much lower when the WER is high."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: This paper proposed a novel taxonomy of hallucination by ASR models.\n- Quality: This paper evaluated the proposed metrics on a wide collection of ASR models and test sets in order to demonstrate its characteristics.\n- Clarity: Most parts of this paper is well organized and clearly written.\n- Significance: As WER decreases, it is ever more important to get a more refined understanding of the sources of ASR errors. This paper is an attempt in that direction."}, "weaknesses": {"value": "-   The proposed metrics, while intuitively sensible, are not backed with empirical evidence for their effectiveness in measuring the claimed properties.\n    -   Good metrics (e.g. BLEU, BLEURT, WER) all follow the same recipe:\n        1.  First a grounded cost objective is established: BLEU and WER aim to approximate the cost of human post editing of machine generated text; BLEURT aims to approximate the human judgement of translation quality.\n        2.  Hypotheses with different costs are collected, and scored using the proposed metrics.\n        3.  Correlation between the cost and the proposed metric is used as evidence of the effectiveness of the proposed metric.\n    -   In this paper, while there are reasonably well defined error categories, none of the proposed metrics has been evaluated to check whether they actually accurately measure these categories of errors.\n-   The overall utility of the proposed metrics are not well demonstrated. This paper reports that when WER is low, SHALLOW closely correlates with WER. Only when the WER is high, the correlation decreases. However, SHALLOW makes heavy use of pretrained NLP models, many of which may not be reliable for high WER inputs (due to these being far from typical text seen during training). Thus SHALLOW numbers for high WER inputs might not be as reliable overall as those for low WER inputs. However, in the low WER case, SHALLOW does not appear to differentiate different models well enough."}, "questions": {"value": "I do not have further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cQupMAGqOT", "forum": "YkfhTzq3hL", "replyto": "YkfhTzq3hL", "signatures": ["ICLR.cc/2026/Conference/Submission25034/Reviewer_NAG3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25034/Reviewer_NAG3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974036656, "cdate": 1761974036656, "tmdate": 1762943290156, "mdate": 1762943290156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SHALLOW (Speech HALLucination OvervieW), a benchmark framework for evaluating hallucinations in ASR systems across four dimensions: lexical fabrications, phonetic fabrications, morphological errors, and semantic errors. The authors evaluate 12 ASR models across 10 diverse speech datasets and demonstrate that SHALLOW metrics reveal error patterns that WER alone cannot distinguish, particularly under degraded acoustic conditions. A synthetic validation dataset of 1,050 pairs confirms metric specificity. Key findings show that SHALLOW metrics correlate with WER when recognition quality is high but decouple significantly as WER increases, with correlations dropping from >0.8 to negative values at high error rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Comprehensive multi-dimensional framework**: Decomposing errors into lexical, phonetic, morphological, and semantic dimensions provides interpretable diagnostic capability that goes beyond binary hallucination detection (Frieske & Shi) and complements HER-based approaches (Atwany et al.)\n\n2. **Validation**: The 1,050-pair controlled dataset (Table 1, Figure 4) with isolated error types demonstrates metric specificity through t-SNE separability, addressing a gap in prior work that relied on either heuristics or human judgment alone. \n\n3. **Correlation breakdown insight**: Figure 3's demonstration that SHALLOW metrics decouple from WER as error rates increase (correlations dropping from >0.8 to <0 at high WER) is a valuable empirical contribution showing when traditional metrics fail\n\n4. **Extensive empirical coverage**: Evaluation across 12 models and 10 datasets spanning standard, noisy, accented, and specialized domains provides robust evidence across diverse conditions\n\n5. **Deterministic and reproducible**: Unlike LLM-based approaches (Atwany et al.), SHALLOW's rule-based metrics enable exact reproduction without API costs or model drift concerns\n\n6. **Medical case study impact**: Table 3 concretely illustrates high-stakes failures where low WER masks critical semantic inversions (\"can not\" → \"can\"), demonstrating real-world relevance beyond academic benchmarks"}, "weaknesses": {"value": "1. **Lack of technical novelty**: No novel algorithms, architectures, or theoretical frameworks are introduced. The contribution is primarily engineering—packaging existing tools into a unified benchmark—rather than methodological innovation. This contrasts with Atwany et al., who introduce HER as a new metric with distribution shift theory, and Frieske & Shi, who propose perturbation-based hallucination detection methods.\n\n2. **Insufficient positioning against concurrent work**: The paper cites Atwany et al. (2025) and Frieske & Shi (2024) but provides no empirical comparison. Critical missing experiments:\n- Apply SHALLOW, HER (Atwany), and heuristic baseline (Frieskeet et al.) to the same test set\n- Compare computational cost: SHALLOW's tool pipeline vs. LLM API calls\n- Validate whether SHALLOW's dimensions correlate with Atwany's coarse/fine-grained categories\n- Show which approach better predicts human judgments of hallucination severity\n\n3. **Weighting schemes lack principled justification**: Weights (e.g., 0.5/0.3/0.2 for LF, 0.4/0.6 for ME) appear arbitrary. The claim of validation \"through analysis of error patterns\" (Appendix B) is descriptive rather than optimized. Unlike Atwany's distribution shift correlation (α=0.91) or Frieske's empirically tuned thresholds, SHALLOW provides no sensitivity analysis or optimization procedure\n\n4. **English-only severe limitation**: All evaluation is English-only despite claims about \"ASR hallucinations\" generally. Semantic metrics explicitly require \"language-specific NLP models,\" yet morphological errors are noted as important for \"low-resource languages where morphological richness carries semantic distinctions\" (none evaluated). This is a critical gap that Atwany's work acknowledges but doesn't address either\n\n5. **Tool dependency uncharacterized**: Reliance on LanguageTool, spaCy, Berkeley parser, BERT/RoBERTa introduces failure modes not explored. When do parsers fail on disfluent speech? How do embeddings handle domain jargon? Atwany's LLM-based approach has similar dependencies but provides verbalized confidence scores (Tables 2-3) to quantify uncertainty\n\n6. **No architectural insights**: Unlike Atwany et al., who demonstrate that encoder-only models exhibit lower HER/WER ratios and that model size affects HER non-monotonically, SHALLOW provides purely diagnostic results without explaining *why* certain architectures produce certain hallucination patterns\n\n7. **Synthetic validation may not transfer**: The clean t-SNE separation (Figure 2) on synthetic data may not reflect real ASR errors where hallucination types are entangled. Frieske & Shi found that dataset noise (unique-unique, repeat-repeat) creates specific error distributions does SHALLOW capture these patterns?\n\n8. **Unclear actionability**: Practitioners receive four numbers (LF/PF/ME/SE) with no interpretation guidance. When is PF acceptable vs. problematic? How should one trade off dimensions? Atwany's HER provides a single interpretable metric; Frieske's binary classification is simpler. SHALLOW's richness may hinder adoption without decision frameworks"}, "questions": {"value": "1. Can the authors compare SHALLOW with Atwany et al.'s HER and Frieske et al.'s heuristic baseline on a shared test set (e.g., LibriSpeech, CHiME-6)? How do the approaches correlate? Which best predicts human hallucination judgments?\n\n2. Tables 4-6 show raw components—can authors report SHALLOW scores under alternative weighting schemes (equal weights, domain-optimized)? Was any optimization performed, or are weights purely intuitive?\n\n3. Do SHALLOW's four dimensions map to Atwany's fine-grained categories (Hallucination Error, Phonetic Error, Language Error, Oscillation Error, No Error)? Can authors show correspondence on overlapping examples?\n\n4. When do spaCy parsing or BERT embeddings fail? Can you characterize error rates for the measurement infrastructure itself? What happens on heavily disfluent speech (e.g., CORAAL, MyST)?\n\n5. Does the t-SNE separability (Figure 2) hold on real ASR outputs? Frieske et al. showed that training data noise (UU, RR) affects error types. Can SHALLOW discriminate these patterns?\n\n7. What's SHALLOW's total cost (compute + tools) and comparison with other works in terms of cost and efficiency?\n\n8. Frieske et al. use perturbations (noise injection) to detect hallucinatory models at test time. Does SHALLOW enable similar detection by measuring dimension changes under perturbation?\n\n10. Given a SHALLOW profile (high PF, low LF), what model improvements would you recommend? Can the authors provide heuristics or decision trees?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OqBxFQtV2a", "forum": "YkfhTzq3hL", "replyto": "YkfhTzq3hL", "signatures": ["ICLR.cc/2026/Conference/Submission25034/Reviewer_fExH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25034/Reviewer_fExH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762139246371, "cdate": 1762139246371, "tmdate": 1762943289714, "mdate": 1762943289714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}