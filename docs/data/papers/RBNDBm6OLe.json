{"id": "RBNDBm6OLe", "number": 715, "cdate": 1756776095777, "mdate": 1763701612299, "content": {"title": "VIQS: Overcoming the Teacher Ceiling with Value-Guided Intervention and Quality-Aware Shaping", "abstract": "Human-in-the-loop reinforcement learning (HIL-RL) incorporates real-time human expert intervention and guidance to address the challenges of brittle reward engineering and learning efficiency. However, existing HIL-RL methods primarily rely on direct action mimicry or rigid value alignment, which inherently suffer from a teacher-quality ceiling—their performance is fundamentally bounded by the human expert's proficiency due to the absence of mechanisms for assessing guidance quality. To overcome this limitation, we propose a novel framework that integrates two synergistic innovations—Value-guided Intervention and Quality-aware Shaping (VIQS)—within a reward-free setting. This design allows the agent to break the teacher-quality ceiling by learning robustly from sparse and potentially imperfect expert guidance. First, we propose a value-guided intervention mechanism where expert intervention is triggered precisely when the agent's chosen action yields significantly lower estimated long-term value compared to an expert-derived reference, preserving autonomy for strategic exploration. Second, we develop a quality-aware shaping mechanism that employs a discriminator to dynamically assess and adaptively incorporate expert intervention data, enabling the agent to filter suboptimal advice while absorbing high-quality guidance. Extensive evaluations are conducted on the challenging MetaDrive benchmark, where pre-trained agents emulate human experts of varying proficiency levels to guide the learning process. Results show that VIQS significantly outperforms prior HIL approaches, while requiring up to 5x fewer interventions. Crucially, it consistently breaks the teacher-quality ceiling across all levels of expert proficiency. Furthermore, integrating our core mechanisms into existing HIL algorithms yields significant and consistent improvements across baselines.", "tldr": "", "keywords": ["Human-in-the-loop reinforcement learning", "value-based intervention", "quality-aware value shaping"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a5907f0e94c7c4a71cf235026484941eb4f37f8.pdf", "supplementary_material": "/attachment/359c5416e531b3b20893dea2feb354813f48c27f.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of the \"teacher-quality ceiling\" in Human-in-the-Loop Reinforcement Learning (HIL-RL), where an agent's performance is bounded by the proficiency of the human expert. The authors propose the VIQS framework, which integrates two core mechanisms: a value-guided intervention trigger that compares the long-term value of the agent's action to an expert's reference action, and a quality-aware shaping mechanism that uses a discriminator to dynamically assess the quality of expert interventions. This operates in a reward-free setting, incorporating guidance through adaptively weighted losses for the critic. Experiments conducted in the MetaDrive simulator demonstrate that VIQS outperforms existing HIL methods in terms of performance and data efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper explicitly formalizes the \"teacher-quality ceiling\" as a limitation in existing HIL methods that rely on direct imitation or fixed proxy values, thereby clarifying and motivating the problem.\n2. VIQS combines value-based triggering (avoiding stylistic mimicry) and discriminator-based quality shaping, generating adaptive proxy rewards. This allows the agent to learn \"critically\" from guidance, filtering suboptimal advice while absorbing high-quality demonstrations. \n3. The paper is well-written and easy to follow. The authors effectively decompose the problem into \"When to intervene\" and \"How to learn,\" logically structuring the paper."}, "weaknesses": {"value": "1. The discriminator Dψ(s, a) plays a central role in shaping, yet no examination is provided regarding failure cases, early-stage instability, or its impact when misjudging high-value actions. Can the authors provide more analysis, such as visualizing the distribution of proxy values ($y_t$), to demonstrate the discriminator's stability and judgment?\n\n2. Key hyperparameters such as τ_Q and α are fixed without robustness studies (Eq. 1, Eq. 4). Although Table 7 lists defaults, there is no exploration of performance sensitivity or tuning guidelines, leaving practical reproducibility uncertain. Could the authors provide a sensitivity analysis for these critical hyperparameters to validate the robustness of the results and offer guidance on implementation?\n\n3. The intervention mechanism fundamentally relies on having access to an expert's action-value function, $Q^E$. This is a very strong assumption in practical HIL-RL scenarios involving real human experts, who do not have an explicit, accessible Q-function. How sensitive is the method to the quality of $Q^E$, and what modifications could make the framework applicable to real human experts who lack an explicit value function?"}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tt45hanjLi", "forum": "RBNDBm6OLe", "replyto": "RBNDBm6OLe", "signatures": ["ICLR.cc/2026/Conference/Submission715/Reviewer_geqJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission715/Reviewer_geqJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761206947911, "cdate": 1761206947911, "tmdate": 1762915588690, "mdate": 1762915588690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Revisions"}, "comment": {"value": "Dear Reviewers,\n\nWe extend our sincere gratitude to all of you for your time and invaluable feedback. Your insightful comments have been instrumental in helping us substantially improve our manuscript. In response to your suggestions, we have undertaken a significant revision, with all changes highlighted in yellow in the updated paper. The primary modifications include:\n\n1.  **Strengthened Empirical Validation:**\n    *   **New Benchmark (LunarLander):** To demonstrate the generalizability of our method, VIQS, we have conducted a full set of experiments on the `LunarLanderContinuous-v2` classic control benchmark. The results, presented in a new **Appendix F (Pages 25-26)**, strongly corroborate our original findings, showing that VIQS is the only method to consistently surpass its suboptimal teacher in this distinct domain.\n    *   **Broader Baselines (HIL-SERL):** We have integrated **HIL-SERL**, a state-of-the-art method from the robotics domain, into our MetaDrive experiments to provide a wider comparative context (**Table 1 (Page 8)** and **Appendix G (Pages 26-27)**).\n\n2.  **Deeper Analysis and Justification:**\n    *   **Discriminator & Q-Value Stability:** We have added new analyses to empirically validate the stability of our learning framework. **Appendix D.1 (Figure 11, Pages 22-23)** now visualizes the rapid convergence and stability of the discriminator, while **Appendix E (Figure 13, Page 25)** demonstrates the stable convergence of the agent's Q-values.\n    *   **Hyperparameter Robustness:** A comprehensive sensitivity analysis for our key hyperparameter, `τ_Q`, has been added in **Appendix D.2 (Figure 12, Pages 23-24)**, demonstrating the robustness of VIQS and providing a principled heuristic for its tuning.\n\n3.  **Enhanced Clarity and Discussion:**\n    *   **Bridging to Real-World Application:** We have significantly expanded the **\"Limitations and Future Work\" section (Page 10)** to explicitly discuss the assumption of an expert $Q$-function and provide a clear roadmap for applying VIQS with real human experts.\n    *   **Clarification of Core Mechanisms:** We have refined our explanations for core concepts, such as the rationale for decoupling the trigger and shaper components, freezing the discriminator, and the value propagation mechanism, throughout the paper.\n\nWe are confident that these revisions have made our paper stronger, clearer, and more robust. We thank you once again for your constructive guidance."}}, "id": "kqK7smsNhs", "forum": "RBNDBm6OLe", "replyto": "RBNDBm6OLe", "signatures": ["ICLR.cc/2026/Conference/Submission715/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission715/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission715/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763701685410, "cdate": 1763701685410, "tmdate": 1763701685410, "mdate": 1763701685410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript presents a novel framework that addresses the imperfect human decisions in the setting of human-in-the-loop reinforcement learning. The motivation is clear, the method is easy to follow, and the experiment results are promising. Nevertheless, there are a few questions concerning the model design that should be clarified more."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation\n2. Structured model description\n3. Extensive and promising experimental results"}, "weaknesses": {"value": "1. Some unclear model details and settings.\n2. Some experimental results conflict with the description of previous works."}, "questions": {"value": "1. How is the value of the threshold $\\tau_Q$ in equation (1) determined? I only saw a value in Table 7 in the Appendix, but there is no further analysis concerning this threshold.\n\n2. There is a small gap between the two components. In the first component, only the ``high-value'' expert actions will trigger the learning. But in the second component, there is a further discriminator trying to filter out high- and low-quality advice. I am not questioning this design, just wondering whether the authors have tried to ensemble the two components together. For example, what if we only let the high-quality advice trigger the learning?\n\n3. In line 214, the discriminator's parameters are frozen after a few steps. Given that the discriminator is trained with the agent's policy from scratch concurrently, how can the authors guarantee that the discriminator is well-trained after a fixed number of steps? Is it possible that the agent's policy learns slowly in the beginning and hence the discriminator learns a loose discrimination threshold?\n\n4. In the introduction part, the authors claimed that existing works may perpetuate the teacher-quality ceiling because of blind trust, e.g., PVP and PVP4REAL, but it seems that, from Table 1, these two methods still outperform the baseline solutions purely using human expert policies. Compared with these two baselines, does VIQS provide other essential advantages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Vxg7HOOTIt", "forum": "RBNDBm6OLe", "replyto": "RBNDBm6OLe", "signatures": ["ICLR.cc/2026/Conference/Submission715/Reviewer_L4Uj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission715/Reviewer_L4Uj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619987862, "cdate": 1761619987862, "tmdate": 1762915588575, "mdate": 1762915588575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method, VIQS, to address HIL-RL in a reward-free setting. The paper has two main contributions: (1) proposing when to conduct active intervention; (2) how to train the Value function with expert data and without external reward. The paper then demonstrates in the MetaDrive environment that their algorithm, after receiving a certain amount of expert data, achieves higher efficiency than traditional online RL established from-scratch via pure exploration ."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### 1. High writing quality: \nThe paper is well-written with a clear and rigorous logical structure, making it easy for readers to understand.\n\n### 2. Reasonable intervention mechanism: \nOne of the paper's key contributions is proposing the use of a value-based comparison ($Q^E(s,a^E) - Q^E(s,a^L)$) rather than an action-based comparison to trigger intervention. This is a reasonable choice because the Q-value (policy value) carries more task-relevant information than the action itself (policy appearance), which allows the intervention to focus on strategy rather than style.\n\n### 3. Novel reward-free Q-learning: \nIn a \"reward-free\" setting, using the discriminator's (GAN) output $D(s,a)$ to construct a bounded proxy value target ($y=2D-1$)—and using this as an anchor ($\\mathcal{L}_{PV}$) to shape the Q-function—is a relatively novel design."}, "weaknesses": {"value": "### 1. Insufficient experimental validation:\nThe method is only tested in a single environment, METADRIVE, which is insufficient to demonstrate its generalizability in different HIL scenarios (e.g., robotic manipulation). Furthermore, the comparison of HIL baselines is limited; other methods in this field (such as HIL-SERL) should be considered for inclusion.\n\n### 2. Lack of sensitivity analysis for key parameters:\nThe method has two hyperparameters: the value intervention threshold $\\tau_Q$ and the training stop point for the discriminator (GAN). The paper does not provide ablation or sensitivity experiments for these two parameters, which raises questions about the method's robustness and the difficulty of tuning.\n\n### 3. Inadequate explanation of core mechanisms:\nThe paper's explanation and justification for several key mechanisms (GAN learning and Q value) are somewhat vague (see detailed questions section)."}, "questions": {"value": "1. Could the authors discuss a broader range of HIL-RL baselines? The comparison is limited, and including methods with different mechanisms (e.g., HIL-SERL) would provide a more comprehensive benchmark.\n\n\n2. Could the method's robustness be validated on more experimental environments beyond METADRIVE? For example, robotic manipulation tasks or other virtual environments would strengthen the claims of generalizability.\n\n\n3. The method seems potentially sensitive to key hyperparameters, particularly the value intervention threshold $\\tau_Q$. Could the authors provide a parameter sensitivity analysis for this?\n\n\n4. Regarding the claim that \"extending this knowledge across the state-space\"(Line308, Page 6), according to the update(Eq. 6), a \"value anchor\" at time $t$ propagates backward in time to states $t-1$, $t-2$, etc. This suggests knowledge is propagated to preceding states, not necessarily the entire state-space. Could the authors clarify this and provide Q-value curves over time to demonstrate convergence and stability?\n\n\n5. Regarding the GAN training: At what point is the discriminator training stopped? The paper states it's frozen after \"a fixed number of training steps\", but the policy $\\pi_L$ continues to update. How can a static $D$ accurately evaluate an evolving $\\pi_L$? This affects the quality of the Q function. Can I interpret this to mean that when the GAN network stops training, it has already completely learned to (distinguish) the expert's actions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WgQRD4zCNi", "forum": "RBNDBm6OLe", "replyto": "RBNDBm6OLe", "signatures": ["ICLR.cc/2026/Conference/Submission715/Reviewer_sst8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission715/Reviewer_sst8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635124750, "cdate": 1761635124750, "tmdate": 1762915588454, "mdate": 1762915588454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the \"teacher-quality ceiling\" in HIL-RL. The authors propose VIQS, a reward-free framework featuring two components: value-guided intervention for intervention timing and quality-aware shaping for robust learning. Experiments on the MetaDrive benchmark demonstrate that VIQS outperforms existing HIL baselines and, notably, consistently surpasses the performance of its guiding expert."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a significant challenge in HIL-RL: enabling an agent to learn from an imperfect expert and eventually surpass the expert's performance. This capability is critical for real-world applications.\n\n2. VIQS consistently outperforms its guiding expert across all quality levels, directly demonstrating its ability to break the teacher-quality ceiling."}, "weaknesses": {"value": "1. The reliance on an expert Q-function is  a strong assumption. Value-guided Intervention requires access to $Q^{E}$ to evaluate the agent's actions. It's unclear if $Q^{E}$ can accurately judge the agent's new actions, which might be very different from the expert actions. Furthermore, a real human expert does not have a $Q^{E}$ network. This method seems designed for an agent-guiding-agent setup, not a human-guiding-agent setup. This limitation should be explicitly discussed.\n\n2. The training data for the quality-aware discriminator appears to be extremely sparse. The quality-aware discriminator is trained only when an intervention happens and VIQS needs very few interventions. This could lead to unstable training or an unreliable discriminator. An analysis of the discriminator's training stability and sample efficiency is missing.\n\n3. All experiments are conducted on the MetaDrive benchmark. It should be evaluated on other benchmarks to strengthen the empirical evidence.\n\n4. The paper is missing clear definitions for several terms, such as $\\delta_{a}$ in line 241 and $\\pi'$ in Eq.(6)."}, "questions": {"value": "Please see the weakness part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pToOOnykD6", "forum": "RBNDBm6OLe", "replyto": "RBNDBm6OLe", "signatures": ["ICLR.cc/2026/Conference/Submission715/Reviewer_fVz9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission715/Reviewer_fVz9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922554511, "cdate": 1761922554511, "tmdate": 1762915588345, "mdate": 1762915588345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}