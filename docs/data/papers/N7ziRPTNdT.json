{"id": "N7ziRPTNdT", "number": 20239, "cdate": 1758304059488, "mdate": 1763743837160, "content": {"title": "Generation is Required for Data-Efficient Perception", "abstract": "Visual perception in the human brain is often thought to result from inverting a generative *decoder* that maps latents to images. In contrast, today’s most successful vision models  are non-generative, relying on an *encoder* that maps images to latents without inverting an image decoder. This raises the question of whether generation is required for machines to achieve human-level visual perception. In this work, we approach this question from the perspective of data efficiency, a core feature of human perception. Specifically, we investigate whether *compositional generalization* is achievable, both in theory and practice, using generative and non-generative methods. We first formalize the inductive biases required to guarantee compositional generalization in generative (decoder-based) and non-generative (encoder-based) methods. We then provide theoretical results suggesting that such inductive biases cannot be enforced on an encoder through practical means such as regularization or architectural constraints. In contrast, we show that enforcing the inductive biases on a decoder is straightforward, enabling compositional generalization through inverting the decoder. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. \nEmpirically, we train a range of non-generative methods on photorealistic image datasets, finding they often fail to generalize compositionally and require large-scale pretraining to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.", "tldr": "", "keywords": ["representation learning", "compositional generalization", "generative models"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83531fb8e414f991cb2a0471b784e36188775fce.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "(Preliminary for disambiguate: the encoder/decoder refers to functions that take an image to a latent vector and vice versa. These are not the encoder/decoder in a transformer.)\n\nThis paper investigates whether using an invertible latent->image decoder helps compositional generalization (A+B, C+D are in-distribution, but A+C is not). The authors define the inductive biases for compositional generalizing in both encoder and decoder-based methods and provide theoretical analysis on why enforcing constraints is only practical at the decoder side. The authors propose efficient inversion mechanisms, i.e., gradient-based search and generative replay. The authors also built a synthetic image dataset with different animals and background combinations to validate their claim. With limited data, generative methods that can generate or search in the latent space are able to classify better than non-generative models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tThe paper theoretically shows the necessity for generative modeling and then uses a sample toy example to validate this idea.\n-\tThe decoder inversion mechanisms of gradient-based search and replay are novel.\n-\tThe experiment results align with the author’s claim that encoders are less generalizable.\n-\tThe paper is organized in a clear way."}, "weaknesses": {"value": "-\tThe abstract is a bit confusing: (1) some of today’s most successive machine vision models, like VLM, have generative decoders; (2) the difference between generative and non-generative is in the distribution that they model, not decoder vs encoder architecture (only from Section 2 we know the encoder-decoder are defined as latent-image transformation functions); (3) the claim that enforcing inductive bias on decoder is easier than in encoder is not a well-known thing and thus need some explanation. It is recommended to define the encoder/decoder early to disambiguate from the encoder/decoder in the popular transformer architectures.\n-\tThe theoretical claims are based on the assumptions that (1) data is generated by a diffeomorphic function and (2) components in an image can be cut into distinct slots in the latent vector. This limits the use case of the model since real images/other datasets may not satisfy these.\n-\tThe generative replay is essentially doing synthetic data augmentation. What if, for the feedforward baselines, we also add synthetic data to the training set?\n-\tIt is recommended to add pseudo code for the gradient-based search and replay."}, "questions": {"value": "-\tText in Fig 5/6 is too small.\n-\tCould the authors elaborate a bit or give an example when the requirements for Eq 2.5/2.6 are not satisfied?\n-\tThe gradient-based search seems to depend on an encoder. How sensitive is it to the errors in the encoder?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7a6uKhJ6KN", "forum": "N7ziRPTNdT", "replyto": "N7ziRPTNdT", "signatures": ["ICLR.cc/2026/Conference/Submission20239/Reviewer_zHMs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20239/Reviewer_zHMs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760671142809, "cdate": 1760671142809, "tmdate": 1762933732374, "mdate": 1762933732374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Reply to All Reviewers"}, "comment": {"value": "We thank the reviewers for their valuable feedback, and their assessment of our work as “significant and valuable“ and “addressing a fundamental and longstanding question” `bY3z`. Furthermore, we appreciate that the reviewers found our theoretical contribution “strong” and “rigorous” `keEf`, aspects of our proposed method as “novel” `zHMs`, and our core argument as “well-motivated and rigorously supported … both in theory and practice” `keEf`.\n\nWe are grateful for the constructive feedback given by all reviewers. Based on this feedback, we have made several additions to the manuscript which are highlighted in yellow in the revised version. We briefly summarize the most significant additions below:\n\n**Experiments with Different Decoders**\n\nReviewer `keEf` noted the importance of exploring different generative setups in our experiments. To address this, we have added additional experiments to Appendix C, which compare the compositional generalization of an unstructured CNN decoder which does not meet our theoretical assumptions, vs. the structured Transformer decoder used in our experiments, based on our theory. We find that the unstructured CNN decoder yields a nearly 40% drop in OOD accuracy relative to the structured decoder (Fig. 9), highlighting the importance of placing inductive biases on a decoder as suggested by our theory.\n\n**Additional Discussion Sections**\n\nBased on the reviewer's comments, we have added a new section (Appendix D) which includes several additional discussion paragraphs. Namely, we have added discussions on:\n* Conducting experiments on real-world image data \n* Limitations of our assumptions in theory and practice\n* Better understanding how data scale can yield compositional generalization \n\nWe go into detail on these discussions in our individual replies.\n\n**Clarity of Manuscript**\n\nReviewer `zHMs` raised several points regarding the clarity of our manuscript when discussing the distinction between generative vs. non-generative methods. Based on this feedback, we have re-written several sentences in the Abstract, Introduction, and Section 2, to improve clarity. Reviewer `dnFU` also highlighted ambiguous writing in Section 3, leading us to rewrite the “Takeaways” paragraph at the end of this section. We discuss these points further in our individual replies."}}, "id": "HTuG9PLYIN", "forum": "N7ziRPTNdT", "replyto": "N7ziRPTNdT", "signatures": ["ICLR.cc/2026/Conference/Submission20239/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20239/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20239/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763744188257, "cdate": 1763744188257, "tmdate": 1763744188257, "mdate": 1763744188257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Generative models have long been proposed to play a critical role in human perception, particularly in explaining feedback connections in the visual cortex. However, the computational advantages of generative models over purely discriminative ones remain debated. This paper provides a theoretical argument that generative models are essential for compositional generalization, as they enable structural constraints to be enforced on learned representations—constraints that cannot be imposed on encoder-only architectures. The paper supports this argument with experiments on the PUG dataset, demonstrating data-efficiency advantages for compositional learning.\n\nThe paper is timely, addresses an important question, and proposes a compelling theoretical and conceptual framework. The experimental results are consistent with the theory, although currently limited to a synthetic domain. The work suggests that integrating generative capabilities—or effective generative replay—into representation learning may be necessary to obtain structured inductive biases for compositionality. The contribution would be strengthened by demonstrating these advantages on real visual datasets, but overall, this is a significant and valuable paper."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Modern computer vision systems are largely discriminative: they encode images for classification or embedding without explicit generative components. In contrast, generative frameworks have been championed in computational neuroscience for explaining feedback and recurrent processing in visual cortex (e.g., Mumford; Lee & Mumford; Rao & Ballard). This paper, therefore, addresses a fundamental and long-standing question: what computational benefits do generative models confer for perception?\n\n2. The work contributes both theoretically and empirically. It articulates why generative models offer advantages—namely, generative architectures can enforce inductive biases and structural constraints that encoder-only systems cannot impose. The experiments using compositional concept/background datasets support this idea by showing that generative models generalize compositionally from limited data, suggesting architectural implications for biological vision systems as well."}, "weaknesses": {"value": "1. All experiments are conducted on the PUG synthetic dataset. While PUG is well-designed for controlled compositional generalization studies, it lacks the semantic richness and visual complexity of natural images (e.g., occlusion, clutter, long-tail distribution shifts). It therefore remains unclear how well the claims would transfer to real-world perception tasks. Testing on real-image compositional benchmarks would strengthen the broader argument that generative mechanisms are required for perception.\n\n2. The results also show that the advantage of generative models diminishes with large-scale training data. Conceptually, this makes sense—large, diverse datasets can approximate compositional coverage, enabling encoder-only models to interpolate rather than extrapolate. However, a deeper explanation or analysis of this transition would improve clarity.\n\n3. Finally, while the proofs appear sound in structure, it is not entirely clear whether the assumptions fully reflect the complexity of natural visual environments. A discussion of the assumptions and their biological and practical implications would be useful."}, "questions": {"value": "What are the limitations of the assumptions made in the mathematical proofs?  Do your assumptions fully reflect the complexity of natural visual environments?  Under what circumstances could your assumptions not appropriate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gB4H9zwf16", "forum": "N7ziRPTNdT", "replyto": "N7ziRPTNdT", "signatures": ["ICLR.cc/2026/Conference/Submission20239/Reviewer_bY3z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20239/Reviewer_bY3z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893254887, "cdate": 1761893254887, "tmdate": 1762933731880, "mdate": 1762933731880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical framework and empirical evidence supporting the necessity of generative models for achieving data-efficient compositional generalization in visual perception tasks. The core argument is that generative inversion (i.e., using a decoder to reconstruct data from latent representations) is fundamentally required to guarantee generalization to out-of-distribution (OOD) data due to the nature of the inverse problem on high-dimensional data manifolds. This is contrasted with non-generative (encoder-based) approaches, which are shown to struggle with OOD generalization unless massively scaled. Experimentally, they demonstrate that Non-generative models (encoders trained with supervised or unsupervised objectives) fail at OOD compositional generalization when trained from scratch or with limited pretraining. Performance only emerges with massive scale (e.g., SigLIP2). WhileGenerative models (autoencoders), when combined with inversion techniques like gradient-based search (online optimization) and generative replay (offline recombination of learned slots), achieve significantly higher OOD accuracy across all datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong Theoretical Foundation: Provides rigorous proofs establishing the infeasibility of enforcing compositional generalization guarantees on encoders (Theorem 3.2, Lemma 3.1, A.4) and the feasibility for decoders (Section 3, Theorem A.8).\n2. Compelling Empirical Validation: Uses controlled, photorealistic datasets (PUG) to demonstrate the practical limitations of non-generative models (Fig 5) and the practical benefits of generative approaches + inversion techniques (Fig 6).\n3. Novel Insights on Inversion Techniques: Clearly demonstrates the power of combining generative pretraining (autoencoders) with efficient inversion methods (gradient search, generative replay) for OOD generalization, going beyond standard VAE training.\n4. Clarity of Core Argument: The central thesis – that generation is required for data-efficient compositional generalization – is well-motivated, rigorously supported, and clearly articulated,  both in theory and empirical practice."}, "weaknesses": {"value": "1. Computational Cost: The generative + search/replay approach is inherently more computationally expensive (per-query optimization or large generative model) than a single forward pass through an encoder. This practical trade-off is acknowledged but not deeply analyzed.\n2. Scalability to Complex Real-World Data: While PUG is controlled, experiments don't scale to the complexity of full ImageNet or real-world uncurated data. \n3. Limited Exploration of Alternative Generative Setups: Primarily uses a specific autoencoder architecture. The performance gains are attributed to the generative paradigm + inversion, but the impact of specific architectural choices (beyond the regularization from Brady et al.) could be explored more.\n4. Related Work Integration: While related work is discussed, existing works on diffusion/generative classifiers could potentially be integrated more deeply into the narrative or limitations discussion."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SPvyIoDVhA", "forum": "N7ziRPTNdT", "replyto": "N7ziRPTNdT", "signatures": ["ICLR.cc/2026/Conference/Submission20239/Reviewer_keEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20239/Reviewer_keEf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917375949, "cdate": 1761917375949, "tmdate": 1762933731454, "mdate": 1762933731454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether non-generative and generative perception approaches can generalize to out-of-distribution data by simply enforcing constraints on architecture both theoretically and empirically. They show that it is infeasible to simply constrain the structure of encoder to achieve compositional perception, and optimization process is the key for non-generative approaches to generalize OOD. In contrast, it is easy to enforce inductive bias on decoder to achieve compositional generalization. Empirically, multiple non-generative and generative approaches are evaluated in terms of generalization performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow. \n\n2. The paper is well motivated. There has been extensive discussions in the literature regarding generative and non-generative perception approaches for a long time, including many empirical studies. However, theoretical analysis regarding this is still missed. This work is trying to bridge this gap."}, "weaknesses": {"value": "1.  Most of theoretical results are based on [Brady et al,. 2025], and thus obtaining results of Lemma 3.1 and Theorem 3.2. seem a bit straightforward.\n\n2. For gradient-based search, when using encoder to provide initial guess, it is hard to determine if part of the compositionality performance of decoder comes from encoder. For Figure 6 B, is an additional encoder used?\n\n3. For generative replay, though not explicitly requiring external data, it is essentially using learned generative model to augment the dataset, such that OOD data are included in IID data when training an additional encoder. It hinges on the generative performance of a decoder. Meanwhile, can we say that the compositionality is traded with extra computation and model?\n\n4. The paper emphasizes that compositional performance of non-generative approaches is only possible by using optimization techniques. However, telling from Figure 6, it looks like generative approaches rely more on optimization strategies (e.g., encoder results as initialization, replay). \n\n5. Figure 6 says reporting OOD performance across three datasets, but actually shows only two. Any reason to drop PUG-object?\n\n6. The paper claims that generative models for perception are more data-efficient than non-generative approaches. However, evidence to support this is not so clear. Especially, the design of experiments is very entangled in the sense that non-generative encode and generative decoder are trained jointly. It is hard to determine if the compositional perception performance of decoder also comes from using features from pretained model in the pipeline. Also, given small dataset, is it possible that encoder (large) overfits while decoder (relatively small) not? A further question is why not training both encoder and decoder with small architecture from scratch on your small dataset for comparison?"}, "questions": {"value": "In 3.1, it is said that inverse generators g do not admit an analytical form similar to Eq. (2.7). Is this always the case? or it is a general assumption to facilitate theoretical analysis? Is it possible to make it this form in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kkXLfVI2Kr", "forum": "N7ziRPTNdT", "replyto": "N7ziRPTNdT", "signatures": ["ICLR.cc/2026/Conference/Submission20239/Reviewer_dnFU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20239/Reviewer_dnFU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20239/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998280075, "cdate": 1761998280075, "tmdate": 1762933730969, "mdate": 1762933730969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}