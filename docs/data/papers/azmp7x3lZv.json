{"id": "azmp7x3lZv", "number": 22328, "cdate": 1758329682420, "mdate": 1763692968143, "content": {"title": "Sparsity and Superposition in Mixture of Experts", "abstract": "Mixture of Experts (MoE) models have become central to scaling large language models, yet their mechanistic differences from dense networks remain poorly understood. Previous work has explored how dense models use $\\textit{superposition}$ to represent more features than dimensions, and how superposition is a function of feature sparsity and feature importance. MoE models cannot be explained mechanistically through this same lens. We find that neither feature sparsity nor feature importance causes discontinuous phase changes, and that network sparsity (the ratio of active to total experts) better characterizes MoEs. We develop new metrics for measuring superposition across experts. Our findings demonstrate that models with more network sparsity exhibit greater $\\textit{monosemanticity}$. We propose a new definition of expert specialization based on monosemantic feature representation rather than load balancing, showing that experts naturally organize around coherent feature combinations when initialized appropriately. These results suggest that network sparsity in MoEs may enable more interpretable models without sacrificing performance, challenging the common assumption that interpretability and capability are fundamentally at odds.", "tldr": "", "keywords": ["Mixture of Experts", "Mechanistic Interpretability", "Sparsity", "Superposition", "Representations"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b3d3096ffd9515202f448e1be60a1537769f0b0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the superposition in toy MoE models. Most of its techniques are borrowed from a blog post by Elhage et al., which analyzes superposition in a toy dense model. Many toy setups, such as a hidden dimension of merely 1, are insufficient to support a full-length regular research paper.\n\nThe authors also observe that the phase change seen in dense models is absent in MoE models, and they study expert specialization and initialization. While the sections on specialization and initialization offer an interesting perspective, they still rely on a hidden dimension of 1. I appreciate the viewpoint here but remain unconvinced by the validity of all the results."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Section 5 presents an interesting perspective. If its findings could be validated in realistic MoE models, rather than the toy setups used, the paper would gain significant value."}, "weaknesses": {"value": "1. The font in this paper’s template appears unusual and does not conform to the formal requirements of ICLR 2026.\n\n2. This paper offers few novel techniques. Section 5 is interesting, but the rest of the paper merely adapts Elhage et al.’s method—originally applied to a dense toy model—to an MoE toy model, with formulations and definitions borrowed from Elhage et al. The evaluation metrics and analytical methods are also mostly identical.\n\n3. It may be acceptable for Elhage et al. (whose work is merely a blog post) to use toy models with a hidden dimension of m=5. However, this paper uses m=6, or even fewer, m=1. Insights derived from such toy models are insufficient to serve as the core experiments of a regular paper.\n\n4. In modern MoE architectures, SwiGLU is universally adopted as the expert structure, rather than the two-layer ReLU MLP used here. Activation functions and model architectures significantly influence model behavior and training dynamics, further rendering this paper overly \"toy-like\" in design.\n\n5. On page 4, the conclusion that \"The greater the number of experts, the less superposition in the model\" is not informative. With more experts, the model is wider, and it naturally does not need to allocate features in a superposed manner. The related experiments are therefore uninformative to me.\n\n6. In Line 220, the claim that the loss gap between the MoE and dense model in Figure 7 is \"negligible\" is incorrect. Every observed gap (on the order of 0.1) is significant.\n\n7. The captions, labels, and discussions related to Figure 4 are confusing and hard to follow. Additionally, Figure 4A uses a setup with n=2, m=1; I cannot be convinced by experiments on models with a hidden dimension of 1."}, "questions": {"value": "1. I do not understand why Figure 2b claims that MoE exhibits far less interference with other features than the dense model, as observed in Figure 1b. In both Figures 2a and 2b, a feature interferes with at most one other feature (there is only one blue dot per row or column).\n\n\n2. To maintain the total parameter count, m=6 is split into three experts with m=2. Is it possible that the inherent separation of a dense MLP into MoE explicitly reduces superposition? In other words, can the reduced superposition in MoE be taken for granted?\n\n3. While I consider Section 5 an interesting perspective, I really can not accept experiments using a hidden dimension of  m=1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pLqRF3niTX", "forum": "azmp7x3lZv", "replyto": "azmp7x3lZv", "signatures": ["ICLR.cc/2026/Conference/Submission22328/Reviewer_zaJm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22328/Reviewer_zaJm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761636709864, "cdate": 1761636709864, "tmdate": 1762942172355, "mdate": 1762942172355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors extend the mechanistic analysis of dense models by Elhage et al. to a toy mixture of experts setup. Through a newly introduced attempt to quantify monosemanticity, they show that large expert counts lead to more monosemantic features in their toy setup, and also that MoEs exhibit less of a phase transition as a function of sparsity. Ultimately, the authors advance our understanding of MoEs and tentatively advocate for their use as a more interpretable alternative to dense layers without performance degradation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "## [S1] Strong motivation + important problem\n\nThe paper addresses an important and timely issue in mechanistic interpretability. As the authors correctly note, the MoE architecture is quickly becoming the dominant paradigm for pushing capabilities, yet our understanding of how they work is in its infancy. This sets up the authors’ topic to be of high interest to both interpretability and capability researchers alike.\n\nUnfortunately, as I discuss below, I see some issues with extending insights here to MoEs in practice, but the promise of the work’s premise still remains."}, "weaknesses": {"value": "## [W1] Toy setup formulation needs polish and discussion about transfer to practice\n\nThe authors’ proposed toy MoE formulation for each expert as $\\text{ReLU}({\\mathbf{W}^e}^\\top\\mathbf{W}^e\\mathbf{x}+b^e)$ is quite different to how MoEs are implemented in practice. Even compared to the original Sparse MoE [1], i see two important differences:\n\n1. MoE expert’s input layers’ weights are **not tied to be symmetric**, like the authors’.\n2. Each expert’s FFN often includes a second linear transformation after the ReLU [1].\n\nThe authors should discuss thoroughly how much the proposed symmetry constraint and omission of final linear transformation hinders our ability to extend insights to the non-toy settings? My concern here is that without explicit justification for why this indeed connects to practice, the authors’ insights might be heavily constrained to their unusual toy model formulation alone. Additionally, many SOTA MoEs in practice now use a shared expert [2,3,4]. The authors should comment on how superposition and/or their analysis is affected under this setup.  \n\n### Modifications needed\n\nImportantly, the technical formulation of the toy MoE setup in Section 3 needs clarity and correction. This is necessary to make the authors’ setup perfectly legible to readers, given its non-standard nature. Whilst each issue alone may appear trivial, the presence of many such errors in presentation leads to the general impression that the paper lacks clarity, and precision--for a paper carefully studying a newly introduced toy setup, it is of paramount importance to clearly and correctly formulate the toy model they are proposing.\n\nSome issues:\n\n- [L111] this equation does not compute. $W_r^\\top x$ would be needed (with the transpose) for this to work.\n- On [L114], there is a confusing inconsistency between the use of the gating weights with $w_e$ and $w^e$ at once, which also clashes with the notation used for the input layer. The authors should define clearly how the normalization is computed, and I would suggest naming this something different entirely (e.g. $a_e$).\n- $W_e$ is not defined, nor are its dimensions (used on [L112]).\n\n## [W2] Missing related work section\n\nThe authors do not include a dedicated discussion of related work. Whilst 16 references do appear throughout the paper, a dedicated section is crucial to place the authors’ contributions in context of the prior literature.\n\nAs one example of why this is important, one of the authors’ key contributions is a definition of expert specialization for monosemantic features ([L066]). However, the authors do not discuss existing attempts to quantify expert monosemanticity in the literature, and why their proposed analysis offers additional insights; measured through ablations in [5,6]. A detailed discussion of how the proposed analysis relates to both existing works should be made to situate the work in relation to existing attempts.\n\n---\n\n## References\n\n[1]: Shazeer, Noam M. et al. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” ICLR 2017\n\n[2] Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., ... & Piao, Y. (2024). Deepseek-v3 technical report. *arXiv preprint arXiv:2412.19437*.\n\n[3] Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., ... & Zhang, H. (2025). Kimi k2: Open agentic intelligence. *arXiv preprint arXiv:2507.20534*.\n\n[4] Meta AI. (2025, April 5). *The Llama 4 herd: The beginning of a new era of natively multimodal intelligence*. https://ai.meta.com/blog/llama-4-multimodal-intelligence/\n\n[5]: Park, Jungwoo, et al. \"Monet: Mixture of monosemantic experts for transformers.\" ICLR 2025.\n\n[6]: Oldfield, James, et al. \"Multilinear mixture of experts: Scalable expert specialization through factorization.\" NeurIPS 2024."}, "questions": {"value": "## [Q1] Mixed definitions of monosemanticity / features\n\nOn [L086], the authors state `Monosemantic features are defined as those that are well-aligned with individual neurons`. \n\nI am a little confused by this definition. In Elhage, monosemanticity is a property of *neurons* (possibly SAE latents), not the high-level concepts; the goal is to establish the independent computational units of meaning. The difference between the two appears to me important.\n\nFor example, there may exist multiple neurons that monosemantically correspond to the *same* high-level concept. This is consistent with the Elhage definition of monosemanticity, but not the authors’, when formulated as a property of the concept.\n\nFurthermore, “features” is used on [L086] to refer to human-interpretable concepts, but again on [L110] onwards to denote the $n$ input neurons. Monosemanticity and superposition suggests that this equivalence does not hold.\n\nMight the authors please clarify their use of the terminology here?\n\n## [Q2] Load balancing mixed use\n\nThe authors should comment on why load balancing is used for Sect. 4 but not for Sect. 3. At the minute, it is left unexplained; and the extent to which experts are balanced should surely influence the kinds of features it learns.\n\nSpecifically, without a load balancing loss for the experiments in Sect. 3, what is preventing the MoE from learning a single expert alone (functionally equivalent to the dense model)? Might the authors please comment on the balance observed in the first section?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None of note."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "78y604MJbR", "forum": "azmp7x3lZv", "replyto": "azmp7x3lZv", "signatures": ["ICLR.cc/2026/Conference/Submission22328/Reviewer_Qdj5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22328/Reviewer_Qdj5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644643449, "cdate": 1761644643449, "tmdate": 1762942172093, "mdate": 1762942172093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper applies the superposition framework of Elhage et al to MoEs, in the context of toy autoencoders. The main claims are: (1) MoEs represent the same number of features as do dense models with the same capacity, but they do so more monosemantically, meaning with less superposition or interference between features. (2) MoEs don't show the same phase transitions that dense models do between monosemantic representation, polysemantic representation and ignoring features, as a function of feature sparsity in the input distribution and imbalance in the feature weights on the loss function."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The approach potentially gives insight into representation and expert specialization in MoEs"}, "weaknesses": {"value": "I had great difficulty figuring out what was done in many parts of the paper. I don’t normally share such detailed notes as I do in the Questions section, but in this case I do to help explain how much work this paper needs.\n\nThe main claim that “MoEs represent the same number of features as the dense model, but more monosemantically” (e.g., L220) seems impossible. How can two models match in the number of features they represent and the number of dimensions (“parameters”) they use, but differ in the number of features per dimension (i.e., in monosemanticity)? \n\nMathematically, the definitions of feature representation (L141) and features per dimension (L204) are nearly the same. If we stack $W=W^{1:E}$ we get an $Em\\times n$ matrix matching the dense model. Setting aside marginal load imbalance (i.e., assuming $p_e=k/E$) the definition of features per dimension is the same for the two models (i.e., stacked and unstacked representations). Moreover the result equals the sum of squared feature strengths: $|W|_F^2 = \\sum_i |W_i|^2$ for the dense model and $|W|_F^2 = \\sum_i \\sum_e |W^e_i|^2$ for the MoE. So if the models really match on summed feature strengths and differ on features per dimension, it seems like this can only be because of the different orders of summation and squaring in how the measures are applied to the two models (and this should be spelled out in the paper), but that would hinge on the seemingly arbitrary choice to define feature strength as $|W_i|$ rather than $|W_i|^2$. Also the fact that strengths in figs 1a and 2a are all near {0,1} suggests that squaring makes little difference.\n\nPutting all this somewhat differently, if superposition happens when a network encodes more features than it has hidden dims then MoE (with the same total number of hidden dims as the dense model) can’t help: monosemanticity will still require dropping some features."}, "questions": {"value": "L58: What phase change, i.e. what is the macroscopic variable and what are the hyperparameters?\n\nL63: It looks like the hyperparameter for the phase change is network sparsity, but that doesn’t apply to dense models.\n\nL87: Does it have to be a single neuron, or can it be an oblique direction in activation space? It should be the latter because of rotation invariance (for standard FF layers). The important criterion is that features have orthogonal representations.\n\nL102: What claim? The previous sentence is just descriptive. I also don’t understand the second half of the sentence: do you want to quantify representational similarity between experts? (I don’t think that’s the topic of this paper.)\n\nL111: $n$ is the number of input features, not the input features (those are denoted $x$)\n\nL112: $W_r$ is $E\\times n$ not $n \\times E$. It would also help to state $W^e\\in\\mathbb{R}^{m\\times n}$.\n\nL114: $w_e$, $w^e$\n\nL116: What is the loss? From what comes next I think it’s squared error with weights $I$. Also what is the dataset or generating distribution? What is the optimization/training procedure? These are critical questions for understanding all the experiments in the paper.\n\nL140: $W^e_i$ is column $i$ of $W^e$?\n\nThere seems to be an assumption that $|W^e_i| \\le 1$. I can see this for $E=1$ because otherwise the reconstruction overshoots. But in MoE the reconstruction is weighted by the gating weights which are $<1$ (see def of $x’$ at L114). For example if a feature is represented by only one expert $e$ then that expert would need to scale up its output by $1/w_e$. \n\nDoes the term ‘dense model’ mean anything more than $E=1$?\n\nHow is superposition in figs 1a and 2a defined?\n\nL146: What does “roughly the same number of features” refer to? I see 10 features (0,1,3,5,6,7,8,9,10,12) represented in 1a and 8 features (0,1,2,3,4,5,6,9) in 2a.\n\nIn what sense do the models have equal total parameters? They have an equal number of hidden dimensions (6) but the MoE also has gating weights.\n\nFigs 1b 2b are described as measuring interference but they don’t match the definition at L143. Also the claim is less interference in the MoE but I count 4 interfering pairs in fig 1b and 5 in 2b.\n\nL204 (please consider numbering equations): This expression doesn’t work as a count if $|W^e_i| > 1$ (see comment above) because then feature $i$ contributes more than 1 to the count.\n\nL262: I think you mean $x=(x_1,x_2,\\dots,rx_n)$. Also it’s poor notation to define the $n$th component of $x$ as $rx_n$.\n\nL263: $x_i\\sim U(0,1)$\n\nL263: \"$S$ likelihood that $x_i=0$\" doesn’t make sense. You want to define a mixture distribution between a uniform and a point mass.\n\nL267: This should be stated formally especially given earlier confusion about param counts. I think you mean $m_{\\rm dense} = km_{\\rm MoE}$.\n\nEquating active dimensions ($m_{\\rm dense} = km_{\\rm MoE}$) doesn’t seem like the right comparison (as opposed to $m_{\\rm dense} = Em_{\\rm MoE}$) because it gives the MoE more capacity than the dense model. The MoE can represent a feature with some expert and choose not to activate that expert when that feature is absent. So it’s not clear whether the differences regarding phase transition in fig 4 are due to dense vs MoE per se or due to differences in model capacity. \n\nFig 4: I think the network diagrams for ABC are meant to indicate the values of $n$ and $m$. The implication about $m$ is ambiguous so it would be better just to state the values. Also what are the values of $k$? (Ok we are eventually told $k=1$.)\n\nEach pixel in fig 4 is a completely separate simulation, and each model should be invariant to permutation of the experts, so how can there be systematic differences between experts 1 and 2 in the first two columns?\n\nWhy is the importance of only one feature being varied? I suspect the figure is showing values only for that feature but the caption suggests otherwise (subscript $i$ instead of $n$).\n\nL368: Does “feature $x$” indicate $x$ denotes a unit vector $e_i$ (i.e., $x_j = \\delta_{ij}$) or does “feature” here refer to any input vector (i.e., arbitrary $x\\in\\mathbb{R}^n$ or perhaps $x\\in[0,1]^n$)?\n\nL371: What measure is used to define volume? Induced Lebesgue measure on the L2 unit sphere?\n\nL373: This claim (“they tend to align experts with particular features”) is not warranted by the tiny sample shown in fig 5. It requires a systematic study, also using more feature dimensions since with $n=2$ it’s nearly impossible to have good load balancing without allocating $x=(1,0)$ and $x=(0,1)$ to different experts.\n\nL409: I don’t think $c_i$ has been defined. Is this a statement about $W_r$ from L111? More importantly, how can the gate matrix be the diagonal (I assume you mean an identity matrix) when it isn’t square ($W_r\\in\\mathbb{R}^{E\\times n}=\\mathbb{R}^{5\\times20}$)?\n\nFig 5: what do the colors represent? (Probably separate questions for even and odd columns)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o9Yc7i1S44", "forum": "azmp7x3lZv", "replyto": "azmp7x3lZv", "signatures": ["ICLR.cc/2026/Conference/Submission22328/Reviewer_JRyu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22328/Reviewer_JRyu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668714025, "cdate": 1761668714025, "tmdate": 1762942171848, "mdate": 1762942171848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the ideas presented in the anthropic blog, with a focus on toy MoE models in the context of superposition. \nThe authors examine the superposition of these toy MoE models and observe the absence of a phase transition, which is seen in dense models. \nAdditionally, the paper explores expert specialization and initialization, which provides insights to the understanding of MoE behavior."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The exploration of expert specialization and initialization is interesting to me. \nThese topics provide insights into a better understanding of MoE behaviors.\n\n- The authors conduct extensive experiments to support their idea."}, "weaknesses": {"value": "- Some of the paper is a straightforward extension of the anthropic blog, which adapts the research on dense models to MoE models.\nAs a result, the contribution feels somewhat limited.\n\n- The authors' findings on toy models are interesting, but they are not entirely convincing to me due to the experimental setups.\nFirstly, the experiments are conducted on toy models with a very small hidden dimension (e.g., 6 or even 1).\nWhile interesting, it is hard for me to trust the conclusions draw from such a toy models;\nMoreover, it is unclear whether the conclusions drawn from the two layer MLP with ReLU are truly relevant to modern FFNs, such as SwiGLU, which limits their applicability to real-world MoEs."}, "questions": {"value": "---\nQ1: Would it be possible for the authors to conduct the experiments in Figure 5 with different configurations?\n\nThe current results, set m=1, do not provide sufficient evidence to convince me.\n\n---\n\nQ2: I find Figure 4 to be somewhat complex and difficult to interpret. \nCould the authors consider revising the caption, improving the labels, and providing additional clarification on the experimental design."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uOwkwJyvPG", "forum": "azmp7x3lZv", "replyto": "azmp7x3lZv", "signatures": ["ICLR.cc/2026/Conference/Submission22328/Reviewer_SA9i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22328/Reviewer_SA9i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802200467, "cdate": 1761802200467, "tmdate": 1762942171603, "mdate": 1762942171603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how mixtures of experts learn some toy models of data, versus how dense MLPs learn them. The paper finds that mixture of experts neurons are much more monosemantic than dense MLP neurons."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The question of how the mixture of experts architecture interacts with how concepts are represented by neurons (monosemantically vs. polysemantically) is interesting.\n\n* The analyses of this paper look like they are probably quite interesting. I just had a very tough time reading them because the basic definitions and setup are not presented."}, "weaknesses": {"value": "* Some definitions are missing, which makes the paper unclear in cases and hard to read in others. (See my questions below for examples.) The paper could greatly benefit from a clearer exposition of definitions so that readers can understand what the authors concretely mean by a \"feature\", or by \"monosemanticity\" in this context.\n\n* The analyses are all conducted on toy models, without any analysis, e.g. of MoE models trained on real data."}, "questions": {"value": "* What does importance I = 0.7^i mean? What is the data distribution in the experiments? This is not described until much later on in the lines 260-264, but I am not sure how to parse this definition. Are the features vectors? Why are they scalars in this definition? Why is only the last feature sampled from a different range from the other ones?\n\n* What do the colors of the bars mean in Figure 1(a), Figure 2(a)? Are these D_i scores from 0 to 1?\n\n* The architecture choice in section 3.1 doesn't make sense to me. Why is the ReLU applied outside of everything else when computing x_e' ? Doesn't that mean that the architecture will always output reconstructions with nonnegative entries?\n\n* How is a feature defined as monosemantic or not?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u1FuTNYNYQ", "forum": "azmp7x3lZv", "replyto": "azmp7x3lZv", "signatures": ["ICLR.cc/2026/Conference/Submission22328/Reviewer_z1ER"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22328/Reviewer_z1ER"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009068252, "cdate": 1762009068252, "tmdate": 1762942171351, "mdate": 1762942171351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}