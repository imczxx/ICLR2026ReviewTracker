{"id": "gZyEJ2kMow", "number": 14820, "cdate": 1758244360327, "mdate": 1759897347383, "content": {"title": "It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization", "abstract": "Designing efficient and effective architectural backbones has been in the core of research efforts to enhance the capability of foundation models. Inspired by the human cognitive phenomenon of attentional bias—the natural tendency to prioritize certain events or stimuli—we reconceptualize neural architectures, including Transformers, Titans, and modern linear recurrent neural networks as associative memory modules with attentional bias. We define and formalize the concept of attentional bias as the internal memory objective deep learning architectures. We show that existing deep learning architectures leverage the same attentional bias based on $L_2$ loss function. Going beyond $L_2$ loss function, we present a set of alternative attentional bias configurations along with their effective approximations. We then reinterpret forgetting mechanisms in modern deep learning architectures as a form of retention regularization. Building upon these insights, we present Miras, a general framework to design deep learning architectures based on the choice of attentional bias objective, retention gate, associative memory architecture, and memory learning algorithm. Our experiments show different designs yield models with varying strengths. Furthermore, our special instances of Miras achieve exceptional performance in language modeling, commonsense reasoning, recall intensive, and time series tasks, outperforming Transformers and other modern linear recurrent models.", "tldr": "", "keywords": ["Test Time Memorization", "Online Optimization", "Recurrent Neural Networks"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b81b2405ddb1987fcc7ebbb5aff0e3281e449034.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This study proposed MIRAS, a unifying framework that reinterprets sequence models like Transformers and RNNs as associative memory modules governed by online optimization. MIRAS decomposes model behavior into two parts, i.e., Attentional Bias and Retention. The authors propose a broader design space by generalizing beyond various losses. To justify the effectiveness of MIRAS, authors conduct experiments on language modeling, commonsense reasoning, and long-context tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. MIRAS provides a new perspective to view and design sequence models through the lens of online optimization and associative memory.\n\n2. MIRAS achieves great performance compared to baselines, especially on long-context tasks."}, "weaknesses": {"value": "1. The training and inference speed and GPU memory of MIRAS are not reported. Can the efficiency be maintained on larger models, e.g., 7B+ parameters?\n2. More ablation studies are needed to justify the effectiveness of the proposed method, such as the impact of different p/q values and different memory algorithms.\n3. The significance test is not reported in the experiments.\n4. MIRAS achieves inferior performance in some tasks (see Table 1, Table 7, and Table 9). Thus, an in-depth analysis should be provided in such cases.\n5. The proposed models, i.e., MONETA, YAAD, and MEMORA, are the incremental combinations of existing ideas, e.g., MLP memory, Huber loss, and softmax gating, rather than fundamentally novel architectures.\n6. The implementation code is not provided to check and reproduce."}, "questions": {"value": "I have provided my concerns in the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CVIxkWaLXl", "forum": "gZyEJ2kMow", "replyto": "gZyEJ2kMow", "signatures": ["ICLR.cc/2026/Conference/Submission14820/Reviewer_VGJG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14820/Reviewer_VGJG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761275128605, "cdate": 1761275128605, "tmdate": 1762925168567, "mdate": 1762925168567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The purpose of this paper is to unify prior work on the optimization and regularization of Transformers and recurrent networks with associative memory and to use the insight gained thereby to improve on the state-of-the-art linear RNNs. The authors create a formal theory that synthesizes disparate viewpoints, design a generic framework – based in part on the aforementioned formal theory – for the architecture of neural language models, and use the framework to guide the design of candidate linear RNNs. The results reported indicate that the authors' candidate networks are competitive with or superior to existing linear RNNs, albeit far from competitive with the conventional Transformer w.r.t. in-context recall, as reported in Appendix H.4."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper has many positive attributes. It is written quite well (some typos noted below), presents a unifying framework of the optimization and regularization of neural language models, proposes a number of architectural variants based on the framework, and contains ample experimental results and ablations. The overall results – reported in the main body of the paper and in the appendix – are competitive or superior, especially needle-in-the-haystack."}, "weaknesses": {"value": "* Given the scope of this work, this reviewer believes the presentation would be improved by the inclusion of a brief limitations section in the main body of the paper. The results of Table 9 in Appendix H.4 should be mentioned there.\n* Code does not appear to be provided. This reviewer considers the lack of code to be a blocker to acceptance.\n* It’s not clear whether the reported results are from single or multiple runs. The lack of reporting of any sort of variance across runs leads this reviewer to believe the reported results are for single runs.\n* The section about ablations is not clear about which dataset and metric are used during the ablations (see Tables 3 and 4).\n* In general, a succinct and comprehensive performance comparison accounting for latency and memory at both train- and test-time -- covering all models used in every evaluation -- would be useful in the main body of the paper.\n\nTypos:\n* Line 20: there's no space between \"loss\" and \"Miras\".\n* Line 185: extra space before comma after \"(Learning-Retaining Viewpoint)\".\n* Line 266: \"As discussed in the previous section, existing work focus\" -> (\"As discussed in the previous section, existing work has focused\" or \"As discussed in the previous section, existing work focuses\")\n* Line 426: \"use needle-in-haystack task\" -> \"use the need-in-a-haystack task\"\n\nIn one focused reading of the paper, I noticed quite a few additional typos -- missing articles, subject-verb disagreements, and singular-plural errors -- elsewhere in the manuscript. I suggest the authors perform a close reading of their own to find them."}, "questions": {"value": "1. Given the substantial gap between Transformer and linear RNNs and Miras-based models on the in-context retrieval task (see Table 9), would you characterize the theoretical and empirical work presented here as providing some improvements over linear RNNs without the promise of bridging the gap with Transformers on this task?\n1. Are all results of single or multiple runs?\n1. Which dataset and metric are used in the ablations (see Tables 3 and 4)?\n1. Some of the results are for \"Transformer++\" and some (in the appendices) are for \"Transformers\". By \"Transformer++\", do you mean the enhanced Transformer named \"Transformer++\" so-named in [Gu and Dao 2023](https://arxiv.org/abs/2312.00752)? Since [Thapak & Hore 2020](https://arxiv.org/abs/2003.04974) also uses the name \"Transformer++\", the paper's readability and future value would be improved by being explicit what is meant by \"Transformer++\". As it stands, the citation for \"Transformer++\" is a LLaMa paper which doesn't explicitly articulate the definition of \"Transformer++\".\n1. In Table 1, the most competitive model vis-a-vis Moneta is Gated DeltaNet-H2. Why is Gated DeltaNet-H2 not shown in Figure 2? Is there something about Gated DeltaNet-H2 that prevents it from being compared to Moneta in the setting of Figure 2?\n1. Where is the code? If there's already a link to the code somewhere in the paper, this reviewer would much prefer it being clearly presented early in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i8pYVoxgUP", "forum": "gZyEJ2kMow", "replyto": "gZyEJ2kMow", "signatures": ["ICLR.cc/2026/Conference/Submission14820/Reviewer_suGh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14820/Reviewer_suGh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761517014218, "cdate": 1761517014218, "tmdate": 1762925167914, "mdate": 1762925167914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MIRAS, a framework that reinterprets sequence models with memory as associative-memory modules governed by online optimization. The framework introduces two key components: (i) Attentional Bias (internal learning objective) for capturing new data, and (ii) Retention (regularization term) for retaining old knowledge. The paper then investigates various loss functions and regularizers (e.g., ℓp, Huber, KL, f-divergences), resulting in three derived architectures (MONETA, MEMORA, and YAAD) claiming improved robustness, stability, and scalability compared to existing linear RNNs and attention-based models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a clear and systematic formalization of memory update in sequence models within a unified optimization-based lens.\n- The connection between the Learning–Retaining and FTRL viewpoints is well-written and could be theoretically valuable.\n- Empirical evaluations contain multiple model variants and tasks."}, "weaknesses": {"value": "- The paper lacks conceptual novelty. The proposed MIRAS framework primarily rephrases existing viewpoints (e.g., online optimization, meta-learning, and energy-based associative memory) under new terminology. Sec. 2.1, 2.2, and 2.3 review the known formulation as cited in the paper. Sec. 2.3 introduces an \"alternative\" view which considers the entire history of sequences, but basically presented before in concrete form in [1, 2]. \n- The proposed “alternative attentional bias” and “retention gates” correspond to standard online learning formulations with user-defined approximations and regularizers. There are no clear insights/proofs why some choices are better than others.\n- The experimental evaluation covers only a small subset of recent linear RNNs and attention models, with little explanation for the chosen baselines. Comparisons with stronger and more recent models, such as attention-based architectures like Titan [3] (cited by the paper) or recurrent approaches like SHM [4], would be necessary to support the claimed advantages.\n- Many reported improvements (0.3–1.0 points) in Table 1 are small and potentially within noise margins, especially since no variance or statistical significance is reported. Table 2 shows better improvement, but the set of baselines is limited. Ablation study, the selection of p=3,q=4 as “optimal” appears arbitrary, and the paper does not provide insight into why higher-order norms should improve robustness beyond qualitative intuition.\n\n[1] Munkhdalai, Tsendsuren, Alessandro Sordoni, Tong Wang, and Adam Trischler. \"Metalearned neural memory.\" Advances in Neural Information Processing Systems 32 (2019).\n\n[2] Bartunov, Sergey, Jack Rae, Simon Osindero, and Timothy Lillicrap. \"Meta-Learning Deep Energy-Based Memory Models.\" In International Conference on Learning Representations, 2019.\n\n[3] Behrouz, Ali, Peilin Zhong, and Vahab Mirrokni. \"Titans: Learning to memorize at test time.\" arXiv preprint arXiv:2501.00663 (2024).\n\n[4] Le, Hung, Dung Nguyen, Kien Do, Sunil Gupta, and Svetha Venkatesh. \"Stable Hadamard Memory: Revitalizing Memory-Augmented Agents for Reinforcement Learning.\" In The Thirteenth International Conference on Learning Representations, 2025."}, "questions": {"value": "- Minor formatting error: lack of spaces between sentences, e.g, in L20 and 338\n- The framework in Fig.1 introduces 4 components, but it looks like only 2 are examined. Have you tried different memory architectures and memory algorithms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0gmJnBHq3E", "forum": "gZyEJ2kMow", "replyto": "gZyEJ2kMow", "signatures": ["ICLR.cc/2026/Conference/Submission14820/Reviewer_6YPG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14820/Reviewer_6YPG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625462636, "cdate": 1761625462636, "tmdate": 1762925167552, "mdate": 1762925167552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified online optimization view that interprets mainstream sequence models as associative memory updates jointly driven by attentional bias and retention, and it treats forget gates as a form of regularization. Building on this framework, the authors move beyond dot product and Euclidean objectives to introduce three attention free and parallelizable architectures that outperform strong baselines on language modeling and long context retrieval while showing greater robustness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a unified online optimization perspective, taking \"attention bias - retention\" as the core paradigm, and systematically reinterpret the forgetting gate as regularization. Further breaking away from the reliance on dot product and binary norm, introducing robust loss and divergence is conceptually enlightening and has promotional value.\n\n2. On benchmarks such as language modeling and long context retrieval, thorough comparisons were made with strong linear RNN baselines to stably obtain gains and demonstrate robustness against noise and outliers. While maintaining O(1) memory overhead and good throughput at the training end, the experimental setup and indicators are relatively comprehensive."}, "weaknesses": {"value": "1. The comparison mainly focuses on several linear RNNS and long context benchmarks, and has not yet made a \"same-scale, same-training budget\" head-to-head comparison with the current first-line inattentional/near-attentional models. It is recommended to supplement the system comparison under the same number of parameters, the same number of training steps and data quota, and report the significance and confidence intervals.\n\n2. Although various forms of bias and retention are presented, there is a lack of strict item-by-item ablation to quantify the independent contribution of each design decision. It is recommended to incorporate grid ablation, negative control (reverting to the two-norm/dot product), and cross-dataset reproduction to verify the robustness of the conclusion.\n\n3. It is enlightening to link \"learning-retention\" with FTRL, but when memory is non-convex (such as with MLP), the guarantee of convergence and stability is not clear. It is suggested to provide applicable conditions, failure cases and stability diagnosis"}, "questions": {"value": "1. The paper provides sensitivity curves or heat maps such as p, q, Huber cutoff thresholds, retention weights, learning rates and step sizes for performance and stability. Is there a robust default configuration or an automatic parameter tuning strategy?\n\n2. How does the framework perform on multimodal long sequences? Is task-specific bias/retention configuration required to earn benefits?\n\n3. Can MIRAS be combined with sparse attention or local attention? Under the same computing budget, what are the trade-offs between pure MIRAS, pure attention, and hybrid solutions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FzOtBwUZTZ", "forum": "gZyEJ2kMow", "replyto": "gZyEJ2kMow", "signatures": ["ICLR.cc/2026/Conference/Submission14820/Reviewer_xTsB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14820/Reviewer_xTsB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860988671, "cdate": 1761860988671, "tmdate": 1762925166792, "mdate": 1762925166792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}