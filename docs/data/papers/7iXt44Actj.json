{"id": "7iXt44Actj", "number": 18446, "cdate": 1758287891723, "mdate": 1759897102910, "content": {"title": "Value Matching: Scalable and Gradient-Free Reward-Guided Flow Adaptation", "abstract": "Adapting large-scale flow and diffusion models to downstream tasks through reward optimization is essential for their adoption in real-world applications, including scientific discovery and image generation. While recent fine-tuning methods based on reinforcement learning and stochastic optimal control achieve compelling performance, they face severe scalability challenges due to high memory demands that scale with model complexity. In contrast, methods that disentangle reward adaptation from base model complexity, such as Classifier Guidance (CG), offer flexible control over computational resource requirements. However, CG suffers from limited reward expressivity and a train-test distribution mismatch due to its offline nature. To overcome the limitations of fine-tuning methods and CG, we propose Value Matching (VM), an online algorithm for learning the value function within an optimal control setting. VM provides tunable memory and compute demands through flexible value network complexity, supports optimization of non-differentiable rewards, and operates on-policy, which enables going beyond the data distribution to discover high-reward regions. Experimentally, we evaluate VM across image generation and molecular design tasks. We demonstrate improved stability and sample efficiency over CG and achieve comparable performance to fine-tuning approaches while requiring less than 5% of their memory usage.", "tldr": "", "keywords": ["diffusion models", "flow models", "black-box reward optimization", "molecular design", "image generation", "stochastic optimal control", "reinforcement learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e8b3316ab684275613bccd804fd07beed8221121.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes an online algorithm called Value Matching (VM) to learn a value function within an optimal control framework, addressing the scalability challenges faced when adapting large-scale flow and diffusion models to downstream tasks via reward optimization. VM enables flexible control over the complexity of the value network, offering adjustable memory and computational requirements. It supports optimization with non-differentiable rewards and operates in an on-policy manner, allowing it to go beyond the training data distribution and explore high-reward regions. Experiments on image generation and molecular design tasks demonstrate that VM achieves better stability and sample efficiency compared to Classifier Guidance (CG)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.It learns the scalar value function directly rather than estimating guidance gradients, leading to more stable training.\n2.It offers significant theoretical and practical value by enabling low-cost reinforcement learning (RL) fine-tuning."}, "weaknesses": {"value": "1.The evaluation is insufficient, with comparisons limited to only CT-PPO.\n2.The core contribution is primarily encapsulated in Equation 11, but the paper devotes excessive space to background, making it hard to follow."}, "questions": {"value": "1.Please clarify how your work differs from the following approaches, and discuss the performance gap:\nInference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance\nEfficient Controllable Diffusion via Optimal Classifier Guidance\n2.Although your method reduces fine-tuning costs, can it outperform the following full fine-tuning approaches?\nLarge-scale Reinforcement Learning for Diffusion Models\nTraining Diffusion Models with Reinforcement Learning\nDPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dh6QDlVMon", "forum": "7iXt44Actj", "replyto": "7iXt44Actj", "signatures": ["ICLR.cc/2026/Conference/Submission18446/Reviewer_U1zm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18446/Reviewer_U1zm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761125914648, "cdate": 1761125914648, "tmdate": 1762928142714, "mdate": 1762928142714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes value matching (VM), an online approach that relies on a small learned value network for guiding the model towards higher rewards. The proposed method is derived from optimal control theory and effectively learns a vector field network and a value network that simultaneously approximate the running cost. On downstream generation tasks, the proposed approach achieved better performance across multiple objective rewards."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper's proposed method is built upon adjoint matching (AM), which is intuitive and easy to follow given the AM framework.\n- The paper, built upon AM, also shares some desirable mathematical properties or guarantees.\n- The proposed method has better running time compared to gradient-based AM models, but the performance comparison remains unclear (see weaknesses)."}, "weaknesses": {"value": "- Although the theoretical part in the paper seems solid to me, the experimental evaluations have **very limited baselines and focus only on small-scale datasets or reward functions** to convincingly demonstrate the effectiveness of the proposed approach.\n  - **Regarding the number of baselines**. For each task, only the CFG and a single model were used as the baseline, even though many existing works were mentioned in the paper. For example, for image generation, there are gradient-based methods such as Adjoint Matching, SOC, FlowGrad [1], and DFlow [2], as well as other RL approaches such as ReFL [3] and [4] (gradient-free) and DRaFT [5] (gradient-based). For molecule generation, DFlow and OCFlow [6] achieved decent generations with almost perfect stability and validity. Current results do not fully demonstrate the superiority of the proposed approach over existing work.\n  - **Regarding the evaluation metric and reward function**. The reward function used in the experiments in this paper is either a toy example from existing work or a non-standard metric not established in previous work on the same task, further weakening the paper's claims, as most metrics are not necessarily comparable. \n    - For example, the standard task in previous flow matching guidance papers on image generation almost all focus on guiding text-to-image models like SD2/3 and use robust metrics like CLIP score, PickScore, or HPSv2 to prevent easily hacking the rewards (e.g., [6] demonstrated that the compression metric can be easily hacked to almost perfect but meaningless generations). For molecule generation, it also remains unclear why the authors did not follow the standard molecule generative modeling evaluation setup in [7] (uses CFG), DFlow, and OCFlow to evaluate the generation and compare the results with these existing works easily. \n    - For molecule generation, it is widely known that quantum chemical calculations are either computationally expensive (ab initio methods) or highly inaccurate (empirical or semiempirical methods). It remains unclear what class the calculation method used in the experiments belongs to, how accurate and generalizable it is (GEOM-Drug is known to have some unreasonable configurations), and what the computational time is, which is crucial for reproducibility. In addition, crucial molecular properties like stability and validity were never mentioned or compared in the paper. Therefore, I am highly skeptical about why the authors did not follow the standard and easy approach in existing work but opted for a seemingly far more complex setup. I would highly discourage such an approach when comparing with baselines for fairness.\n\nTo summarize, for a paper emphasizing the scalability of the proposed method, I believe the existing experiments are, in contrast, limited in scope and poorly credible in supporting its fundamental claims. \n\n- **The theoretical contributions in this paper are limited to me**. The core idea is almost identical to [VGG-Flow](https://openreview.net/forum?id=6MmOy2Ji8V). The scale of the experiments and the number of baselines in this paper fall significantly short of those in VGG-Flow, even if the latter is to be considered concurrent. In addition, there are existing works that have explored the role of the value function or its equivalent, the Q-function, for generative modeling, such as [8] and [9]. Despite different application domains, the underlying core ideas are pretty similar to me. Given that the theoretical results primarily come from the adjoint-matching paper, the theoretical contributions are limited.\n \n- The method's scalability hinges on the value network being \"significantly smaller than the base model.\" But what happens when the reward function is extremely complex? A small network may fail to accurately model the true value function, creating a new performance bottleneck. Is there a trade-off between VM's memory savings and its ability to represent a complex reward landscape? This weakness also echoes in the paper's limited, small-scale evaluation, as thoroughly mentioned in the first part.\n\n- The algorithm learns by regressing the value network's predictions $V_{\\theta}(x_t, t)$ onto a Monte Carlo estimate of the cost functional $\\hat{J}$ (Eq. 12). This target $\\hat{J}$ is based on a single sample trajectory and also depends on the current value function itself. This can be a very high-variance target, which is known to make value-based RL difficult to stabilize. The paper uses a weighting scheme, but the inherent stability of this learning process, especially for very long trajectories, is a potential concern.\n\n- To sample from VM, one must run both the large base model and the (smaller) value network at every step to compute the guiding gradient $\\nabla V_{\\theta}$. While this is still much faster than other gradient-based inference-time optimization schemes, it's not \"free\" and adds computational overhead compared to using a single fine-tuned model. Other gradient-free approaches, such as ReFT and [6], should be benchmarked to support a more credible claim. Additionally, this issue may be coupled with the expressive power of the value net for more complex rewards mentioned above, and it may not be easy to find a balance. \n\n[1] Liu, Xingchao, et al. \"Flowgrad: Controlling the output of generative odes with gradients.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[2] Ben-Hamu, Heli, et al. \"D-flow: Differentiating through flows for controlled generation.\" arXiv preprint arXiv:2402.14017 (2024).\n\n[3] Luong, Trung Quoc, et al. \"Reft: Reasoning with reinforced fine-tuning.\" arXiv preprint arXiv:2401.08967 (2024).\n\n[4] Fan, Jiajun, et al. \"Online reward-weighted fine-tuning of flow matching with wasserstein regularization.\" The Thirteenth International Conference on Learning Representations. 2025.\n\n[5] Clark, Kevin, et al. \"Directly fine-tuning diffusion models on differentiable rewards.\" arXiv preprint arXiv:2309.17400 (2023).\n\n[6] Wang, Luran, et al. \"Training free guided flow matching with optimal control.\" arXiv preprint arXiv:2410.18070 (2024).\n\n[7] Hoogeboom, Emiel, et al. \"Equivariant diffusion for molecule generation in 3d.\" International conference on machine learning. PMLR, 2022.\n\n[8] Zhang, Shiyuan, Weitong Zhang, and Quanquan Gu. \"Energy-weighted flow matching for offline reinforcement learning.\" arXiv preprint arXiv:2503.04975 (2025).\n\n[9] Alles, Marvin, et al. \"FlowQ: Energy-Guided Flow Policies for Offline Reinforcement Learning.\" arXiv preprint arXiv:2505.14139 (2025)."}, "questions": {"value": "Please refer to the list of weaknesses above. In addition:\n- VM is an *on-policy* algorithm (Algorithm 1), which means it discards past trajectories after each update. On-policy methods are generally known to be sample-inefficient. While the paper shows VM is more efficient than CG, how does its absolute sample efficiency compare to (hypothetical) offline value-based methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "F4X7pyNRVc", "forum": "7iXt44Actj", "replyto": "7iXt44Actj", "signatures": ["ICLR.cc/2026/Conference/Submission18446/Reviewer_9Esi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18446/Reviewer_9Esi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761432913764, "cdate": 1761432913764, "tmdate": 1762928142225, "mdate": 1762928142225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Value Matching (VM), a novel, scalable, and gradient-free algorithm for reward-guided adaptation of large-scale flow and diffusion models. VM uses less memory than finetuning methods and offers improved stability compared to classifier guidance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. By decoupling the value learning from the base model, VM requires a separate, small value network. This reduces memory usage by over 95% compared to fine-tuning.\n2. VM operates on-policy, which gives it enhanced reward expressivity and stability compared to CG.\n3. The presentation of this work is clear and easy to follow, with abundant theoretical justifications."}, "weaknesses": {"value": "1. The main quantitative results in Figures 8 and 9 focus only on Reward, KL, and FID. Why not directly compare the generated samples' diversities?\n2. The paper contrasts VM's simplicity with the \"extensive hyperparameter search\" required by CT-PPO. However, this work does not include essential ablations studies on the architecture and size of the separate value network. It is important to know the sensitivity of VM's final performance to variations in the value network. Does VM also require a careful choice of a value network to support acceptable performance?\n3. A concern lies in the selection of inference-time techniques. While CG is a classic baseline, its performance and controllability are not comparable to other non-fine-tuning guidance methods. Why not compare with those in experiments?\n4. Figure 6 does not compare with any baselines. I think any training or non-training methods can perform well for the compressibility task.\n5. In Figure 9, why is VM only compared with CT-PPO? For example, for aesthetic scores, many direct propagation algorithms can easily achieve >7 aesthetic reward in 10 epochs, but they are not mentioned. The current results are also confusing. It seems unclear what this means \"VM demonstrates performance comparable to CT-PPO but with more predictable and stable behavior.\""}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gY1abvkSh1", "forum": "7iXt44Actj", "replyto": "7iXt44Actj", "signatures": ["ICLR.cc/2026/Conference/Submission18446/Reviewer_UG8D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18446/Reviewer_UG8D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761802092186, "cdate": 1761802092186, "tmdate": 1762928141877, "mdate": 1762928141877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an online algorithm for learning the value function of the flow matching models by formulating the reward-guided generation as an optimal control problem. By drawing the analogy that classifier guidance is seen as offline value function learning, the method enables reward-guided generation without reward fine-tuning the base flow matching model. Specifically, it trains a separate value model as the value function. This admits flexible reward model design (in terms of both architectures and model sizes) and significantly reduces memory usage compared to directly fine-tuning the base model. The method achieves comparable performance with PPO."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The connection between CG and VM is intuitive (VM viewed as an online generalization of CG)\n+ The results of VM achieving comparable performance with PPO on image and molecule generation tasks look promising."}, "weaknesses": {"value": "+ One of the major benefits of using reward guidance over directly doing reward fine-tuning (if the reward is non-differentiable, one can use an approach similar to [1]) is the reduced memory footprint. However, one can adopt LoRA to effectively reduce the memory requirement of the latter. I suggest the author compare different LoRA setups to show the trade-off of memory usage vs. fine-tuned performance of the baseline method to better illustrate how the proposed method does in preservation performance while using much less memory.\n+ Since the value model is decoupled from the base model, it would be good to perform a bit of a scaling study on the scale of the value model.\n+ Optimization with more reward functions will make the results more convincing.\n+ It would be good to add [1] to related work as it also incorporates value function learning for reward fine-tuning flow-matching models.\n\n[1] Reward Fine-Tuning Two-Step Diffusion Models via Learning Differentiable Latent-Space Surrogate Reward, CVPR 2025"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WwJw1N2NTx", "forum": "7iXt44Actj", "replyto": "7iXt44Actj", "signatures": ["ICLR.cc/2026/Conference/Submission18446/Reviewer_1EuJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18446/Reviewer_1EuJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18446/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762296810977, "cdate": 1762296810977, "tmdate": 1762928141436, "mdate": 1762928141436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}