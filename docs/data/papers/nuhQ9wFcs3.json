{"id": "nuhQ9wFcs3", "number": 5529, "cdate": 1757918114268, "mdate": 1759897969374, "content": {"title": "Planning in Stochastic Environments with Awareness of Catastrophic Chance Events", "abstract": "Planning in stochastic environments is a research topic of high interest that remains underexplored in robustness, despite its importance for real-world applications. To consider robustness, previous methods typically assume an aggressive adversary that constantly attacks the agent, limiting the ability to learn and distorting the stochastic dynamics of chance events. In addition, expectation-centric methods implicitly discount low-probability events, failing to address rare catastrophes. To address this, we introduce Robust Stochastic Zero, the first method to be aware of catastrophes while maintaining the inherent stochastic dynamics. Specifically, it replaces the environment with a lurking adversary that mostly preserves the dynamics but selectively intervenes at the most critical moments. By targeting rarely occurring catastrophic chance events using tree-based planning, our method enables the agent to anticipate and avoid risky decisions, and also develops an adversary capable of delivering malicious impact with minimal intervention. On two benchmark stochastic environments, 2048 and Tetris Block Puzzle, Robust Stochastic Zero achieves an average of 122.1% of the baseline performance over both environments while intervening in only 0.05% of events, and remains comparable when no interventions occur. Our findings demonstrate that the right rather than constant intervention is a direction to robust planning in stochastic environments.", "tldr": "We propose Robust Stochastic Zero, which improves the planning robustness in stochastic environments using minimal adversarial interventions at the most catastrophic chance events.", "keywords": ["deep reinforcement learning", "stochastic AlphaZero", "stochastic environments", "search-based planning", "MCTS"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/30cd2c385b944984d6614bbdafe73d5ad0d1b46b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents an adapted MCTS algorithm improving an agent's performance in the presence of rare events or adversarial interventions. The key lies in identifying situations, in which the performance is particularly vulnerable to potential perturbations. To this end, the authors introduce the notion of attackability, defined by the product of worst-case performance (expected reward) loss and the likelihood of an intervention. The attackability coefficient is used to inject interventions during MCTS' rollouts. An empirical evaluation in the 2048 and Tetris puzzles demonstrates performance advantages over a baseline MCTS that does not optimize for rare event robustness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Properly dealing with rare events is crucial in many sequential decision-making applications, in particular safety critical applications, where endangering situations are not so common but one needs to be sure that the system reacts to them appropriately. The sparsity of the events combined with their criticality on the agent's performance makes it especially challenging for simulation based RL and MCTS approaches to deal with such problems. The paper presents a new methodology to handle rare event inside MCTS, and they demonstrate empirically that their method can indeed improve performance in two puzzle games."}, "weaknesses": {"value": "The placement of this work in terms of general topic and with respect to related work, and therewith also the impact of the contributions is not clear to me at all. The authors do not properly and formally describe the problem that is addressed. Where do the \"catastrophic chance events\" come from? Are those events potentially occurring in a stochastic environment, or are those events deliberately injected by an adversarial agent? In the latter case, we are clearly dealing with a two-player game. This, one, raises the question about the rules of this game, and secondly bags for a comparison to other two-player algorithms (which is not provided). If catastrophic events, on the other hand, are stochastic in nature, then handling such problems also has long history in (stochastic) planning and RL -- chance-constrained POMDPs, importance sampling, safe RL, etc. Consequently, the authors should compare their method to such approaches. Despite that the paper contains a related work section, the placement with respect to which is not clear to me at this point.\n\nDespite a clear problem formulation, the clarity of the write up in general leaves much room for improvements. The concept of \"afterstate\" is not properly introduced, and while knowledge about the general functioning of MCTS can be assumed, the notion of afterstate is rather uncommon. This point is particularly crucial given the centrality of this concept for the authors' overall method. The authors also do not really explain their algorithm changes; they just describe what they change without providing the rationale behind it. The examples assume precise knowledge of the 2048 puzzle game. The paper should hence at least briefly introduce the rules and actions of this game (in the main text).\n\nA discussion of of the effects of the authors' algorithm changes on MCTS' well-known properties (convergence, etc.) is not provided.\n\nLastly, the benchmark design (2048 and Tetris) is completely artificial, and I can hardly connect them to the general motivation of handling catastrophic events. I would much rather see an evaluation of, e.g., safety critical benchmarks such as the safety gym. In addition to the more clever baselines, all the experiment setup makes it hard for me to judge the actual impact of the developed techniques."}, "questions": {"value": "1. Can you elaborate the specific problem that you want to address (cf. main review)?\n\n2. Why could you not compare to other approaches, like the robust RL method discussed in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7UnmkXZUyB", "forum": "nuhQ9wFcs3", "replyto": "nuhQ9wFcs3", "signatures": ["ICLR.cc/2026/Conference/Submission5529/Reviewer_9RUn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5529/Reviewer_9RUn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761052262721, "cdate": 1761052262721, "tmdate": 1762918114327, "mdate": 1762918114327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a stochastic planning approach called Robust Stochastic Zero (RSZ) approach. This approach builds on top of Stochastic Alpha Zero (SAZ) approach. It adds to it by adding robustness to adversarial events — resulting a more conservative plan by the agent and avoidance of most critical actions that can result in game ending states. The authors state that it helps to have the model trained with just the right amount of adversarial actions, if there is over attacking or under attacking in training it can lead to degradation of the performance. The authors claim that their approach achieves 122.1% over the baseline of SAZ in two domains. They perform ablation experiments to show how the configuration they choose is a better one."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The overall idea behind the approach of using adversarial attacker to improve the overall robustness of the approach is interesting and definitely relevant to the field. \n\nThe authors explain the baseline approach very clearly, which helps with understanding of their overall problem setup. \n\nThe paper is quite readable for the most part and is overall well-written. The authors use good amount of formal notations without making the paper dense to read."}, "weaknesses": {"value": "The related work section is not given much thought and is quite underwhelming. It would be useful to keep the related work section in the start of the paper in between introduction and discussion on the preliminary works, as against the end of the paper where there is no surrounding context. \n\nThe experiment section is somewhat weak. It raises several questions as against strengthening the original hypothesis of the paper. Until the experiment section, the paper is well written. Thereafter the premise and conclusion of the experiments is not very clear. \n\nThe two domains chosen for experimentation tend to have different observations for each experiment. So no observation can be derived conclusively. Maybe it helps to add more domains for experiments to have a more general narrative for each experiment and the corresponding result."}, "questions": {"value": "- For every experiment, clearly state the hypothesis that you start with and how the results support the conclusion of your hypothesis. \n- On line 196, there is mention of dynamically determining the attackable threshold. In the experiment setup, How was this value dynamically computed? \n- Ablation section currently does not discuss why the other configurations do not yield better results — it just states the scores for each configuration.\n- From Figure 5, it seems that when there is no attacker in training and in test, there is best performance. Also, it seems that as attacking increases in training the performances across 4 test groups differs less. It is not clear to me what conclusions we can draw from the different observations available in this figure. \n- How was the average performance gain of 122% computed in your experiments? \n- Line 170 — typo (c) should be (g)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4mp6ugD9GB", "forum": "nuhQ9wFcs3", "replyto": "nuhQ9wFcs3", "signatures": ["ICLR.cc/2026/Conference/Submission5529/Reviewer_jTgh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5529/Reviewer_jTgh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761170154882, "cdate": 1761170154882, "tmdate": 1762918113821, "mdate": 1762918113821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for planning in stochastic environments that accounts for the possibility of low-probability events that could, when applied at critical moments, significantly affect performance. The proposed method uses a tree-based planning approach that accounts for an adversary potentially intervening at high-impact times. Results on 2048 and Tetris Block Puzzle show that the proposed method gives better robustness for low attack rates, with little suboptimality in the nominal setting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is likely less conservative than worst-case robust approaches, yet it accounts for low-probability adversaries, effectively finding high-impact attacks through tree-based planning.\n- The comparison in Section 4.3 against straighforward methods is a good ablation motivating the proposed method. \n- The method's ability to pinpoint the most critical afterstates (Section 4.4) is interesting, as it could potentially also be used to understand how a problem could be slightly modified to obtain a more robust closed-loop system."}, "weaknesses": {"value": "- Missing problem definitions: Section 2 lacks a formal MDP or stochastic game definition, making it difficult to understand what assumptions the method relies on. For instance, is an MDP or POMDP considered? Is a finite state-action space assumed? What is an afterstate? What is a chance event? Defining these terms would help readers familiar with MDPs and POMDPs understand the proposed method.\n\n- Limited related work discussion: The paper does not cite and discuss works in risk-averse (risk-constrained) RL that are relevant and address similar problems. Without this discussion, it is difficult to assess the novelty claims.\n\n- Results: Comparisons against risk-averse or worst-case methods are missing, and would help assess the benefits of the proposed method over existing work. Also, the method is better than SAZ for low attack rates, but it is equivalent or worse for high attack rates, which is surprising as one could expect increased robustness for all attack levels.\n\n- Minor feedback: 1) Acronyms should be defined before they are used, such as \"pUCT\". 2) In equation (1), there is no need for the symbol $\\times$  if it denotes a multiplication. 3) Typos: \"corresponds\" => \"corresponding (line 75), \"derivation\" => \"deviation\" (line 276)."}, "questions": {"value": "- How does this work positions itself with respect to the literature on risk-averse RL? Adding a paragraph on this topic would strengthen the contribution.\n- Please clarify the definition of the severity $T$ in (5) is unclear: $x$ is fixed, so $\\min x$ gives $x$, which is probably not the desired definition.\n- Results in Figure 3 (b) show worse performance than SAZ for high attack rates, which raises questions regarding the improved robustness of the proposed method. I would have expected improved robustness, even though $\\lambda_{\\text{train}}<\\lambda_{\\text{test}}$. Do you have an hypothesis for this performance degradation? \n- The concept of critical afterstates and their identification is interesting, (Section 4.4), see my previous comment in \"Strengths\". Can similar vulnerability identification capabilities be obtained using other methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q76eb2e2KW", "forum": "nuhQ9wFcs3", "replyto": "nuhQ9wFcs3", "signatures": ["ICLR.cc/2026/Conference/Submission5529/Reviewer_qR9X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5529/Reviewer_qR9X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938208743, "cdate": 1761938208743, "tmdate": 1762918113278, "mdate": 1762918113278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Robust Stochastic Zero (RSZ), a planning framework for stochastic environments that aims to be robust to rare but catastrophic chance events without distorting the environment's inherent randomness. Instead of the stepwise minimax adversaries common in robust RL, RSZ introduces a lurking adversary that selectively intervenes only at critical afterstates, i.e., chance nodes where some outcomes would trigger avoidable catastrophes. Criticality is quantified by an attackability score that combines severity and rarity. RSZ integrates this notion into a modified search procedure, Robust Stochastic MCTS, layered on Stochastic AlphaZero/MuZero, and augments the network with a head that predicts value‑drop magnitudes for chance outcomes to guide selective interventions. In experiments on 2048 and Tetris Block Puzzle, agents trained with a tiny attack rate outperform baselines under equally rare attacks, remain comparable with no attacks, and preserve empirical tests of environment randomness at small attack rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The paper articulates a nice gap i.e., expectation‑centric planning tends to ignore tail events, while stepwise minimax adversaries distort dynamics and induce over‑conservatism. The lurking adversary that intervenes rarely is a compelling middle ground.\n\n(2) Focusing criticality at afterstates is in my opinion a good design choice for stochastic MCTS (it isolates the locus where nature acts and aligns with SAZ/SMZ's separation of decision and chance nodes). \n    \n(3) Decomposing criticality into (i) severity and (ii) rarity is an intuitive, implementable criterion. The normalization choices (tanh, max‑normalization) aim to make the score stable across scales and arities.\n\n(4) The empirical results are very promising."}, "weaknesses": {"value": "(1) The work would be stronger if it situated RSZ within a known formal objective (e.g., a budget‑limited adversary model, distributionally robust return, or a tail‑risk criterion). As written, \\tau and the thresholding policy are well‑motivated but heuristic. This leaves questions about what RSZ is optimizing (e.g., an upper bound on tail risk, a CVaR‑like surrogate under a budget constraint, etc.) and about convergence properties of RS‑MCTS under that objective.\n\n(2) There is no argument that, with accurate \\delta and an appropriate \\tau_th estimator, RS‑MCTS improves tail risk, bounds regret against a \\lambda‑budget adversary, or preserves expectation in the no‑attack limit. Even a simple proposition (e.g., monotonic effect of \\lamda on a tail metric) would help.\n\n(3) (a) The claim under equation (6) i.e., \"falls short of the parent value q...\" is not necessarily true. Because softmax(a + const) = softmax(a), subtracting q has no effect. \\delta depends only on the relative ordering and spread of the x's. That is fine in practice, but the text should not claim it is directly tied to q; while it clearly isn't.\n\n(3) (b) \\sigma tending to 0 (for example, if all event values are (near) equal) would lead to ill-conditioned values, I suppose the text should infact be refactored to represent a principled \\epsilon stabilization. \n\n(4) (a) I believe the novelty claim should be contextualized i.e., \"First method to be aware of catastrophes while maintaining inherent stochastic dynamics\" is too strong. There are established strands of risk‑sensitive and robust planning that try to protect tails without fully adversarializing every step (e.g., tail‑risk criteria, constrained risk, distributionally robust value backups). It is fine to to not cite specific works here, but the claim should acknowledge neighboring paradigms and be scoped precisely.\n\n(4) (b) The coverage of the related works section is too narrow.\n\n(5) In SMZ, how do you prevent attacked events from corrupting supervision of \\rho (and r)? Do you mask attacked samples when training \\rho, or correct them with importance weights?\n\n(6) The core claims are broad (robust planning in stochastic environments). But the evidence (from expts) comes from 2 single‑player puzzles. Ablations and Table 1 are shown for only one of them. It would strengthen the case to replicate key ablations in Tetris Block Puzzle.\n\n(7) From my understanding, tests that aggregate over all steps (e.g., overall position frequency uniform over empty cells) can be easily confounded by which states the agents visit. Importantly, RSZ may steer into different state distributions. A deviation in position frequency may reflect state visitation changes, not chance‑rule bias.  I would suggest conditioning the tests on state context: e.g., compute uniformity per step over the set of empty cells and aggregate step‑wise p‑values (or use weighted tests that normalize by the number of empty positions per step). Similarly, sequence tests should control for non‑IID structure induced by varying legal positions/events across states.\n\nOther comments (parts that I might not have fully understood):\n\n(8) The paper's narrative emphasizes low‑probability catastrophes, but does H (Eq. 6) not measure outlierness among child values? i.e., not the probability of those children under \\rho. As a result, can \\tau not be large even when the catastrophic child is not rare? \n    \n(9) Looks like T and H are sensitive to the scale and dispersion of child values. But there is no demonstration that \\tau is affine‑invariant or robust across tasks with different reward scales, which complicates cross‑domain tuning and the \\tau threshold estimator.\n\n(10) If adversarially chosen chance events are treated as observed during training, won't they bias the learned \\rho (and potentially r)? i.e., I am talking about potential contamination of model heads.  \n\n(11) Minor: the work claims RS‑MCTS \"ensures\" every chance outcome at root is visited at least once, but this may be budget‑infeasible (right?) in high‑arity states. There is no fallback rule for incomplete vectors and no accounting of the opportunity cost (reduced depth)."}, "questions": {"value": "Please see the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CnsnwXTVOq", "forum": "nuhQ9wFcs3", "replyto": "nuhQ9wFcs3", "signatures": ["ICLR.cc/2026/Conference/Submission5529/Reviewer_342r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5529/Reviewer_342r"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972226196, "cdate": 1761972226196, "tmdate": 1762918112795, "mdate": 1762918112795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers the problem of performing robust planning in stochastic environments that are subject  to rare, catastrophic events. A specific model for such stochastic environments with rare, catastrophic events is proposed. For this model, a version of stochastic Monte Carlo tree search (MCTS) that leverages a neural network to estimate the system model, called Robust Stochastic Zero (RSZ), is proposed as a solution method. The core difference between this solution method and existing MCTS approaches is a procedure for inflating the perceived probability of the outcome of the rare, catastrophic events during the backpropagation phase of MCTS. Experiments are provided comparing the performance of RSZ with Stochastic Alpha Zero (SAZ), a practical implementation of non-robust MCTS for stochastic environments, on the games 2048 and Tetris."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Development of robust learning methods for sequential decision-making problems is an extremely active area, and the goal of developing versions of MCTS for stochastic environments that incorporate robustness to high-impact, low-probability events is a worthy one. For this reason, the topic of the paper is timely and likely of interest to the community. In addition, the technical presentation of the proposed model and method is solid, supporting reproducibility."}, "weaknesses": {"value": "1. The paper suffers from clarity issues. Specifically, insufficient explanation is provided of the reasoning behind key steps in the proposed model and proposed approach. Importantly, the rationales underlying the notion of attackability in Sec. 3.2 and equations (5) and (6) and the procedure in  equations (7) and  (8) for inflating the perceived outcomes of rare, catastrophic events are unclear. This makes it difficult to connect the proposed model and approach to concrete applications, and it also makes it challenging to accurately assess how they differ from existing models and methods.\n2. The motivation behind the specific model proposed in Sec. 3.1 and Sec. 3.2 for stochastic environments with rare, catastrophic events is weak. Specifically, it is not clear what kinds of real-world applications or important problems with relevance to the research community are captured by this model. This makes it difficult to accurately assess the significance of the proposed model. It would be useful to provide specific examples of important problems that either fit this model or where the model and corresponding approach lead to good approximate solutions.\n3. The proposed approach, RSZ, appears to be designed to  apply only to problems that can be captured using the specific model mentioned in Weakness 2 above. In particular, the mechanism described in Sec. 3.3 and in equations  (7)-(8) for inflating the perceived outcome probabilities for rare, catastrophic events are tied to the notions of attackability and the risk thresholds that are developed in Sec. 3.2. This dependence makes it unclear whether the proposed approach can apply to problems beyond the specific model the authors propose.\n4. There are two main issues with the experimental results:\n\n    (i) Though the abstract and introduction present the paper as overcoming the overly  conservative nature of existing robust planning and robust learning methods, the only baseline that the experiments compare against is SAZ, which is a non-robust MCTS method for stochastic problems. This makes is difficult to situate the proposed method within the very active literature on methods for robust learning in stochastic environments, an overview of which is provided in the related works in Sec. 5. Comparison with key representatives of the robust methods described in the related works is needed.\n    \n    (ii) There are inconsistencies in the experimental results presented. Specifically, in Fig. 3, RSZ trained with $\\lambda_{train} = 0.05\\%$ is shown to mostly outperform SAZ (which is equivalent to RSZ with $\\lambda_{train} = 0\\%$) for a variety  of values of $\\lambda_{test}$. However, Fig. 5 appears to show that SAZ ($\\lambda_{train} = 0\\%$) outperforms RSZ for all values of $\\lambda_{train} >  0\\%$ on both environments, and Figs. 7, 9, 10, and 11 in the appendix show decidedly mixed results between the two approaches. These issues need to be clarified to enable accurate assessment of the effectiveness of RSZ."}, "questions": {"value": "1. What is the rationale behind the selection rules in equations (1) and (2)?\n2. What are the precise definitions of $p^\\ell, v^\\ell, \\rho^\\ell, \\nu^\\ell$ in line 88? How are they going to be obtained?\n3. What is the reasoning behind the update rule in equation (3)?\n4. What is the intuitive reasoning behind the notion of \"attackability\" defined in Sec. 3.2? Why is the definition provided a good one?\n5. What is the rationale behind equations (4), (5), and (6)? Are there alternatives, and why is this definition preferable to them?\n6. Regarding the \"Attackable Threshold\" defined in lines 190-198: what concrete types of scenarios can this model capture and how?\n7. What is the intuition and rationale behind the specific form of the update given in equations (7), (8)?\n8. Regarding the **Decision** description on lines 264-269: for what specific scenarios is this type of adversary decision procedure a good model?\n9. Can you comment on the inconsistencies mentioned in Weakness 4(ii) from **Weaknesses** above?\n10. Is there a mitigating reason that no robust planning or learning baselines were compared with in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sSJkbBEy7c", "forum": "nuhQ9wFcs3", "replyto": "nuhQ9wFcs3", "signatures": ["ICLR.cc/2026/Conference/Submission5529/Reviewer_Vm4N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5529/Reviewer_Vm4N"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission5529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762277127873, "cdate": 1762277127873, "tmdate": 1762918112353, "mdate": 1762918112353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}