{"id": "t697jIOfO8", "number": 19033, "cdate": 1758292934740, "mdate": 1759897064809, "content": {"title": "Decoupling Prompt Tuning for Long-Tailed Visual Recognition", "abstract": "Long-tailed visual recognition encounters difficulties in feature learning, particularly for tailed classes. Recently, fine-tuning visual prompt of pre-trained large models with powerful feature extraction capabilities for long-tailed data has been investigated. Although the result is promising, visual prompts are easily affected by semantically irrelevant parts of images, resulting in diminished effectiveness. To address this issue, we propose a Long-tailed Decoupling Prompt Tuning (LT\u0002DPT) method for long-tailed visual recognition. LT-DPT explicitly decouples visual prompts into foreground-related prompts and background-related prompts, respectively. Specifically, foreground-related prompts emphasize saliency regions of the image which includes the most discriminative information for classification while background-related prompts capture common background features shared across classes, regardless of their category. In comparison with the state-of-the-art methods, extensive experiments demonstrate that the proposed method achieves better performance on long-tailed visual recognition benchmark datasets.", "tldr": "Decoupling Prompt Tuning for Long-Tailed Visual Recognition", "keywords": ["Long-tailed Data Learning", "Visual Recognition", "Decoupling Prompt"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8b3437f3c8ef053d7e6a7f00f86852a31f6d4baa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to address a key issue in long-tail visual recognition: existing Visual Prompt Tuning methods tend to be distracted by semantically irrelevant background information when learning features for tail classes (categories with few samples), which can negatively impact model performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "[1] The idea is novel and intuitive: decoupling prompts into foreground and background components is both clever and straightforward. It directly addresses the shortcomings of existing methods, such as dispersed attention and susceptibility to background interference, providing a clear solution.\n[2] The experimental validation is thorough: the authors conducted comprehensive experiments, comparing their method with various approaches—including training from scratch and prompt-tuning methods—across multiple datasets. They also designed detailed ablation studies to verify the effectiveness of foreground prompts, background prompts, and various hyperparameters (such as the weight λ of the orthogonal loss and the number of layers H where background prompts are applied), thereby strengthening the reliability of their conclusions."}, "weaknesses": {"value": "[1] If the saliency detection algorithm used performs poorly on certain images—such as those where the subject blends with the background, the scene is complex, or the objects are very small—incorrect saliency maps may mislead the model and potentially degrade performance.\n[2] The method simply divides image regions into foreground and background. However, in real-world images, elements such as the middle ground or context related to the main subject may also contain important discriminative information.\n[3] It's unfair that the paper was trained on ImageNT21k but evaluated on ImageNet."}, "questions": {"value": "[1] In scenarios where saliency detection is inherently challenging—such as with camouflage or harsh lighting—does the method’s performance degrade significantly?\n[2] Could this “decoupling” concept be applied to other vision tasks? For example, in fine-grained image classification, could prompts for different components (such as a bird’s head or wings) also be decoupled? Would this approach be effective in object detection or segmentation tasks as well?\n[3] What exactly are the “general patterns” learned by the background prompts, and to what extent do they help the model recognize tail classes?\n[4] How does your method improve performance compared to LIFT?(Long-Tail Learning with Foundation Model: Heavy Fine-Tuning Hurts)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MLKA8P4FyQ", "forum": "t697jIOfO8", "replyto": "t697jIOfO8", "signatures": ["ICLR.cc/2026/Conference/Submission19033/Reviewer_Fw1V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19033/Reviewer_Fw1V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621854131, "cdate": 1761621854131, "tmdate": 1762931073252, "mdate": 1762931073252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores an interesting phenomenon: prompt tuning schemes tend to degrade model performance due to background attention bias under long-tail visual data distributions (characterized by imbalanced data categories) and proposes a method named Long-Tail Decoupled Prompt Tuning (LT-DPT). The core framework demonstrated by this method involves decoupling visual prompts into two complementary types: foreground-related prompts and background-related prompts. Foreground-related prompts are trained using the supervision of prior saliency maps, thus forcing the model to focus on the most discriminative object regions in images. Background-related prompts are guided to learn a shared generic background, maintaining diversity during actual inference through orthogonality loss. By concentrating on target regions and coordinating diversity and generalization in background prompts, the model achieves significant performance gains on classification tasks under long-tail distribution scenarios. The authors demonstrate the effectiveness of this decoupling strategy by surpassing existing state-of-the-art methods on multiple benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The description of the design and approach in this paper is relatively clear.\n\nThe validation, parameter analysis, and ablation study are sufficiently comprehensive."}, "weaknesses": {"value": "One of the core idea of this paper is to utilize saliency maps as supervisory signals, decoupling visual prompts into “foreground” and “background” components. While intuitive, this approach raises a significant concern: “Significant” and “usable for classification” do not exhibit a perfect correspondence, and the mask also contains some noise, which casts doubt on the validity of the supervision. From a logical standpoint, the performance ceiling of this method is likely to be significantly constrained by the quality of the significance detector. The paper does not appear to discuss the impact of the quality of the saliency map on the final performance.\n\nThe paper aims to provide a solution for long-tail visual recognition problems, but the solution proposed resembles a generic attention enhancement mechanism rather than a tailored solution targeting the core issue of class imbalance. The fundamental challenge in long-tail problem is the lack of diverse samples, which results in low-quality feature learning.\nThis approach enhances the model's focus salient regions within images, benefiting all categories(including head categories). However, it does not directly address the problem of learning discriminative representations for tail categories. Even if a model demonstrates strong attention guidance toward objects in a tail category, it still need to learn an ideal feature representation from an extremely limited number of samples. The method itself does not inherently achieve feature space rebalancing or enhancement of tail category representations, which are the key of solving the long-tail problem.\nEssentially, the paper appears to be alleviating a symptom of the long-tail problem (e.g., distracted attention due to under-trained tail categories) rather than its root cause (insufficient representation learning stemming from data imbalance). The authors should provide a more in-depth analysis and establish a logical chain to demonstrate how separating foreground and background specifically alleviates the feature learning bottleneck for tail classes."}, "questions": {"value": "Since the authors introduced external supervision signals based on saliency maps for prompt training, why not directly use the saliency maps as prompts, as demonstrated in the ECCV 2024 paper “Attention Prompting on Image for Large Vision-Language Models”? The author may need to provide relevant arguments to explain why their more complex prompt-based guidance mechanism outperforms these simpler, more direct masking strategies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9JjwEbMLXd", "forum": "t697jIOfO8", "replyto": "t697jIOfO8", "signatures": ["ICLR.cc/2026/Conference/Submission19033/Reviewer_ss6g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19033/Reviewer_ss6g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968532381, "cdate": 1761968532381, "tmdate": 1762931072866, "mdate": 1762931072866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a foreground–background decoupled visual prompt tuning method for long-tailed recognition. I thought the heuristic idea generally reasonable. However, the heuristic lacks solid justification whether theoretical or qualitative analysis. The only evidence of motivational is Fig. 1, which uses low-quality images and insufficient examples. In addition, recognition with saliency map has been studied (ref1). The manuscript does not offer insights beyond what is commonly known. It largely reads as a straightforward application of such ideas within visual prompt tuning. Given the modest compelling performance gains, I lean toward rejection.\n\n[ref1] Decoupling Common and Unique Representations for Multimodal Self-supervised Learning, TPAMI2025"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Problem framing is clear: the paper targets the foreground/background attention allocation issue in prompt tuning for long-tailed recognition and proposes saliency-guided attention biasing. The idea is intuitive and easy to follow."}, "weaknesses": {"value": "- Heuristic nature and limited novelty. The core idea (i.e., decoupling prompts into foreground- and background-related components and injecting saliency/inverse-saliency as an attention bias) is essentially a heuristic perturbation to Q–K scoring in self-attention. The paper lacks principled justification, theoretical grounding, or new insights specific to long-tailed recognition or visual prompt learning."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8Ds8FG87WX", "forum": "t697jIOfO8", "replyto": "t697jIOfO8", "signatures": ["ICLR.cc/2026/Conference/Submission19033/Reviewer_hP7a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19033/Reviewer_hP7a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152494713, "cdate": 1762152494713, "tmdate": 1762931072417, "mdate": 1762931072417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}