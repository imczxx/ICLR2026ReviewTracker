{"id": "Z29e9WM0FY", "number": 10289, "cdate": 1758166075207, "mdate": 1759897660522, "content": {"title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs", "abstract": "Logical reasoning is a fundamental aspect of human intelligence and an essential capability for multimodal large language models (MLLMs). Despite the significant advancement in multimodal reasoning, existing benchmarks fail to comprehensively evaluate their reasoning abilities due to the lack of explicit categorization for logical reasoning types and an unclear understanding of reasoning. To address these issues, we introduce **MME-Reasoning**, a comprehensive benchmark designed to evaluate the reasoning ability of MLLMs, which covers all three types of reasoning (*i.e.*, inductive, deductive, and abductive). We carefully curate the data to ensure that each question effectively evaluates reasoning ability rather than perceptual skills or knowledge breadth, and extend the evaluation protocols to cover the evaluation of diverse questions. Our evaluation reveals substantial limitations of SoTA MLLMs when subjected to holistic assessments of logical reasoning capabilities. Even the most advanced MLLMs show limited performance in comprehensive logical reasoning, with notable performance imbalances across reasoning types. In addition, we conducted an in-depth analysis of approaches such as ``thinking mode'' and Rule-based RL, which are commonly believed to enhance reasoning abilities. We hope the community can pay more attention to the comprehensive reasoning capabilities of MLLMs instead of only focusing on its subset, such as Math.", "tldr": "", "keywords": ["Multimodal reasoning", "evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/98c449c4dba83945c129d0bc1d9b672c1532078f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a new benchmark, MME-Reasoning, designed to evaluate the logical reasoning capabilities of multimodal large language models (MLLMs).It comprises 1,188 questions, which are systematically categorized into three reasoning types: inductive, deductive, abductive. The dataset further includes metadata on difficulty (easy/medium/hard), question types (multiple‐choice, free‐form, rule‐based), and capability tags (e.g., pattern analysis, planning & exploring, spatial/temporal). Experiments evaluate a range of state-of-the-art MLLMs (both open-source and closed) across “chat” vs “thinking” modes, which show the cons and pros of current MLLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Curation and metadata richness: The dataset includes multiple dimensions of annotation (reasoning type, difficulty, capability tag, question type) which allows finer‐grained analysis of model behavior rather than only single accuracy numbers, which is critical for both benchmark itself and evaluated models.\n\n2. Focus on reasoning rather than perception and knowledge: The paper tries to filter out questions that are mostly about image recognition or domain‐knowledge recall and push the focus towards reasoning. This improves the validity of the benchmark as a reasoning test.\n\n3. The writing is clear and easy to follow."}, "weaknesses": {"value": "1. Knowledge vs reasoning vs perception boundary: While the authors try to reduce dependence on domain knowledge, the distinction between “reasoning” and “heavy factual knowledge” is somewhat fuzzy. Some tasks may still lean on domain knowledge (e.g., biology diagrams, chemistry processes) which adds potential confounds. Likewise, decoupling reasoning and perception is equally difficult, and I hope to see more evidence of both to prove that the benchmark focuses on reasoning.\n\n2. The novelty may be limited.  I think the motivation of this paper is similar to VisualPuzzles[1], EMMA[2] and RBench-V[3]. I hope the author can elaborate on the differences with these works.\n\n[1]:Song Y, Ou T, Kong Y, et al. VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge[J]. arXiv preprint arXiv:2504.10342, 2025.\n\n[2]. Hao Y, Gu J, Wang H W, et al. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. ICML 2025\n\n[3]. Guo M H, Chu X, Yang Q, et al. RBench-V: A primary assessment for visual reasoning models with multi-modal outputs. NeurIPS 2025.\n\n3. Abductive reasoning definition and annotation: The classification into inductive/deductive/abductive is useful but also somewhat subjective. The criteria for categorisation, inter-annotator agreement (if any) and the boundary cases might be less well described in the paper. This could limit replicability or interpretability of reasoning‐type results."}, "questions": {"value": "Seeing weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8amSKCHueE", "forum": "Z29e9WM0FY", "replyto": "Z29e9WM0FY", "signatures": ["ICLR.cc/2026/Conference/Submission10289/Reviewer_rc6C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10289/Reviewer_rc6C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761539236804, "cdate": 1761539236804, "tmdate": 1762921641932, "mdate": 1762921641932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new comprehensive benchmark called MME-Reasoning, designed to evaluate the logical reasoning capabilities of Multimodal Large Language Models (MLLMs). The authors argue that existing multimodal reasoning benchmarks are insufficient for assessing logical reasoning, mainly because they lack a clear taxonomy and coverage of different reasoning types, inductive, deductive, and abductive, and often conflate perceptual ability or knowledge breadth with reasoning ability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark follows clear design principles (comprehensiveness, going beyond perception, minimizing knowledge dependence, and diverse evaluation formats), and the construction pipeline is well described.\n2. Data are sourced from textbooks, logic workbooks, online resources, exams, existing benchmarks, and author-designed/synthetic problems, with manual filtering to remove items that primarily rely on perception or complex domain knowledge.\n3. A broad set of recent, representative MLLMs (closed/open source, chat/thinking models, and rule-based RL models) are evaluated.\n4. The paper is well structured and clearly written."}, "weaknesses": {"value": "1. The core goal of MME-Reasoning is to systematically assess MLLMs’ logical reasoning—explicitly covering inductive, deductive, and abductive types—while striving to decouple perception and domain knowledge from reasoning. However, many similar benchmarks already exist, e.g.:\n   - Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal Reasoning Benchmark\n   - VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models\n   - MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal Models\n     Their data sources likely also contain abductive items, so this benchmark may not clearly reveal deficiencies of current MLLMs relative to what existing benchmarks already show.\n2. Much of the data comes from academic exams (e.g., K-12 math/physics), standardized logic questions (civil service exams), and hand-crafted puzzles, with limited coverage of real-world reasoning scenarios. This weakens the benchmark’s ability to assess “real-world reasoning.”\n3. Some data (e.g., Chinese civil-service logic questions) may introduce evaluation bias for MLLMs developed by non-Chinese institutions. Such institutions may not train on similar data, whereas Chinese institutions might, making the benchmark less reliable for unbiased capability assessment.\n4. The difficulty levels (easy/medium/hard) are defined mainly by the time human experts need to solve each problem, which introduces subjectivity. It would be better to incorporate more objective metrics or report inter-annotator agreement. The paper defines five core abilities, but how this taxonomy was derived—and whether it is standard or newly proposed—should be further justified for soundness and completeness.\n5. Typographical error: “casual chain analysis” → “causal chain analysis.”"}, "questions": {"value": "Same as the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hzp9Ipy3gu", "forum": "Z29e9WM0FY", "replyto": "Z29e9WM0FY", "signatures": ["ICLR.cc/2026/Conference/Submission10289/Reviewer_guty"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10289/Reviewer_guty"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558517863, "cdate": 1761558517863, "tmdate": 1762921641480, "mdate": 1762921641480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MME-Reasoning, a comprehensive benchmark that evaluates LLMs' logical reasoning capabilities and investigates different models' performance with case studies. The dataset is manually curated, covering all three types of reasoning (i.e., inductive, deductive, and abductive). As findings, the benchmark still poses a significant challenge to existing open and closed source models, signifying room for improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper looks at an interesting aspect of LLM's reasoning capabilities. Unlike standard reasoning benchmarks that evaluate the models on hard, domain-specific knowledge, the proposed benchmark tries to evaluate the logical reasoning capabilities of LLMs on their own. This distinction can eliminate, to some degree, the knowledge bias that different LLMs have and focus on their logical inference capacities. \n2. The paper summarizes a list of insights and findings from evaluating different models, including both open and closed source, on MME-Reasoning. The findings are insightful and shed lights on the current limitations of LLMs, as well as giving insights on what contributes to better reasoning performance. \n3. The paper is effortful both in terms of the manual data curation process, the comprehensiveness of the evaluation, as well as the extensive supplementary study delineating the details about the paper. This clearly shows the amount of effort put into this project."}, "weaknesses": {"value": "1. Since logical inference is an ability common to all human beings, I wonder if it also makes sense to access non-human expert's performance on this benchmark and compare that with that of the models. The table shows that the models lag behind human-experts who are PhD students, I wonder how do the models' performance compares with other groups of people. Do the models already achieve on-par performance or they still lag behind?"}, "questions": {"value": "1. Are there any potential biases in the benchmark collected? It seems that the problems main consists of puzzle solving. What would be some of the reasoning tasks that are not covered by the benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "P5wKQkCFkQ", "forum": "Z29e9WM0FY", "replyto": "Z29e9WM0FY", "signatures": ["ICLR.cc/2026/Conference/Submission10289/Reviewer_Zr1J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10289/Reviewer_Zr1J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708620349, "cdate": 1761708620349, "tmdate": 1762921640893, "mdate": 1762921640893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}