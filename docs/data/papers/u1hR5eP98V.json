{"id": "u1hR5eP98V", "number": 3787, "cdate": 1757521245924, "mdate": 1759898069815, "content": {"title": "gp2Scale: A Class of Compactly-Supported Non-Stationary Kernels and Distributed Computing for Exact Gaussian Processes on 10 Million Data Points", "abstract": "Despite a large corpus of recent work on scaling up Gaussian processes, a stubborn trade-off between computational speed, prediction and uncertainty quantification accuracy, and customizability remains. This is because the vast majority of existing methodologies exploit various levels of approximations that lower accuracy and limit the flexibility of kernel and noise-model designs --- an unacceptable drawback at a time when expressive non-stationary kernels are on the rise in many fields. Here, we propose a methodology we term \\emph{gp2Scale} that allows us to scale exact Gaussian processes to more than 10 million data points without relying on approximations, but instead by working with the existing capabilities of a GP: the kernel design. Highly flexible, compactly supported, and non-stationary kernels lead to the identification of naturally occurring sparse structure in the covariance matrix, which is then exploited for the calculations of the linear system solution and the log-determinant for training. We demonstrate our method's functionality on several real-life datasets and present comparisons to state-of-the-art approximation algorithms. Although we show superiority in approximation performance in many cases, the method's real power lies in the total agnosticism regarding arbitrary GP customizations --- core kernel design, noise, and mean functions --- or the type of input space, making the method optimally suited for modern Gaussian process applications.", "tldr": "The paper proposed a method to scale up exact Gaussian Processes with the help of non-stationary, compactly supported kernels and HPC.", "keywords": ["Gaussian Process", "HPC", "Kernels"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/45e8ec51c0bdeb1280076bf069a632b946348862.pdf", "supplementary_material": "/attachment/dcc8a3ccd9442d64f219ca9d7bfa546e866d9040.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes new kernels for scaling up (exact) Gaussian process for large datasets.\nThe main idea is to use sparse kernel matrices so that it is amenable to distributed computing across multiple GPUs.\n\nTo this end, the authors propose using a product of a non-stationary kernel and a Wendland kernel.\nThe Wendland kernel induces sparse kernel matrices which helps scale up inference, and the non-stationary kernel is supposed to help capture long range interactions in the domain."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper manage to train Gaussian processes on 10 million data points across 1024 GPUs by distributed computing.\nThis seems to be a nontrivial engineering effort, even though the authors mentioned that \"they are much more mundane compared to the kernel designs\".\nI actually think these technical details deserve a spot in the main paper.\nIn particular, I am personally interested in how sparsity patterns are handled and how the matrix blocks are distributed across GPUs."}, "weaknesses": {"value": "1. Most experiments are conducted on datasets with \\\\(\\leq 10^5\\\\) data points.\nThe only truly large dataset is the temperature dataset (Menne et al., 2012) that has 10 million data points.\nIdeally, it would be better to have more diverse datasets in the middle range, e.g., say larger than 100 thousand and smaller a few million.\n    - The missing of CRPS (or similar metrics that capture the quality of the predictive distributions) on the largest dataset is unfortunate, because the current evaluation does not access the predictive variance.\n    - Datasets smaller than \\\\(10^5\\\\) should be well-handled by specialized numerical methods nowadays, e.g., conjugate gradients and the Lanczos algorithm.\n    In fact, given that the authors have access A100 GPUs (assuming the 80G version), even Cholesky decomposition on dense kernel matrices should scale up to \\\\(90000\\\\) data points.\n\n1. There are some discrepancies on how the hyperparameters are learned when comparing to the baselines.\nThe authors mentioned explicitly that gp2scale employs MCMC, while the baselines all seem to use maximum likelihood.\nThus, it is not clear whether the improvements shown in the experiments come from the kernel design (and exact inference) or MCMC.\nIt would be much better to fix the hyperparameter learning method across the experiments, e.g., use maximum likelihood throughout.\n(Learning variational GP hyperparameters by MCMC might be difficult as it would mix variational inference with MCMC.)"}, "questions": {"value": "1. Why the non-stationary kernel manage to capture long range interactions?\n\n1.Why the delta kernel in Eq (12) is very flexible?\nIf \\\\(\\mathbf{x}_i\\\\) and \\\\(\\mathbf{x}_j\\\\) are both outside the training data, isn't the kernel value always zero?\nIn addition, does the kernel described in Section 4.4 used anywhere in the paper?\n\n1. Why the kernel used in gp2scale changes in different datasets?\nHave the authors tried all kernel variants in Section 4 in the experiments?\nIf so, does the performance vary?\n\n1. Do the baselines use Matern kernel (\\\\(\\nu = 1.5\\\\)) throughout the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2ub3Soqynq", "forum": "u1hR5eP98V", "replyto": "u1hR5eP98V", "signatures": ["ICLR.cc/2026/Conference/Submission3787/Reviewer_KYQR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3787/Reviewer_KYQR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943758844, "cdate": 1761943758844, "tmdate": 1762917022759, "mdate": 1762917022759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose gp2Scale, a GP regression methodology that uses compactly supported non-stationary kernels, distributed covariance matrix computation, and MCMC for hyperparameter inference. Several compactly supported non-stationary kernel constructions are presented following standard approaches for kernel design. Scalability to 10M points is demonstrated on a 3D temperature dataset using a cluster with 1024 A100 GPUs. The approach is termed \"exact GP\" since no explicit approximations beyond the kernel choice are introduced."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The systematic construction of non-stationary compactly supported kernels (Sections 4.1-4.5) provides a useful taxonomy for practitioners. The combination kernel offers flexibility for encoding complex non-stationary structure. The paper includes a study on a 3D regression dataset with 10M points using extreme computational resources (1000+ GPUs)."}, "weaknesses": {"value": "The authors state that: “In this work, we argue that the fundamental problem of scaling Gaussian processes (GPs) stems from the misconception that the covariance matrix is inherently dense.” I have to disagree with this sweeping generalization. The problem of designing kernels that lead to covariance matrices with computationally advantageous structure is an old and extensively studied topic. The framing adopted by the authors presents a well-studied area as if it contains a fundamental misconception, when in reality the field has long recognized and exploited covariance structure.\n\nThe non-stationary extension of compactly supported kernels follows from standard kernel construction approaches, not a fundamental breakthrough. The authors state that the core idea has been demonstrated on 5M points by Noack et al. (2023). My impression is that this paper is primarily a software release with benchmarks, not a methodological innovation.\n\nThe paper places emphasis on \"exact\" GP inference versus \"approximate\" methods. Accurate kernel computation is necessary but not sufficient for good predictive performance. The overarching goal is to learn an optimal data-dependent kernel, not to exactly compute some pre-specified kernel that may itself be an ad hoc choice. Kernel approximations (e.g., random Fourier features, inducing points) introduce their own inductive biases that can be beneficial—indeed, approximations to known kernels can sometimes outperform the \"exact\" kernel they approximate because the approximate representation learns a better kernel than the original specification. The ability to exactly represent an initially specified kernel does not guarantee superior generalization.\n\nIf I understand correctly, only the covariance matrix computation is parallelized in the implementation. If so, I am surprised that the covariance matrix factorization and the log determinants were computed on the host-node - a choice that I expect to significantly hurt parallel efficiency. For sparse matrices, covariance computation scales as O(nnz(K)). If a Krylov method is used for computing K^{-1}y, each iteration costs O(nnz(K)) for the matrix-vector product. Even with a modest condition number, the solve phase will require significantly more operations than covariance matrix computation (this would be true even if a sparse direct solver is used). For the log determinant, stochastic trace estimation would also require multiple matrix-vector products. The authors appear to have parallelized the cheapest operation while leaving the dominant computational costs serial. This raises questions about whether the 1000+ GPU deployment is actually achieving meaningful parallel efficiency or simply brute-forcing a poorly designed algorithm.\n\nFor the temperature data, the authors report that the study was carried out on cluster of 1024 A100 GPUs and that one MCMC iteration took 477 seconds. However, there is no discussion of computational cost breakdown into covariance matrix computations and the other steps. \n\nSparse operations scale with nnz(K) not n, but sparsity pattern, fill-in during factorization (if using a direct solver), communication costs in distributed setting all matter enormously. Without this analysis, it is hard to assess if the method is fundamentally efficient or simply brute-forces the problem with massive parallelism."}, "questions": {"value": "- Were baseline implementations ran with non-stationary kernels (e.g., deep kernels) for fair comparison?\n\n- Have the authors considered using deep kernel learning with Wendland kernels as a simpler baseline for non-stationary compact support?\n\n- How were the terms $K^{-1}y$ and $\\log | K|$ computed ? \n\n- For the 10M points test case (1024 GPUs, 477 sec/iteration): what fraction is covariance computation vs computation of $K^{-1}y$ and $\\log | K|$ ? \n\n- What is the parallel efficiency (strong scaling) when going from 1 to 1024 GPUs? Given the serial factorization bottleneck, I suspect it is extremely poor.\n\n- What is $\\text{nnz}(K)/n^2$ for each experiment? \n\n- Given Noack et al. (2023) demonstrated similar ideas at 5M points, what is the core algorithmic innovation beyond scaling the implementation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Br2IteGrPi", "forum": "u1hR5eP98V", "replyto": "u1hR5eP98V", "signatures": ["ICLR.cc/2026/Conference/Submission3787/Reviewer_urA5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3787/Reviewer_urA5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985956622, "cdate": 1761985956622, "tmdate": 1762917022057, "mdate": 1762917022057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is based on the premise that compactly supported kernels can unviel sparse structure in the data and speed up training in exact GPs. They propose several variants of non-stationary kernels which when combined with compactly supported kernels induce the right mix of properties, the other building blocks of the method are distributed computing and block-MCMC.  They propose several kernel constructions from literature, Wendland-style, convolutional, bump-function, and delta-collapsed hybrids to that effect. Across a range of experiments (synthetic data, topography, California housing, and a 10-million-point temperature dataset), gp2Scale achieves accuracy comparable to or better than approximate methods such as SVGP, VNNGP, SKI, and Vecchia, while remaining fully exact."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and clear, they point out all the caveats. \n- There have been many works trying to address scalability in exact GPs, but viewing it through the lens of compactly supported kernels to unveil sparsity and speed up training is a new and, what seems to be an effective approach. \n- The authors  demonstrate that exact GP inference on up to 10 million points is possible using their framework. The large-scale experiments (topography, housing, MNIST, and temperature data) showcase strong empirical performance while maintaining exactness.\n- The gp2Scale approach remains compatible with arbitrary user defined kernels, mean, and noise models. This makes it broadly applicable to diverse scientific domains where bespoke kernel designs are needed..if true this is a major strength but more clarity is needed on how this would work through an example."}, "weaknesses": {"value": "- The authors should be providing a bit more detail about how exactly the distributed computing works, as well as the block MCMC. They just mention it rather than describe / explain. \n- It is unclear whether the goal is accuracy, scalability, or flexibility, the narrative oscillates between all three.\n- Claims that gp2Scale is “exact” and “agnostic to any input space” are not fully substantiated, discrete or structured domains may still pose issues.. I feel the claims maybe a bit overstated. \n- No NLPD, calibration error, or coverage probability is reported, only RMSE ad CRPS. As such it is hard to comment on UQ.\n- The model abandons the standard GP evidence maximization pipeline without clearly discussing trade-offs between sampling and point-estimate optimization for the hypers. \n- the idea is that you can plug in your own kernel and use their compactly supported masks to induce sparsity while retaining the interpretability or domain structure of your base kernel - again need an example of this as i am not sure how the base kernel inductive biases wouldn't get distorted by combining it in this way."}, "questions": {"value": "- For intuition it may be beneficial to visualise heatmaps of the non-stationary kernel variants for fixed hypers and some fixed input grid. \n- Also visualise function samples from the prior for each version. \n- Why not plot p(y) bands as well? how to tell if the noise level estimated is large enough to incorporate the ground truth?\n- Since these kernels cannot easily be integrated into most approximate methods, it’s hard to disentangle whether the gains stem from the kernel design or from the scaling strategy? can the authors comment on that. \n- The authors state that compactly supported non-stationary kernels “discover natural sparsity” leading to favorable scaling, but provide no further proof of that in a quantitative or qualitative sense. Can the authors substantiate this claim? e.g. sparsity patterns in the covariance matrix, statistics on nonzero entries, or empirical scaling curves?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kNhA84Cl6d", "forum": "u1hR5eP98V", "replyto": "u1hR5eP98V", "signatures": ["ICLR.cc/2026/Conference/Submission3787/Reviewer_3F7B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3787/Reviewer_3F7B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3787/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762311913840, "cdate": 1762311913840, "tmdate": 1762917021104, "mdate": 1762917021104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}