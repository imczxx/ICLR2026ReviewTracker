{"id": "im2a2MHoke", "number": 24126, "cdate": 1758353072062, "mdate": 1759896780694, "content": {"title": "Zero-Shot Non-Autoregressive TTS Beyond Autoregressive Models Using Soft Alignment Generation and Residual Modeling", "abstract": "Autoregressive TTS leverages soft alignment generated by the attention mechanism, which provides the decoder with a well-designed context vector. Subsequently, the decoder receives both the semantic representation and the acoustic representation generated at the previous time step. For this reason, autoregressive TTS achieves strong performance. Thus, we propose novel algorithms to bring similar benefits to non-autoregressive TTS. First, we propose a method to distill soft alignments—originally provided by attention in autoregressive models—into a flow matching model trained between mel-spectrograms and text representations. This allows non-autoregressive models to leverage attention-like context vectors without requiring autoregressive decoding. Second, we introduce an invertible encoder, designed based on normalizing flow, to disentangle semantic and residual acoustic representations. The invertible encoder maps the residual information, which is absent in the context vector, closer to a Gaussian distribution. During inference, we can treat the context vector as the semantic representation and Gaussian noise as the acoustic representation. Lastly, to improve zero-shot TTS performance, we propose a prompt-aware lightweight convolution, where the kernel weights are dynamically adjusted for each speech prompt. With the proposed methods, our non-autoregressive TTS model achieves comparable performance to existing autoregressive models.", "tldr": "This study proposes three methods to bridge the performance gap between non-autoregressive TTS and autoregressive TTS.", "keywords": ["Speech synthesis", "zero-shot text-to-speech", "non-autoregressive model", "generative model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2ae708f19760702c81c65dc145ef54315c446189.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The author contributes by proposing methods to enhance non-autoregressive (NAR) text-to-speech (TTS) models:\n1) They introduce a technique to distill soft alignments from autoregressive models into a flow matching model, enabling non-autoregressive models to use attention-like context vectors.\n2) They design an invertible encoder using normalizing flow to separate semantic and acoustic representations.\n3) They propose a prompt-aware lightweight convolution to improve zero-shot TTS performance."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This approach of distilling soft alignments from autoregressive models into a flow-matching model to enable attention-like context vectors in non-autoregressive models is an intriguing concept."}, "weaknesses": {"value": "**[W1]** The evaluation in this paper has significant issues that make the experimental results completely unconvincing. The authors did not use mainstream practices:\n1) LibriSpeech test-clean 1234 samples (VALL-E release);\n2) full set of Seed-TTS test-EN/ZH/Hard (MaskGCT, F5-TTS, CosyVoice 1/2/3);\n\nInstead, they cherry-picked the Seed-TTS test set, creating 180 audio samples from 30 randomly selected speech prompts for evaluation. The WER values of the baselines (Spart-TTS, F5-TTS, MaskGCT) in Table 4 are much worse than the values reported in the original papers. \n\nAdditionally, the authors obtained audio samples from their official demo pages for comparison, which is highly unusual (it's the first time I've seen such an approach) and unreliable.\n\n---\n**[W2]** The majority of the references in the author's work are from three years ago, with only one being from 2025. The author’s approach to NAR's zero-shot TTS model is still based on the encoder-decoder architecture (Transformer-based, T5-based), which has already become outdated in the current era of decode-only models. Therefore, I question the timeliness of this work."}, "questions": {"value": "The following are the revision suggestions:\n\n1) Redo the experiments. For zero-shot TTS, use LibriTTS for training with small data, and use Libriheavy or Emilia for experiments with large data.\n\n2) Redo the evaluation. For the test set in English, use: LibriSpeech test-clean 1234 samples & Seed-TTS test-EN full set with 1000+ samples\n\n3) Clearly report the ASR model variant used for WER, such as the most widely used Whisper Large v3, to avoid ambiguity.\n\n4) Subjective experiments should report more details, and it is recommended to include them in the appendix, such as the number of participants, their native language background, instructions, scoring tables, etc.\n\n5) Add efficiency metrics, including RTF.\n\n6) For the baseline, use SOTA methods such as NAR F5-TTS, ZipVoice, MaskGCT for non-autoregressive (NAR) models, and CosyVoice 2 for autoregressive (AR) models. Using the numbers from the original papers will make the comparison more convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KmDPCV9Tez", "forum": "im2a2MHoke", "replyto": "im2a2MHoke", "signatures": ["ICLR.cc/2026/Conference/Submission24126/Reviewer_D4GQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24126/Reviewer_D4GQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760775786211, "cdate": 1760775786211, "tmdate": 1762942948200, "mdate": 1762942948200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **RisoTTo**, a zero-shot non-autoregressive (NAR) text-to-speech (TTS) model that attempts to transfer the benefits of autoregressive models through three main components:  \n1. **Soft Alignment Generation (SAG)** based on flow matching,  \n2. **Invertible Encoder (IE)** for residual acoustic modeling, and  \n3. **Prompt-Aware Lightweight Convolution (PAL)** conditioned on a 3-second speech prompt.  \n\nThe authors claim that this approach achieves comparable quality to large autoregressive (AR) systems while maintaining faster inference and fewer parameters."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The topic — improving zero-shot NAR TTS — is relevant and timely.  \n- The integration of existing techniques (flow matching, invertible flows, lightweight convolution) might have some limited engineering value."}, "weaknesses": {"value": "### **1. Lack of real novelty**  \n- The paper merely combines known ideas (flow matching, normalizing flow, speaker-conditioned convolution) without introducing new theory or insight.  \n- It ignores recent, stronger NAR paradigms such as diffusion- or consistency-based TTS (e.g., F5-TTS, Ditto-TTS).  \n\n### **2. Unconvincing and inconsistent design**  \n- The model still relies on many modules and losses (SAG, invertible encoder, duration predictor, post-net, etc.), which contradicts the goal of scalability.  \n- The 3-second *fixed-length* prompt embedding limits speaker modeling capacity.  \n- The duration predictor is trained only with an **L2 loss**, producing flat prosody and unnatural rhythm.  \n- The “prompt-aware convolution” is poorly justified and shows negligible improvement.  \n\n### **3. Questionable experimental credibility**  \n- **No demo samples** are provided, which is unacceptable for a TTS paper.  \n- **Table 4** is highly suspicious — latency and SECS values are reported for models (e.g., VALL-E, NaturalSpeech2, T5-TTS) that have **no public code**. It is unclear how these results were obtained.  \n- The model allegedly outperforms systems with 10× more parameters and training data, which is implausible without human evaluation.  \n- The results rely solely on automatic MOS (NISQA), which cannot replace human perceptual tests.  \n\n### **4. Weak evaluation and analysis**  \n- The zero-shot setting is not convincingly tested — no cross-lingual, unseen-speaker, or unseen-domain evidence is provided.  \n- Ablation studies are minimal and do not explain *why* each module helps.  \n\n### **5. Poor reproducibility and transparency**  \n- The paper does not release code, training details, or implementation settings.  \n- Multiple flow-based components and loss functions make the approach unstable and difficult to reproduce.  \n- Some reported results (e.g., SECS across tables) are inconsistent and lack statistical validation."}, "questions": {"value": "1. How were latency and SECS values for VALL-E, NaturalSpeech2, and T5-TTS obtained?  \n2. Why are no audio samples or demos provided?  \n3. How does the model compare against modern diffusion or consistency-based NAR TTS (e.g., F5-TTS, Ditto-TTS)?  \n4. Why use a fixed 3-second prompt and static embedding instead of variable-length conditioning?  \n5. How sensitive is performance to the weighting of multiple losses (Lmel, Lpost, Ldur, LNF, LSAG)?  \n6. Can the authors release implementation or evaluation code for verification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ewFvxzoR7o", "forum": "im2a2MHoke", "replyto": "im2a2MHoke", "signatures": ["ICLR.cc/2026/Conference/Submission24126/Reviewer_NUUy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24126/Reviewer_NUUy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878883483, "cdate": 1761878883483, "tmdate": 1762942947834, "mdate": 1762942947834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RisoTTo, a non-autoregressive (NAR) model for zero-shot text-to-speech (TTS) that aims to incorporate the benefits of autoregressive (AR) models. The authors propose a collection of techniques to bridge the performance gap between NAR and AR systems. These include: 1) a Soft Alignment Generation (SAG) module, based on flow matching, to distill soft alignments from AR attention mechanisms for use in a NAR setting; 2) an Invertible Encoder (IE) based on normalizing flows to model the residual acoustic information not captured by the text-aligned context vector; and 3) a Prompt-Aware Lightweight Convolution (PAL) to improve speaker adaptation. The paper presents experimental results showing that the proposed model achieves performance comparable to several state-of-the-art AR and NAR zero-shot TTS systems, particularly in terms of speaker similarity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles the important and challenging problem of improving non-autoregressive TTS by emulating the strengths of autoregressive models. The core ideas are technically interesting; using flow matching to learn a soft alignment mechanism is a novel approach, and the use of an invertible encoder to explicitly model residual information is a conceptually sound way to enrich the decoder's input. The Prompt-Aware Lightweight Convolution is also a clever, efficient mechanism for speaker conditioning. The experimental results, particularly the high speaker embedding cosine similarity (SECS) scores, are a clear strength, suggesting that the model is very effective at capturing and reproducing the target speaker's voice characteristics."}, "weaknesses": {"value": "My main concern with this work is its overall complexity and fragmented nature. The proposed model is an intricate assembly of many distinct, sophisticated components (flow matching for alignment, a separate invertible encoder for residuals, a specific prompt-aware convolution, a post-net, etc.). This design philosophy feels somewhat contrary to the current trend in the field, which is moving towards more unified, end-to-end, and scalable architectures. \n\nFurthermore, while the combination of these techniques yields good results, the central innovation seems to be quite incremental. The idea of using an invertible encoder or VAE to model residual or stochastic information between text and speech is not entirely new and bears a strong resemblance to prior work. For instance, the general architecture is reminiscent of approaches proposed in papers such as \"VARA-TTS: Non-Autoregressive Text-to-Speech Synthesis based on Very Deep VAE with Residual Attention\" and \"VAENAR-TTS: Variational Auto-Encoder based Non-AutoRegressive Text-to-Speech Synthesis,\" which also use VAEs to model this information gap in NAR TTS. The paper would be strengthened by acknowledging and differentiating itself from these highly relevant prior works."}, "questions": {"value": "I have a few questions that I hope the authors can address to help clarify their contribution:\n1. The model's architecture is quite complex, integrating multiple distinct generative modeling paradigms. Could you comment on the scalability of this \"fragmented\" approach? Do you see a path to simplifying this design while retaining its benefits, in line with the field's move towards more unified models?\n2. The central contribution appears to be the use of an invertible encoder to model the residual information between the mel-spectrogram and the text-aligned context vector. This is a very subtle but important component. How does this approach fundamentally differ from prior works, such as \"VARA-TTS: Non-Autoregressive Text-to-Speech Synthesis based on Very Deep VAE with Residual Attention\" and \"VAENAR-TTS: Variational Auto-Encoder based Non-AutoRegressive Text-to-Speech Synthesis,\" which also use a VAE-based structure to model latent variables for NAR TTS? A direct comparison or discussion would be very insightful.\n3. Given the complexity, could you provide a more detailed ablation or analysis to show which of the three proposed components (SAG, IE, PAL) is most critical for the performance gains? For example, what is the performance if you only use the Invertible Encoder on top of a standard NAR baseline with duration-based upsampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EukjgJs6hI", "forum": "im2a2MHoke", "replyto": "im2a2MHoke", "signatures": ["ICLR.cc/2026/Conference/Submission24126/Reviewer_fcTM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24126/Reviewer_fcTM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921315243, "cdate": 1761921315243, "tmdate": 1762942947635, "mdate": 1762942947635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RisoTTo, a Zero-Shot Non-Autoregressive Text-to-Speech (TTS) system, that aims to bridge the performance gap between non-autoregressive (NAR) and autoregressive (AR) models with the following fetures:\n\nSoft Alignment Generation (SAG) is a flow-matching model that learns soft text–mel alignments distilling the AR attention behavior.\n\nInvertible Encoder (IE) is a normalizing flow–based encoder that disentangles semantic and acoustic residual representations, enabling Gaussian sampling during inference.\n\nPrompt-Aware Lightweight Convolution (PAL) are  dynamically modulated convolutional kernels conditioned on the speaker prompt for improved zero-shot adaptation.\n\nExtensive experiments across multiple datasets (LibriTTS-R, HiFi-TTS, LJSpeech, VCTK, Seed-TTS) show that RisoTTo achieves competitive MOS, SECS, and WER compared to strong AR baselines (T5-TTS, MaskGCT, VALL-E) while being significantly more efficient (33M parameters, ~0.89s latency per 10s audio). \n\nAblation studies verify that IE primarily improves intelligibility, SAG enhances naturalness, and PAL increases speaker similarity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Well-motivated problem: Addressing the quality gap between NAR and AR zero-shot TTS is an important and active research direction.\n\nElegant architecture: The SAG, IE, and PAL modules complement each other to improve contextual representation, residual modeling, and speaker conditioning.\n\nEfficiency: Very lightweight model with low inference latency, which is valuable for deployment."}, "weaknesses": {"value": "Numerous inconsistencies and apparent errors exist in the relevant experimental results and experimental design.\n\nExperimental Reliability and Validity:\n- The latency and WER numbers reported for _VALL-E_, _T5-TTS_, and _NaturalSpeech2_ in Table 4 are fundamentally unreliable because they are derived from **official demo samples** rather than controlled, reproducible experiments.\n- Demo pages do not provide inference-time measurements, and audio files are typically pre-rendered and post-processed, meaning that latency cannot be accurately derived from them.\n- It is unclear how the authors computed latency for these baselines, as official demos do not expose real-time generation logs, model configuration, or hardware conditions.\n- Without identical inference hardware, batch size, and pipeline settings (text frontend, vocoder, postnet, etc.), comparing latency values is meaningless.\n- For  WER demo pages contain very few utterances—often <20 per model—and the spoken texts differ across demos. This violates any controlled variable condition required for fair metric comparison.\n- The mismatch in phonetic content and text length across models directly biases WER values. For example, longer sentences or punctuation differences can inflate WER independently of model quality.\n\nImplausibly Strong Results Given the Small Model Size and Limited Data:\n- The reported zero-shot performance of RisoTTo appears unrealistically strong considering its training scale and model size.\n- The paper claims to train on a few open datasets (LibriTTS-R, HiFi-TTS, LJSpeech) and a model with ~33M parameters.\n- Table 4 shows RisoTTo achieving comparable or superior MOS and SECS scores to systems like F5-TTS and Spark-TTS, which are trained on hundreds of thousands of hours of proprietary or large-scale multi-speaker data.\n- Such a large performance gap—without proportional training data—raises questions about the evaluation setup, the correctness of metric computation, or potential data leakage.\n- Without a fair data and model size comparison, the claim that RisoTTo “matches large-scale zero-shot TTS models” lacks credibility.\n- The claimed zero-shot performance seems implausible given the small model scale and modest training data. Either additional large-scale pre-training was used (which must be disclosed), or the reported numbers are over-optimistic due to limited and uncontrolled evaluation.\n\nLack of Controlled Experimental Setup\n- Table 4 mixes results from different evaluation sources (demo pages, papers, and authors’ experiments\n- The texts, speakers, sampling rates, and vocoders are inconsistent across systems.\n- Therefore, any claim of superiority in MOS, SECS, or WER cannot be scientifically substantiated.\n- The absence of a controlled, unified test set invalidates the quantitative comparison. The authors should rerun all baselines on the same zero-shot evaluation corpus using released checkpoints and report consistent metrics.\n\nThe experimental results in Table 4 appear methodologically flawed. Latency values for demo-based models are meaningless; WER metrics are computed from non-aligned texts and insufficient sample size; and RisoTTo’s performance claims seem inconsistent with its limited training scale. Without controlled evaluation or transparent methodology, Table 4 undermines the credibility of the entire empirical section. The authors must either (1) rerun fair baseline comparisons under identical conditions or (2) remove the questionable numbers from the paper.\n\nThe paper does not provide any demo page or audio samples, which is a major omission for a TTS paper. For any text-to-speech work, especially those claiming improvements in naturalness, speaker similarity, and zero-shot generalization, it is essential to allow reviewers and readers to **listen to generated samples**. The entire evaluation of perceptual quality (MOS, SECS, WER) depends on audio output quality — yet the paper does not share any listening evidence or demonstration website. \n\nMoreover, since Table 4 comparisons rely on demo samples from other models (VALL-E, T5-TTS, etc.), **it is inconsistent and unfair** that the proposed method itself provides no equivalent demo for verification. This makes the evaluation **non-transparent and non-reproducible**, undermining the credibility of the claimed superiority in Table 4 and Table 5. \n\nThe absence of a demo page is a serious flaw for a speech synthesis paper. The authors must provide listening examples of their model’s output—ideally aligned with the same evaluation texts used for WER and MOS calculations—to enable fair and transparent assessment. Without such evidence, it is impossible to judge the claimed improvements in perceptual quality."}, "questions": {"value": "How was the weighting factor λ=10 chosen? The total loss combines heterogeneous objectives (mel distance, flow loss, KL loss) with different scales. Without sensitivity analysis, optimization stability is questionable.\n\nThe paper reports latency and WER for VALL-E, T5-TTS, and NaturalSpeech2 using official demo samples. How were these numbers computed if demo pages do not provide inference logs, and texts differ across samples?\n\nWhy does the paper not include a demo page for listening verification? In TTS research, reviewers must hear the generated audio to judge perceptual quality. Without demos, the automatic MOS values cannot be trusted.\n\nThe paper claims the method “goes beyond autoregressive models,” yet there is no formal analysis showing that the NAR model is more expressive or achieves comparable likelihoods. Could you provide a theoretical justification or empirical evidence for this claim? Without a probabilistic comparison, the “beyond AR” statement reads as marketing rather than a substantiated claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iqfoJZXdxm", "forum": "im2a2MHoke", "replyto": "im2a2MHoke", "signatures": ["ICLR.cc/2026/Conference/Submission24126/Reviewer_63E9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24126/Reviewer_63E9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24126/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970834251, "cdate": 1761970834251, "tmdate": 1762942947449, "mdate": 1762942947449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}