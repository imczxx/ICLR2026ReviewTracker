{"id": "uqxNmKw7DI", "number": 19178, "cdate": 1758294152602, "mdate": 1759897053835, "content": {"title": "Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation", "abstract": "Large Language Models (LLMs) can self-improve through reinforcement learning, where they generate trajectories to explore and discover better solutions. However, this exploration process is computationally expensive, often forcing current methods to assign limited exploration budgets to each task. This uniform allocation creates problematic edge cases: easy tasks consistently succeed while difficult tasks consistently fail, both producing zero gradients during training updates for the widely used Group Relative Policy Optimization (GRPO). We address this problem from the lens of exploration budget allocation. Viewing each task's exploration as an \"item\" with a distinct \"value\" and \"cost\", we establish a connection to the classical knapsack problem. From this, we derive an optimal assignment rule that transfers exploration budgets from easy tasks to challenging ones. When applied to GRPO, our method increases the effective ratio of non-zero policy gradients by 20–40% during training. As a computational \"free lunch\", it also enables substantially larger exploration budgets (e.g., 93 rollouts) for especially challenging tasks—budgets that would be computationally prohibitive under uniform allocation. These improvements translate to meaningful gains on mathematical reasoning benchmarks, with average improvements of 2–4 points and peak gains of 9 points on specific tasks. Notably, achieving comparable performance with traditional homogeneous allocation would require about 2x the computational resources.", "tldr": "", "keywords": ["Reinforcement Learning", "Exploration", "Large Language Models", "Policy Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d3aee1f024e7cb2b3f616b6c9ea5482e01411d08.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identifies a practical bottleneck in RL fine-tuning of LLMs under limited per-task rollout budgets: with homogeneous rollout allocation, many prompts yield all-success or all-failure groups and therefore produce zero gradients under GRPO. The authors formalize exploration-budget allocation as a knapsack problem where each (task, budget) pair is an item with cost and a derived value function Value(N, p) = ProbNonZeroGradient(N, p) × InfoGain(p). They solve the discrete knapsack to reallocate a fixed global budget heterogeneously across prompts. The paper provides theory on required rollouts for non-zero gradients, an implementation integrating with large RL pipelines, and experiments show improved effective-gradient ratios and average benchmark gains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation tied to practical RL failure mode\n- The proposed method is simple and principled\n- The paper evaluates models with varied sizes."}, "weaknesses": {"value": "1. Limited applicability. The method requires one epoch of training to estimate success rates, which is impractical when the dataset is large and training steps are few. Additionally, ssing fixed estimated success rates can also fail under rapidly changing training dynamics.\n2. Narrow evaluation. Experiments are limited to the DAPO-Math-17K dataset, GRPO algorithm, and Qwen models, leaving generality across tasks, algorithms, and model families unverified.\n3. No efficiency analysis. The paper does not report training time, throughput, or compute usage, making it unclear whether the method improves real efficiency.\n4. Lack of design justification. The combination of ProbNonZeroGradient and InfoGain in task value lacks ablation to show its necessity. Moreover, there is also no visualization showing how rollout allocation varies with empirical success rates, which would clarify how the method behaves across different prompt difficulty levels.\n5. Missing related work. The paper overlooks relevant studies on prompt sampling and LLM exploration [1, 2, 3, 4, 5].\n\n[1] Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning\\\n[2] Can prompt difficulty be online predicted for accelerating rl finetuning of reasoning models?\\\n[3] Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay\\\n[4] Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration\\\n[5] The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rOxwkcGd59", "forum": "uqxNmKw7DI", "replyto": "uqxNmKw7DI", "signatures": ["ICLR.cc/2026/Conference/Submission19178/Reviewer_DJud"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19178/Reviewer_DJud"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743675423, "cdate": 1761743675423, "tmdate": 1762931182268, "mdate": 1762931182268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the challenge of inefficient exploration in LLM reinforcement learning, where uniform budget allocation often results in \"easy\" or \"hard\" tasks producing zero policy gradients for algorithms like GRPO.\n\nThis is a real problem in LLM-RL. The paper offered a straightforward solution that seems easy to implement in VeRL. Overall, simple solutions have large practical impacts. This is a good engineering paper. However, if we think ICLR/AI is a science community (people can disagree with me here on whether ICLR/AI is a science or engineering community), then I will hold the paper to the science standard.\n\nThe paradigm explored in the paper is of: estimate x quantity, use a solver, and apply solution, see gains. \n\nIn order to contribute to the science understanding, I would like the authors to do more analysis on the statistics side of the algorithm. I raised my questions later. Overall, I think this is a good paper. I'm nitpicking because it feels like this type of contribution fits well in a company (as an internal memo/doc), but does not encourage future directions/explorations for other scientists. \n\nSince this is my personal opinion, I am perfectly ok if the AC believes otherwise or other reviewers find this work interesting."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Simple and **implementable** solutions can make a large impact for progress in open-source AI community.\n2. The paper is very clearly written, easy to read."}, "weaknesses": {"value": "1. I'm hesitant to call Theorem 1 as a theorem. It's perhaps a \"Remark\"? The concentration bound is alright, but the expected number of rollouts is very straightforward. Maybe you can call it a theorem if you can define your success probability estimator $\\hat{p}$ and see how the estimation quality impact your estimation of rollout, and **bound** your estimation error (difference between $N$ and $\\hat{N}$) -- then that can become your theorem? Again, if this is a company internal memo/doc, you don't need to do this :) \n\n2. The gain over GRPO (Table 1) is not super big except for really small models (1.5B) (where you need a lot rollouts to guarantee high success). You might not have time to try this during rebuttal, but maybe think about agentic tasks where failure is more likely to happen? In there, your method might see a much larger gain across benchmarks like tau2-bench and such."}, "questions": {"value": "1. How do you estimate the expected number of rollouts? The expectation has a close-form equation but you have to estimate `p`, so instead of true p, you have $\\hat{p}$ -- you should be able to derive an error term between $E [N_{first}]$ and your estimator $\\hat E [N_{first}]$. What is it? Also, if the authors truly aim to provide a theoretical contribution to the ML community, they should explain how the gap between $p$ and $\\hat p$ can impact the solver's optimality. \n\n2.  Can the authors explain more on how $\\hat p$ is estimated? I guess this is estimated as an online learning procedure. However, because the model weights keep changing because of GRPO, shouldn't you need to estimate $\\hat p$ at every single update (or do you choose some type of delay factor -- how does this delay factor impact your algorithm's performance?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XtMBrUuxqg", "forum": "uqxNmKw7DI", "replyto": "uqxNmKw7DI", "signatures": ["ICLR.cc/2026/Conference/Submission19178/Reviewer_63Nn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19178/Reviewer_63Nn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836879534, "cdate": 1761836879534, "tmdate": 1762931181843, "mdate": 1762931181843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that uniform exploration budgets across tasks result in wasted computation and stalled learning in RL-based LLM finetuning. The authors reframe the exploration budget allocation problem as a knapsack optimization task, where each prompt is treated as an item with an associated \"value\" (learning potential) and \"cost\" (exploration effort). The proposed Knapsack-RL dynamically reallocates exploration budgets under fixed computational constraints. The method increases the proportion of non-zero policy gradients across responses and demonstrate empirical gains on reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of adaptively allocating exploration budgets across tasks of different difficulty levels is very reasonable.\n- The derivation of the probability of obtaining non-zero gradients and the corresponding sample complexity bounds provides solid intuition for why uniform budgets fail.\n- The authors test the method under different rollout budgets and demonstrate robustness under varying computational constraints.\n- The paper is well-organized."}, "weaknesses": {"value": "- The motivation and goal across the Introduction, Theory, and Method are inconsistent:\n    - Introduction (line 53): \"Hard tasks, which require extensive (could even require more than 100) exploration to find useful trajectories, receive too little effort under a uniform rule. Easy tasks, which require minimal exploration, waste compute by being over-sampled.\" This claims that hard tasks should be allocated more rollouts, while easy ones should receive fewer.\n    - Theorem 1: suggests that both easy and hard tasks require more rollouts to obtain effective gradients probabilistically.\n    - Method (Section 4): when optimizing the core objective $Value(N_i, p_i)$ (line 272), the paper does not clarify what kind of prompts (in terms of difficulty $p_i$) will receive more budget. The paper does not show how this objective can actually guide the allocation (for example, by providing an approximately optimal solution of $N_i$), leaving the validity of the optimization unproven. The contour plot Figure 4 does not provides intuition about the solution of the knapsack problem. Since $InfoGain(p_i)$ approaches zero as $p_i$ tends toward 0 or 1, it can at least be inferred that the budget is unlikely to be assigned to either very easy or very difficult problems, which contradicts the earlier motivation. Experimentally, while the authors show an overall increase in effective gradients at the response level, they do not provide evidence on how the rollout budgets are actually redistributed across prompts. Based on the statement in line 377, \"This stems from dynamically distributing exploration budgets, targeting tasks with mixed successful and failed trajectories\", it seems the method reallocates budget toward medium-difficulty tasks, which can indeed increase the effective gradient ratio but contradicts the earlier motivation.\n- The practical algorithm introduces a fallback operation for $\\hat{p_i} = 1$ or 0 (lines 702–713), which falls outside the scope of the knapsack optimization framework and contradicts its theoretical solution. However, this empirical mechanism proves highly effective in practice, as demonstrated in Figure 16. Moreover, since prompts with $\\hat{p_i} = 1$ or 0 are often numerous and critical, their exclusion from the core optimization process raises concerns about the validity and completeness of the knapsack-based design.\n- To address uneven workload and inefficient resource utilization caused by heterogeneous rollout allocation, the authors employ a rollout balancing strategy (line 714), treating each allocated rollout as an independent job. However, it is unclear whether this introduces higher computational overhead. Although the authors claim \"negligible computational overhead,\" explicit runtime measurements would strengthen this claim.\n- In implementation (line 310), the first epoch uses homogeneous budget allocation, and the adaptive allocation only becomes effective from the second epoch onward. However, RLVR training often runs for only a few epochs, limiting the general applicability of the algorithm. In addition, estimation errors in the success rate can negatively impact performance.\n- Some figures could benefit from clearer legends or font size rescaling to improve readability."}, "questions": {"value": "- Under the same total rollout budget, does heterogeneous rollout allocation incur higher computational cost than homogeneous allocation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iN1EvJ7UUD", "forum": "uqxNmKw7DI", "replyto": "uqxNmKw7DI", "signatures": ["ICLR.cc/2026/Conference/Submission19178/Reviewer_cK1B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19178/Reviewer_cK1B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921279225, "cdate": 1761921279225, "tmdate": 1762931181400, "mdate": 1762931181400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work relates the budget allocation problem in RL for LLM with classical knapsack problem. Based on this, this work derive the assignment rule to transfer exploration budgets from easy to challenging tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of framing the RL trajectory budget via the classical knapsack problem is intriguing and well-motivated.\n2. The paper’s motivation is clearly presented, and the increase in the effective gradient ratio shown in Figure 5 aligns well with that motivation.\n3. The empirical analysis is comprehensive. Figure 4 demonstrates the interplay between success rate, exploration and the value clearly."}, "weaknesses": {"value": "1. The asynchronous estimation of the success rate may introduce errors. Could you elaborate on the rationale for adopting this design and provide a more detailed analysis? Have you compared it against the online estimation techniques mentioned in the paper?\n2. The paper lacks comparisons with method-level baselines, such as approaches that filter easy and hard tasks. Could you include such comparisons?"}, "questions": {"value": "1. The asynchronous estimation of the success rate may introduce errors. Could you elaborate on the rationale for adopting this design and provide a more detailed analysis? Have you compared it against the online estimation techniques mentioned in the paper?\n2. The paper seems lack experimental comparisons with method-level baselines, such as approaches that filter easy and hard tasks [1, 2]. Could you include such comparisons in an aligned setting? Including such comparisons under equivalent computational budgets would better demonstrate the relative advantages of the knapsack-based allocation.\n3. While the paper frames the budget allocation problem as knapsack optimization and claims to derive an \"optimal assignment rule,\" the optimality relies heavily on the definition of $\\text{Value}(N_i, p_i)$,  which rests on several strong simplifying assumptions. Specifically, the multiplicative decomposition $\\text{Value}(N_i, p_i) = \\text{ProbNonZeroGradient}(N_i, p_i) \\times \\text{InfoGain}(p_i)$ implicitly assumes these two components are independent."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MFiGHXEQVu", "forum": "uqxNmKw7DI", "replyto": "uqxNmKw7DI", "signatures": ["ICLR.cc/2026/Conference/Submission19178/Reviewer_vT2w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19178/Reviewer_vT2w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984400603, "cdate": 1761984400603, "tmdate": 1762931180993, "mdate": 1762931180993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}