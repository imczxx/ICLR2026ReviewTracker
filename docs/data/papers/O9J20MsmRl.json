{"id": "O9J20MsmRl", "number": 10202, "cdate": 1758163732968, "mdate": 1763727883681, "content": {"title": "BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation", "abstract": "Diffusion transformers currently lead the field in high-quality video generation, but their slow iterative denoising process and prohibitive quadratic attention costs for long sequences create significant inference bottlenecks. While both step distillation and sparse attention mechanisms have shown promise as independent acceleration strategies, effectively combining these approaches presents critical challenges---training-free integration yields suboptimal results, while separately training sparse attention after step distillation requires prohibitively expensive high-quality video data. To overcome these limitations, we propose $\\textit{BLADE}$, an innovative data-free joint training framework that introduces: (1) an Adaptive Block-Sparse Attention (ASA) mechanism for dynamically generating content-aware sparsity masks to focus computation on salient spatiotemporal features, and (2) a sparsity-aware step distillation paradigm, built upon Trajectory Distribution Matching (TDM), directly incorporates sparsity into the distillation process rather than treating it as a separate compression step and features fast convergence. We validate BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B, and our framework demonstrates remarkable efficiency gains across different scales. On Wan2.1-1.3B, BLADE achieves a 14.10$\\times$ end-to-end inference acceleration over a 50-step baseline. Moreover, on models such as CogVideoX-5B with short video sequence lengths, our framework delivers a robust 8.89$\\times$ speedup. Crucially, the acceleration is accompanied by a consistent quality improvement. On the VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from 0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further corroborated by superior ratings in human evaluations.", "tldr": "", "keywords": ["sparse attention; video generation; step distillation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/99d00a8fe23eabca2672e9b6e3f24e41c049d3d1.pdf", "supplementary_material": "/attachment/6b4e4b607faefea1f95fa3d1d6e96bf7b01ae032.zip"}, "replies": [{"content": {"summary": {"value": "BLADE unifies sparse attention and step distillation to accelerate video diffusion transformers. It introduces Adaptive Block-Sparse Attention (ASA), which dynamically selects salient regions for computation, and integrates this sparsity directly into a Trajectory Distribution Matching (TDM) distillation process. This joint, data-free approach enables efficient few-step video generation without quality loss. Tested on CogVideoX-5B and Wan2.1-1.3B, BLADE achieves up to 14× faster inference and improved VBench-2.0 scores, showing that sparsity-aware distillation yields both speed and fidelity gains"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Unifies two dominant acceleration axes—few-step distillation (via TDM) and sparse attention—by making sparsity part of the distillation loop rather than a post-hoc swap. The results matches current practice in few-step distillation (Trajectory Distribution Matching).\n\n2. A diverse set up metric, including VBench, human evaluation, and PSNR/SSIM.\n\n3. Offering both ASA (inference-only) and ASA-G (distillation-aware with global tokens) gives a practical path for immediate speedups and better quality when (light) training is allowed."}, "weaknesses": {"value": "1. The paper discusses SpargeAttention and VSA but does not include either as baselines. I find ASA (inference-only) to highly resembles SpargeAttention, while VSA represents a trainable sparse attention design closely related to the ASA-G variant.  Moreover, the claim that VSA is limited by video resolution is inaccurate — VSA is not constrained by resolution in their open-sourced implementation. I feel including SpargeAtteniton as baseline is necessary (published at ICML) and including VSA is optional (a more recent work), but the author should discuss the difference.\n\n2.  The distillation component essentially follows Trajectory Distribution Matching (TDM). The overall framework is thus a straightforward combination of two existing techniques. While I find incremental A+B paper to be acceptable, it should be supported by strong results. However,  this paper’s experimental evidence is weak, as discussed in later points.\n\n3. One of the central arguement of this paper is combining sparse attention and distillation in a single training stage is better than doing them sequentially. However, there is not comparions against a. sparse attention tuning and then distillation. b. distillation and then sparse attention tuning.\n\n4. The experimental evaluation is limited to small or medium-sized models and short video sequences at low resolution. No experiments are conducted on Hunyuan Video or Wan 2.1 14B, and no results are presented for long-sequence or high-resolution scenarios — where sparse attention truly matters due to quadratic scaling of attention cost. \n\n5. The paper claims that ASA improves generation quality, but this is not supported by sufficient evidence. On Wan 1.3B, full attention after distillation achieves a higher VBench score. The only improvement shown is on CogVideoX, an arguably old model released last year, which does not strongly support the generality of the quality improvement claim.\n\n6. In Table 2, the authors evaluate on the H20 GPU but use FlashAttention-2 (FA2) as the dense baseline. On H20, FlashAttention-3 (FA3) should be the standard baseline, as it is roughly 40% faster than FA2. Given the reported effective sparsity rate of 0.798, a theoretical 5× speedup over FA3 is expected, yet only a 3.3× gain over FA2 is reported — indicating significant implementation inefficiencies and optimization headroom that are not analyzed.\n\n7. This claim  “Moreover, on models such as CogVideoX-5B with short video sequence lengths, our framework delivers a robust 8.89× speedup” is misleading in the abstract, the speedup mostly comes from distillation (7.93), putting this sentence make people think sparse attention play a huge part.\n\n8. I believe DMD2 alone can reduce the number of inference steps to 3 or 4 steps on Wan 2.1, which is faster than the proposed sparse attention + TDM solution."}, "questions": {"value": "See weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zLj8chEsUL", "forum": "O9J20MsmRl", "replyto": "O9J20MsmRl", "signatures": ["ICLR.cc/2026/Conference/Submission10202/Reviewer_Sg3J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10202/Reviewer_Sg3J"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761435137810, "cdate": 1761435137810, "tmdate": 1762921565081, "mdate": 1762921565081, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BLADE, which unifies Adaptive Block-Sparse Attention (ASA) and Sparsity-Aware Step Distillation, introducing a new sparse attention operator to enhance efficiency. The method achieves 8.9×–14.1× acceleration on CogVideoX-5B and Wan2.1-1.3B with maintained or improved quality. The idea is clear, technically solid, and promising for efficient video generation, though validation on larger models (e.g., Wan2.1-14B) is still needed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Effective under both training-free and distillation-based settings.  \n\n- Large speedups with stable quality (VBench and human evaluation confirmed).  \n\n- Robust at high sparsity (~80%), outperforming similar methods.  \n\n- Detailed pseudocode and source code are provided, making the method easy to follow and reproduce."}, "weaknesses": {"value": "- Lacks large-scale and long-sequence experiments;\n\n- ASA is currently implemented in a custom Triton kernel and Block Sparse Attention library, and a more detailed analysis of the runtime contribution of each component would be helpful."}, "questions": {"value": "Have you considered including training-free results on larger models such as Wan2.1-14B to strengthen the evaluation?\n\nCould you provide a more detailed breakdown of the runtime for each component of ASA to better explain the performance gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p2y4gD6IxX", "forum": "O9J20MsmRl", "replyto": "O9J20MsmRl", "signatures": ["ICLR.cc/2026/Conference/Submission10202/Reviewer_WLSr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10202/Reviewer_WLSr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553193576, "cdate": 1761553193576, "tmdate": 1762921564500, "mdate": 1762921564500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BLADE, a framework that integrates Adaptive Block-Sparse Attention (ASA) with step distillation for efficient video generation. It proposes a data-free joint training approach, leveraging ASA to generate dynamic, content-aware sparsity masks and sparsity-aware Trajectory Distribution Matching (TDM) to enhance quality. Experiments on CogVideoX-5B and Wan2.1-1.3B demonstrate significant speedups (up to 14.10×) and quality improvements, validated by VBench-2.0 and human evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Integration of adaptive block-sparse attention with step distillation, enabling data-free joint training for efficient video generation.\n- ASA mechanism dynamically generates content-aware sparsity masks that enable high sparsity levels, achieving hardware-friendly acceleration without quality loss when combined with distillation training.\n- Demonstrates substantial speedups (up to 14.10×) on diverse models like CogVideoX-5B and Wan2.1-1.3B, with consistent quality improvements on VBench."}, "weaknesses": {"value": "This paper lacks details on experimental settings and comparative results, for example:\n- Lack of reporting on specific GPU hours, training batch size, and memory usage for the 100-200 distillation iterations.\n- Lack of inference results demonstrating video quality across low-to-high sparsity levels to illustrate the impact."}, "questions": {"value": "Please refer to the **Weaknesses** above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XWDrRKmJda", "forum": "O9J20MsmRl", "replyto": "O9J20MsmRl", "signatures": ["ICLR.cc/2026/Conference/Submission10202/Reviewer_asWM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10202/Reviewer_asWM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797917905, "cdate": 1761797917905, "tmdate": 1762921564146, "mdate": 1762921564146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents BLADE, a novel framework designed to accelerate video diffusion models. BLADE significantly improves inference speed while maintaining or even enhancing generation quality by combining the dynamic, content-aware Adaptive Block-Sparse Attention (ASA) mechanism with the data-free Trajectory Distribution Matching (TDM) distillation process. Extensive experiments on various video models demonstrate significant improvements in kernel-level efficiency, end-to-end inference speed, and generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The innovative BLADE framework effectively addresses the computational bottleneck in accelerating inference for video diffusion models by jointly training the sparse attention mechanism (ASA) with trajectory distillation (TDM). This solution not only accelerates the generation process but also maintains high-quality outputs, especially in high sparsity conditions, achieving high-quality video generation with fewer steps, outperforming traditional methods. The paper is clearly motivated, well-written, and the diagrams are easy to understand, ensuring good readability."}, "weaknesses": {"value": "1. Although ASA's performance is compared with traditional sparse attention methods (e.g., STA, RaA, SVG), the paper does not delve into the impact of different sparsity patterns (e.g., varying threshold settings, block sizes) on generation quality. Ablation experiments with different sparse configurations could provide further insights.\n2. BLADE optimizes generation performance by jointly training sparse attention and trajectory distribution matching. The core innovation here is the fusion of sparsity with the distillation process. However, there is a lack of further experimental evidence to demonstrate the advantages of joint training."}, "questions": {"value": "1. The paper mentions that ASA performs excellently in accelerating the generation process, but increasing sparsity might negatively impact generation quality. Specifically, how can the generation quality be ensured when sparsity is very high, while still maintaining significant computational acceleration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x5bnA5MZkl", "forum": "O9J20MsmRl", "replyto": "O9J20MsmRl", "signatures": ["ICLR.cc/2026/Conference/Submission10202/Reviewer_YBVV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10202/Reviewer_YBVV"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10202/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902098159, "cdate": 1761902098159, "tmdate": 1762921563751, "mdate": 1762921563751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}