{"id": "36CzrZYEch", "number": 14485, "cdate": 1758237064134, "mdate": 1763718998282, "content": {"title": "IRIS: Intrinsic Reward Image Synthesis", "abstract": "Despite the success of Reinforcement Learning from Human Feedback (RLHF) in language reasoning, its application to autoregressive Text-to-Image (T2I) generation is often constrained by the limited availability of human preference data. This paper explores how an autoregressive T2I model can learn from internal signals without relying on external rewards or labeled data. Contrary to recent findings in text generation, we show that maximizing self-uncertainty, rather than self-certainty, improves image generation. We observe that this is because autoregressive T2I models with low uncertainty tend to generate simple and uniform images, which are less aligned with human preferences. Based on these observations, we propose **IRIS** (**I**ntrinsic **R**eward **I**mage **S**ynthesis), \nthe first framework to improve autoregressive T2I models with reinforcement learning using only an intrinsic reward. Empirical results demonstrate that applying IRIS to  autoregressive T2I models achieves performance that is competitive with or superior to external rewards.", "tldr": "", "keywords": ["vision-language models", "reinfocement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/894faecd7798bf6037d643ba50e22903c5ee75f0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces IRIS, which is a RL reward based on minimizing self-confidence / maximizing self uncertainty. The authors propose this objective and use GRPO to train an autoregressive text to image model. The performance of the IRIS model is on par with other methods which use a reward signal for training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well written and does a good job of communicating the idea\n- The ablations are good.\n- I appreciate the novelty, and I appreciate the insight that encouraging mode covering behavior in autogregressive image generators trends towards human preferences."}, "weaknesses": {"value": "- This approach seems limited to optimizing for human preferences as measured by these specific benchmarks. If for example, I wanted to fit to the preference of some specific group, then this method would not work.\n- The motivation for choosing this method over alternatives isn't entirely clear to me. For instance, would the approach remain effective with a different base model than Janus Pro? Would this generalize better to other human preference benchmarks?\n- I'd appreciate more understanding into the underlying mechanisms of this behavior. Is this an emergent property of the pretraining phase, or something else?\n- I am surprised that figure 2 allows one to draw this conclusion. For example, [0] and other papers indeed observe an increase in entropy in addition to an increase in reward. I notice that the GRPO for the LLM is on a MATH task, which entropy does usually decrease.\n\n- Unimportant but appendix b.1 refers to GRPO as Generative Reward Process Optimization, instead of Group Relative Policy Optimization.\n\n[0] Tonyi Deep Research, https://arxiv.org/pdf/2510.24701"}, "questions": {"value": "- is IRIS more robust to multiple benchmarks? why would i prefer iris over other methods?\n- what is the hypothesis for this working?\n- does this method work for other families of models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TBYhOCzjRR", "forum": "36CzrZYEch", "replyto": "36CzrZYEch", "signatures": ["ICLR.cc/2026/Conference/Submission14485/Reviewer_kaX6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14485/Reviewer_kaX6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788700360, "cdate": 1761788700360, "tmdate": 1762924885546, "mdate": 1762924885546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response"}, "comment": {"value": "Thanks for the reviewers for the valuable feedbacks. We address some common questions here. \n\n- **Motivation and contributions** \n\nWe updated our contributions in Section 1. \n\n    - We propose IRIS, the first RL-based alignment method for text-to-image generation using only an intrinsic reward}. We minimize the self-certainty of the T2I models.\n    - We observe and confirm that the model's self-certainty exhibits task-dependent behaviors. Specifically, higher self-certainty benefits the model in domains requiring objective reasoning (e.g., mathematical and code reasoning), but lower self-certainty benefits the model in subjective generation tasks (e.g., text-to-image generation).\n    - Empirically, we show that IRIS can improve the autoregressive T2I models without any external supervision. Post-training by IRIS achieves competitive or better performance than training with external rewards. We call for better external reward models to capture features that are beyond the capabilities of the intrinsic model signals.\n\n- **Typo correction**\n\nSorry for the confusion. There is a typo in Equation (2): $${\\rm SC}(o_t|q,o_{<t}) := +{\\rm KL}(U\\|\\pi_{\\theta}(o_{t}|q,o_{<t}))$$ is the correct definition. This definition is aligned with the paper's discussion. \n\n\n- **Reward aggregation details**\nWe have discussed in Section 3.2 that\n```\nFor the image tokens, as we discussed in~\\cref{fig:Intro}, overly confident models usually generate uniform and plain figures, whereas models with a moderate confidence can generate images with richer and more diverse features. For text tokens, we argue that maximizing NSC encourages the generation of more diverse semantic CoTs, thereby facilitating better exploration during training.\n```\nSorry for the confusion for the reviewers. We make it more clear here. \nIn the autoregressive T2I models, the generated output $o_i$ consists of text and image tokens, denoted by Let $o_{i,\\text{text}}$ and $o_{i,\\text{img}}$ respectively. We simply concatenate $o_{i,\\text{text}}$ and $o_{i,\\text{img}}$ to be $o_t$. Therefore, our objective is to maximize the uncertainty (NSC) for both text and image tokens. We have added this discussion in Section 3.2. \n\n- **Additional benchmarks**\nWe conduct experiments on TIIF-Bench. TIIF-Bench (Text-to-Image Instruction Following Benchmark) is a comprehensive, and difficulty-graded benchmark designed to assess modern T2I models' ability to follow complex textual instructions, addressing limitations like simplistic prompts and coarse evaluation found in prior benchmarks. It has three major catgeries:{Basic Following} ({Attribute}, {Relation}, {Reasoning}), {Advanced Following} ({Attribute+Relation}, {Attribute+Reasoning}, {Relation+Reasoning},{Style},{Text}), and {Real World Following}. For each prompt, this benchmark contains the {short} and {long} versions to more systematically evaluate the model's ability to follow short and long instructions. Please see the entire results in Table B.7. In summary, IRIS achieves better performance given short prompts (IRIS (69.64) vs T2I-R1 (64.18) vs base (61.29)).\n\n- **Beyond auto-regressive models**\n\nOur method is currently limited to autoregressive (AR) text-to-image models. The primary reason is that its foundation in RL requires calculating the next-token probability distribution. Applying RL to other types of text-to-image models, such as flow matching or masked models, is challenging. AR models simplify this because they directly model the per-token log-probabilities. Consequently, the sequence-level log-probability $\\left(\\log\\pi_{\\rm AR}(o|q)=\\sum_{i=1}^t\\log\\pi_{\\rm AR}(o_t|q,o_{t-1})\\right)$ can be easily computed through the chain rule using a single forward pass.\nRecently, [2] proposed Flow-GRPO, which first integrates policy gradient RL into flow matching models. However, these models do not operate on discrete tokens like Janus-Pro or Skywork-UniPic. Instead, their update rule involves continuous values, as seen in $x_{t+\\delta t}-x_t=\\mu(x_t,t) \\delta t + \\mathcal{N}(0, \\sigma_t\\sqrt{\\delta t})$. This means the policy is an isotropic Gaussian distribution with fixed $\\sigma_t$, and its forward KL divergence with respect to the uniform distribution is a non-parameterized constant that cannot be optimized directly. However, there is a growing body of work that focuses on manipulating $\\sigma_t$ directly to achieve better exploration during GRPO post-training [4,5]. Although our method approaches the problem from a different perspective, we share the same underlying motivation of enhancing exploration.\nFurthermore, masked text-to-image models lack an autoregressive structure and thus do not have the sequential factorization of the sequence log-probability. Therefore, current AR-based RL algorithms, such as GRPO, cannot be directly applied to them. Addressing these models with RL, as seen in works like [2] and [3], is a research direction separate from our current focus."}}, "id": "8lFVal7IrB", "forum": "36CzrZYEch", "replyto": "36CzrZYEch", "signatures": ["ICLR.cc/2026/Conference/Submission14485/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14485/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14485/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763718188630, "cdate": 1763718188630, "tmdate": 1763718525621, "mdate": 1763718525621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes IRIS, a label-free RL fine-tuning scheme for text-to-image models (Janus-Pro) that replaces external reward models with an intrinsic token-level signal called Negative Self-Certainty (NSC), defined as the forward KL from a uniform distribution to the model’s next-token distribution; the authors consider both text and image tokens and ultimately use NSC for both. The reward is optimized with a GRPO objective. Experiments compare three reward schemes—external, SC, and NSC—on Janus-Pro and evaluate early training (first ~800 steps) on GenEval, T2I-CompBench, and WISE, where IRIS reports gains over the base models and competitive performance with the external-reward baseline."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- IRIS uses an intrinsic reward (their NSC) instead of any human labels or domain-specific verifiers, which makes the setup lightweight and potentially scalable to new domains. \n- The GRPO objective and token-level advantage computation are written explicitly, which lowers the barrier for reproduction or adaptation.\n- The plots in the experimental results consistently report improvements early in training."}, "weaknesses": {"value": "- This paper’s statements about self-confidence are mistaken. As written, IRIS is actually maximizing self-confidence, not minimizing it. The paper’s prose repeatedly says the opposite, but the mathematics and the reward used in training point the other way. Eq. (2) defines Self-Certainty (SC) at a token as $SC = -{KL}(U\\ \\Vert\\ \\pi_\\theta)$, contradicting the typical definition [r1]. \n- There is a severe mischaracterization of the proposed method IRIS. The papers asserts that forward KL to uniform is “mode-covering,” contrasting it with entropy; but with the paper’s own definition (forward KL: ${KL}(U\\Vert \\pi)$), increasing NSC pushes peaky (confidence-seeking) distributions, not mode-covering.\n- Metric terminology drifts mid-paper. Fig. 2’s paragraph says they “compute the self-confidence measured by ${KL}(U\\Vert \\pi)$,” which is NSC by their Eq. (2), while elsewhere “self-certainty” is plotted and discussed, resulting in inconsistent naming (SC vs. NSC vs. “confidence”).\n- Novelty relative to prior work (INTUITOR [r1]) is thin at the algorithmic core. IRIS’s intrinsic reward is exactly the token-level forward KL to uniform, used inside GRPO—the same scalar that INTUITOR optimizes (called “self-certainty” there) using the same GRPO algorithm. The contribution is primarily domain/application (T2I) and pipeline choices (semantic CoT), not a novel objective. This should be explicitly acknowledged and compared.\n- Gains in the experimental results may stem from the semantic CoT stage rather than the intrinsic reward per se. While the appendix shows with/without CoT comparisons, the main results and claims still bundle CoT with IRIS; stronger controls are needed (e.g., CoT-matched baselines and cross-combinations with external-reward pipelines).\n- Although IRIS trains without external verifiers, most metrics are pretrained reward/evaluator models (HPSv2, etc). Improvements against these may reflect alignment with those evaluators rather than human preference; explicit human-preference studies can only be implied with blinded A/B tests.\n\n[r1] Zhao, X., Kang, Z., Feng, A., Levine, S. and Song, D., 2025. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590."}, "questions": {"value": "- What exactly is “self-confidence” in Fig. 2? The text says you “compute the self-confidence measured by KL between uniform and the model’s distribution.” Is this NSC (i.e., ${KL}(U\\|\\pi)$)? If so, please clarify terminology and axes so readers don’t confuse SC with NSC. \n- Why NSC on both text and image tokens? You conclude “using NSC as the intrinsic reward in both text and image tokens can achieve the best results.” Could you share ablations with mixed choices (e.g., NSC on image, SC on text), modality-specific weights, or temperature controls to support this decision?\n- Many plots focus on the first ~800 steps. Do your gains persist or change at longer horizons, and how does variance across random seeds look over full training? Please include long-run curves and seed-wise spreads. \n- The reward NSC is differentiable against the model parameters as it’s an analytic function of the model’s softmax probabilities, hence smoothly differentiable w.r.t. the logits/parameters. It means that it can be direclty optimized using gradient-based algorithm like SGD. What is the meaning of using RL to optimize it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xa0XG5yP4Q", "forum": "36CzrZYEch", "replyto": "36CzrZYEch", "signatures": ["ICLR.cc/2026/Conference/Submission14485/Reviewer_TNZX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14485/Reviewer_TNZX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890547286, "cdate": 1761890547286, "tmdate": 1762924884614, "mdate": 1762924884614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response (continue)"}, "comment": {"value": "- **Human preference survey**\n\nUnluckily we are unable to conduct large scale human preference survey in a university research lab. We conduct a small scale human preference collection. Besides, since the reward models we used, such as HPSv2, are trained on human preferences, we believe can represent the general human preference on images. \n\nFor the human evaluation, we randomly sample 100 prompts from the three benchmarks (WISE, T2I-Compbench, GenEval). Due to limited resources, we only evaluated three checkpoints: (1) *Janus-Pro-1B*: base generation model, (2) *T2I-R1* (external reward): Janus-Pro-1B finetuned with four external rewards, and (3) *IRIS* (internal reward): Janus-Pro-1B finetuned with our intrinsic reward. For (2) and (3), we use the best checkpoints selected on WISE. We then ask 9 human evaluators to choose the best image among the three for each prompt and record the number of times each model is upvoted. The following table shows that our method achieves performance comparable to the external-reward-based model.\n\n| Method             | Janus-Pro-1B | T2I-R1 External reward | IRIS (Internal reward) |\n|--------------------|-------------:|------------------------:|-----------------------:|\n| Upvoting Rate               |         0.13 |                    0.42 |                   0.45 |\n\n\n[1] Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590, 2025b.\n[2] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025.\n[3] Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025a\n[4] Li, J., Cui, Y., Huang, T., Ma, Y., Fan, C., Yang, M., & Zhong, Z. (2025). Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802.\n[5] He, X., Fu, S., Zhao, Y., Li, W., Yang, J., Yin, D., ... & Zhang, B. (2025). Tempflow-grpo: When timing matters for grpo in flow models. arXiv preprint arXiv:2508.04324."}}, "id": "H3nnGsjr44", "forum": "36CzrZYEch", "replyto": "36CzrZYEch", "signatures": ["ICLR.cc/2026/Conference/Submission14485/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14485/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14485/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763718253282, "cdate": 1763718253282, "tmdate": 1763718253282, "mdate": 1763718253282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IRIS (Intrinsic Reward Image Synthesis), a reinforcement learning framework designed to improve text-to-image generation without relying on external human feedback or labeled data. The key idea is to use intrinsic rewards derived from the model’s own self-uncertainty, showing that maximizing uncertainty—rather than minimizing it—leads to more diverse, detailed, and human-preferred images. The authors demonstrate that autoregressive T2I models with low uncertainty tend to produce oversimplified and uniform results, while optimizing intrinsic uncertainty enhances visual richness and alignment with user intent. Empirical experiments confirm that IRIS effectively improves generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a interesting observation of the relationship between model confidence and reward in the training process, and the different patterns between text generation and image generation.\n\n2. This paper proposes a reinforcement learning method that improves text-to-image generation without external reward or human annotation. After training with intrinsic reward, the model outperforms the baseline model by a large margin, and achieves comparable performance with external reward-guided training.\n\n3. This paper is motivated by a observation presented in Figure2, and fluently leads to the method design."}, "weaknesses": {"value": "1. The performance using intrinsic reward is slightly lower than using external reward. It is sometimes acceptable to achieve slightly lower performance without the need of external reward, but in some cases people may prefer a higher overall performance.\n\n2. Captions of figure 5 and 6 need to switch. The captions of some other figures also seem unclear. It would be better to summarize the key observations of each figure in the caption.\n\n3. In the experiment, the authors compare the best checkpoints of T2I-R1 and IRIS, which may lead to unfair comparison. It would be better to compare them at the same training steps. Since RL training are often unstable and costly, sometimes the training step is also an important factor that we want to compare. Besides, in figure 3, it seems T2I-R1 consistently outperform IRIS by a large margin. This does not well align with the claimed results in Table1."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jL6pUzmHDJ", "forum": "36CzrZYEch", "replyto": "36CzrZYEch", "signatures": ["ICLR.cc/2026/Conference/Submission14485/Reviewer_ggZ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14485/Reviewer_ggZ2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982066221, "cdate": 1761982066221, "tmdate": 1762924884031, "mdate": 1762924884031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IRIS (Intrinsic Reward Image Synthesis), a reinforcement learning framework for text-to-image (T2I) generation that uses negative self-certainty (NSC) as an intrinsic reward. Unlike RLHF or external reward-based approaches, IRIS optimizes autoregressive T2I models without human labels or domain-specific verifiers. The key insight is that reducing self-confidence in image token predictions enhances visual richness and diversity. Experiments on Janus-Pro across GenEval, T2I-CompBench, and WISE benchmarks show IRIS performs competitively or better than models trained with external rewards."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[+] Challenges standard RL intuition demonstrates that minimizing self-confidence improves image diversity.\n\n[+] Defines intrinsic rewards via forward KL divergence; mathematically grounded and well-motivated.\n\n[+] Broad evaluation across multiple benchmarks and ablations (text vs. image tokens, KL direction, semantic CoT)."}, "weaknesses": {"value": "[-] Reward aggregation details (text/image scaling, normalization) are unclear.\n\n[-] Not includes the comparison with preference- or diffusion-based RL.\n\n[-] There are no human studies to demonstrate that visual preference."}, "questions": {"value": "1. How are text and image token rewards normalized or weighted—are they on comparable scales?\n1. Has IRIS been tested on diffusion or masked models beyond autoregressive Janus-Pro?\n1. Any user preference studies confirming that lower self-confidence yields subjectively better images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yFVGmQhzsO", "forum": "36CzrZYEch", "replyto": "36CzrZYEch", "signatures": ["ICLR.cc/2026/Conference/Submission14485/Reviewer_ZMs2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14485/Reviewer_ZMs2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996897428, "cdate": 1761996897428, "tmdate": 1762924883099, "mdate": 1762924883099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}