{"id": "36CzrZYEch", "number": 14485, "cdate": 1758237064134, "mdate": 1759897367472, "content": {"title": "IRIS: Intrinsic Reward Image Synthesis", "abstract": "Despite the success of Reinforcement Learning from Human Feedback (RLHF) in language reasoning, its application to autoregressive Text-to-Image (T2I) generation is often constrained by the limited availability of human preference data. This paper explores how an autoregressive T2I model can learn from internal signals without relying on external rewards or labeled data. Contrary to recent findings in text generation, we show that maximizing self-uncertainty, rather than self-certainty, improves image generation. We observe that this is because autoregressive T2I models with low uncertainty tend to generate simple and uniform images, which are less aligned with human preferences. Based on these observations, we propose **IRIS** (**I**ntrinsic **R**eward **I**mage **S**ynthesis), \nthe first framework to improve autoregressive T2I models with reinforcement learning using only an intrinsic reward. Empirical results demonstrate that applying IRIS to  autoregressive T2I models achieves performance that is competitive with or superior to external rewards.", "tldr": "", "keywords": ["vision-language models", "reinfocement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8bea2ed1584e7cf866f203b071e1fc52ca759baa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces IRIS, which is a RL reward based on minimizing self-confidence / maximizing self uncertainty. The authors propose this objective and use GRPO to train an autoregressive text to image model. The performance of the IRIS model is on par with other methods which use a reward signal for training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is well written and does a good job of communicating the idea\n- The ablations are good.\n- I appreciate the novelty, and I appreciate the insight that encouraging mode covering behavior in autogregressive image generators trends towards human preferences."}, "weaknesses": {"value": "- This approach seems limited to optimizing for human preferences as measured by these specific benchmarks. If for example, I wanted to fit to the preference of some specific group, then this method would not work.\n- The motivation for choosing this method over alternatives isn't entirely clear to me. For instance, would the approach remain effective with a different base model than Janus Pro? Would this generalize better to other human preference benchmarks?\n- I'd appreciate more understanding into the underlying mechanisms of this behavior. Is this an emergent property of the pretraining phase, or something else?\n- I am surprised that figure 2 allows one to draw this conclusion. For example, [0] and other papers indeed observe an increase in entropy in addition to an increase in reward. I notice that the GRPO for the LLM is on a MATH task, which entropy does usually decrease.\n\n- Unimportant but appendix b.1 refers to GRPO as Generative Reward Process Optimization, instead of Group Relative Policy Optimization.\n\n[0] Tonyi Deep Research, https://arxiv.org/pdf/2510.24701"}, "questions": {"value": "- is IRIS more robust to multiple benchmarks? why would i prefer iris over other methods?\n- what is the hypothesis for this working?\n- does this method work for other families of models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TBYhOCzjRR", "forum": "36CzrZYEch", "replyto": "36CzrZYEch", "signatures": ["ICLR.cc/2026/Conference/Submission14485/Reviewer_kaX6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14485/Reviewer_kaX6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788700360, "cdate": 1761788700360, "tmdate": 1762924885546, "mdate": 1762924885546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes IRIS, a label-free RL fine-tuning scheme for text-to-image models (Janus-Pro) that replaces external reward models with an intrinsic token-level signal called Negative Self-Certainty (NSC), defined as the forward KL from a uniform distribution to the model’s next-token distribution; the authors consider both text and image tokens and ultimately use NSC for both. The reward is optimized with a GRPO objective. Experiments compare three reward schemes—external, SC, and NSC—on Janus-Pro and evaluate early training (first ~800 steps) on GenEval, T2I-CompBench, and WISE, where IRIS reports gains over the base models and competitive performance with the external-reward baseline."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- IRIS uses an intrinsic reward (their NSC) instead of any human labels or domain-specific verifiers, which makes the setup lightweight and potentially scalable to new domains. \n- The GRPO objective and token-level advantage computation are written explicitly, which lowers the barrier for reproduction or adaptation.\n- The plots in the experimental results consistently report improvements early in training."}, "weaknesses": {"value": "- This paper’s statements about self-confidence are mistaken. As written, IRIS is actually maximizing self-confidence, not minimizing it. The paper’s prose repeatedly says the opposite, but the mathematics and the reward used in training point the other way. Eq. (2) defines Self-Certainty (SC) at a token as $SC = -{KL}(U\\ \\Vert\\ \\pi_\\theta)$, contradicting the typical definition [r1]. \n- There is a severe mischaracterization of the proposed method IRIS. The papers asserts that forward KL to uniform is “mode-covering,” contrasting it with entropy; but with the paper’s own definition (forward KL: ${KL}(U\\Vert \\pi)$), increasing NSC pushes peaky (confidence-seeking) distributions, not mode-covering.\n- Metric terminology drifts mid-paper. Fig. 2’s paragraph says they “compute the self-confidence measured by ${KL}(U\\Vert \\pi)$,” which is NSC by their Eq. (2), while elsewhere “self-certainty” is plotted and discussed, resulting in inconsistent naming (SC vs. NSC vs. “confidence”).\n- Novelty relative to prior work (INTUITOR [r1]) is thin at the algorithmic core. IRIS’s intrinsic reward is exactly the token-level forward KL to uniform, used inside GRPO—the same scalar that INTUITOR optimizes (called “self-certainty” there) using the same GRPO algorithm. The contribution is primarily domain/application (T2I) and pipeline choices (semantic CoT), not a novel objective. This should be explicitly acknowledged and compared.\n- Gains in the experimental results may stem from the semantic CoT stage rather than the intrinsic reward per se. While the appendix shows with/without CoT comparisons, the main results and claims still bundle CoT with IRIS; stronger controls are needed (e.g., CoT-matched baselines and cross-combinations with external-reward pipelines).\n- Although IRIS trains without external verifiers, most metrics are pretrained reward/evaluator models (HPSv2, etc). Improvements against these may reflect alignment with those evaluators rather than human preference; explicit human-preference studies can only be implied with blinded A/B tests.\n\n[r1] Zhao, X., Kang, Z., Feng, A., Levine, S. and Song, D., 2025. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590."}, "questions": {"value": "- What exactly is “self-confidence” in Fig. 2? The text says you “compute the self-confidence measured by KL between uniform and the model’s distribution.” Is this NSC (i.e., ${KL}(U\\|\\pi)$)? If so, please clarify terminology and axes so readers don’t confuse SC with NSC. \n- Why NSC on both text and image tokens? You conclude “using NSC as the intrinsic reward in both text and image tokens can achieve the best results.” Could you share ablations with mixed choices (e.g., NSC on image, SC on text), modality-specific weights, or temperature controls to support this decision?\n- Many plots focus on the first ~800 steps. Do your gains persist or change at longer horizons, and how does variance across random seeds look over full training? Please include long-run curves and seed-wise spreads. \n- The reward NSC is differentiable against the model parameters as it’s an analytic function of the model’s softmax probabilities, hence smoothly differentiable w.r.t. the logits/parameters. It means that it can be direclty optimized using gradient-based algorithm like SGD. What is the meaning of using RL to optimize it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xa0XG5yP4Q", "forum": "36CzrZYEch", "replyto": "36CzrZYEch", "signatures": ["ICLR.cc/2026/Conference/Submission14485/Reviewer_TNZX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14485/Reviewer_TNZX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890547286, "cdate": 1761890547286, "tmdate": 1762924884614, "mdate": 1762924884614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IRIS (Intrinsic Reward Image Synthesis), a reinforcement learning framework designed to improve text-to-image generation without relying on external human feedback or labeled data. The key idea is to use intrinsic rewards derived from the model’s own self-uncertainty, showing that maximizing uncertainty—rather than minimizing it—leads to more diverse, detailed, and human-preferred images. The authors demonstrate that autoregressive T2I models with low uncertainty tend to produce oversimplified and uniform results, while optimizing intrinsic uncertainty enhances visual richness and alignment with user intent. Empirical experiments confirm that IRIS effectively improves generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a interesting observation of the relationship between model confidence and reward in the training process, and the different patterns between text generation and image generation.\n\n2. This paper proposes a reinforcement learning method that improves text-to-image generation without external reward or human annotation. After training with intrinsic reward, the model outperforms the baseline model by a large margin, and achieves comparable performance with external reward-guided training.\n\n3. This paper is motivated by a observation presented in Figure2, and fluently leads to the method design."}, "weaknesses": {"value": "1. The performance using intrinsic reward is slightly lower than using external reward. It is sometimes acceptable to achieve slightly lower performance without the need of external reward, but in some cases people may prefer a higher overall performance.\n\n2. Captions of figure 5 and 6 need to switch. The captions of some other figures also seem unclear. It would be better to summarize the key observations of each figure in the caption.\n\n3. In the experiment, the authors compare the best checkpoints of T2I-R1 and IRIS, which may lead to unfair comparison. It would be better to compare them at the same training steps. Since RL training are often unstable and costly, sometimes the training step is also an important factor that we want to compare. Besides, in figure 3, it seems T2I-R1 consistently outperform IRIS by a large margin. This does not well align with the claimed results in Table1."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jL6pUzmHDJ", "forum": "36CzrZYEch", "replyto": "36CzrZYEch", "signatures": ["ICLR.cc/2026/Conference/Submission14485/Reviewer_ggZ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14485/Reviewer_ggZ2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982066221, "cdate": 1761982066221, "tmdate": 1762924884031, "mdate": 1762924884031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IRIS (Intrinsic Reward Image Synthesis), a reinforcement learning framework for text-to-image (T2I) generation that uses negative self-certainty (NSC) as an intrinsic reward. Unlike RLHF or external reward-based approaches, IRIS optimizes autoregressive T2I models without human labels or domain-specific verifiers. The key insight is that reducing self-confidence in image token predictions enhances visual richness and diversity. Experiments on Janus-Pro across GenEval, T2I-CompBench, and WISE benchmarks show IRIS performs competitively or better than models trained with external rewards."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "[+] Challenges standard RL intuition demonstrates that minimizing self-confidence improves image diversity.\n\n[+] Defines intrinsic rewards via forward KL divergence; mathematically grounded and well-motivated.\n\n[+] Broad evaluation across multiple benchmarks and ablations (text vs. image tokens, KL direction, semantic CoT)."}, "weaknesses": {"value": "[-] Reward aggregation details (text/image scaling, normalization) are unclear.\n\n[-] Not includes the comparison with preference- or diffusion-based RL.\n\n[-] There are no human studies to demonstrate that visual preference."}, "questions": {"value": "1. How are text and image token rewards normalized or weighted—are they on comparable scales?\n1. Has IRIS been tested on diffusion or masked models beyond autoregressive Janus-Pro?\n1. Any user preference studies confirming that lower self-confidence yields subjectively better images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yFVGmQhzsO", "forum": "36CzrZYEch", "replyto": "36CzrZYEch", "signatures": ["ICLR.cc/2026/Conference/Submission14485/Reviewer_ZMs2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14485/Reviewer_ZMs2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14485/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996897428, "cdate": 1761996897428, "tmdate": 1762924883099, "mdate": 1762924883099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}