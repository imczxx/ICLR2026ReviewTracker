{"id": "LUiqtv6vrd", "number": 470, "cdate": 1756741335855, "mdate": 1763731881299, "content": {"title": "Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense", "abstract": "Ensuring Large Language Model (LLM) safety remains challenging due to the absence of universal standards and reliable content validators, making it difficult to obtain effective training signals. We discover that aligned models already possess robust internal safety beliefs: they consistently produce high-confidence refusals to harmful requests while exhibiting high entropy when generating potentially dangerous content. This entropy gap reveals an untapped signal—models intrinsically \"know\" when to refuse. We introduce Safety Instincts Reinforcement Learning (*SIRL*), which transforms this internal confidence into a self-generated reward signal, eliminating dependence on external validators or human annotations. *SIRL* teaches models to trust their safety instincts by reinforcing low-entropy refusal behaviors. Evaluated on Llama and Qwen models, *SIRL* maintains 89\\%+ Defense Success Rates (DSRs) against 20+ jailbreak methods, from static prompts to automated attacks. Using only 15,000 unlabeled prompts, *SIRL* surpasses resource-intensive supervised methods while preserving performance on mathematics, coding, and conversation benchmarks. Our work demonstrates that effective alignment can emerge from within, paving the way for more autonomous and robust AI safety mechanisms that scale without extensive human oversight.", "tldr": "", "keywords": ["Large Language Models", "Jailbreak Defense", "Self-Alignment", "Intrinsic Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/78dbd3e8527a69498710a4b6c1d003314f9838b7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "*Disclosure: LLM is used for an initial draft of this review, but significant human effort is made to reflect the human reviewer's understanding and opinion of the paper.*\n\nThe authors address the issue in safety post-training where dense and reliable safety annotations for model responses are hard to obtain. The key observation of this paper is that existing well-aligned LLMs already possess \"safety instincts.\" When presented with a harmful request, the model exhibits high entropy (i.e., uncertainty) if it starts generating a harmful, compliant response. Conversely, it shows low entropy (high confidence) when generating a safe, refusal response (e.g., \"I cannot help with that request.\"). Based on this discovery, the authors propose Safety Instincts Reinforcement Learning (SIRL), which repurposes the negative entropy of its response as its own reward. By using reinforcement learning, the model is trained to favor its own low-entropy (high-confidence) outputs. Since these high-confidence outputs are overwhelmingly safe refusals, the model effectively teaches itself to \"trust its gut\" and become more robustly safe."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method is very simple and operates in a completely self-supervised manner, requiring only a small set of unlabeled prompts. The result reward signal is dense (token-level) while requiring no human effort (except picking the set of prompts). This makes the method efficient and scalable.\n- The proposed method SIRL achieves a defense success rate (DSR) of over 93% across diverse jailbreak attack methods, dramatically outperforming a wide range of baselines."}, "weaknesses": {"value": "- The entire method hinges on the discovery that already aligned models have this entropy gap. SIRL seems to be an amplifier of existing, correct safety instincts rather than a creator of them. The paper does not explore what would happen if SIRL were applied to a non-aligned model (e.g. base model).\n- The method's sole objective is to make refusals more confident on its given training prompts. There is no \"other side\" to the reward signal that punishes the model for refusing safe prompts. This one-sided optimization could potentially lead to over-refusal, especially if the 15,000 unlabeled training prompts contain borderline, \"gray area\" cases.\n-  Standard benchmarks like MATH, BBH, and HumanEval are distributionally very different from safety-related queries. A model can be perfectly helpful on a coding problem but still overly refuse a nuanced, safe prompt about a sensitive topic. The evaluation is missing tests on dedicated over-refusal benchmarks (e.g., OR-Bench) that are designed to measure this exact failure mode.\n- Minor: There is theoretically possibility for reward hacking as we do not monitor the actual content of the model completions. The model could potentially discover that the optimal strategy is to output a single, nonsensical but very high-confidence phrase in response to all challenging prompts. While this doesn't seem to be empirically observed, there seems to be some degree of such collapse already happening, as all the examples provided in the appendix all the result responses end with the sentence \"Can I help you with something else?\""}, "questions": {"value": "- As discussed, I would appreciate an evaluation on a dedicated over-refusal benchmark such as OR-Bench.\n- I would also like to see demonstration of necessity of RL. Specifically, comparison over the following baseline: sample multiple responses (best-of-n) and select the one with the lowest average entropy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XTMVcpjfrm", "forum": "LUiqtv6vrd", "replyto": "LUiqtv6vrd", "signatures": ["ICLR.cc/2026/Conference/Submission470/Reviewer_DfTG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission470/Reviewer_DfTG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757746167, "cdate": 1761757746167, "tmdate": 1762915526702, "mdate": 1762915526702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors discover that aligned LLMs already possess robust internal safety beliefs: they consistently issue high-confidence refusals to harmful requests while exhibiting high entropy when generating potentially dangerous content. Based on this finding, they propose SIRL (Safety Instincts Reinforcement Learning), a framework for improving safety performance during post-training. The key idea is to transform internal confidence (revealed by entropy) into a self-generated reward signal, thereby eliminating the need for external validators or human annotations. They present evaluation results demonstrating the effectiveness of their method, showing that their models achieve state-of-the-art safety performance without sacrificing general capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) Timely and Important Problem: The paper addresses the critical issue of LLM safety, which is essential for the broader deployment of language models in real-world applications. \n\n(2) Interesting Findings: The observed correlation between model confidence and safety in LLM completions is intriguing. While it may partly reflect memorization or overfitting, it nonetheless highlights an underexplored signal that could be leveraged in safety training. \n\n(3) Comprehensive Experiments: The authors conduct extensive experiments across both the LLaMA and Qwen model families, evaluating performance on safety-related and general reasoning benchmarks. The results are strong, with nearly 100% safety performance on the JBB tasks. \n\n(4) Clear and Cohesive Writing: The paper is well-written, with a clear structure and logical presentation of key ideas, making it easy to follow and understand."}, "weaknesses": {"value": "(1) Dependence on Confidence-Safety Correlation:\nThe success of SIRL heavily depends on the assumption that model confidence correlates with safety. While this correlation is supported by the experiments in Section 3, concerns remain—particularly regarding generalization. For instance, when confronted with novel attacks or new evaluation benchmarks, it is unclear whether the models will still reliably refuse harmful requests.\n\n\n(2) Overstated Claims about Paradigm Shift:\nThe authors claim their findings suggest a paradigm shift—from relying on complex external supervision to building robust AI systems by trusting models' intrinsic safety instincts, thus enabling scalable, self-reliant defenses against evolving threats. However, this assertion may be overstated. The observed confidence-safety correlation likely stems from the fact that many attack patterns and corresponding refusals were already present in the model's pretraining data. If the community shifts away from explicit safety supervision and instead relies solely on model-internal signals (e.g., entropy) as safety indicators during training, we risk stagnation—reinforcing existing patterns without advancing the frontier of safety capabilities.\n\n\n(3) Limited Novelty Beyond Safety Context:\nThe concept of post-training via entropy minimization has been extensively explored in prior LLM literature [1]. Although this work applies it in the context of safety, the scope and insights are confined to this specific domain, limiting broader methodological novelty.\n\nReference:\n\n[1] Shivam Agarwal, et al. The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning. 2025"}, "questions": {"value": "(1) Ablation on Temperature: Since entropy is closely tied to the decoding temperature during reinforcement learning, it would be valuable to include an ablation study varying the temperature. This could help clarify whether the observed safety-confidence correlation remains robust under different decoding settings.\n\n(2) Complete Evaluation Results: In Experiment 5.5, only half of the evaluation dataset was reported. For completeness and transparency, it would be helpful to include the results on the remaining four benchmarks, following the format of Table 2.\n\n(3) Generalization Test with Older Models: To partially address Weakness (1), I suggest adding an experiment that tests older models, such as the LLaMA2 series, on the same benchmarks. These models are less likely to have been exposed to the recent attacker data distributions and could provide a more rigorous test of generalization beyond memorized safety patterns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nBuhwheU5H", "forum": "LUiqtv6vrd", "replyto": "LUiqtv6vrd", "signatures": ["ICLR.cc/2026/Conference/Submission470/Reviewer_WniN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission470/Reviewer_WniN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934848885, "cdate": 1761934848885, "tmdate": 1762915526602, "mdate": 1762915526602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a strong intrinsic reward signal for safety training. They show that instruction-tuned models already exhibit high-entropy on unsafe responses and low-entropy on safe responses to harmful queries. They design an RL method to reinforce this signal and demonstrate that it outperforms traditional safety finetuning techniques (such DPO, RLHF... etc). Importantly, they evaluate their method against a suite of adaptive attacks and show improved robustness.\n\n## Contributions\n \n * They identify response entropy as an intrinsic signal that correlates with safe responses\n * They design an RL algorithm to optimize this signal\n * They evaluate their method on 3b and 8b instruction-tuned base models and show improved safety performance without capability degradation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall, this looks like a promising and novel method for jailbreak defense. It identifies an intrinsic signal (response entropy) that seems to be correlated with safety as models concentrate their responses for safe responses/refusals. The authors evaluate against adaptive attacks and show impressive improvements over standard RLHF or DPO safety training. Their evaluations train on PKU-SafeRLHF prompts without labels and compare against methods that do use the explicit labels.\n\nWhile the paper has some room for improvement, the novelty, combined with strong evaluation and good results, makes it a very strong submission."}, "weaknesses": {"value": "# Clarity about initial conditions\n\nThe method relies on having a sufficiently aligned starting model. However, it is not clear what models would meet this bar. For example, the experiments all begin from instruction-tuned models. Does this mean that the method should work with any instruction-tuned model? Can this be applied to models that have already undergone RLHF or DPO-style training? More clarity about what initial conditions need to be satisfied would help readers apply the method appropriately.\n\n# Dependence on the prompt dataset for training\n\nWhile the method is unsupervised, it does seem like it implicitly relies on the training dataset of prompts to be safety-relevant. It isn't clear if this would be an appropriate reward signal to include for arbitrary prompts. This seems like an important form of supervision that the method relies on. It seems like this is reinforcing the model to exhibit mode collapse on unsafe queries. This is fine, but potentialy relies on the queries being carefully selected to avoid mode collapse in alternative domains.\n\n# Effect on response diversity\n\nSimilar to the above comment, it seems like this method might induce undesired mode collapse in the final model. This does not appear to be directly evaluated in the work.\n\n# Quality of models used for evaluation\n\nThe models used for evaluation are relatively small. As a result, it's possible that the results won't generalize with model scale. I'm sympathetic to concerns about cost for the evaluation. Maybe there's a way to confirm that the underlying correlation holds for larger models without doing a full training run? In any case, this should be clearly discussed within the paper and highlighted as a weakness/limitation.\n\n# Overclaiming of results\n\nThe contributions list a 6x improvement on average safety performance. While this is correct, it is somewhat misleading as the adaptive attacks show minimal improvement on the llama models in comparison to DPO."}, "questions": {"value": "* What is the effect of this method on the diversity of responses or mode collapse? It seems like this method might substantially contribute to mode collapse within the model. \n * Your paper says that 'aligned' models exhibit a difference in entropy for safe vs unsafe responses. What models count as sufficiently aligned for this self-improvement operator to work well? Would it make sense to apply this to a model that has already gone through refusal training via RLHF or something similar? \n * In table 2, can you clarify what the specific methods are in more detail?\n * Does the dataset used for SIRL matter? This seems like it might be quite important as it focuses the reinforcement on harmful queries. Would this method run into difficulties if it was run on a more general dataset of prompts?\n * Would this method work as an inference-time intervention (e.g., to filter BoN responses)? Why or why not?\n * What explains the performance increases over RLHF? Can you say something about the differences between the learned reward model and the intrinsic reward?\n * How would you expect these results to transfer to larger or more recent models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cIS8QgUXbE", "forum": "LUiqtv6vrd", "replyto": "LUiqtv6vrd", "signatures": ["ICLR.cc/2026/Conference/Submission470/Reviewer_gcsB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission470/Reviewer_gcsB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762043803206, "cdate": 1762043803206, "tmdate": 1762915526479, "mdate": 1762915526479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to use response entropy as a reward signal for aligning LLMs. The main insight is that aligned LLMs naturally have high-confidence refusals to harmful requests (i.e. low entropy), whereas completions to the same harmful queries have higher entropy. This gives a natural signal to reinforce existing safety behaviour by formulating a reward function as the negative entropy $r_i = - \\bar{H}(o_i \\mid q)$, where $\\bar H$ is the entropy, and $o_i$ is the response to the query $q$. The authors demonstrate the efficacy of their approach with very strong results on JailBreakBench (JBB), without negatively impacting utility. It’s also very practical as this is an intrinsic reward, so it’s easier to work with over training a reward model or labelling responses. Overall I think this is a good and simple approach to strengthen a model’s preexisting alignment training, but I have a few concerns about the evaluations (see weaknesses/questions).\n\nSome other comments: \n\n- I think some of the claims are too grandiose, eg: “*…SIRL represents a fundamental shift toward self-reliant AI safety…*”, or “*This validates that confidence-based optimization reinforces fundamental safety reasoning rather than learning attack-specific patterns, …*”. The presented method clearly works well to reinforce the existing safety beliefs in the model, but I think it's also clear that this approach still relies on the base model being well aligned, and it does not solve the problem of *what* the models should be aligned to.\n- I appreciate the DSR heatmaps to break down the results, however I would recommend that 1) you keep a constant ordering of the attack methods between the figures and b) you keep a constant colour scale across all the figures rather than setting red as min/green as max for a particular plot; this would make comparing the results much easier.\n- You make the claim of using ‘adaptive attacks’, but the attacks conducted (GCG, PAIR, RandomSearch) are not adaptive, they are simply automated attacks. An adaptive attack would require the attack to account for the defense (see weaknesses section)\n\nI currently am leaning towards reject, but I am open to increasing my score with more evaluations being done to better understand how robust the method is, as well as if it has potential negative impact on over-refusal or hallucinations (see weaknesses/questions)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple, novel, and clever idea to use to reinforce the model’s preexisting safety beliefs.\n2. The empirical results are very strong, achieving 98%+ DSR across JBB (20+ jailbreak methods) while maintaining utility. The method generalizes well across different model families (Llama, Qwen) and sizes (3B-8B parameters).\n3. The approach is data efficient, using only 15,000 unlabeled prompts without human annotations, preference pairs, or reward models is a significant practical advantage over traditional methods like RLHF, DPO, or SFT."}, "weaknesses": {"value": "1. I don’t think evaluating only on JBB is sufficient; the results shown are very strong but I think more effort should be done in breaking the defense. Furthermore, there are only 100 test samples, all of which follow a mostly prescribed format which could potentially be memorized.(e.g. [1] shows how pretty much every defense method can be broken. I don’t think you need to be perfect in order for the core idea of the paper to be useful, but it should be evaluated properly). More diverse test prompts would be ideal, and other types of jailbreaks could be included (e.g. multi-turn, pre-filling, stronger algorithmic attacks, etc.)\n2. There is a concern of over-refusal and no evaluation for this is conducted other than basic utility. I would like to see evaluations on XSTest/OR-bench to quantify this\n3. Unclear relationship between hallucinations; see Q1\n4. (minor) The LLM-as-judge evaluation could have biases; the authors do evaluate whether the judges (Llama 3.3-70B, Qwen2.5-72B, and GPT-4o) and are consistent with each other, but in my experience, I have had misleading results with all three of them before while evaluating jailbroken responses. \n\n[1] Nasr, Milad, et al. \"The attacker moves second: Stronger adaptive attacks bypass defenses against llm jailbreaks and prompt injections.\" *arXiv preprint arXiv:2510.09023* (2025)."}, "questions": {"value": "1. Do you think that this training paradigm (reinforcing the model’s confidence in it’s own response) would result in an increased rate of hallucinations? There are a few benchmarks for this and I think it would be important to check/quantify.\n2. What is the relationship between the base model’s original alignment (e.g. ability to “know” what’s harmful) and the overall performance of SIRL? What’s the minimum amount of separation needed in order to bootstrap with SIRL, and how nuanced must the model’s internal understanding be?\n3. It would be interesting to see if there were any connections with the literature on the geometry of refusals [1,2], have you thought about this at all?\n\n[1] Arditi, Andy, et al. \"Refusal in language models is mediated by a single direction.\" Advances in Neural Information Processing Systems 37 (2024): 136037-136083.\n\n[2] Wollschläger, Tom, et al. \"The geometry of refusal in large language models: Concept cones and representational independence.\" arXiv preprint arXiv:2502.17420 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ljIkNHjlAO", "forum": "LUiqtv6vrd", "replyto": "LUiqtv6vrd", "signatures": ["ICLR.cc/2026/Conference/Submission470/Reviewer_a3QH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission470/Reviewer_a3QH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762297094020, "cdate": 1762297094020, "tmdate": 1762915526386, "mdate": 1762915526386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Dear Reviewers,\n\nWe sincerely thank you for the thorough and constructive feedback, which has substantially strengthened our work. We have uploaded a revised manuscript and provide detailed point-by-point responses below.\n\nWe hope the revisions address your concerns satisfactorily.\n\nBest regards,\nThe Authors"}}, "id": "gJlInF9gEa", "forum": "LUiqtv6vrd", "replyto": "LUiqtv6vrd", "signatures": ["ICLR.cc/2026/Conference/Submission470/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission470/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission470/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763732339969, "cdate": 1763732339969, "tmdate": 1763732339969, "mdate": 1763732339969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}