{"id": "wnhwQp8Umq", "number": 10228, "cdate": 1758164581594, "mdate": 1759897664928, "content": {"title": "EpiCache: Episodic KV Cache Management for Long Conversational Question Answering", "abstract": "Modern large language models (LLMs) extend context lengths to up to millions of tokens, enabling AI assistants to generate coherent and personalized responses grounded in long conversational histories. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting the KV cache after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to a single query, leading to failure cases in multi-turn conversations.\nWe introduce EpiCache, a training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget across layers accordingly.\nAcross three LongConvQA benchmarks, EpiCache improves accuracy by up to 40\\% over recent baselines, sustains near-full KV accuracy under $4$–$6\\times$ compression, and reduces latency and memory by up to $2.4\\times$ and $3.5\\times$, thereby enabling efficient multi-turn interaction under strict resource constraints.", "tldr": "Episodic KV cache management method for multi-turn conversation on memory-constrained devices", "keywords": ["KV Cache", "Long Context", "Conversation", "LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/49f7bca77eef6e14a14787a3c09771b58b20e6c7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces EPICACHE, a training-free framework for managing the Key-Value (KV) cache in large language models (LLMs) to enable long conversational question answering (LongConvQA) under strict, fixed memory budgets. The core problem it addresses is the linear growth of the KV cache with context length, which becomes a bottleneck in resource-constrained environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Significance of the Problem: The paper addresses a critical and timely problem in deploying LLMs: the memory and computational cost associated with very long contexts. The focus on LongConvQA under fixed memory budgets is highly relevant for practical applications like AI assistants on personal devices.\n\n\n\nNovel Methodology: The core idea of combining block-wise prefill with episodic clustering is novel and well-motivated. Instead of treating the context as a monolithic block, partitioning it into semantically coherent topics is an intuitive and powerful approach for managing conversational memory. The framework is also training-free, which is a significant advantage for practical adoption."}, "weaknesses": {"value": "Coarse Granularity and Untested Failure Modes: The method's core mechanism relies on retrieving a coarse-grained \"episode-level\" cache. This design may be insufficient for tasks requiring fine-grained information retrieval or cross-topic reasoning. For instance, it is unclear how EPICACHE would handle complex queries that need to synthesize details from multiple distinct episodes within the same conversation. Furthermore, its coarse granularity might cause it to fail on popular long-context benchmarks like Needle-in-a-Haystack (NIAH), which test precise fact retrieval, or coding tasks from benchmarks like LongBench, which require understanding fine-grained dependencies across a large codebase. The evaluation is limited to conversational QA and does not explore these likely failure points.\n\n\n\nLack of Batching Support and Deployment Inefficiency: The paper's efficiency analysis focuses exclusively on per-turn latency for a single user. The proposed method, which requires dynamically retrieving a different, query-specific cache for each request, is fundamentally incompatible with traditional batching techniques used in production environments. This lack of batching support means that while it may be efficient for a single user, it would be highly inefficient for actual deployment in a multi-user service, leading to low overall throughput.\n\n\nScalability and Hyperparameter Sensitivity: The episodic clustering approach relies on a fixed number of clusters, E (set to 4 in all experiments). It is unclear how this approach would scale to extremely long and diverse conversations that might contain dozens of distinct topics. The process would either lead to very broad, less useful clusters or require a much larger E, which would increase the offline storage cost and potentially the query-matching overhead. The paper lacks a sufficient analysis of the sensitivity to this crucial hyperparameter.\n\nInsufficient Engagement with Concurrent Literature: The paper's literature review appears to be missing several highly relevant and concurrent works, making it difficult to fully situate its contribution. Specifically, the following works should be discussed and compared against:\n\n\n* Cartridges [1], which proposes an alternative offline training approach (SELF-STUDY) to distill a large corpus into a small, parameterized KV cache.\n\n* OracleKV [2] and ChunkKV [3], which explore semantic-aware compression techniques.\n\n* FlowKV [4], which specifically addresses multi-turn conversational coherence, a goal very similar to EPICACHE's.\n\nReference:   \n> [1] Eyuboglu, Sabri, et al. \"Cartridges: Lightweight and general-purpose long context representations via self-study.\" arXiv preprint arXiv:2506.06266 (2025).  \n> [2] Zhu, Yuanbing, et al. \"OracleKV: Oracle Guidance for Question-Independent KV Cache Compression.\" ICML 2025 Workshop on Long-Context Foundation Models. 2025.  \n> [3] Liu, Xiang, et al. \"Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference.\" arXiv preprint arXiv:2502.00299 (2025).  \n> [4] Liu, Xiang, et al. \"FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management.\" arXiv preprint arXiv:2505.15347 (2025)."}, "questions": {"value": "See the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0f7epXzkXy", "forum": "wnhwQp8Umq", "replyto": "wnhwQp8Umq", "signatures": ["ICLR.cc/2026/Conference/Submission10228/Reviewer_xjth"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10228/Reviewer_xjth"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760471823129, "cdate": 1760471823129, "tmdate": 1762921588152, "mdate": 1762921588152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EPICACHE, a training-free Key–Value (KV) cache management framework aimed at improving memory efficiency in long conversational question answering (LongConvQA). The key innovation lies in combining: Block-wise prefill for bounded memory growth, Episodic KV compression based on semantic clustering of conversation history, and Sensitivity-aware layer-wise budget allocation for better per-layer cache distribution. EPICACHE achieves up to 40% higher accuracy than existing methods (e.g., KVzip, InfiniPot) under fixed cache budgets and sustains near-full KV performance under 4–6× compression. Experiments span multiple LongConvQA benchmarks (Realtalk, LoCoMo, LongMemEval) and models (LLaMA, Qwen)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Evaluations across three benchmarks and four LLMs demonstrate robustness. EPICACHE consistently outperforms recent baselines.\n\n2. Using unsupervised clustering to segment dialogue into semantically coherent “episodes” is intuitive for conversational QA."}, "weaknesses": {"value": "1. Stage 1 of EPICACHE performs semantic segmentation and clustering of the full conversation history using a sentence encoder. While the paper describes this as “offline” and excludes it from latency measurements (Figure 7a), this is not realistic for many LongConvQA use cases, where conversations evolve dynamically and the clustering may need to be updated online.\n- Cost not reflected in latency (TTFT): The encoder and K-Means clustering operations are non-trivial, especially when histories exceed tens of thousands of utterances. The cost should contribute to time-to-first-token (TTFT) or system-level latency, since users cannot interact until episodic caches are prepared.\n\n2. Stage 2 performs episodic, query-independent KV eviction, meaning each episode cache is built once using the medoid segment as a static patched prompt. This design ensures bounded memory but raises two serious concerns:\n- Because eviction decisions are made offline and not conditioned on actual upcoming queries, the cache may not capture relevant tokens for unseen topics or cross-episode reasoning. The approach could fail when conversations shift topics abruptly or when queries require combining information across episodes.\n- In natural multi-turn dialogues, new utterances and model-generated responses continuously extend the context. The paper does not clarify: 1. Whether these new turns are inserted into existing episode clusters or trigger re-clustering. 2. Whether episodic caches are periodically rebuilt or incrementally updated. If the clustering remains static, cache quality will degrade over time as the conversation drifts semantically from initial clusters. If frequent re-clustering is required, the overhead could become prohibitive.\n\n3. The proposed sensitivity-aware layer budgeting (Eq. 10) is empirically effective but seems somewhat ad hoc; an ablation isolating this contribution is only briefly mentioned."}, "questions": {"value": "1. How expensive is Stage 1 in realistic settings (e.g., a 20 K–50 K token conversation)? Does this cost contribute to TTFT or total inference latency?\n\n2. Can the clustering be performed incrementally or asynchronously to amortize cost?\n\n3. How does EPICACHE handle newly generated tokens in an ongoing conversation? Are they appended to existing clusters? Does the system periodically rebuild episodic caches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WBKqWSX0JB", "forum": "wnhwQp8Umq", "replyto": "wnhwQp8Umq", "signatures": ["ICLR.cc/2026/Conference/Submission10228/Reviewer_tiys"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10228/Reviewer_tiys"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761357773435, "cdate": 1761357773435, "tmdate": 1762921587549, "mdate": 1762921587549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents EPICACHE, a training-free framework for Key-Value (KV) cache management in large language models (LLMs) designed for Long Conversational Question Answering (LongConvQA) under fixed memory constraints.EPICACHE introduces (a) episodic KV compression, that clusters conversation histories into semantically coherent episodes for topic-aware caching, and (b) sensitivity-aware layer-wise budget allocation, which allocates cache budgets based on each layer's sensitivity to eviction. Experiments across three benchmarks (Realtalk, LoCoMo, and LongMemEval) and four LLMs (LLaMA-3 and Qwen2.5 variants) demonstrate up to 40% accuracy improvement and 3.5× memory reduction over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. This paper tackles an important problem of long conversation compression.\n\nS2. The paper proposed a well structured way to find the importance of different layers, and allocate different budget to them.\n\nS3. The paper is well written and structured."}, "weaknesses": {"value": "W1. EpiCache is limited to multi-round conversation-based long context. It can not be directly applied to long context and long generation tasks.\n\nW2. I am not fully convinced by the effectiveness of the embedding model that can group the semantically similar episodes together. My intuition is that the episodes in a same conversation is very similar or maybe even all under the same topics. So it is doubtable whether different episodes can be effectively grouped.\n\nW3. The layer-wise sharpness hyperparameter alpha in equation 10 should also be studied in the sensitivity experiments.\n\nW4. It would be more convincing if the authors can include the performance on a medium-size models, like 32B models. In addition, Qwen2.5 should be replace by the new Qwen3 models."}, "questions": {"value": "Q1. If the conversation continues to group, how would you change the number of the clusters? Would you and how often would you re-cluster the episodes?\n\nQ2. Can you play with different embedding models?\n\nQ3. Can you also compare your approach with more recent sparse KV cache baselines, like IceCache, MagicPig, ArkVale, InfiniGen, etc? I understand some of them are not optimized for long prefilling, but most of them can run on single model of 8B models for up to ~100k context."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "msZCEWDhiS", "forum": "wnhwQp8Umq", "replyto": "wnhwQp8Umq", "signatures": ["ICLR.cc/2026/Conference/Submission10228/Reviewer_Wyhg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10228/Reviewer_Wyhg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049515042, "cdate": 1762049515042, "tmdate": 1762921586580, "mdate": 1762921586580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}