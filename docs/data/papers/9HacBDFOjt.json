{"id": "9HacBDFOjt", "number": 13789, "cdate": 1758222628037, "mdate": 1763664972614, "content": {"title": "From Static Benchmarks to Dynamic Protocol: Agent-Centric Text Anomaly Detection for Evaluating LLM Reasoning", "abstract": "The evaluation of large language models (LLMs) has predominantly relied on static datasets, which offer limited scalability and fail to capture the evolving reasoning capabilities of recent models. To overcome these limitations, we propose an agent-centric benchmarking paradigm that moves beyond static datasets by introducing a dynamic protocol in which autonomous agents iteratively generate, validate, and solve problems. Within this protocol, a teacher agent generates candidate problems, an orchestrator agent rigorously verifies their validity and guards against adversarial attacks, and a student agent attempts to solve the validated problems. An invalid problem is revised by the teacher agent until it passes validation. If the student correctly solves the problem, the orchestrator prompts the teacher to generate more challenging variants. Consequently, the benchmark scales in difficulty automatically as more capable agents are substituted into any role, enabling progressive evaluation of large language models without manually curated datasets. Adopting text anomaly detection as our primary evaluation format, which demands cross-sentence logical inference and resists pattern-matching shortcuts, we demonstrate that this protocol systematically exposes corner-case reasoning errors that conventional benchmarks fail to reveal. We further advocate evaluating systems along several complementary axes including cross-model pairwise performance and progress between the initial and orchestrator-finalized problems. By shifting the focus from fixed datasets to dynamic protocols, our approach offers a sustainable direction for evaluating ever-evolving language models and introduces a research agenda centered on the co-evolution of agent-centric benchmarks.", "tldr": "We present a dynamic, agent-driven benchmark where teacher, orchestrator, and student agents generate, validate, and solve problems—enabling scalable evaluation without static datasets and exposing reasoning failures missed by standard benchmarks.", "keywords": ["Agent-centric benchmark", "Language model assessment", "Textual anomaly detection", "Adaptive benchmarks"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dc2e1747edad5c1a00107ffedb47f20d0d0a1959.pdf", "supplementary_material": "/attachment/22bddb5bb6fb4f05cdb1709e872a6b404ff798cd.zip"}, "replies": [{"content": {"summary": {"value": "Traditional LLM evaluation via static datasets (e.g., MMLU) has flaws like poor scalability and overfitting. The paper proposes ATAD, a dynamic protocol using three agents (Teacher, Orchestrator, Student) to generate/solve 7 text anomaly tasks without static data—harder tasks emerge if the Student (evaluated LLM) succeeds, with the Orchestrator ensuring quality. Experiments confirm ATAD works: LLM accuracy drops ~37.3% (valid difficulty), it adapts to future LLMs, and results are stable. ATAD thus enables scalable, sustainable LLM reasoning assessment.​"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Pioneering Dynamic Evaluation Paradigm: The paper introduces a novel, agent-centric dynamic protocol to address the fundamental issues of data contamination and saturation in static benchmarks, offering a sustainable and scalable solution for LLM evaluation.\n\n2. Rigorous Quality Control Mechanism: It introduces the crucial \"Orchestrator\" agent to guarantee the fairness and high quality of generated problems. This effectively prevents the creation of flawed or ambiguous questions that result from an unvalidated competitive process aimed solely at increasing difficulty.\n\n3. Clear and Comprehensive Task Design: The methodology is exceptionally clear and built upon a robust and transparent taxonomy of seven distinct text anomaly detection tasks. This ensures the evaluation framework is well-grounded, broad, and fine-grained."}, "weaknesses": {"value": "1. The claims in Section 2.3 need to be substantiated with data. As it currently stands, the section is unconvincing because it lacks the necessary data and statistics for support.\n2. The benchmark focuses only on text anomaly detection. While useful, it does not cover other critical abilities like tool use, multi-step reasoning, code generation or multimodal understanding, which limits external validity. I believe this technology would be far more meaningful if it were applied to a broader range of tasks.\n3. The method defines difficulty by the moment when the Student first fails, which may be due to randomness (e.g., sampling variance) rather than true capability limits. A more rigorous approach would be to let the Student attempt multiple independent trials (e.g., 5 times) and define difficulty based on the failure rate or success rate, rather than deciding from a single attempt.\n4. Even within text anomaly detection, the benchmark examples may not fully represent the diversity of real-world anomalies. They are often artificial and may not match naturally occurring errors in user-generated content or professional texts."}, "questions": {"value": "1. Could you clarify how the Orchestrator evaluates and filters ambiguous or low-quality questions? Are there explicit criteria or rules, or is it purely model judgment?\n2. Why did you choose text anomaly detection specifically as the first application of your adaptive benchmark? Were other tasks considered, and if so, why were they not selected?\n3. When different LLM families are used as Teachers, do they generate qualitatively different types of anomalies? Could you share examples of such differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SMjyWRbjjG", "forum": "9HacBDFOjt", "replyto": "9HacBDFOjt", "signatures": ["ICLR.cc/2026/Conference/Submission13789/Reviewer_pyPR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13789/Reviewer_pyPR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760603099837, "cdate": 1760603099837, "tmdate": 1762924320183, "mdate": 1762924320183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the growing limitations of static benchmarks for evaluating LLM reasoning, such as data contamination and performance saturation. The authors propose a novel and significant contribution: a dynamic, agent-centric benchmarking protocol named ATAD (Agent-centric Text Anomaly Detection). This protocol leverages a multi-agent system where a \"Teacher\" agent generates problems, a \"Student\" agent attempts to solve them, and a critical \"Orchestrator\" agent validates problem quality, coherence, and fairness. The benchmark's core strength lies in its adaptive difficulty scaling: if the Student solves a problem, the Teacher is prompted to generate a more challenging version, thus allowing the benchmark's difficulty to co-evolve with the Student model's capabilities. By focusing on text anomaly detection—a task requiring deep, cross-sentence logical inference—the paper demonstrates that this protocol can effectively surface nuanced reasoning failures in state-of-the-art LLMs that are often missed by traditional static evaluations, offering a more sustainable and scalable alternative."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a well-motivated and timely problem, clearly articulating the critical failings of static benchmarks, such as data contamination and performance saturation. This strong motivation establishes a clear need for the proposed dynamic evaluation protocol. By addressing these limitations, the work provides a valuable and forward-looking contribution to LLM evaluation.\n\n2. The evaluation is comprehensive, featuring a wide array of models from different families (e.g., GPT, Gemini, Claude, LLaMA) to demonstrate the protocol's broad applicability. The experiments wisely test these models in the various agent roles—Teacher, Student, and Orchestrator—in addition to using them for final evaluation. This thorough approach provides strong evidence for the protocol's robustness and its ability to function as intended across different model capabilities.\n\n3. The presentation is exceptionally clear, with diagrams and a well-defined protocol that makes the complex multi-agent interaction easy to understand. The paper is further strengthened by insightful qualitative examples, particularly those showing problems rejected and refined by the Orchestrator. These examples compellingly illustrate the subtleties of the generated tasks and provide a concrete demonstration of how the validation step ensures benchmark quality and fairness."}, "weaknesses": {"value": "1. The paper's primary contribution is the novel protocol (ATAD) and its application, rather than a new underlying model architecture or training technique. While this agent-centric framework is a significant engineering and conceptual achievement, the work relies entirely on existing LLMs as its core components. This focus on the evaluation system means there is limited technical novelty in terms of model-level innovations.\n\n2. While the paper demonstrates that the protocol creates more difficult tasks, it lacks a deeper analysis of how these tasks generalize. It is unclear if the \"difficulty\" stems from truly novel reasoning challenges or from exploiting specific, narrow blind spots in the Student model used during generation. Further analysis is needed to determine if the finalized benchmarks test for robust, generalizable reasoning or if they risk overfitting to the particular Teacher-Student dynamic of a specific agent family."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "su8dMSnes2", "forum": "9HacBDFOjt", "replyto": "9HacBDFOjt", "signatures": ["ICLR.cc/2026/Conference/Submission13789/Reviewer_6GgZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13789/Reviewer_6GgZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761696458380, "cdate": 1761696458380, "tmdate": 1762924319418, "mdate": 1762924319418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ATAD (Agent-Centric Text Anomaly Detection), a dynamic benchmarking protocol designed to overcome the limitations of static LLM benchmarks such as MMLU and GSM8K. Instead of relying on fixed datasets, ATAD employs a three-agent architecture—a Teacher, Orchestrator, and Student—to iteratively generate, validate, and solve text anomaly detection (TAD) tasks. The Teacher produces candidate problems, the Orchestrator validates them for clarity and fairness, and the Student attempts to solve them. The benchmark evolves dynamically: when the Student succeeds, the Teacher generates harder variants, validated again by the Orchestrator.\n\nEmpirical results on datasets generated by GPT-4o, Claude-3.5-Sonnet, Gemini-2.0-Flash, and LLaMA-3.3-70B show that the ATAD framework effectively scales difficulty and exposes reasoning weaknesses unseen in traditional benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed agentic co-evolution of models and benchmarks represents a natural next step after efforts like BenchAgents and DyVal, making this work both timely and forward-looking.\n\n* The inclusion of Orchestrator-regulated validation and failure-driven sample finalization help control over problem difficulty and fairness.\n\n* The taxonomy of seven anomaly types—each probing distinct reasoning skills (contextual, logical, referential, stylistic, etc.)—is well-grounded and illustrative"}, "weaknesses": {"value": "* While the empirical results are strong, the notion of “difficulty increase” remains heuristic and behaviorally defined (based on Student failure). A more formal definition would strengthen claims.\n\n* The paper cites prior dynamic evaluation works (DyVal, DARG, Benchmark Self-Evolving), but comparative experiments are missing. It is unclear how ATAD quantitatively improves over these prior agent-based or meta-probing systems beyond conceptual novelty.\n\n* Although text anomaly detection is reasoning-oriented, it remains a single task family. The authors argue that it generalizes to reasoning evaluation, but this may not hold for domains like math, code, or multimodal reasoning. Broader applicability should be demonstrated.\n\n* The same model family is used for all three agent roles in most experiments (e.g., GPT-4o Teacher = Student = Orchestrator). This setup may limit adversarial diversity. \n\n* Although the authors observe a ~37 pt accuracy drop after adaptive scaling, it is not clear why certain tasks become more difficult—whether due to semantic subtlety, linguistic ambiguity, or distractor complexity. More granular ablations (e.g., by anomaly type, linguistic factor) would enhance interpretability."}, "questions": {"value": "* Can the authors propose or measure an explicit difficulty metric to formalize the notion of “increased difficulty”?\n\n* Since LLMs generate benchmark data, how do the authors ensure that cultural or linguistic biases in generation do not propagate into the benchmark? Have they evaluated cross-language or demographic stability?\n\n* The paper mentions “failure cases rejected by the Orchestrator” in the Appendix but does not elaborate in the main text. Could the authors analyze typical rejection patterns—e.g., ambiguity vs. incoherence—to demonstrate the Orchestrator’s learning or filtering capacity?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7KirmjXcQt", "forum": "9HacBDFOjt", "replyto": "9HacBDFOjt", "signatures": ["ICLR.cc/2026/Conference/Submission13789/Reviewer_JAkP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13789/Reviewer_JAkP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976146901, "cdate": 1761976146901, "tmdate": 1762924318783, "mdate": 1762924318783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Agent-Centric Text Anomaly Detection (ATAD), a dynamic benchmarking framework designed to address the saturation of static LLM benchmarks. ATAD employs a three-agent protocol in which a teacher generates questions, an orchestrator validates their quality, and a student attempts to solve them, with task difficulty escalating iteratively based on student performance. This automated process enables scalable benchmark generation and adaptive evaluation of LLM reasoning. Using this protocol, the paper constructs a benchmark for text anomaly detection, and experiments across four LLM families (GPT, Gemini, Claude, and LLaMA, used both as question generation and evaluated models) show that ATAD effectively increases task difficulty and reveals reasoning limitations even in strong LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a timely issue where static benchmarks are becoming saturated and insufficient to capture the evolving reasoning capabilities of modern LLMs.\n\n2. The proposed teacher–orchestrator–student framework is conceptually interesting and provides a scalable, automated approach for generating dynamic and evolving benchmarks.\n\n3. The experiments provide meaningful insights into LLM reasoning across different model families on text anomaly detection and the contributions of individual components within the proposed framework."}, "weaknesses": {"value": "1. Based on the prompts shown in Appendix D, the current framework appears to generate questions primarily from the LLM’s inherent knowledge rather than external sources. It is unclear how well this approach generalizes to reasoning problems that require access to factual data or rigorous mathematical reasoning, given that LLMs’ capabilities in these domains are still evolving and may limit the robustness of generated questions.\n\n2. Since the benchmark questions are generated by LLMs, potential model-family-specific biases could be implicitly embedded in the dataset, e.g., models might perform better on questions generated by LLMs from the same family. Although Table 2 suggests no strong bias, a formal analysis and discussion of this issue would strengthen the validity of the proposed protocol. \n\n3. The paper does not include an analysis of the generation cost or number of iterations involved in producing the final benchmark. Reporting metrics such as the average number of iterations per valid question across different anomaly types, and the overall cost (e.g. in $) would help practitioners assess the effectiveness and cost trade-off for future benchmark construction."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xl4H8v9WPU", "forum": "9HacBDFOjt", "replyto": "9HacBDFOjt", "signatures": ["ICLR.cc/2026/Conference/Submission13789/Reviewer_srE2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13789/Reviewer_srE2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13789/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129134838, "cdate": 1762129134838, "tmdate": 1762924318342, "mdate": 1762924318342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}