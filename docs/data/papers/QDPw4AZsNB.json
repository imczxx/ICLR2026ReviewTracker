{"id": "QDPw4AZsNB", "number": 13472, "cdate": 1758218324212, "mdate": 1759897435051, "content": {"title": "REDISTRIBUTING FOUNDATION MODEL LOGITS FOR BETTER DOMAIN GENERALIZATION IN LOW-SHOT CLASSIFICATION", "abstract": "Confidence calibration is an emerging challenge in real-world decision systems that repurpose  foundations models  for downstream vision classification tasks. Due to various reasons ..., logit scores on the CLIP head remain large irrespective of whether the image-language pairs reconcile. Ideally, they should be proportional to that reconciliation. This paper adaptively regulates that 'temperature'. We propose a penalty incorporated into loss objective that penalizes incorrect classifications whenever one is made during finetuning, by moving an amount of log-likelihood to the true class commensurate to the relative amplitudes of the two likelihoods. We refer to it as \\textit{confidence misalignment penalty (CMP)}. Extensive experiments on $12$ vision datasets and $5$ domain generalization datasets supports the calibration performance of our method against stat-of-the-art. CMP outperforms the benchmarked prompt learning methods, demonstrating average improvement in Expected Calibration Error (ECE) by average $6.01$\\%, $4.01$ \\% at minimum and $9.72$\\% at maximum.", "tldr": "", "keywords": ["foundation models", "calibration", "distribution shift"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ec016464e2a7e738b8ff556b5daa5ac9c2afc05d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Confidence Misalignment Penalty (CMP), a regularization term integrated into the contrastive loss of vision-language foundation models like CLIP to improve confidence calibration in low-shot classification and domain generalization settings. The authors identify overconfidence in misclassified predictions as a key issue stemming from factors like neural collapse, contrastive pre-training biases, and distribution shifts. CMP dynamically penalizes misclassifications by redistributing probability mass from incorrect classes to the true class, proportional to the logit misalignment, while leaving correct predictions unaffected. Theoretical guarantees are provided, including bounds on confidence, selective penalization, and gradient stability. Extensive experiments on 12 vision datasets (e.g., ImageNet, CIFAR10) and 5 domain generalization variants (e.g., ImageNet-R, -S) demonstrate average ECE reductions of 6.01% (min 4.01%, max 9.72%) when integrated with prompt learning methods like CoOp, CoCoOp, and MaPLe. Comparisons with benchmarks like CLIPCalib show further gains in calibration metrics."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The analysis of CLIP's conflict with the Maximum Entropy Principle (Theorem 3.1) and propositions on CMP's bounds and stability (e.g., Propositions 3.1–3.3) provide a principled basis, distinguishing it from purely empirical methods.\n2. Experiments cover a wide range of datasets and baselines (CoOp, CoCoOp, ProDA, MaPLe, KgCoOp, CLIPCalib). Results are robust, with consistent ECE/ACE/MCE improvements and ablation studies (e.g., λ tuning, standalone CMP on ImageNet). The illustrative example (Fig. 1) effectively visualizes the calibration gains.\n3. CMP is lightweight (plug-and-play into existing losses), requires no extra hyperparameters beyond λ, and shows domain generalization benefits, making it appealing for downstream adaptations."}, "weaknesses": {"value": "1. The manuscript has numerous grammatical errors, typos (e.g., \"oss objective\" likely means \"loss\" in abstract), and inconsistencies (e.g., \"CLIPCMP\" vs. \"CMP\", \"CLIP + CMP\" would be better). \n2. While CMP is a clever adaptation, it shares conceptual similarities with existing techniques like focal loss (down-weighting confident samples) or margin-based smoothing. Yet, experimental comparisons are missing in this paper.\n3. Focus is primarily on CLIP; extensions to other foundation models (e.g., Flamingo, ALIGN) are mentioned but not evaluated."}, "questions": {"value": "The address the questions mentioned in the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9FwcFxYNhg", "forum": "QDPw4AZsNB", "replyto": "QDPw4AZsNB", "signatures": ["ICLR.cc/2026/Conference/Submission13472/Reviewer_KSsh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13472/Reviewer_KSsh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761294840824, "cdate": 1761294840824, "tmdate": 1762924090580, "mdate": 1762924090580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on the confidence calibration in fine-tuned CLIP. The authors attribute this to miscalibration from contrastive pre-training that conflicts with the maximum entropy principle. To this end, they propose a novel loss regularization CMP, which redistributes probability mass from the overconfident incorrect class to the true class. Extensive experiments show that integrating CMP with prompt learning methods improves calibration performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe motivation is well-structured. The paper provides a clear theoretical analysis of overconfidence in CLIP, which links the contrastive pre-training objective to a violation of the maximum entropy principle.\n2.\tThe proposed method is intuitive. CMP applies a penalty only to misclassified examples and can integrate seamlessly into existing fine-tuning methods.\n3.\tThe experimental evaluation is comprehensive. The effectiveness is verified on 12 standard datasets and 5 domain generalization datasets. The results show that the proposed CMP generalizes well and show consistent improvement across settings."}, "weaknesses": {"value": "1.\tThe focus between accuracy and calibration is unclear. The paper does not clearly claim whether its primary objective is to improve accuracy or confidence calibration after fine-tuning. If the focus is calibration, this intent should be reflected more explicitly in the title and motivation sections.\n2.\tThe claim on CLIP logits needs to be modified. The author claims that logit scores on the CLIP head remain large irrespective of whether the image–language pairs reconcile, which appears inaccurate or at least overstated. Zero-shot CLIP models are generally recognized to exhibit relatively good calibration [1, 4].\n3.\tThe link between theoretical analysis and proposed loss is weak. The theoretical discussion explains why overconfidence emerges in softmax-based training in CLIP. However, it does not show how the CMP ratio or its logarithmic variant follows from those principles.\n4.\tThe formulation of CMP is inconsistent. The manuscript defines the loss as a direct ratio (Eq. 2), while the appendix reformulates it as a negative log ratio (Proposition B.4).\n5.\tLack of comparison with training-based calibration methods for CLIP. Since the paper focuses on training-based calibration for CLIP, it does not compare with recent works [2-4].\n\n[1] Minderer M, Djolonga J, Romijnders R, et al. Revisiting the calibration of modern neural networks[J]. NeurlPS, 2021.\n\n[2] Murugesan B, Silva-Rodríguez J, Ayed I B, et al. Robust calibration of large vision-language adapters. ECCV, 2024.\n\n[3] Oh C, Lim H, Kim M, et al. Towards calibrated robust fine-tuning of vision-language models. NeurlPS 2024.\n\n[4] Wang S, Li Y, Wei H. Understanding and mitigating miscalibration in prompt tuning for vision-language models. ICML, 2025."}, "questions": {"value": "1.\tHow does the proposed method affect the confidence distribution? It seems that the improvement of ECE might largely stem from accuracy rather than a correction of confidence in Figure 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "109ymtEtot", "forum": "QDPw4AZsNB", "replyto": "QDPw4AZsNB", "signatures": ["ICLR.cc/2026/Conference/Submission13472/Reviewer_XjAw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13472/Reviewer_XjAw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535551640, "cdate": 1761535551640, "tmdate": 1762924089787, "mdate": 1762924089787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a Confidence Misalignment Penalty (CMP), a logit calibration method designed to redistribute the overconfidence of misclassified classes toward the correctly predicted ones. The goal is to achieve confidence levels proportional to the alignment of image–text pairs. Experimental results demonstrate that CMP effectively improves both accuracy and confidence calibration performance, achieving stronger in-domain and out-of-domain results, particularly in few-shot scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is well-grounded. Existing fine-tuning approaches often distort the logit distribution, leading to unreliable model confidence.\n\n2. The proposed method is simple and intuitive. The authors introduce a Confidence Misalignment Penalty (CMP) loss, which directly penalizes misclassified classes to achieve better confidence calibration."}, "weaknesses": {"value": "1. Limited novelty. The Confidence Misalignment Penalty (CMP) appears to be a relatively straightforward application of probability redistribution, a concept already present in various calibration techniques such as label smoothing and focal loss. While the authors emphasize CMP's \"adaptive\" nature through logit amplitudes and its targeted use with foundation models like CLIP, the core mechanism of reallocating probability mass from overconfident incorrect predictions isn't a fundamentally new theoretical contribution. \n\n2. Limited gains. the magnitude of improvement often appears rather modest. For example in Table 1, the accuracy improvement by integrating CMP is mostly within 0~1%, and in Table 2 for domain generalization, the improvement is often negative, especially with ProDA (around -1%). The authors should explain the compatibility of CMP with other prompt learning methods due to these failure cases.\n\n3. Poor presentation. In terms of writing, some descriptions in the paper appear to violate basic grammatical conventions (e.g., “We understand the problem to have emerged from a mix of factors”), making the content difficult to follow. For the figures, such as Figure 1, essential information about the x- and y-axes is missing, which hinders quick comprehension. Regarding the overall structure, it seems that the authors inadvertently separated the experimental results from the experimental settings, resulting in Section 4 containing only one subsection on settings. The authors are encouraged to spend more time refining the paper to improve its clarity, readability, and organization before submission."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qMVKw3Zu1K", "forum": "QDPw4AZsNB", "replyto": "QDPw4AZsNB", "signatures": ["ICLR.cc/2026/Conference/Submission13472/Reviewer_gaUv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13472/Reviewer_gaUv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553850210, "cdate": 1761553850210, "tmdate": 1762924088992, "mdate": 1762924088992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Confidence Misalignment Penalty (CMP), a plug-in regularizer for CLIP-style models that activates only on misclassified examples and “redistributes” probability mass from overconfident wrong classes toward the ground-truth class. Theoretical notes argue CMP is bounded and yields stable gradients. Experiments cover 12 datasets for few-shot prompt learning and ImageNet domain-shift variants, reporting ECE/ACE/MCE and accuracy; the paper claims consistent ECE reductions and small accuracy changes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper motivates why frozen or lightly-tuned CLIP heads can be overconfident and frames CMP as a selective penalty that triggers only on errors, leaving correct predictions largely undisturbed.\n\n2. CMP is easy to add to standard training loops (few lines around the loss), model-agnostic across prompt-learning methods (CoOp, CoCoOp, ProDA, MaPLe, KgCoOp).\n\n3. Results report calibration metrics on 11 downstream datasets at 8-shot and ImageNet-A/R/S/V2 for shift; tables list both accuracy and calibration (ECE/ACE/MCE). There is also a CLIPCalib comparison for ResNet-50 and ViT-B/16."}, "weaknesses": {"value": "1. CMP’s form (a ratio of softmax probabilities applied when an example is misclassified) resembles prior confidence-suppressing or redistribution ideas (e.g., entropy/label smoothing). The paper motivates via a Maximum Entropy view, but a sharper positioning vs. these families would help establish novelty.\n\n2. With ViT-B/16, CLIP+CMP average ECE improves vs CLIP and is competitive with CLIPCalib/C-tpt/B-PEFT, while average accuracy lags slightly behind CLIPCalib/C-tpt in the table shown."}, "questions": {"value": "1. Can you add temperature scaling baselines to contextualize ECE gains?\n\n2. When does CMP hurt? Any diagnostics for cases where accuracy drops (e.g., ProDA+CMP in some targets)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ps7tuoyKUO", "forum": "QDPw4AZsNB", "replyto": "QDPw4AZsNB", "signatures": ["ICLR.cc/2026/Conference/Submission13472/Reviewer_2cNF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13472/Reviewer_2cNF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923942463, "cdate": 1761923942463, "tmdate": 1762924088477, "mdate": 1762924088477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}