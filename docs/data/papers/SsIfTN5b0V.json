{"id": "SsIfTN5b0V", "number": 16024, "cdate": 1758258773974, "mdate": 1759897266815, "content": {"title": "Beyond Moment : Rethinking Evaluation para-digm for Timeline Summarization in the era of LLMs", "abstract": "Timeline summarization (TLS) aims to condense large collections of temporally ordered documents into concise and coherent narratives of key events.  While recent advances with large language models (LLMs) have improved, progress in TLS cannot be assessed objectively due to the lack of reliable evaluation metrics.  Existing evaluation metrics rely on the assumption that milestones aligned at the same timestamp convey identical semantic meaning. This design choice inherently biases against abstractive or semantically equivalent outputs while emphasizing temporal consistency ( Date F1 and A-ROUGE ). Consequently, such evaluation protocols fail to adequately reflect the genuine improvements brought by LLM and deviate from human judgments when comparing the relative merits of different methods. To more faithfully assess whether the predicted timeline and the reference timeline truly refer to the same events, we propose a new evaluation framework in which all metrics are grounded on semantically aligned sentence pairs rather than merely time-aligned milestones. We leverage LLM to compute semantic similarity, align sentence pairs via maximum-weight bipartite matching, and compute a Pair-Match score. Building on this alignment, Date-F1 and ROUGE metrics are further introduced to jointly evaluate semantic coverage and temporal fidelity, which we term Pair-Date F1 and Pair-ROUGE, respectively. To validate the effectiveness of our proposed metrics, we introduce a full-stage LLM-TLS (FS-LLM-TLS) approach and conduct comparisons against prior methods. Experiments demonstrate that FS-LLM-TLS not only surpasses prior methods on existing evaluation metrics but also that its advantages are more faithfully and effectively reflected under our evaluation framework, which offers a fairer and more reliable assessment of method quality. This evaluation framework establishes a new paradigm for TLS evaluation, laying the foundation for future experimentation and system development.", "tldr": "This work proposes semantic alignment–based evaluation metrics and a full-stage LLM-TLS framework to advance timeline summarization.", "keywords": ["Timeline Summarization (TLS)  Evaluation Metrics  Semantic Alignment  Large Language Models (LLMs)  Bipartite Matching"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ad8b6fc68f9ee77a5c7cf1b6432d3ebcde3ffb2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper revisits evaluation for timeline summarization in the LLM era, proposing a semantic-alignment-based evaluation framework (SA metrics) that uses large language models to align predicted and reference events semantically rather than temporally, providing a more faithful reflection of model quality and human judgment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clearly identifies a long-standing flaw in traditional TLS evaluation, overreliance on temporal alignment, and reframes it through semantic alignment.\n\n2. Proposes a concrete and implementable set of metrics (SA Score, SA-Date F1, SA-ROUGE, STA-ROUGE) that effectively capture semantic, temporal, and textual dimensions.\n\n3. Demonstrates strong empirical validation across multiple datasets and LLM architectures, showing the robustness and interpretability of the proposed evaluation paradigm."}, "weaknesses": {"value": "1. Missing key literature in timeline summarization (e.g., Timeline Generation through Evolutionary Trans-temporal Summarization, Learning towards Abstractive Timeline Summarization), which weakens the positioning of this work in prior research.\n\n2. Ignores the non-uniqueness of references, which is a central challenge in summarization, where different annotators may emphasize different key events; this limitation is especially critical for an evaluation-focused paper.\n\n3. Lacks qualitative case studies illustrating the practical weaknesses of existing metrics or how SA metrics provide clearer, more human-consistent judgments."}, "questions": {"value": "Can you give a representative case study to show the limitations of the current summarization models? The intuitive understanding is that LLM can already perform pretty well on summarization tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OX0uD3UtaY", "forum": "SsIfTN5b0V", "replyto": "SsIfTN5b0V", "signatures": ["ICLR.cc/2026/Conference/Submission16024/Reviewer_i2ue"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16024/Reviewer_i2ue"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760772096883, "cdate": 1760772096883, "tmdate": 1762926227459, "mdate": 1762926227459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new evaluation framework in which all metrics are grounded on semantically aligned sentence pairs rather than merely time-aligned milestones, followed by four derived metrics: SA Score, SA Date-F1, SA-ROUGE, and STA-ROUGE. Compared with the traditional metrics such as Date F1, A-ROUGE, the proposed metrics can more faithfully assess whether the predicted timeline and the reference timeline truly refer to the same events. To illustrate the benefits, this paper developed FS-LLM-TLS, a refined LLM-based summarizer, and evaluated on three datasets (Entities, Crisis, T17). Results show their semantic metrics align better with human evaluations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1)  The proposed evaluation framework includes four sub-metrics, which can capture the candidate's quality from multiple perspectives such as semantic coverage, temporal fidelity, and textual quality.\n(2)  This paper conducts numerous ablation studies to further validate the effectiveness of the method proposed in the paper."}, "weaknesses": {"value": "(1)  The paper tries to validate the effectiveness of the proposed metrics by comparing the performance gains achieved by traditional metrics and the proposed metrics in evaluating the baseline and FS-LLM-TLS, while it is insufficient to verify.\n(2)  This paper lacks a quantitative comparison between the proposed evaluation metrics and the baseline evaluation capability. It would be better to calculate the correlation coefficients between the quality score obtained from evaluation metrics and human evaluators, which can reflect the correlation between metrics and human ratings. The representative correlation coefficients include Kendall-Tau and Spearman, which are typically used to evaluate the generative text metrics, such as BERTSCORE (https://arxiv.org/pdf/1904.09675), G-Eval (https://arxiv.org/abs/2303.16634).\n(3)  The paper lacks detailed descriptions of the method and experiments, which weakens the reproducibility.\n(4)  There are some typos in this paper, such as the extra “Hu et al.” in line 104-105, the missing citation of “Martschat & Markert (2017)” in line 316.\n(5)  This paper focus on proposing a semantic-based timeline summarization equation framework, but in the experimental section, the paper mostly demonstrates and analyzes FS-LLM-TLS, lacking sufficient discussion on the proposed metrics."}, "questions": {"value": "(1)  Which exact LLM and prompt were used for the yes/no event-equivalence decision? Was the same LLM used across all datasets and all model-size experiments, or was it tied to the generation model?\n(2)  This paper uses the large language model to evaluate the semantic similarity between predicted and reference, and then according to which calculated F1 and ROUGE scores. Has this paper tried designing a scoring prompt that directly scores the predicted, like a prompt-based evaluator? (such as G-Eval, GEMBA)\n(3)  For CE and DACE, are those your own human annotations, or numbers copied from prior TLS work? If the former, could you provide inter-annotator agreement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rklSgPnwp8", "forum": "SsIfTN5b0V", "replyto": "SsIfTN5b0V", "signatures": ["ICLR.cc/2026/Conference/Submission16024/Reviewer_svAZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16024/Reviewer_svAZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899884407, "cdate": 1761899884407, "tmdate": 1762926226963, "mdate": 1762926226963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of objectively evaluating timeline summarization systems, which generate concise event narratives from temporally ordered documents. Current metrics, like Date-F1 and A-ROUGE, assume that events aligned by date share the same meaning, unfairly penalizing abstractive or semantically equivalent summaries. To address this problem, they propose a semantic alignment–based evaluation framework that uses large language models to measure similarity between sentences, align them through bipartite matching, and compute a Semantic-Alignment Score. They introduce Semantic-Alignment Date-F1 and Semantic-Alignment ROUGE to jointly assess semantic coverage and temporal accuracy. They also introduce a new Full-Stage LLM-TLS method, and experiments show that both the approach and the metrics better capture true system performance and align more closely with human judgments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality:\n- the new metric seems novel and appropriate to update evaluation to the LLM-era\n\nQuality:\n- the metric and proposed method are effective relative to prior approaches\n\nClarity:\n- helpful figures and clear detailing of the method\n\nSignificance:\n- future work can use this evaluation metric to better assess performance improvements"}, "weaknesses": {"value": "1. Appendix information is not included in this version so I can't assess things like prompt templates.\n\n2. There is not section on ethics and LLM use.\n\n3. It is not clear to me whether the authors conduct a study of human judgments, and if they do, there are not enough details to understand what they did.\n\n4. Figure and table captions should include more details to explain the metrics, settings, and takeaways."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bFGXiMn0av", "forum": "SsIfTN5b0V", "replyto": "SsIfTN5b0V", "signatures": ["ICLR.cc/2026/Conference/Submission16024/Reviewer_24HJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16024/Reviewer_24HJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922661704, "cdate": 1761922661704, "tmdate": 1762926226512, "mdate": 1762926226512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}