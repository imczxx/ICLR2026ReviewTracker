{"id": "WatS7243Zl", "number": 20889, "cdate": 1758311480080, "mdate": 1759896953552, "content": {"title": "ReDiG: Reinforced Diffusion on Graphs for Decentralized Coordinated Multi-Robot Navigation with Smooth Formation Adaptation", "abstract": "Coordinated navigation is a fundamental capability for multi-robot teams to traverse complex unstructured environments.\nDuring navigation, robots are often required to maintain mission-specific formations, such as wedge formations for enhanced visibility and area coverage.\nHowever, rigid formations can hinder navigation in challenging scenarios like narrow corridors, which demand formation adaptation.\nReinforcement learning (RL) is commonly used for coordinated multi-robot navigation due to its ability to learn through interaction with the environment.\nHowever, its step-wise decision-making process often results in jerky motion.\nIn contrast, diffusion models generate smoother trajectories through probabilistic denoising, but rely heavily on high-quality demonstrations.\nCollecting such demonstrations is challenging in multi-robot systems due to the coordination and synchronization required among individual robots.\nTo address these issues, we introduce a novel method named Reinforced Diffusion on Graphs (ReDiG) to enable\ndecentralized coordinated multi-robot navigation with smooth formation adaptation. \nUnder a unified learning paradigm, ReDiG integrates:\n(1) graph learning for decentralized coordination to enable formation adaptation,\n(2) diffusion models for generating smooth individual robot trajectories, and\n(3) online RL to refine noisy demonstrations by leveraging feedback from environment interaction, which enables robot synchronization and guides effective diffusion training.\nWe evaluate ReDiG through extensive experiments in both indoor and outdoor environments using physical robot teams and robotics simulations.\nExperimental results show that ReDiG enables smooth formation adaptation and achieves state-of-the-art performance in coordinated multi-robot navigation within complex environments.\nMore details are available on the project website: https://anonymous23885.github.io/ReDiG", "tldr": "We propose Reinforced Diffusion on Graphs (ReDiG), which unifies graph learning, diffusion models, and online reinforcement learning to enable decentralized coordinated multi-robot navigation with smooth formation adaptation.", "keywords": ["Decentralized multi-robot systems", "Diffusion model", "Reinforcement Learning", "Graph Learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5f537fcac8ca259aca522d0bfb77708c0bc2eee6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper develops Reinforced Diffusion on Graphs (ReDiG), a unified learning paradigm for multi-robot navigation. ReDiG contains three components: (1) a graph neural network for decentralized coordination, (2) a diffusion model for individual trajectory generation, and (3) an online reinforcement learning method for team synchronization. Experiments conducted in both simulated and real-world scenarios with various settings illustrate the remarkable performance of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-structured and presented in a clear academic style.\n2. The authors provide strong motivation for their approach.\n3. Experiments with various robot types and formation shapes validate the effectiveness of ReDiG."}, "weaknesses": {"value": "1. ReDiG is a combination of several different algorithms rather than an elegant one.\n2. The graph neural network (GNN) is used as an encoder to encode the robot's observation and state for diffusion models. Similar techniques have been widely employed in prior works [1, 2].\n3. A standard diffusion model is used to generate trajectories without any guidance or projection, and prior work indicates its performance degrades rapidly as the number of robots increases [3, 4].\n4. The number of robots used in experiments is very limited (even in simulated environments). There is no challenging scenario considered in experiments.\n5. The Contextual Formation Integrity (CFI) of ReDiG is lower than that of AFOR in multiple settings.\n6. No significance testing is reported across methods\n\n\n\n\n\n**References**:\n\n[1] Wang, Yutong, et al. \"Scrimp: Scalable communication for reinforcement-and imitation-learning-based multi-agent pathfinding.\" 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2023.\n\n[2] Ma, Yixiao, et al. \"Privileged Reinforcement and Communication Learning for Distributed, Bandwidth-limited Multi-robot Exploration.\" arXiv preprint arXiv:2407.20203 (2024).\n\n[3] Shaoul, Yorai, et al. \"Multi-Robot Motion Planning with Diffusion Models.\" The Thirteenth International Conference on Learning Representations.\n\n[4] Liang, Jinhao, et al. \"Simultaneous Multi-Robot Motion Planning with Projected Diffusion Models.\" Forty-second International Conference on Machine Learning."}, "questions": {"value": "1. The authors claim that ReDiG can generate smooth trajectories. Did they employ any guidance or additional methods during the training or sampling process to ensure the smoothness of the generated trajectories, or did they simply rely on a standard diffusion model?\n2. Does the proposed approach provide any theoretical guarantees regarding feasibility, smoothness, or formation?\n3. What are the smoothness results of the baseline methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tUFYWdHLYU", "forum": "WatS7243Zl", "replyto": "WatS7243Zl", "signatures": ["ICLR.cc/2026/Conference/Submission20889/Reviewer_t7Md"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20889/Reviewer_t7Md"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760477678088, "cdate": 1760477678088, "tmdate": 1762999991566, "mdate": 1762999991566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ReDiG presents a decentralized online learning framework that synergizes graph neural networks, conditional diffusion policies, and actor-critic reinforcement learning to achieve smooth, adaptive formation navigation for multi-robot teams. Theoretically grounded by convergence bounds for both diffusion and value approximation, the method is validated in simulation and on physical robots, demonstrating great task success rate and superior trajectory smoothness. Limitations include shape-specific retraining, reliance on single-robot demonstrations."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.Propose a new multi-robot collaboration framework for formation controlling. This framework unifies decentralized graph neural networks, diffusion-based trajectory generation, and reinforcement-learning-driven formation synchronization. \n\n2.Rigorous convergence guarantee: Proves the explicit upper bounds on the KL divergence between learned and true action distributions for the conditional diffusion model, isolating prior mismatch, denoising error, and discretization error; provides a parallel finite-sample bound for the critic that separates statistical and algorithmic errors, ensuring monotonic improvement of both trajectory smoothness and value estimates during online training.\n\n3.Eliminates expert-demonstration bottleneck : This article bootstraps a diffusion policy via combining it with online RL, converting environmental reward into synchronized, formation-aware demonstrations without costly multi-robot experts."}, "weaknesses": {"value": "1.Fixed communication radius: The topology of multi-robot system is set manually, the ability of adaptation is absent.\n\n2.Formation-specific training: A separate model must be retrained for each desired shape (wedge, line, circle), precluding on-the-fly formation switching.\n\n3.Demonstration dependency: Initial supervision relies on single-robot planners (A*, RRT); no curriculum or self-supervised pre-training is explored."}, "questions": {"value": "1.Although 60-step early stopping achieves 223Hz, the latency and CPU/GPU utilisation for the full 100-step model are not given; such data are critical for deployment on resource-constrained situations.\n\n2.Recent multi-agent diffusion or offline RL baselines (MADiff, DoF) are omitted; a comparative discussion would verify ReDiG’s contribution.\n\n3.Only aggregate success rates are presented. Illustrative failure cases—e.g., localization drift, packet loss, or corridor congestion—and their recovery statistics would help ascertain robustness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2kL7DKatt7", "forum": "WatS7243Zl", "replyto": "WatS7243Zl", "signatures": ["ICLR.cc/2026/Conference/Submission20889/Reviewer_AVUS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20889/Reviewer_AVUS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654596572, "cdate": 1761654596572, "tmdate": 1762999991583, "mdate": 1762999991583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ReDiG, a framework which integrates GNNs, diffusion models, and RL to enable multi-robot motion planning. The method relies on a graph neural network to coordinate between trajectories generated by independent diffusion models corresponding to each robot. This ensemble is well motivated by prior success utilizing GNNs, and the reported results demonstrate greater efficiency than the compared baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Technical Presentation:** The paper is written in a technical format that is appropriate for the target audience. The exposition is fairly intuitive.\n\n- **Real-World Evaluation:** The inclusion of results from real-world deployment are compelling, providing support for the authors' claims of practical utility.\n\n- **Time Travel Efficiency:** The faster paths provided by ReDiG provide are interesting, indicative of more optimal paths."}, "weaknesses": {"value": "- **Baseline Selection:** It is surprising that the paper does not compare to any recent diffusion motion planning baselines, e.g., [1-3]. This seems appropriate, especially when claiming SOTA performance. It is difficult to assess the performance on the method without this analysis. Comparison is limited to older RL baselines and GNN-based approaches; because of this, the evaluation seems incomplete. Could the authors please comment on why these methods have not bee compared to?\n\n- **Scope of Contribution:** Presently, I'm not convinced of the overall novelty of the framework. While this ensemble of methods is unique, the methodology centers on ensembling existing tools (e.g., GNNs for coordination, Diffusion Models for motion planning, and RL). While this is indeed a contribution, it seems to fall more on the engineering side of things.\n\n\n---\n\n[1] Carvalho, Joao, et al. \"Motion planning diffusion: Learning and planning of robot motions with diffusion models.\" 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2023.\n\n[2] Shaoul, Yorai, et al. \"Multi-robot motion planning with diffusion models.\" arXiv preprint arXiv:2410.03072 (2024).\n\n[3] Liang, Jinhao, et al. \"Simultaneous Multi-Robot Motion Planning with Projected Diffusion Models.\" arXiv preprint arXiv:2502.03607 (2025)."}, "questions": {"value": "- It seems that better TT comes at the cost of tolerance to $\\delta$? Is ReDiG capable of performing this trade-off (e.g., tighter tolerance is required for a particular task)? Has any analysis of this been conducted?\n\n- Could the authors clarify which components of the framework are algorithmically novel, as opposed to a composition of existing techniques?\n\n- What limitations currently prevent ReDiG from scaling to a higher number of robots?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aQF5JaEBSV", "forum": "WatS7243Zl", "replyto": "WatS7243Zl", "signatures": ["ICLR.cc/2026/Conference/Submission20889/Reviewer_qZAD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20889/Reviewer_qZAD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856838589, "cdate": 1761856838589, "tmdate": 1762999992015, "mdate": 1762999992015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a learning-based method to achieve decentralized multi-robot navigation with smooth formation adaptation. Experimental results in both indoor and outdoor environments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Extensive experiments are conducted in both indoor and outdoor environments using physical robot teams and robotics simulations."}, "weaknesses": {"value": "The related work section is insufficiently discussed. The use of the diffusion model and its training process are unclear. The proposed algorithm is not clearly described and seems questionable. See Questions part for more details."}, "questions": {"value": "(1) Since this paper focuses on the distributed formation navigation problem, the authors are strongly suggested to compare their method with existing control-based methods, such as bearing-based and angle-based formation control methods, which enable multi-robot systems to reduce their formation size while maintaining the formation shape when passing through narrow passages.\n\n(2) It is stated in the appendix that classic path planning algorithms (e.g., RRT) can be used to generate expert demonstrations. Could you clarify how the expert trajectories are aligned or matched with the state–action pairs in your method?\n\n(3) The loss function in (1) is hard to understand. How do you determine $\\epsilon_k$? Why do you use $a_i^0$ instead of $a_i^k$? How is the notation $\\psi$ related to this loss function?\n\n(4) The gradient form of the loss function (1) in line 6 in Algorithm 1 seems wrong.\n\n(5) The actor update step in line 11 in Algorithm 1 seems wrong.\n\n(6) Please give the specific gradient formula for the GNN parameters.\n\n(7) The authors put only the expert trajectory and the transition generated by the diffusion model in the replay buffer. How do you update your actor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UfaUsOQigU", "forum": "WatS7243Zl", "replyto": "WatS7243Zl", "signatures": ["ICLR.cc/2026/Conference/Submission20889/Reviewer_igfk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20889/Reviewer_igfk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20889/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900595611, "cdate": 1761900595611, "tmdate": 1762937687393, "mdate": 1762937687393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}