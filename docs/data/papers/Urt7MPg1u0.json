{"id": "Urt7MPg1u0", "number": 14996, "cdate": 1758246599255, "mdate": 1759897336718, "content": {"title": "Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models", "abstract": "1-bit LLM quantization offers significant advantages in reducing storage and computational costs. However, existing methods typically train 1-bit LLMs from scratch, failing to fully leverage pre-trained models. This results in high training costs and notable accuracy degradation. We identify that the large gap between full precision and 1-bit representations makes naive adaptation difficult. In this paper, we introduce a consistent progressive training for both forward and backward, smoothly converting the full-precision weights into the binarized ones. Additionally, we incorporate binary-aware initialization and dual-scaling compensation to reduce the difficulty of progressive training and improve the performance. Experimental results on LLMs of various sizes demonstrate that our method outperforms existing approaches. Our results show that high-performance 1-bit LLMs can be achieved using pre-trained models, eliminating the need for expensive training from scratch.", "tldr": "we present a novel paradigm for training 1-bit large language models (LLMs) by leveraging pretrained floating-point models, addressing the limitations of existing 1-bit quantization methods that rely on training from scratch.", "keywords": ["Binary Nueral Networks", "Large Languge Models", "Quantizaion-awareTraining"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e4ea81aa8b50a2b58cecd083a0ffa39e1ac88a37.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new 1-bit LLM quantization paradigm to overcome the instability and accuracy loss of existing binary quantization methods. The authors propose a consistent progressive training method to gradually transition from a near-linear quantization function to a sign function, reducing quantization error and preserving pre-trained knowledge: Binary-aware initialization ensures stable optimization. while dual-scaling compensation introduces learnable scaling factors to maintain accuracy. Experiments across various LLM scales show that this approach narrows the performance gap between 1-bit and full-precision models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written."}, "weaknesses": {"value": "1. PTB perplexity evaluation is confusing. For example, on the 3B model, BitNet b1.58 clearly outperforms the proposed BinaryLLM on C4 and WikiText2 perplexity, but performs significantly worse on PTB perplexity. This inconsistency is also observed in other model evaluations, including the 1.3B experiments. It is recommended to recheck the PTB perplexity evaluation.\n\n2. In Section 4.2, the training data and base LLM are not aligned. In the 130M experiment, BinaryLLM uses SmolLM data, while FBI-LLM is trained on the Amber dataset. The SmolLM data is of notably higher quality. Moreover, compared with OneBit, BinaryLLM adopts a stronger base LLM (e.g., LLaMA-3B). These differences could lead to unfair comparisons.\n\n3. BitNet also proposes a 1-bit LLM [1], but the paper lacks a comparison with this baseline under the same dataset and experimental setting.\n\nI recommend that the authors **conduct comparisons under strictly comparable experimental conditions**, e.g., training on the same dataset and starting from the same BF16 model, rather than simply comparing with results reported in previous papers. This is particularly important considering that all experiments are conducted on relatively small-scale models.\n\n[1] Wang, Hongyu, et al. \"Bitnet: Scaling 1-bit transformers for large language models.\" arXiv preprint arXiv:2310.11453 (2023)."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HAG3Iv9UUF", "forum": "Urt7MPg1u0", "replyto": "Urt7MPg1u0", "signatures": ["ICLR.cc/2026/Conference/Submission14996/Reviewer_A8sH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14996/Reviewer_A8sH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760960730157, "cdate": 1760960730157, "tmdate": 1762925324045, "mdate": 1762925324045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a novel 1-bit training framework for large language models (LLMs). It introduces a progressive training strategy to enable smooth transition from full-precision to binarized weights, complemented by binary-aware initialization and dual-scaling mechanisms to further boost performance. Experimental results demonstrate that the framework requires fewer training tokens while achieving superior performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. For the 130M model, BinaryLLM requires only 20B training tokens. This is significantly fewer than the 1.26T tokens needed for training a model from scratch.\n2. The insight that well-trained models are harder to quantize while they still outperform under-trained ones after binarization, is interesting, and this point is thoroughly discussed.\n3. The motivation is clear. The method is generalizable, and the performance is satisfactory."}, "weaknesses": {"value": "1. For the ablation study on progressive training (Table 4), the comparison should use BinaryLLM without binary-aware initialization and dual-scaling. Merely comparing results between BinaryLLM and IR-Net fails to highlight the effectiveness of progressive training.\n2. The binary-aware initialization yields only marginal improvements, as shown in Table 9.\n3. There are some typos. For instance, line 372 should read \"from smallest to largest\"."}, "questions": {"value": "1. The authors claim in the paper: \"At convergence, inference is performed with the Sign function instead of F(x, t), incurring negligible error.\" Are there any quantitative results to support this assertion?\n2. How did the authors determine the coefficients of the function t(c) = 1.3 × e^(0.22c) − 1.3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9UtyBWCjVH", "forum": "Urt7MPg1u0", "replyto": "Urt7MPg1u0", "signatures": ["ICLR.cc/2026/Conference/Submission14996/Reviewer_BYvM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14996/Reviewer_BYvM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663108888, "cdate": 1761663108888, "tmdate": 1762925323433, "mdate": 1762925323433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BinaryLLM, a new framework for training 1-bit large language models directly from pre-trained weights instead of training from scratch. The method introduces consistent progressive training to smoothly transition weights from full-precision to binary form, binary-aware initialization to preserve salient weights across layers, and dual-scaling compensation to balance quantization error and accuracy. Experiments on multiple LLMs show that BinaryLLM achieves great results among 1-bit LLMs, significantly reducing performance gaps with full-precision models while requiring far less training cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a thoughtful and well-motivated attempt to make 1-bit quantization more practical for large language models. The idea of leveraging pre-trained weights rather than training from scratch is both efficient and timely, and the proposed progressive training and dual-scaling strategies are conceptually sound. The paper is clearly written with comprehensive experiments."}, "weaknesses": {"value": "Although the authors include additional results on LLaMA2-7B in the appendix, larger-scale validation (e.g., 13B or 30B models) is still missing, leaving some uncertainty about scalability under truly large-model settings. The comparison baselines are reasonably strong, but despite explicitly discussing the instability of 1-bit quantization on newer models such as Qwen3, the authors do not include direct experiments on it. This omission leaves open how well BinaryLLM performs on the latest LLM architectures. Moreover, the discussion on training stability and convergence focuses mainly on the design of the progressive parameter t and its scheduler, without quantitative analysis to substantiate robustness claims. In addition, while binary-aware initialization and dual-scaling compensation are described in detail and empirically shown to help, their theoretical justification and computational complexity are only briefly discussed."}, "questions": {"value": "1. Could the authors provide results or discussion on how BinaryLLM scales to larger models? Even limited experiments or resource estimates would help clarify whether the proposed training strategy remains stable and effective at larger scales.\n2. Since the paper explicitly mentions the instability of 1-bit quantization on newer architectures such as Qwen3, could the authors include or discuss experiments on such models to verify BinaryLLM’s generalization ability to the latest LLM families?\n3. The paper explains the progressive parameter t and its scheduling strategy, but lacks quantitative analysis. Could the authors provide sensitivity studies to better support the stability claims?\n4. While binary-aware initialization and dual-scaling compensation are shown to be effective empirically, could the authors elaborate more on their theoretical justification to clarify their computational overhead and convergence behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wYSiJzkKjA", "forum": "Urt7MPg1u0", "replyto": "Urt7MPg1u0", "signatures": ["ICLR.cc/2026/Conference/Submission14996/Reviewer_Zmu3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14996/Reviewer_Zmu3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14996/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930194513, "cdate": 1761930194513, "tmdate": 1762925322903, "mdate": 1762925322903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}