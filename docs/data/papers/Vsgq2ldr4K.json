{"id": "Vsgq2ldr4K", "number": 15818, "cdate": 1758255665865, "mdate": 1763696967371, "content": {"title": "Reasoning without Training: Your Base Model is Smarter Than You Think", "abstract": "Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during RL but are not present in the base models. In our work, we approach this question from a different angle, instead asking whether comparable reasoning capabilities can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, we show that our algorithm offers substantial boosts in reasoning that nearly match and even outperform those from RL on a wide variety of single-shot tasks, including MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in diversity over multiple samples that is characteristic of RL-posttraining. Crucially, our method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.", "tldr": "We find a training-free sampling algorithm that achieves reasoning boosts on base models comparable to those obtained by RL techniques.", "keywords": ["LLMs", "reasoning", "MCMC", "sampling", "inference-time compute"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/28265acf2346b4e058cb9a3769f2429b60296f39.pdf", "supplementary_material": "/attachment/0b907119eef470086fd13d0a090363cbb4c08d92.zip"}, "replies": [{"content": {"summary": {"value": "a new inference scaling method that can match the RL performance by spending cost during decoding, but does not require training, verifiers, datasets."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the method is conceptually simple and straightforward\n- the method does not require any training, data or verifier, and is entirely formed by the base model's own distribution"}, "weaknesses": {"value": "- need to pay cost for each sample, and if not paying for any inference scaling cost degenerates to sampling from p.\n- the evaluated benchmarks are rather older. do we expect the method to be still tractable with harder datasets that require longer CoT lengths? at least evaluate AIME25 or better HMMT25/BRUMO25, which are common math reasoning benchmarks to be tested on"}, "questions": {"value": "- why was there a decrease in performance when increasing the MCMC steps, was this more noisy as high likelihood samples could also be wrong? could be good to provide distributional stats for plot 5 b.\n- did you try your method on top of GRPO'd models, what will happen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bdEeeuGd0K", "forum": "Vsgq2ldr4K", "replyto": "Vsgq2ldr4K", "signatures": ["ICLR.cc/2026/Conference/Submission15818/Reviewer_nAW4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15818/Reviewer_nAW4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760685387069, "cdate": 1760685387069, "tmdate": 1762926047270, "mdate": 1762926047270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Great work!"}, "comment": {"value": "Dear authors, \n\nThis is an excellent work. Novel techniques to perform reasoning without any training are much needed, particularly since RL techniques can be expensive. I wanted to bring a concurrent work to your notice: Video Reasoning without Training (https://arxiv.org/abs/2510.17045). We also focus on the similar \"Reasoning without Training\" idea but from a video reasoning perspective. Our technique (called V-Reason) is based on an inference time optimization, so it is orthogonal to sampling based methods. We also show that inference time for our method is significantly faster than the RL-trained models simply by reducing output tokens.\n\nWe plan to discuss your paper in our work as a concurrent method. I would greatly appreciate if you could discuss our paper in a similar vein. The community needs to pay attention to this novel topic.\n\nAll the best,\nKartikeya"}}, "id": "OhirXRHMSN", "forum": "Vsgq2ldr4K", "replyto": "Vsgq2ldr4K", "signatures": ["~Kartikeya_Bhardwaj1"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Kartikeya_Bhardwaj1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15818/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763754205718, "cdate": 1763754205718, "tmdate": 1763754205718, "mdate": 1763754205718, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates how distribution sharpening can improve reasoning without additional training. The authors introduce a simple iterative sampling algorithm that leverages the base model’s own likelihoods to refine generation. Across three different base models and a range of benchmarks, the proposed method achieves competitive performance with alignment-trained models—while requiring no post-training or additional data."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper reframes alignment and reasoning improvement as a distribution-sharpening problem, bridging the gap between post-training RL methods and pure inference-time algorithms.\n- The proposed iterative sampling algorithms is simple, straightforward but exceptionally effective.\n- The authors conducted solid experiences across 3 base models and several benchmarks to demonstrate the proposed method's effectiveness."}, "weaknesses": {"value": "- The experiments are only conducted on small models such as Qwen2.5-7B. Experiments on more advanced models and larger models can further solidify the empirical exploration. But it is understandable given the resources in academia.\n- The evaluation spans four benchmarks, which, while representative, do not fully capture the diversity of reasoning and generation tasks."}, "questions": {"value": "- Why does applying GRPO to Phi-3.5-mini-instruct result in performance degradation on two math benchmarks? The authors should further analysis the underlying cause of this phenomenon."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CMME9dcpy2", "forum": "Vsgq2ldr4K", "replyto": "Vsgq2ldr4K", "signatures": ["ICLR.cc/2026/Conference/Submission15818/Reviewer_4b7A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15818/Reviewer_4b7A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761439999824, "cdate": 1761439999824, "tmdate": 1762926046755, "mdate": 1762926046755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free method to improve reasoning in large language models (LLMs) by sampling from a sharpened distribution of the base model. Inspired by ideas from Markov chain Monte Carlo (MCMC), the authors introduce an inference-time sampling algorithm that approximates sampling from a power distribution, where higher-likelihood sequences under the base model are exponentially upweighted."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper offers a fresh conceptual lens by interpreting RLVR-based improvements as a form of distribution sharpening, reframing how we think about reasoning emergence in LLMs.\n- The authors demonstrate that a simple, training-free algorithm can match or surpass GRPO-trained models across multiple benchmarks and model families."}, "weaknesses": {"value": "As noted by the authors, the method can only exploit existing capabilities of the base model—it cannot exceed its representational capacity. A deeper discussion or analysis of the top-k ceiling would help contextualize the results."}, "questions": {"value": "- Have the authors evaluated Power Sampling on open-ended tasks like story writing or dialogue? If not, what properties of reasoning tasks (e.g., verifiability, objective correctness) make sharpening effective?\n- Is a higher base model likelihood always desirable? Could overly sharp distributions lead to degenerate or repetitive text, as seen in low-temperature sampling?\n- In my own experiments, sharpened sampling does not seem to improve autoregressive image generation. Do the authors believe that *reasoning tasks* uniquely satisfy conditions that make $p^{\\alpha}$ beneficial?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BCLVWVAb5N", "forum": "Vsgq2ldr4K", "replyto": "Vsgq2ldr4K", "signatures": ["ICLR.cc/2026/Conference/Submission15818/Reviewer_mX2n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15818/Reviewer_mX2n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917911849, "cdate": 1761917911849, "tmdate": 1762926046261, "mdate": 1762926046261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper shows that strong reasoning performance can be achieved from base LLMs without any additional training by changing how we sample from them.\n- It introduces a power sampling algorithm that draws from a sharpened distribution $p_{\\alpha}(\\mathbf{x}) \\propto p(\\mathbf{x})^\\alpha$, emphasizing higher-likelihood reasoning traces.\n- The method uses an MCMC-style resampling process to approximate sampling from this distribution efficiently at inference time.\n- Across several benchmarks such as MATH500, HumanEval, and GPQA, this training-free approach matches or surpasses GRPO-based post-training.\n- The results suggest that base models already encode strong reasoning abilities, and that better inference-time sampling can unlock them without retraining."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The method achieves RL-level reasoning performance without any additional training, data, or reward signals.\n- It provides a clear theoretical analysis showing why power sampling differs fundamentally from conventional low-temperature sampling.\n- The algorithm is simple, mathematically principled, and broadly applicable to existing base language models.\n- The work reframes reasoning enhancement as an inference-time sampling problem rather than a post-training or reward-learning problem."}, "weaknesses": {"value": "- It depends on tuning the power factor, block size, and number of MCMC steps, which may limit plug-and-play usability.\n- The algorithm provides no theoretical guarantee of convergence to the true power distribution in large sequence spaces.\n- The paper evaluates only single-shot responses and does not test multi-turn reasoning or chain-of-thought extensions."}, "questions": {"value": "- How can $\\alpha$ be selected practically and automatically without extensive tuning?\n- Does power sampling remain effective for longer reasoning chains or multi-turn dialogues?\n- How does the diversity of outputs change with increasing $\\alpha$ and MCMC steps, and is there a risk of collapsing to repetitive or overconfident generations?\n- Have you considered evaluating the proposed method on more challenging or diverse reasoning benchmarks such as MMLU?\n- Could you provide insights into why $\\alpha = 4.0$ consistently works well across tasks?\n- Have you considered using tempered transitions within your MCMC framework to improve chain mixing, especially when sampling from highly sharpened distributions for long reasoning sequences?\n- In Line 17, Line 46, and more, capabilites -> capabilities\n- In Line 356, algorihtms -> algorithms\n- The figures use multiple shades of blue that differ only in brightness, making it difficult to distinguish between lines or bars, especially when printed or viewed in grayscale. The authors are encouraged to adopt a more distinguishable color palette.\n- Use \\citep and \\citet appropriately.\n- Ensure that the citation formats are consistent, the capitalization is correct, and the reference information is up-to-date."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There is no particular ethical concern."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eTj8b7DmBd", "forum": "Vsgq2ldr4K", "replyto": "Vsgq2ldr4K", "signatures": ["ICLR.cc/2026/Conference/Submission15818/Reviewer_L8G7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15818/Reviewer_L8G7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15818/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967036056, "cdate": 1761967036056, "tmdate": 1762926045614, "mdate": 1762926045614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}