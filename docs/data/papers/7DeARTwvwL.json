{"id": "7DeARTwvwL", "number": 4412, "cdate": 1757675977859, "mdate": 1759898033921, "content": {"title": "FlowBind: Efficient Any-to-Any Generation with Bidirectional Flows", "abstract": "Any-to-any generation seeks to translate between arbitrary subsets of modalities, enabling flexible cross-modal synthesis. \nDespite recent success, existing flow-based approaches are challenged by its inefficiency, as they require large-scale datasets often with restrictive pairing constraints, incur high computation cost from modeling joint distribution, and multi-stage training pipeline.\nWe propose \\textbf{FlowBind}, an efficient framework for any-to-any generation. \nOur approach is distinguished by its simplicity: it learns a shared latent space capturing cross-modal information, with modality-specific invertible flows bridging this latent to each modality. \nBoth components are optimized jointly under a single flow-matching objective, and at inference the invertible flows act as encoders and decoders for direct translation across modalities.\nBy factorizing interactions through the shared latent, FlowBind naturally leverages arbitrary subsets of modalities for training, and achieves competitive generation quality while substantially reducing data requirements and computational cost.\nExperiments on text, image, and audio demonstrate that FlowBind attains comparable quality while requiring up to 6× fewer parameters and training 10× faster than prior methods.", "tldr": "We propose an efficient flow-based multimodal generation model with bidirectional flows.", "keywords": ["Generative models", "Flow matching", "any-to-any generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5733399092696e5da6ff79e85add34384c50ee62.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces FlowBind, a novel framework for any-to-any multimodal generation. The core idea is to learn a shared latent space that captures cross-modal semantics, and to connect each modality to this space via a dedicated, invertible flow model. The shared latent and the modality-specific flows—are trained jointly under a single flow-matching objective, which allows the model to learn from arbitrary data pairings (e.g., text-image, image-audio) in a unified, single-stage training process.\n\nThe authors claim this design offers significant advantages in efficiency and data flexibility. At inference, the invertible flows function as encoders and decoders, enabling direct translation between any subset of modalities by first mapping the source modality to the shared latent space and then mapping from the latent space to the target modality. The paper presents experiments on text, image, and audio generation tasks, arguing that FlowBind achieves competitive performance against prior methods like CoDi and OmniFlow, while requiring substantially fewer parameters and less training computation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "*   **Novel and Elegant Framework Design**: The paper proposes a conceptually clean and novel framework for any-to-any generation. The idea of factorizing the complex joint distribution of multiple modalities through a learnable shared latent space, connected by modality-specific invertible flows, is an elegant and original approach to this problem.\n\n*   **Efficiency and Training Simplicity**: A strength of FlowBind is its computational and data efficiency. By design, the framework avoids the quadratic complexity of joint modeling and simplifies the training process into a single stage with a unified objective. The reported reductions in parameter count and training time are substantial, which is a valuable contribution.\n\n*   **Clarity of Presentation**: The paper is well-written and the core concepts are explained with notable clarity. The illustrations provide an intuitive and effective overview of the training and inference processes, making the proposed methodology easy to follow and understand."}, "weaknesses": {"value": "Despite its novel framework, the paper's experimental evaluation has several weaknesses that make it difficult to accurately assess its claimed \"competitive\" performance in the context of the current state of the art.\n\n1.  **Outdated and Insufficient Baselines**: The primary weakness of this work is the choice of baselines for comparison, many of which are no longer representative of the state-of-the-art. For instance, in the \"Specialists\" category of Table 2 and 3, **SD3-Medium** was released over a year ago, and **BLIP2** over two years ago. Similarly, audio models like **AudioLDM-L-full** and **WavCaps** are also established prior work. While comparing to \"Generalists\" like CoDi is relevant, the rapid progress in the field means that a truly competitive evaluation must include more recent and powerful models. The lack of comparison to any contemporary MLMMs or recent foundation models makes it impossible to gauge how FlowBind's efficiency-focused design trades off against the generation quality achieved by top-tier systems. This omission leaves a significant gap in the evaluation.\n\n2.  **Limited Scope of Modalities and Tasks**: The experiments are confined to only three modalities (text, image, audio) and primarily focus on one-to-one generation tasks.\n    *   While the framework is presented as \"any-to-any,\" its true scalability and effectiveness on a larger set of modalities (e.g., video) remain unproven.\n    *   The qualitative examples for many-to-one or one-to-many generation (e.g., Figure 2) are interesting but lack quantitative evaluation or strong baseline comparisons, making it difficult to assess performance on these more complex tasks which are central to the \"any-to-any\" promise.\n\n3.  **Contribution Confounded by Frozen Pre-trained Models**: The framework's true contribution is obscured by its heavy reliance on powerful, frozen pre-trained models such as EmbeddingGemma, CLIP, CLAP, and the Stable-UnCLIP decoder. This design effectively reduces the core learning task to aligning the latent spaces of these pre-existing expert models, rather than learning the full generative process. Consequently, the high generation quality reported may be largely inherited from the strong generative priors of these frozen components, which makes it difficult to isolate and evaluate the novel contribution of the FlowBind framework itself."}, "questions": {"value": "1.  **On the Choice of Baselines**: My primary concern is that the baselines used for comparison are not representative of the current state of the art, making it difficult to assess the true performance of FlowBind. Could you provide a justification for this choice? More importantly, to better situate your results, could you provide comparisons against more recent and powerful models, even on a subset of key metrics? For example:\n    *   **Text-to-Image**: How does FlowBind compare to recent diffusion models like **FLUX.1[1]**?\n    *   **Image-to-Text**: How does it perform against current vision-language models like **LLaVA-NeXT[2]**?\n    *   **Text-to-Audio**: How does it stack up against state-of-the-art audio generators like **Tangoflux[3]**, or **AudioX[4]**?\n    *   **Audio-to-Text**: How does its performance compare to captioning systems built on top of strong speech models like **Qwen2-audio**?\n\n2.  **On the Contribution of the FlowBind Framework**: The method relies heavily on very powerful frozen pre-trained models (Gemma, CLIP, Stable-UnCLIP decoder, etc.). This makes it difficult to disentangle the performance gains from your novel flow-based alignment versus the strong priors inherited from these components. To better isolate the contribution of your framework, could you provide an ablation study that uses different encoders/decoders? This would help clarify how much of the final quality is attributable to the learned flow itself.\n\n3.  **On the Scalability of the \"Any-to-Any\" Claim**: The experiments are limited to three modalities. While the framework is designed to be general, have you performed any preliminary experiments or analysis on how FlowBind would scale to a larger set of modalities, such as video?\n\n[1] Labs B F, Batifol S, Blattmann A, et al. FLUX. 1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space[J]. arXiv preprint arXiv:2506.15742, 2025.\n\n[2] Li F, Zhang R, Zhang H, et al. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models[J]. arXiv preprint arXiv:2407.07895, 2024.\n\n[3] Hung C Y, Majumder N, Kong Z, et al. Tangoflux: Super fast and faithful text to audio generation with flow matching and clap-ranked preference optimization[J]. arXiv preprint arXiv:2412.21037, 2024.\n\n[4] Tian Z, Jin Y, Liu Z, et al. Audiox: Diffusion transformer for anything-to-audio generation[J]. arXiv preprint arXiv:2503.10522, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LON8rx9WIg", "forum": "7DeARTwvwL", "replyto": "7DeARTwvwL", "signatures": ["ICLR.cc/2026/Conference/Submission4412/Reviewer_1rzN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4412/Reviewer_1rzN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842940525, "cdate": 1761842940525, "tmdate": 1762917351177, "mdate": 1762917351177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FlowBind, a flow-based generative model designed for efficient any-to-any multimodal generation. The core innovation lies in the introduction of a learnable shared latent space that captures cross-modal commonalities, enabling each modality to connect to this latent via its own invertible flow. The model is trained end-to-end under a single flow-matching objective, eliminating the need for multi-stage pipelines or fully-paired datasets. Experiments across text, image, and audio modalities demonstrate that FlowBind achieves competitive performance with significantly reduced computational and data requirements compared to existing baselines like CoDi and OmniFlow.\nThe contribution can be summarized as:\n1. FlowBind enables any-to-any generation by factorizing multimodal interactions through a shared latent space, reducing computational complexity and supporting training with partially paired data.\n2. The model jointly optimizes the shared latent and modality-specific flows under a single objective, avoiding complex multi-stage training pipelines.\n3. FlowBind achieves strong performance with only 568M trainable parameters, 48 GPU-hours of training, and a fraction of the data required by prior methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured, clearly written, and easy to understand. The idea of a learnable shared latent as a dynamic anchor for multimodal flows is both solid and well-motivated, addressing limitations of fixed anchors and joint conditioning models.\n2. The evaluation covers six one-to-one cross-modal tasks (text-image, text-audio, image-audio, and their reverses) and complex many-to-many generation, with rigorous metrics (FID, FAD, CIDEr, CLIP, CLAP, AIS) for both quality and alignment. The qualitative results (e.g., preserving fine-grained details in multi-modal inputs) effectively showcase the framework’s expressiveness.\n3. The empirical results clearly demonstrate FlowBind’s advantages: requiring up to 6× fewer parameters, training 10× faster, and using less than 10% of the data while achieving competitive or better generation quality. The single-stage training pipeline (avoiding multi-stage alignment/generation decoupling) further enhances its practicality for real-world scenarios."}, "weaknesses": {"value": "1. While the paper empirically demonstrates that FlowBind outperforms larger models trained on more data, it lacks a thorough and detailed analysis explaining why this is the case. The authors should provide a deeper investigation into the factors contributing to this unreasonable efficiency. \n2. Relying on CLIP (and analogous pre-trained encoders) introduces inherent limitations in capturing fine-grained details, directly constraining generation accuracy. CLIP’s training is driven by coarse-grained semantic alignment between images and text. This may lead to systematic loss of fine visual patterns—such as object positions, subtle textures, or local structure variations.\n3. Although the paper includes a basic interpolation analysis (Figure 3) and alignment metrics (Table 5), it lacks a comprehensive interpretability analysis of the learned shared latent space. The authors should provide a visualization of the relationship between the latents and generated contents.\n4. (Not Important) The experiments are limited to text, image, and audio. The paper does not address how FlowBind would scale to additional modalities. Questions remain about potential bottlenecks in the shared latent space or computational cost as the number of modalities grows."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bl1PURRFOd", "forum": "7DeARTwvwL", "replyto": "7DeARTwvwL", "signatures": ["ICLR.cc/2026/Conference/Submission4412/Reviewer_XtMR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4412/Reviewer_XtMR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894837736, "cdate": 1761894837736, "tmdate": 1762917350871, "mdate": 1762917350871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work replaces the usual logic of any-to-any generation, “everything must talk to text” or “one giant joint flow over all modalities”, with a learnable shared latent and one invertible flow per modality, all trained with one flow-matching loss. On text–image–audio cross-modal generation, they beat/are competitive with CoDi/OmniFlow on 6 one-to-one tasks, especially strong on image-audio, while using 6× fewer params and ~10× less compute than OmniFlow (568M, 48 GPU-hr on H100)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. One shared latent + per-modality flows = no quadratic blow-up over modalities, unlike joint-time rectified flows.\n2. No “align to text first, then joint train” and also no post-merge stage. This offers engineering advantage.\n3. Each flow only needs its modality + latent, so they can actually use image–audio pair easily.\n4. Simple averaging in latent still gives multi-condition outputs"}, "weaknesses": {"value": "1. For competing source modalities (conflicting audio + image like two contradicting semantics), how robust is plain averaging?\n2. This work focuses more on single cross-modal generation not joint-outputs generation.\n3. ODE cost at inference is per modality. There’s still ODE solves on both sides. And there is not enough report on runtime / steps / efficiency as the main claim of this framework is efficiency."}, "questions": {"value": "Right now you average the per-modality latents. Did you try learned fusion (attention / confidence weighting) and does it improve conflicting-condition cases (e.g. image says “dog” but audio says “car”)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "a8NfhP2wO3", "forum": "7DeARTwvwL", "replyto": "7DeARTwvwL", "signatures": ["ICLR.cc/2026/Conference/Submission4412/Reviewer_Hcpe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4412/Reviewer_Hcpe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988712940, "cdate": 1761988712940, "tmdate": 1762917350318, "mdate": 1762917350318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FlowBind, a flow‑based any‑to‑any generative framework that replaces a fixed Gaussian prior with a learnable, shared latent “anchor.” Each modality connects to this anchor via its own invertible drift network; all drifts and the auxiliary encoder producing the shared latent are trained jointly with a single flow‑matching objective. At inference, per‑modality ODEs are integrated backward to the shared latent and forward to the target modality, enabling many‑to‑many translation. The implementation freezes modality encoders/decoders (EmbeddingGemma for text, CLIP+Stable‑UnCLIP for images, CLAP for audio) and uses small MLP drifts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- [S1] The idea is simple and easy to follow.  \n- [S2] Promising efficiency: strong results with a lightweight model, fewer GPU‑hours than previous works.\n- [S3] Competitive results on several quantitative measurements (e.g., lower FID for T→I and A→I than baselines)."}, "weaknesses": {"value": "- [W1] Insufficient ablations for core claims. The main “analysis” section contains only two small studies: fixed text‑anchor vs learnable shared anchor (Table 4) and a CKNNA alignment probe (Table 5). There are no ablations on (i) the shared‑latent aggregation rule (simple averaging vs alternatives such as weighted averaging, or learned averaging), (ii) latent dimensionality or encoder/decoder choices, or (iii) the fraction of partially paired data. Without these, it is hard to attribute gains to the proposed factorization rather than to frozen backbones or evaluation choices.  \n- [W2] Partial‑pairing claim not quantified. The paper argues for learning from arbitrary partially paired data, but does not vary the paired/unpaired ratios or show robustness curves. This leaves the “data‑flexible” claim largely untested.  \n- [W3] Many‑to‑many evaluation is qualitative only. This use‑case is shown via figures and a website but lacks quantitative measurements.  \n- [W4] Mixed performance is under‑discussed. FlowBind underperforms OmniFlow on some alignment metrics (e.g., T→I CLIP), while excelling on others (e.g., AIS, CLAP); the paper does not analyze why or when factorization helps/hurts. More explanations are needed. \n- [W5] (Minor) Cost comparison caveat. The reported 10× compute reduction (48 vs 480 GPU‑hr) compares FlowBind’s full training to OmniFlow’s final joint stage only. Given the also‑much‑smaller data used by FlowBind, the speedup is promising but not an apples‑to‑apples pipeline cost."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The system can synthesize convincing cross‑modal content (e.g., voice from images/text, imagery guided by audio/text) which raises impersonation and misinformation risks if deployed without safeguards. The paper does not discuss provenance, watermarking, or misuse mitigation; adding such guidance is advisable."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aggaGo3XZU", "forum": "7DeARTwvwL", "replyto": "7DeARTwvwL", "signatures": ["ICLR.cc/2026/Conference/Submission4412/Reviewer_kcBk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4412/Reviewer_kcBk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4412/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047944255, "cdate": 1762047944255, "tmdate": 1762917349907, "mdate": 1762917349907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}