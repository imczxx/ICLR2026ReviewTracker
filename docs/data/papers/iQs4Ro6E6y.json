{"id": "iQs4Ro6E6y", "number": 3136, "cdate": 1757340551842, "mdate": 1763653437064, "content": {"title": "OmniFace: Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "abstract": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework OmniFace, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate OmniFace outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "tldr": "", "keywords": ["Video Face Swapping", "Diffusion Transformer"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b4f8b1bab2c62f29bb0177aa3ab0348bcce9caf0.pdf", "supplementary_material": "/attachment/cf3b90aa942a32f1b2ec2fadaa7f786194e116e2.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces OmniFace, a Diffusion Transformer-based framework for Video Face Swapping (VFS). It leverages advances from Image Face Swapping (IFS) to address challenges in maintaining identity similarity, attribute preservation, and temporal consistency in videos. The authors propose a data pipeline, SyncID-Pipe, and a new benchmark, IDBench-V, for video face swapping. Experiments show that OmniFace achieves superior performance and versatility compared to state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is well written and easy to follow.\n\n- IDBench-V is a new and diverse benchmark for video face swapping.\n\n- The authors conduct various experiments to demonstrate the effectiveness of the proposed method, both quantitatively and qualitatively."}, "weaknesses": {"value": "- While the paper is well executed and well written, I do not consider the core idea to be particularly novel. Both the data pipeline SyncID-Pipe and the OmniFace framework build on existing algorithms, and few new ideas or inspirations are proposed. The authors are encouraged to better highlight their contributions.\n\n- Simply being the first to apply an existing architecture (such as a Diffusion Transformer) to a new domain (video face swapping) is not always a significant or impactful novelty. The authors should provide more evidence as to whether unique challenges of the domain are addressed and whether the resulting system delivers meaningful improvements or insights.\n\n- The results in Figure 13 show that although this method can better change the identity, it does not seem to handle hair occlusion very well."}, "questions": {"value": "- What are the specific benefits of using a Diffusion Transformer (DiT) for face swapping, especially considering the computational cost, compared to other existing architectures?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "The improved realism and consistency of video face swapping can be misused for creating deepfakes."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BcQcb0tjT8", "forum": "iQs4Ro6E6y", "replyto": "iQs4Ro6E6y", "signatures": ["ICLR.cc/2026/Conference/Submission3136/Reviewer_9Kun"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3136/Reviewer_9Kun"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817904377, "cdate": 1761817904377, "tmdate": 1762916566978, "mdate": 1762916566978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents OmniFace, a comprehensive framework that transfers the strengths of image face swapping (IFS) into the video face swapping (VFS) domain. The approach introduces (1) SyncID-Pipe, a data generation pipeline that creates bidirectional ID quadruplets using an Identity-Anchored Video Synthesizer (IVS); (2) a Diffusion Transformer (DiT)–based architecture with Modality-Aware Conditioning (MC) for identity, structure, and context fusion; and (3) a Synthetic-to-Real Curriculum and Identity-Coherence Reinforcement Learning (IRL) mechanism to enhance realism and temporal consistency.\nThe authors also propose IDBench-V, a new benchmark for VFS, and show that OmniFace achieves state-of-the-art performance across identity similarity, attribute preservation, and video quality metrics, outperforming DreamID, CanonSwap, and Stand-In."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Data and benchmark contribution: SyncID-Pipe provides explicitly supervised video–image pairs, and IDBench-V offers a much-needed standardized evaluation dataset.\n\n2. Strong empirical results: Extensive experiments, ablations, and user studies consistently demonstrate superior performance in both quantitative metrics and perceptual quality.\n\n3. Well-structured paper: Clear methodology, detailed theoretical grounding, and transparent training setup."}, "weaknesses": {"value": "1. Lighting robustness. It remains unclear how the model performs under complex or rapidly changing illumination conditions. An ablation or robustness analysis in such scenarios would strengthen the paper.\n\n2. Identity change in Figure 1. I noticed that the identity appears to change in the first row of Figure 1. Does the method work reliably only when the source and target identities share similar facial structures or appearances? Could this limitation be related to the pose guidance mechanism?\n\n3. Ethical considerations. The paper does not sufficiently address the ethical implications of high-fidelity face swapping, such as potential misuse and the need for responsible use guidelines. Including a short discussion on these aspects would improve completeness and balance."}, "questions": {"value": "1. Lighting robustness. It remains unclear how the model performs under complex or rapidly changing illumination conditions. An ablation or robustness analysis in such scenarios would strengthen the paper.\n\n2. Identity change in Figure 1. I noticed that the identity appears to change in the first row of Figure 1. Does the method work reliably only when the source and target identities share similar facial structures or appearances? Could this limitation be related to the pose guidance mechanism?\n\n3. Ethical considerations. The paper does not sufficiently address the ethical implications of high-fidelity face swapping, such as potential misuse and the need for responsible use guidelines. Including a short discussion on these aspects would improve completeness and balance."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "Ethical considerations. The paper does not sufficiently address the ethical implications of high-fidelity face swapping, such as potential misuse and the need for responsible use guidelines. Including a short discussion on these aspects would improve completeness and balance."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J70TL1pgSH", "forum": "iQs4Ro6E6y", "replyto": "iQs4Ro6E6y", "signatures": ["ICLR.cc/2026/Conference/Submission3136/Reviewer_q98f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3136/Reviewer_q98f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839009517, "cdate": 1761839009517, "tmdate": 1762916566384, "mdate": 1762916566384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the challenges of video face swapping task in maintaining identity similarity and attribute preservation while preserving temporal consistency, the paper proposes the first video face swapping framework OmniFace based on DiT. A novel data pipeline SyncID-Pipe is introduced to transfer the superiority of IFS to VFS. IDBench-V, a comprehensive benchmark tailored for the video face swapping task is also introduced."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. New Data Pipeline: The proposed SyncID-Pipe pipeline transfers IFS’s strong ability of identity preservation to VFS task.\n2. Well-structured Writing: Each module is clearly described with informative figures.\n3. Comprehensive Experiments: The evaluation includes three major dimensions (identity consistency, attribute preservation, and video quality) with several SOTA models. A user study is also conducted.\n4. Thorough Experiments: Ablation study convinces the effect of each proposed component and their necessity.\n5. New Benchmark: The proposed IDBench-V benchmark fills a gap and provides a standardization for VFS evaluation."}, "weaknesses": {"value": "1. The introduction is redundant and overlapped with related works. The logic is confusing since modules of OmniFace are stacked together and lack correspondence with the addressed challenges.\n2. The novelty is incremental. DiT has been used in video generation tasks and Stand-In uses Wan2.1 as the video generation base model, which adopts DiT architecture. This undermines the contributions in terms of novelty.\n3. The training requires 6,000 GPU hours, which is costive. There should be a comparison about the computation cost and inference speed.\n4. The introduction of IDBench-V benchmark is insufficient. The IDBench-V includes only 200 samples, which is limited in size and diversity."}, "questions": {"value": "1. A comparison about the computation cost and inference speed should be provided.\n2. The modules in OmniFace is more like an integration of several strategies. Their correlation and correspondence with the goal should be better illustrated.\n3. Ablation study doesn’t include the Modality-Aware Conditioning module.\n4. Stand-in project page provides application in video face swapping. The comparison in experiment should adopt this version.\n5. Visualization and examples of the challenging scenarios (extreme head poses, severe occlusions, complex and dynamic expressions, and cluttered multi-person scenes) in IDBench-V should be provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W8hHXVR5TD", "forum": "iQs4Ro6E6y", "replyto": "iQs4Ro6E6y", "signatures": ["ICLR.cc/2026/Conference/Submission3136/Reviewer_BHGz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3136/Reviewer_BHGz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985240149, "cdate": 1761985240149, "tmdate": 1762916566023, "mdate": 1762916566023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniFace, a comprehensive framework designed to bridge the gap between image and video face swapping using a Diffusion Transformer (DiT) architecture. The authors propose a novel SyncID-Pipe data pipeline for constructing explicitly supervised paired data (bidirectional quadruplets) by integrating strengths from state-of-the-art image face swapping models and an Identity-Anchored Video Synthesizer (IVS). OmniFace employs a modality-aware conditioning mechanism to disentangle and inject multimodal information, a synthetic-to-real curriculum learning scheme, and an Identity-Coherence Reinforcement Learning (IRL) approach to improve robustness and consistency. The work further contributes a new benchmark, IDBench-V, to better evaluate video face swapping systems. Experimental results on IDBench-V and ablation studies demonstrate state-of-the-art performance, high versatility, and adaptability to extended human-centric swapping tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The SyncID-Pipe pipeline ingeniously leverages strong image face swapping performance and adapts it for video by crafting bidirectional ID quadruplets, ensuring explicit and effective supervision for video models. This pipeline is well illustrated in Figure 2 and is a core element in improving alignment between image and video domains.\n2.The Diffusion Transformer–based formulation is appropriately tailored for video, with a carefully crafted Modality-Aware Conditioning (MC) module for disentangling spatiotemporal, structural, and identity information (explained and visualized in Figure 3).\n3.The introduction of IDBench-V (as shown in Figure 8) fills an acknowledged gap by systematically evaluating face swapping methods in diverse real-world scenarios.\n4.All core equations (e.g., adaptive pose attention, Q-value definition, IRL loss, guidance purification) are specified with notation, and Appendix A.5 provides a proper theoretical justification for the IRL concept, relating it to reward-weighted likelihood maximization."}, "weaknesses": {"value": "1.Some aspects of the training process are not entirely transparent.\n2.The baseline selection (as shown in Table 1) is mostly well done, but some important recent works in video face swapping, especially those involving subject-agnostic or reenactment models (e.g., direct comparison with FSGAN, which is not referenced), are not included. Even though these may not be directly SOTA, including them would clarify how much improvement is due to architectural advances versus data curation. More detailed explanations for the omission of certain non-diffusion approaches would help.\n3.The limitations section (Appendix A.8) briefly notes issues with speed and lighting preservation but lacks concrete error analysis on where (or why) the model produces visible artifacts, identity drift, or temporal inconsistency."}, "questions": {"value": "1.Could the authors provide a more detailed breakdown of common failure cases for OmniFace, including qualitative examples and quantitative error rates for scenarios such as extreme lighting changes, multi-subject videos, or minority demographics?\n2.Is overfitting to the synthetic domain a concern?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mfuKAO53DT", "forum": "iQs4Ro6E6y", "replyto": "iQs4Ro6E6y", "signatures": ["ICLR.cc/2026/Conference/Submission3136/Reviewer_mwKb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3136/Reviewer_mwKb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3136/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762177800725, "cdate": 1762177800725, "tmdate": 1762916565286, "mdate": 1762916565286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We sincerely thank all the reviewers for their insightful reviews and constructive feedback. Taking into account each concern and question posed by the reviewers, we have given thorough responses within our rebuttal. Below is a summary of the key clarifications and improvements.\n- **Clarification of Novelty and Contributions:**\n    - We have more clearly summarized our contributions and novelty (Reviewer BHGz, 9Kun).\n    - We have improved the **Introduction** to more concisely summarize related work in video face swapping (Reviewer BHGz).\n    - We have more clearly articulated the motivation for building the model based on DiT (Reviewer 9Kun).\n- **Robustness Analysis and Visualization in Challenging Scenarios:**\n    - **Lighting Robustness** (Reviewer mwKb, q98f): We have designed a user study to evaluate lighting robustness and added more visual results: [Performance in Complex Lighting](https://github.com/iclr2026sub3136/iclr2026sub3136/blob/main/rebuttal.md#performance-in-complex-lighting).\n    - **Hair Occlusion Cases** (Reviewer 9Kun): Visual results are provided: [Performance on Hair Occlusion Scenarios](https://github.com/iclr2026sub3136/iclr2026sub3136/blob/main/rebuttal.md#performance-on-hair-occlusion-scenarios).\n    - **Limitations and Failure Cases Causing Artifacts** (Reviewer mwKb): We have added analysis on artifacts caused by low-quality source images: [Limitation from Low-Quality Source Image](https://github.com/iclr2026sub3136/iclr2026sub3136/blob/main/rebuttal.md#limitation-from-low-quality-source-image).\n    - **Facial Structure Differences Scenarios** (Reviewer q98f): [Handling Different Facial Structure](https://github.com/iclr2026sub3136/iclr2026sub3136/blob/main/rebuttal.md#handling-different-facial-structure).\n    - **Minority Demographics and Multi-Subject Videos** (Reviewer mwKb): [Generalization to Minority Demographics](https://github.com/iclr2026sub3136/iclr2026sub3136/blob/main/rebuttal.md#generalization-to-minority-demographics) and [Performance in Multi-Person Scenes](https://github.com/iclr2026sub3136/iclr2026sub3136/blob/main/rebuttal.md#performance-in-multi-person-scenes).\n- **Training Resources and Inference Costs:**\n    - We have added details on the training process and costs (Reviewer mwKb, 9Kun, BHGz).\n    - We have benchmarked the inference costs for both the baselines and OmniFace (Reviewer 9Kun, BHGz).\n- **Expanded Benchmarking and Ablation Studies:**\n    - We have constructed IDBench-V1000 (Reviewer BHGz, mwKb) and tested OmniFace's performance on it. We also included more visual results for IDBench-V (Reviewer BHGz): [Visualization of Challenging Scenarios in IDBench-V](https://github.com/iclr2026sub3136/iclr2026sub3136/blob/main/rebuttal.md#visualization-of-challenging-scenarios-in-idbench-v).\n    - We have added an ablation study for the Modality-Aware Conditioning module (Reviewer BHGz).\n- **Enhanced Comparisons and Ethical Considerations:**\n    - We have added FSGAN as a new baseline (Reviewer mwKb), with visual results available here: [Qualitative Comparison with FSGAN](https://github.com/iclr2026sub3136/iclr2026sub3136/blob/main/rebuttal.md#qualitative-comparison-with-fsgan).\n    - We have clarified the distinction with Stand-in (Reviewer BHGz), specifying that Stand-in is an ID-preserving model, not a video face swapping model, and have provided more comparative results: [Qualitative Comparison with Stand-in](https://github.com/iclr2026sub3136/iclr2026sub3136/blob/main/rebuttal.md#qualitative-comparison-with-standin).\n    - We have added a new Ethical Considerations section in **Appendix A.9** (Reviewer q98f, 9Kun).\n\nWe have provided all visual comparison results in https://github.com/iclr2026sub3136/iclr2026sub3136/blob/main/rebuttal.md to further support our claims and all modifications made to the paper have been highlighted in blue text for easy identification."}}, "id": "ukuPIKeXeB", "forum": "iQs4Ro6E6y", "replyto": "iQs4Ro6E6y", "signatures": ["ICLR.cc/2026/Conference/Submission3136/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3136/Authors"], "number": 18, "invitations": ["ICLR.cc/2026/Conference/Submission3136/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763718536820, "cdate": 1763718536820, "tmdate": 1763718536820, "mdate": 1763718536820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}