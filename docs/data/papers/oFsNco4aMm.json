{"id": "oFsNco4aMm", "number": 1621, "cdate": 1756898011493, "mdate": 1759898198071, "content": {"title": "UP2You: Fast Reconstruction of Yourself from Unconstrained Photo Collections", "abstract": "We present UP2You, the first tuning-free solution for reconstructing high-fidelity 3D clothed portraits from extremely unconstrained in-the-wild 2D photos. Unlike previous approaches that require \"clean\" inputs (e.g., full-body images with minimal occlusions, or well calibrated cross-view captures), UP2You directly processes raw, unstructured photographs, which may vary significantly in pose, viewpoint, cropping, and occlusion. Instead of compressing data into tokens for slow online text-to-3D optimization, we introduce a data rectifier paradigm that efficiently converts unconstrained inputs into clean, orthogonal multi-view images in a single forward pass within seconds, simplifying the 3D reconstruction. Central to UP2You is a pose-correlated feature aggregation module PCFA, that selectively fuses information from multiple reference images w.r.t. target poses, enabling better identity preservation and nearly constant memory footprint, with more observations. Extensive experiments on 4D-Dress, PuzzleIOI, and in-the-wild captures demonstrate that UP2You consistently surpasses previous methods in both geometric accuracy (Chamfer-15\\%$\\\\downarrow$, P2S-18\\%$\\\\downarrow$ on PuzzleIOI) and texture fidelity (PSNR-21\\%$\\\\uparrow$, LPIPS 46\\%$\\\\downarrow$ on 4D-Dress). UP2You is efficient (1.5 minutes per person), and versatile (supports arbitrary pose control, and training-free multi-garment 3D virtual try-on), making it practical for real-world scenarios where humans are casually captured. Both models and code will be released to facilitate future research on this underexplored task.", "tldr": "", "keywords": ["3D clothed human reconstruction", "image-based reconstruction", "human digitization", "SMPL", "multi-view diffusion model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83e3562067ce03c8e45c48a7be40c4ccca6ce5f2.pdf", "supplementary_material": "/attachment/0235aa4db285f85707f6fb9c843c7e7e03e5ca4e.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces UP2You, a tuning-free method for reconstructing high-fidelity 3D textured meshes of clothed humans from unconstrained photo collections (e.g., partial views, occlusions, varying poses, and viewpoints). Unlike prior approaches that rely on personalization via fine-tuning (e.g., DreamBooth + SDS optimization, which take hours), UP2You adopts a \"data rectifier\" paradigm: it aggregates features from input photos using a Pose-Correlated Feature Aggregation (PCFA) module within an MV-Adapter diffusion backbone to generate clean orthogonal multi-view RGB images and normals in seconds. It also includes a perceiver-based multi-reference shape predictor to estimate SMPL-X parameters without ground-truth templates. The pipeline culminates in mesh carving and texture baking adapted from PSHuman."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The tuning-free design enables fast reconstruction (1.5 minutes total, with rectification in <15 seconds), scaling well to varying numbers of inputs (1 to dozens) with near-constant memory via PCFA's selective aggregation. This makes it suitable for real-world scenarios like personal photo albums, outperforming slow optimization-based methods like PuzzleAvatar (4+ hours).\n2. PCFA effectively fuses pose-correlated features from diverse photos, leading to better identity preservation and 3D consistency. Quantitative gains over SOTAs (e.g., PuzzleAvatar, AvatarBooth, PSHuman) in geometry and texture metrics, plus qualitative scaling with more inputs, demonstrate robustness to occlusions, croppings, and viewpoints.\n3. The perceiver-based shape predictor eliminates the need for pre-captured templates or ground-truth shapes, enabling flexible applications like random pose animation and multi-garment virtual try-on—all without additional training."}, "weaknesses": {"value": "1. Limited core innovation in the pipeline: The method heavily builds on existing components (e.g., MV-Adapter for diffusion-based multi-view generation, PSHuman for final mesh reconstruction), with novelty primarily in PCFA and the shape predictor. This feels like a serial integration (rectification via finetuned diffusion + off-the-shelf reconstruction), potentially lacking a unified, groundbreaking advance beyond combining A+B-style modules.\n2. Dependency on diffusion priors and potential hallucinations: As a diffusion-based approach, it inherits risks of inconsistent outputs in extreme cases (e.g., highly occluded or sparse inputs), and the paper does not deeply address failure modes or ablation on the limits of rectification without further personalization.\n3. Evaluation scope and generalizability: Benchmarks are limited to specific datasets (PuzzleIOI, 4D-Dress, in-the-wild captures); lacks broader comparisons to recent diffusion-free methods or real-time systems. Runtime claims (1.5 minutes) may not hold on lower-end hardware, and virtual try-on is demonstrated but not quantitatively evaluated against specialized try-on models."}, "questions": {"value": "1. Could you elaborate on the novelty of PCFA beyond standard attention mechanisms in diffusion models? How does it differ from similar feature selection in prior works like ReferenceNet or multi-view adapters, and what ablations justify its design over simpler top-k strategies without pose correlation?\n2. The pipeline relies on MV-Adapter finetuning for rectification and PSHuman for reconstruction—how much of the performance gains stem from these existing tools versus UP2You's contributions? Would the method work with alternative backbones, or is it tightly coupled to diffusion-based generation?\n3. For the shape predictor, how robust is it to very sparse inputs (e.g., 1-2 photos with heavy occlusions)? The paper mentions consistency across inputs, but could you provide metrics on shape estimation variance or comparisons to single-image HMR methods like BEDLAM or HYBRIK?\n4. The weakness in innovation seems tied to the serial A+B nature (diffusion rectification + existing recon); how does UP2You advance beyond a straightforward combination of MVDiffusion and PSHuman, especially since normals/meshes are generated using adapted priors from these?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vJ5UMhpdT4", "forum": "oFsNco4aMm", "replyto": "oFsNco4aMm", "signatures": ["ICLR.cc/2026/Conference/Submission1621/Reviewer_VnCH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1621/Reviewer_VnCH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551229585, "cdate": 1761551229585, "tmdate": 1762915834904, "mdate": 1762915834904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "UP2You is a tuning-free method for reconstructing high-fidelity 3D clothed portraits from unconstrained 2D photos. It uses a \"data rectifier\" paradigm (via modules like PCFA and a perceiver-based shape predictor) to convert raw, unstructured inputs into orthogonal multi-view images and normal maps, enabling fast (1.5 minutes per person) and high-quality reconstruction. It outperforms baselines on geometric accuracy and texture fidelity across datasets like PuzzleIOI and 4D-Dress."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Processes unconstrained photos quickly (1.5 minutes total) with nearly constant memory usage regardless of input count.\n2. Surpasses SOTA methods in key metrics (e.g., 15% lower Chamfer distance on PuzzleIOI, 21% higher PSNR on 4D-Dress) via selective feature aggregation.\n3. Supports arbitrary pose control and training-free 3D virtual try-on without relying on pre-captured body templates."}, "weaknesses": {"value": "1. In 3D clothed portrait reconstruction, how does the rectifier ensure data quality when converting unconstrained photos into orthogonal views? What mechanisms maintain multi-view texture consistency and preserve fine details like clothing patterns and facial features, avoiding distortion or misalignment?\n\n2. For unconstrained inputs with unknown or noisy camera parameters, how does the rectifier align orthogonal views with accurate intrinsic and extrinsic settings? Does it use camera calibration or pose/shape priors (e.g., SMPL-X normal maps)? Is the alignment achieved via iterative optimization or direct mapping?\n\n3. How does this rectification improve the geometric accuracy of subsequent 3D reconstruction, such as reducing mesh carving or texture baking errors?\n\n4. The paper examines reference image count and feature aggregation (e.g., averaging vs. PCFA) but lacks ablation on key PCFA hyperparameters—like γ (feature retention ratio) or transformer block number. How do these affect reconstruction accuracy (e.g., smaller γ causing detail loss or larger γ adding noise), and why were the final values chosen?"}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "07MBVZPbvX", "forum": "oFsNco4aMm", "replyto": "oFsNco4aMm", "signatures": ["ICLR.cc/2026/Conference/Submission1621/Reviewer_8gj9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1621/Reviewer_8gj9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904685555, "cdate": 1761904685555, "tmdate": 1762915834704, "mdate": 1762915834704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new method, called UP2You, for reconstructing 3D humans from unstructured inputs with various poses, cameras, and visible body parts. To achieve this goal, the authors separate the procedure into two parts: multi-view orthogonal images generation and 3D mesh reconstruction. The major point lies in the generation of high-quality multi-view images. Specifically, (1) the authors propose a correlation map prediction that allows for selecting the most informative features from input images. Furthermore, this technique helps maintain the computational efficiency while having more input images. (2) MVDiffusion model is applied to generate the multi-view images based on the selected feature, pose, and normal maps. Extensive experiments have been proposed to demonstrate the performance of UP2You, presenting much better performance than existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The authors propose a novel and efficient method for reconstructing 3D humans from unstructured input images. Additionally, the reconstruction quality and efficiency are demonstrated to be much better than existing SOTAs.\n\n+ The proposed correlation map prediction is useful in finding the most informative features, improving the efficiency of the networks.\n\n+ The paper is well-writen and easy to follow"}, "weaknesses": {"value": "The reviewer believe the method is well-designed and may have some concerns regarding the experiments:\n\n- The method lacks the qualitative and quantitative comparisons with standard human reconstruction, such as PIFu, ICON, ECON. This would help demonstrate the reason why we need this type of method with unstructured inputs. \n\n- The paper would benefit from more results with loose clothing, complex poses as in ECON to demonstrate the performance.\n\n- The paper may also benefit from the evaluation of the pose variations and occlusion ratio presented in the input? For example, (1) can UP2You handle inputs with largely different poses? (2) what if some parts of the human body are not presented in the input images? (3) what's the maximal occlusion ratio in the used test set? One example might be: the input contains an image which captures only the foot part, while the other images don't contain the foot."}, "questions": {"value": "Besides the weaknesses listed above, the reviewer has one more question: is the current version of UP2You capable of animating the reconstructed human mesh, considering the input contains various poses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iPHChMqhaq", "forum": "oFsNco4aMm", "replyto": "oFsNco4aMm", "signatures": ["ICLR.cc/2026/Conference/Submission1621/Reviewer_zhP9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1621/Reviewer_zhP9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966577753, "cdate": 1761966577753, "tmdate": 1762915834398, "mdate": 1762915834398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for 3D reconstruction from a human mesh from relatively unconstrained photographs with the only requirement being that the person must be wearing the same clothes. The overall pipeline consists of three stages:\n\n1) First, from unconstrained photographs, six full body views are generated (with the appropriate inpainting)\n2) Second, using the aforementioned images and coarse SMPL normals, detailed normals are predicted\n3) Finally, features from the first step are used to refine the global shape using a perceiver like architecture.\n\nQualitative and quantitatively the proposed method outperforms prior work and is relatively fast."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper is well written and easy to follow\n2) The methodology is well motivated\n3) The single great insight of this paper is the discovery that correlation maps derived from DINOv2 via regression of its features helps generate multi-view images\n4) Strong quantitative and qualitative results"}, "weaknesses": {"value": "One weakness of this paper is the lack of proper ablation regarding two of it’s non-trivial decisions\n\n1) Why DINOv2 features? A plethora of semantic segmentation networks exists (including DINOv1) and at least some of them must be ablation to demonstrate the efficacy of constructing features maps via DINOv2\n\n2) The justification for using the Perciever model for shape refinement if relatively ad-hoc, it would be great if the authors could also share with more baselines such as a transferformer or a few MLPs"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CGXdPcaP3E", "forum": "oFsNco4aMm", "replyto": "oFsNco4aMm", "signatures": ["ICLR.cc/2026/Conference/Submission1621/Reviewer_FySx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1621/Reviewer_FySx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060344444, "cdate": 1762060344444, "tmdate": 1762915834071, "mdate": 1762915834071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}