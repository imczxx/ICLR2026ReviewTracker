{"id": "H0BZJxOmE4", "number": 4399, "cdate": 1757673622391, "mdate": 1759898034584, "content": {"title": "Unpacking Evaluation Pitfalls on Standard GNN Benchmarks", "abstract": "Graph Neural Networks (GNNs) have achieved substantial progress in graph-structured learning, with recent innovations targeting heterophilic graphs and attention-based designs such as Graph Transformers. These models are typically evaluated on widely used standard benchmark datasets for node and graph classification. In this work,  we identify a critical and often overlooked issue: these widely used benchmarks frequently suffer from significant class imbalance. Despite this prevalence, the GNN community predominantly relies on individual aggregate metrics namely \\textit{standard accuracy} and \\textit{AUROC}, on these datasets, often overlooking their limitations. While convenient, the existing aggregate measures could obscure class-level disparities and lead to incorrect conclusions about architectural effectiveness. Our work provides empirical evidence to demonstrate this limitation and advocate for a more robust evaluation framework that incorporates a diverse set of metrics (including balanced accuracy, AUPRC, and per-class metrics) to enable a transparent and reliable assessment of GNN capabilities.", "tldr": "Unpacking evaluation pitfalls on standard GNN benchmarks, particular imbalanced datasets.", "keywords": ["GNN", "Evaluation Pitfalls", "Imbalance", "Heterophily", "Node and Graph Classification", "Evaluation criteria."], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dedddb068e1b2d74d09f1e24bac6b88f77e1b569.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper argues that existing benchmarks for node and graph classification have focused on metrics, such as accuracy, that do not capture the class imbalance in the datasets considered. They argue in favor of the use of metrics, such as balanced accuracy and AUPRC, that better capture the behavior of GNNs on samples of the minority class. Empirically, the authors show that GNNs trained with standard protocols favor examples of the majority classes, as one would intuitively expect. \n\nAs the authors themselves put it, “This gap is not merely a statistical nuance; it profoundly limits our understanding of a model’s true generalization capabilities and its reliability in real-world, high-stakes applications like drug discovery or anomaly detection”."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of the paper is giving a voice to the often-neglected aspect of proper empirical evaluations in the GNN space, with emphasis on the class imbalance aspect. The article is mostly well written and employs a clear organization that makes it easy to understand and follow. \n\nThe paper feels like a very linear position paper, as it describes the problem of improper use of evaluation metrics for a selected number of common datasets, provides empirical evidence that current GNNs have not been thoroughly evaluated with respect to the class imbalance of datasets. I would not say that this paper proposes to perform a careful re-evaluation of GNNs considering the imbalance problem, as the training procedure itself has not been set up to take into account imbalance (but for the early stopping analysis at the end of Section 4). As such, the following comments will reflect this view that the paper wants to point out the problem rather than address it."}, "weaknesses": {"value": "Overall, I think this is a good paper, so the decision boils down to one question: “how impactful is this paper for the GNN community?” This question is particularly pertinent because the paper makes an excellent observation, but the results do not add much more to that in my opinion. This is the biggest limitation. I find that the text re-iterates many times on the concept that it is important to use the right metrics; however, it is very much expected to see such a big gap between accuracy and class imbalance-aware metric when an ML model is trained irrespective of such imbalance.\n\nIn this sense, it is my personal view that the paper should provide a more substantial contribution if we still want to believe ICLR is a top-conference and reviews try to improve submissions. An observation alone cannot be enough. More concretely, I believe that the paper could possibly (just suggestions, not requests for the rebuttal):\n- Provide precise evaluation guidelines for future works to help researchers correctly validate their models. These pertain to data splitting (e.g., stratified), how GNNs should be trained (e.g., early stopping based on better metrics, but also imbalance-aware training strategies to assess the true ability of models of capturing the class imbalance), etc.\n- With these evaluation guidelines in mind, perform a careful empirical re-evaluation of a few base models as well as SOTA ones on these datasets, including a more fine-grained hyper-parameter selection that considers different sizes for the hidden layer, for instance.\n\nDiscussing the first point only would already greatly improve the usefulness of the paper, but it still does not bring it to the expected level for an A*-level conference. Without a strong re-evaluation that addresses the highlighted problem, I would suggest that the authors submit to a less prestigious venue."}, "questions": {"value": "Questions:\n- Is the standard 80/10/10 split of previous works stratified? If not, I would expect this could significantly bias the results shown in the paper.\n- Why did the authors not perform hyperparameter tuning for the hidden layer size?\n- What is the added value of Figure 3 compared to Figure 1?\n\nSome further comments:\n- I find it interesting that some models like NodeFormer display a different accuracy on misrepresented classes, but it would be nice to see these results on a more consolidated framework. The results may suggest that NodeFormer, for instance, is not capturing well the class imbalance, but a proper training that accounts for it might completely change the ranking. This is one more reason why I believe that the empirical results in the paper do not represent an as impactful contribution as the main problem the authors pointed out.\n- Appendix B can be substantially improved, starting with the definition of Precision and Recall and trying to keep a uniform terminology across the different metrics. For instance, “Recall” is first called True Positive Rate and then sensitivity, which does not help the reader fix the concept in mind.\n- Please try to give proper credit, in the introduction and related work section, to the inventors of GNNs, namely the GNN of Scarselli et al. (2009, TNNLS) and the NN4G of Micheli (2009, TNNLS)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QvwwLV8bU0", "forum": "H0BZJxOmE4", "replyto": "H0BZJxOmE4", "signatures": ["ICLR.cc/2026/Conference/Submission4399/Reviewer_xAoy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4399/Reviewer_xAoy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760528631510, "cdate": 1760528631510, "tmdate": 1762917341988, "mdate": 1762917341988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that metric typically reported in graph machine learning research on standard graph datasets are poorly fit for the job. Specifically, as many of these standard datasets exhibit significant class imbalance, typically reported metrics such as accuracy for multiclass classification and AUROC for binary classification are unreliable, while metrics such as balanced accuracy and AUPRC provide more meaningful results. The paper shows that when models are evaluated with balanced accuracy and AUPRC, model performance appears to be much further from perfect than standard metrics suggest. Further, using balanced accuracy and AUPRC can uncover strong performance differences between models that appear to have nearly identical performance when evaluated with standard metrics. The paper also advocates the use of per-class metrics to obtain fine-grained insights into model behavior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important issue of selecting proper metrics for performance evaluation and can be viewed as part of a recent line of works criticizing the evaluation practices in graph machine learning (e.g., [1-3]). While the main message of the paper is very simple (when a dataset exhibits strong class imbalance, researchers should use appropriate evaluation metrics), it is an important message, and it is worth to bring it to the attention of the graph machine learning community. The paper highlights how unreliable modern evaluation practices can be. The experiments in the second part of Section 4.2 (Divergent Metrics paragraph) are interesting: they show that while two model modifications might appear performing almost identically under accuracy or AUPRC, using metric that are more appropriate for class-imbalanced datasets can show that there is in fact substantial difference between these models.\n\n\n\n[1] Graph Learning Will Lose Relevance Due To Poor Benchmarks (ICML 2025)\n\n[2] No Metric to Rule Them All: Toward Principled Evaluations of Graph-Learning Datasets (ICML 2025)\n\n[3] GraphLand: Evaluating Graph Machine Learning Models on Diverse Industrial Data (NeurIPS 2025)"}, "weaknesses": {"value": "- The paper does not discuss why metrics taking class imbalance into account can be more appropriate for model evaluation in practical applications. For example, in the setting of multiclass classification, if an incorrect prediction for any object leads to equal negative outcomes (e.g., monetary losses), then accuracy is a perfectly valid metric. However, if errors for objects from smaller classes lead to larger negative outcomes (e.g., from the point of view of business objectives or societal fairness), then balanced accuracy would be a better metric. I believe discussing these issues with concrete examples from popular datasets will significantly strengthen the paper.\n\n- The range of the datasets considered in the paper is rather narrow. While almost all popular datasets in graph machine learning exhibit class imbalance, the paper only considers a few of them. For example, only 4 datasets are considered for multiclass classification, and I believe one of them (squirrel-filtered) should not actually be used. This dataset was introduced in [4] to show that the original squirrel dataset is flawed because it has duplicate nodes and removing these nodes (thus obtaining squirrel-filtered) significantly changes model performance and ranking. However, it is not known what exactly is the source of duplicate nodes in the original dataset and whether removing them is meaningful. Thus, instead of using squirrel-filtered, [4] proposes to use an entirely new set of datasets for model evaluation under heterophily (and some of them are used in this work). Thus, I suggest not considering squirrel-filtered and replacing it with more meaningful datasets. For binary classification, only 3 datasets are considered. I suggest looking at more datasets from the MoleculeNet suit of molecular graphs (these datasets are also available via OGB). For example, the MUV dataset (mol-muv in OGB) has 17 binary prediction tasks each with strong class imbalance, while almost all works use AUROC to evaluate model performance on it, which is not an appropriate metric.\n\n- While the experiments in the second part of section 4.2 are interesting (as discussed above), the experiments in the first and third parts are not so interesting. The third part (Optimizing for different objectives) simply shows that if we optimize a certain metric A, then we will get better values of metric A than if we optimize some other metric B, which is a very expected result. The first part (Performance variation when taking class imbalance into consideration) shows that the absolute values of balanced accuracy or AUROC are lower than the absolute values of accuracy or AUPRC, respectively. However, these values are not directly comparable (since the metrics are different and have different interpretations). It would be more interesting to investigate whether model rankings change under different metrics (it is partly shown in the second part, but only for 2 variations of the same model, not for more variations or between different models). Overall, I believe providing more experiments that show how using inappropriate metrics can lead to making suboptimal decisions (e.g., choosing a model that is not the best) will significantly strengthen the paper.\n\n\n\nI am willing to raise my score if my concerns are addressed.\n\n\n\n[4] A critical look at the evaluation of GNNs under heterophily: Are we really making progress? (ICLR 2023)"}, "questions": {"value": "See weaknesses.\n\nMinor details:\n\n- Lines 319-323 repeat the same thing twice.\n\n- \"On several imbalanced ogbn-arxiv node classification datasets (Fig. 1e)\" - there seem to be erroneously omitted and/or inserted words here.\n\n- While the whole paper is dedicated to proper metrics for datasets with imbalanced classes, this is not reflected in the paper title. I suggest making the paper title more specific."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xefqiFcLty", "forum": "H0BZJxOmE4", "replyto": "H0BZJxOmE4", "signatures": ["ICLR.cc/2026/Conference/Submission4399/Reviewer_TdYK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4399/Reviewer_TdYK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760557315703, "cdate": 1760557315703, "tmdate": 1762917340348, "mdate": 1762917340348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work considers widely used GNN graph classification / node classification data sets. It argues that most papers consider accuracy/AUROC as their primary metric which may not be ideal in the face of severe class imbalance, particularly in the case of multi-class classification. They then conduct a series of experiments showing that different metrics lead to different insights / interpretations of the model performances."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a real issue that is underdiscussed, namely the overuse of \"overall acccuracy\" without much thought into the merits of this choice."}, "weaknesses": {"value": "For a paper that aims to improve the field, by promoting better use of evaluation metrics, there is relatively little discussion of the relative merits of each metric. For instance, when should one perfer balanced F1 scores over overall accuracy (and why)? What are the pros and cons of each? The definitions are in Appendix B, but there should be more discussion in the main paper (beyond definitions). \n\nSeveral of the classification tasks are actually ordinal regression tasks. For instance in Amazon, a mistake of 1 star vs 4 stars is significantly worse than 3 stars vs 4 stars. Similar with squirrel. These data sets, are of course, commonly treated as \"classification data sets\" but that is part of the problem that this paper, in its best form (future iterations maybe) should address.\n\nIts generally unclear what the contribution of this paper is other than (correctly) pointing out that ML papers commonly ignore experimental best practices, either for the sake of convenience or lack of statistical training. This is a real issue, but to in order for the paper to help remedy the situation, there should be more discussion of which metrics are appropriate in which settings and why"}, "questions": {"value": "For squirrel filtered (and other wikipedia derived data sets), the goal is to predict traffic level which, in principal is a a regression task (that is turned into an ordinal regression task by coarse graining in order to make the problem \"easier\".) Is there enough data publicly available to reformulate this is a regression task? That seems like this would alleviate the issues with accuracy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VSMFimcrTq", "forum": "H0BZJxOmE4", "replyto": "H0BZJxOmE4", "signatures": ["ICLR.cc/2026/Conference/Submission4399/Reviewer_vz42"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4399/Reviewer_vz42"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760888051169, "cdate": 1760888051169, "tmdate": 1762917339239, "mdate": 1762917339239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors identify several graph learning benchmarks that have significant class imbalance, yet are typically evaluated with class-imbalance-insensitive aggregate metrics like (standard) accuracy and AUROC, leading to diminished utility and potentially misleading conclusions on architectural effectiveness. They then demonstrate how class-imbalance-insensitive and -sensitive metrics and the models developed based on either sets of metrics differ through a series of studies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Stengths:**\n1. The authors identify a very important and clear underlying issue in (a subset of) graph learning benchmarks.\n2. The overall framing and message of the paper is very clear.\n3. Most experimental studies, while not without limitations (See Weakness 3), are well set-up and demonstrate the effect of evaluation metric selection on the resulting models in a variety of scenarios."}, "weaknesses": {"value": "**Weaknesses:**\n1. There is little to no novelty in the paper; it reads more like a “position-track” paper, albeit with more empirical support. This is not to downplay the claims or potential impact of the paper — on the contrary, such papers on evaluation procedures are very valuable in steering graph learning (or ML in general) research in the right direction. However, the paper’s standalone contributions are very limited beyond (a) identifying graph learning datasets with class imbalance, and (b) arguing that they should use more class-imbalance-sensitive metrics, with experimental studies to compare evaluations using class-imbalance-sensitive vs. -insensitive metrics. The lack of novelty is also somewhat obscured by the fact that the paper does not refer to similar problems or studies in general ML research or specific ML domains _at all_, which is an issue in itself (see Weakness 2).\n2. Class imbalance and appropriate metric usage has been fairly thoroughly discussed in general ML research and/or domain-specific research, which is not mentioned in related work. In fact, there are several counter-studies questioning the superiority of AUPRC in class imbalance settings [1, 2]; so while the overall argument for favoring AUPRC over AUROC is intuitive in certain cases or under some assumptions, I think the so-called superiority is not fully settled. I think the authors take this superiority for granted, and in the experimental section 4 simply show how the Acc vs Bal-Acc and AUROC vs AUPRC results (and downstream architectural decisions) differ, with little justification on why the results from one set of metrics are necessarily “better” than the other. While I do agree overall with the conclusions set forth by the authors, given the minimal novelty of the paper, I think the theoretical/empirical justifications provided need to be _much_ stronger than their current state to merit acceptance; something I expand upon in Weakness 3. Please also see the Questions section for some potential ideas towards this direction.\n3. I think the “divergent metrics” and “optimizing for different objectives” studies in section 4.2 are decent initial attempts to justify the use of class-imbalance-sensitive metrics from an empirical standpoint. However, the conclusions fall quite flat because of Weakness 2. This leads to trivial concluding arguments such as:\n   > In fig. 4, we observe that models selected based upon validation _balanced-accuracy_ yield a higher performance on _balanced-accuracy_, highlighting the choice of validation metric also plays a role in evaluation.\n\n   It should come as a surprise to no one that “a model selected upon validation metric X yields a higher performance on the same metric X”. I think more effort is required to go beyond trivial statements (unless I am misunderstanding the point made here).\n\n**Minor issues (no effect on score):**\n1. Why call it _GNN_ benchmarks rather than _graph learning_ benchmarks? I think the term GNN is somewhat interchangably used either to refer to (a) most graph learning architectures in general, or (b) ones that specifically operate on the graph structure, the latter case being common when distinguishing models like graph transformers (GT) as a separate class of graph learning methods. While I think the meaning is clear in your case, I think referring to these datasets as _graph learning_ benchmarks is a straightforward solution that avoids any potential semantic ambiguity.\n2. Typos:\n   - Some opening parantheses (“(”) lack a preceding space across the paper.\n   - L64-65: `\\citet` used instead of `\\citep` for Luo et al. (2024).\n\n[1] Richardson, E., Trevizani, R., Greenbaum, J. A., Carter, H., Nielsen, M., & Peters, B. (2024). The receiver operating characteristic curve accurately assesses imbalanced datasets. Patterns (New York, N.Y.), 5(6), 100994. doi:10.1016/j.patter.2024.100994\n\n[2] McDermott, M. B. A., Zhang, H., Hansen, L. H., Angelotti, G., & Gallifant, J. (2024). A closer look at AUROC and AUPRC under class imbalance. In Proceedings of the 38th International Conference on Neural Information Processing Systems (NeurIPS '24), Vol. 37. Curran Associates Inc., Red Hook, NY, USA, Article 1400, 44102–44163. doi:10.48550/ARXIV.2401.06091"}, "questions": {"value": "**Questions:** \n1. (Suggestion) The authors need to lean further in on already existing analyses of relevant ML evaluation metrics (from both sides such as the aforementioned counter-studies), perhaps relying more on statistical studies and theoretical work (again, I think the counter-studies [1, 2] are good examples on how to approach this problem, independent of the conclusions that may contradict this work) rather than intuition to provide a more convincing theoretical footing and demonstrate a more tangible contribution for the paper.\n2. (Suggestion) From a more empirical standpoint, the authors can look into whether models developed with the class-imbalance-sensitive metrics like AUPRC in the evaluation pipeline employ better generalization properties across tasks with different class distributions (e.g. in pre-training settings). I think providing several “case studies” in this vein that go beyond the examples associated with Weakness 3 would help demonstrate the merits of metric selection in a more clear manner.\n\n**Conclusion:** I think this paper is significantly more suitable for a position paper track or a benchmarking track in its current state; the limited novelty and the inability to convincingly argue for its conclusions lead me to vote for a (somewhat harsh) clear reject; even though I very much like the premise and message of the paper. I certainly encourage the authors to revise the paper accordingly to arrive at a much stronger work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zp3LzvHSEg", "forum": "H0BZJxOmE4", "replyto": "H0BZJxOmE4", "signatures": ["ICLR.cc/2026/Conference/Submission4399/Reviewer_bwF3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4399/Reviewer_bwF3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4399/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762043633108, "cdate": 1762043633108, "tmdate": 1762917338969, "mdate": 1762917338969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}