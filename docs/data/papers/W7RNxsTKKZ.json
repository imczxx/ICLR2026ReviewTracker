{"id": "W7RNxsTKKZ", "number": 16700, "cdate": 1758267817561, "mdate": 1763723650167, "content": {"title": "Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in LLMs", "abstract": "Facts change over time, making it essential for Large Language Models (LLMs) to handle time-sensitive factual knowledge accurately and reliably. Although factual Time-Sensitive Question-Answering (TSQA) tasks have been widely developed, existing benchmarks often face manual bottlenecks that limit scalable and comprehensive TSQA evaluation. To address this issue, we propose TDBench, a new benchmark that systematically constructs TSQA pairs by harnessing temporal databases and database techniques, such as temporal functional dependencies, temporal SQL, and temporal joins. We also introduce a new evaluation metric called time accuracy, which assesses the validity of time references in model explanations alongside traditional answer accuracy for a more fine-grained TSQA evaluation. Extensive experiments on contemporary LLMs show how TDBench enables scalable and comprehensive TSQA evaluation while reducing the reliance on human labor, complementing current TSQA evaluation approaches that largely center on Wikipedia/Wikidata by enabling LLM evaluation on application-specific data.", "tldr": "", "keywords": ["Large Language Models Benchmarking and Evaluation", "Time-Sensitive Question-Answering"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/29d1adea15a4369ff68ab01d3ae78ecb49373d62.pdf", "supplementary_material": "/attachment/25c371a80748e835b750c048b4746eca9f1668b4.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles a key problem: creating benchmarks for time-sensitive QA is slow and manual. The proposedTDBench uses temporal databases and SQL to automatically generate diverse and reliable QA pairs, bypassing the usual human effort. A key contribution is the *Time Accuracy* metric, which goes beyond checking the final answer to also verify the dates in the model's explanation. This provides a much deeper look into model factuality and hallucination."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The core idea of using temporal database theory to systematically generate a benchmark is highly innovative and effective. It is a bridge between two fields.\n- TDBench automates a tedious, expensive process. This makes it highly practical for both academic research and industry applications where up-to-date, domain-specific evaluation is needed.\n- Time accuracy reveals a critical layer of hallucination—correct answers with faulty temporal reasoning—that is otherwise missed by standard benchmarks.\n- The framework's use of 13 temporal relations ensures a much broader and more rigorous test of LLM capabilities than existing template-based methods."}, "weaknesses": {"value": "- The framework's success hinges on having a well-structured temporal database. This shifts the manual effort from question creation to data modeling and curation, which can be a significant bottleneck in itself.\n- The paper notes a ~9% error rate in the SQL-to-text step. This introduces a small but non-negligible risk that models are being evaluated on slightly flawed or ambiguous questions.\n- The need to explicitly prompt models for date information to measure *Time Accuracy*, while necessary, creates a test condition that may not fully reflect natural user interaction."}, "questions": {"value": "The framework elegantly shifts the manual effort from question design to database design. How do you see TDBench being applied in settings where the source data is semi-structured or messy (e.g., tables scraped from the web)? Does the automation promise hold if a significant data curation phase is required first?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9gUtJ1Af8p", "forum": "W7RNxsTKKZ", "replyto": "W7RNxsTKKZ", "signatures": ["ICLR.cc/2026/Conference/Submission16700/Reviewer_wK6u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16700/Reviewer_wK6u"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827675503, "cdate": 1761827675503, "tmdate": 1762926752819, "mdate": 1762926752819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the TDBench framework to evaluate LLMs on time-sensitive QA tasks. The framework addresses manual bottlenecks in existing TSQA benchmarks by automating both the construction and evaluation processes. TDBench uses Temporal Functional Dependencies (TFDs) to automatically identify valid QA attribute pairs. Temporal SQLs are generated attributes identified and 13 temporal relations. These SQLs are now translated into Natural Language Questions using GPT-4o, and the ground-truth answer is obtained by executing the query against the database. A new metric, \"Time Accuracy\", is also introduced, which evaluates the correctness of the temporal references in a generated answer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Automated and Scalable QA construction. Unlike prior works, which use manual curation or templates, TDBench automates this process with TFDs.\n2. TDBench can be extended to any database with temporal data."}, "weaknesses": {"value": "1. The paper argues that it eliminates the manual bottleneck of benchmark creation. Well-structured temporal databases with defined TFDs for many domains may not exist, and it's not an easy task to automate their creation.\n2. In many real-world databases, schemas are updated with time as well. Even in Wikipedia, many infobox keys & values are constantly being updated. This is a major limitation for TDBench like tasks.\n3. While the time accuracy metric is good, it's not anything new."}, "questions": {"value": "1. Can you provide guidance or tools for automatically discovering TFDs from existing databases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oITLEssc3O", "forum": "W7RNxsTKKZ", "replyto": "W7RNxsTKKZ", "signatures": ["ICLR.cc/2026/Conference/Submission16700/Reviewer_YGnM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16700/Reviewer_YGnM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978873545, "cdate": 1761978873545, "tmdate": 1762926751887, "mdate": 1762926751887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TDBench, a benchmark for time-sensitive factual question answering. The benchmark leverages three database techniques to construct questions directly from temporal databases: using Temporal Functional Dependencies to select answerable facts, employing temporal SQL to encode diverse temporal contexts, and utilizing temporal joins to create multi-hop event-event reasoning items. Additionally, the paper introduces a new evaluation metric, \"time accuracy,\" which verifies whether a model's explanation cites correct temporal evidence in addition to providing the correct answer. Experiments on multiple large language models reveal a significant gap between answer accuracy and answer-plus-time accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The use of Temporal Functional Dependencies  for knowledge selection and temporal SQL based on all 13 Allen relations to generate questions is rigorous, systematic, and scalable. This enables the creation of more diverse and complex temporal reasoning questions than those commonly found in existing benchmarks.\n\n2. The introduction of the \"time accuracy\" concept correctly emphasizes that in TSQA, the rationale is as important as the answer itself.\n\n3. By building on fundamental database principles, TDBench is not tied to specific knowledge sources. The authors demonstrate this advantage by evaluating LLMs on domain-specific datasets, revealing performance variations overlooked by Wikipedia-centric benchmarks. This makes the framework highly valuable for creating application-specific evaluations."}, "weaknesses": {"value": "1. The accuracy of LLM generation and evaluation is relatively low, with known misjudgment issues. This may introduce label noise and lead to benchmark bias. It is recommended to incorporate more automated verification or manual validation mechanisms.\n\n2. Temporal information in many scenarios includes open/uncertain intervals or requires granularities beyond dates (e.g., partial months, fiscal quarters). The current time accuracy standard, which only validates the start/end fields of each relation, is relatively limited in many contexts.\n\n3. The same provider's model (GPT-4o) is used in the pipeline (SQL-to-text) and is also an evaluation target. Although the authors avoid using GPT-4o as a judge to reduce bias, this dual role still requires further analysis.\n\n4. Many existing studies on TSQA dataset construction are not effectively cited or analyzed. Beyond template-based approaches, other methods such as manual construction and LLM-automated construction [1][2][3] should be included and discussed in the related work section.\n\n[1] A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation\n\n[2] SituatedQA: Incorporating Extra-Linguistic Contexts into QA\n\n[3] ComplexTempQA: A 100m Dataset for Complex Temporal Question Answering"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kUfmnmiVz2", "forum": "W7RNxsTKKZ", "replyto": "W7RNxsTKKZ", "signatures": ["ICLR.cc/2026/Conference/Submission16700/Reviewer_AktZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16700/Reviewer_AktZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995957992, "cdate": 1761995957992, "tmdate": 1762926751416, "mdate": 1762926751416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TDBench, a framework that auto-constructs time-sensitive QA (TSQA) benchmarks from temporal databases using temporal functional dependencies (TFDs), temporal SQL over the full set of 13 temporal relations, and SQL-to-text generation, then evaluates models on both answer accuracy and a proposed time-accuracy metric that checks the correctness of referenced timestamps in explanations. The pipeline (knowledge selection via TFDs → temporal SQL query generation → SQL-to-text question conversion) yields scalable benchmarks spanning Wikipedia-like and domain-specific datasets (e.g., law, carbon tax, UNESCO, Netflix). Experiments on 8 LLMs show a sizable gap between answer-only and answer-plus-time evaluation, and analyses cover relation types, answer cardinality (multiple/unique/none), temporal spans, and multi-hop (via temporal joins). The authors report high agreement with manual verification and argue TDBench generalizes beyond Wikidata/Wikipedia and reduces manual curation burden."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well-positioned relative to prior work. It offers clear contrast to template-based and Wikidata-constrained temporal QA benchmarks by covering a wider range of relations and incorporating explanation evaluation, an element often absent from earlier efforts.\n\n* The proposed framework is clear and modular. It employs TFDs for target selection, leverages temporal SQL to ensure comprehensive coverage of 13 Allen relations, and uses SQL-to-text conversion to generate natural questions. This approach is both principled and generalizable across data schemas.\n\n* The evaluation extends meaningfully beyond answer accuracy. The introduction of a time-accuracy metric enables explanation-level verification, revealing cases where models identify correct entities but hallucinate timestamps, which results in a notable 21.7% average performance drop.\n\n* The empirical analysis is comprehensive. The authors examine relation-wise performance (e.g., strong on equal, weaker on contain or overlap), answer cardinality (including “none”), temporal span effects, and multi-hop reasoning with hop-wise hallucination (H1/H2), providing fine-grained diagnostic insights."}, "weaknesses": {"value": "* The paper lacks clarity in how “current” time is grounded. Temporal-alignment questions depend on explicit date references (e.g., “as of 2025”), yet it remains unclear how “now” is defined, stored, and exposed across datasets to ensure reproducibility and longitudinal comparability. Clearer date-pinning and versioning protocols would strengthen the methodology.\n\n* The paper makes strong assumptions about the validity of TFDs and data quality. The proposed approach presumes clean temporal tables with unique, non-overlapping records, but real-world data often violate these constraints (e.g., acting roles, contested tenures). Robustness tests under noisy or violated TFDs, along with explicit mismatch-handling mechanisms, are needed.\n\n* The proposed framework demonstrates limited generalization beyond structured tables. Although the SQL-based design is very elegant for relational temporal data, many time-sensitive facts originate from semi-structured or textual sources. Extending TDBench to hybrid settings that integrate temporal tables with text provenance would broaden its practical relevance.\n\n* The analysis of RAG underperformance is insufficiently developed. While the paper notes that RAG often retrieves temporally misaligned passages (Lines 1401-1403), a more detailed investigation, such as experiments with retrieval time-filtering, temporal re-ranking, or query rewriting, could provide actionable insights for practitioners."}, "questions": {"value": "Please address all questions raised in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LUWrG568MS", "forum": "W7RNxsTKKZ", "replyto": "W7RNxsTKKZ", "signatures": ["ICLR.cc/2026/Conference/Submission16700/Reviewer_fr2Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16700/Reviewer_fr2Z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16700/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762177523799, "cdate": 1762177523799, "tmdate": 1762926751003, "mdate": 1762926751003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}