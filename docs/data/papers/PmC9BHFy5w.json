{"id": "PmC9BHFy5w", "number": 10961, "cdate": 1758185726602, "mdate": 1759897617904, "content": {"title": "Clue2Geo: Fine-Grained Image Geolocation via Cluemap and Multi-Stage Fine-Tuning", "abstract": "Global image geolocation aims to identify the location where an image was captured, but achieving high precision and robust localization remains challenging.To enhance geolocation precision, we present Clue2Geo, a cue-driven framework for global image geolocation,powered by a Large Vision Language Model (LVLM) for coordinate reasoning. Firstly, an LVLM is employed to extract diverse geographic cues from images, after which the reliability and contribution of these cues are assessed by computing their local consistency and semantic coherence.Based on that,a cue graph named “cluemap” is constructed,which is used as an auxiliary input both during model fine-tuning and  inference. Subsequently, we build a large-scale Street View dataset with coordinates and cluemaps to support a three-stage progressive fine-tuning strategy.This strategy is to enhance the downstream model’s reasoning capabilities for fine-grained localization tasks.Finally, a post-processing refinement based on Retrieval-Augmented Generation (RAG) using a GPS database is applied after reasoning to reduce the offset of the predicted coordinates, improving both accuracy and stability. Extensive experiments demonstrate that Clue2Geo achieves state-of-the-art performance on fine-grained metrics, particularly at the street levels.", "tldr": "We propose Clue2Geo, a cue-driven framework for global image geolocation leveraging LVLMs, structured cluemaps and multi-stage fine-tuning to achieve high-precision, street-level localization, clearly outperforming existing methods in fine-grained.", "keywords": ["Image Localization;Fine-grained Geolocation; Multimodal Reasoning; Large Vision-Language Models;Multi-stage Fine-tuning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3e18e61d5135d5232af1d4b5220032f11c60a0d8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a framework called Clue2Geo for global image geolocation. It contains an LVLM component and a postprocessing refinement based on RAG. Additionally, a street-view dataset is proposed for the LVLM fine-tuning. This dataset provides images, coordinates, addresses, and constructed visual cues (named “cluemap” in this paper). The experiments on the public benchmark IM2GPS3K and YFCC4K show the better performance of Clue2Geo, and ablations show the contribution of different components."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel cluemaps are introduced to supplement the prompt for the LVLM\n2. The combination of all components leads to decent performance"}, "weaknesses": {"value": "Based on the thorough review of the manuscript, several significant weaknesses have been identified:\n\n- At the bottom of the first page in this manuscript, there is a github repo https://github.com/xxxxxx, which is not an anonymous github. I am not sure if this potentially violates double-blind review principles.\n- Besides the above issue, there are also some concerns regarding the Method and Experiments.\n  - Method\n    - This manuscript claims that  \"Since cues often describe local regions of an image\" (line 191-192), it is no doubt, however, the authors choose Scheme 2, which divides the image into fixed-size patches, calculates the similarity between the cues and each patch (line 195-197). This approach sounds unreasonable because the small single patch may not be associated with any large-scale buildings. These splits break the structure of the image and introduce bias during the process of evaluating cues.\n    - The authors empirically set the $\\alpha$=0.5 (line 230-231). I want to know whether different settings of $\\alpha$ have an impact on the cluemap construction. Is there more discussion about it?\n    - Regarding the landmark levels and some examples mentioned in this manuscript (line 250-278), there is a question here: does every image have a landmark? The example shows the “Eiffel Tower” as the predicted landmark, but how about other images?\n  - Experiments\n    - The proposed Clue2Geo uses street-view images as training data (line 318), but there is no discussion about its performance on street-view benchmarks like [1] and [2], or a manually curated street-view test set\n    - The Clue2Geo contains an LVLM component, but there is no comparison with open/closed-source LVLMs, e.g., LLaVA, Qwen-VL, or GPT4V, as you mentioned in Related Work\n    - The Clue2Geo uses the same RAG module proposed in G3 [3], which includes a large-scale retrieval database. It would be better to give more comparisons between these two methods, such as efficiency, failure cases, and intermediate results; otherwise, the proposed Clue2Geo may appear unconvincing, as the performance gains could be attributed primarily to the G3 RAG module rather than the method itself.\n    - In Table 1, you should clearly indicate which data you obtained by redeploying the tests yourself. It seems that many of the results are different from those reported in the paper.\n    - The Clue2Geo achieves SOTA performance among the methods compared in this manuscript; however, another accepted paper [4] reports results that surpass this SOTA.\n\n- Literature review: This manuscript has some wrong statements about some work\n  - Line 063-064, GeoCLIP[5] does not contain a RAG-based post-processing module\n  - Line 365-367, GeoReasoner[6] evaluates only on a highly locatable subset of IM2GPS3k[7], making its results incomparable\n  - Line 391-400, the IM2GPS3k[7] was also sourced from Flickr and consists of diverse and noisy user-uploaded images, similar to YFCC4k[7]; this cannot be regarded as a meaningful difference between them.\n\n- Writing: This manuscript has many writing issues, such as missing spaces between sentences (line 012, 014, 017, 018, 021, 022, ......), incorrect quotation mark usage (line 173), and does not provide citations or footnotes when first introducing specific platforms, datasets, or model names (line 077, 164, 215, 396)\n\n\n\n[1] Where we are and what we’re looking at: Query based worldwide image geo-localization using hierarchies and scenes, CVPR 2023       \n[2] OpenStreetView-5M: The Many Roads to Global Visual Geolocation, CVPR2024     \n[3] G3: An Effective and Adaptive Framework for Worldwide Geolocalization Using Large Multi-Modality Models, NeurIPS 2024    \n[4] GeoRanker: Distance-Aware Ranking for Worldwide Image Geolocalization, NeurIPS 2025  \n[5] GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization, NeurIPS 2023     \n[6] GeoReasoner: Geo-localization with Reasoning in Street Views using a Large Vision-Language Model, ICML 2024      \n[7] Revisiting IM2GPS in the Deep Learning Era, ICCV 2017"}, "questions": {"value": "Please see the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tYB48Hu1FT", "forum": "PmC9BHFy5w", "replyto": "PmC9BHFy5w", "signatures": ["ICLR.cc/2026/Conference/Submission10961/Reviewer_Zwz8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10961/Reviewer_Zwz8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761248518022, "cdate": 1761248518022, "tmdate": 1762922157595, "mdate": 1762922157595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new pipeline for global image geo-localization called Clue2Geo. The central idea is to convert LVLM-generated semantics into a “clue map” and use a multi-stage fine-tuning strategy to improve fine-grained localization. Experiments on IM2GPS3K and YFCC4K report state-of-the-art results, with the largest gains on street-level tracks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Leveraging LVLMs to generate additional semantic cues beyond images is a compelling direction. Formalizing LVLM outputs as a ClueMap and fusing them with image patches in a multi-stage regimen is novel.\n- The proposed method achieves state-of-the-art performance on two benchmarks, the improvements are more significant on fine-grained street-level geo-localization."}, "weaknesses": {"value": "- The motivation and the challenge this paper aims to address are not clear. The proposed method reads like a stack of components (ClueMap, multi-stage finetuning, RAG) without a clear motivation and a focus on key challenges. For example, why is a ClueMap needed, and why in combination with multi-stage tuning and RAG, what challenge in previous works does the proposed method solve?\n- Eq. 3 introduces a term that calculates the “semantic coherence”, which compares the similarity of a given cue across the set of all cues. The definition and scope of C are ambiguous (if it is for each image or the whole dataset). Also, the underlying mechanism of this semantic coherence is a bit hard to understand. The intuition and rationality under this design need further clarification. Moreover, no ablations or analyses are performed to validate the effectiveness of this strategy.\n- In section 3.1, the graph structure is not explained in text or figures, making it hard to imagine what does the graph look like, and how to apply the graph in the pipeline.\n- It is unclear how the LVLM-based fine-tuning is performed given the data template, the training loss, optimization settings, and schedules are needed.\n- Experiments are performed on small datasets, the generalization on larger datasets like the YFCC26k needs further validation.\n- Implementation details about training and evaluation are missing, making it hard to assess the fairness of the comparison and reproduce the results.\n- Formatting issues: \n1) Missing spaces around punctuation in places\n2) The provided github link is not working\n3) Inconsistent use of \\citep{} and \\cite{}"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m6etSD8v5k", "forum": "PmC9BHFy5w", "replyto": "PmC9BHFy5w", "signatures": ["ICLR.cc/2026/Conference/Submission10961/Reviewer_TX7S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10961/Reviewer_TX7S"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761593933619, "cdate": 1761593933619, "tmdate": 1762922156537, "mdate": 1762922156537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Clue2Geo, a cue-driven global image geolocalization framework that uses a LVLM for fine-grained coordinate reasoning. It constructs a ClueMap, a structured graph of visual cues extracted from images, where each cue’s reliability is assessed through local consistency and semantic coherence. A three-stage fine-tuning strategy progressively enhances geographic reasoning from city to landmark levels, followed by a retrieval-augmented refinement module to improve coordinate accuracy. Experiments on IM2GPS3K and YFCC4K show that Clue2Geo achieves state-of-the-art fine-grained localization, outperforming previous methods such as GeoCLIP and G3 at street and city scales."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is logically well-structured, with clear figures and tables, providing a pleasant reading experience.\n2. The idea of extracting cues and constructing a cue graph is quite smart, as it introduces a new data structure to the field.\n3. The related work section is comprehensive and covers nearly all existing methods in this area."}, "weaknesses": {"value": "1. In many places, a space should be added after the period before continuing the sentence, but it is currently missing.\n2. The second paragraph of the Introduction section has no citations at all and should be revised.\n3. The open-source link is only a placeholder and does not provide an actual anonymous repository.\n4. Typo in line 173, the quotation marks are incorrect.\n5. In line 194, the description of Scheme 1 is very unclear and difficult to understand.\n6. In Section 3.2, is the training performed stage by stage, that is, after finishing one stage, you attach the LoRA and continue training in the next stage? This is somewhat similar to GeoReasoner, which could be cited here.\n7. In Section 3.4, will the dataset be released?\n8. The implementation details section is missing.\n9. The experimental section is rather limited, containing only overall results and ablation studies. There is no hyperparameter analysis, such as the effect of different backbones or backbone scales on the results.\n\nOverall, this paper is a good work, but the experimental section is not solid enough. The current results are too limited to convincingly demonstrate the effectiveness of so many modules. If the authors can provide more experimental evidence to support the validity of these components, I would be willing to adjust my score."}, "questions": {"value": "please refer to the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dhp7Y9MIHt", "forum": "PmC9BHFy5w", "replyto": "PmC9BHFy5w", "signatures": ["ICLR.cc/2026/Conference/Submission10961/Reviewer_BZAd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10961/Reviewer_BZAd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737668881, "cdate": 1761737668881, "tmdate": 1762922155862, "mdate": 1762922155862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}