{"id": "88qmNifKvg", "number": 11887, "cdate": 1758204494820, "mdate": 1759897548598, "content": {"title": "MontageAug: Enhancing Long-tail Robustness And Semantic Consistency of VLMs", "abstract": "Vision-Language Models (VLMs) have made significant strides in multimodal understanding tasks, yet their robustness faces severe challenges when dealing with the long-tail data distributions common in the real world, especially in high-stakes domains like medical image analysis. To address this challenge, we propose MontageAug, a compositional data augmentation approach designed specifically for VLMs. It strategically composes images from different sources (particularly from head and tail classes) to construct a novel visual scene (a montage image) and synchronously generates a perfectly corresponding compositional text description. This method not only fundamentally guarantees the semantic fidelity of the augmented samples but also effectively alleviates the long-tail data problem by creating information-rich \"hard positive samples.\" We conducted extensive experimental validation on a model based on the InternVL architecture using ophthalmic medical benchmarks. The results show that MontageAug significantly enhances the model's recognition performance and generalization on tail classes, achieving state-of-the-art (SOTA) performance that surpasses existing augmentation methods on several public benchmarks. Furthermore, to explore the approach's general potential and architectural dependencies, we applied it to the general-domain LLaVA model for analysis. This experiment revealed a critical interplay between the compositional augmentation strategy and the model's underlying architecture. Our work ultimately demonstrates that MontageAug, as an efficient, low-cost, and semantics-preserving VLM augmentation strategy, holds immense practical value in solving the long-tail problem in medical image analysis. We plan to open-source our code, benchmark data, and models upon paper acceptance.", "tldr": "", "keywords": ["Data Augmentation", "Vision Language Model", "Long-tail Task"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9289e7cd74aacad0ef7cb409e0f841b521a21f78.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MontageAug, an augmentation method to enhance VLMs' long-tail robustness. MontageAug comprises three major components: (1) Hard Sample Prioritization Sampler, which prioritizes the sampling of rare and hard samples; (2) Visual Montage Composition, which vertically concatenates images from head and tail classes to form an augmentation image; (3) Textual Montage Composition, which concatenates captions through a template to ensure semantic consistency. Experimental results show that MontageAug surpasses simple oversampling and single-modality-based augmentation (mixup and caption-based image generation) in medical and general-purpose VLM benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. It is sound to consider semantic consistency in long-tail augmentation methods for VLM, instead of single-modality augmentation.\n2. The experimental results show that MontageAug surpasses simple oversampling and single-modality augmentation."}, "weaknesses": {"value": "1. **Over-claimed generalizability.**  The paper has claimed its effectiveness in the general domain. However, the experimental results on general-purpose benchmarks are only left in the appendix, with only one baseline (simple SFT), which is not convincing.\n\n2. **Confusing writing.** Some important details are missing, such as training data and MLLM for report generation. And the organization of the paper is also confusing. For detailed questions, please see below.\n\n3. If the evaluation benchmarks are from the medical domain, why does this paper choose a general-purpose VLM as the base model instead of widely-used medical VLMs like LLaVA-Med?"}, "questions": {"value": "- Line 265: Which powerful MLLM is used for report generation?\n- Sec 4.1.1, what open-source data has been used?\n- In Figure 3, how does the model learn to differentiate part 1 and part 2? My concerns stem from the experimental results on general benchmarks that MontageAug suppresses performance on tasks that rely on fine-grained perception (e.g., MME). Does this really relate to resolution, or due to the misalignment between different parts?\n- Line 130, it is suggested to replace \"comprehensively surpasses\" with \"consistently surpasses\"\n- Missing '.' in the captions of Figure 3 and Figure 4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WKREufdIcK", "forum": "88qmNifKvg", "replyto": "88qmNifKvg", "signatures": ["ICLR.cc/2026/Conference/Submission11887/Reviewer_xo91"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11887/Reviewer_xo91"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761273120472, "cdate": 1761273120472, "tmdate": 1762922900937, "mdate": 1762922900937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a novel augmentation technique called MontageAug specifically created for VLMs and long-tailed problems, which conserves the semantic coherence of image-text pairs by leveraging “hard-positive” samples.\nMontageAug creates a montage of multiple images and a text template to pair it with the respective textual descriptions. Further, the method chooses images based on the class abundance and on the instance difficulty, which is assessed by the performance of a base model. MontageAug achieves SOTA performance on Ophthalmology datasets outperforming simple SFT as well as other augmentation approaches like generative augmentations, sampling techniques or mixing techniques."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "MontageAug convinces with its simplicity and broad applicability to most VLMs and tasks.\nThe method is well motivated in the need of improving models for long-tailed problems."}, "weaknesses": {"value": "1.) It is unclear how an epoch is defined for each method and therefore it is unclear whether the presented performance gain is solely due to the fact that MontageAug sees more image-text pairs e.g. compared to SFT. While Table 2 indicates an overfitting with more epochs for SFT (i.e. 2 epochs), it remains to be shown if sampling based methods would benefit from more epochs here. Sampling techniques could benefit from more epochs in this case, e.g. increasing the numbers of samples seen to the amount of image-text pairs the MontageAug sees. \n\n2.) Further, due to the montage the \"effective\" batch size could be larger in the case of MontageAug which could be the source of improvement, an ablation with SFT and Oversampling with a 40% (since the best $\\alpha$ for MontageAug was 0.4) increased batch size would be interesting as well. To be more explict: one could conduct an experiment where instead of the montaged images those images which would make up the montage will end up in the same batch with a batch size accomodating the additional images of the montage so that the information exposure per training step is the same.\n\n3.) Due to the fact that the epoch for each method is not properly defined, the training times presented in table 3 are against expectation i.e. why the training time for “Oversampling” should be higher than the “MontageAug”/”Vanilla SFT” method.\n\n4.) It is unclear (cf lines 265ff) how the training dataset was generated. Both the used MLLM/VLM and the used templates are not specified. Further, to be more clear the authors should already link to the listing of the training datasets within the appendix around lines 265.\n\n5.) Regarding the textual montage the exact template being used is not provided.\n\n6.) The captions of tables 1, 2 and 4 could be improved as they do not mention the metrics being used as well as what bold values are supposed to mean (i.e. best perfromance). In table 3 it could be specified which \"medical dataset\" was being used (e.g. referring to the main body).\n\n7.) While there is a small hyper-parameter grid-search done for the MontageAug method the other ablated methods were only done with one set of possible sub-optimal hyper-parameters which could make MontageAug superior due to a better set of hyper-parameters with respect to the other methods. E.g. for the hard oversampling one could find a more suitable $\\alpha$ (oversampling frequency).\n\n8.) Line 144: The authors state that their method demonstrates practical value for medical image analysis in general while it is only shown for Ophthalmology. I.e. this claim is too broad for what is shown in the paper.\n\n9.) Line 48ff: How many private samples were collected? Should also be included in the caption of Figure 1.\n\n10.) Line 270ff: In listing their validation data they don't state the number of classes for all datasets (i.e. GMAIMMBench and FundusMMBench).\n\n11.) Line 411: The authors state \"... the MontageAug method shows continuous performance improvement with more training epochs\" whereas it is only shown for 1 and 2 epochs so the statement is too general.\n\n12.) Why did the authors chose LLaVA-1.5 to ablate the method on other tasks, as it is clear that the method benefits strongly from a VLM which supports dynamic resolution? Overall section 4.2 feels rushed.\n\n13.) The authors should explicitly refer to figure 1 in line 130.\n\n14.) Regarding the reading flow the listing starting from line 095 feels abrupt and a little out of context. Further in line 337 there should be some transition between the listing of the comparison methods and the baseline methods.\n\n15.) The statement in line 030/031 and 144 is too broad \"demonstrating its practical value for medical image analysis.\", while it is only shown for ophthalmology images."}, "questions": {"value": "How exactly are epochs defined in your paper? If an epoch is one run through all training cases how can it be that in Table 3 the training time of Vanilla SFT and Oversampling differ so much? If the Oversampling technique additionally sees 40% more rare cases on top than one would expect a training time 40% larger (around 53h) than SFT which is not the case.\n\nWhy is the increase in training time of the MontageAug method only marginal compared to SFT in table 3, if InternVL uses dynamic resolution there should be 40% more tokens being processed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fknl4MOoa4", "forum": "88qmNifKvg", "replyto": "88qmNifKvg", "signatures": ["ICLR.cc/2026/Conference/Submission11887/Reviewer_WyrV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11887/Reviewer_WyrV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821646980, "cdate": 1761821646980, "tmdate": 1762922900550, "mdate": 1762922900550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to tackle a well-posed problem: instruction-tuned VLMs struggle on long‑tail categories, particularly in ophthalmic fundus imaging, where rare diseases (“tail classes”) have few labeled instances. In this problem, the training data are instruction–answer pairs and the objective is to improve downstream multiple‑choice diagnostic accuracy especially on tail categories, while preserving image–text semantic consistency. Specifically, the proposed method composes $k$ images into a grid montage and synchronously composes the paired texts with a deterministic template, yielding hard positive examples that are visually richer but semantically perfectly aligned with their new text. A hard-sample prioritization sampler biases secondary images toward (i) rare categories and (ii) samples mispredicted by a strong VLM, identified by an evaluator LLM. A training item is replaced by a montage with a predefined probability $\\alpha$. \n\nEmpirically, the method is evaluated using the dataset constructed using ~200k fundus images and ~300k instruction pairs with 11/4 ratio of normal and harm examples and on four fundus benchmarks. Under equal budgets on InternVL‑Chat‑V3.0‑8B, MontageAug improves average accuracy by 2.5% percents. In contrast, oversampling matched for tail exposure gives +0.4 on average, while Mixup and RetinaLogos-based generation reduce performance. Against specialized medical VLMs and closed models (e.g., GPT-4o), the InternVL model fine-tuned with the proposed method is competitive or better on this benchmarking suite. Finally, a generality probe on LLaVA‑1.5 shows benefits on compositional reasoning but degradation on fine‑grained perception, attributed to montage downsampling and spatial‑label incompatibilities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Figure 1 indeed ties head–tail skew to accuracy collapse, justifying a tail‑focused augmentation that preserves alignment.\n2. Composing both vision and text enables the proposed method to avoid label noise that plagues baselines Mixup/generation.\n3. The comparisons are generally fair and extensive.\n4. The method does not introduce significant computing overhead, as shown by Table 3, where the proposed method shows nearly identical training time to vanilla and a lower cost than generation."}, "weaknesses": {"value": "1. Weak matching is currently dependent on GPT-4o without prompt/post-processing details. Meanwhile, the robustness to other open evaluators is unreported.\n2. The hard-sample pool depends on Qwen2.5-72B judgments. The sensitivity to thresholding and the evaluator are not analyzed.\n3. The strongest results are in fundus VQA-style tasks, while the general-domain LLaVA results are relatively mixed.\n4. The authors do not provide control for image-only montage or alternate templates. In this case, the relative role of textual scaffolding is not that clear."}, "questions": {"value": "1. Can you release the GPT-4o prompts and normalizers used for weak matching and replicate with other open-sourced models to quantify the evaluator variance?\n2. Can you compare full montage, image-only montage (i.e., keep original text), text-only concatenation (i.e., no visual montage), and alternate templates to isolate the effects?\n3. It is also suggested to add a public long-tail benchmark outside medicine (i.e., single-image evaluation) to assess the cross-domain utility of this method.\n4. For fixed-resolution encoders (e.g., LLaVA-1.5), have you attempted with tiled cropping/higher-resolution encoders/etc to recover MME performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0VjasZSSCf", "forum": "88qmNifKvg", "replyto": "88qmNifKvg", "signatures": ["ICLR.cc/2026/Conference/Submission11887/Reviewer_x45S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11887/Reviewer_x45S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143897896, "cdate": 1762143897896, "tmdate": 1762922900166, "mdate": 1762922900166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}