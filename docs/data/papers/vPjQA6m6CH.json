{"id": "vPjQA6m6CH", "number": 15573, "cdate": 1758252765503, "mdate": 1763120865492, "content": {"title": "Self-Reflective Reinforcement Learning for Diffusion-based Image Reasoning Generation", "abstract": "Recent years have witnessed the wonderful effects of diffusion models in generative task. However, existing image generation methods still suffer from the significant \"reasoning-oriented generative\" dilemma.\nMotivated by the success of Chain of Thought (CoT) and Reinforcement Learning (RL) in LLMs, \nwe propose SRRL, a self-reflective RL algorithm for diffusion models to achieve reasoning generation of logical images by performing reflection and iteration across generation trajectories.\nThe intermediate samples in the denoising process carry noise, making accurate reward evaluation difficult. To address this challenge, SRRL treats the entire denoising trajectory as a CoT step with multi-round reflective denoising process and introduces condition guided forward process, which allows for reflective iteration between CoT steps. \nThrough SRRL-based iterative diffusion training, we introduce image reasoning through CoT into generation tasks adhering to physical laws and unconventional physical phenomena for the first time.\nNotably, experimental results of case study exhibit that the superior performance of our SRRL algorithm even compared with advanced T2I models.", "tldr": "", "keywords": ["Reinforcement Learning", "Diffusion Models", "Image Reasoning Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/80cc9321db421fc794f9d31393a050bcbc05dc75.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes SRRL, a self-reflective reinforcement learning algorithm for diffusion-based image generation, specifically targeting logical reasoning in images. Inspired by Chain-of-Thought (CoT) methods in LLMs, SRRL treats the entire denoising trajectory as a reasoning step, introducing multi-round reflective denoising and a condition-guided forward process. This approach allows diffusion models to generate images that adhere to physical laws and counterintuitive phenomena."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written with a clear and logical structure.\n\n- The work innovatively applies CoT to diffusion models to achieve reasoning generation.\n\n- Quantitative results show notable improvements in metrics like CLIP Score, ImageReward, and VQAScore."}, "weaknesses": {"value": "- A critical limitation is that all employed reward models and metrics primarily focus on semantic alignment between images and prompts, lacking the ability to capture fine-grained differences in reasoning generation of logical images. In other words, while the paper proposes a novel image reasoning generation task with interesting cases, there is a lack of direct evidence that the framework actually enables the model to learn reasoning, rather than merely generating semantically consistent images.\n\n- The experimental section is relatively weak. It only compares against the basic DDPO baseline without benchmarking against specialized reflection-optimized models such as Reflect-DiT [1].\n\n- The ablation studies are incomplete, e.g., the effectiveness of key components like the Condition Guided Forward Process is unvalidated.\n\n[1] Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection."}, "questions": {"value": "- Multimodal LLMs with visual understanding (e.g., GPT-4oV) or human studies may help better evaluate and improve logical reasoning in generated images."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PvV8gJcLgK", "forum": "vPjQA6m6CH", "replyto": "vPjQA6m6CH", "signatures": ["ICLR.cc/2026/Conference/Submission15573/Reviewer_q5CG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15573/Reviewer_q5CG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832503010, "cdate": 1761832503010, "tmdate": 1762925846633, "mdate": 1762925846633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "wo1Auott9N", "forum": "vPjQA6m6CH", "replyto": "vPjQA6m6CH", "signatures": ["ICLR.cc/2026/Conference/Submission15573/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15573/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763120863945, "cdate": 1763120863945, "tmdate": 1763120863945, "mdate": 1763120863945, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method for guiding text-to-image generation with reward models, mimicking the chain-of-thought methodologies that have become widespread in the LLM field. Using pretrained diffusion models, the authors propose fine-tuning the generative model using a reward model, through multiple generation trajectories created using DDIM inversion. The authors suggest that this enables the model to reflect and improve upon previous generation trajectories. The paper includes examples of unconventional prompts and results generated by different trajectories of the proposed method, showing improving alignment with the progression of the method."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The topic of the paper is highly valuable. Progress in aligning text-to-image models with user’s preferences has wide impacts.\n* The concept of using inversion in reward-model-guided generation is novel to the best of my knowledge."}, "weaknesses": {"value": "* The writing clarity could be improved. The main method was difficult to understand from the paper. The training objective should be stated explicitly and clearly. Including the training algorithm in the main paper (even in a more concise form) would greatly assist in the readability of the method. Also, key pieces of information included in the appendix are not referred to in the text, for example the list of prompts used to create Tab. 1.\n* The authors do not include related work on test-time scaling. While test-time scaling does not change the model’s weights, the use of reward model for guided generation as well as injecting noise repeatedly has been explored in many test-time scaling works, including [1] [2] [3].\n* The qualitative results are not highly convincing, and do not necessarily improve with the progression on the method (Figs. 3 and 4). Moreover the results become saturated, perhaps due to the repeated use of CFG. I believe the claim that the model’s results are on par with those generated by other advanced models is not supported by the limited qualitative evidence (personally I prefer the outputs of Nano-Banana or GPT-4o out of the options).\n* The quantitative results are problematic for several reasons. First, the models were tested on a small set of prompts instead of using widely accepted benchmarks. Second, the same metrics were used both for the reward models as well as for evaluation, raising concerns of “reward hacking”. Finally, very few alternative methods are considered.\n\n[1] Kim, Sunwoo, Minkyu Kim, and Dongmin Park. \"Test-time alignment of diffusion models without reward over-optimization.\" arXiv preprint arXiv:2501.05803 (2025).\n\n[2] Singhal, Raghav, et al. \"A general framework for inference-time scaling and steering of diffusion models.\" arXiv preprint arXiv:2501.06848 (2025).\n\n[3] He, Haoran, et al. \"Scaling Image and Video Generation via Test-Time Evolutionary Search.\" arXiv preprint arXiv:2505.17618 (2025)."}, "questions": {"value": "* From Fig. 2, it seems that the reward model compares across samples generated with the same prompt, but in Alg. 1 it seems that different prompts are used. Could the authors clarify this point?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pKNHSLcwEj", "forum": "vPjQA6m6CH", "replyto": "vPjQA6m6CH", "signatures": ["ICLR.cc/2026/Conference/Submission15573/Reviewer_dWsd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15573/Reviewer_dWsd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833282503, "cdate": 1761833282503, "tmdate": 1762925846254, "mdate": 1762925846254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduct reinforcement learning to diffusion models to improve reasoning capabilities. The paper proposes SRRL, which uses a \"multi-round reflective denoising process\" and a \"condition guided forward process\" to iteratively refine the generation policy across multiple trajectories. Results show SRRL improves performance on metrics like VQAScore and ImageReward compared to baselines (Vanilla SD, DDPO)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Mapping CoT reasoning to an iterative, trajectory-wise refinement process for diffusion is a novel concept.\n\n- Addresses the critical challenge of infusing generative models with reasoning."}, "weaknesses": {"value": "- Uses reward models (VQAScore, ImageReward) built for alignment, not for the target task of physical reasoning. Also, same reward models are used for evaluations, prone to reward hacking.\n\n- The multi-round, re-noising (DDIM inversion) design is extremely costly.\n\n- The model is evaluated on the same prompts it was trained on, raising concerns about overfitting vs. true generalization."}, "questions": {"value": "- What was the total wall-clock training time (in GPU hours) required to fine-tune a model for one experiment?\n\n- Can you discuss core difference to Flow-GRPO [A]?\n\n[A] Liu et al., Flow-GRPO: Training Flow Matching Models via Online RL, NeurIPS 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KPdJgp6ifM", "forum": "vPjQA6m6CH", "replyto": "vPjQA6m6CH", "signatures": ["ICLR.cc/2026/Conference/Submission15573/Reviewer_MpRA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15573/Reviewer_MpRA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884590279, "cdate": 1761884590279, "tmdate": 1762925845873, "mdate": 1762925845873, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Self-Reflective Reinforcement Learning (SRRL), which introduces Chain-of-Thought reasoning into diffusion models to enable self-reflective image generation. Through multi-round denoising and reinforcement learning optimization, SRRL allows models to produce images consistent with physical laws and logical reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses a novel problem. \n* The paper is clearly written and easy to follow."}, "weaknesses": {"value": "* Lack of analysis of computational overhead. Intuitively, even in the forward process, the model needs to perform k complete inferences and k-1 VQA model inferences, which is extremely expensive.\n* Some images are difficult to understand. In Figure 5, both downward and rightward directions are labeled as *Round ↑* . This makes it difficult to understand the actual process. And the figure doesn't convey the conclusion being described.\n* The specific value of K used in forward process is not clearly stated, making the improvement difficult to assess.\n* Lack of detailed impact of the K value on the results. In Figure 7, the paper mentions that the quality of generated images improves as the number of reflection rounds increases, and observes the phenomenon of *Reflection Refinement*. However, the paper does not provide a detailed analysis, and the experiments are limited to only two case studies."}, "questions": {"value": "* What specific model is used in the paper to obtain the VQAScore?\n* According to my understanding, the step-by-step PPO training in this paper still relies on alignment-based metrics (VQAScore). Why can the use of the alignment metric improve the \"reasoning\" ability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qc6lvn1Xpf", "forum": "vPjQA6m6CH", "replyto": "vPjQA6m6CH", "signatures": ["ICLR.cc/2026/Conference/Submission15573/Reviewer_Rdcf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15573/Reviewer_Rdcf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15573/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891972406, "cdate": 1761891972406, "tmdate": 1762925844440, "mdate": 1762925844440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}