{"id": "si3mQHv7An", "number": 22941, "cdate": 1758337308765, "mdate": 1759896839312, "content": {"title": "MSE-Break: Steering Internal Representations to Bypass Refusals in Large Language Models", "abstract": "The flexibility of internal concept embeddings in large language models (LLMs) enables advanced capabilities like in-context learning---but also opens the door to adversarial exploitation. We introduce MSE-Break, a jailbreak method that optimizes a soft-prompt prefix via gradient descent to minimize the mean squared error (MSE) between harmful concept embeddings in refused and accepted contexts. The resulting soft prompt $p$ is concept-specific but prompt-general, enabling it to jailbreak a wide range of queries involving that concept without further tuning. Applied to four popular open-source LLMs---including Gemma-2B-IT and LLaMA-3.1-8B-IT---MSE-Break achieves attack success rates exceeding 90\\%. Its interpretability-driven design enables MSE-Break to outperform existing methods like GCG and AutoDAN---while converging in a fraction of the time. We find that harmful concept embeddings are linearly separable between refused and accepted contexts---structure that MSE-Break actively exploits. We further show that concept representations can be drastically steered in-context with as little as a single token. Our findings underscore the brittleness of LLM representations---and their susceptibility to targeted manipulation---highlighting the urgency for more robust and interpretable safety mechanisms.", "tldr": "MSE-Break is an interpretability-driven jailbreak that manipulates internal concept embeddings via soft prompts to bypass model refusals", "keywords": ["Large Language Models", "Interpretability", "Jailbreaking", "Alignment Vulnerabilities", "Adversarial Prompting", "Model Refusal Behavior", "Embedding Manipulation"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3dffafb61c502a86a6854a5a2efd030660b0861.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper leverages the PCA separability of the hidden states within LLMs to train jailbreak prompts, allowing them to output malicious content. Experiments show that these prompts can precisely manipulate the models' internal refusal mechanism, achieving a much higher attack success rate than similar methods such as GCG and AutoDAN."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper identifies a relevant and timely problem, namely understanding and bypassing refusal mechanisms in LLMs. \n\nThe creation of a new dataset, although small and under-documented, shows an intent to provide standardized evaluation materials for future work. If expanded and released, it could benefit reproducibility and comparative research in AI safety."}, "weaknesses": {"value": "This paper lacks novelty, which constitutes a major reason for my recommendation of rejection. In addition, its experimental setup also has many defects, and the writing is not good.\n\n**Novelty Concerns**\n\n- The authors inadequately investigate the coverage of jailbreak research. In fact, there have been quite a number of jailbreak works that go beyond ''surface-level input manipulation'', including the manipulation of embedding, attention, and activation [1,2,3,4].\n- Enabling jailbreak by manipulating the concept of refusal is also a topic that has been explored [1,3].\n- Creating soft prompts with internal hidden states as the optimization target has also been explored [2].\n- This paper mentions that the candidate prompts required for the soft prompts training need to be carefully selected, which seems to be an additional constraint. In [2], the positive samples based on the linear classifiers are randomly selected.\n\n[1] Refusal in Language Models Is Mediated by a Single Direction, https://arxiv.org/abs/2406.11717 (NeurIPS 2024)\n\n[2] Uncovering Safety Risks of Large Language Models through Concept Activation Vector, https://arxiv.org/abs/2404.12038 (NeurIPS 2024)\n\n[3] Stronger Universal and Transferable Attacks by Suppressing Refusals, https://aclanthology.org/2025.naacl-long.302.pdf\n\n[4] Sugar-Coated Poison: Benign Generation Unlocks Jailbreaking, https://arxiv.org/abs/2504.05652 (EMNLP 2025)\n\n---\n\n**Experimental Defects**\n\n- **The narrative of the threat model is misleading**. In Introduction, the authors claim that MSE-break does not require *updating model weights or accessing logits*, but their experiments are based on open-source models, and the training of soft prompts also requires accessing embedding information from any layer, resulting in significant inconsistency. The authors also do not present how the trained prompts are applied to black-box APIs to support their claim.\n- **The authors claim to have established a new dataset, but the construction pipeline is opaque**. This raises issues:\n  -  How do researchers construct new datasets on new ''concepts''?\n  -  The ''concepts'' for constructing the dataset seem to be based on the authors' subjective judgment and lack consistent evidence in model internal representations.\n  -  The number is too small, with 75 prompts divided into five topics, and Cybercrime has three subtopics. According to the equal distribution assumption, the subset of a single concept may consist of fewer than 10 questions. Based on the blessing of high-dimensional separability, this separability may not necessarily reflect the refusal mechanism truthfully.\n- **Lack of baselines**. Comparing the authors' method with GCG and AutoDAN, which create adversarial prompts, is reasonable, but at least it needs to be compared with similar methods, such as [1,2,3]. In particular, [2] points out that the robustness of using PCA to train refusal vectors is not as good as that of linear regression classifiers. Comparisons with such methods have the potential to become a useful empirical study in the field.\n\n---\n\n**Writing & Demonstration**\n\n- Soft prompt seems not to be a widely accepted concept. The author needs to demonstrate its definition in Introduction, or how it acts on malicious requests to produce an attack.\n- At L125, the reasons for using the instruct version models are not fully explained. How does this setting play a role in the effectiveness of their attack?\n- Section 4.3 does not provide any figures, tables, or experimental evidence.\n- Figure 3 is too small to read, with a lot of blank space on both sides. A better presentation could be used. (This seems to be an AI-generated interface, and I think it would be easy to adjust)\n\n**Typos**\n\n- At L137, Gpt-4o -> GPT-4o\n- At L299, Layer 9-16(middle layers) -> Layer 9-16 (middle layers)\n- At L352, a period is missing"}, "questions": {"value": "Major as listed in weakness section."}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The dataset required to build the soft prompts is the key to the reproducibility of this paper, but the proposed direct release may raise ethical issues. Since the research is based on open-source models, anyone can train soft prompts based on this to obtain malicious content.\n\nAdditionally, it is unclear whether this work has been approved by the IRB of the authors' institution. They need to supplement relevant evidence in their ethical statement section."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dIfHAajCgQ", "forum": "si3mQHv7An", "replyto": "si3mQHv7An", "signatures": ["ICLR.cc/2026/Conference/Submission22941/Reviewer_dcSm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22941/Reviewer_dcSm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760596943131, "cdate": 1760596943131, "tmdate": 1762942447412, "mdate": 1762942447412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MSE-Break, a method that steers internal representations of harmful concepts in large language models using a soft prompt optimized via mean squared error minimization. The approach aims to align harmful and benign embeddings to bypass refusal behavior across multiple open-weight models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. **Clear Motivation.** The paper is grounded in the observed separation between harmful and benign embeddings.\n2. **Simple and Reproducible Method.** The approach is easy to implement and converges quickly."}, "weaknesses": {"value": "1. **Paper Structure.** The paper’s organization could be improved. For instance, including implementation details in the Preliminaries section feels unconventional and disrupts the logical flow.\n\n2. **Limited Effectiveness.** There already exist strong jailbreak attacks capable of breaking LLM safeguards efficiently [1,2], particularly for open-source models like Qwen or LLaMA. A comparison with such methods would strengthen the evaluation.\n\n3. **Incremental Novelty.** The proposed technique mainly involves applying a weighted MSE objective to selected residual layers and tuning a short soft prompt. Similar ideas—using single directions or activation manipulations to alter refusal behavior—have been well explored in prior work [3], even though that work is cited in this paper.\n\n[1] Ding, Peng, et al. \"A wolf in sheep's clothing: Generalized nested jailbreak prompts can fool large language models easily.\" NAACL (2024).\\\n[2] Andriushchenko, Maksym, Francesco Croce, and Nicolas Flammarion. \"Jailbreaking leading safety-aligned llms with simple adaptive attacks.\" ICLR (2025).\\\n[3] Arditi, Andy, et al. \"Refusal in language models is mediated by a single direction.\" NeurIPS (2024)."}, "questions": {"value": "1. I understand that this is a white-box attack, but is there any potential for applying it to proprietary models? For example, is the learned soft prompt transferable across models? I assume not, since soft prompts are model-specific. [1] \n2. Recently, several methods have been proposed for safety alignment at the representation level [2,3]. Is your method still effective against such defenses? It would strengthen your argument to demonstrate robustness against these baselines, as they aim to harden the very representations your attack targets.\n\n[1] Jia, Xiaojun, et al. \"Improved techniques for optimization-based jailbreaking on large language models.\" ICLR (2025).\\\n[2] Zou, Andy, et al. \"Improving alignment and robustness with circuit breakers.\" NeurIPS (2024).\\\n[3] Yousefpour, Ashkan, et al. \"Representation bending for large language model safety.\" ACL (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oePcrUMbfc", "forum": "si3mQHv7An", "replyto": "si3mQHv7An", "signatures": ["ICLR.cc/2026/Conference/Submission22941/Reviewer_vvaj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22941/Reviewer_vvaj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760681439753, "cdate": 1760681439753, "tmdate": 1762942447199, "mdate": 1762942447199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MSE-Break, a novel, interpretability-driven jailbreaking technique for large language models. The core idea is to optimize a continuous soft-prompt prefix via gradient descent. Unlike traditional methods that target output logits, MSE-Break's objective is to minimize the Mean Squared Error (MSE) between the internal representation of a specific harmful concept (e.g., \"bomb\") in a refused context and its representation in a carefully selected benign, accepted context. The authors first provide strong empirical evidence that for harmful concepts, the internal representations of refused versus accepted prompts are linearly separable in the model's activation space. MSE-Break directly exploits this separability. On a testbed of four open-source, safety-aligned LLMs (including Gemma-2B-IT and LLaMA-3.1-8B-IT), the method achieves attack success rates (ASR) often exceeding 90%, significantly outperforming and converging orders of magnitude faster than strong baselines like GCG and AutoDAN. The resulting soft prompt is concept-specific but generalizable across many different user prompts involving that concept."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* It identifies a new, potent, and highly efficient attack vector. The fact that this method is orders of magnitude faster than baselines (minutes vs. 30+ hours, Table 3) by optimizing a reusable, concept-general prompt is a major finding.\n\n* It underscores a deep vulnerability in current alignment techniques. The results strongly suggest that existing safety training, while effective at a surface level, fails to create robust representations at the concept level."}, "weaknesses": {"value": "* The method relies on a white-box setting, requiring full gradient access to optimize the soft prompt. This is acknowledged by the authors but remains the primary barrier to this attack's applicability to closed-source, black-box models, which are a major part of the safety landscape.\n\n* The experiments are limited to smaller-scale open-source models (<= 8B parameters). It is an open question whether the core empirical finding—the clean linear separability of harmful/benign concept embeddings—holds true for much larger models (e.g., 70B+ or frontier models). Refusal mechanisms and representational geometry might differ significantly at scale.\n\n* The method's success appears to be critically dependent on the selection of a \"good\" benign candidate prompt (Section 4.2). The current process involves a set of manual heuristics and a scoring function, which introduces a human-in-the-loop component and makes the attack seem less automated than methods like GCG."}, "questions": {"value": "Following the weakness above, how crucial is the candidate prompt scoring function (Section 4.2)? What is the performance (ASR) drop if you simply pick a random accepted prompt for a given concept, versus the top-scoring one selected by your metric?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B871Rp0FF6", "forum": "si3mQHv7An", "replyto": "si3mQHv7An", "signatures": ["ICLR.cc/2026/Conference/Submission22941/Reviewer_Ggpr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22941/Reviewer_Ggpr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847607237, "cdate": 1761847607237, "tmdate": 1762942446986, "mdate": 1762942446986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an innovative method to jail-break LLMs. The authors hypothesized that the decision to refuse to answer a prompt is triggered by specific sensitive concepts, and found that the representations of such concepts is linearly separable between harmful and benign contexts. The proposed method optimizes a soft prompt such that, when it is prepended to a harmful request containing a target concept, the representation of the context is maximally similar to the representation of the same concept in a benign context. Experiments show that the attack is highly effective, and is computationally inexpensive."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* While it was known that the last-token representations of harmful and harmless prompts are linearly separable, the insight that this derives from a similar property of the sensitive concepts mentioned in the prompt is very interesting and, to my knowledge, novel.\n* The discovery of the portability of the attack across prompts involving a same concept is also surprising and interesting\n* The effectiveness of the proposed method draws attention on a vulnerability that deserves to be further understood and mitigated."}, "weaknesses": {"value": "[W1] The experimental section is borderline sufficient. Only GCG and AutoDAN are used as baselines. The paper would be stronger if jailbreaking methods not based on adversarial prompt search were added to the experiments. In particular, since MSE-break requires access to model weights, it would be interesting to see it compared to the weight orthogonalization method of Arditi et al. which can also be easily implemented with inference-time interventions by suppressing the contribution of all MHA and MLP components along the refusal direction. It would be great to have at least one model at a larger scale than 8B.\n\n[W2] Minor: Section 4.2 on effective prompt candidate selection could be better clear. It was not initially clear to me that this was about generating the benign contexts for the harmful concepts that would not trigger a refusal: this should be explained upfront. There should also be more clarity on what happens when the harmful concept spans more than one token: is it the last token of the concept that is considered? L206 mention a token position (singular), but L207-208 mention averaging token vectors."}, "questions": {"value": "Figure 4: what model and layer are these plots relative to?\n\nL485: what are the 'tailored embeddings' being referred to here?\n\nPage layout and figures could be improved"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The paper describes an effective model jailbreaking method that could enable malicious agents. While I concur with the authors that the value of the contribution is net-positive, I am not an expert ethicist, and expert advice should be sought."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Te5vjrG3YY", "forum": "si3mQHv7An", "replyto": "si3mQHv7An", "signatures": ["ICLR.cc/2026/Conference/Submission22941/Reviewer_NDrd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22941/Reviewer_NDrd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22941/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865473084, "cdate": 1761865473084, "tmdate": 1762942446674, "mdate": 1762942446674, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}