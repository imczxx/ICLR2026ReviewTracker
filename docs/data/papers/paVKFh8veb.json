{"id": "paVKFh8veb", "number": 3045, "cdate": 1757320894870, "mdate": 1759898111806, "content": {"title": "MIDUS: Memory-Infused Depth Up-Scaling", "abstract": "Scaling large language models (LLMs) demands approaches that increase capacity without incurring excessive parameter growth or inference cost. Depth Up-Scaling (DUS) has emerged as a promising strategy by duplicating layers and applying Continual Pre-training (CPT), but its reliance on dense feed-forward networks (FFNs) limits efficiency and attainable gains. We introduce MIDUS (Memory-Infused Depth Up-Scaling), which replaces FFNs with a head-wise memory layer (HML). Motivated by observations that attention heads exhibit diverse roles both across and within layers, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval and injecting information into subsequent layers while preserving head-specific functional structure. This design breaks the usual efficiency–performance trade-off by combining sparse memory access with head-specific representations, and includes an efficient per-head value factorization module. In large-scale CPT, MIDUS consistently outperforms strong DUS baselines while using fewer trainable parameters and significantly less training-time memory, and quality improves further by increasing memory capacity even at fixed depth. These results establish head-wise memory as a highly effective and efficient mechanism for DUS of general-purpose LLMs.", "tldr": "MIDUS (Memory-Infused Depth Up-Scaling) advances DUS by replacing dense FFNs with head-wise memory layers, overcoming the conventional efficiency–performance trade-off and enabling more efficient and generalizable LLMs.", "keywords": ["Depth Up-Scaling", "Memory Layer", "Large Language Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/57792424d83de43c4e5295ec0ab61dcd39c8a2e5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MIDUS (Memory-Infused Depth Up-Scaling), a method for Depth Up-Scaling (DUS) that replaces FFN layers with a \"Memory Block\". The main contributions include the Head-wise Memory Layer (HML), which assigns memory per attention head, and efficient storage mechanisms (PKM for keys, HIVE for values). The authors report that this method achieves performance comparable to or better than DUS baselines while adding significantly fewer trainable parameters and offering comparable or faster computation times."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. MIDUS can achieve strong performance for DUS using siginifantly fewer parameters\n2. Ingenious design of the memory block. Using two separate $K$ to reduce computational cost and parameter count."}, "weaknesses": {"value": "1. Limited scope. The paper only explore the interleaving memory blocks. And other DUS policies are not discussed which may also enhance thier efficency.\n2. New hyperparameter complexity. MIDUS introduces new and non-trivial design choices such as the memory size, which increases the difficulty of finding the optimal hyperparameters."}, "questions": {"value": "1. The paper explicitly avoids stacking Memory Blocks. Was this simply out of scope, or did you find any evidence that stacking $M^{HML}$ blocks leads to instability?\n2. The decomposition of $K_h$ into two parts for the two-dimensional search is an effective strategy. Has this approach been extended to higher dimensions, such as a three-dimensional search?\n\ntypo: line 81-82, \"\\\\$H\\\\$\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "D0HfUxmnVZ", "forum": "paVKFh8veb", "replyto": "paVKFh8veb", "signatures": ["ICLR.cc/2026/Conference/Submission3045/Reviewer_NZXf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3045/Reviewer_NZXf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761221107391, "cdate": 1761221107391, "tmdate": 1762916524621, "mdate": 1762916524621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MIDUS (Memory-Infused Depth Up-Scaling), a drop-in alternative to standard depth up-scaling (DUS) that replaces duplicated FFN layers with Memory blocks. Each Memory block combines an attention layer without the output projection (Attn′) and a Head-wise Memory Layer (HML) that performs per-head product-key retrieval with an efficient value factorization (HIVE). The design preserves identity at initialization by zero-initializing memory values and routing the retrieved signal through a residual path, so the expanded model initially matches the base model’s outputs. Experiments on Llama-3.2-1B with 8 inserted blocks demonstrate lower perplexity and improved average zero-shot accuracy compared to strong DUS baselines, while reducing trainable parameters and training-time memory. Figure 1 contrasts DUS vs. MIDUS; Figure 2 (p.5) details the six-step Memory-block pipeline."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear, modular design (Memory block + HML + HIVE) that can be inserted wherever DUS would add FFNs; identity-preserving init is well motivated.\n\n2. Consistent gains across CPT and SFT with lower parameter/memory cost than DUS, plus ablations and placement analysis.\n\n3. Clarity & completeness: math formalization, stepwise diagram (p.5), and a reproducibility-friendly recipe; an anonymized code link is included."}, "weaknesses": {"value": "0. Some claims, especially those related to the major motivation, that existing methods rely \"on dense feed-forward networks\", are not accurate. For example, papers [1][2] are using \"mixture of depth\" like a sparse module for up-scaling. There is no discussion on the difference between these works, and they were not included as baselines.  \n\n1. All results use a 1B backbone. Claims about general-purpose LLM scaling would be stronger with a 7B-class (or larger) model and at least one instruction-tuned setting beyond Alpaca-GPT-4.\n\n2. Iteration times are reported, but sensitivity to sequence length (e.g., 8k–32k) isn’t analyzed; PKM’s two-stage top-k might have different break-even points. (Tables 2–4 give per-iter stats only)\n\n3. Helpful ablations are included, but further disentangling the impact of (i) Attn′ vs. full MHA, (ii) exact k and n choices, and (iii) HIVE’s parameterization per head would clarify where the gains come from.\n\n4. The benchmark suite is knowledge-centric; evaluating on reasoning-heavy or long-context tasks would test whether HML helps beyond factual retrieval\n\n[1] Raposo, David, et al. \"Mixture-of-depths: Dynamically allocating compute in transformer-based language models.\" arXiv preprint arXiv:2404.02258 (2024).\n\n[2] Tan Z, Dong D, Zhao X, et al. Dlo: Dynamic layer operation for efficient vertical scaling of llms[J]. arXiv preprint arXiv:2407.11030, 2024."}, "questions": {"value": "1. Can you detail the difference/limitation of the related works I mentioned above, preferably conduct an experiment against them?\n\n2. How does MIDUS-HML scale on 7B–13B backbones? Any obstacles (e.g., memory-bank thrashing) at larger widths? \n\n3. Can you report latency/flops scaling with sequence length and batch size (train & inference), and compare to DUS? Where’s the crossover vs. dense FFN?\n\n4. How sensitive are results to k, n, and the per-head transform size in HIVE? Any head-importance-aware allocation strategies tried?\n\n5. For CPT, do MIDUS and DUS baselines share identical data order, schedule, and optimizer hyperparameters? If not, please provide tuned-per-method tables and/or a unified ablation.\n\n6."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FPSsJQ6S44", "forum": "paVKFh8veb", "replyto": "paVKFh8veb", "signatures": ["ICLR.cc/2026/Conference/Submission3045/Reviewer_9rn7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3045/Reviewer_9rn7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941183043, "cdate": 1761941183043, "tmdate": 1762916524230, "mdate": 1762916524230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MIDUS, a novel method for scaling LLMs by increasing depth through memory-based rather than feed-forward expansion. MIDUS replaces these FFNs with Memory Blocks built around a Head-wise Memory Layer (HML), where each attention head maintains an independent memory bank for sparse retrieval. To further improve efficiency, the authors introduce Head-wise Implicit Value Expansion (HIVE), which factorizes per-head value spaces to preserve head alignment without redundant parameter storage. The design allows capacity to be added in a retrieval-based, head-aligned manner, effectively decoupling model quality gains from dense computation. Experiments on continual pre-training (CPT) and supervised fine-tuning (SFT) with Llama-3.2-1B demonstrate that MIDUS-HML consistently surpasses strong DUS baselines in both perplexity and zero-shot accuracy, while using fewer trainable parameters and less GPU memory."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- MIDUS achieves depth expansion through sparse retrieval rather than dense FFN projections, decoupling performance gains from the heavy parameter and activation costs of FFNs.\n- MIDUS–HML achieves the low perplexity and high average zero-shot accuracy, particularly excelling in benchmarks such as CSQA, BoolQ, PIQA, and MMLU.\n- HML assigns an independent memory bank per attention head, enabling selective retrieval aligned with head specialization, thereby minimizing cross-head interference compared to block-shared memories."}, "weaknesses": {"value": "- The work lacks formal justification or theoretical analysis of why memory retrieval at head level leads to better generalization or gradient propagation.\n- The paper would benefit from visualization or analysis of what the head-wise memories actually learn or retrieve—whether they store task-specific patterns, contextual cues, or token-level semantics.\n- Since MIDUS replaces dense FFN expansion with sparse retrieval, it introduces the need to carefully determine memory size and placement, which may affect optimal scaling."}, "questions": {"value": "- The authors fix the learning rate. Could the authors clarify whether this fixed rate was empirically tuned or simply adopted from earlier works?\n- What underlying dynamics cause internal residual connections to weaken retrieval signals?\n- What is the per-token inference overhead introduced by HML compared to standard FFNs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0BrKZmNm1i", "forum": "paVKFh8veb", "replyto": "paVKFh8veb", "signatures": ["ICLR.cc/2026/Conference/Submission3045/Reviewer_9Rab"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3045/Reviewer_9Rab"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962648708, "cdate": 1761962648708, "tmdate": 1762916524072, "mdate": 1762916524072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates depth upscaling to enhance LLM performance with light continued pretraining or supervised fine-tuning. Since attention heads specialize differently and additional dense FFN layers are computationally heavy, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval also in FFN. In experiments, the depth-upscaling layers adapt quickly with only light continued pretraining or supervised fine-tuning and deliver better accuracy than baseline models on commonsense reasoning tasks. The method consistently outperforms prior work while using fewer parameters and achieving higher efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper proposes a memory-based alternative to FFN replication, motivated by the head-independent representations in attention. Thus, it can be more efficient than prior depth-scaling approaches that use dense FFN layers, and it may be easier to learn due to the sparse, head-wise connections.\n\n* The experiments report both accuracy and efficiency. It also appears that the paper compares fairly with prior work and consistently outperforms it.\n\n* The method is easy to understand, and the presentation is clear."}, "weaknesses": {"value": "* Efficiency. As I understand it, the method adds additional layers. Then, why does it use fewer parameters and less GPU memory compared to the original model? Also, for latency, does the paper measure prefilling time or decoding time?\n\n* Task coverage. The experiments seem to focus on general-purpose commonsense reasoning. Could the authors also report results on harder domains such as code and math? Baselines may already perform well in these specialized areas, whereas depth upscaling trained on 2B web tokens may not transfer as effectively. In short, can depth upscaling still perform well (better performance than the original model) on code, math, or long-context tasks under the 2B web token CPT setup?\n\n* Comparisons. Why is the same zero-shot accuracy repeated in Table 2 as in Table 1? How does depth upscaling with CPT/SFT compare to training the same total number of layers from scratch? How does the method perform—in both efficiency and accuracy—on larger models such as 3B or 7B?\n\n\n* MIDUS layer design. In MIDUS, there appears to be no hidden-state mixing across heads in the feedforward layer; the whole hidden-state mixing happens only in the initial projections that produce Q/K/V. Do the authors think this could introduce any implicit limitations?"}, "questions": {"value": "Please see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DK7npl0E5D", "forum": "paVKFh8veb", "replyto": "paVKFh8veb", "signatures": ["ICLR.cc/2026/Conference/Submission3045/Reviewer_xNN6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3045/Reviewer_xNN6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3045/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992912513, "cdate": 1761992912513, "tmdate": 1762916523899, "mdate": 1762916523899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}