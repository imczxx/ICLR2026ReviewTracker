{"id": "kpLRYtPGt3", "number": 23288, "cdate": 1758341762134, "mdate": 1763275754085, "content": {"title": "Neon: Negative Extrapolation From Self-Training Improves Image Generation", "abstract": "Scaling generative AI models is bottlenecked by the scarcity of high-quality training data. The ease of synthesizing from a generative model suggests using (unverified) synthetic data to augment a limited corpus of real data for the purpose of fine-tuning in the hope of improving performance. Unfortunately, however, the resulting positive feedback loop leads to model autophagy disorder (MAD, aka model collapse) that results in a rapid degradation in sample quality and/or diversity. In this paper, we introduce Neon (for Negative Extrapolation frOm self-traiNing), a new learning method that turns the degradation from self-training into a powerful signal for self-improvement. Given a base model, Neon first fine-tunes it on its own self-synthesized data but then, counterintuitively, reverses its gradient updates to extrapolate away from the degraded weights.  We prove that Neon works because typical inference samplers that favor high-probability regions create a predictable anti-alignment between the synthetic and real data population gradients, which negative extrapolation corrects to better align the model with the true data distribution. Neon is remarkably easy to implement via a simple post-hoc merge that requires no new real data, works effectively with as few as 1k synthetic samples, and typically uses less than 1\\% additional training compute.  We demonstrate Neon’s universality across a range of architectures (diffusion, flow matching, autoregressive, and inductive moment matching models) and datasets (ImageNet, CIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the xAR-L model to a new state-of-the-art FID of 1.02 with only 0.36\\% additional training compute.", "tldr": "Instead of simply fine-tuning a generative model on its own synthetic outputs, briefly fine-tune it to find the direction of model collapse, then apply the reverse of that update to the original model for a major performance boost.", "keywords": ["Generative Models", "Self-Improvement", "Weight Merging", "Image Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6539a45e45cc6e58f16db163a72f52db45afa3f5.pdf", "supplementary_material": "/attachment/3dacf8c290aed3a2d51a7d5d46f940d0708dbb22.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose Neon, a self-training method that fine-tunes a base model using its own self-synthesized data with reversed gradient updates. Remarkably, Neon is effective even with as few as 1,000 synthetic samples. Its performance is validated across a wide range of architectures and datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Neon is simple to implement and effectively addresses common self-training challenges such as model autophagy disorder (MAD) and model collapse.\n\n* The method is evaluated across a wide range of architectures—including diffusion, flow-matching, autoregressive, and momentum-matching models—and diverse datasets such as ImageNet, CIFAR, and FFHQ."}, "weaknesses": {"value": "* The effectiveness of Neon hinges on the assumption that s < 0 (negative gradient alignment). However, this condition may not consistently hold in practice, raising concerns about the robustness of the approach.\n\n* The hyperparameter configuration—particularly the training budget and negative extrapolation strength w —could introduce additional tuning complexity. Ablation studies in Figures 3 and 4 suggest that Neon is sensitive to these hyperparameters, which may impact its ease of deployment."}, "questions": {"value": "* The key idea behind Neon—\"synthetic degradation and real-data improvement point in opposite directions\"—is intriguing. However, the current derivation of the supporting theorems appears loosely structured. Could you provide a clearer logical overview of how these derivations are organized? Additionally, how should this statement be interpreted intuitively in the context of model training? \n\n* In Figure 3, the FID increases as the self-training budget B grows. Does this suggest that extended training with Neon may lead to performance degradation? If so, does it challenge the assumption that s < 0 (negative gradient alignment) consistently holds?\n\n* In the images presented in Appendices E through I, are these generated samples from the Neon self-trained models, or are they the synthesized images used during the self-training process? Could you clarify this distinction and provide an analysis of the synthesized images used for self-training? Additionally, a visual interpretation explaining why these images are effective in contributing to the self-training process would be highly valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hFOEfSpugk", "forum": "kpLRYtPGt3", "replyto": "kpLRYtPGt3", "signatures": ["ICLR.cc/2026/Conference/Submission23288/Reviewer_BLA3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23288/Reviewer_BLA3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761052905970, "cdate": 1761052905970, "tmdate": 1762942591979, "mdate": 1762942591979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes NEON (Negative Extrapolation from self-traiNing): briefly self-train a generative model on its own samples to obtain degraded weights , then extrapolate backward from the degradation direction via a simple parameter merge . The authors give a theoretical account showing that common mode-seeking samplers create anti-alignment between synthetic and population gradients, so reversing the self-training direction reduces true data risk. Empirically, NEON improves diffusion, flow-matching, autoregressive, and few-step models on CIFAR-10, FFHQ, and ImageNet, often with <1% extra compute; notably it pushes xAR-L on ImageNet-256 from FID 1.28 → 1.02 with only 0.36% additional training compute."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Simple, general, post-hoc procedure that requires no extra real data, no auxiliary models, and no inference changes; just a short self-training run and a weight merge. \n\nClear theory: formalizes sampler-induced anti-alignment and gives conditions where negative extrapolation lowers population risk; also analyzes failure cases (diversity-seeking samplers). \n\nBroad empirical coverage across diffusion, flow matching, AR, and few-step models with consistent FID gains; includes precision/recall analysis explaining NEON’s recall-boosting mechanism. \n\nStrong results with tiny cost (often ≤2% of base training compute; sometimes as low as 0.36%), and improvements with as few as 1k synthetic samples. \n\nSOTA highlight: ImageNet-256 xAR-L FID 1.02, plus useful studies on (w, γ) co-tuning and cross-architecture transfer of the degradation signal."}, "weaknesses": {"value": "Positioning vs. simple weight merges: needs stronger comparisons to generic weight interpolation/extrapolation baselines (e.g., linear checkpoint merges/SWA-style extrapolation) to isolate NEON’s specific benefit beyond “negative LR step”. \n\nBenchmark scope: focuses on standard class-conditional/unconditional image generation; lacks large-scale text-to-image or broader modalities, and relies mainly on FID + P/R without human eval. \n\nHyperparameter sensitivity: performance depends on w (and γ for AR/CFG); while grids are shown, guidance on automatic selection or stability across training checkpoints could be expanded."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2sNI4GMr3s", "forum": "kpLRYtPGt3", "replyto": "kpLRYtPGt3", "signatures": ["ICLR.cc/2026/Conference/Submission23288/Reviewer_Srik"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23288/Reviewer_Srik"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642034774, "cdate": 1761642034774, "tmdate": 1762942591680, "mdate": 1762942591680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Usually training/retraining generative models on synthetic data can degrade performance metrics. On the opposite, this paper proposed a technique to leverage (bad) synthetic data from generative models, and improve generation. This work is in the direct line of work following Karras 2024 and Alemohammad 2024."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written, and the idea is clearly explained. Experiments are overall rather well presented. Figure 2 explains well the approach."}, "weaknesses": {"value": "- I do not understand the novelty with respect to previous works.\nCould authors explain what is the difference between their work and [1] and mostly [2]? Especially, I really would like to better understand better the difference with [2]: the idea of using negative guidance from synthetic data is already in [2], in particular, Equation (1) of the proposed manuscript resembles line 4 in algorithm 1 of [2]. Are you doing SIMS, but in the parameter space? Could authors comment on that?\n- How significant do you consider the empirical results? Can you show the same plots on test set, with the Dinov2 embedding?\n- in particular I would be interested to see if the minimum value on the training set correlates with the minimum value on the test set\n\n\n[1] Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine.\nGuiding a diffusion model with a bad version of itself.\n\n[2] Sina Alemohammad, Ahmed Imtiaz Humayun, Shruti Agarwal, John Collomosse, and Richard Baraniuk. Self-improving diffusion models with synthetic data\n\n\nNon-scientific comment: I would remove the following quote, \"In the words of Martin Luther King, Jr., “Sometimes to move forward, we have to go backward.”, used to motivate negative parameter guidance. It does not feel appropriate"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HymAAX64GS", "forum": "kpLRYtPGt3", "replyto": "kpLRYtPGt3", "signatures": ["ICLR.cc/2026/Conference/Submission23288/Reviewer_YZmD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23288/Reviewer_YZmD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929748061, "cdate": 1761929748061, "tmdate": 1762942591384, "mdate": 1762942591384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the author proposed a new self-training algorithm for image generative models NEON. Specifically, NEON uses synthetic images generated by the generative model to finetune the model, and use the performance degredation as a learning signal. The key insight is that most inference samplers are mode-seeking, biasing samples towards high-density regions of the model distribution, resulting in model collapasing, and worsens FID. Consequently, NEON uses negative extrapolation between the reference model and briefly self-trained model, avoids mode collapsing. The author tested their NEON algorithm across different generative models and datasets and shows consistent FID improvement with little computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Overall, the paper introduces Neon, a simple but effective method that interprets self-training degradation as a useful signal for improvement. The approach is theoretically grounded and empirically validated across diffusion, flow, and autoregressive models.\n\n- The core idea of reversing the direction of self-training degradation is both simple and effective. The implementation requires only a single weight extrapolation step and no architectural or loss modifications, making the method easy to implement and apply.\n- Neon adds little computational overhead and scales well across model families. Its simplicity makes it readily to deploy in large-scale training pipelines.\n- The experiments are thorough, spanning multiple datasets (CIFAR-10, FFHQ, ImageNet-256/512) and model architectures. Across all cases, Neon consistently improves FID and recall. The recall and precision analysis provided nice support to the theory of Neon. It treats precision for recall, encouraging diveristy by negative extropolation. \n- Ablations show that Neon’s benefits generalize across architectures and even compensate for reduced real data availability.\n- The controlled toy experiment from the appendix effectively illustrate the theoretical predictions and help readers grasp the distinction between mode-seeking and diversity-seeking regimes.\n\nI did not verify all derivations in detail, but at an intuitive level the theoretical claims are consistent and well-motivated. I defer to other reviewers for a closer assessment of the mathematical rigor and proofs."}, "weaknesses": {"value": "- The paper does not include quantitative comparisons against other self-training baselinesin the precision/recall/FID analyses. Even showing results for the simplest baseline of direct self-finetuning would help clarify how much of Neon’s gain comes from the negative extrapolation itself versus the self-training process. Moreover, comparisons in terms of data and compute efficiency would contextualize Neon’s benefits relative to prior self-improvement algorithms.\n- While the quantitative results are compelling, the paper would benefit from a few qualitative visual examples to illustrate how Neon changes the generative behavior. It remains a bit unclear whether the performance gains are purely distributional (i.e., improved diversity and recall) or also reflect higher fidelity and perceptual realism compared to naive self-training."}, "questions": {"value": "- Can the authors provide qualitative visual examples to illustrate how Neon changes the generative distribution? Are the observed gains primarily due to improved diversity (recall), or do they also enhance perceptual fidelity compared to naïve self-training—or possibly hurt fidelity due to the diversity–precision trade-off?\n- Can the author also include some baselines for the precsion/recall/FID analysis?\n- Do the authors see Neon as applicable to other domains such as NLP generation?\n- For figure 9, what is the synhetic dataset used samped from for the 30k model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MTr5AwlSjR", "forum": "kpLRYtPGt3", "replyto": "kpLRYtPGt3", "signatures": ["ICLR.cc/2026/Conference/Submission23288/Reviewer_dZE2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23288/Reviewer_dZE2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23288/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968324428, "cdate": 1761968324428, "tmdate": 1762942591153, "mdate": 1762942591153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Changes to the Revised Manuscript"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful feedback. We have made the following major changes to the manuscript:\n\n**Note:** Changes in the main paper are highlighted in blue for easy identification.\n\n---\n\n### **1. Strengthened Theoretical Presentation**\n\n- **Added Gaussian Warmup Example (new Appendix B.1):** A fully worked analytical example demonstrating Neon's mechanism on Gaussian estimation with closed-form calculations.\n\n- **Added Proof Roadmap (new Appendix B.2):** A detailed roadmap outlining the five-stage logical structure of our proofs.\n\n- **Reorganized Section 3.1:** Improved transitions and clearer connections between geometric intuition and formal guarantees.\n\n- **Improved Proof Flow:** Added linking explanations between theorems throughout the appendix for better readability of proofs.\n\n---\n\n### **2. Extensive Qualitative Visualizations**\n\n- **Single-Sample Effect Visualization (new Appendix K):** Demonstrates how Neon affects individual samples through pixel-wise heatmaps, showing semantically meaningful changes rather than random noise.\n\n- **Systematic Grid Visualizations (Appendices H, I, J):** 2D grids showing joint effects of merge weight w and CFG scale γ across diverse ImageNet classes for IMM, VAR-d36-s, and EDM-VP models.\n\n- **Extended Baseline Comparisons (Figures 4, D.1):** Added w = −1 (direct self-training) to demonstrate that self-training degrades performance while Neon improves it.\n\n---\n\nWe believe these revisions substantially improve the manuscript's clarity and empirical support. We are grateful for the constructive feedback.\n\n---"}}, "id": "5goFPjEPy7", "forum": "kpLRYtPGt3", "replyto": "kpLRYtPGt3", "signatures": ["ICLR.cc/2026/Conference/Submission23288/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23288/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission23288/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763276260358, "cdate": 1763276260358, "tmdate": 1763276260358, "mdate": 1763276260358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}