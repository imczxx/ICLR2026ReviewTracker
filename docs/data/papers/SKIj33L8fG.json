{"id": "SKIj33L8fG", "number": 1845, "cdate": 1756952105330, "mdate": 1759898182658, "content": {"title": "SIDDesign: Sequence-Informed Distillation for Tertiary Structure-Based RNA Design", "abstract": "Tertiary structure-based RNA design, which aims to design nucleotide sequences that fold into a given 3D structure, is a fundamental challenge in synthetic biology and structure-guided design. Although recent work has advanced geometric encoders and architectural innovations, progress remains constrained by the scarcity and bias of resolved RNA tertiary structures. Considering that RNA folding and design are approximate inverse tasks, and that RNA foundation models employed in folding models capture rich RNA priors from large-scale sequence data, we are inspired to exploit these representation-level priors to enhance the RNA design task. Motivated by this perspective, we propose the Sequence-Informed Distillation framework for structure-based RNA Design (SIDDesign), which aligns structure-derived embeddings with sequence-level representations obtained from RNA foundation models. To bridge modality and contextual gaps between sequence and structure representations, we design a Similarity-aware Contextual Refinement (SimACR) module based on cross-attention. To further mitigate edge noise introduced during graph construction, we introduce a Stochastic Topology Regularization (STR) strategy during training that improves the robustness of message passing. Extensive experiments on benchmark datasets demonstrate the effectiveness of SIDDesign, with consistent and significant improvements over existing approaches.", "tldr": "We present SIDDesign, a distillation framework that transfers sequence-level priors from RNA foundation models to enhance tertiary structure-based RNA design through contextual refinement and stochastic topology regularization.", "keywords": ["Tertiary Structure-Based RNA Design", "Cross-Modal Distillation", "Stochastic Topology Regularization"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f330611f9905af2f771384083d3e6a314674adf3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes SIDDesign, a framework that transfers large-scale sequence priors from pretrained RNA foundation models into the structure-side latent space by aligning structure encoder embeddings with token-level teacher representations while tackling two core failure modes: a modality/context mismatch and spurious edges in kNN graphs. To narrow the modality gap, the Similarity-aware Contextual Refinement (SimACR) first injects structure context into sequence embeddings before alignment; to improve robustness to noisy topology, Stochastic Topology Regularization (STR) randomly perturbs graph connectivity during training with degree-consistent normalization. Implementation follows an existing RDesign-style geometric graph pipeline to ensure comparability, and at inference time, the teacher and refinement modules are removed, so there is no extra runtime cost. Experiments show consistent gains in recovery rate and Macro-F1 with more balanced performance across short, medium, and long sequences, particularly strong improvements on short sequences where graph noise is more pronounced, and the method generalizes well on external benchmarks such as Rfam and RNA-Puzzles."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Rather than naively aligning embeddings, the method first refines sequence representations with structure context to correct the global-vs-local contextual asymmetry before alignment, which is a principled fix to the modality mismatch. \n\n- The work explicitly tackles spurious edges from fixed-k kNN graphs using stochastic edge dropping and degree-aware normalization, yielding topology-level data augmentation and improved generalization. \n\n- The authors follow the RDesign pipeline for geometric graph construction and message passing, isolating the gains to the proposed distillation and regularization modules.\n- Gains appear in both recovery rate and Macro-F1 across length ranges, with especially strong improvements on short sequences where noisy edges are more likely."}, "weaknesses": {"value": "- I have a very general question about those cross-modality biomolecule sequence design papers: The method leans heavily on RNA foundation model as the teacher for representation alignment, which risks importing that model’s inductive biases, such as dataset composition and pretraining objectives, into the structure-side space. The paper mentions that the sequence embedding is not aligned with the structural embedding in the beginning. Could the authors further elaborate on that in detail? For example, why/how are those embeddings aligned, and how could you say that it is not aligned? I think it is a very important question in the field. If you adopt the same MPNN scheme as RDesign or similar approaches, the Z_struct should have the same order as the Z_seq. \n\n\n- Evaluation is dominated by token-level correctness rather than functional plausibility. Recovery rate and Macro-F1 are useful diagnostics, but the problem of inverse folding ultimately depends on whether generated sequences fold back to the target structure and sit in favorable energy ranges while respecting secondary-structure pairings and chemical constraints. Optimizing for per-position classification may overfit to local statistics and under-represent global folding consistency. The study would be more convincing if it included fold-back checks with at least some secondary-structure predictors, and if it reported how often designed sequences reach the correct topologies or satisfy known structural constraints.\n\n\n- Diversity-accuracy trade-off is not quantified. In structure-based design there are typically many valid sequences per target; an approach that increases accuracy but collapses diversity can be ill-suited for downstream screening or optimization (like a sequence full of G/Cs). Distillation from a strong teacher can inadvertently narrow the output distribution, particularly if the alignment is tight and decoding is deterministic. The paper would benefit from sampling-based analyses (temperature, top-k/top-p), diversity metrics (e.g., unique sequence ratio, pairwise Hamming distances).\n\n- The method’s success depends on where and how the refinement is inserted relative to the structure encoder and predictor head, but the paper provides limited analysis on numbers of layers or attention heads. Different placements can change the balance between local geometric fidelity and global sequence context, affect training stability, and carry different compute footprints. A careful ablation would help guide re-use in other architectures and clarify whether most gains come from a single well-placed refinement block or from stacking.\n\n- During training, the model benefits from a teacher branch and from stochastic topology perturbations; at inference, both are absent. While such regularization patterns are common, the paper does not quantify how sensitive the final predictor is to the strength of these training-only signals, nor whether certain settings over-regularize and harm clean-graph inference. Reporting curves that vary teacher alignment strength and STR rates, together with inference-time robustness tests, would clarify stability and reveal whether the method is robust to mismatches between training and deployment conditions. As it might be time-consuming, I think the authors could only explain that part in more detail, the experiments could be added later."}, "questions": {"value": "Please see the weaknesses part, I have included my questions in it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4gwwKuCtJm", "forum": "SKIj33L8fG", "replyto": "SKIj33L8fG", "signatures": ["ICLR.cc/2026/Conference/Submission1845/Reviewer_h4Nv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1845/Reviewer_h4Nv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761410782074, "cdate": 1761410782074, "tmdate": 1762915909759, "mdate": 1762915909759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the SIDDesign framework, which innovatively addresses the core challenge of data scarcity in tertiary structure-based RNA design. It distills rich sequence-level priors from pre-trained RNA foundation models into structure-derived representations, introduces a Similarity-aware Contextual Refinement (SimACR) module to handle cross-modal alignment, and employs Stochastic Topology Regularization (STR) to enhance the robustness of graph representations. The experimental results report significant and consistent performance improvements on benchmark tests."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The central contribution lies in the innovative use of rich sequence-level prior knowledge from pre-trained RNA Foundation Models (RNA-FMs) to inform and guide structure-based design. This approach effectively mitigates the performance limitations imposed by the scarcity of resolved RNA tertiary structures, which is a significant challenge in the field.\n\n2. The proposed Stochastic Topology Regularization (STR) strategy appears effective in enhancing the robustness and generalization capability of the message-passing mechanism within the graph neural network, thereby mitigating potential noise introduced during the geometric graph construction.\n\n3. The authors present extensive empirical evidence across multiple benchmark datasets (RDesign, Rfam, and RNA-Puzzles), demonstrating improvements over a set of existing baselines across the standard evaluation metrics of Recovery Rate and Macro-F1."}, "weaknesses": {"value": "1. The baseline comparisons are insufficient. The generalization tests on Rfam and RNA-Puzzles do not include results for gRNAde. Furthermore, RhoDesign, which has shown strong performance on this task, was not included for comparison. These omissions weaken the persuasiveness of the model's performance claims\n\n2. The ablation study results regarding the Similarity-aware Contextual Refinement (SimACR) module warrant further analysis. The reported metrics suggest a non-uniform impact: while the module leads to performance gains for short sequences, the performance on medium and long sequences appears to decrease when SimACR is included. This raises a critical question about whether the module is consistently and effectively facilitating cross-modal alignment across diverse sequence lengths and structural complexities. A deeper investigation into this variance is required."}, "questions": {"value": "1. To better assess the model's true generalization capability and stability, have the authors considered evaluating the test set based on clustering by sequence similarity? This analysis would help confirm the model's performance on genuinely novel structures that are sequence-distant from the training data.\n\nAll other questions reiterate the items listed under Weaknesses. I am very willing to increase my overall score if the authors successfully address these concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WcauXn8J3W", "forum": "SKIj33L8fG", "replyto": "SKIj33L8fG", "signatures": ["ICLR.cc/2026/Conference/Submission1845/Reviewer_Y7Ny"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1845/Reviewer_Y7Ny"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854744269, "cdate": 1761854744269, "tmdate": 1762915909559, "mdate": 1762915909559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SIDDesign, a framework for tertiary structure-based RNA design that integrates priors from large-scale RNA foundation models. The key idea is to leverage sequence-informed representations to guide structural representation learning, addressing the data scarcity problem inherent in RNA tertiary structures. The method introduces two main components: (1) a Similarity-aware Contextual Refinement (SimACR) module that aligns sequence and structure embeddings through cross-attention, and (2) Stochastic Topology Regularization (STR), which randomly perturbs graph connectivity during training to mitigate edge noise. Experiments on benchmark and external datasets show that SIDDesign achieves improved recovery rates and Macro-F1 scores over prior models such as RDesign and gRNAde, demonstrating better robustness and generalization in RNA sequence design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper addresses an important and timely problem in tertiary structure-based RNA design, proposing a creative way to leverage sequence-level priors from RNA foundation models to compensate for structural data scarcity. The idea of aligning latent spaces between sequence and structure modalities via sequence-informed distillation is original and well-motivated from a representation learning perspective. The methodological design, particularly the Similarity-aware Contextual Refinement and Stochastic Topology Regularization, is conceptually sound and clearly described. The experiments are comprehensive, comparing with strong baselines and demonstrating consistent, though moderate, improvements. The paper is clearly written and logically structured, making it easy to follow despite the technical depth."}, "weaknesses": {"value": "The main limitation lies in novelty. Similar sequence-informed or foundation-model–guided strategies have already been explored in protein inverse folding (e.g., using ESM embeddings to enhance design models). Thus, while the adaptation to RNA is meaningful, the methodological contribution is somewhat incremental."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7zko84Kjkf", "forum": "SKIj33L8fG", "replyto": "SKIj33L8fG", "signatures": ["ICLR.cc/2026/Conference/Submission1845/Reviewer_jxpd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1845/Reviewer_jxpd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931323701, "cdate": 1761931323701, "tmdate": 1762915907959, "mdate": 1762915907959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}