{"id": "EbU7xoof80", "number": 1035, "cdate": 1756829457534, "mdate": 1759898231672, "content": {"title": "Beyond Noise: Non-Transitive Preferences as Consistency Checks for Robust LLM Evaluation", "abstract": "Large language models (LLMs) are increasingly deployed as judges of text quality, yet their verdicts often exhibit non-transitive preferences. We present a systematic study of Condorcet cycles as a diagnostic lens for LLM-as-a-judge. Across 688 debate motions and five frontier models (including the judge model itself, GPT-4o), we show that (i) cycle frequencies obey a tightly fitted negative binomial law ($R^2=0.9973$), (ii) linguistic properties such as syntactic complexity ($\\beta=0.130$) and readability ($\\beta=-0.085$) reliably modulate cycle formation, and (iii) models display a strong preliminary scale — consistency tradeoff (Pearson $r=0.924$): larger models achieve higher average rankings but participate in more inconsistency cycles. These findings reframe cycles from \"noise to be removed\" into actionable diagnostics, with practical metrics (stance-level: 7.19\\%; motion-level: 14.10\\%) and graph-theoretic tools that support reproducible, consistency-aware evaluation. This perspective opens a new dimension of scaling law research — the scaling laws of consistency — and provides actionable diagnostics for robust evaluation paradigms of LLMs in high-stakes settings.", "tldr": "Non-transitive preferences in LLM evaluation are not noise but structured signals that follow a negative binomial distribution, are modulated by linguistic features, and reveal a scale–consistency tradeoff.", "keywords": ["LLM Evaluation", "Non-Transitive Preferences", "Condorcet Cycles", "Evaluation Robustness", "Evaluation Consistency", "Model Scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/32620543162f903efc33165903539420c65e95a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a systematic investigation of non-transitive preferences (Condorcet cycles) in LLM-as-a-judge evaluation systems. Rather than treating these cycles as noise to be eliminated, the authors reframe them as diagnostic signals for evaluation reliability. Through analysis of 688 debate motions across five contemporary LLMs using GPT-4o as judge, they demonstrate that cycle frequencies follow a negative binomial distribution ($R^2 = 0.9973$), correlate with linguistic features such as syntactic complexity and readability, and exhibit a preliminary scale-consistency tradeoff where larger models participate in more cycles despite higher performance. The work introduces stance-level (7.19%) and motion-level (14.10%) inconsistency metrics as complementary measures to traditional ranking systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a conceptually interesting contribution by reframing non-transitive preferences from artifacts to be removed into structured diagnostic signals. This perspective shift is valuable for the community as it provides a more nuanced understanding of LLM judge reliability. The statistical rigor is commendable, particularly the demonstration that cycle counts follow a negative binomial distribution with exceptional fit. This finding suggests that inconsistency is not random but follows reproducible statistical regularities, which is an important theoretical insight.\n\nThe experimental design is well thought out, especially the blind evaluation protocol where GPT-4o serves as both judge and contestant. This setup allows for direct investigation of self-preference effects while controlling for explicit bias. The finding that GPT-4o ranks third overall without showing absolute self-preference, yet participates extensively in cycles (72.48%), provides an interesting contrast to prior work on self-recognition in summarization tasks [1]. \n\nThe paper is generally well written with clear motivation and appropriate contextualization within social choice theory and LLM evaluation literature. The introduction of two-level inconsistency metrics (stance-level and motion-level) provides practical tools for quantifying judge reliability at different granularities."}, "weaknesses": {"value": "The most significant limitation is the reliance on a single judge model (GPT-4o) throughout the entire study. While this ensures consistency, it raises questions about whether the observed patterns are general properties of LLM-as-a-judge systems or specific to GPT-4o's evaluation behavior. The paper would be substantially strengthened by including at least one or two additional judge models to demonstrate that the negative binomial law and linguistic correlations hold across different evaluators. Without this, we cannot distinguish between universal evaluation phenomena and model-specific quirks.\n\nThe scale-consistency analysis, while intriguing, suffers from several methodological issues beyond the acknowledged small sample size. The parameter counts are heterogeneous in source and reliability, with GPT-4o's count being an estimate rather than published specification. More critically, the five models span vastly different architectures (mixture-of-experts vs. dense transformers) and training paradigms, making it difficult to isolate the effect of scale from architectural innovations. The authors acknowledge these confounds but perhaps underestimate how severely they limit interpretability. For instance, Qwen3-14B's exceptional efficiency (3.69 wins per billion parameters) suggests that architecture may dominate scale effects entirely.\n\nThe permutation testing approach, while systematic, creates an artificial evaluation scenario that may not reflect real-world deployment conditions. Evaluating all 120 possible orderings per stance generates 165,120 tournament graphs, but in practice, we would typically see only a single ordering or a small subset. The paper does not adequately discuss how cycle rates under exhaustive permutation relate to what we would observe in natural evaluation settings. This gap between experimental design and practical application limits the immediate utility of the reported inconsistency rates.\n\nThe comparison with Chatbot Arena (Table 1) is presented as external validation, but the substantial ranking differences (DeepSeek-R1 ranks 1st in debate Elo but 8th in Arena) and limited model overlap make this comparison somewhat superficial. The authors appropriately caution against direct comparison, but then the inclusion of this table raises questions about what we should actually learn from it. Either the comparison is meaningful enough to warrant deeper analysis of the discrepancies, or it should be de-emphasized.\n\nThe linguistic feature analysis, while statistically significant, explains relatively little variance in cycle formation. The effect sizes are modest (largest $\\beta = 0.130$ for syntactic complexity), and the paper does not report model fit statistics (e.g., pseudo-$R^2$) for the Poisson regression. Understanding how much of the cycle variation is actually captured by these linguistic features would help assess their practical importance for benchmark design."}, "questions": {"value": "I would like the authors to address several points that could strengthen the paper or clarify its contributions. First, can you provide any preliminary evidence that the observed patterns generalize beyond GPT-4o as judge? Even a small-scale replication with one additional judge model (e.g., Claude or Gemini) would substantially increase confidence in the universality of your findings. If such experiments are infeasible for the camera-ready version, can you at least discuss what specific predictions your framework makes about how different judge models might differ in their cycle formation patterns?\n\nSecond, regarding the scale-consistency correlation, have you considered analyzing this relationship within a single model family? For instance, if you could evaluate using Qwen3 models of different sizes (8B, 14B, and potentially larger variants if available) as contestants while holding the judge constant, this would provide much cleaner evidence about scale effects. What is your intuition about whether the correlation would hold within-family, or is it driven by cross-family architectural differences?\n\nThird, the paper would benefit from a more concrete discussion of how practitioners should use your inconsistency metrics. If a benchmark reports 15% motion-level inconsistency, what should we conclude about its reliability? Are there natural thresholds or comparisons that would help interpret these numbers? Related to this, how do your metrics relate to inter-annotator agreement measures in human evaluation, which the community is more familiar with?\n\nFourth, I am curious about the relationship between position bias and cycle formation. You report substantial order effects (mean bias 0.3356) and note they contribute to cycles, but the exact mechanism is unclear. Do cycles occur primarily in cases where position bias is strong, or are they independent phenomena? Could you quantify what proportion of cycles would disappear if position bias were perfectly corrected?\n\nFinally, I find your suggestion to explore these signals in human-as-judge settings quite interesting and would encourage you to expand on this direction. What specific patterns would you expect to see differently in human evaluation data? Human judges presumably also exhibit non-transitive preferences, but would they follow the same negative binomial law? Would the linguistic correlates be similar or different? This extension could significantly broaden the impact of your framework beyond LLM evaluation.\n\n[1] Panickssery et al., \"LLM evaluators recognize and favor their own generations,\" NeurIPS 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QBOZZ1DUEc", "forum": "EbU7xoof80", "replyto": "EbU7xoof80", "signatures": ["ICLR.cc/2026/Conference/Submission1035/Reviewer_h7Gd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1035/Reviewer_h7Gd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550096233, "cdate": 1761550096233, "tmdate": 1762915659319, "mdate": 1762915659319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic investigation of non-transitive preferences (Condorcet cycles) in LLM-as-a-judge evaluation systems. Rather than treating these cycles as noise to be eliminated, the authors reframe them as diagnostic signals for evaluation reliability. Through analysis of 688 debate motions across five contemporary LLMs using GPT-4o as judge, they demonstrate that cycle frequencies follow a negative binomial distribution ($R^2 = 0.9973$), correlate with linguistic features such as syntactic complexity and readability, and exhibit a preliminary scale-consistency tradeoff where larger models participate in more cycles despite higher performance. The work introduces stance-level (7.19%) and motion-level (14.10%) inconsistency metrics as complementary measures to traditional ranking systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a conceptually interesting contribution by reframing non-transitive preferences from artifacts to be removed into structured diagnostic signals. This perspective shift is valuable for the community as it provides a more nuanced understanding of LLM judge reliability. The statistical rigor is commendable, particularly the demonstration that cycle counts follow a negative binomial distribution with exceptional fit. This finding suggests that inconsistency is not random but follows reproducible statistical regularities, which is an important theoretical insight.\n\nThe experimental design is well thought out, especially the blind evaluation protocol where GPT-4o serves as both judge and contestant. This setup allows for direct investigation of self-preference effects while controlling for explicit bias. The finding that GPT-4o ranks third overall without showing absolute self-preference, yet participates extensively in cycles (72.48%), provides an interesting contrast to prior work on self-recognition in summarization tasks [1]. \n\nThe paper is generally well written with clear motivation and appropriate contextualization within social choice theory and LLM evaluation literature. The introduction of two-level inconsistency metrics (stance-level and motion-level) provides practical tools for quantifying judge reliability at different granularities."}, "weaknesses": {"value": "The most significant limitation is the reliance on a single judge model (GPT-4o) throughout the entire study. While this ensures consistency, it raises questions about whether the observed patterns are general properties of LLM-as-a-judge systems or specific to GPT-4o's evaluation behavior. The paper would be substantially strengthened by including at least one or two additional judge models to demonstrate that the negative binomial law and linguistic correlations hold across different evaluators. Without this, we cannot distinguish between universal evaluation phenomena and model-specific quirks.\n\nThe scale-consistency analysis, while intriguing, suffers from several methodological issues beyond the acknowledged small sample size. The parameter counts are heterogeneous in source and reliability, with GPT-4o's count being an estimate rather than published specification. More critically, the five models span vastly different architectures (mixture-of-experts vs. dense transformers) and training paradigms, making it difficult to isolate the effect of scale from architectural innovations. The authors acknowledge these confounds but perhaps underestimate how severely they limit interpretability. For instance, Qwen3-14B's exceptional efficiency (3.69 wins per billion parameters) suggests that architecture may dominate scale effects entirely.\n\nThe permutation testing approach, while systematic, creates an artificial evaluation scenario that may not reflect real-world deployment conditions. Evaluating all 120 possible orderings per stance generates 165,120 tournament graphs, but in practice, we would typically see only a single ordering or a small subset. The paper does not adequately discuss how cycle rates under exhaustive permutation relate to what we would observe in natural evaluation settings. This gap between experimental design and practical application limits the immediate utility of the reported inconsistency rates.\n\nThe comparison with Chatbot Arena (Table 1) is presented as external validation, but the substantial ranking differences (DeepSeek-R1 ranks 1st in debate Elo but 8th in Arena) and limited model overlap make this comparison somewhat superficial. The authors appropriately caution against direct comparison, but then the inclusion of this table raises questions about what we should actually learn from it. Either the comparison is meaningful enough to warrant deeper analysis of the discrepancies, or it should be de-emphasized.\n\nThe linguistic feature analysis, while statistically significant, explains relatively little variance in cycle formation. The effect sizes are modest (largest $\\beta = 0.130$ for syntactic complexity), and the paper does not report model fit statistics (e.g., pseudo-$R^2$) for the Poisson regression. Understanding how much of the cycle variation is actually captured by these linguistic features would help assess their practical importance for benchmark design."}, "questions": {"value": "I would like the authors to address several points that could strengthen the paper or clarify its contributions. First, can you provide any preliminary evidence that the observed patterns generalize beyond GPT-4o as judge? Even a small-scale replication with one additional judge model (e.g., Claude or Gemini) would substantially increase confidence in the universality of your findings. If such experiments are infeasible for the camera-ready version, can you at least discuss what specific predictions your framework makes about how different judge models might differ in their cycle formation patterns?\n\nSecond, regarding the scale-consistency correlation, have you considered analyzing this relationship within a single model family? For instance, if you could evaluate using Qwen3 models of different sizes (8B, 14B, and potentially larger variants if available) as contestants while holding the judge constant, this would provide much cleaner evidence about scale effects. What is your intuition about whether the correlation would hold within-family, or is it driven by cross-family architectural differences?\n\nThird, the paper would benefit from a more concrete discussion of how practitioners should use your inconsistency metrics. If a benchmark reports 15% motion-level inconsistency, what should we conclude about its reliability? Are there natural thresholds or comparisons that would help interpret these numbers? Related to this, how do your metrics relate to inter-annotator agreement measures in human evaluation, which the community is more familiar with?\n\nFourth, I am curious about the relationship between position bias and cycle formation. You report substantial order effects (mean bias 0.3356) and note they contribute to cycles, but the exact mechanism is unclear. Do cycles occur primarily in cases where position bias is strong, or are they independent phenomena? Could you quantify what proportion of cycles would disappear if position bias were perfectly corrected?\n\nFinally, I find your suggestion to explore these signals in human-as-judge settings quite interesting and would encourage you to expand on this direction. What specific patterns would you expect to see differently in human evaluation data? Human judges presumably also exhibit non-transitive preferences, but would they follow the same negative binomial law? Would the linguistic correlates be similar or different? This extension could significantly broaden the impact of your framework beyond LLM evaluation.\n\n[1] Panickssery et al., \"LLM evaluators recognize and favor their own generations,\" NeurIPS 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QBOZZ1DUEc", "forum": "EbU7xoof80", "replyto": "EbU7xoof80", "signatures": ["ICLR.cc/2026/Conference/Submission1035/Reviewer_h7Gd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1035/Reviewer_h7Gd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550096233, "cdate": 1761550096233, "tmdate": 1763312614338, "mdate": 1763312614338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This research propose a systematic study of Condorcet cycles for LLM-as-a-judge. Instead of eliminating the cycles, they learning it from a novel perspective of logical consistency of judgement by LLM.  They introduce a novel metric called \"inconsistent judgements\" which quantifying reliability through stance-level and motion-level inconsistency rates. Through their experiments across 5 different models over 688 test instances of debate motions from IBM DebaterCoPA dataset. The results indicate that Condorcet cycles are structured signals rather than noise. The global rankings remain stable, but GPT-4o oddly participates in many cycles. At the end, they conclude a preliminary scale-consistency trade-off that shows a larger models participate in more cycles even as average strength improves."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This study adapts a novel perspective on the inconsistent logical cycles observed in LLM-as-a-judge. Rather than treating non-transitive preferences as noise, it frames them as an analytical signal and demonstrates consistent findings across multiple LLMs. In summary, I see several notable strengths:\n\n- Compelling problem formulation: The paper clearly articulates the challenge of non-transitive judgments in LLM-based evaluation and convincingly motivates why cycles should be analyzed rather than suppressed.\n\n- Transparent and well-specified methodology: The method is described with clarity, including dataset choice, experimental procedures, and the precise definition of evaluation metrics, making the study easy to follow and reproduce.\n\n- Direct yet effective experimental design: The empirical setup is straightforward but robust, covering five distinct models to validate the generality of the findings across systems."}, "weaknesses": {"value": "Although I find the insights provided in this paper refreshing, there are still several points that I believe are worth mentioning, as they may undermine the validity of the findings presented in this work.\n- External Validity Limitation: The majority of conclusions are based on GPT-4o serving as the sole judge. The absence of cross-verification among diverse reviewers (including a second closed-source judge or a robust open-source judge) and human referees means that the external generalizability of the findings and their sensitivity to different review styles remain unclear \n- Lack of Causality Analysis: The relationship between linguistic features and cycles mainly stems from regression correlations, lacking intervention manipulation experiments on factors such as readability, syntactic complexity, and semantic concentration to verify the direction of causality \n- \"Scaling laws of consistency\": To a degree, this paper successfully reframes logical inconsistency as a measurable system and demonstrates that the resulting diagnostics yield structured, high-fidelity signal distributions. However, from a scaling-law perspective, the work does not establish that optimizing these consistency metrics causally improves end-task model performance."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "77K70KrXoL", "forum": "EbU7xoof80", "replyto": "EbU7xoof80", "signatures": ["ICLR.cc/2026/Conference/Submission1035/Reviewer_6ZfT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1035/Reviewer_6ZfT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775298143, "cdate": 1761775298143, "tmdate": 1762915659135, "mdate": 1762915659135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents some interesting analysis on the presence of cycles when using LLMs as judges. The analysis focuses on debate motions. A single judge (GPT-4o) is used as a judge on generations from 5 other models. The pairwise judgements are mapped to a graph and cycles discovered as indicators of inconsistency in the judgements. The work recognizes that the inconsistencies (tracked by stance- and motion-level rates) are associated with linguistic factors for this data. Finally, they make some claims associated with scale and the correlation to inconsistencies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The analysis presented in this paper is pretty nice and quite interesting. The framing of measuring cycles as a diagnostic tool for LLM judges seems useful to think about for better reliability in evaluations.  \nThe framework for measuring cycles seems broadly useful.  \nThe correlation between linguistic features and the presence of inconsistencies is an interesting finding, though it seems hard to generalize this to broader settings."}, "weaknesses": {"value": "It’s hard to know how general the findings are. The paper would benefit from using additional judges or additional settings beyond debates.  \n\nAll the discussion on scale seems over-claimed. It’s distracting and the paper would be fine and interesting without it. The controls for that analysis are just not there.. The discussion is framed as “exploratory” and “preliminary” anyway and nothing in the experimental setup supports conducting that analysis. It would be interesting to study, but this paper does not."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AT0OZ73qLy", "forum": "EbU7xoof80", "replyto": "EbU7xoof80", "signatures": ["ICLR.cc/2026/Conference/Submission1035/Reviewer_kAUp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1035/Reviewer_kAUp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865647619, "cdate": 1761865647619, "tmdate": 1762915658979, "mdate": 1762915658979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies Condorcet cycles in LLM-as-a-judge evaluations over 688 debate motions and five models. It reports cycle distributions fitting a negative binomial, linguistic predictors of cycles, and a preliminary scale–consistency trade-off, proposing cycle metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Stance-level and motion-level inconsistency rates are easy to report and replicate; graph-theoretic framing is appropriate for tournaments.\n\n2. Negative-binomial overdispersion and linguistic correlates (e.g., syntactic complexity ↑, readability ↓ cycles) are plausible and practically useful."}, "weaknesses": {"value": "1. The paper states that it enumerates 5! permutations per stance (120) to “stress-test” cycle formation and generates 165,120 graphs. In a complete tournament (all pairwise comparisons fixed), the number of cycles is invariant to node ordering. It is unclear what is being permuted (presentation order? edge subsets?) and why this creates new graphs rather than re-labellings. As written, this step risks double-counting cycles and inflating distributional analyses.\n\n2. The paper insufficiently situates its contribution within existing research on preference inconsistency and logical coherence in LLMs. Several recent studies have already examined related phenomena but are not discussed here, such as [1,2,3,4].\n\n3. GPT-4o serves as both judge and one of the contestants. Even under identity blinding, shared inductive biases, length/style preferences, or safety policies could create dependencies. Using only one judge limits generalizability and makes order-bias corrections specific to that judge.\n\n4. The scale–consistency correlation (r=0.924, p=0.025) is based on five heterogeneous models and partially speculative parameter counts for closed models. This is underpowered and sensitive to measurement error; causal or “scaling law” language is premature.\n\n5. APIs evolve; temperature=0 reduces variance but does not eliminate non-determinism with tool use/safety filters. There is no reported test–retest reliability, prompt ablation (length control, verbosity control), or judge-ensemble analysis.\n\nReference:\n\n[1] ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions (https://arxiv.org/pdf/2406.08842)\n\n[2] Language Model Preference Evaluation with Multiple Weak Evaluators (https://arxiv.org/pdf/2410.12869)\n\n[3] Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment (https://arxiv.org/pdf/2502.14354)\n\n[4] Aligning with Logic: Measuring, Evaluating and Improving Logical Preference Consistency in Large Language Models (https://arxiv.org/pdf/2410.02205)\n\nThe omission of these works weakens the paper’s theoretical grounding and novelty claims, as it overlooks an emerging body of literature directly addressing LLM preference consistency and contradiction resolution."}, "questions": {"value": "See weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cILvoPAyHS", "forum": "EbU7xoof80", "replyto": "EbU7xoof80", "signatures": ["ICLR.cc/2026/Conference/Submission1035/Reviewer_i4uH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1035/Reviewer_i4uH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992404469, "cdate": 1761992404469, "tmdate": 1762915658849, "mdate": 1762915658849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}