{"id": "0DekoBl3te", "number": 9221, "cdate": 1758115541458, "mdate": 1759897737118, "content": {"title": "Dual-MoE: Learning Time and Channel Dependencies via Dual Mixture-of-Experts for Time Series Forecasting", "abstract": "Multivariate time series forecasting holds significant value in finance, energy, and transportation systems, yet faces critical challenges in jointly modeling temporal heterogeneity and dynamic channel dependencies. Existing approaches exhibit limitations in balancing long-term trends with short-term fluctuations, while struggling to capture time-varying inter-variable relationships. This paper proposes Dual-MoE, a dual mixture-of-experts framework that synergistically integrates temporal and channel modeling. The temporal expert dynamically combines multi-scale historical features (e.g., hourly details and weekly patterns) through adaptive gating mechanisms, whereas the channel expert learns dependency weights between variables via frequency-aware interaction modeling. Extensive experiments on real-world datasets demonstrate Dual-MoE's superior forecasting accuracy and robustness compared to state-of-the-art baselines. Its modular architecture provides a flexible and scalable paradigm for complex temporal dependency modeling, paving the way for further advancements in time series analysis.  Code is available in Appendix.", "tldr": "", "keywords": ["Time Series Forecasting"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d91d2446dc98942c3a8851d632fb64f3af14172.pdf", "supplementary_material": "/attachment/5f14436e934acc36691aa0e8867c141ea4f6105d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Dual-MoE, a dual-path mixture-of-experts framework for multivariate time series forecasting that jointly models temporal distribution shifts and noisy channel dependencies. The model consists of two complementary components: the Temporal Fusion MoE and the Channel Fusion MoE. Additionally, a Mask Loss mechanism is introduced to enhance robustness. Extensive experiments conducted on ten real-world datasets demonstrate that the proposed method outperforms existing approaches."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "**S1.** The paper presents a unified and interpretable framework that effectively addresses the challenges of temporal distribution shift and noisy channel dependencies, with clear motivation and a sound design.\n\n**S2.** Strong methodological contribution through the dual-path MoE design that jointly addresses temporal distribution shift and inter-variable dependency — a long-standing and insufficiently studied issue in time series modeling.\n\n**S3.** Comprehensive experiments across ten datasets, with ablation and sensitivity studies verifying module contributions and showing robustness under noisy or high-entropy conditions.\n\n**S4.** The paper is well-structured, with clear writing and informative visualizations that aid understanding."}, "weaknesses": {"value": "**W1.** The paper introduces a multi-scale lookback time-imaging mechanism, but the process for determining these scales remains unclear. The study currently relies on manually fixed configurations, without discussing whether a systematic or learnable strategy could automatically identify optimal lookback ranges for different datasets.\n\n**W2.** The Mask Loss filters noisy channels using a quantile threshold. How is this threshold selected, and how sensitive is the model’s performance to this hyperparameter? \n\n**W3.** The interpretability of the learned probability matrices in the Channel Fusion MoE is insufficiently discussed. It would strengthen the paper to include visual or quantitative analyses linking these learned relationships to known inter-variable dependencies, thereby substantiating the interpretability claims."}, "questions": {"value": "See **W1-W3**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RsDwEJdjBM", "forum": "0DekoBl3te", "replyto": "0DekoBl3te", "signatures": ["ICLR.cc/2026/Conference/Submission9221/Reviewer_FnY9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9221/Reviewer_FnY9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642903079, "cdate": 1761642903079, "tmdate": 1762920881576, "mdate": 1762920881576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dual-MoE, a dual mixture-of-experts framework for multivariate time series forecasting. It integrates a Temporal-fusion MoE for adaptive long–short term pattern modeling and a Channel-fusion MoE for inter-variable dependency learning. A quantile-based Mask Loss enhances robustness to noise. The framework is modular, flexible, and demonstrates consistent improvements across real-world datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes Dual-MoE, a dual mixture-of-experts framework designed to jointly model temporal dynamics and inter-variable dependencies in multivariate time-series forecasting.\n2. By integrating a Temporal-fusion MoE that captures both short-term fluctuations and long-term trend dynamics using Exponential Moving Average (EMA), and a Channel-fusion MoE that adaptively models inter-variable dependencies, the framework effectively handles temporal heterogeneity and dynamic correlations.\n3. Extensive experiments across diverse real-world datasets demonstrate that Dual-MoE consistently outperforms state-of-the-art baselines, showing superior robustness and forecasting accuracy under varying noise levels and temporal distribution shifts."}, "weaknesses": {"value": "1. Code Implementation-Paper Description Inconsistency\na. The code provided in the Supplementary Material does not implement the distance computation in Equation (2), indicating a discrepancy between the mathematical description and actual implementation.\nb. The parameter \\delta, described as a threshold for filtering noisy dependencies, is not used in the provided implementation. The paper does not clarify why it was omitted or how this affects performance.\n2. Terminological and Conceptual Inaccuracy\na. The term \"frequency-aware\" used for the Channel-fusion MoE is inconsistent with the implementation, which operates on time-domain patch embeddings without any frequency-domain transformation.\nb. Line 196 mentions \"Top-K expert selection based on mutual information maximization criteria\", but no such mechanism is described or implemented, suggesting either an omission or inconsistency in reporting.\n3. Ambiguity and Lack of Empirical Support\na. Figure 3 lacks labels for each time series, and the terms \"local fluctuations\" and \"global trends\" are used without clear quantitative definitions.\nb. The argument in line 149—that short lookback windows capture local fluctuations while long windows preserve global trends—is supported by only a single example, without dataset-level validation.\nc. The ablation study includes EMA removal but not a comparison with Simple Moving Average (SMA), which is a key baseline in prior work (e.g., DLinear). This makes it unclear whether improvements stem from EMA or from the proposed design."}, "questions": {"value": "1. The Supplementary Material implementation does not appear to include Equation (2). Was this distance computation intentionally omitted or replaced during experiments?\n2. The paper introduces \\delta as a threshold parameter but it is unused in the code—could the authors explain this discrepancy?\n3. Why is the Channel-fusion MoE described as \"frequency-aware\" when the implementation is based on time-domain operations?\n4. Could the authors provide more details or an explanation for the \"Top-K expert selection based on mutual information maximization criteria,\" which is mentioned in the text but not implemented?\n5. Could the authors clarify how \"local fluctuations\" and \"global trends\" are formally defined or measured across datasets? Is there a quantitative evaluation that supports the qualitative claim in Figure 3 regarding short vs. long lookback windows?\n6. Have the authors compared the proposed EMA-based decomposition with the Simple Moving Average (SMA) approach used in DLinear to isolate the impact of EMA?\n7. In Table 5, were model dimensions such as d_model kept consistent when computing MACs to ensure a fair efficiency comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "80ACZmjGE9", "forum": "0DekoBl3te", "replyto": "0DekoBl3te", "signatures": ["ICLR.cc/2026/Conference/Submission9221/Reviewer_CLRo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9221/Reviewer_CLRo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808369418, "cdate": 1761808369418, "tmdate": 1762920881235, "mdate": 1762920881235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dual-MoE, a novel and highly complex framework for multivariate time series forecasting. The work is motivated by two challenges: 1) Temporal Distribution Shift, where a fixed-length lookback window struggles to balance long-term trends and short-term shocks, and 2) Noisy Channel Dependencies, where models either over-rely on all channel correlations (like Transformers) or ignore them completely (like channel-independent models). The authors solve this challenge with Dual-MoE with two different mixture-of-experts and quantile-based loss function."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The preliminary study in Section 3 is good. It clearly demonstrates the non-trivial, U-shaped relationship between lookback window size and performance (Fig 1), providing a solid, data-driven justification for the proposed multi-scale temporal fusion approach.\n\nS2. The model demonstrates consistent and significant SOTA performance across a wide range of benchmarks (Table 1), outperforming a strong and recent set of baselines (iTransformer, PatchTST, DLinear, FITS, etc.). The high \"Count\" of first-place finishes (14) suggests the model is robust.\n\nS3. The paper is well-written, and easy to follow."}, "weaknesses": {"value": "W1. The methodology section (Sec 4) is exceptionally difficult to follow and contains several contradictions, making the proposed method nearly impossible to reproduce.\n\n> Figure 4(d) shows the \"Temporal-fusion MoE\" occurring after the main encoder stack, fusing features from a \"Multi-Scale Representation.\" In contrast, Equation 9 describes the T-MoE as a weighted sum of $Linear(Flatten(X_j))$, where $X_j$ appears to be the raw input from different windows before any deep encoding. These two descriptions are mutually exclusive. Does the fusion happen at the input layer or the output layer?\n\n> Section 4.2 mentions \"Multi-scale Lookback Time Imaging\" and \"ID-to-2D reshaping,\" strongly implying a 2D-convolutional or 2D-attention mechanism (like TimesNet). However, this is never mentioned again. The \"Transformer Encoder\" is described as standard (Sec 4.2/C.2), which typically uses 1D inputs. This is a critical, unclarified architectural detail.\n\n> The Channel-fusion MoE (Sec 4.3, Eq 2.) is based on a heuristic formula for the probability matrix P. The definitions of $D_{max}$ and the threshold $\\delta$ are unclear. The entire formula appears arbitrary and lacks theoretical or strong empirical justification.\n\n\nW2. The proposed model itself seems a combination of the previous works and marginally improved version of the other works, including M-scale inputs, EMA decomposition, utilization of multiple MoE, and quantile-based loss functions.\n\n> More importantly, the core concepts, especially the MoE with multiple roles [1] and the quantile-based loss functions [2] are already proposed in the previous works but is not cited in this paper. \n\n[1] TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts, in ICLR'24\n\n[2] Spatial Mixture-of-Experts, in NeurIPS'22\n\nW3. The efficiency analysis (Table 5) is weak. It compares MACs and inference time against \"Pathformer\" and \"PDF,\" but not against the SOTA baselines from Table 1, such as the highly efficient DLinear, iTransformer, or FITS. This feels like a cherry-picked comparison. Given the model's complexity, a comprehensive efficiency benchmark against the actual competitors is necessary.\n\nW4. The Mask Loss (Sec 4.5) is a problematic component. It essentially \"gives up\" on channels the model finds difficult. While this may improve aggregate MSE, it is undesirable in real-world scenarios where all channels must be forecast. Discussion about this concern is also discussed in the previous papers [1,2], but this paper missed it and make the optimization slow."}, "questions": {"value": "Q1. Does the Temporal-fusion MoE (Sec 4.4, Fig 4d, Eq 9) fuse the raw multi-scale inputs (like Eq 9 suggests) or the encoded representations from a deep stack (like Fig 4d suggests)? These are fundamentally different models.\n\nQ2. What is \"Time Imaging\"? Is the Transformer Encoder (Sec 4.2) a 1D model, or is it a 2D model (like TimesNet) that operates on the reshaped 2D \"images\"? This is a critical, missing architectural detail.\n\nQ3. Can you provide a full efficiency comparison (MACs, training time, inference time) against the primary SOTA baselines from Table 1, especially DLinear, iTransformer, and PatchTST? The comparison in Table 5 is insufficient.\n\nQ4. Can you provide a better justification for the Mask Loss? How does it affect the performance on the worst-performing (masked) channels? Does it not simply learn to ignore the most difficult, and potentially most critical, signals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "91XPN8sUjb", "forum": "0DekoBl3te", "replyto": "0DekoBl3te", "signatures": ["ICLR.cc/2026/Conference/Submission9221/Reviewer_5WjE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9221/Reviewer_5WjE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993933064, "cdate": 1761993933064, "tmdate": 1762920880734, "mdate": 1762920880734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript introduces Dual-MoE, a dual mixture-of-experts (MoE) framework for multivariate time series forecasting (MTSF). The model integrates two complementary modules: (1) a Temporal Fusion MoE that dynamically balances long- and short-term temporal dependencies via exponential moving average (EMA) and multi-scale lookback windows, and (2) a Channel Fusion MoE that models inter-variable relationships using a learnable probability matrix based on frequency-domain similarities. A quantile-based Mask Loss Function further enhances robustness to noisy or unpredictable channels."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is easy to follow."}, "weaknesses": {"value": "1. **Dispersed and Unfocused Innovation**\n   The paper’s innovation appears fragmented. Both *Temporal Distribution Shift* and *Channel Dependencies* are substantial and independent research problems, each worthy of separate, focused exploration. Presenting them together dilutes the novelty and makes the work feel more suitable for a journal article than a conference paper, which typically values concise and insight-driven contributions.\n\n   Furthermore, the paper seems to conflate short-/long-term dependencies with lookback window size. A larger temporal window already contains information about both short- and long-term patterns; thus, the real challenge lies not in window length itself, but in whether the model can *learn to attend to the right historical segments dynamically*. The current motivation does not make this distinction clear.\n\n   On the other hand, *Channel Dependencies* have been extensively studied in spatiotemporal forecasting literature. Although the datasets used in those works differ slightly in semantics, their data structures are identical, and the corresponding methods are largely transferable. This relevant body of work is not sufficiently acknowledged or contrasted, which weakens the originality of this component. Finally, the fourth paragraph of the Introduction is overly generic and lacks a clear connection to the two motivating problems discussed earlier.\n\n---\n\n2. **Overly Engineering-Oriented Method Design**\n   The proposed framework introduces many modules, but the motivation and underlying insights for these design choices are insufficiently analyzed. This again fragments the novelty and makes the contribution appear as a collection of engineering add-ons rather than a coherent conceptual advance.\n\n   For example, line 56 states that *“The Temporal Fusion MoE formulates the fusion of long- and short-term temporal patterns as a classification problem.”* However, the paper does not explain *why* this formulation is appropriate or advantageous, nor does it clarify *why* the Exponential Moving Average (EMA) is specifically chosen to track trends and distributional shifts. Without these theoretical or empirical justifications, readers may find it difficult to grasp the key insights that differentiate this work from existing architectures.\n\n---\n\n**Overall**, the paper lacks a clearly articulated motivation, concrete evidence supporting that motivation, and insightful reasoning linking the identified challenges to the proposed design. The methodological innovations are incremental and insufficiently conceptual to meet the originality standard expected for ICLR-level contributions."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LJZU9Yb0RV", "forum": "0DekoBl3te", "replyto": "0DekoBl3te", "signatures": ["ICLR.cc/2026/Conference/Submission9221/Reviewer_h2qw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9221/Reviewer_h2qw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9221/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998617572, "cdate": 1761998617572, "tmdate": 1762920879823, "mdate": 1762920879823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}