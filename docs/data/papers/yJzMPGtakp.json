{"id": "yJzMPGtakp", "number": 7999, "cdate": 1758050221011, "mdate": 1763503549597, "content": {"title": "Alignment, Convexity and Completeness: Mechanisms Behind GroupDRO", "abstract": "Models trained with Empirical Risk Minimization (ERM) often fail to generalize under spurious correlations. Group Robustness Methods (GRMs)—notably Group DRO (GDRO)—mitigate this by reweighting losses across groups defined by labels and spurious attributes, yet why they work remains only partially understood. We study the learning dynamics of GRMs and their effects on both the classifier head and the representation. Theoretically, in a head-only fine-tuning setting (fixed features), we analyze the classifier learned by GDRO and show: (i) GDRO aligns less with a spurious classifier and more with an oracle non-spurious classifier than ERM; (ii) when group losses are $\\mu$-strongly convex, the alignment gap controls performance, yielding an upper bound on the worst-group performance gap between ERM and GDRO; and (iii) for convex losses, adding L2 regularization induces $\\mu$-strong convexity, so the same guarantees apply—providing an explanation for the empirical gains of GDRO with L2 reported in prior work. Empirically, across standard image and text benchmarks, we confirm the predicted alignment behavior. Beyond the head, under end-to-end training GDRO also reshapes the representation:  through a measure called Completeness, we show that task-relevant information is spread across multiple dimensions in GDRO while ERM tends to concentrate it in fewer, making it more susceptible to rely on spurious attributes for prediction. Together, our theory and measurements clarify the mechanisms by which GroupdDRO outperforms ERM.", "tldr": "We find that GDRO works by both a strong classifier effect and by inducing lower completeness in learned representations.", "keywords": ["robustness fairness deep learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87330bb2673bf5792dde0de70724785e1e234baf.pdf", "supplementary_material": "/attachment/a32f0c593f5ab7ef3e57fbd911661e32c932b68d.zip"}, "replies": [{"content": {"summary": {"value": "This paper compares the classifier learned by GroupDRO, which minimizes the maximum loss on any subpopulation in the dataset, with the standard ERM classifier. Theoretical analysis is provided for a linear model with fixed features on a four-points dataset, and it is shown that the GroupDRO classifier aligns more with the non-spurious classifier. Experiments are also provided which analyze the effect of GroupDRO on the neural network representation; it is hypothesized that GroupDRO performs better because it distributes weight across multiple different features instead of focusing its magnitude on a single shortcut."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The idea to use the DCI framework [1] for analysis of neural network representations under spurious correlations is interesting (though insufficiently explored, see Weakness 4).\n\n[1] Eastwood and Williams.  A Framework for the Quantitative Evaluation of Disentangled Representations. ICLR 2018."}, "weaknesses": {"value": "1. It is unclear whether this paper studies the standard GroupDRO formulation, i.e., minimizing the maximum loss over any group. In Section 2.2.3 the paper defines GroupDRO as a version of reweighting with group-wise softmax weights. Minimizing the loss $\\mathcal{L}^{DRO}$ is equivalent to minimizing the standard GroupDRO loss only as $\\epsilon \\to \\infty$, a regime which is not discussed. While the theoretical results appear to use the typical min-max definition, the GroupDRO algorithm used for the empirical results in Section 4 is not made explicit, and it may differ from results obtained via the min-max objective. See [1, Section 4] for further discussion of the relationship between GroupDRO and importance weighting.\n\n2. The second listed contribution, “Role of regularization”, is not novel. The fact that adding $\\ell_2$ regularization to a convex loss induces strong convexity is one of its most basic properties. I also believe Theorem B.1, the main technical result in this paper upon which Proposition B.6 follows by definitions of ERM and strong convexity, is not very novel. The main observation of Theorem B.1 is that for convex losses the optimal GroupDRO classifier corresponds to an importance-weighted classifier with certain weights, but this is exactly the characterization of Proposition 1 of [1].\n\n3. From a technical perspective, the theoretical results are not exceptionally novel or sophisticated, and they do not introduce any techniques which might be generalizable beyond the scope of this paper. For example, analysis of linear models with fixed features on the four-points dataset is a simple low-dimensional setting which has been well-studied [2]. Moreover, no theoretical analysis of representation learning is provided despite this having been identified as an important challenge in the literature [3] and addressed in the experiments. Finally, the theoretical results study the optimal GroupDRO classifier $\\theta^*$ and not the actual SGD solution; it is unclear whether gradient descent/flow will converge to the min-max solution, i.e., under the continuous-time alternating minimization-like gradient flow of Prop. B.4.\n\n4. The empirical study is not convincing and overall lacking in rigor:\n\n    a. Section 4.1 and the associated Table 1 do not provide additional insights beyond similar experiments already presented in [3, 8, 9].\n\n    b. The metrics of the DCI framework are not formally defined, and how they are computed is not specified. In addition, metrics are computed after PCA reduction to only 50 dimensions, and the amount of preserved variance is not quantified, nor are comparisons made between metrics computed on PCA and non-PCA features.\n\n    c. The claim that “completeness is consistently lower for GDRO” is insufficiently supported. First, the strength of the claim is limited as GroupDRO matches the completeness of other methods in MNIST-CIFAR and CelebA, and is lowest by only a small amount on MultiNLI and 4/5 of the correlation settings on Waterbirds. Second, error bars are not provided in Figure 2, making it difficult to assess whether the lower completeness may be due to randomness or hyperparameters.\n\n5. Finally, comparison to previous work in theoretical analysis of spurious correlations and GroupDRO is severely lacking. Here are a few references within which the theoretical contributions of this paper should be contextualized: [2, 4, 5, 6, 7]\n\n[1] Sagawa et al. Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization. ICLR 2020.\n\n[2] Nagarajan et al. Understanding the failure modes of out-of-distribution generalization. ICLR 2021.\n\n[3] Izmailov et al. On Feature Learning in the Presence of Spurious Correlations. NeurIPS 2022.\n\n[4] Wang and Wang. On the Effect of Key Factors in Spurious Correlation: A Theoretical Perspective. AISTATS 2024.\n\n[5] Puli et al. Don't blame Dataset Shift! Shortcut Learning due to Gradients and Cross Entropy. NeurIPS 2023.\n\n[6] Sagawa et al. An Investigation of Why Overparameterization Exacerbates Spurious Correlations. ICML 2020.\n\n[7] Holstege et al. Optimizing importance weighting in the presence of sub-population shifts. ICLR 2025.\n\n[8] Kirichenko et al. Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. ICLR 2023.\n\n[9] Idrissi et al. Simple data balancing achieves competitive worst-group-accuracy. CLeaR 2022."}, "questions": {"value": "Please see the Weaknesses section; I would especially appreciate clarifications on points (1) and (4)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T0gOyzWrFa", "forum": "yJzMPGtakp", "replyto": "yJzMPGtakp", "signatures": ["ICLR.cc/2026/Conference/Submission7999/Reviewer_cURQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7999/Reviewer_cURQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761615990174, "cdate": 1761615990174, "tmdate": 1762920002291, "mdate": 1762920002291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretically grounded and empirically validated analysis of why GDRO outperforms ERM under spurious correlations. It shows that GDRO learns classifiers that align less with spurious directions and more with non-spurious directions, and that under µ-strongly convex losses, this alignment gap directly bounds the worst-group performance difference between ERM and GDRO. The authors further demonstrate that adding L2 regularization induces strong convexity, offering a principled explanation for GDRO’s reliance on strong L2 penalties. Extensive experiments on both image and text benchmarks confirm the theoretical predictions and reveal that GDRO not only adjusts the classifier head but also reshapes the learned representation by lowering completeness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Alignment-based reasoning reflected in consistent empirical behavior across multiple benchmarks in both vision and language domains.\n\nExplains why strong L2 regularization improves GDRO, which had been previously empirical folklore.\n\nThe paper is clearly written, well-organized, and supported by reproducible experiments."}, "weaknesses": {"value": "The theoretical framework is largely developed under the fixed-feature linear classifier assumption (Section 3.1 and Appendix B).\n\nSeveral simplifying assumptions, including symmetric group shifts, shared subspaces, and identical loss functions overlook real-world heterogeneity.\n\nThe completeness analysis in Section 5.3 focuses on correlations rather than causal relationships, leaving it uncertain whether reduced completeness plays a direct role in enhancing robustness.\n\nClarifying these aspects or discussing their implications would strengthen the overall contribution."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BTuhDG9yiD", "forum": "yJzMPGtakp", "replyto": "yJzMPGtakp", "signatures": ["ICLR.cc/2026/Conference/Submission7999/Reviewer_cv7c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7999/Reviewer_cv7c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949489612, "cdate": 1761949489612, "tmdate": 1762920001827, "mdate": 1762920001827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Objective of the paper is to theoretically explain the merits of Group-DRO (sagawa et.al.) and motivate the need for strong parameter regularization in Group-DRO."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Sub-population shift is a problem with practical significance."}, "weaknesses": {"value": "The main contribution of the paper is theoretical analysis, but I find the corresponding section 3 very hard to parse to the best of my efforts. It contained either too many symbols or symbols that are not defined. Too many to even list them out here. I expected the authors would present a summary (in vernacular text) before or after proposition statement, but they did not. In any case, what the authors embark to prove in section 3 is too simplified, classifier is trained with frozen representations. \n\nThe paper argues that GDRO leads to improved disentanglement of features, which in turn makes it easier for classifier to ignore the spurious features. But from Figure 2 (top row), GDRO is not significantly better than ERM on disentanglement."}, "questions": {"value": "GDRO paper justifies regularization with a simple reason: to avoid overfitting on the minority group. I do not understand the mu-strongly convex argument of the paper but I find the justification of the GDRO paper straightforward. Please explain why it is important to understand regularization beyond the simple argument? \n\nAlso, please address the concerns raised in the \"weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HqxnZGCeYG", "forum": "yJzMPGtakp", "replyto": "yJzMPGtakp", "signatures": ["ICLR.cc/2026/Conference/Submission7999/Reviewer_CjL4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7999/Reviewer_CjL4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7999/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762356064282, "cdate": 1762356064282, "tmdate": 1762920001255, "mdate": 1762920001255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}