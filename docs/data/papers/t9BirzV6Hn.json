{"id": "t9BirzV6Hn", "number": 22206, "cdate": 1758327758762, "mdate": 1763072617327, "content": {"title": "Reconstructing Humans with Articulated Hands using Transformers", "abstract": "In this paper, we introduce an approach to reconstruct 3D humans with expressive hands given a single image as input. Current methods for pose estimation display robust performance for either bodies or hands. Unfortunately, these methods fail to simultaneously produce accurate 3D body and hand reconstructions. To address this limitation, we take a more cohesive approach to ensure both coarser\nand finer features of the human body are properly localized. Our approach is based on a feedforward network and following recent best practices, we adopt a fully transformer-based architecture. One of the key design choices we make is to leverage two separate backbone networks, one for 3D human pose and one for 3D hand pose estimation. These backbones process independently the body region and the hand regions and can make estimates about the bodies and the hands of the person. However, when the estimates are made independently, they tend to be inconsistent with one another and lead to unsatisfying reconstruction. Instead, we introduce a coupling transformer decoder that is trained to consolidate the intermediate features from the individual backbones into making a consistent estimate for the body and the hands. The full system is trained on multiple datasets, including images with body ground truth, with hand ground truth, as well as images that include both body and hand ground truth. We evaluate our approach on the\nAGORA, ARCTIC, and COCO datasets, reporting metrics for both bodies and hands reconstruction accuracy to highlight our model’s robustness over previous baselines.", "tldr": "", "keywords": ["3D Reconstruction", "SMPL-H", "Computer Vision", "Transformers"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/21aa8deb60926a7d19d6fab416161dfdc1cd6dd4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces BodhaHMR, a transformer-based method for reconstructing 3D humans with expressive articulated hands from a single RGB image. The approach employs two separate Vision Transformer (ViT-H) backbones—one for body processing (initialized from HMR 2.0) and one for hand processing (initialized from HaMeR). These backbones process crops of the body and hand regions independently, producing feature tokens that are then fused by a unifying transformer decoder to predict SMPL-H parameters. The method is trained on multiple datasets including COCO-Wholebody, Human3.6M 3D WholeBody, and SynthMoCap. Evaluations on AGORA, ARCTIC, and COCO datasets demonstrate state-of-the-art performance on 2D hand pose estimation while maintaining competitive body reconstruction accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method achieves state-of-the-art performance on 2D hand pose estimation across all tested datasets (AGORA, ARCTIC, COCO) while maintaining competitive body reconstruction accuracy, demonstrating the effectiveness of the unified approach.\n2. Figures 3-5 demonstrate that the method handles diverse poses, viewpoints, and challenging scenarios including occlusions and egocentric views effectively.\n3. The dual-backbone architecture with a coupling transformer decoder is a sensible design choice for balancing coarse body features and fine hand details, addressing a clear gap in existing expressive mesh recovery methods."}, "weaknesses": {"value": "1. The approach primarily combines existing pretrained backbones (HMR 2.0 and HaMeR) with a standard transformer decoder. The main contribution is the coupling strategy rather than novel architectural components, which limits the technical innovation.\n2. While 2D hand results are strong, the method underperforms on 3D hand metrics (MPJPE) on ARCTIC compared to the Frankenstein baseline. The paper acknowledges this but doesn't provide sufficient analysis of why the coupling decoder sacrifices 3D accuracy for 2D alignment.\n3. Dataset evaluation concerns:\n- For COCO evaluation, the validation set is pruned to only include subjects with visible ear keypoints to accommodate Multi-HMR baselines, which may bias results\n- No evaluation on egocentric datasets despite showing qualitative egocentric examples"}, "questions": {"value": "1. Can you provide more insight into why the method achieves better 2D hand alignment but sometimes worse 3D hand pose compared to Frankenstein? Is this inherent to the coupling approach or a tunable trade-off?\n2. How does the method perform when hand crops are of very low resolution or heavily occluded? Are there failure cases that could be characterized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kL6WKp7Ihc", "forum": "t9BirzV6Hn", "replyto": "t9BirzV6Hn", "signatures": ["ICLR.cc/2026/Conference/Submission22206/Reviewer_QwTA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22206/Reviewer_QwTA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901011917, "cdate": 1761901011917, "tmdate": 1762942113505, "mdate": 1762942113505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "N3j2ZWRZ0e", "forum": "t9BirzV6Hn", "replyto": "t9BirzV6Hn", "signatures": ["ICLR.cc/2026/Conference/Submission22206/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22206/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763072616535, "cdate": 1763072616535, "tmdate": 1763072616535, "mdate": 1763072616535, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BodhaHMR, a single-image method for expressive human mesh recovery that jointly reconstructs the body and articulated hands using SMPL-H parameters. The system uses two dedicated ViT backbones—one for the full body and one shared for the two hands—whose token outputs are fused by a unifying transformer decoder to regress body pose, hand poses, shape, and camera in a coherent way. Training leverages a mix of in-the-wild, synthetic, and controlled datasets with 2D/3D supervision for bodies and hands; evaluation is conducted on AGORA, ARCTIC, and COCO-WholeBody with both 3D (MPJPE/PA-MPJPE) and 2D (PCK) metrics. Empirically, the method achieves acceptable but not well hand reconstruction performance, while maintaining competitive body accuracy compared to recent baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper’s main design—separate body/hand encoders with a unifying transformer head—explicitly tackles the cross-part inconsistency that arises when body and hands are predicted independently, yielding a single consistent SMPL-H estimate. This architectural choice is clearly specified and motivated."}, "weaknesses": {"value": "The idea of using separate bounding boxes and encoders/heads for body vs. hands has ample precedent tracing back to early works such as PIXIE and many subsequent studies. If the core contribution is primarily this decomposition plus a fusion head, the architectural novelty feels just incremental.\n\nWrist–arm consistency not properly evaluated. If the model is initialized from HaMeR and then jointly trained for body+hand, one would expect better wrist–forearm alignment. However, standard MPJPE is largely insensitive to local orientation/kinematic consistency (a wrist joint can have low position error despite poor orientation or continuity). The paper should design targeted evaluations e.g., wrist–forearm relative orientation error, kinematic continuity, or per-frame twist/roll consistency, to validate the intended coupling.\n\n If a separate hand module or hand-specific crop is used, the paper should compare against hand-focused methods (e.g., WiLoR) rather than primarily full-body baselines. Likewise, beyond ARCTIC, including widely used hand benchmarks such as HO3D and FreiHand would provide a more complete and convincing assessment of hand reconstruction performance.\n\n\nIn Figure 5, the qualitative hand reconstructions (especially rows 3–7) are noticeably weak; in fact, they appear worse than what one would expect from the HaMeR initialization alone. The authors should analyze why full-body joint training degrades hand quality (e.g., loss weighting, or token fusion design ? ) and provide ablations or training diagnostics to identify and explain these not-even-should-happened failure cases. \n\nThe paper allocates a relatively large portion of the main text to qualitative figures, some of which do not strongly reinforce the quantitative findings. While visual examples are valuable for demonstrating reconstruction quality, the extensive use of such figures gives the impression that the paper relies on visuals to fill space rather than to provide meaningful analysis."}, "questions": {"value": "I don’t have any major questions — I believe this paper is not yet ready for submission."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0cd3S1I0yZ", "forum": "t9BirzV6Hn", "replyto": "t9BirzV6Hn", "signatures": ["ICLR.cc/2026/Conference/Submission22206/Reviewer_W4kC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22206/Reviewer_W4kC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969767898, "cdate": 1761969767898, "tmdate": 1762942113206, "mdate": 1762942113206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BodhaHMR, a method for reconstructing 3D expressive humans with articulated hands from a single RGB image. Unlike prior approaches that treat body and hand estimation separately, BodhaHMR unifies them to produce consistent and detailed 3D meshes. BodhaHMR is trained on a combination of synthetic, controlled, and in-the-wild datasets: COCO-WholeBody, Human3.6M, SynthBody, SynthHand, using a mix of 2D/3D keypoint, pose, shape, and adversarial losses. BodhaHMR outperforms existing work on AGORA, ARCTIC, COCO dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Extract the hand separately because of its small resolution makes sense\n2. The proposed method outperforms recent baselines."}, "weaknesses": {"value": "1. The results show noticeable misalgnment between the estimated human mesh and the input image\n2. No ablation on the design choices and losses, such as fusion scheme, except for backbone.\n3. The loss in Eq. 4 seems outdated. Recent methods show better performance with learned priors, like https://dposer.github.io/\n4. Not clear why the proposed method outperform existing methods on separated hand and body benchmarks while it is essentially just trained a combined of hand and body datasets."}, "questions": {"value": "Please respond to the concerns in the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sDSM593e9D", "forum": "t9BirzV6Hn", "replyto": "t9BirzV6Hn", "signatures": ["ICLR.cc/2026/Conference/Submission22206/Reviewer_adMw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22206/Reviewer_adMw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22206/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762063765544, "cdate": 1762063765544, "tmdate": 1762942112973, "mdate": 1762942112973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}