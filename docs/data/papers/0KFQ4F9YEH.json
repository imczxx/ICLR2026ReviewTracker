{"id": "0KFQ4F9YEH", "number": 17639, "cdate": 1758278688637, "mdate": 1759897163165, "content": {"title": "LoC-Decomp: LLM Autoformalization via Logical Concept Decomposition and Iterative Feedback Correction", "abstract": "Automated formalization—the process of converting natural language mathematical statements into machine-verifiable formal code—plays a critical role in ensuring the reliability of mathematical reasoning generated by large language models (LLMs). Recent studies show that LLMs exhibit strong potential in automating this process, producing formal code for systems such as Lean4, Coq, and Isabelle. Despite prominent advances, existing LLM-based autoformalization methods remain limited: they lack the ability to provide reliable semantic consistency checks to ensure that the formal code accurately preserves the meaning of the original statement. Furthermore, such methods are unable to support iterative improvement through corrective feedback. To address these limitations, we propose Loc-Decomp, a novel framework that integrates an automatic semantic consistency checker and the Lean4 compiler to iteratively refine LLM-generated formalizations, ensuring both semantic consistency and syntactic correctness. Our approach introduces three key innovations: (1) A structured formalization template that decomposes complex formalization tasks into modular, foundational components, and systematically assembles them—like building blocks—into a complete formal expression. (2) A semantic self-checking mechanism based on a divide-conquer-merge strategy to detect subtle inconsistencies between the formalization and the original statement. (3) An iterative feedback-driven refinement loop that leverages both semantic and syntactic error signals to guide the LLM in progressively improving the formal output. By integrating these innovations, Loc-Decomp significantly enhances the accuracy of LLM-driven formalization, reduces reliance on human intervention, and moves closer to truly reliable automated reasoning. Extensive experiments on the MATH and miniF2F datasets demonstrate that our approach achieves a significantly higher formalization success rate compared to baseline methods and previous state-of-the-art (SOTA) approaches. On the miniF2F dataset, for instance, our method attains a success rate of 91.16%, substantially outperforming the previous SOTA result of 46.70%.", "tldr": "Loc-Decomp is a novel framework that enhances LLM-based autoformalization by integrating semantic consistency checks and iterative refinement, achieving a 91.16% success rate on the miniF2F dataset.", "keywords": ["Autoformalization", "Automated theorem proving", "Large language model"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/15b61b01f5eb2824f607514c0784ca29d3163ec4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces **Loc-Decomp**, a framework that improves automated formalization—the process of converting natural language math statements into formal, machine-verifiable code. It combines a **structured Lean4 template**, a **semantic consistency checker**, and an **iterative feedback loop** that jointly refine both meaning and syntax. Unlike previous methods, Loc-Decomp detects subtle semantic mismatches and uses compiler errors to iteratively correct them. Experiments on MATH and miniF2F datasets show major improvements, achieving up to **90% formalization accuracy**, far surpassing previous state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A **divide–conquer–merge–based semantic self-checking mechanism** is proposed to detect subtle inconsistencies between the formalization and the original statement, which aligns well with the motivation behind **Retrieval-Augmented Generation**.  \n- For the first time, it combines semantic inconsistency feedback with compiler error information to perform iterative refinement.  \n- The method is **simple and easy to understand**."}, "weaknesses": {"value": "- The experiments are too limited; please provide **comparative experiments on the ProofNet and Putnam datasets**.  \n- Please provide a **detailed ablation study** of **Logical Concept Decomposition** and **Iterative Feedback Correction**, explicitly isolating and analyzing their individual effects."}, "questions": {"value": "Please Refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SgXD3tSfYT", "forum": "0KFQ4F9YEH", "replyto": "0KFQ4F9YEH", "signatures": ["ICLR.cc/2026/Conference/Submission17639/Reviewer_2crV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17639/Reviewer_2crV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815771885, "cdate": 1761815771885, "tmdate": 1762927497531, "mdate": 1762927497531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LoC-DeCOMP, a framework for automated formalization of mathematical statements using off-the-shelf LLMs. The approach integrates a structured decomposition template for Lean4 code generation, a semantic self-checking mechanism (ASCC) leveraging divide-conquer-merge back-translation, and an iterative feedback-driven refinement loop employing both semantic and syntactic (compiler) error signals. Experimental evaluation on the MATH-500 and miniF2F datasets demonstrates substantial improvements over baseline and prior state-of-the-art methods. Furthermore, experiments on the human-verified MATH-Level5-50 subset show a notable 30 percentage point improvement over the baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tClarity and Presentation: The paper is exceptionally well-written and easy to follow. The proposed method, LoC-DeCOMP, and its evaluation are described with great clarity, which significantly aids in understanding the contribution.\n2.\tAccessibility and Ease of Use: A key strength of this work is that it proposes a training-free workflow. By designing a system that effectively orchestrates off-the-shelf LLMs, the framework is highly accessible and can be readily implemented without the need for costly model fine-tuning.\n3.\tDemonstrated Effectiveness: The method shows significant empirical success. The 30 percentage point improvement over the baseline on the human-verified MATH-Level5-50 dataset is impressive and underscores the practical utility and effectiveness of the proposed workflow."}, "weaknesses": {"value": "1.\tPotential Loss of Expressive Capabilities: The framework's reliance on a predefined template for formalization, as mentioned in Appendix A.4, is a central concern. This template-based approach may inherently restrict the expressive power of the Lean4 language. The paper currently lacks a thorough analysis of this trade-off. Specifically, there is no justification or evidence to suggest that the template is sufficient for formalizing mathematics beyond the scope of high-school-level problems. If the template's expressiveness is severely limited, it could significantly impact the generalizability and overall contribution of the proposed method.\n2.\tNarrow Scope of Evaluation: The empirical evaluation is confined to the MATH and miniF2F datasets, both of which primarily consist of high-school-level mathematics. The paper would be much stronger with experiments on more advanced or diverse mathematical domains (e.g., undergraduate-level abstract algebra or analysis). This weakness is particularly concerning when considered alongside the potential loss of expressiveness (Weakness 1). Without empirical evidence of its applicability to more complex mathematics, it is difficult to assess the method's general utility.\n3.\tMissing Comparison with Specialized Systems: The paper compares its framework against general-purpose LLMs but does not include comparisons with models or systems specifically designed or fine-tuned for formalization tasks. While the primary contribution is the workflow itself, which is well-demonstrated by comparing it against base LLMs, including a baseline from a specialized formalizer would provide a more complete picture of its performance."}, "questions": {"value": "1.\tCould you please elaborate on the extent to which the proposed template may limit the expressive capability of Lean4? A theoretical discussion on this trade-off, perhaps analyzing constructs that are difficult or impossible to represent, would greatly strengthen the paper.\n2.\tIf a theoretical analysis (Question 1) is difficult, could you provide some empirical evidence demonstrating the framework's applicability to mathematical domains beyond high school-level problems? \n3.\tWould it be possible to include an experimental comparison with a system specifically designed or fine-tuned for formal theorem proving? This would help benchmark the performance of your training-free workflow against alternative approaches."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GHzN41Jw6S", "forum": "0KFQ4F9YEH", "replyto": "0KFQ4F9YEH", "signatures": ["ICLR.cc/2026/Conference/Submission17639/Reviewer_DZ3h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17639/Reviewer_DZ3h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819792369, "cdate": 1761819792369, "tmdate": 1762927496974, "mdate": 1762927496974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a pipeline, Loc-Decomp, for autoformalization of math problems. It consists of four components. FormalTrans introduces the use of a structured formalization template, to decompose the statement into parts such as functions, types, constraints, and the problem statement. BackTrans translates the formal statement back into natural language. ASCC-R and CpC-R use a divide-and-conquer “LLM-as-a-judge” approach to verify semantic alignment and syntactic correctness, respectively, and leverage error feedback to iteratively improve accuracy. The method seems to achieve strong results on subsets of MATH-500 and miniF2F, namely MATH-Level5-50 and MATH-ASCC-Eval-150."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed FormalTrans converts autoformalization into a structured template. Using free variables as answer placeholders (instead of the more conventional “sorry” placeholders) is a novel idea.\n\n2. Figure 5 shows that ASCC’s divide-and-conquer plus majority-vote strategy for semantic alignment checking is effective and produces judgments that align better with human evaluation.\n\n3. The paper’s figures present the pipeline clearly."}, "weaknesses": {"value": "1. Lack of novelty. Much of the paper repeats prior work and reads more like an engineering integration than a substantive methodological advance; the novelty is therefore marginal. In particular, the idea of semantic consistency checking by decomposing the formalization code has been previously explored [1], and iterative feedback-based methods for semantic and syntactic correction have been investigated in prior work [2] and [3]. Using back-translation plus an “LLM-as-a-judge” for semantic checks is also a standard practice in this area [4]. The authors should provide explicit comparisons to demonstrate what is new in this paper.\n\n   \n\n2. Questionable evaluation methodology. The two baselines in Table 1 (SymEQ and Lean-workbook) use different datasets and evaluation criteria; reporting their absolute scores side-by-side with the proposed method is misleading and does not support a fair comparison of method superiority. Besides, miniF2F is primarily used as a benchmark for automated theorem proving rather than autoformalization. Using miniF2F for formalization evaluation is not a common practice and should be justified.\n\n\n\n3. Limited and potentially biased MATH-ASCC-Eval-150. The MATH-ASCC-Eval-150 split is very small, relies heavily on manual annotation, and depends on a single formalization model (DeepSeek V3). As a result, the evaluation of semantic-alignment judgment ability is limited: it is unclear whether the reported ability would generalize to formalizations produced by other models. The authors should provide variants of MATH-ASCC-Eval-150 derived by different formalization models or present an analysis of generalization across models; otherwise the practical value of this dataset is questionable.\n\n  \n\n4. Writing quality and presentation issues. Overall the manuscript would benefit from careful proofreading and stricter editing for consistent terminology and formatting.\n\n  - The manuscript uses many nonstandard abbreviations and inconsistent spellings, which make it difficult to follow and reduce readability.\n\n  - The term “autoformalization” appears with multiple spellings (“automated formalization” on Line 013, “auto formalization” on Line 108, “auto-formalization” on Line 117, “autoformalization” on Line 051). These inconsistencies are embarrassing and should be standardized.\n\n  - The paper mostly uses “Lean4,” but “Lean 4” appears on Line 219; the usage should be consistent.\n\n  - Typographical/spacing issues: missing space after commas on Line 062; missing space after periods on Line 110 and Line 351.\n\n[1] Zhang, J., Zhong, C., Xu, H., Li, Q., & Zhou, Y. (2025). *KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment* (No. arXiv:2507.08665). arXiv. https://doi.org/10.48550/arXiv.2507.08665\n\n[2] Wang, H., Unsal, M., Lin, X., Baksys, M., Liu, J., Santos, M. D., Sung, F., Vinyes, M., Ying, Z., Zhu, Z., Lu, J., Saxcé, H. de, Bailey, B., Song, C., Xiao, C., Zhang, D., Zhang, E., Pu, F., Zhu, H., … Li, J. (2025). *Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning* (No. arXiv:2504.11354). arXiv. https://doi.org/10.48550/arXiv.2504.11354\n\n[3] Liu, C., Shen, J., Xin, H., Liu, Z., Yuan, Y., Wang, H., Ju, W., Zheng, C., Yin, Y., Li, L., Zhang, M., & Liu, Q. (2023). *FIMO: A Challenge Formal Dataset for Automated Theorem Proving* (No. arXiv:2309.04295). arXiv. https://doi.org/10.48550/arXiv.2309.04295\n\n[4] Ying, H., Wu, Z., Geng, Y., Wang, J., Lin, D., & Chen, K. (2024). *Lean Workbook: A large-scale Lean problem set formalized from natural language math problems* (No. arXiv:2406.03847). arXiv. https://doi.org/10.48550/arXiv.2406.03847"}, "questions": {"value": "1. For miniF2F there is ground-truth formalization available. Given a dataset with ground-truth formal statements, BEq can evaluate formal-statement equivalence and reduce the need for manual annotation. Please explain the rationale for preferring LLM-as-a-judge over BEq in this setting.\n\n2. In the example in Figure 2, the auxiliary functions and constraints introduced appear to duplicate existing definitions in the standard math library (mathlib)—for instance, continuity is already defined in mathlib. Could this duplication create unnecessary obstacles for downstream automated theorem proving? Is this duplication an intentional design choice or a potential problem? How does LoC_Decomp avoid or mitigate such issues in practical formalization workflows?\n\n3. Line 203 states that instead of using the traditional automated theorem proving placeholder “sorry” as a solution placeholder, this work uses an input 's', a free variable representing the solution. This is not a standard practice and might harm the usefulness of the resulting autoformalization dataset for automated theorem proving. Can the authors provide further justification for avoiding “sorry” as the placeholder?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hCHKc73ab9", "forum": "0KFQ4F9YEH", "replyto": "0KFQ4F9YEH", "signatures": ["ICLR.cc/2026/Conference/Submission17639/Reviewer_8Wwq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17639/Reviewer_8Wwq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17639/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970726417, "cdate": 1761970726417, "tmdate": 1762927496376, "mdate": 1762927496376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}