{"id": "pabByp2986", "number": 14626, "cdate": 1758240486888, "mdate": 1759897358791, "content": {"title": "SpecEval: Evaluating Model Adherence to Behavior Specifications", "abstract": "Companies that develop foundation models often publish behavioral guidelines they pledge their models will follow, but it remains unclear if models actually do so as there has been no systematic audit of adherence to these guidelines. We propose a simple but imperative baseline: at minimum, a foundation model should consistently satisfy its developer's own behavioral specifications when judged by the developer's own evaluator models. Thus our central focus is on __three-way consistency__ between a provider's specification, the provider's model outputs, and adherence scores from the provider model as a judge; an extension of prior two-way generator-validator consistency. We introduce an automated framework that audits models against their providers' specifications by (i) parsing statements that delineate desired behaviors, (ii) generating targeted prompts to elicit the aforementioned behaviors, and (iii) using the responses as inputs to models to judge adherence. We apply our framework to 16 models from six developers across 100+ behavioral statements, finding three-way consistency  gaps of up to 20\\% across providers.", "tldr": "We present an automated framework that audits foundation models against their own published specifications, introducing a three-way consistency check between specification, model outputs, and provider judges.", "keywords": ["model compliance", "evals", "alignment auditing", "LM as a judge"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab40d4c4cdf5e200c8c3d76ab0fdf1d674fe6c9a.pdf", "supplementary_material": "/attachment/b8a7e4a4639488f94b0493f4a52bea78a6315bb2.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces SpecEval, a framework designed to evaluate whether LLMs from different providers actually follow the model specifications (or \"constitutions\") that those providers claim. The authors propose a novel three-way consistency metric to assess adherence to these specifications. Experimental results show that Claude best follows its stated specifications, while Google’s models do not always do so."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important and timely question: whether LLMs truly follow the constitutions defined by their providers. The motivation is therefore strong and well-justified.\n\n- The writing is clear and well-structured. The formulation of the evaluation task is easy to follow, and the use of illustrative examples and figures makes the paper highly accessible.\n\n- Although techniques such as self-evolving prompt synthesis and LLM-as-a-judge are not new, their use here feels intuitive and appropriate for this particular task."}, "weaknesses": {"value": "- My main concern lies with the three-way consistency metric. Since the paper’s stated goal is to evaluate whether an LLM follows its specification, the evaluation should focus on whether the model produces responses that violate that specification.\nHowever, the three-way consistency score depends not only on this factor but also on the model’s ability as a Judge to correctly identify violations. In my view, this second aspect \"judging\" is not directly aligned with the paper’s main objective, which concerns generation rather than discrimination.\nTherefore, while three-way consistency is an interesting and novel metric, it may not be the most suitable one for assessing specification adherence. Traditional metrics such as violation rate or refusal rate might better capture this goal. I see three-way consistency more as a measure of how well a model’s generation and judgment behaviors are aligned, rather than as a pure indicator of specification compliance.\n\n- I'm also somewhat confused about the relationship between TestMaker, Judge, and Candidate.\nMy understanding is that the Judge and Candidate are the same model, but what about the TestMaker? Is it also the same as the Candidate, or is it fixed to GPT-4.1?\nThis seems inconsistent: the Introduction states, “Our primary focus is on the slice of data using the same model from a provider as both Candidate and Judge against the provider’s specification.” However, the Experimental Setup section says, “We run the synthetic data generation pipeline described in Section 3.2 to generate the prompts for the specifications above, with the TestMaker, Judge, and Candidate fixed to be GPT-4.1.”\nThese two statements appear contradictory. Does this mean that during data generation there is a separate set of TestMaker, Judge, and Candidate models? If so, what role does the “Candidate” mentioned in Section 3.2 actually play (since only Judge and TestMaker are described)? If not, then it’s unclear what model is used for TestMaker throughout the paper. Perhaps I am missing something, but clarification would be very helpful."}, "questions": {"value": "1. Why is three-way consistency considered a sufficient metric for evaluating specification adherence? Have the authors considered using simpler metrics such as violation rate (or compliance rate)? If not, what is the rationale for preferring the proposed metric?\n\n2. Which model is used as the TestMaker? Is it the same as the Candidate or fixed to GPT-4.1 throughout the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AimwItUqXP", "forum": "pabByp2986", "replyto": "pabByp2986", "signatures": ["ICLR.cc/2026/Conference/Submission14626/Reviewer_YngW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14626/Reviewer_YngW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828140456, "cdate": 1761828140456, "tmdate": 1762925002938, "mdate": 1762925002938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SpecEval, an automated framework for auditing whether foundation models adhere to the intended behavioral specifications. The authors propose evaluating \"three-way consistency\" between a provider's specification document, their model's outputs, and adherence scores from their own model used as a judge. The framework operates in three stages: (1) parsing behavioral statements from specification documents, (2) using a TestMaker model to generate diverse, targeted prompts that test each statement, and (3) employing Judge models to evaluate response adherence. The authors apply their framework to audit 16 models from six major providers (OpenAI, Anthropic, Google, Meta, DeepSeek, and Alibaba) across 100+ behavioral statements, generating a dataset of 2,360 prompts with model responses and judgments. Key findings reveal significant consistency gaps across providers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Automatically auditing LLMs against the pre-defined specifications is an important topic.\n+ Overall, the paper's writing is clear and understandable"}, "weaknesses": {"value": "**Overly reliance on LLMs in the pipeline for quality control without verifiable standards, making the quality and reliability of evaluation rather noisy.**\n\nThe proposed workflow has a design weakness due to its overly dependence on LLM-generated components throughout the entire evaluation pipeline, without establishing robust verification standards, which undermines the reliability and validity of the audit results. Specifically, the pipeline uses LLMs in three key stages: (1) a TestMaker model (GPT-4.1) generates test scenarios and prompts, (2) Candidate models produce responses, and (3) Judge models evaluate adherence using Likert scales that are later binarized. A fundamental concern is that it remains unclear whether the automatically generated test prompts actually reflect or rigorously test the intended behavioral specifications. Guaranteeing alignment between abstract behavioral statements (e.g., \"be rationally optimistic\") and concrete test prompts is inherently difficult because: (1) behavioral specifications are often vague, subjective, and open to interpretation, (2) there is no ground truth to validate whether a generated prompt meaningfully tests the specification versus testing some related but distinct concept. While the authors mention \"manually check and curate a small, randomly sampled subset of all prompts to ensure prompt quality and realism\", they provide limited insights into the overall quality. \n\n**Using the same model as the generator and judge introduces further noise in the valuation due to the varied model capabilities in \"judging\"**\n\nThe paper's core methodological choice to use the same model as both Candidate (generator) and Judge to measure \"three-way consistency\" introduces biases that undermine the validity of the evaluation. While the authors justify this approach as measuring whether \"a foundation model should consistently satisfy its developer's own behavioral specifications when judged by the developer's own evaluator models\", this design conflates model adherence with model judging capability, making it unclear whether low scores reflect genuine specification violations or simply poor judging ability. A more rigorous approach would require using unified judge models with validated judging reliability to evaluate all candidate models. Without such controls, the reported three-way consistency scores may primarily reflect which providers have trained models that are simultaneously good at judging and happen to favor their own outputs, rather than which models truly adhere to their specifications.\n\n**Limited Generalizability Due to Unavailable Specifications and Unfair Cross-Provider Evaluation**\n\nThe evaluation framework has limited generalizability as it can only audit the minority of providers who publicly release behavioral specifications, and applying one provider's specification to judge another provider's models is methodologically problematic. The paper evaluates 16 models from six providers, but can only measure three-way consistency for the three providers (OpenAI, Anthropic, Google) with public specifications, while models from Meta, DeepSeek, and Alibaba are evaluated against other companies' standards. This cross-provider evaluation is inherently unfair because different organizations train their models with distinct objectives, values, and use cases in mind. This inconsistency undermines the utility of auditing models without their own specifications."}, "questions": {"value": "- Can you provide stronger evidence that your automatically generated test prompts actually measure adherence to the intended behavioral specifications rather than related but distinct concepts?\n- How can you separate a model's true adherence to specifications from its capability as a judge, given that your three-way consistency metric conflates these two factors?\n- What is the intended use case for your framework when evaluating models from providers without public specifications, and how should users interpret such results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GYAVuhPFfC", "forum": "pabByp2986", "replyto": "pabByp2986", "signatures": ["ICLR.cc/2026/Conference/Submission14626/Reviewer_7RhD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14626/Reviewer_7RhD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943456277, "cdate": 1761943456277, "tmdate": 1762925002363, "mdate": 1762925002363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The current paper promotes one of the first automated auditing frameworks for model adherence to behavior specifications. They provide an extensive evaluation of 16 models across 6 developers with over 100 behavioral guidelines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Leveraging existing model behavior specifications from existing model developers is a novel technique. It is also grounded in practice, since such specifications are openly available to the public. It is also encouraging that the authors conducted a deep dive into documentation, technical reports, and more, despite this information not always being readily available with model releases.\n2. Good variety models in the experimental setup, in terms of open vs closed source. \n3. It is encouraging to read about the potential societal benefits by providing such analyses to regulators, civil-society groups, and end users. More work in the future and potential case studies with these groups would be interesting and valuable.\n4. Good quantitative and qualitative analysis of results."}, "weaknesses": {"value": "1. It would have been interesting to include one or two performant smaller models (i.e., 20B params or fewer), to serve as more baselines for comparison with the presented results."}, "questions": {"value": "What is the reason for fixing the teacher model to be gpt-4.1? Especially given prior work that shows how models prefer their own family's generations and other self-preference bias issues (e.g., https://aclanthology.org/2024.acl-long.511/, https://arxiv.org/abs/2404.13076, https://aclanthology.org/2024.acl-long.826/) it would have been good to either use both an open and closed source model in conjunction, or use a variety of models in order to avoid the potential bias from only having a single teacher model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NH0eE4QcMf", "forum": "pabByp2986", "replyto": "pabByp2986", "signatures": ["ICLR.cc/2026/Conference/Submission14626/Reviewer_g9U6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14626/Reviewer_g9U6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958757545, "cdate": 1761958757545, "tmdate": 1762925001982, "mdate": 1762925001982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an automated auditing framework that measures how well foundation models adhere to their providers’ own behavioral specifications. The core metric is three-way  consistency among: the provider’s specification, the provider’s model outputs, and the provider’s\nmodel acting as the judge. The pipeline parses specification statements, synthesizes targeted  prompts to probe each statement, and uses LM-as-a-judge to score adherence"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The work conducts a broad empirical study across multiple providers and commercial models, producing insights on specification adherence"}, "weaknesses": {"value": "1. The core metric (three-way consistency) is introduced without enough motivation for why this metric represents behavioral soundness.\n2. Score differences are presented, but the story about what these differences truly mean is thin.\n3. Three-way consistency uses a provider’s own model as both Candidate and Judge, inviting self-grading bias. Although cross-judge comparisons and a small human study are reported, human anchoring is still limited"}, "questions": {"value": "1. When the same model plays Candidate and Judge, how do you avoid circular  confirmation? Where is the quantitative human anchoring to show validity?\n2. You collect Likert/confidence signals during generation. Why reduce the final result to binary? Does this hide uncertainty and borderline cases?\n3. How does SpecEval compare against simpler alternatives such as direct human scoring or random prompt sampling aligned to each spec?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "A7v8ydtXpn", "forum": "pabByp2986", "replyto": "pabByp2986", "signatures": ["ICLR.cc/2026/Conference/Submission14626/Reviewer_brv4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14626/Reviewer_brv4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762041708763, "cdate": 1762041708763, "tmdate": 1762925001488, "mdate": 1762925001488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}