{"id": "O2WvMkJbws", "number": 5960, "cdate": 1757948637229, "mdate": 1759897942278, "content": {"title": "ES-dLLM: Efficient Inference for Diffusion Large Language Models by Early-Skipping", "abstract": "Diffusion large language models (dLLMs) are emerging as a promising alternative to autoregressive models (ARMs) due to their ability to capture bidirectional context and the potential for parallel generation. Despite the advantages, dLLM generation remains time-consuming, as it processes the full context for inference at each iteration. In this work, we analyze the generation characteristics of dLLMs and observe that intermediate states (e.g., key, value, and hidden states) change only subtly across iterations. Leveraging this insight, we propose ES-dLLM, a training-free inference acceleration framework for dLLM that reduces computation by skipping tokens with low importance scores in earlier layers of the model. Importance is estimated based on intermediate tensor variation and confidence scores from previous iterations. Experiments on LLaDA-8B and Dream-7B show that ES-dLLM achieves throughput of up to 226.57 and 308.51 tokens per second (TPS) on an NVIDIA H200 GPU, delivering 5.6$\\times$ to 16.8$\\times$ speedup over the original implementation and up to 1.85$\\times$ over the state-of-the-art caching method, while preserving generation quality.", "tldr": "", "keywords": ["Diffusion Large Language Model", "Inference Acceleration", "KV Caching"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7695ee3450016c59bc411acf221b1f6deef1c0de.pdf", "supplementary_material": "/attachment/71aec6ab94a5be41ff60513594fb0d4b99072be0.zip"}, "replies": [{"content": {"summary": {"value": "To solve the high inference latency of diffusion-based LLMs (dLLMs), caused by full-sequence computation in each iteration, the paper proposes ES-dLLM, a training-free acceleration framework. It is built on two key observations: (1) Intermediate states (hidden/key/value tensors) of most tokens have subtle variations across consecutive iterations; (2) Confidence scores of token positions change minimally. ES-dLLM implements two core components: Importance Score Estimation and Partial Cache Update & Early Skip. Experiments on LLaDA-8B/Dream-7B (5 benchmarks: GSM8K/MATH/BBH/HumanEval/MBPP)  show ES-dLLM achieves speedup over vanilla dLLMs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.  Unlike heuristic acceleration methods, ES-dLLM’s logic is rooted in quantitative analysis of dLLM generation characteristics. For example, PCA and L1-norm metrics confirm hidden state variation follows an exponential distribution (concentrated near 0) across layers 10–30 of LLaDA-8B; log-scale distribution of confidence scores shows 80% of positions have variation <0.05 after iteration 50. These observations directly validate the \"early-skipping\" rationale, low-variation tokens contribute little to final outputs, justifying computational reduction.\n2. ES-dLLM can be seamlessly integrated with confidence-aware parallel decoding (PD), further boosting speedup."}, "weaknesses": {"value": "1. ES-dLLM is only tested on 7B/8B models, for 13B/70B dLLMs, the computational overhead of importance score calculation (layer-wise L1-norm of hidden states) may scale sharply with model size (more layers/heads increase tensor comparison cost), offsetting speed gains. For sequences >512 tokens (e.g., 1k legal documents), fixed skip positions (r₄=r₈) are suboptimal: deep layers (e.g., layer 16+) have higher tensor variation, but ES-dLLM does not adjust skipping layers dynamically, which may lead to under-skipping in deep layers or over-skipping in early layers. It needs some discussions here.\n2. Theoretical 60% FLOP reduction only translates to 1.85× speedup over DualCache in practice, as LLM inference shifts to memory-bound (model weights/KV cache access unchanged). ES-dLLM does not integrate system-level optimizations  to unlock full speedup potential. For example, on Dream-7B/HumanEval, ES-dLLM achieves 308.51 TPS, but memory bandwidth constraints (H200’s 3.35TB/s) limit it to 40% of theoretical throughput (771 TPS)."}, "questions": {"value": "1. How does ES-dLLM perform on 13B/70B dLLMs (e.g., Dream-13B)? Does the overhead of importance score calculation (L1-norm of hidden states) scale linearly with the number of layers/heads? Can lightweight optimizations (e.g., sparse hidden state comparison, only checking top-20% varying dimensions) reduce this overhead? Please provide TPS/accuracy data for Dream-13B across GSM8K/MATH and analyze layer-wise computation time.\n2. For 1k/2k-token sequences (e.g., LongBench benchmarks), how to adjust skip positions and ratios dynamically? Does a variation-aware skipping strategy maintain accuracy while increasing speedup? Please compare fixed vs. dynamic skipping on 1k-token PubMed abstracts (generation length 512) in terms of FLOP reduction and performance on benchmarks.\n3. How to adjust prompt/block cache refresh periods dynamically based on sequence type? For example, legal documents (long sentences) may need shorter prompt refresh periods (16 vs. 64) to retain context, while short math problems (GSM8K) can use longer periods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G59M6ERkm3", "forum": "O2WvMkJbws", "replyto": "O2WvMkJbws", "signatures": ["ICLR.cc/2026/Conference/Submission5960/Reviewer_6Gok"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5960/Reviewer_6Gok"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760952582313, "cdate": 1760952582313, "tmdate": 1762918374815, "mdate": 1762918374815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ES-dLLM for efficient inference for diffusion LLMs. The motivation is clear for slow DLLM inference, and similarly for activations during different diffusion steps. The idea of skipping low-importance tokens in early layers is effective. Results show up to 16.8× speedup with negligible accuracy drop.\n\nThe paper is well organized, experimental results are extensive, and the method is training-free, which leads to weak accept."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The motivation is strong that the hidden-state variation statistics convincingly demonstrate redundancy. We believe it has been observed in diffusion video generation models and image generation models [1], but it is new in DLLM. This paper clearly demonstrates its motivation. I like that.\n\nCompromising generation quality is important for real-world applications, which helps a lot for this paper.\n\nWe believe training-free is essential for effective inference, which this paper achieves.\n\n\n[1] Silveria A, Govande S V, Fu D Y. Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas[C]//ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models. 2025."}, "weaknesses": {"value": "Novelty risk: This method appears to be an extension of the original implementation and DualCache. Fortunately, DualCache is not a sparsity work, which distinguishes the two works, but we hope to see more discussion on the degree of differentiation. Also, as we claimed in the Strengths, a similar idea has been proposed in the traditional diffusion model. \n\nComparison and related work: The paper compares only with the original method and DualCache, limiting the scope of the comparison. However, this paper only talks about PRELIMINARY with DLLM, DualCache, and does not talk about related work (I can't see any section on related work) using a lossy method to accelerate DLLM. No related work and no comparison with related work are the main problems of this paper."}, "questions": {"value": "How ES-dLLM perform on MMLU?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rZr5kE7wZ7", "forum": "O2WvMkJbws", "replyto": "O2WvMkJbws", "signatures": ["ICLR.cc/2026/Conference/Submission5960/Reviewer_dw3w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5960/Reviewer_dw3w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533016686, "cdate": 1761533016686, "tmdate": 1762918374447, "mdate": 1762918374447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Diffusion large language models (dLLMs) have bidirectional context and parallel generation advantages but suffer from high inference latency due to full-sequence computation per iteration. ES-dLLM, a training-free framework, addresses this by leveraging the observation that most tokens’ intermediate states (hidden states, KV tensors) and confidence scores vary slightly across iterations. It estimates token importance via prior confidence and normalized tensor variations, skips low-importance tokens in early layers, updates caches only for selected tokens (with periodic refreshes to avoid errors), and achieves 5.6×–16.8× speedup over original dLLMs (LLaDA-8B, Dream-7B on NVIDIA H200) and 1.2×–1.85× over DualCache, while maintaining generation quality; it also integrates with parallel decoding for further speedups."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ **Training-free design for easy deployment**: ES-dLLM requires no model fine-tuning or structural modification. It only optimizes the inference process through early-skipping and partial cache updates, which can be directly integrated into open-source dLLMs (e.g., LLaDA, Dream) without reconstructing the underlying framework."}, "weaknesses": {"value": "+ **Fixed heuristic for importance score, lacking adaptability**：The importance score of ES-dLLM relies on a fixed linear weighting of prior confidence and intermediate tensor variation (default α=0.5), without considering task-specific differences. Ablation experiments show that using only tensor variation (α=0) performs better on the MATH dataset than the default α=0.5, while relying solely on confidence (α=1) leads to noticeable quality degradation. However, ES-dLLM does not propose a dynamic adjustment mechanism (e.g., adapting α based on real-time token variation or task type), which may cause misjudgment of early-skipping in complex scenarios.  \n+ **Missing validation in extreme scenarios**：\n  + **Ultra-long sequence generation**：The experiments only cover sequence lengths up to 512 tokens (HumanEval, MBPP) and do not test ultra-long sequences (>1K tokens). It remains unproven whether ES-dLLM’s cache refresh frequency and accumulated memory overhead (e.g., hidden state cache) can be controlled in longer sequences. \n  - **Comparison with more SOTA methods**：The baseline only includes the original dLLM implementation and DualCache, without comparing with cross-paradigm acceleration schemes (e.g., D2Cache for dLLMs, ReSA for sparse attention), making it impossible to fully reflect ES-dLLM’s relative advantages in the broader dLLM acceleration field.  \n+ **Insufficient analysis of the rationality of early-skipping layer selection**：ES-dLLM only tests early-skipping at \"1/8 and 1/4 of all layers\" (e.g., layers 4 and 8 for LLaDA) but does not explain why these layers are optimal. Although ablation experiments involve different layer configurations (e.g., layer 16, layer 0), they fail to quantify the trade-off between \"layer position, efficiency, and quality\" or provide general guidelines for layer selection, which is unfavorable for adapting ES-dLLM to other dLLMs with different depths.  \n+ **Incomplete details on memory overhead**：While the paper mentions that memory overhead is controllable (≤644MB per sample for LLaDA-8B), it does not report the memory growth curve under different sequence lengths or compare memory usage with DualCache. This makes it impossible to evaluate ES-dLLM’s applicability in memory-constrained scenarios (e.g., edge devices with limited VRAM).  \n+ **Unverified adaptability to ultra-large dLLMs**：The experiments are only conducted on mid-sized models (LLaDA-8B, Dream-7B) and do not involve ultra-large dLLMs (70B+ parameters). The dynamics of KV caches and intermediate states in ultra-large models may differ significantly from mid-sized ones, and it is unclear whether ES-dLLM’s early-skipping strategy can maintain efficiency and quality in such models.\n+ **Rough graphical and textual expression**: The paper’s data visualization and textual description of figures lack clarity and detail, affecting result interpretability."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rSz5O2JBeR", "forum": "O2WvMkJbws", "replyto": "O2WvMkJbws", "signatures": ["ICLR.cc/2026/Conference/Submission5960/Reviewer_id9o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5960/Reviewer_id9o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989961202, "cdate": 1761989961202, "tmdate": 1762918373905, "mdate": 1762918373905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed an efficient inference technique for diffusion LLM. This method leverage intermediate tensor variation and confidence\nscores to determine which token to skip. Experiments show significant speedup while maintaining the task performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This work propose a method well-substantiated by empirical findings.\n- Consistent and significant speedup across all the datasets tested."}, "weaknesses": {"value": "- The experiment session only compared to two other methods. Are there any other methods addressing the same problem?"}, "questions": {"value": "- Roughly how many tokens are generated in each dataset? Do you know why some datasets show more speedup while others show less?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "N0GAEowAmO", "forum": "O2WvMkJbws", "replyto": "O2WvMkJbws", "signatures": ["ICLR.cc/2026/Conference/Submission5960/Reviewer_ybg1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5960/Reviewer_ybg1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5960/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762247218200, "cdate": 1762247218200, "tmdate": 1762918373465, "mdate": 1762918373465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}