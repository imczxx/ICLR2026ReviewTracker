{"id": "xPBsWooORO", "number": 5430, "cdate": 1757908644337, "mdate": 1759897975918, "content": {"title": "ActDR: Action Difference Reasoning via Keypoint Guided Tree Search", "abstract": "Analyzing fine-grained differences in skilled activities, such as sports or surgery, poses a significant challenge for computer vision, demanding both precise action understanding and domain-specific reasoning. While prior work has made progress in evaluating individual performance, existing methods fall short in comparing two similar actions (e.g., penalty kick in soccer) conducted by different performers and explaining \\textit{how} their actions differ. To address this gap, we introduce \\textbf{Action Difference Reasoning (ADR)}, a novel task that jointly provides \\textit{quantitative} performance scores and \\textit{qualitative} explanations of inter-performer differences, enabling actionable feedback for improvement. To support this task, we construct the ADR dataset, built upon Ego-Exo4D dataset, comprising paired videos annotated with both performance scores and natural language descriptions of action differences. We further propose \\textbf{KEPT}, a \\textit{\\textbf{ke}y\\textbf{p}oint guided \\textbf{t}ree search} framework that explicitly models the reasoning process behind performance differences by capturing fine-grained kinematic cues. Experiments on the ADR dataset show that KEPT significantly outperforms existing baselines, including large vision-language models, on both score prediction and action difference explanation. Moreover, our framework generalizes effectively to traditional Action Quality Assessment (AQA) settings, surpassing state-of-the-art approaches on benchmarks including JIGSAWS and FitnessAQA. Code, model and dataset will be released after the review process.", "tldr": "We introduce a novel task that jointly provides quantitative and qualitative evaluations on athlete performance and propose  a keypoint-guided Monte Carlo Tree Search framework to model the reasoning process behind performance differences.", "keywords": ["action difference reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ccf71f96b66d5b9d00272d7f58ba951ecf72e21e.pdf", "supplementary_material": "/attachment/fbc07583474da25662a3fc0b7967f1f51d37e823.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a new task, Action Difference Reasoning (ADR): given two videos depicting the same action, the model must output both a quantitative action-quality score and a qualitative, textual explanation of the differences. The authors construct a new ADR dataset (sourced from exocentric videos in Ego-Exo4D) and propose a keypoint-guided tree-search framework (KEPT). They claim state-of-the-art performance on both scoring and difference description, with additional generalization to standard AQA benchmarks (JIGSAWS, FitnessAQA)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Idea of action differencing and reasoning is useful as shown by previous work.\n\n- This work outperforms some baselines."}, "weaknesses": {"value": "- Limited novelty. Pairwise comparison has been widely used in action assessment field, for example: [Refs 1,2,3 below]. This paper applies it to a different problem, which is marginal novelty. Furthermore, the architecture is very similar to [Ref 2 below]. Paper also conveniently does not discuss any of these work and comparisons.\n\n- There have been many work in action quality assessment which offer qualitative assessment, for example, [Refs 4,5,6 below] to name a few.\n\n- Paper does not discuss any related work. Moreover, all the references are mostly after year 2023.   \n\n- Tree search clarity. The method builds a limited-depth keypoint tree with joint angle/velocity/position features, uses cross-modal attention with the video to score each leaf path, and selects the highest-scoring path. Despite the terminology, this is closer to scoring and selecting from a fixed candidate set than to full MCTS; key components (e.g., UCT, rollouts, backpropagation of values), policy priors, and complexity analysis are missing. The paper should provide a bona fide search algorithm and details.\n\n- Potential same-source bias. Ground-truth difference descriptions are generated by Gemini, while training/evaluation also rely on LLM semantic representations. The paper should introduce human-written references or task-based evaluation (e.g., “choose the better performer based on the description”) as primary metrics, or at least include sensitivity analyses across different LLM encoders/evaluators. Moreover, only 50 samples are checked and unreliably concluded that dataset is reliable.\n\n- Fairness of closed-source VLM baselines. Clearly specify inference settings for proprietary LVLMs (temperature, context length, frame sampling, prompts, and cost constraints), and report zero-shot/few-shot/fine-tuning configurations for both open- and closed-source models to ensure fair longitudinal comparisons.\n\n- Reproducibility and resources. Provide complete training details (optimizer, learning rate/schedule, batch size, sampling, input resolution), the keypoint extractor and 3D reconstruction pipeline, the effect of clip duration/frame count, and inference speed/compute cost.\n\n- Other minor issues:\n\nLine 165, Figure 1 should be Table 1.\n\nLine 447, MCTS should be KEPT or KGT.\n\n\nReferences:\n\n[1] Jain, Hiteshi, Gaurav Harit, and Avinash Sharma. \"Action quality assessment using siamese network-based deep metric learning.\" IEEE Transactions on Circuits and Systems for Video Technology 31.6 (2020): 2260-2273.\n\n[2] Yu, Xumin, et al. \"Group-aware contrastive regression for action quality assessment.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n[3] Doughty, Hazel, Dima Damen, and Walterio Mayol-Cuevas. \"Who's better? who's best? pairwise deep ranking for skill determination.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n\n[4] Parmar, Paritosh, and Brendan Tran Morris. \"What and how well you performed? a multitask learning approach to action quality assessment.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n\n[5] Du, Zexing, et al. \"Learning Semantics-Guided Representations for Scoring Figure Skating.\" TMM 2024.\n\n[6] Zhang, Shiyi, et al. \"Narrative action evaluation with prompt-guided multimodal interaction.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."}, "questions": {"value": "Please see weaknesses and may consider them as questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wVJqoK38tE", "forum": "xPBsWooORO", "replyto": "xPBsWooORO", "signatures": ["ICLR.cc/2026/Conference/Submission5430/Reviewer_1Eem"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5430/Reviewer_1Eem"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742914277, "cdate": 1761742914277, "tmdate": 1762918058059, "mdate": 1762918058059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new computer vision task called Action Difference Reasoning (ADR). The goal of ADR is to not only assign quantitative performance scores to two similar actions performed by different individuals but also to generate a qualitative, natural language explanation of the key differences between them. The authors propose a framework named KEPT (Keypoint-guided Tree Search), which models the sequential visual reasoning process of a human expert as a search through a tree of possible keypoint-based observations. The framework consists of three main components: a keypoint-guided tree search to find the most salient reasoning path, an expert knowledge alignment module that uses textual expert commentary to guide the model, and an action difference reasoning module that generates both scores and textual descriptions. The authors constructed a new dataset, also called ADR, derived from the Ego-Exo4D dataset, which includes paired videos from sports like basketball, soccer, and rock climbing."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe authors introduce a new task named Action Difference Reasoning (ADR) and it seems to help machines provide more reasonable feedback.\n2.\tThe authors construct a new dataset named ADR which may contribute to the community."}, "weaknesses": {"value": "1.\tLack of important comparative experiments. The author claims that the proposed method, KEPT, can be generalized to the AQA (Action Quality Assessment) settings and surpasses the state-of-the-art methods. However, the author did not conduct experiments on widely used AQA datasets in recent years (e.g., MTL-AQA[1], FineDiving[2]), and also lacks comparison with the latest methods since 2025. This reduces the credibility of the method.\n\n[1] Parmar, P., & Morris, B. T. (2019). What and how well you performed? a multitask learning approach to action quality assessment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 304-313).\n\n[2] Xu, J., Rao, Y., Yu, X., Chen, G., Zhou, J., & Lu, J. (2022). Finediving: A fine-grained dataset for procedure-aware action quality assessment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 2949-2958).\n\n2.\tRegarding the quality control of the dataset. Although the authors used Gemini to generate the action difference descriptions and conducted a sampling-based check, the reported consistency of only 85% with the ground-truth score differentials is concerning. This may lead to a low-quality dataset, potentially introducing significant noise that could affect the validity of the results.\n3.\tThe presentation of the tables requires improvement. Some tables are scaled too small to be legible, particularly Tables 3 and 5. There is also a noticeable inconsistency in the scaling used for different tables, which negatively impacts the overall layout and readability of the manuscript.\n4.\tThe figures in this paper are complex and cluttered, which makes them difficult to understand. For example, Figures 3 and 4 are particularly hard to interpret.\n5.\tTypo. Gmini in Table 2, Our in Table2."}, "questions": {"value": "1.\tComputational Cost. The paper utilizes a graph neural network (GNN) for Action Score Prediction, which may introduce more computational overhead and inference latency. Have the authors conducted any statistical analysis on this?\n2.\tSearch mechanics. Do the authors use MCTS or beam search in Keypoint Guided Tree Search? Please detail selection if MCTS is indeed used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OgTCGirdHj", "forum": "xPBsWooORO", "replyto": "xPBsWooORO", "signatures": ["ICLR.cc/2026/Conference/Submission5430/Reviewer_nSwa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5430/Reviewer_nSwa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914167084, "cdate": 1761914167084, "tmdate": 1762918057770, "mdate": 1762918057770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the Action Difference Reasoning (ADR) task, which aims to compare two videos of similar actions to generate both performance scores and natural language descriptions of their differences. To address ADR, the authors propose KEPT, a framework that models expert-like reasoning by combining keypoint-based motion analysis with tree search to capture fine-grained kinematic cues. They introduce a new dataset, also called ADR, built upon the Ego-Exo4D dataset, with annotated video pairs from sports like basketball and soccer."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper overall is easy to follow."}, "weaknesses": {"value": "- My main concern is the value of the proposed tasks as well as the proposed solution compared to a simple cascade system. It is natural and well-motivated to want a model that can both produce accurate AQA score as well as provide a detailed explanation in natural language. However, I find the AQA performance of the proposed model is largely behind AQA models proposed in 2021. On JIGSAWS dataset, CoRe [r1] proposed in 2021 can achieve over 80%ρ while recent work like RICA² [r2] and MVLA [r3] can achieve ~90%ρ  which largely outperform the 67% result presented in the paper. So I think it is easy to outperform the proposed method by a cascade system with a 2021 model and any recent MLLM for producing explanations.\n\n[r1] Group‑aware Contrastive Regression for Action Quality Assessment, ICCV 2021\n\n[r2] RICA²: Rubric‑Informed, Calibrated Assessment of Actions, ECCV 2024\n\n[r3] Vision‑Language Action Knowledge Learning for Semantic‑Aware AQA, ECCV 2024"}, "questions": {"value": "Please refer to my comments above. Overall, I think the authors fail to show the value the proposed task and model compared to a simple combination of two sub-tasks. I think the quality of the paper is clearly below the bar of ICLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "92Hfxg9U4B", "forum": "xPBsWooORO", "replyto": "xPBsWooORO", "signatures": ["ICLR.cc/2026/Conference/Submission5430/Reviewer_dEjZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5430/Reviewer_dEjZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995892612, "cdate": 1761995892612, "tmdate": 1762918057507, "mdate": 1762918057507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses fine-grained differences in skilled activities like sports or surgery, introducing the Action Difference Reasoning (ADR) task and dataset based on Ego-Exo4D for quantitative scores and qualitative explanations of inter-performer differences. It proposes KEPT, a keypoint-guided tree search framework that captures kinematic cues, outperforming baselines on ADR and generalizing to traditional Action Quality Assessment (AQA) benchmarks like JIGSAWS and FitnessAQA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- I agree with the authors' perspective that generating action difference scores alone is insufficient; it is also essential to produce action reasoning explanations.\n- The paper is clearly presented with careful ablation studies."}, "weaknesses": {"value": "- The proposed task is not novel and has been widely explored in recent years. For instance, works [1-2] have proposed methods to address this problem. The authors did not cite these papers and lack detailed comparisons with these methods.\n- The proposed ADR dataset is essentially extracted from the existing Ego-Exo4D dataset with specific filtering and annotation. Thus, the claimed contribution of \"introducing a new dataset\" is not entirely convincing; it can be regarded more as an optimized version of an existing dataset.\n- There is potentially significant noise in the data. The authors themselves noted that expert commentary is noisy. However, there is no human verification or cleaning beyond summarization with LLMs, which could introduce even more noise.\n- Additionally, since there is no human verification of the ground truth annotations, one approach to validate the data could be to have typical humans (or ideally domain experts) perform the task and use their results as baselines. This would confirm that the data is clean and that humans can achieve strong performance on it. Such baselines are not provided, raising the possibility that the ground truth data is so noisy that the upper bound of the metrics is constrained by this noise.\n\n[1]: Li Y M, Wang A L, Lin K Y, et al. TechCoach: Towards Technical-Point-Aware Descriptive Action Coaching[J]. arXiv preprint arXiv:2411.17130, 2024.\n\n[2]: Ashutosh K, Nagarajan T, Pavlakos G, et al. ExpertAF: Expert actionable feedback from video[C]//Proceedings of the Computer Vision and Pattern Recognition Conference. 2025: 13582-13594."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4m7eXnEP3g", "forum": "xPBsWooORO", "replyto": "xPBsWooORO", "signatures": ["ICLR.cc/2026/Conference/Submission5430/Reviewer_y4XT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5430/Reviewer_y4XT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762621296081, "cdate": 1762621296081, "tmdate": 1762918057138, "mdate": 1762918057138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}