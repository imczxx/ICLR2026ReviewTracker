{"id": "KfxRzCmRSX", "number": 15051, "cdate": 1758247212815, "mdate": 1763282615060, "content": {"title": "ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization", "abstract": "Autoformalization, which translates natural language mathematics into machine-verifiable formal statements, is critical for using formal mathematical reasoning to solve math problems stated in natural language. While Large Language Models can generate syntactically correct formal statements, they often fail to preserve the original problem's semantic intent. This limitation arises from the LLM approaches' treating autoformalization as a simplistic translation task which lacks mechanisms for self-reflection and iterative refinement that human experts naturally employ. To address these issues, we propose ReForm, a Reflective Autoformalization method that tightly integrates semantic consistency evaluation into the autoformalization process. This enables the model to iteratively generate formal statements, assess its semantic fidelity, and self-correct identified errors through progressive refinement. To effectively train this reflective model, we introduce Prospective Bounded Sequence Optimization (PBSO), which employs different rewards at different sequence positions to ensure that the model develops both accurate autoformalization and correct semantic validations, preventing superficial critiques that would undermine the purpose of reflection. Extensive experiments across four autoformalization benchmarks demonstrate that ReForm achieves an average improvement of 22.6 percentage points over the strongest baselines. To further ensure evaluation reliability, we introduce ConsistencyCheck, a benchmark of 859 expert-annotated items that not only validates LLMs as judges but also reveals that autoformalization is inherently difficult: even human experts produce semantic errors in up to 38.5\\% of cases.", "tldr": "", "keywords": ["Autoformalization", "Automated Theorem Proving"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b040fa6a8dad5e1cc7e25d7b990dc020eb03026c.pdf", "supplementary_material": "/attachment/b93f64a7a9d43f5180ee8dd94fed514be9b186f6.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces ReForm, a novel Reflective Autoformalization framework designed to convert natural language mathematics into machine-verifiable formal statements more accurately. Unlike previous one-pass translation approaches that often lose semantic fidelity, ReForm integrates semantic self-validation directly into the autoformalization process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides a **ConsistencyCheck benchmark** that rigorously evaluates the reliability of **LLM-based judges** and quantifies the challenges of **autoformalization**.  \n- Introduces **iterative reflection techniques from reasoning models** into the process of **autoformalization**.  \n- The paper is **well-written**."}, "weaknesses": {"value": "- LLM is not a perfect supervisory signal, and I would like to know to what extent the capability of the judge model affects the stability of training.  \n- How did you handle **semantic alignment** in your method — was it **integrated into the reward design**?  \n- Please provide an **ablation study comparing process-level supervision and outcome-level supervision**."}, "questions": {"value": "Please check the **Weaknesses** section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JsUinJU3NB", "forum": "KfxRzCmRSX", "replyto": "KfxRzCmRSX", "signatures": ["ICLR.cc/2026/Conference/Submission15051/Reviewer_63fq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15051/Reviewer_63fq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813305176, "cdate": 1761813305176, "tmdate": 1762925372924, "mdate": 1762925372924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ReForm, a novel reflective autoformalization framework that aims to improve the semantic fidelity of translating natural language mathematics into formal statements. Unlike traditional “one-pass” autoformalization models that generate a single formal output, ReForm introduces an iterative self-correction loop where the model alternates between formal statement generation and semantic self-validation. To train this reflective process effectively, the authors propose a new reinforcement learning algorithm, Prospective Bounded Sequence Optimization (PBSO), which integrates heterogeneous rewards for both the main task (final correctness) and auxiliary critiques (intermediate semantic validation). Extensive experiments across four challenging benchmarks (miniF2F, ProofNet, PutnamBench, AIME2025) demonstrate an average +17.2 percentage point improvement in semantic consistency over the strongest baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The target problem is well-motivated and addresses a clear bottleneck in formal reasoning: the semantic fidelity in autoformalization.\n- The integration of reflective reasoning and reinforcement learning is novel and effective; the experiments demonstrate consistent and interpretable improvements across multiple benchmarks.\n- The newly established ConsistencyCheck benchmark provides a valuable resource for quantitatively assessing the reliability of LLM-based metrics and for understanding the intrinsic challenges of mathematical autoformalization."}, "weaknesses": {"value": "- The paper relies heavily on LLM-based semantic evaluation metrics, which, despite the ConsistencyCheck benchmark, may still introduce bias or circularity in measuring semantic consistency.\n\n- The computational cost and efficiency trade-offs of the reflective multi-iteration process are not fully analyzed — it remains unclear how scalable the approach is for large-scale or more complex formal systems.\n\n- Several important related works are missing; please refer to the recent surveys [1, 2] for a broader overview.\n\n- (Minor) There are too many autoformalization papers recently, which may cause aesthetic fatigue in the community.\n\n[1] A Survey on Deep Learning for Theorem Proving, COLM 2024. \n\n\n[2] Autoformalization in the Era of Large Language Models: A Survey, arXiv 2025."}, "questions": {"value": "1. The paper both trains and evaluates with LLM-based judges (CriticLean for RL and Qwen3 for evaluation).  Could the authors verify cross-judge robustness, e.g., whether ReForm still shows similar gains when evaluated with a different LLM judge, such as Gemini or GPT-5?\n2. In Sec. 4.4, the authors show that responses get longer during RL, but longer ≠ is better. Can we disentangle “more tokens” from “better critiques”? For example, is there an automatic or human rating showing that later reflections actually introduce new semantic constraints (quantifier scopes, hidden assumptions, edge cases), rather than just paraphrasing previous critiques? A precision/recall–style analysis on detected error types would clarify this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W0RY9EFwOa", "forum": "KfxRzCmRSX", "replyto": "KfxRzCmRSX", "signatures": ["ICLR.cc/2026/Conference/Submission15051/Reviewer_d2BC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15051/Reviewer_d2BC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987739364, "cdate": 1761987739364, "tmdate": 1762925372443, "mdate": 1762925372443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **ReForm**, a reflective autoformalization paradigm that shifts from one-pass translation to an iterative process combining formal statement generation with semantic self-validation. To train this model effectively, the authors propose **Prospective Bounded Sequence Optimization (PBSO)**, a novel RL algorithm that uses heterogeneous rewards at different sequence positions to ensure both accurate autoformalization and faithful self-critiques. Extensive experiments on four benchmarks show ReForm achieves an average improvement of **17.2 percentage points** in semantic consistency over state-of-the-art baselines. The authors also introduce **ConsistencyCheck**, a benchmark of 859 expert-annotated items, which reveals that autoformalization is inherently difficult, even human experts make semantic errors in up to 38.5% of cases."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Novel Reflective Paradigm:** The core innovation is shifting autoformalization from a one-pass translation task to an iterative, self-correcting process. By mimicking the human expert's cycle of generation, validation, and refinement, ReForm directly addresses the critical challenge of semantic fidelity, moving beyond mere syntactic correctness.\n\n2.  **Effective Training with PBSO:** The proposed Prospective Bounded Sequence Optimization (PBSO) algorithm is a clever solution to the multi-objective credit assignment problem. It effectively trains the model to produce high-quality final formalizations *and* accurate intermediate critiques, preventing the self-validation mechanism from degenerating into superficial or hallucinated feedback.\n\n3.  **Rigorous and Comprehensive Evaluation:** The paper provides extensive empirical validation across four challenging benchmarks, demonstrating substantial and robust improvements. The creation of the expert-annotated **ConsistencyCheck** benchmark adds significant rigor, not only validating the use of LLMs as judges but also quantifying the inherent difficulty of the task itself."}, "weaknesses": {"value": "1. **Heacy Dependence on LLM-based Evaluation**: The entire training and evaluation framework relies heavily on LLM judges (like Qwen3-235B and CriticLean-14B) to assess semantic consistency. While the authors rigorously validate these judges with their ConsistencyCheck benchmark, they still have a non-trivial error rate (about 17%). \n2. **Insufficient Ablation on Key Algorithmic Components**: While the paper demonstrates the overall effectiveness of PBSO, it lacks ablation studies on several critical design choices. The contribution of position-specific advantages is not isolated from the core bounded return mechanism, making it unclear if this complexity is necessary. Furthermore, the individual and interactive effects of the Task Reward &   Auxiliary Rewards are not thoroughly dissected.\n3. **Lack of Details of Human Evaluation**: Number of the annotators in total? Number of the annotators per statement? Background of the annotators?  and so on...\n4. The performance of 32B and 8B is very close, yet the author has not provided a reasonable explanation for this."}, "questions": {"value": "See the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cfOS9yUCTN", "forum": "KfxRzCmRSX", "replyto": "KfxRzCmRSX", "signatures": ["ICLR.cc/2026/Conference/Submission15051/Reviewer_4Tg9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15051/Reviewer_4Tg9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762749925434, "cdate": 1762749925434, "tmdate": 1762925371944, "mdate": 1762925371944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response 2: About the Use and Critical Evaluation of LLM-as-Judge"}, "comment": {"value": "We thank all reviewers (4Tg9, d2BC, 63fq) for the critical and unified discussion regarding our paper's reliance on \"LLM-as-judge.\" This is indeed a crucial topic, and we appreciate the opportunity to clarify our perspective, as **we believe this point represents one of our paper's primary contributions, not a weakness**.\n\n1. LLM-as-Judge is the current SOTA solution for RL in Reasoning:\nWe must first claim that using an LLM-based reward model is the established, state-of-the-art (SOTA) standard for training RL agents on complex reasoning tasks. This is not a choice unique to our paper; it is the only scalable method to provide the necessary semantic, non-differentiable rewards.\n\nLeading works in autoformalization and theorem proving, such as Goedel-Prover-V2 [1], DeepSeek-Prover-V2 [2], CriticLean [3], and Kimina [4], all fundamentally rely on an \"LLM-as-judge\" paradigm. This approach is also standard in broader reasoning domains (e.g., DeepMind's work on constitutional AI). **Our methodology aligns with the best practices of the entire field**.\n\n2. Our Contribution is to Critically Evaluate This Standard, Not Blindly Trust It.\n\nWhere our paper differs from prior work is that we did not blindly trust this paradigm. **A core contribution of our work is to be one of the first to directly confront, rigorously quantify, and transparently report the limitations of these SOTA judges**.\n\nTo this end, we introduced the ConsistencyCheck benchmark—a new, expert-annotated dataset of 859 items—precisely to measure the reliability of these judges.\n\n3. **This is a Finding of our paper, Not a Flaw**.\n\nThe 17% error rate cited by Reviewer 4Tg9 is not a methodological weakness of our paper, but a key scientific finding by our paper.\n\nWe respectfully argue that **it is unreasonable to penalize our work for being the \"messenger\" that exposes a systemic issue affecting the entire community**. Our research quantifies the noise floor and risk that all current SOTA models are training with. This is a contribution to transparency and rigor, encouraging the field to move towards more robust evaluation.\n\n4. The Alternative (Human-in-the-Loop) is Unscalable and Less Reliable.\n\nThe only viable alternative—using humans for RL feedback—is unscalable. Furthermore, our ConsistencyCheck analysis also revealed that this \"gold standard\" is deeply flawed: human experts err in up to 38.5% of cases on this \"inherently difficult\" task. This critical finding further validates that SOTA LLMs, while imperfect (17% error), are currently the most scalable and most reliable supervisory signal available.\n\nSummary:\nWe did not just use an LLM-judge. We critically evaluated it, benchmarked it, reported its failure rate, and proved it is still superior to the human alternative. **We believe this transparency and rigorous analysis should be viewed as a strength and a primary contribution that strengthens our findings, rather than a weakness that undermines them.**\n\n[1] Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction\n\n[2] DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition\n\n[3] CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization\n\n[4] kimina-prover preview: towards large formal reasoning models with reinforcement learning"}}, "id": "zOBBeYpNDM", "forum": "KfxRzCmRSX", "replyto": "KfxRzCmRSX", "signatures": ["ICLR.cc/2026/Conference/Submission15051/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15051/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15051/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763280165283, "cdate": 1763280165283, "tmdate": 1763280250465, "mdate": 1763280250465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response 1"}, "comment": {"value": "We sincerely thank all reviewers for their diligent work and constructive feedback.\n\nWe noted several key points in the reviews that may stem from misunderstandings of our novel approach. As our goal is to thoroughly address these points and answer all valuable questions, our responses are necessarily detailed. We have provided specific, multi-part replies for each reviewer to ensure all concerns are resolved.\n\nGiven the exhaustive nature of these replies, we humbly ask for your patience in reading them, as they contain crucial clarifications regarding our methodology, experiments, and core contributions. **We deeply appreciate the extra time and effort this may require**.\n\nFurthermore, to respond directly to your feedback, **we have submitted a revised manuscript and supplementary material**. All significant changes have been marked in the text with `red font`, allowing for convenient review.\n\nWe believe these clarifications and modifications fully address the initial concerns. We thank you again for your time and guidance, and we respectfully hope you will re-evaluate our work based on this new information."}}, "id": "Nwq2VQMagL", "forum": "KfxRzCmRSX", "replyto": "KfxRzCmRSX", "signatures": ["ICLR.cc/2026/Conference/Submission15051/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15051/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15051/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763280220376, "cdate": 1763280220376, "tmdate": 1763280270963, "mdate": 1763280270963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}