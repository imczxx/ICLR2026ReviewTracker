{"id": "AF3buTGlPX", "number": 16143, "cdate": 1758260549432, "mdate": 1759897258678, "content": {"title": "DRL-STAF: A Deep Reinforcement Learning Framework for State-aware Forecasting of Complex Multivariate Hidden Markov Process", "abstract": "Multivariate hidden Markov process forecasting remains challenging due to nonlinearity, nonstationarity, hidden state transitions, and cross-sequence dependencies. Deep learning (DL) methods have shown strong predictive performance in time series forecasting but generally lack explicit state modeling and interpretable state estimation, while Hidden Markov Model (HMM) and its variants can provide explicit state representations but are limited in capturing complex nonlinear observation patterns and suffer from scalability issues. To address these limitations, we propose a Deep Reinforcement Learning based framework for STate-Aware Forecasting of complex multivariate hidden Markov process (DRL-STAF), which simultaneously predicts the next-step observation and estimates the corresponding hidden state. In the proposed framework, deep learning is used as the emission function to capture complex nonlinear observation patterns, while deep reinforcement learning models state transitions, supporting flexible adaptation to diverse transition patterns without predefined structural assumptions. In particular, DRL-STAF remains effective when dealing with complex multivariate hidden Markov processes, such as coupled higher-order semi-Markov dynamics, that typically suffer from state-space explosion. Comprehensive experiments demonstrate superior predictive performance and accurate state estimation compared with HMM and its variants, standalone deep learning methods, and existing DL-HMM hybrid methods.", "tldr": "We propose a new state-aware forecasting framework for multivariate hidden Markov processes, which integrates deep learning for flexible emission modeling with deep reinforcement learning to learn complex time-varying latent state transitions.", "keywords": ["Multivariate Hidden Markov Process", "State Estimation", "Hidden Markov Model", "Deep Reinforcement Learning", "Deep learning"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/44fa0b00a3b030ab55370f173c0d5c1b875c10bc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "he paper proposes DRL-STAF, a two-stage framework for state-aware forecasting of multivariate hidden Markov processes. Emissions are produced by per-variable deep networks with m state-specific heads; latent state transitions are inferred by a deep RL agent that performs hard decoding (argmax over states). Stage-1 trains each variable independently; Stage-2 freezes the emission nets and uses a ResGAT layer to coordinate cross-variable interactions, updating only the RL policy (with occasional unfreezing if a shaped “gain” reward is positive). Experiments on two synthetic CHOSMM datasets and two real datasets (SMachine, Exchange) report strong forecasting metrics and seemingly high state-estimation scores vs HMM variants, Markovian-RNN, and DEN-HMM"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear separation of emission modeling (DL) and transition inference (RL); the architectural overview and training algorithms are laid out with pseudocode and figures.  \n\nAttempts to address multivariate state-space blow-up via per-variable Stage-1 training and a Stage-2 ResGAT coordination layer.  \n\nReleasable code (anonymous 4open link) and explicit synthetic data-generation procedures (CHOSMM) aid reproducibility."}, "weaknesses": {"value": "Positioning DRL-STAF as the first fully deep learning–based HMM (both transitions and emissions via deep models) seems overstated given prior “neural HMM / DL-HMM” lines that already parameterize transitions and emissions with NNs and train end-to-end; here the new element is chiefly using RL for state selection, not a fundamentally new probabilistic formulation. The paper itself cites DL-HMM hybrids and Markovian-RNNs; claiming “first” needs a much tighter survey and careful scoping.  \n\nThe method casts state inference as an RL policy over m actions, yet the reward is built directly from supervised prediction errors of state-conditioned heads (MSE deltas vs a baseline), plus continuity penalties. This looks like a complicated surrogate for (a) supervised per-step state classification (with argmax/hard Viterbi at test time) or (b) differentiable hard-decoding via straight-through / Gumbel-Softmax. The paper does not demonstrate that RL brings superior accuracy, stability, or sample-efficiency relative to these simpler alternatives.  \n\nThe text describes a policy π, returns, discount γ, and bespoke reward shaping, but never states which RL algorithm (e.g., PPO, A2C, DQN, REINFORCE with baseline) is used, how variance is controlled, or how entropy/advantage baselines are handled. Given the tight coupling between policy and emission nets (and the bespoke sample-screening and soft-update heuristics), training stability is a core risk that is not rigorously addressed.  \n\nStage-1 sample screening retains steps where the chosen state’s head has the lowest MSE (or long continuity segments), then uses those to train the emission network—this can self-reinforce early mistakes, entrenching whatever head was momentarily best, and bias the measured gains. There’s no ablation to show results without screening, nor diagnostics on how often the screening discards contradictory evidence.  \n\nThe method explicitly assumes infrequent transitions and penalizes action switches, effectively baking in state persistence. On datasets whose regimes are indeed sticky (including the authors’ synthetic CHOSMM), this can artificially improve apparent state accuracy and MAE/MSE by smoothing, rather than by genuinely better transition modeling. There’s no stress test on rapid-switching regimes or ablation removing the continuity term.  \n\nReal-data evaluation uses (i) a handpicked 3-dim subset of SMachine test data with anomaly labels, and (ii) Exchange rates with no state labels. Modern forecasting baselines (PatchTST/iTransformer/SSM-based models) and switching state-space alternatives are absent; e.g., a plain strong forecaster with regime-aware losses or a neural HMM trained with Viterbi/EM would be natural comparisons. Reported gains on SMachine are tiny in MAE/MSE (e.g., DRL-STAF 0.0189/0.0010 vs Markovian-RNN 0.0190/0.0010), yet the table headlines state metrics (e.g., precision 100%) that are hard to interpret without prevalence/threshold details; meanwhile DEN-HMM is curiously orders of magnitude worse on Exchange, raising tuning-fairness concerns.  \n\nTables show point metrics but no confidence intervals, paired tests, or across-seed variance—critical given small margins and bespoke reward shaping. The synthetic tasks are tightly matched to the method’s assumptions (sticky regimes, AR(1) emissions with two states), so real-world generality remains unproven.  \n\nThe Stage-2 ResGAT coordination is presented as a remedy for joint-state explosion, but there’s no analysis of how closely it approximates the true joint posterior (or hard MAP sequence) as N or m grow, nor any complexity/runtime discussion vs structured inference (e.g., factorized Viterbi or mean-field with coupling)"}, "questions": {"value": "Which RL algorithm and objective do you use (exact loss, baselines, entropy terms, clipping)? Please report training stability (success rate across seeds) and compute/time.  \n\nWhy not a supervised state classifier (cross-entropy over m states) trained on the same signals, or a Gumbel-Softmax / straight-through hard-decoding? Provide a head-to-head comparison.  \n\nAblate sample screening and the continuity penalty. How do forecasting and state metrics change?  \n\nEvaluate on fast-switching regimes and additional real datasets; include strong modern forecasters and switching SSM baselines.  \n\nProvide CIs/paired tests and per-dataset effect sizes (ΔMAE/ΔMSE) rather than only point estimates"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ma612dX14u", "forum": "AF3buTGlPX", "replyto": "AF3buTGlPX", "signatures": ["ICLR.cc/2026/Conference/Submission16143/Reviewer_WBEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16143/Reviewer_WBEf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760727093745, "cdate": 1760727093745, "tmdate": 1762926312834, "mdate": 1762926312834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a hybrid framework that combines deep learning, HMMs, and reinforcement learning to model complex multivariate time series. Instead of fixed transition matrices and simple emissions, it uses neural networks for both allowing the model to learn nonlinear, time-varying, and context-dependent state transitions. First , each variable is treated as an independent agent with its own networks that predict observations and infer hidden states through RL-based rewards involved to the prediction accuracy.\nIn the second stage, the method connects all variables using a graph attention network that refines their hidden state probabilities jointly. This stage helps capture cross-variable dependencies and updates the local models if performance improves."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- I like the introduction of the RL perspective into HMMs. It’s an interesting and less-explored angle, especially through the use of rewards and policies.\n\n- A good visualization in the related works section.\n\n- The paper is has clear algorithms (in appendix) and figures that make the training pipeline and the two-stage process easier to follow (in contrast to the text presentation which is not very easy to follow).\n\n- The appendix is detailed and informative, providing extra visualizations and implementation details. It shows effort in transparency and reproducibility."}, "weaknesses": {"value": "- The paper does not state a single end-to-end objective before breaking into sub-goals, which makes the optimization story hard to follow. Please define a unified objective (e.g., ELBO or a constrained risk) and then show how each parameterization in your work approximates/optimizes parts of it.\n\n- Many reward terms feel hand-crafted each has intuition but little anchoring to standard formulations. It would help to map terms to known constructs (sticky priors, advantage baselines, load-balancing mixture-of-experts (MoE), duration models) and then present your empirical approximations as principled relaxations.\n\n- The design creates separate $DEM_i$ , $\\pi_{i,\\theta_{A}}$  and $\\pi_{i, \\theta_{A'}}$  per variable. This is impractical for large $N$ (e.g., vision sequential data). Please discuss/shared-backbone alternatives (e.g., conv/attention encoder + per-state heads, or a shared policy with variable embeddings) and add an ablation on sharing vs. per-variable separated networks.\n\n- Test-time steps aren’t clearly described. Given a new sequence, how are states inferred, how do you roll without future observations, and how do you handle missing data or cold starts? If your model is only able to follow up with filtering-style forward (see observation and update states), it should be clearly mentioned as a limitation.\n\n- The method seems resource-intensive (many networks, two stages, GAT). If not saying a theoretical complexity analysis, at least, the paper needs an empirical analysis (like wall-clock running time, FLOPs/param counts, or memory profiling). Please report training/inference time and scaling curves. \n\n- Experiments compare mainly to classical HMM variants. Consider adding neural/learned-transition baselines (neural HSMM like SALT [1], switching state-space models (parameterized by NN, e.g. in GIN[2] or in SSI[3]), deep Kalman/SSM variants like [4], or hard-EM/Viterbi neural HMMs like [5]) to better position your gains.\n\n- The paper lacks a code repo with an easy-to-read instruction such as readme file to make it easier to follow up.\n\n\n\n[1] - Lee, Hyun Dong, et al. \"Switching autoregressive low-rank tensor models.\" Advances in Neural Information Processing Systems 36 (2023): 57976-58010.\n\n[2] - Ikderi and Wan Choi. \"Gated inference network: Inference and learning state-space models.\" Advances in Neural Information Processing Systems 37 (2024): 39036-39073.\n\n[3] - Ruhe, David, and Patrick Forré. \"Self-supervised inference in state-space models.\" arXiv preprint arXiv:2107.13349 (2021).\n\n[4] - Song, Xiangchen, et al. \"Temporally disentangled representation learning under unknown nonstationarity.\" Advances in Neural Information Processing Systems 36 (2023): 8092-8113.\n\n[5] - Jiang, Xiajun, et al. \"Sequential latent variable models for few-shot high-dimensional time-series forecasting.\" The Eleventh International Conference on Learning Representations. 2023."}, "questions": {"value": "Thanks for your draft and all of efforts. I have some questions about this work:\n\n- Could you formalize the overall training objective more clearly? It would help to start from the general evidence expression $ \\log p(X) = \\log \\sum_S p(X, S) $ or a risk minimization like $\\mathbb{E}[l]$ subject to some constraints (for reference please take a look to eq 7 in [5]. It is a common and standard practice of defining objectives). Then show where the approximation is made, and then derive an approximate ELBO or a theoretical lower bound if possible. The current setup feels similar to a hard-EM procedure, where the E-step estimates errors and rewards and the M-step updates the networks. Please clarify the connections and the exact optimization target.\n\n- Could you clarify the statistical reasoning behind the action continuity term? It appears similar to the sticky prior used in sticky-state HMMs, but here it’s presented in a more heuristic way. From a probabilistic view, this term acts like a time-dependent prior on the transition model that initially are more non-transitions. Please consider expressing it as a formal prior or likelihood term rather than a hand-crafted penalty so readers can better understand its origin and justification.\n\n- Could you clarify the role of the $e^{\\text{base}}$ term in your prediction gain? Is it intended as a control variate similar to baseline subtraction in reinforcement learning to reduce variance and stabilize updates, or does it have another interpretation? Why not define the reward simply in terms of -$e^{(a)}$ without including $e^{\\text{base}}$? Any try on this?\n\n- The hand-crafted discrepancy term between state errors makes the formulation harder to follow. It may read more clearly if it were framed as a variance-of-risk penalty or connected to a Bayesian prior interpretation, rather than appearing as an ad-hoc L1 difference. Even a short explanation linking it to those ideas would help readers understand its purpose and statistical grounding.\n\n- The overall model feels large and hard to scale. Just example, if the data were a $(N=1000 \\times 1000, T)$ image sequences, would the current setup still be feasible (for example take a look to experiments of [2] and [5])? Why not use shared backbones or partially shared modules instead of per-variable networks? An ablation or discussion on model sharing and scalability would strengthen the paper.\n\n- Please include a complexity analysis for both training and inference. Even if a full theoretical analysis is difficult, an empirical comparison (e.g., wall-clock time, FLOPs, or memory usage) would help. Using variable-dependent separate networks is clearly expensive, so checking this overhead would make the paper more transparent. (for example take a look to table 1 in SALT [1] or table 6 in GIN [2])\n\n- The gradient computation path is not entirely clear. Since the model involves discrete sampling (e.g., argmax during hard decoding), it is not obvious how gradients are propagated in theory (e.g. equation 7-8 in [4]) . The many hand-crafted components make this kinda harder to trace and could introduce stability issues. Did you encounter such problems in experiments, and if so, could you mention them (e.g. in your table of results adding a column and report the success  percentage over your random seeds run.) and describe any remedies or tricks used to stabilize training? \n\n- The classical HMM baselines (even neural versions with parametric emissions or transitions) are relatively weak comparisons. I would suggest including more advanced smooth or continuous state-estimation baselines such as neural state-space models, switching Kalman filters, or variational sequence models to provide a stronger and fairer benchmark.\n\n---\nA few minor issues:\n- In line 42 \"three key limitations ...\"\n- in lines 386-389, better to mention specifically DEM_i are trained based on the updated probabilities in second stage. A little confusing.\n- Despite your model, many of the baselines see improvement ,, at least in accuracy, when variable size increases. Making sense as model uses correlational info for richer state inference, but what do you think about this? Also would be nice a short sentence being added about this.\n- If you have an ablation only on second stage policy network without mean-field style behavior in first stage, please report it (i.e. directly optimized your policy when variables interaction is modeled and first stage policy learning is dropped)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sRmhuR4OHL", "forum": "AF3buTGlPX", "replyto": "AF3buTGlPX", "signatures": ["ICLR.cc/2026/Conference/Submission16143/Reviewer_Qm9x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16143/Reviewer_Qm9x"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761229340164, "cdate": 1761229340164, "tmdate": 1762926312270, "mdate": 1762926312270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DRL-STAF to capture the complex nonlinear observation patterns in Multivariate hidden Markov processes. In the proposed framework, deep learning is used as the emission function to capture complex nonlinear observation patterns, while deep reinforcement learning models state transitions, supporting flexible adaptation to diverse transition patterns without predefined structural assumptions.  Experiments show the effectiveness of the proposed framework."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is overall well-organized and easy to follow.\n2. The proposed framework, DRL-STAF, is intuitive and is supported by several theoretical and experimental results.\n3. The experiment of predicting exchange rates is interesting in the field of reinforcement learning."}, "weaknesses": {"value": "1. Although hidden Markov models are introduced, multivariate hidden Markov decision processes are not sufficiently introduced in the introduction. Relevant literature and intuitive examples are not provided.\n2. The proposed method should be connected to the model-based RL method, where a model is trained to predict the future states and rewards in MDPs or POMDPs.\n3.  The authors should provide a detailed introduction of the reward in the experimental part.\n4. The text in some images, such as Figure 1, is too small to read."}, "questions": {"value": "1. What is the difference between the Multivariate hidden Markov process and the partially observable Markov process?\n2. Can RL methods for the partially observable Markov process be used in solving the Multivariate hidden Markov process?\n3. Could you discuss the computational cost and scalability of the proposed method in detail, especially as the sequence length increases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KL9kMkAPUb", "forum": "AF3buTGlPX", "replyto": "AF3buTGlPX", "signatures": ["ICLR.cc/2026/Conference/Submission16143/Reviewer_xt2e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16143/Reviewer_xt2e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761234253279, "cdate": 1761234253279, "tmdate": 1762926311024, "mdate": 1762926311024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DRL-STAF, a novel multivariate time series framework by combining HMM-like state space model with deep RL. First, the prediction module (a DEN-HMM where each head is responsible for each state) utilizes a Deep Emission Network to model state-dependent patterns. Second, a state estimation problem is framed as a Deep RL problem with two-stage training. Experiments are extensive and results are promising."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem of multivariate time series forecasting is long-standing and challenging.\n- The core idea of estimating states as a deep RL problem where the policy is a transition function is novel to my knowledge.\n- The methodology is technically sound and sophisticated.\n- Presentation and figures are clear, well-structured and well-written."}, "weaknesses": {"value": "- Assumption of infrequent state transitions is strong and may not hold true in practice.\n- No computational analysis in terms of training and inference cost are provided. I am curious to see if this framework can be applied for online, real-time streaming data.\n- Comparisons with other DL-based solutions such as TFT[1] for multivariate forecasting should be considered in experiments.\n[1] Lim et.al., Temporal Fusion Transformers for interpretable multi-horizon time series forecasting, International Journal of Forecasting, 2021"}, "questions": {"value": "- Can this framework used in multistep (long-horizon) forecasting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1FZZ8oAfzh", "forum": "AF3buTGlPX", "replyto": "AF3buTGlPX", "signatures": ["ICLR.cc/2026/Conference/Submission16143/Reviewer_huyU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16143/Reviewer_huyU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16143/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761517764226, "cdate": 1761517764226, "tmdate": 1762926309550, "mdate": 1762926309550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}