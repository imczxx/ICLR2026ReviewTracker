{"id": "1H1HrHCHyz", "number": 18009, "cdate": 1758282879416, "mdate": 1759897139638, "content": {"title": "Meta-Prompt Optimization for LLM-Based Sequential Decision Making", "abstract": "Large language models (LLMs) have recently been employed as agents to solve sequential decision-making tasks such as Bayesian optimization and multi-armed bandits. These works usually adopt an LLM for sequential action selection by providing it with a fixed, manually designed meta-prompt. However, numerous previous works have found that the prompt has a significant impact on the performance of the LLM, which calls for a method to automatically optimize the meta-prompt for LLM-based agents. Unfortunately, the non-stationarity in the reward observations during LLM-based sequential decision making makes meta-prompt optimization highly challenging. To address this challenge, we draw inspirations from adversarial bandit algorithms, which are inherently capable of handling non-stationary reward observations. Building on this foundation, we propose our EXPonential-weight algorithm for prompt Optimization (EXPO) to automatically optimize the task description and meta-instruction in the meta-prompt for LLM-based agents. We also extend EXPO to additionally optimize the exemplars (i.e., history of interactions) in the meta-prompt to further enhance the performance, hence introducing our EXPO-ES algorithm. We use extensive experiments to show that our algorithms significantly improve the performance of LLM-based sequential decision making.", "tldr": "We propose an adversarial bandit-based approach to automatically optimize the meta-prompt for LLM-based sequential decision making.", "keywords": ["Prompt optimization", "large language models", "multi-armed bandits"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8e23b97b4a7109de9ac10d3324790a9e7f3fae2.pdf", "supplementary_material": "/attachment/6316255747d4b80cde8232b0ade2a48ddb6b44a0.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the meta-prompt optimization problem in LLM-based sequential decision-making tasks, proposing a novel adversarial bandit approach called EXPO and its extension EXPO-ES. These methods effectively handle non-stationary reward signals by automatically optimizing task descriptions, meta-instructions, and exemplar selection. The methodology is well-designed, experiments are thorough, and results significantly outperform existing hand-crafted prompts and general prompt optimization techniques, demonstrating strong innovation and practical value. While computational cost may be a limitation, overall this is a high-quality research contribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. To my knowledge, this is the first to frame and solve the non-stationary meta-prompt optimization problem for LLM-based sequential decision making, backed by adversarial-bandit regret guarantees.\n\n2. Non-trivially extends EXP3 to infinite, dynamic arm spaces via a neural reward predictor that scores entire prompt pools in one forward pass, while preserving theoretical bounds.\n\n3. Plug-and-play: no internal model changes; simply swap the hand-crafted prompt for the sampled one and gain 20–70 % error/regret reduction across BO, MAB, TSP and instruction tuning tasks.\n\n4. Open-source, lightweight implementation—two lines of code enable reproducible gains, offering an immediate boost to any LLM agent pipeline."}, "weaknesses": {"value": "1. The hyperparameters introduced for evaluating the neural network—such as its architecture, learning rate, and sliding-window size—are task-specific and may need to be re-tuned for other tasks.\n\n2. While performance gains have been demonstrated on single-prompt tasks, the approach could prove ill-suited for multi-turn conversations, especially in the currently prominent area of agentic RL.\n\n3. Since the prompt pool is generated by rewriting the initial prompt with an LLM, how can we guarantee that this pool actually contains better prompts that drive continuous optimization?"}, "questions": {"value": "1. The paper uses an LLM to paraphrase the initial prompt to construct the prompt space. Will this method lead to redundancy or incomplete coverage of the prompt space? Are there more systematic ways to build a more representative prompt space?\n\n2. Are there plans to validate the method on more complex tasks (such as RL environments, dialogue systems, or multimodal tasks)?\n\n3. Due to the capability gaps among different LLMs, will using a weaker model cause the performance of EXPO to collapse? Since commercial APIs are costly, can smaller open-source LLMs achieve similar effects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rkp7HuC3Bk", "forum": "1H1HrHCHyz", "replyto": "1H1HrHCHyz", "signatures": ["ICLR.cc/2026/Conference/Submission18009/Reviewer_Vhwb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18009/Reviewer_Vhwb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903336204, "cdate": 1761903336204, "tmdate": 1762927802469, "mdate": 1762927802469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to solve a meta-prompt optimization problem for LLM-based agents in sequential decision-making tasks.\nThe authors propose a method called EXPO, which utilizes adversarial bandit ideas to optimize meta-prompts, including task descriptions, meta-instructions, and exemplars. The authors formulate the problem as an adversarial bandit problem, where each candidate meta-prompt is treated as an \"arm\". The method employs an EXP3-style algorithm with a neural score estimator over text embeddings to randomize selection among many prompts. The method is evaluated on various tasks, demonstrating consistent improvements over existing baselines.\nExperiments on Linear Regression (LR), Traveling Salesman Problem (TSP), instruction optimization on GSM8K, and LLM-driven MAB show consistent gains over OPRO and other prompt-optimization baselines, plus ablations on exploration strength and component contributions. The paper claims improved efficiency and final performance, and notes a limitation regarding multi-agent settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The author uses a neural score estimator that takes text embeddings as input to estimate the reward of each meta-prompt. This allows the method to generalize better to unseen prompts and reduces the need for extensive exploration.\n\n2. The author includes a comprehensive set of experiments on various tasks, demonstrating the effectiveness of the proposed method. The results show consistent improvements over existing baselines, indicating the robustness of the approach."}, "weaknesses": {"value": "1. The author argues that the adversarial bandit formulation is more suitable for the meta-prompt optimization problem than the stochastic bandit formulation used in prior work. However, I am not fully convinced by this argument. The environments in the experiments do not appear to be highly adversarial, and it is unclear how much benefit the adversarial formulation provides in practice. We acknowledge that the environments may not be stable but it is not necessarily adversarial. More discussion and empirical evidence are needed to support this claim.\n\n2. This paper lacks a detailed analysis of the computational complexity of the proposed method. While the authors mention that EXPO is more efficient than prior methods, it would be helpful to provide a more thorough analysis of the time and space complexity of the algorithm, especially in comparison to other prompt optimization methods. Since the method is based on an EXP3-style algorithm, it may enjoy a similar computational complexity as EXP3, but this should be explicitly stated and analyzed."}, "questions": {"value": "1. How does $k$ (number of exemplars) affect performance in EXPO-ES? In Algorithm 1, is $k$ fixed or adaptive? Do we need to specify $k$ in advance?\n\n2. How does the task description and the meta-instruction evolve during training? More specifically, how to obtain $\\mathcal{D}\\_{t+1}$ and $\\mathcal{I}_{t+1}$ from $P_t$? If $\\mathcal{D}$ and $\\mathcal{I}$ are sampled from some fixed pool, what is the size of the pool and does the agent need to take that as input?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DpeKBQrkWQ", "forum": "1H1HrHCHyz", "replyto": "1H1HrHCHyz", "signatures": ["ICLR.cc/2026/Conference/Submission18009/Reviewer_Qq81"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18009/Reviewer_Qq81"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947487607, "cdate": 1761947487607, "tmdate": 1762927801960, "mdate": 1762927801960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel algorithmic framework, EXPO (Exponential-weight Algorithm for Prompt Optimization) and its extended version EXPO-ES (with Exemplar Selection), designed to automatically optimize the \"meta-prompt\" in LLM-driven sequential decision-making tasks. The core idea of using adversarial bandits to handle non-stationary rewards is well-motivated and sound. The motivation of the paper is clear, and the algorithm design is reasonable. Several experiments (Linear Regression, TSP, Instruction Optimization, MAB) validate its effectiveness. Overall, this work provides useful insights for the research of \"LLMs as decision-making agents\" and introduces a robust optimization solution based on adversarial multi-armed bandits."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper systematically introduces the problem of Meta-Prompt Optimization, emphasizing that unlike traditional prompt optimization methods that assume stationary reward distributions, the rewards in LLM-based agent environments are inherently non-stationary. This observation is highly insightful and effectively bridges the gap between prompt engineering and sequential decision making.\n2. The paper integrates the adversarial bandit (EXP3) framework with LLM-based prompt search, proposing EXPO to dynamically optimize both task descriptions and meta-instructions. The extended version, EXPO-ES, further introduces exemplar selection, supported by clear motivation.\n3. The experiments encompass several types of tasks and the results demonstrate good generalizability of the proposed approach."}, "weaknesses": {"value": "1.  The comparisons do not include stronger baselines. There is no direct evaluation against more advanced RL-based LLM optimizers, such as PromptAgent. Some baselines, such as INSTINCT and MIPRO, are only tested on specific tasks, resulting in a lack of fair and unified experimental settings.\n2. The paper lacks an analysis of the internal model dynamics, such as changes in embeddings or L2 divergences, which could provide deeper insight into the behavior of the optimization process.\n3. The paper would benefit from substantial revision of its language and presentation."}, "questions": {"value": "1. The paper clearly states that non-stationary reward observations are the key challenge that renders prior stochastic MAB-based prompt optimization methods unsuitable. Could you provide a more formal analysis or a simple synthetic example to illustrate how the non-stationarity manifests? \n2. The non-stationarity is attributed to the changing state of the LLM-based agent. Is this primarily due to the evolving interaction history (exemplars) in the prompt, or are there other factors? Have you conducted any ablations to isolate the primary source of non-stationarity?\n3. The computational cost of EXPO seems non-trivial, involving frequent LLM calls for action selection, neural network training in every iteration, and embedding computations for a large domain of prompts. Could you provide a rough analysis of the computational overhead compared to the base algorithms (OPRO, LLM-MAB)? Are there strategies to reduce this cost for more practical deployment? Was there any issue with training stability or catastrophic forgetting, and if so, how was it addressed (e.g., learning rate schedules, replay buffers)?\n4. The experimental task in this paper is relatively simple, and it is suggested to add more comparative experiments. It is suggested to add more experiment scenarios, and comparison with non LLM based classical optimization methods (such as genetic algorithm and Bayesian Optimization) on TSP tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iK3aiwKeCs", "forum": "1H1HrHCHyz", "replyto": "1H1HrHCHyz", "signatures": ["ICLR.cc/2026/Conference/Submission18009/Reviewer_ccsB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18009/Reviewer_ccsB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983294286, "cdate": 1761983294286, "tmdate": 1762927801377, "mdate": 1762927801377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles meta-prompt optimisation. The authors propose EXPO, an EXP3-inspired scheme that (i) builds a discrete domain of task descriptions and meta-instructions via LLM paraphrasing, (ii) trains a small NN over embeddings to estimate arm scores, accumulates these estimates, and (iii) samples the next meta-prompt from a softmax over cumulative estimates; they then extend this to EXPO-ES to also optimize exemplar subsets and order with a second adversarial bandit. \n\n1. While the shift from stochastic to adversarial bandits for meta-prompts is the central idea, much of EXPO is a reasonable assembly of known parts: LLM-generated domains, embedding-based reward regression, and exponential-weights sampling. The paper claims prior prompt-opt methods are “unsuitable” due to non-stationarity, but evidence is largely empirical; a stronger conceptual/theoretical separation from, e.g., neural-bandit prompt optimisers is missing. I would ask the authors to prove a regret advantage (even with approximation) over stochastic-MAB-style optimisers under stated assumptions. Otherwise, I find it hard to understand this shift. \n\n2. I would like to kindly ask the authors to conduct an analysis of what happens under score-estimation misspecification. \n\n3. Please report diversity metrics (embedding spread, lexical diversity), performance vs k, and failure cases. It would be great to understand its robustness better. \n\n4. Please include at least one agent benchmark AgentBench, and a coding one to show that their method indeed helps in planning. \n\n5. Am I right to understand that EXPO-ES maintains histories of NN parameters and recomputes cumulative estimates over randomly sampled exemplar sequences each iteration? If so, this can be computationally heavy and potentially unstable as |Θ| grows. This paper gives a simplified variant only for small-arm MAB, but not a scalable recipe for large exemplar pools. Please help me understand if this is not correct. \n\n6. Why adversarial? Can you provide a synthetic setting with controlled non-stationarity showing stochastic-MAB prompt optimisers fail while EXPO succeeds (with a regret gap)?\n\nI am happy to change my score if the questions mentioned above are answered."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Please see above"}, "weaknesses": {"value": "Please see above"}, "questions": {"value": "Please see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5X5GoKATqU", "forum": "1H1HrHCHyz", "replyto": "1H1HrHCHyz", "signatures": ["ICLR.cc/2026/Conference/Submission18009/Reviewer_GULo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18009/Reviewer_GULo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18009/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762079670396, "cdate": 1762079670396, "tmdate": 1762927800987, "mdate": 1762927800987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}