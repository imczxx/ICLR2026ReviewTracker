{"id": "1BXojAgNrg", "number": 25508, "cdate": 1758368714274, "mdate": 1759896718544, "content": {"title": "MedAraBench: Large-scale Arabic Medical Question Answering Dataset and Benchmark", "abstract": "Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.", "tldr": "", "keywords": ["Dataset Benchmark", "Large Language Models", "Arabic Natural Language Processing", "Medical Question Answering"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6fd11f6243e10a4727f8831dc5dc6ecc43837990.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MedAraBench, a large-scale Arabic medical QA benchmark of 24K multiple-choice questions across 19 specialties and five difficulty levels. The dataset was digitized from Arabic medical exams, manually filtered, and evaluated by human experts and through an “LLM-as-a-judge” framework. Eight LLMs (open-source and proprietary) are benchmarked in zero-shot settings. The goal is to provide a specialty-diverse Arabic medical QA benchmark for evaluating LLMs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Significant manual data cleaning and digitization effort, which adds credibility and quality to the dataset.\n\n2. Diverse specialty coverage and structured annotation across difficulty levels, ensuring representativeness within the medical domain.\n\n3. Inclusion of a human expert evaluation component, which is commendable and adds qualitative depth to the study.\n\n4. Contributes to Arabic NLP, a domain with limited existing benchmarks and resources."}, "weaknesses": {"value": "1. Unjustified selection of evaluator LLMs (Section 3.2.2)\nThe paper provides no justification for the selection of the three LLMs used as evaluators in the LLM-as-a-judge setup. There is no discussion of why these particular models were chosen, nor any rationale for excluding medical or Arabic-specialized LLMs, such as BiMediX (arabic+medical) or Fanar(arabic) or medgemma(medical+multilingual) etc .\n A more rigorous approach would have been to compare multiple candidate evaluators and measure their correlation with human expert scores (e.g., Pearson or Spearman coefficients) to identify which LLM aligns best with human judgment.\n\n2. Missing mention of existing Arabic medical benchmarks (Table 1, lines 058–059). \nThe comparison table omits BiMediX (arXiv:2402.13253), which already provides Arabic translations of MedQA and MedMCQA.\n\n3. Unsupported validation and overstated conclusions (lines 360–362, Table D2)\nThe conclusion that “ the potential of LLMs to be used for data quality evaluation in the medical domain” (lines 362–363) is overstated and empirically unsupported. The only evidence presented is a superficial similarity in average scores between human and LLM evaluations (Table D2). However, this does not constitute proof of agreement or reliability.\nSeveral methodological issues invalidate the comparison:\n- Different evaluation scales: Human experts used a binary high/low rating, while LLMs used a 1–5 Likert scale, making numerical averages non-comparable.\n- Different samples: Humans and LLMs did not evaluate the same subset of data\n- No agreement metrics: No statistical measure of correlation or agreement between human evaluators and the LLM judges (e.g., Pearson, Spearman, or Cohen’s κ) is reported. \n- Moreover, the statement in lines 360–362 that : “While they are not directly comparable due to varying evaluation scales, we note that the results of LLM-as-a-judge and expert quality evaluation are comparable.” is internally contradictory comparability cannot be claimed if the scales and samples differ.\nThe clause “pending further alignment with medical standards” implicitly acknowledges this weakness, but does not substitute for empirical validation.\n\n4. Unjustified use of the Likert 1–5 scale \nThe 1–5 scale is applied without defining intermediate values (2–4) in the prompt (Appendix C), and no rationale is provided for using a 5-point scale instead of a binary one matching the human evaluation. This undermines comparability and interpretability.\n\n5. Absence of medical Arabic LLM baselines (Section 3.3)\nAlthough the work benchmarks several proprietary and open models (GPT-5, Gemini, Claude, etc.), no Arabic or medical LLMs are tested, despite the availability of models like BiMediX (Arabic and medical), medgemma (medical and multilingual) etc.\n\n6. Invalid cross-benchmark and cross-model comparisons and flawed analysis of model progress (discussion lines 384–394; Table D1)\nIn the discussion (lines 384–394), the authors claim to observe “evolution of model performance across generations”.\n This analysis is methodologically invalid, as it compares different models on different benchmarks (MedArabiQ vs. MedAraBench).\n Because neither the models nor the datasets are constant, performance differences cannot be attributed to either factor.\nTable D1 seems intended to show that MedAraBench might provide a more informative or challenging evaluation, but the comparison is not correctly designed, and the caption does not clarify what the table represents.\n The two columns correspond to different model generations, making them not directly comparable.\nThis is a missed opportunity:\n If the authors had evaluated the same models on both MedArabiQ and MedAraBench, they could have shown whether the new benchmark is more challenging and thus more valuable.\n Alternatively, testing different generations of the same model family (e.g., GPT-4 vs GPT-5) on MedAraBench would have allowed a valid analysis of progress over time.\n As it stands, the setup conflates dataset variation with model advancement, so conclusions about “model evolution” are unsupported.\n Both the discussion and Table D1 should be revised: either clarify that the comparison is descriptive or conduct controlled, same-model evaluations.\n\n7. Invalid comparison of models (lines 365–367)\nThe authors conclude that “proprietary models outperform open-source models,” yet the proprietary models are orders of magnitude larger than open ones. Such comparisons are meaningless without controlling for scale.\n\n8. Dataset imbalance (Figure 2a, Table A2)\nOver 56 % (Figure 2a, Table A2) of questions are Year-1 level and only 5 % Year-5, resulting in a dataset dominated by basic-science items. This imbalance likely makes the benchmark less challenging and limits its capacity to assess advanced reasoning.\n\n9. Suspicious perfect accuracies without explanation (Table 3)\nIn Table 3, several models report perfect accuracies (1.0) for the ABCDEF configuration, while scores on other subsets remain between 0.55–0.77.\nThis sudden jump to perfect accuracy across models is highly suspicious and atypical for medical QA tasks.\nNo explanation or investigation is provided. The authors should have clarified whether the ABCDEF subset:\n- contains very few items (inflating accuracy), \n- includes only Year-1 questions (simpler), or \n- whether the addition of letter choices (A–F) helped models guess the correct answer (e.g., positional or formatting cues).\nWithout such clarification, the results appear unreliable and raise concerns about evaluation validity.\n\n10. Limited novelty and under-utilization of the dataset\nWhile the dataset is valuable for Arabic medical NLP, the contribution is incremental rather than conceptual, there is no new evaluation framework or modeling insight beyond prior work (MedArabiQ). The paper advertises ~24K MCQs, yet only ~4.9K test items are actually used in experiments; the ~20K training split is never explored (no fine-tuning, few-shot, or human/LLM evaluation on train). As a result, the empirical scope is limited to the test set, leaving the benchmark largely under-utilized. The most tangible contribution remains the digitization and manual curation of Arabic medical MCQs."}, "questions": {"value": "Questions:\n\n1. Did you test Arabic or medical-specialized LLMs as potential judges?\n\n2. How is the “Average (Fraction of 5)” metric in Table 3 calculated?\n\n3. How do explain the fact that several models reach perfect (1.0) accuracy in the ABCDEF configuration?\n\n4. What does Table D1 intend to represent, benchmark comparison or model evolution?\n\n5. Could you share the prompts used to evaluate the benchmarked models, including input format, language setup, and answer extraction method?\n\nRemarks:\n\n1. Misplaced or unclear citation (line 099).\nThe citation to the GPT-4 technical report (Achiam et al., 2023) does not logically connect to the preceding sentence. If the authors meant to refer to GPT-4 being evaluated on translated benchmarks, the sentence should be rephrased for clarity.\n\n2. Missing cross-reference (lines 170–171)\nIn Section 3.1, methodological details are discussed without referencing the appropriate subsection (Section 4.1), reducing readability.\n\n3. Lack of explanation for evaluation platform (line 215)\nThe authors mention that expert evaluations were conducted using Qualtrics, yet they do not explain what it is nor provide a footnote or citation.\n\n4. Incomplete sentence (line 331)\n\n5.  Invalid link (line 452): \nThe repository link (https://anony-mous.4open.science/r/medarabench-3BE4/) is inaccessible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gW5UQZbnvw", "forum": "1BXojAgNrg", "replyto": "1BXojAgNrg", "signatures": ["ICLR.cc/2026/Conference/Submission25508/Reviewer_BRmF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25508/Reviewer_BRmF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816138886, "cdate": 1761816138886, "tmdate": 1762943455955, "mdate": 1762943455955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MedAraBench, a benchmark for evaluating LLMs on Arabic medical MCQ task. The dataset contains about 24k questions across 19 medical domains and 5 difficulty levels, manually digitized from Arabic medical school materials. The authors conduct both clinical expert evaluation and LLM-as-a-judge assessments to measure data quality, finding moderate agreement and generally acceptable accuracy. They then benchmark open-source and proprietary models like GPT-5, Gemini 2.0, and Claude 4, showing that proprietary models outperform open-source ones but still fall short of expert level accuracy. The paper provides a structured resource for testing Arabic medical tasks but is mainly limited to multiple-choice formats and zero-shot evaluations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The manual digitization and expert validation of data from non-digital academic sources shows significant effort and ensure the dataset’s authenticity and reliability.\n- The dataset spans 19 medical specialties at various difficulty levels, offering a structured framework that supports fine grained evaluation of LLM performance across various domains of medical knowledge for the Arabic language."}, "weaknesses": {"value": "- For validating data quality using LLM-as-a-judge, the authors employ GPT-4, Gemini 1.5 Pro, and Claude 3.5 Sonnet. However, there is no justification provided for selecting these specific models, Are they known to outperform others in Arabic understanding? Moreover, the prompt instructs the models to act as medical education expert but does not account for the Arabic language aspect of the task. The capability of these models in Arabic medical understanding needs further evaluation.\n- The literature review and experimental comparisons are not comprehensive. Open-source models like BiMediX [1] have benchmarked Arabic medical tasks and have released translated and verified datasets. Additionally, compare with more medical open-source models like Apollo [2], Med42 [3], Meditron [4]. Proprietary models like Gemini 2.5 Pro [5] and Flash are also missing from the evaluations.\n- If LLM as a judge is an automated framework to evaluate the quality of the proposed dataset. Extending this validation to the training set could help further filter and enhance the quality of the data. Currently, the validation is limited only to the test set.\n\n[1] *Pieri, Sara, et al. \"Bimedix: Bilingual medical mixture of experts llm.\" arXiv preprint arXiv:2402.13253 (2024).*\n\n[2] *Wang, Xidong, et al. \"Apollo: A lightweight multilingual medical LLM towards democratizing medical AI to 6B people.\" arXiv preprint arXiv:2403.03640 (2024).*\n\n[3] *Christophe, Clément, et al. \"Med42-v2: A suite of clinical llms.\" arXiv preprint arXiv:2408.06142 (2024).*\n\n[4] *Chen, Zeming, et al. \"Meditron-70b: Scaling medical pretraining for large language models.\" arXiv preprint arXiv:2311.16079 (2023).*\n\n[5] *Comanici, Gheorghe, et al. \"Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.\" arXiv preprint arXiv:2507.06261 (2025)*"}, "questions": {"value": "Please address the above weaknesses. \n- How were the medical specialties determined for the data samples? Was this an automated process or done with the help of clinical experts?\n- Line 197: “No Cueing: options do not provide clues to other answers.” Is it meant to be “Clueing”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FfPDIwGxcn", "forum": "1BXojAgNrg", "replyto": "1BXojAgNrg", "signatures": ["ICLR.cc/2026/Conference/Submission25508/Reviewer_m6wR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25508/Reviewer_m6wR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840517725, "cdate": 1761840517725, "tmdate": 1762943455738, "mdate": 1762943455738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a benchmark for medical knowledge in Arabic. The dataset comprises multi-choice questions in 19 medical specialities, extracted and refined manually from exams and lecture notes of medical schools in the Arabic-speaking world, then quality-checked by experts and LLM-as-a-judge scheme. The paper presents benchmarking results for a number of SOTA LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a useful resource for Arabic medical understanding, and benchmarking results for a number of SOTA models."}, "weaknesses": {"value": "Some missing details about the construction of the dataset and the implementation of the benchmarking are mentioned in Questions below"}, "questions": {"value": "- How is it that Arabic is underrepresented in the medical domain because of its rich morphology and dialectal variation? And why the uneven linguistic landscape calls for medical LLMs? The motivation of this work in Sec.1 should be revised.\n\n- The reference to Appendix A is missing in Sec. 3.1\n\n- MedArabiQ is missing from Table 1\n\n- How were lecture notes converted to meaningful MCQs? This is an important detail that deserves to be discussed.\n\n- The LLMs chosen for benchmarking do not include any Arabic-focused model. Why not? The differences in the benchmarking results could be due to the models' inherent limitation in Arabic itself rather than in medical knowledge, where bigger, proprietary models have an edge, but this angle is not explored.\n\n- Line 331 (Sec 4.3) is truncated.\n\n- How big were the subsets of MCQ? Guessing from Fig. E3. subset ABCDEF has only 2 questions in the test set and 9 question overall, so it might be better not to be considered at all as a separate category.\n\n- In Sec. 5 (Discussion) and Table D1, how can MCQ scores of different benchmarks be at all comparable?\n\n- How did the authors homogenize medical terminology in their dataset given the lack of standardization in the Arabic medical domain?\n\n- In Appendix A, how comes the average question length is just over 8 characters?\n\n- How was the MCQ benchmarking implemented? e.g. using logit ranking or post-processing of model answer for choice character?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DCOrVbbaTi", "forum": "1BXojAgNrg", "replyto": "1BXojAgNrg", "signatures": ["ICLR.cc/2026/Conference/Submission25508/Reviewer_ratc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25508/Reviewer_ratc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25508/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005385935, "cdate": 1762005385935, "tmdate": 1762943455488, "mdate": 1762943455488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}