{"id": "lYNsZdKn5R", "number": 1991, "cdate": 1756974880022, "mdate": 1759898174714, "content": {"title": "Steerable Adversarial Scenario Generation through Test-Time Preference Alignment", "abstract": "Adversarial scenario generation is a cost-effective approach for safety assessment of autonomous driving systems. \nHowever, existing methods are often constrained to a single, fixed trade-off between competing objectives such as adversariality and realism. This yields behavior-specific models that cannot be steered at inference time, lacking the efficiency and flexibility to generate tailored scenarios for diverse training and testing requirements.\nIn view of this, we reframe the task of adversarial scenario generation as a multi-objective preference alignment problem and introduce a new framework named Steerable Adversarial scenario GEnerator (SAGE). SAGE enables fine-grained test-time control over the trade-off between adversariality and realism without any retraining. We first propose hierarchical group-based preference optimization, a data-efficient offline alignment method that learns to balance competing objectives by decoupling hard feasibility constraints from soft preferences. Instead of training a fixed model, SAGE fine-tunes two experts on opposing preferences and constructs a continuous spectrum of policies at inference time by linearly interpolating their weights. We provide theoretical justification for this framework through the lens of linear mode connectivity. Extensive experiments demonstrate that SAGE not only generates scenarios with a superior balance of adversariality and realism but also enables more effective closed-loop training of driving policies.", "tldr": "We introduce a new paradigm for adversarial scenario generation with test-time steerability.", "keywords": ["Adversarial Scenario Generation", "Autonomous Driving", "Traffic Modeling", "Test-time Alignment"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5fad00d198f5c41d887de211180abf5d23716158.pdf", "supplementary_material": "/attachment/748add4292ce82e143233fb13a757130469b8110.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a method to fine-tune a pre-trained motion generation model to generate adversarial scenarios. The post-training would result in two separate expert models excelling at demonstrating realistic behavior and adversarial behavior, respectively. Model merging by linear interpolation then allows us to control the trade-off between realism and the capability of inducing collision. The model shows good performance in the open-loop benchmark, showing a more natural behavior than other baselines with a nearly SOTA collision rate. The following RL experiments show that agents trained with the adversarial scenario generated by the proposed model achieve SOTA in both nominal scenarios and long-tail scenarios, outperforming agents trained in scenarios generated by other baselines. Further study shows the effectiveness of model merging and the post-training method HGPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Long-tail/near-accidental/safety-critical scenario generation is important for real-world robotics applications. In this field, adversarial policy modeling is important, which trades off the realism of the generated trajectories. The method proposed in this paper strikes a good balance between these two contradictory goals and enables controlling the extent to which the adversarial behavior should be.\n2. The experiment is comprehensive, and the results indeed demonstrate the usefulness of the proposed method. It shows the method can achieve an SOTA attack success rate while maintaining a good trajectory realism. \n3. The model merging and proposed post-training method, HGPO, is validated to be useful and plays the key role of the success of the whole pipeline.\n4. Paper writing is easy to understand, and experimental details are well-documented"}, "weaknesses": {"value": "In the RL experiment, the RL agents trained in scenarios generated by different methods should be benchmarked in their respective generated test splits as well. For now, SAGE/CAT/Rule-based agents are benchmarked in scenarios generated by only SAGE and log-replay. It makes me wonder whether the superior performance of the SAGE agent comes from overfitting due to the similarity between the training and testing split, as they are all generated by the SAGE method.\nIt would be better to additionally benchmark all 3 agents in scenarios generated by the CAT and Rule-based method. The best results we can expect are that the SAGE agent shows comparable even better, results than CAT agents on CAT-generated held-out scenarios. But even if it is a bit worse, the good results of SAGE agents on the log-replay benchmark can still demonstrate another benefit of SAGE: maintaining the nominal driving ability of RL trained policies."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "93ujgd58Np", "forum": "lYNsZdKn5R", "replyto": "lYNsZdKn5R", "signatures": ["ICLR.cc/2026/Conference/Submission1991/Reviewer_jZqG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1991/Reviewer_jZqG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757710106, "cdate": 1761757710106, "tmdate": 1762915987141, "mdate": 1762915987141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This research addresses limitations in existing adversarial scenario generation methods for autonomous driving, which often lack flexibility to tailor scenarios for specific needs. The authors introduce SAGE (Steerable Adversarial scenario GEnerator), a new framework that allows fine-grained control over the balance between adversariality and realism during scenario creation without retraining. By using preference optimization and interpolating expert policies, SAGE generates more effective training data and improves driving policy performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Walking along Pareto frontier between realism and adversarial objectives\n* Ability to do it w/o retraining\n* Correct (at least looking to be) theoretical statement about the suboptimality gap limited by weighted squared norm of weights difference from above\n* Correct (at least looking to be) theoretical statement about the advantage of weight mixing over explicit output ensemble"}, "weaknesses": {"value": "* Eq. (8) and lines 304-305 (+ additionally in the Appendix C.3): that's still not clear why the first term - benefit from path flatness - is the dominant one. Would be nice to see any proof, empirical or (better) theoretical one\n* It's not clear how preference labels (see \"Preference within Feasibility\", lines 196-198) are generated\n* No ablation on $\\delta_{m}$ (line 198); moreover, is it the same as $\\sigma = 0.2$ (line 902)? \n* Appendix B.2: unclear, what are values of $\\omega_{turn}$ and $\\omega_{stop-turn}$ in Eq. (13)\n* Quite unclear, why the approach is not that useful for IDM in comparison to Replay that makes it questionable to use in practice (Appendix D.1)\n* Minor remark: need to define $R_{real}$ earlier (as the negative expectation of $P_{real}$ on page 1373)\n\nOverall, the theoretical concepts brought to autonomous driving really makes the reviewed paper suitable for ICLR conference."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TuBqqcapf5", "forum": "lYNsZdKn5R", "replyto": "lYNsZdKn5R", "signatures": ["ICLR.cc/2026/Conference/Submission1991/Reviewer_be6Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1991/Reviewer_be6Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788340497, "cdate": 1761788340497, "tmdate": 1762915987012, "mdate": 1762915987012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reframes adversarial scenario generation for autonomous driving as a multi-objective preference alignment problem and proposes SAGE. Contributions are: (i) Hierarchical Group-based Preference Optimization (HGPO), which decouples hard feasibility (map compliance) from soft preferences (adversariality vs. realism) by gating infeasible samples with a binary feasibility function $F(\\tau,\\mathcal M)$ and learning from group-wise, multi-pair preferences within the feasible set to improve sample efficiency; (ii) training two expert policies from a shared pretrained model (an adversarial-leaning expert and a realism-leaning expert) and interpolating their weights at test time to steer along the trade-off; and (iii) integrating SAGE into an existing closed-loop RL framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "SAGE provides test-time steerability across the adversariality–realism trade-off, and handles feasibility by treating map rules as hard constraints. Empirically, in open-loop benchmarking, SAGE traces a better realism for a given adversarial reward and achieves lower map-violation penalties while remaining competitive in attack metrics; figures and tables exemplify smooth control as the adversarial weight increases."}, "weaknesses": {"value": "1. **Novelty concerns:** The core ideas (i.e., preference alignment and weight-space interpolation between experts) may feel incremental, which explicitly build on prior literature. Please emphasize the novelty of SAGE. The key idea includes filtering out map-infeasible samples (F=0) from the preference set, and also constructing group-wise, multi-pair preferences within the feasible set and optimizing a reference-regularized HGPO objective. That said, the contribution remains incremental within the existing preference-optimization paradigm rather than a fundamentally new algorithmic framework.\n\n2. **Generation Efficiency:**  Against the Replay policy, SAGE and its variations report lower attack success rates and lower rewards than their baselines, raising questions about the effectiveness of the generated adversarial behaviors. \n\n3. **Closed-loop result discrepancy and fairness:** In the paper’s closed-loop results, CAT underperforms Replay (no adv) on completion/collision, whereas the original CAT paper typically reports gains from adversarial training. Please explain why CAT underperforms here and provide diagnostics to ensure a well-tuned baseline. Besides, CAT’s closed-loop results here appear worse than those reported in the original paper in terms of collision and completion; this gap should be reconciled as well."}, "questions": {"value": "1.  Baselines include Rule, CAT, KING, AdvTrajOpt, and SEAL, but omit GOOSE (2024) from related work; Is there a reason why it doesn't appear in the experiments as a baseline method?\n\n2. I wonder how diverse generated adversarial behaviors are. Could authors give visualizations or quantitative results on this perspective?\n\n3. please address issues in point 3. in weakness.\n\n4. What is the expert reward/penalty design (component weights, margins, clipping constants, and final thresholds actually used in experiments)? What is the full pre-training recipe for the shared reference model (e.g., optimizer, learning-rate schedule, batch size, training duration/epochs, data splits and augmentations) used for DenseTNT, or whether a public checkpoint was used? for HGPO fine-tuning, what are the concrete values for group size (N), pair-selection margin ($\\delta_m$), the number of candidates (K) per context, and any additional regularization or gradient-scaling coefficients?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3l9VwzkBgy", "forum": "lYNsZdKn5R", "replyto": "lYNsZdKn5R", "signatures": ["ICLR.cc/2026/Conference/Submission1991/Reviewer_ZEJW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1991/Reviewer_ZEJW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111341302, "cdate": 1762111341302, "tmdate": 1762915986866, "mdate": 1762915986866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method for generating adversarial scenarios for driving by interpolating weights between two experts trained on opposing preferences. In contrast to previous works, SAGE’s approach balances adversarial behavior with realism by taking advantage of Linear Mode Connectivity, which enables adversarial behavior to be explored at test-time rather via re-training. Overall, I find this approach to be novel, useful, and interesting."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The motivation and methodology are well-written and well-justified. \n- Additional results in supplementary is helpful. \n- Authors provide very extensive details on experiment setup. Without the  code provided, I feel as though the work is reproducible based on the text.\n- Authors show theoretical and empirical applicability of LMC to traffic simulation problems.\n- Results are comprehensive and provide actionable insight to the simulation community."}, "weaknesses": {"value": "- Even though a lot of information is referenced and provided in the appendices, I found it difficult to interpret results off first glance in the main body. I suggest the authors make captions of Figures and Tables more self-contained and avoid having the reader look for references. It would be nice to know off first glance 1) what policies govern adv and ego vehicles, and 2) if certain qualitative results correspond to any tabular results. For example, I found Figure 2 difficult to interpret. Even though the images show that SAGE produces more realistically adversarial behavior, I’m not exactly sure what policy the ego vehicle is following. The main body also does not seem to specify. \n- Some notation definitions are missing (see question 1 below)."}, "questions": {"value": "1. Eq 2. Not sure if I missed something, but I’m a bit confused at the -P_{real} term in the reward definition. “P_{real} ensures tau to adhere to the patterns of naturalistic driving; it should be statistically plausible.. “. To me, it would make sense that the objective should reward both adversarial and realistic behavior. Is P_{real} supposed to denote the probability of the trajectory being drawn from the distribution of realistic trajectories? In which case, shouldn’t the term’s sign be flipped (unless it denotes NLL)?\n2. Table 1: Are the baselines (other than IDM) trained with a similar reward objective, even if it is the naive objective corresponding to a single point on the Pareto frontier as mentioned before? \n3. (Appendix) Table 6/7: How is KING producing a distributional difference of ~256? It would be nice if authors discussed this / provided examples to demonstrate this failure mode. \n4. Figure 2: What policy is governing the ego vehicle? Tables 1 and 2 present results against two different ego vehicle policies, so I am confused about which results Figure 2 correspond to.\n5. At what point (if any) of increasing W_{adv} result in violation of map rules? Does the enforced “feasibility first” inductive bias ensure that all adversarial agents avoid violating map constraints, even if W_{adv} increases? \n6. A very common adversarial scenario where the adversarial vehicle is at fault is rear end collisions (the ego vehicle is in front). Does the adversarial policy demonstrate this behavior when the log begins with the adversarial agent behind the ego vehicle in the same lane? I noticed that a lot of the qualitative examples (Figure 11) demonstrate cut-in behavior. I was curious if the adversarial policy was mostly limited to cut-ins, or if there are other types of adversarial behavior types."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fWlSCUf5d4", "forum": "lYNsZdKn5R", "replyto": "lYNsZdKn5R", "signatures": ["ICLR.cc/2026/Conference/Submission1991/Reviewer_mqnz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1991/Reviewer_mqnz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1991/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146815023, "cdate": 1762146815023, "tmdate": 1762915986720, "mdate": 1762915986720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}