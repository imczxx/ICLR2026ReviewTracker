{"id": "p2HAR5Qz6C", "number": 11809, "cdate": 1758203995751, "mdate": 1763092295038, "content": {"title": "Unified Pose Embeddings: Utilizing Euclidean Space for Simplified Topology Alignment", "abstract": "Generative models for human motion synthesis have demonstrated remarkable capabilities across tasks such as text-to-motion generation, motion inbetweening, style transfer, and motion captioning. \nHowever, their adoption in industry remains limited, largely due to challenges in data representation. \nIndustry applications often require diverse articulated skeleton topologies tailored to specific use cases, which are further constrained by limited data availability. \nExisting methods address these challenges by aligning datasets through shared subsets or unified representations. However, these approaches rely on error-prone alignment processes, limiting their flexibility and scalability.\nIn this work, we leverage Euclidean space to represent human poses, bypassing the need for alignment in configuration space and significantly simplifying the learning objective.\nUsing Euclidean space also frees us from the need to use a common subset representation and allows us to represent poses in any complexity we desire.\nTo disentangle pose and body shape, we introduce a simple yet effective learning strategy.\nOur method achieves robust inverse kinematics with minimal data requirements, needing just over five minutes of motion capture data to integrate new topologies. \nWe demonstrate the effectiveness of our topology-agnostic representation across three downstream tasks: motion retargeting, text-to-motion generation, and motion captioning.", "tldr": "Skeleton-agnostic pose representation", "keywords": ["Human Pose"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/bfbd0cdaefa0f12d13653388135244bc806f319a.pdf", "supplementary_material": "/attachment/00fcb8b4865311d322bbaba4ccf6413e196aa637.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the long-standing challenge of representing motion across diverse human skeleton topologies in generative human motion modeling. It proposes a topology-agnostic motion representation that learns in Euclidean space rather than joint rotation space. Specifically, for each skeleton topology, joint rotation data is converted to Euclidean coordinates via forward kinematics, after which a learned projector maps the pose into a shared anchor space. An autoencoder is then used to encode the pose into a latent space and reconstruct it within the anchor skeleton. Finally, the reconstructed anchor pose is mapped back to the target topology, and a learned inverse kinematics network projects the reconstructed Euclidean pose back into the joint rotation space. The paper compares the proposed method with several baseline approaches across different applications, including motion reconstruction, motion retargeting, and motion generation, and reports consistent improvements on these tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper raises an interesting and timely question about how motion representation affects generalization in generative motion modeling, particularly by contrasting Euclidean-space representations with traditional joint rotation spaces.\n\n- The proposed method is straightforward, conceptually sound, and easy to implement, making it both practical and broadly applicable.\n\n- Extensive experiments are conducted across multiple applications—including motion reconstruction, retargeting, and motion generation—demonstrating the versatility and effectiveness of the proposed representation."}, "weaknesses": {"value": "- Although the overall idea is straightforward, some methodological details are difficult to follow. For instance, the roles of \\(x_{-}\\) and \\(x_{=}\\) are not clearly explained, and it is unclear how these representations contribute to the overall learning pipeline.  \n\n- In the experiments, the paper compares the proposed modular framework with baseline methods such as NKN, Skeleton-Aware Networks, and SAME. However, these baselines are designed as universal models that handle multiple skeletons within a single network, whereas the proposed approach relies on modular components trained separately. This difference raises concerns about the fairness and validity of the comparisons.  \n\n- The architectural novelty of the method is limited, as it primarily combines standard components such as autoencoders and MLP-based inverse kinematics networks. While the simplicity of the design is appreciated, the paper would be stronger with deeper analysis or insight into *why* and *how* representing poses in Euclidean space leads to the observed performance improvements."}, "questions": {"value": "- In Section 4.1, the paper studies the learned inverse kinematics results. Could the authors clarify whether there are any new technical contributions or innovations in how the inverse kinematics model is learned, beyond adopting an MLP-based approximation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WUmX1DNY4d", "forum": "p2HAR5Qz6C", "replyto": "p2HAR5Qz6C", "signatures": ["ICLR.cc/2026/Conference/Submission11809/Reviewer_F16V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11809/Reviewer_F16V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623425611, "cdate": 1761623425611, "tmdate": 1762922830532, "mdate": 1762922830532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Thank you - Withdrawal"}, "comment": {"value": "The consensus among the reviewers is that the paper is not ready for publication, and we thus withdraw our submission. We thank all the reviewers for the time that they took to read and critic our work.\n\nWe will individually respond to each reviewer to answer their questions and comment on some of the criticisms. In appreciation of everyone’s time we do NOT want to start a discussion (as we are withdrawing the paper and will not be able to comment anyway)."}}, "id": "T2pjoYWpcn", "forum": "p2HAR5Qz6C", "replyto": "p2HAR5Qz6C", "signatures": ["ICLR.cc/2026/Conference/Submission11809/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11809/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11809/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763091821621, "cdate": 1763091821621, "tmdate": 1763091821621, "mdate": 1763091821621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "The consensus among the reviewers is that the paper is not ready for publication, and we thus withdraw our submission. We thank all the reviewers for the time that they took to read and critic our work.\n\nWe individually responded to each reviewer to answer their questions and comment. We highly appreciate the valuable feedback that we have received."}}, "id": "2Pqt6KOlhT", "forum": "p2HAR5Qz6C", "replyto": "p2HAR5Qz6C", "signatures": ["ICLR.cc/2026/Conference/Submission11809/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11809/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763092294174, "cdate": 1763092294174, "tmdate": 1763092294174, "mdate": 1763092294174, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to unify the humanoid skeletal pose representation with an embedding learned from joint positions, or Euclidean space, as in the paper. Different humanoid skeletal morphologies are canonicalized to the SMPL-H skeleton (\"anchor\" skeleton as termed in the paper) using an off-the-shelf retargeter or mapping to preset skeletons (without fingers, with simplified fingers, and with full fingers) to learn the mappings between the anchor skeleton and the source skeletons. From the canonicalized joint positions, the method learns the latent pose space with an encoder-decoder architecture. The challenge here is to learn the latent pose representation agnostic to the skeletal proportions (phrased as \"body shape\" in the paper) described by the \"skeleton offsets\" (translations relative to the parent joint). For this disentanglement, the encoder outputs both z_o (\"skeleton offset\" latent) and z (pose latent). An auxiliary decoder is then trained to recover the skeleton offset given z_o. The pose decoder is also trained to recover the joint positions given the pose latent and the augmented skeleton offset latent (output from another MLP given skeleton offset). The skeleton offset augmentation randomly scales up, down, and samples skeleton offsets from other characters to encourage the pose latent to be agnostic to the skeleton offsets. The setup is numerically evaluated in errors in IK, reconstruction, and retargeting. The applications in retargeting, text-to-motion, and motion-to-text (captioning) are demonstrated."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The goal to propose a unified pose representation is ambitious."}, "weaknesses": {"value": "* Confusing writing\n  * It is fine to use words like Euclidean space / Cartesian coordinates, but essentially, all this paper is doing is learn with joint positions\n  * \"Body shape\" in this paper is not the shape (outer skinned geometry) as in SMPL and related papers. Instead, in the context of this paper, \"body shape\" refers to bone lengths or body proportions\n  * L264 and L267: why put loss notation next to the predicted quantity???\n  * L473: \"absence of global transformation\" ??? FK should put joint positions in the world space, and the joint positions should capture the global transformation\n* Questionable premise and insufficient discussions on its limitations\n  * The premise is that joint positions are better. But the autoencoder setup tries hard to eliminate the entanglement of the joint positions and the bone lengths (identity-specific feature). Then why not use joint rotations, which are not tied to bone lengths? There are previous papers (e.g., papers by Sebastian Starke and Daniel Holden) using the combination of joint positions and rotations for the pose representations.\n  * Poses/motions learned with joint positions embedded in the world space become translation and rotation-dependent. E.g., a motion translated on the ground plane should be recognized as the same motion. The proposed setup breaks this expectation.\n  * Positional representation will lose the original rotation information (twist along the bone direction). IK, whether learned or not, cannot recover this twist component, as this is ambiguous.\n* Insufficient visual presentation of results\n  * Skeletal poses and animations should always be presented with the skinning to make sure there are no twisting artifacts (stick figures of skeletons cannot tell the twist)"}, "questions": {"value": "Please answer the questions raised in the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZMGHSEjvHE", "forum": "p2HAR5Qz6C", "replyto": "p2HAR5Qz6C", "signatures": ["ICLR.cc/2026/Conference/Submission11809/Reviewer_DNnF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11809/Reviewer_DNnF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762513835, "cdate": 1761762513835, "tmdate": 1762922829535, "mdate": 1762922829535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Unified Pose Embeddings (UPE), a topology-agnostic human-motion representation learned in Euclidean joint space instead of configuration space. The method uses (i) an anchor representation (SMPL-H joints) with optional reduced presets x^{-}, x^{=}, x^{\\equiv} to cover datasets lacking paired anchors; (ii) a lightweight autoencoder that disentangles pose z from shape o via offset regression and shape augmentation; and (iii) a per-topology neural IK module (5-layer MLP, with optional gradient refinement) to map 3D joints back to rotations. Experiments report: strong IK accuracy across five datasets (Table 1–2), competitive retargeting on Mixamo with ~10 minutes of per-character data (Table 4, Fig. 6), text-to-motion with MDM/MARDM showing comparable recall and precision but somewhat higher FID (Table 5), and captioning results via k-means tokenization (Table 6). Claimed data efficiency: IK generalizes with ≈16k frames (~5.33 minutes at 50 Hz) for a new topology (Fig. 5)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- anchor representation + disentanglement + per-topology IK is easy to implement and extend; components can be trained independently.  \n- convincing curves and table ablations (linear vs MLP-2/MLP-5, with/without post-opt). The stricter accuracy metric (max-limb threshold) is thoughtful.  \n- x^{-}, x^{=}, x^{\\equiv} reduce reliance on fragile full retargeting when anchors are unavailable.  \n- shape-offset regression + augmentation measurably helps reconstruction/disentanglement (Table 3; PCA analyses)."}, "weaknesses": {"value": "- Although topology-agnostic is claimed, the pipeline anchors to SMPL-H and still requires learning g(\\cdot) and IK r(\\cdot) per topology; cross-domain generality is thus partly deferred to new training. Evidence on unseen, highly non-human or production rigs is limited.  \n- On text-to-motion, recall improves but FID worsens vs native HumanML3D; attribution to conversion shift is plausible but leaves the core question—does Euclidean anchoring improve generative quality—only partially answered. Captioning trails MotionAgent/GPT on alignment (Table 6).  \n- Authors note intra-structural style transfer is weaker (per-frame design; pose fidelity over style). Stronger baselines (e.g., SAME, AnyTop) are only partially covered, and the “cross-structural becomes trivial in Euclidean space” risks overstating difficulty reduction without broader rigs.  \n- No stress tests for noisy joints, missing markers, frame-rate variance, or domain shift. The “5.33 minutes” data claim depends on 50 Hz and Human3.6M; sensitivity to sampling rate and motion diversity isn’t quantified.  \n- Many results ultimately traverse anchor --> target or target --> anchor paths; cumulative conversion noise is acknowledged but not bounded with diagnostics (e.g., cycle errors per joint, hand articulation fidelity)."}, "questions": {"value": "1. How far can rigs deviate (extra twist bones, non-human proportions, non-tree constraints) before the preset/anchor approach fails? Any results on production rigs beyond SMPL/H36M/LAFAN?  \n2. How does the “~5.33 min” generalization change with frame rate, motion diversity, or label noise? Please plot accuracy vs. minutes at 25/30/60 Hz and with missing joints.  \n3. Can you report anchor-native evaluation (no conversion to HumanML3D) or a conversion-robust metric (e.g., Procrustes-aligned feature FID) to isolate representation benefits?  \n4. What are wall-clock train times and parameter counts for g(\\cdot) and r(\\cdot) per new topology? Any amortization via meta-learning or shared adapters?  \n5. Could a temporal/style head (or latent offset predictor) mitigate intra-structural style loss without sacrificing your pose-fidelity goal (cf. your Appendix A.9)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YpjVR6ftem", "forum": "p2HAR5Qz6C", "replyto": "p2HAR5Qz6C", "signatures": ["ICLR.cc/2026/Conference/Submission11809/Reviewer_orpB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11809/Reviewer_orpB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892335522, "cdate": 1761892335522, "tmdate": 1762922828952, "mdate": 1762922828952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose to use the euclidean joint positions to represent the human pose,\nwhich is a simple and effective way to represent the human pose across different skeletons.\nA set of latent encoder and decoder is used and latent space is trained to be disentangled betweenthe body shape and the pose"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I really like what the paper is studying and trying to solve.\nMy team actually had a similar discussion about the universal motion representation across different morphologies,\nand we believed \"end-effectors\" positions can be a potential candidate for this.\nThis is similar to the idea of euclidean space in the paper but the authors push the idea further by introducing a disentanglement learning strategy.\n\n1. The paper discuss a curical problem in animation,\nhow to handle retargetting across different proportions and morphologies.\n\n2. The paper provides multiple resolution of skeleton representation which is flexible to use for different applications.\n\n3. Disentanglement is studied in the paper, which opens the door to unsupervised motion feature learning.\nThis will allow us to learn the features from huge ton of skeleton morphologies and proportions."}, "weaknesses": {"value": "1. The paper does not seem finished or ready.\n\nThe result section is very weak and we are seeing very limited visual results.\nThere are no meshed character visualized at all except for figure 1, which is also not very informative.\nIt's hard to draw any conclusion from the results.\n\n2. The scalability of the number of characters or morphologies is not well studied.\nA small subsets of characters of in mixamo (4 i believe?) is not enough to make a general statement about the scalability for the proposed method.\n\n3. The algorithm does not consider character mesh, which is crucial to consider retargetting.\nThis is central in industry application and was briefly studied in [1, 2].\nThis needs to be studied before this retargeter can be useful for industry applications.\n\n[1] Ho, Edmond SL, Taku Komura, and Chiew-Lan Tai. \"Spatial relationship preserving character motion adaptation.\"\nIn ACM SIGGRAPH 2010 papers, pp. 1-8. 2010.\n[2] Yang, Lujie, Xiaoyu Huang, Zhen Wu, Angjoo Kanazawa, Pieter Abbeel, Carmelo Sferrazza, C. Karen Liu, Rocky Duan, and Guanya Shi. \n\"OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction.\" arXiv preprint arXiv:2509.26633 (2025).\n\n4. There's no studied about keeping the semantic meaning of the motion.\nFor example a slow walking might be retargeted into fast walking for a smaller person,\nwhich is common in traditional retargetting methods."}, "questions": {"value": "please refer to the above section.\nI think the project is going towards the right direction, but it's far from being ready to be viewed as a complete project."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K4u4c0R6Yx", "forum": "p2HAR5Qz6C", "replyto": "p2HAR5Qz6C", "signatures": ["ICLR.cc/2026/Conference/Submission11809/Reviewer_sv7N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11809/Reviewer_sv7N"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972548601, "cdate": 1761972548601, "tmdate": 1762922828531, "mdate": 1762922828531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}