{"id": "YCHE8YOqyx", "number": 12951, "cdate": 1758211961160, "mdate": 1759897474575, "content": {"title": "FiRL : Finslerian Reinforcement Learning for Risk-Aware Anisotropic Locomotion", "abstract": "Legged locomotion is inherently anisotropic and risk-sensitive: the energy cost and risk of failure vary significantly with the direction and speed of motion. Standard reinforcement learning (RL) methods neglect this asymmetry, typically using isotropic cost/reward functions and optimizing only for expected returns. This leaves agents vulnerable to rare but catastrophic outcomes. We propose Finslerian Reinforcement Learning (FiRL), a novel RL framework that integrates a Finsler metric into the cost function for directional energy-awareness, and optimizes a Conditional Value-at-Risk ($CVaR_\\alpha$) objective for tail-risk robustness. FiRL formulates the locomotion cost as $F(x,v)$, a Finsler metric that varies with state $x$ and motion $v$, capturing uphill vs.\\ downhill effort, lateral friction, and other direction-dependent costs. We derive a risk-sensitive Bellman equation based on $CVaR$ and prove that the corresponding CVaR–Finsler Bellman operator is a $\\gamma$-contraction, yielding a unique fixed-point value function that induces a quasi-metric structure (satisfying a triangle inequality despite asymmetry). We develop a FiRL actor–critic algorithm to learn policies under this anisotropic, risk-averse objective. In simulated MuJoCo locomotion benchmarks, FiRL achieves safer and more energy-efficient behaviors than SOTA baselines (e.g., risk-neutral PPO). For example, on a $12^\\circ$ slope Hopper task FiRL reduces worst-case ($CVaR_{0.1}$) impact forces by over $35%$ and total energy cost by $15%$, while attaining a higher success rate.", "tldr": "", "keywords": ["Reinforcement Learning", "Finsler Geometry", "Risk-sensitive RL", "Conditional Value-at-Risk (CVaR)", "Anisotropic Locomotion", "Quasimetric Learning", "Robot Locomotion"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ed78a1d6904218df776ad195e3a28f34d578957c.pdf", "supplementary_material": "/attachment/3a1510d0842f857e8ef46b77e6550c2e9e3b10ee.zip"}, "replies": [{"content": {"summary": {"value": "This is an interesting paper on applying a velocity and state-dependent energy cost to robotics locomotion problems, with risk-sensitive control. The authors propose to use a Finslerian metric to penalize the robot when climbing slopes. The authors show that this design is well defined and consistent with quasi-metric RL frameworks. In addition, a risk-averse distributional RL framework has been applied to ensure safe locomotion. The authors show that the Bellman operator acting on the value function is a gamma-contraction and establish the validity of the proposed actor-critic algorithm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is nicely written and presents the idea clearly. \n- The idea is interesting and very relevant in robotics learning, especially for legged locomotion. The concepts are well explained, and the approach makes sense to me. \n- The theoretical result for the Finslerian metric is interesting, as it exploits geometric properties of robot locomotion. I find that to be novel and appealing. \n- The results on the contraction of the Bellman operator are encouraging, providing support for the algorithm's design. \n- The experiments make sense and support the arguments well."}, "weaknesses": {"value": "1. The equations under Figure 1 do not make sense. What does \\\\( \\pi^{\\text{hi}} : \\max_w \\\\) mean? Why is it maximizing over \\\\( w \\\\) in \\\\( V(w, g) \\\\), which I assume is the value function of state \\\\( w \\\\)? \\\\(V_F\\\\) does not seem to be defined anywhere. The definition of \\\\( A(s_t, a) \\\\) or \\\\( V \\\\) does not seem right. \n2. While not present in MuJoCo, the design of the drift penalty and foot slip is prevalent in legged locomotion works, among other reward terms. And as the reward is an instantaneous function dependent on the $ (s,s')$, the reward functions in these works seem to be anisotropic already. E.g., on slopes, if the humanoid robot walks in a different direction or at a different speed, the energy cost would differ. Am I missing something here? This seems to be a significant problem with this work. \n3. The application of distributional RL is a bit forced. I am not very convinced by the approach's motivation, even though the results look good. Since risk-sensitive control seems to be a substitute for regular PPO, any work that uses PPO can potentially use a risk-sensitive policy with CVaR. Then, what specific insight or connection does this paper bring to this additional reward function design?\n4. For the experiments: \n\n    a. The setting of the experiments is too simple for a serious comparison of legged locomotion. Consider conducting experiments that already have slopes, e.g., in IsaacLab with real legged robots. The MuJoCo benchmark in Gym or Gymnasium does not represent the state of the art in locomotion design. Results conducted on outdated or overly simplified environments hold limited value. \n    \n    b. CVaR seems to increase energy, as suggested by the first and second rows. But why is the energy cost lower in FiRL than in the second-to-last row? Also, the CVaR value decreased dramatically, unlike in PPO vs CVaR-PPO. I do not see why adding the additional reward will have such an impact.\n\n5. There are lots of typos in the appendix. E.g., Definition F.1 with the \\mathbb{1}. In Prop. F.2, many notations do not make sense. I don't know if they are typos or just content generated by an LLM that the authors chose to ignore. Some results appear to be established in the literature; please properly cite the source. \n6. Conclusions like the rate of convergence are thrown out lightly without explanation. I don't think this is the right way to do it. \n7. The proposed algorithm is clearly not off-policy, as we need to estimate the return distribution to compute the CVaR. In general, since the algorithm is based on PPO, I don't think the authors should discuss any sample-complexity results.\n\nMinor:\nPlease cite properly."}, "questions": {"value": "See limitation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U4rKoEbfUL", "forum": "YCHE8YOqyx", "replyto": "YCHE8YOqyx", "signatures": ["ICLR.cc/2026/Conference/Submission12951/Reviewer_85rV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12951/Reviewer_85rV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761594145003, "cdate": 1761594145003, "tmdate": 1762923707890, "mdate": 1762923707890, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Finslerian Reinforcement Learning (FIRL). The main idea is to integrate a directional Finsler metric into the cost term and optimize the conditional value at risk. Experiments on MuJoCo demonstrate that FIRL achieves higher performance on the designed tasks while reducing both energy consumption and risk."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper provides theoretical proofs for the Bellman construction and the quasi-metric value property of FIRL.\n2. The results show that FIRL surpasses other reinforcement learning methods across multiple tasks."}, "weaknesses": {"value": "1. The experimental tasks are designed with asymmetric properties. Since the tasks themselves are asymmetric, it is natural to design asymmetric cost functions. Consequently, it is expected that reinforcement learning methods using asymmetric costs would outperform those using symmetric costs.\n2. The properties of the Finsler metric play an important role in FIRL, but there are no experiments provided to justify this. It would be beneficial to conduct experiments comparing asymmetric costs that do and do not satisfy Finsler metric properties, as this would help isolate and demonstrate the unique contribution of the Finsler metric.\n3. Section 4.1 discusses the Bellman contraction and quasi-metric value properties, but there are no experiments showing why these properties are important in real applications. For instance, it would be helpful to analyze how these properties affect the convergence behavior of reinforcement learning in the experiments in the main paper. \n4. FIRL is compared with multiple baselines such as Riemannian RL and Quasi-metric RL from different perspectives, but the main paper only reports success rates, energy consumption, and risk. It would strengthen the work to further investigate why FIRL outperforms these baselines, rather than simply showing that it does.\n5. There are no visualizations from the simulations—only simplified sketches. Providing simulation visualizations would make the results more intuitive. In addition, it would be useful to include quantitative analyses of behavioral differences between FIRL and the baselines. For example, collect 100 rollouts and compute the frequency of the robot appearing in different locations, then analyze how these frequencies relate to success and failure rates.\n6. The quasi-metric value property arises from the integration of the Finsler metric and CVaR. It would be better to include a brief explanation of this relationship in the main paper rather than relegating it to the appendix.\n7. In Equation (5), the input of F should be x and v. However, f(x,u) represents the next state. Although v can be derived from x and x', it would be clearer to explicitly state this in the mathematical formulation."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2xKnjaksqy", "forum": "YCHE8YOqyx", "replyto": "YCHE8YOqyx", "signatures": ["ICLR.cc/2026/Conference/Submission12951/Reviewer_GjBz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12951/Reviewer_GjBz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948343893, "cdate": 1761948343893, "tmdate": 1762923707556, "mdate": 1762923707556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FiRL, a reinforcement learning framework for legged locomotion that combines (i) an anisotropic, direction-dependent cost based on a Finsler metric and (ii) a risk-sensitive objective based on CVaR. The key motivation is that locomotion on sloped or directionally biased terrains (e.g., up-slope, cross-slope, wind) is intrinsically asymmetric, but standard RL costs are typically symmetric and optimize only expected return. The authors define a Finsler-style per-step cost that penalizes uphill, lateral, and “against the disturbance” motions more than downhill/along-disturbance motions, and then derive a CVaR Bellman operator over this cost. They prove contraction and show that the resulting value induces an asymmetric quasi-metric. Experiments on MuJoCo locomotion tasks with added slope/wind show that (Finsler cost + CVaR) outperforms (Finsler only) and (CVaR only), indicating the two components are complementary."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear formulation: The paper cleanly integrates anisotropic geometry (Finsler) with a risk-sensitive objective; this is more principled than simply reshaping rewards by hand.\n2. Motivation is believable: For locomotion on slopes or in the presence of directional disturbances, symmetric/Riemannian costs are indeed a mismatch.\n3. Theory: The contraction of the CVaR–Finsler Bellman operator and the quasi-metric interpretation make the method less ad-hoc than many locomotion reward tweaks.\n4. Ablations are useful: The 2×2 comparison (with/without Finsler, with/without CVaR) is convincing that both parts matter."}, "weaknesses": {"value": "1. No real robot experiments. All results are on MuJoCo-style models (Hopper, Walker2d, HalfCheetah) in sloped or windy variants. This is the clearest gap.\n2. No realistic robot model. Even if hardware is hard, they could at least test on a simulator of an actual platform (ANYmal, Unitree, ANYbotics-style morphology) to show that the anisotropic cost remains meaningful when the robot has real actuation limits and contact schedules.\n3. Task realism: The scenarios are engineered to showcase anisotropy. It would be good to see that the method still helps on less “designed” terrains.\n4. Sensitivity to Finsler parameters: The method relies on directional weights (uphill term, lateral penalty). It would help to see how robust performance is to mis-specified anisotropy."}, "questions": {"value": "1. At least a real-robot–style model: Add one experiment on a widely used quadruped model (e.g., ANYmal or Unitree A1/G1 in Isaac/MuJoCo/Isaac Gym) with an actual 3D base, realistic mass/inertia, joint limits, and contact pattern. This can still be in sim, but it should be a real robot model, not Hopper/Walker2d.\n2. A small hardware demo. It can be modest (e.g., slope walking with a commercial quadruped on foam/ramp, or walking against a fan/wind proxy), but this would make the “direction-aware + risk-aware” story much stronger and closer to [1] and [2].\n3. Report failure/tail metrics on that setup: Since the whole paper is about CVaR, show tail improvements (falls, large base pitch, large base roll, large joint torque) on the real-robot(-model) task too.\n4. Clarify how to set Finsler weights in practice: a short guideline for choosing the uphill and lateral penalties from physical quantities (slope angle, friction coefficient, or external force magnitude).\n\n\n[1] Shi, Jiyuan, et al. \"Robust quadrupedal locomotion via risk-averse policy learning.\" 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024.\n\n[2] Cheng, Yi, et al. \"HuRi: Humanoid Robots Adaptive Risk-ware Distributional Reinforcement Learning for Robust Control.\" Arxiv"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aBwnK9W2XL", "forum": "YCHE8YOqyx", "replyto": "YCHE8YOqyx", "signatures": ["ICLR.cc/2026/Conference/Submission12951/Reviewer_Avzv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12951/Reviewer_Avzv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984884765, "cdate": 1761984884765, "tmdate": 1762923707249, "mdate": 1762923707249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors proposed a RL framework that integrates a Finsler metric into the cost function for directional energy-awareness, and optimizes a Conditional Value-at-Risk objective for tail-risk robustness. The proposed method was evaluatied in simulated MuJoCo benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "A clear theoretical formulation and a clean actor–critic instantiation."}, "weaknesses": {"value": "The present scope is misaligned with practical deployment. Robust RL-based locomotion/whole-body control already operates on physical platforms under realistic noise, latency, contact uncertainty, and compute constraints [1–6], with domain randomization repeatedly shown to be both effective and straightforward to apply. By comparison, this paper primarily tunes the objective via CVaR, lacking integration with real world robot system and offering no on-robot validation; results are confined to idealized MuJoCo scenarios. Demonstrating practical value would require a closed-loop hardware deployment and apples-to-apples evaluations against established RL pipelines.\n\n[1] HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit\n\n[2] Real-World Humanoid Locomotion with Reinforcement Learning\n\n[3] DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction\n\n[4] Humanoid Parkour Learning\n\n[5] Robust and Versatile Bipedal Jumping Control through Reinforcement Learning\n\n[6] Robot Parkour Learning"}, "questions": {"value": "Please clarify the concrete failure modes you aim to address, the mechanism and assumptions by which the proposed method mitigate them, and provide deployment-relevant evidence showing advantages over established RL pipelines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7OxygcH5O9", "forum": "YCHE8YOqyx", "replyto": "YCHE8YOqyx", "signatures": ["ICLR.cc/2026/Conference/Submission12951/Reviewer_D5E7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12951/Reviewer_D5E7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990390889, "cdate": 1761990390889, "tmdate": 1762923706655, "mdate": 1762923706655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}