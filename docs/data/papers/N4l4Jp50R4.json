{"id": "N4l4Jp50R4", "number": 24847, "cdate": 1758361043790, "mdate": 1759896745660, "content": {"title": "Enough is as good as a feast: A Comprehensive Analysis of How Reinforcement Learning Mitigates Task Conflicts in LLMs", "abstract": "Model merging plays a crucial role in consolidating multiple specialized models into a single, unified model, especially in the era of large language models (LLMs). Recent research has primarily focused on developing strategies to enhance merging performance with the trained models, while the impact of training paradigms, such as supervised fine-tuning (SFT) and reinforcement learning (RL), on the effectiveness of model merging remains underexplored. In this study, we systematically explore the merging behavior of RL-trained LLMs compared to those trained with traditional SFT. Through comprehensive evaluations across five representative tasks, we find that RL significantly reduces task conflicts and results in less performance degradation after merging, making RL-trained models particularly well-suited for this process.\nTo unearth the reasons behind the superior suitability of RL for model merging, we conduct extensive empirical experiments and theoretical analyses. Our findings highlight three key factors: (1) On-policy training data in RL control the gradient updates in a smaller magnitude, reducing the risk of overwriting existing knowledge for other tasks in the model. (2) The RL optimization objective, which favors \"\\textit{enough is as good as a feast}\", progressively reduces the magnitude of parameter updates as the model converges, thereby alleviating inter-task conflicts. (3) Joint optimization of positive and negative examples in RL steers the model towards an unbiased task-specific parameter subspace, ensuring robust performance while further preventing parameter conflicts.", "tldr": "We provide a comprehensive analysis of how reinforcement learning mitigates task conflicts in LLMs", "keywords": ["Large language model", "Reinforcement learning", "Model merging"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87ebf4c0c7c4bb54d83072e4e087c2f117e41115.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a systematic empirical and theoretical analysis of how reinforcement learning (RL) and supervised fine-tuning (SFT) affect model merging performance in large language models (LLMs). By evaluating five representative NLP and reasoning tasks, the authors demonstrate and dissect why RL-trained models are more robust to parameter/task conflicts and preserve higher performance post-merging. The study identifies three contributing mechanisms: reduced parameter update magnitude due to on-policy data in RL, diminishing update strength from RL’s adaptive optimization objectives, and the role of joint positive/negative example optimization in RL. These analyses collectively suggest RL-trained LLMs are better-suited for consolidation via model merging than their SFT-trained counterparts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tComprehensive Empirical study: This paper systematically explores the effects of SFT vs. RL across multiple LLMs, tasks (math, code, instruction following, logical puzzles, ranking), and merging algorithms (Averaging, TIEs, Arithmetic, DARE), providing robustness to the experimental findings (see Table 1, Table 4, Table 5).\n2.\tThe writing is meticulous: The paper proposes three hypothesis of why RL performs better than SFT from three different perspectives and verify these hypothesis both from theoretical analysis and experiments"}, "weaknesses": {"value": "1.\tThe experiments focus on pairwise merges, but the field is also interested in merging larger ensembles of specialized models. Do the observed RL advantages scale when merging 3, 4, or more models across highly distinct or hierarchical tasks? If not, are there any limiting factors?\n2.\tFor some tasks/methods, performance loss is reported as -1% to -4%. Are these differences statistically significant? Including error bars or statistical significance tests would add clarity and reliability to interpretation."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "J8TACe1kCC", "forum": "N4l4Jp50R4", "replyto": "N4l4Jp50R4", "signatures": ["ICLR.cc/2026/Conference/Submission24847/Reviewer_W4Vs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24847/Reviewer_W4Vs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761392566883, "cdate": 1761392566883, "tmdate": 1762943219529, "mdate": 1762943219529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the role of reinforcement learning (RL) versus supervised fine-tuning (SFT) in mitigating task conflicts during large language model (LLM) merging. The authors demonstrate that RL-trained models significantly outperform SFT-trained models in preserving task-specific performance after merging. Through theoretical and empirical analyses, the paper highlights three key mechanisms underlying RL's superiority: (1) the use of on-policy data, which reduces gradient magnitudes and parameter conflicts; (2) RL's adaptive optimization objective, which naturally decreases update magnitudes as training converges; and (3) joint optimization over positive and negative samples, which steers models toward unbiased task-specific parameter subspaces. The results offer valuable insights into building robust, scalable multi-task LLMs without retraining from scratch."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper addresses a novel and important problem—how different post-training paradigms influence task conflicts in LLM merging. The focus on RL as a more suitable paradigm for model merging is both creative and impactful.\n\nThe experiments are comprehensive, covering diverse tasks, multiple RL algorithms, and various merging strategies. Theoretical insights are rigorously derived and supported by empirical evidence.\n\nThe paper is well-organized, with clear explanations of concepts, methodologies, and results. Figures and tables effectively complement the narrative."}, "weaknesses": {"value": "See questions."}, "questions": {"value": "Could the authors provide more details about the experiments conducted on the Qwen model? Specifically, were there any unique observations regarding the performance of RL-based training on this model compared to others?\n\nHow do RL training hyperparameters, such as the coefficients for KL divergence and entropy, impact the performance of model merging?\n\nIn the GRPO algorithm, how does the choice of the parameter n (rollout number for each sample) influence the results? Were any ablation studies conducted to analyze its effect on task performance or parameter conflicts? What is the impact of batch size in the context of model merging? Does a larger batch size help stabilize training or reduce task conflicts more effectively?\n\nReferring to the recent \"LoRA Without Regret\" paper by Thinking Machines, could the improved performance of RL in model merging also be attributed to the low-rank nature of its parameter updates? For example, does RL inherently constrain updates to a lower-rank subspace, similar to LoRA, thereby reducing task conflicts and preserving task-specific knowledge better? If so, could this be explicitly analyzed or compared in the context of your experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UMJ44Z1Zgy", "forum": "N4l4Jp50R4", "replyto": "N4l4Jp50R4", "signatures": ["ICLR.cc/2026/Conference/Submission24847/Reviewer_uswu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24847/Reviewer_uswu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694928338, "cdate": 1761694928338, "tmdate": 1762943219043, "mdate": 1762943219043, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a crucial question in Large Language Models (LLMs): how the training paradigm, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), impacts the efficiency of model merging. Model merging aims to merge multiple specialized models into a single model, which is often plagued by multi-task conflicts that lead to performance degradation. The paper demonstrates that RL-trained models preserve performance better after merging compared to SFT-trained models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+  Novel problem: The paper identifies a fundamental and significant question. The finding that RL can mitigate task conflicts is a major contribution, shifting the researcher's focus from post-hoc merging strategies to the pre-hoc training paradigm.\n\n +  Good empirical support: The problem is validated across 5 tasks , 4 merging methods (Table 1) , 3 RL algorithms (Figure 2) and 3 base models (Figure 3), almost fully proves the robustness of the core findings. \n\n+  Deep insight: It not only proves RL is better, but also explore why. The three analysis (on-policy , RL objective , pos/neg samples) is insightful and well-supported by ablations. The performance landscape visualization (Figure 4) shows SFT models are \"brittle\" in task-specific directions while RL models are ''robust``."}, "weaknesses": {"value": "-  In Table 1, the \"Puzzle\" task shows a very large performance drop even for RL-trained models (e.g., -56% for Averaging, -24% for TIEs). While this is still better than SFT's degradation (-65% to -56%), it is a much starker drop than for other tasks (like Code or IF). The paper notes this but doesn't fully explore why the Puzzle task remains a sharp drop for both paradigms. Does this suggest a limitation of RL's conflict mitigation, or is there a unique property of the logical puzzle task (e.g., requiring very sparse, specific parameter activations) that makes it an outlier? \n\n- Attribution of Norm Reduction (SFT & RFT & RL):In Section 4.1 and Table 2, the update norms are SFT (6.50) > RFT (2.36) > RL (0.78) for the Math task. The author says that on-policy data helps regulate the norm (SFT vs. RFT). However, the gap between RFT (on-policy SFT) and RL (on-policy RL) is also very large (2.36 vs. 0.78). This suggests that the RL objective is also a major contributor to the small norm in Table 2. The paper presents these as separate sections (4.1 and 4.2), but the experiment in 4.1 seems to conflate both effects. The argument could be strengthened by clarifying how much of the norm reduction is due to on-policy data vs. the RL objective. \n\n- The paper theorizes in Section 4.2 that the advantage A vanishes as the model converges. It would be good to see an empirical plot of the average reward r and/or average absolute advantage ∣A∣ over training steps. This would link the theory to the experimental results, strengthening this hypothesis."}, "questions": {"value": "Q1. Why does the Puzzle task in Table 1 show severe degradation (-24% to -56%) even for RL models? Does this imply a unique property of this task's parameter subspace that makes it resistant to the \"orthogonalization\" effect of RL? \n\nQ2. The norm for RL (0.78 for Math) is much smaller than for RFT (2.36). Since both are on-policy, is this difference entirely attributable to the RL objective from Section 4.2? Could the authors provide a clearer quantitative attribution of how much of the total norm reduction (from SFT's 6.50) comes from \"on-policy data\" (SFT to RFT) versus the \"RL objective\" (RFT to RL)? \n\nQ3. Did the authors record the average reward and/or average absolute advantage ∣A∣ over training? A plot showing reward saturation and ∣A∣ decaying to 0 would be a powerful empirical validation for the \"enough is as good as a feast\" hypothesis. \n\nQ4. Does the \"RL-Pos\" variant (learning only from positive samples) also result in a larger update norm (just like in Table 2) or a faster-growing conflict norm (like in Figure 6) compared to full RL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical issues identified."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y9zO14zJ8a", "forum": "N4l4Jp50R4", "replyto": "N4l4Jp50R4", "signatures": ["ICLR.cc/2026/Conference/Submission24847/Reviewer_ojWS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24847/Reviewer_ojWS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963801610, "cdate": 1761963801610, "tmdate": 1762943218595, "mdate": 1762943218595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discovers a phenomenon that the model trained by RL is more suitable for merging than model trained by SFT. From three aspects, the gradient magnitude, conflict parameter magnitude and effect of positive and negative samples, the paper designs experiments and explains the reasons why merging RL models has less performance degeneration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and the motivation is clear.\n2. The experiment results are consistent with the proposed ideas."}, "weaknesses": {"value": "The designed experiments in the paper can basically verify the proposed factors which affects the model merging. However, there still are some shortcomings. First, the largest number of tasks is 5. Then, the paper does not study the performance of merging model trained on Math and Coder and model trained on IF and Puzzle. It is not clear that whether the number of training data will influence the results. Furthermore, if we keep the gradient of SFT and RL the same, will it affect the results? Finally, if we train SFT in continuous labels, will it affect the results? Also, please see questions below."}, "questions": {"value": "1. The number of merging models is up to 5. What will happen if the models is more?\n2. Whether the number of training data affects the results is not studied. What will happen if merging a model trained on two datasets with a model trained on other two datasets?\n3. The comparison between SFT and RL models’ gradient magnitude is not enough. What will happen if we keep them the same gradient magnitude?\n4. If we train SFT with normalized continuous label like the advantages of RL, will it affect the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R73MQheSGY", "forum": "N4l4Jp50R4", "replyto": "N4l4Jp50R4", "signatures": ["ICLR.cc/2026/Conference/Submission24847/Reviewer_6WLa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24847/Reviewer_6WLa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24847/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145235919, "cdate": 1762145235919, "tmdate": 1762943218161, "mdate": 1762943218161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}