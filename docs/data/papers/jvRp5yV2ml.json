{"id": "jvRp5yV2ml", "number": 5391, "cdate": 1757906577342, "mdate": 1759897978337, "content": {"title": "KNOWPLAN: Knowledge-Driven AI Agents for Smart Degree Pathway Planning", "abstract": "Recent advances in large language models (LLMs) provide powerful capabilities for knowledge-driven course planning. However, building reliable, constraint- aware study planners from publicly available course webpages remains challenging due to heterogeneous data sources, complex multi-constraint requirements, and dynamic course catalogs. To address these challenges, this paper proposes KNOWPLAN, a proactive, self-evolving multi-agent AI platform that integrates LLM-based extraction, knowledge-graph retrieval, and constraint-aware reasoning to generate adaptive, personalized study plans. The platform addresses four key challenges in degree planning. First, an Agent Forest leverages LLMs to transform heterogeneous, unstructured course catalogs into unified representations of courses and prerequisite dependencies. Second, a graph-construction agent assembles these representations into a directed knowledge graph, forming the platform’s core infrastructure that faithfully preserves complex multi-prerequisite structures and multiple dependencies. Third, a course-to-fine recommender pro- duces highly personalized course plans: the Course Planning Agent generates coarse, multi-constraint learning trajectories, while the Curriculum Alignment Agent refines these trajectories into term-level schedules, systematically resolving temporal conflicts and optimizing plans according to individual preferences and backgrounds to enable timely degree completion. Finally, a unified Graph- RAG backbone ensures cross-agent consistency and adapts to catalog drift, enabling dynamic refinement with evolving curricula. Across multiple universities, KNOWPLAN achieves 99.5% accuracy in major requirements and 98.7% ac- curacy in prerequisites. By integrating graph-based reasoning with a term-level scheduler, it delivers feasible, compliant, and personalized study plans that adapt to preferences, workload limits, and exceptions, demonstrating superior performance compared to state-of-the-art methods.", "tldr": "", "keywords": ["Large Language Model", "Artificial Intelligence", "Data Mining", "Recommender Systems", "Course Planning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee66d892d9c02627393a4cfe1b87227776fbfd4b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper is organized mostly as a product idea / process pipeline. I don’t think of this as a piece of novel research. However, there are no rigorous benchmarks for this “compound task”. And, that makes it hard to objectively assess the value of the pipeline.\n\n\nThe paper discusses a process to help university students pick coursework to optimize for their interest, account for all the prerequisites automatically & course period conflicts. They do this using multiple LLM calls for different phases of the process. Process outlined below\n\n## Claimed Contributions\n1. “Agent Forest” to extract structured data from course catalogues\n2. Graph construction agent converts this to KG (DAG)\n3. Course Planning Agent generates coarse, multi-constraint learning trajectories, while the Curriculum Alignment Agent refines these trajectories into term-level schedules\n4. Account for schedule conflicts, preferences, total time to completion\n\n\n## Process\n1. Use LLM calls on university courses site HTMLs to extract structured details about requirements, class timings, etc\n2. Create course prerequisite graph taking care of multi-requirements, & one-of requirements\n3. Assign scores to courses (nodes in teh above graph) using heuristics like student’s interest, prerequisites\n4. Find potential course-plan by maximizing scores & accounting for perceived “difficulty”\n5. LLM converts user preferences to formal rules to help set constraints\n    - Use LLMs to “parse preferences/policy text, select the target mode, and generate rationales”\n6. Pass through CP-SAT scheduler for class timing overlap avoidance\n\n\n## Required Readings\n- Ng & Fung: https://arxiv.org/abs/2407.11773"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addressed the cycle-breaking problem in course picking logic\n2. Good details of course score formulation in section 3.3.1\n3. Significant details about course-prerequisites graph generation - dealing with ”and” and “one-of” course requirements"}, "weaknesses": {"value": "1. Paper focuses a lot of effort and work into extracting (parsing) systematic data out of webpages. I understand that it is a problem for a product, but it is not a research problem. I’d have focused starting with organized/tabulated clean data & built on top of that.\n2. The claim that they “introduce the concept of” Agent Forest is extremely misleading - essentially, they have a custom prompt(s) for all the LLM call(s) (with some extraction examples) for each university’s webpage\n3. The sentence: “The pipeline of Agent Forest is demonstrated in Figure 2” is inaccurate. Fig 2 shows the entire process of their pipeline - not just agent forest for course details extraction\n4. As for the *results*, they admit that “the ground truth data does not include prerequisites or course relationships” - which is the part that the agent forest was going to solve.\n5. The claimed accuracy is on these ground-truth values, & hence not reliable either\n6. They don’t attempt to quantify the overall process & quality.\n7. Graph building process feels like adding LLM into a use-cases where it’s not needed\n    - For “handling the logical expressions of prerequisites with multiple options and addressing circular dependencies” while constructing graphs, they can again use deterministic regex-matching or smaller text annotation models to identify entity relationships (ie which courses are prerequisites for which ones)\n---\n\n* IIUC, the only material improvement this process makes on top of an algorithmic system is that:\n1. LLM-based course-work details extraction (for which getting json feed from the university, or writing custom parsers are better solutions, & also, the paper admits that the setup didn't really work)\n2.  it can account for user’s natural-language preference & encode that into course selection logic."}, "questions": {"value": "1. They mention “course-to-fine recommender” in several places. Was this intended to be “coarse” instead? - as in coarse granularity vs fine granularity? If this was intended to be word-play (since they’re using it for course selection), they should clarify that upfront. (they do write “coarse” in other places - so I believe its a typo)\n2. Why the graph construction agent? If the requirements are clear, creating a DAG is a purely algorithmic process, & introducing an LLM agent only uncertainty in the result\n3. In section 3.2, “This article utilizes this table to construct …” - what article? Kindly rephrase, or add relevant references\n4. A flowchart of the overall process would be extremely helpful. For example, the course selection “agent” is not exactly a fully agentic system - it relies on well-defined mathematical formulations for the course nodes & then an algorithmic process to pick non-conflicting options. LLM is only used to distil user’s NL preferences into clean logic statements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hfHVLo381Y", "forum": "jvRp5yV2ml", "replyto": "jvRp5yV2ml", "signatures": ["ICLR.cc/2026/Conference/Submission5391/Reviewer_UBq7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5391/Reviewer_UBq7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626864092, "cdate": 1761626864092, "tmdate": 1762918035083, "mdate": 1762918035083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces KNOWPLAN, a multi-agent system that automatically generates accurate, personalized, and conflict-free degree plans using only public university course catalogs. It integrates large language model (LLM)-based extraction, a curricular knowledge graph, retrieval-augmented generation (Graph-RAG), and a constraint-aware scheduler (CP-SAT) to produce term-level plans tailored to students’ goals, preferences, prior credits, and workload. The system achieves 99.5% accuracy on major requirements and 98.7% on prerequisites across multiple universities, and effectively eliminates infeasible plans that arise from coarse-grained approaches.\nKNOWPLAN comprises several coordinated components: a university-specific parsing framework that uses a panel of LLMs for data extraction and validation; a graph-construction module that encodes prerequisite and corequisite structures while enforcing acyclicity; and a planning pipeline that progresses from high-level plan skeletons to detailed term-wise schedules via constraint solving. Evaluation on data from over 6,000 institutions, along with ground-truth validation from four universities, demonstrates high extraction accuracy and robust scheduling performance. Compared to commercial tools, KNOWPLAN stands out for its reliance on public data, ability to generalize across institutions, and integration of structured knowledge representations with language-guided reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a well-structured and comprehensive system, with a clear, modular multi-agent architecture that spans from web data ingestion to knowledge graph construction, planning, and CP-SAT-based scheduling. It explicitly models complex curricular structures, including prerequisite logic and cycle detection, and visualizes them effectively. A key strength is its commitment to feasibility: instead of stopping at LLM-generated plans, the system produces executable, conflict-aware term-level schedules under real-world constraints. The coarse-to-fine personalization pipeline supports multiple user goals while maintaining policy compliance. The system is robust to catalog heterogeneity, employing per-university parsing agents and multi-LLM consensus to achieve high extraction accuracy. Finally, the work is well-positioned against existing tools, with a fair comparison to commercial systems that rely on internal student information systems, highlighting KNOWPLAN’s ability to operate entirely on public data."}, "weaknesses": {"value": "- The paper lacks clarity regarding prerequisite verification. Although the abstract reports 98.7% accuracy, the ground truth data do not include prerequisite relations, and the authors do not explain how these labels were obtained or annotated. This omission raises doubts about the validity of one of the paper’s central claims (Abstract; §4.1).\n\n- The evaluation of the scheduling component is limited. Beyond the reported “82% reduction” in infeasible plans, the paper does not provide key metrics such as the proportion of skeleton plans that schedule successfully, average time to degree under constraints, or robustness across multiple terms. There are no comparisons with existing baseline systems (e.g., e.g., uAchieve Schedule Builder’s auto-combination generation, Series25 optimizer) or user studies.\n\n- The paper lacks ablation studies and error analysis. It would benefit from comparisons between single- and multi-LLM extraction, Graph-RAG versus text-only or no RAG, CP-SAT versus heuristic or greedy schedulers, sensitivity to hyperparameters (λ, μ, ν) in the scoring functions, and an analysis of extraction and scheduling failure cases.\n\n- The data provenance for the “difficulty” and cost metrics is unclear. Using average grades as a proxy for difficulty introduces bias because grade distributions are affected by inflation, disciplinary differences (e.g., STEM grade penalties), instructor variation, and incomplete publication of grade data. Without transparent documentation of data sources, time span, institutional coverage, and normalization methods, the metric risks producing biased recommendations that favor easier courses over essential but challenging ones.\n\n- Scalability and maintenance issues are not discussed. The Agent Forest appears to require institution-specific agents and prompts, yet the paper does not quantify the maintenance overhead as catalogs evolve, the latency of updates, or the long-term consistency of the Graph-RAG framework.\n\n- Reproducibility is limited. No code, prompts, datasets, or scheduler configurations are released. Although the NCES source is cited, the four labeled ground-truth datasets and configuration files used in evaluation are not provided, preventing independent verification.\n\n- The accuracy of policy formalization is unverified. The LLM translates catalog and policy text into formal scheduling rules, but the authors do not measure the precision or recall of this translation process. Such evaluation is essential to ensure compliance with institutional and accreditation policies.\n\n- The contribution is primarily engineering rather than methodological. The paper integrates established techniques—LLM-based extraction, knowledge graphs, and constraint programming—into a coherent system but does not introduce fundamentally new algorithms or theoretical insights. Its main value lies in system design and implementation rather than conceptual innovation."}, "questions": {"value": "1.\tPrerequisite accuracy: How did you measure the reported 98.7% when the ground truth lacks prerequisites? Was there a separate human-labeled set, internal SIS data, or faculty validation? Please detail labeling protocol, sample size, and inter-rater agreement. \n2.\tPolicy rule translation: How do you evaluate the LLM’s policy-to-constraint mapping? Provide a labeled set of policy snippets → formal constraints with accuracy metrics and typical errors (e.g., unit brackets, co-req timing, repeat rules). \n3.\tAblations: Quantify benefits of multi-LLM extraction vs. single LLM; Graph-RAG vs. no-graph retrieval; CP-SAT vs. heuristic scheduling. \n4.\tEnd-to-end feasibility: For a fixed set of majors across multiple schools/terms, what fraction of students obtain a fully feasible 4-year (or 2-year) schedule under typical constraints? Report time-to-degree, average deferrals, and units per term with confidence intervals. \n5.\tScalability & drift: What’s the maintenance cost per university (prompt edits, agent updates) as catalogs change? Do you version and regression-test KGs/constraints as part of “self-evolving” claims? \n6.\tDifficulty metric & fairness: Where do AvgGrade statistics come from? Have you tested whether difficulty-based ranking skews against certain departments/instructor pools? Consider reporting robustness/fairness analyses. \n7.\tComparative baselines: Can you run head-to-head schedule feasibility or plan-quality comparisons against uAchieve Schedule Builder (auto combinations) or Series25 (admin scheduling), at least on a common synthetic/neutral benchmark? (CollegeSource, https://collegesource.com/degree-planning-tools/uachieve-schedule-builder/)\n8.\tUser study: Any student/advisor evaluations (usability, trust, perceived correctness) versus text-only LLM baselines and existing campus tools?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gAjAcwf0YD", "forum": "jvRp5yV2ml", "replyto": "jvRp5yV2ml", "signatures": ["ICLR.cc/2026/Conference/Submission5391/Reviewer_9y4A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5391/Reviewer_9y4A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815183514, "cdate": 1761815183514, "tmdate": 1762918034859, "mdate": 1762918034859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of constraint-aware study planners. This problem arises because of heterogeneous data sources, complex multi-constraint requirements, and dynamic course catalogs in educational institutions. The paper proposes a self-evolving multi-agent platform that integrates LLM-based extraction, knowledge-graph retrieval, and constraint-aware reasoning to generate adaptive, personalized study plans."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The motivation of the work is clear, the problem well presented, related work well presented, well organized, and the description of the approach is clear, understandable and reproducible. The paper proposes an innovative approach to solve a common problem in the education domain. The paper proposes a platform which achieves more than 98% accuracy across multiple universities."}, "weaknesses": {"value": "The paper presents several problems in references, missing examples to help readers understand the paper, typos, punctuation, missing experimental environement, \nLine 46:  the reference provided is not an appropriate reference for the claim\nLine 53: “Some of these systems serve general purposes (provide references for this claim), while others are tailored (provide a reference for this claim)” → Providing references at this stage is very important for readers\nLine 54-57:  Providing references and examples is very helpful for the readers\nLine 95: What is educational AI?\nLine 135: “However, most educational RAG systems still treat constraints loosely and do not integrate exact solvers for timetable feasibility.” This claim should be supported by references or a study of existing work and this is not the case in the paper.\n\nLack of examples to explain how the agents performed their tasks. For instance, the paper may take the example of a math course and show step by step how the method is used\n\nLine 151-152: such claim should be supported with references\n\nLine 177: “extracted results with both RAG and JSON files.” → Extract results with JSON files or store in JSON file?\n\nLine 266-267, line 427: check the punctuation\n\nFor the experiments, for the reproducibility, provide the LLMs used, hyperparameter, the performance of the tools used, libraries, etc.\n\nHow data is collected is not presented, and the title is data collection and preparation\n\nLine 287: provide the meaning of GE\n\nLine 432: SOTA is not presented in the experimentation settings"}, "questions": {"value": "Line 185-185: What is the role of each LLM?\nLine 208: “we devised” → We designed?\nAfter reading the whole paper, one question remains: How the agents are orchestrated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VL1YYxRSKM", "forum": "jvRp5yV2ml", "replyto": "jvRp5yV2ml", "signatures": ["ICLR.cc/2026/Conference/Submission5391/Reviewer_tkg5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5391/Reviewer_tkg5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825078955, "cdate": 1761825078955, "tmdate": 1762918034507, "mdate": 1762918034507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces KNOWPLAN, a multi-agent AI system for degree pathway planning that integrates large language models (LLMs), knowledge graphs (KGs), and retrieval-augmented generation (RAG). The proposed pipeline orchestrates several specialized agents: an Agent Forest that parses heterogeneous course catalogs through multi-LLM extraction, a Graph-Construction Agent that builds a directed knowledge graph of prerequisites and dependencies, a Course Planning Agent that generates coarse multi-constraint learning trajectories, and a Curriculum Alignment Agent that refines them into conflict-free, term-level schedules using constraint programming. Experiments conducted across four universities demonstrate the effectiveness of the proposed system."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "[+] An interesting and significant task in educational technology. \n\n[+] Well-designed multi-agent architecture integrating LLMs and constraint reasoning\n\n[+] Handles cross-institution heterogeneity using public data sources"}, "weaknesses": {"value": "[-] The motivations of some module designs are unclear. For example, the equations (1) and (2) contain multiple items, and are they grounded by educational theories?\n\n[-] Many technical details are missed, such as $π_{int}(v)$ and the weights of target-driven objectives for several modes.\n\n[-] The evaluation metrics are not described, and there is no user or institutional validation.\n\n[-] Heavy reliance on prompt design and multi-LLM extraction.\n\n[-] No ablation or comparison on efficiency."}, "questions": {"value": "1. Can the authors clarify how the Graph-RAG backbone prevents conflicts or duplication when curricula evolve?\n1. Have the authors compared KNOWPLAN to a single-LLM baseline (e.g., GPT-4 with a prompting pipeline) to show the benefit of the multi-agent design?\n1. Could the authors provide an ablation study or efficiency analysis of each agent’s contribution?\n1. Could you provide some feedback on the system from real-world users?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dSzouPgXMM", "forum": "jvRp5yV2ml", "replyto": "jvRp5yV2ml", "signatures": ["ICLR.cc/2026/Conference/Submission5391/Reviewer_fdeD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5391/Reviewer_fdeD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990068682, "cdate": 1761990068682, "tmdate": 1762918034312, "mdate": 1762918034312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a multi-agent platform, KNOWPLAN, a graphRAG system for degree pathway planning. An LLM-based KG construction agent first captures the prerequisites of the courses; a course planning agent then creates personalized plans that extract a subgraph that satisfies the prerequisite requirements. An experimental study verified the tool's usefulness across several scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. Having a RAG system for course selection and curriculum suggestion is an important application scenario. \nS2. Real-world education datasets are used for illustration. \nS3. The tool has addressed personalized recommendations."}, "weaknesses": {"value": "W1. The technical challenges and contributions are limited. The method's generality needs further elaboration. \nW2. There is no guarantee of a certain quality for the recommended results. The solution looks straightforward. \nW3. There is no formal analysis on scalability and cost analysis to evaluate its overhead."}, "questions": {"value": "D1. Some details are missing. For example, how the ground truth will be formally characterized and used to evaluate the output at each stage remains unclear. The section mentioned \"manual work\"—more elaboration is needed. \n\nD2. The problem seeks a Top-K solution—yet it remains unclear how, as an optimization problem, it would be tackled by a search strategy, and how LLMs and other overhead can be mitigated. \n\nD3. There is a lack of necessary details, such as how the scheduler works in a principled way with hyperparameters and machine-readable rules. If any new solution is used, it deserves more in-depth analysis. \n\nD4. Quite a few tasks have been outsourced to LLMs, including handling violations or constraints. There is an analysis of how LLM accuracy affects the quality of recommendations. Another missing link is whether there is a unique optimal solution, or how a top-K solution is computed to find a trade-off when these measures conflict (e.g., fees and total credits)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WIB1qB5EG8", "forum": "jvRp5yV2ml", "replyto": "jvRp5yV2ml", "signatures": ["ICLR.cc/2026/Conference/Submission5391/Reviewer_DJMB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5391/Reviewer_DJMB"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission5391/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762222703577, "cdate": 1762222703577, "tmdate": 1762918034073, "mdate": 1762918034073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}