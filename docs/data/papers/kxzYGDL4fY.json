{"id": "kxzYGDL4fY", "number": 11860, "cdate": 1758204291879, "mdate": 1759897550275, "content": {"title": "Probing in the Dark: State Entropy Maximization for POMDPs", "abstract": "Sample efficiency is one of the main bottlenecks for optimal decision making via reinforcement learning. Pretraining a policy to maximize the entropy of the state visitation can substantially speedup reinforcement learning of downstream tasks. It is still an open question how to maximize the state entropy in POMDPs, where the true states of the environment, or their entropy, are not observed. In this work, we propose to maximize the entropy of a sufficient statistic of the history, which is called an information state. First, we show that a recursive latent model that predicts future observations is an information state in this setting. Then, we provide a practical algorithm, called LatEnt, to simultaneously learn the latent model and a latent-based policy maximizing the corresponding entropy objective from reward-free interactions with the POMDP. We empirically show that our approach induces higher state entropy than existing methods, which translates to better performance on downstream tasks.  As a byproduct, we open-source PROBE, the first benchmark to test reward-free pretraining in POMDPs.", "tldr": "", "keywords": ["unsupervised RL", "State entropy maximization", "POMDPs", "Information states"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e9a40bb69cca92ccb79ad8b5acc12d17e3fcbd31.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Authors propose a new reward-free RL pre-training entropic objective (\"information state\") applicable to POMDPs. They analyze this representation statistic on sufficiency and compactness. Moreover they evaluate (1) whether pre-training with LatEnt indeed boosts the initial state entropy, and (2) whether pre-training with LatEnt improves RL sample efficiency over two strong baselines. The answers are yes in three toy environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality and significance: The analysis on the sufficient/compact statistics is novel in the context of POMDP latent state representation. That said, I am not completely up-to-date on RL pre-training. Empirical results are strong.\n\nQuality and clarity: The experiment design and ablation studies seem solid and precisely target RQs. One can always ask for more environments or more baselines, but I think in this case the theoretical contribution and empirical results stand as is."}, "weaknesses": {"value": "One confounder is exploration frames. What if you give the baselines (ppo and dreamer) extra global step budgets for what they have missed out in pre-training? If they still cannot sample effectively, this result would strengthen your IS argument even more, beyond the basic \"pre-trained policies had longer simulation time to explore\"."}, "questions": {"value": "What if your objective is used for auxiliary regularization during RL instead of two-staged training? It has been done in MDP and could potentially reuse some rollouts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rRtimum6LG", "forum": "kxzYGDL4fY", "replyto": "kxzYGDL4fY", "signatures": ["ICLR.cc/2026/Conference/Submission11860/Reviewer_B9Ww"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11860/Reviewer_B9Ww"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761624628546, "cdate": 1761624628546, "tmdate": 1762922879679, "mdate": 1762922879679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work concerns unsupervised pretraining of (exploration) policies for partially observable environments. The work discusses in detail the difference between maximum state entropy and maximum observation entropy to motivate why the former is more informative. Still as it this would require knowledge of true states, the work introduces a surrogate in the form maximum latent entropy as objective. In this setting the objective is to maximize the entropy over the latent states that are induced by the current policy under the current dynamics model. The work then discusses all relevant design decisions to derive the LatEnt algorithm for on-policy pretraining using PPO.\nTo assess the quality of the proposed method the work introduces the PROBE benchmark, which provides partially observable variants of existing benchmarks, such as Pendulum or Ant. The work empirically verifies that the maximum latent entropy objective outperforms the maximum observation entropy objective."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The work tackles an important problem by proposing a novel solution. I am a big fan of code being available already during the review phase and want to highlight it. The work does a good job discussing the different pretraining objectives and explaining the proposed solution approach and positioning itself in the related research. I only have minor questions related to the approach:\n* Isn't the choice of null-action for $a_{-1}$ (as stated on line 228) potentially very environment specific? Would it make sense to sample this value from the full action range to enable potential different initializations?\n* Is the choice of PPO for policy learning crucial or is LatEnt agnostic to the policy learning mechanism, as long as dynamics model learning updates happen less frequently than policy learning? I am wondering if the constraint updates of PPO are particularly benefitial for the way LatEnt is learning the representations.\n* Lines 295- 297 claims that a larger batch size is required for the entropy objective but it's not substantiated how much bigger this is required and I do not immediately see the reasoning behind this statement.\n\nMinor comments: \n* Line 237 missing space after colon symbol\n* Footnote 1: Shouldn't the reward still be part of the POMDP definition, even if you consider the reward free setting or differing tasks?"}, "weaknesses": {"value": "The introduction of the PROBE benchmark seems like the biggest weakness to me. I) The environment descriptions are confusing. For example it is not fully clear from the text what exactly it means that something is unobservable in Pendulum if $y=0$ (line 319). It is also not well motivated why Pendulum requires both an unobservable region and the occlusion of the velocity from the observation vector. Similarly, the statement that Ant's z-axis is fully observable only when the ant is on the ground (line 337) requires a bit more explanation. Does that mean that at least one of the legs needs to touch the ground or is there the same cutoff as in Pendulum? (Fiugre 1 b suggests the latter). Directly after, it is stated that extrinsic forces are unobserved at all times in Ant but it is not clarified what exactly that means. I intuitively understood it as a form of \"random wind\" being applied to the environment that is not directly observed by the agent but has to be inferred. Lastly in Pusher it is not clear if the noisy puck position is just on the observation or if it is noise on the true state.\nThese details need to be clarified to better justify the choice of introducing and using these benchmarks.\nI understand that there are more descriptions of the environments in Appendix C, but those do not provide descriptions of why the environment modifications were chosen. In any case, it is unlikely that most readers will immediately look into Appendix C when reading the paper and would be more confused by the unclear environment description of the main paper.\n\nFurther, I understand that the authors feel that existing POMDP benchmarks are inadequate for their purposes (Line 308-309). Still, I believe it would be good to evaluate LatEnt on at least one such benchmarks to better quantify the advantages/disatvantages that stem from the novel algorithm. Solely evaluating on a newly proposed benchmark seems questionable to me.\n\nAs it stands, I believe this is a very interesting paper with a lot of interesting ideas that should spark good discussions at the conference. However, the above shortcomings in experimental design make me hesitant to vote for acceptance. In particular the sole focus on the newly introduced environments seems indefensible to me. For now I vote borderline leaning towards rejection. I am happy to increase my score if the authors provide a better justification why only the PROBE benchmark results should be deemed sufficient or provide some small-ish experiments that show LatEnts behavior on established benchmarks."}, "questions": {"value": "Please see the sections above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OmdEIL7E6W", "forum": "kxzYGDL4fY", "replyto": "kxzYGDL4fY", "signatures": ["ICLR.cc/2026/Conference/Submission11860/Reviewer_LwEB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11860/Reviewer_LwEB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665599242, "cdate": 1761665599242, "tmdate": 1762922878957, "mdate": 1762922878957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Clarification on the ``general observability properties'' and the PROBE benchmark"}, "comment": {"value": "Reading reviewer comments, we noticed a source of confusion deriving from an insufficient elaboration on what we mean by ``general observability properties'', why current benchmarks do not have them, and why we designed a new benchmark with those properties.\n\nIn the paper, we mention the ``general observability properties'' we aim to tackle with a reference to Zamboni et al 2024b. Their main result (Th. 4.1) shows that  maximizing the observation entropy is enough whenever the maximum singular value of the emission matrix $\\sigma_{\\text{max}} (O)$ and the Hadamard inverse (having elements $O_{ij}^{\\circ -1} = 1 / O_{ij}$) of the emission matrix $\\sigma_{\\text{max}} (O^{\\circ - 1})$ are both small. Coarsely, $\\sigma_{\\text{max}} (O)$ is large when a state can emit various distinct observations, while $\\sigma_{\\text{max}} (O^{\\circ - 1})$ is large when a single observation can be emitted by several distinct states. In our paper, we aim to target domains where either $\\sigma_{\\text{max}} (O)$ or $\\sigma_{\\text{max}} (O^{\\circ - 1})$ is large. \n\nThe most common benchmark in prior works, the DeepMind Control suite (Tassa et al 2018), $\\sigma_{\\text{max}} (O^{\\circ - 1})$ and $\\sigma_{\\text{max}} (O)$ are both relatively small. $\\sigma_{\\text{max}} (O)$ is small because every state can emit just one observation. $\\sigma_{\\text{max}} (O^{\\circ - 1})$ is not necessarily small as every observation can be emitted by several distinct states (e.g., same position with different velocities), but it becomes small once you stack a few consecutive observations (frames), which is the default setup of prior works. In this kind of settings, Zamboni et al. 2024b shows that maximizing the observation entropy is enough. \n\nIn our paper, we aim to target settings in which either $\\sigma_{\\text{max}} (O)$ or $\\sigma_{\\text{max}} (O^{\\circ - 1})$ is not small, which we informally describe as ``general observation properties''. To this end, we develop a new benchmark with those properties: In *Delusional Pusher* every state can emit several distinct observations (large $\\sigma_{\\text{max}} (O)$), in *Masked Pendulum* and *Vertically Blind Ant* the observations of the masked regions are emitted by multiple states (large $\\sigma_{\\text{max}} (O^{\\circ - 1})$).\n\nFinally, to address the concern that the proposed method is only evaluate in a new benchmark, we conducted additional experiments in the most common DeepMind Control suite (Tassa et al. 2018). \nWe summarize in the table below the results obtained by LatEnt against the maximum observation entropy (solution proposed by prior works) and the maximum state entropy (ideal target). As the reviewers can see, LatEnt and maximum observation entropy achieve nearly identical performance across all metrics. This is in line we the explanation above and the results of Zamboni et al. 2024b.\n\n**Table: DMC Walker pretraining (10 seeds, mean ± std)**\n\n|                              | **State entropy**     | **Observation entropy** | **Latent entropy**    |\n|------------------------------|------------------------|--------------------------|-------------------------|\n| **Max state entropy**        | 2.61 ± 0.008          | 1.528 ± 0.011           | 0.847 ± 0.013          |\n| **Max observation entropy**  | 2.35 ± 0.027          | 1.589 ± 0.012           | 0.85 ± 0.01             |\n| **LatEnt**                   | 2.36 ± 0.012          | 1.58 ± 0.005            | 0.863 ± 0.012           |\n| **Random policy (baseline)** | 2.26 ± 0.001          | 1.49 ± 0.001            | NA                      |\n\n\n**Table: DMC Hopper pretraining (10 seeds, mean ± std)**\n\n|                              | **State entropy**     | **Observation entropy** | **Latent entropy**    |\n|------------------------------|------------------------|--------------------------|-------------------------|\n| **Max state entropy**        | 1.77 ± 0.056          | 1.19 ± 0.039            | 0.757 ± 0.042          |\n| **Max observation entropy**  | 1.73 ± 0.03           | 1.3 ± 0.01              | 0.81 ± 0.01            |\n| **LatEnt**                   | 1.71 ± 0.03           | 1.28 ± 0.024            | 0.818 ± 0.017          |\n| **Random policy (baseline)** | 1.39 ± 0.002          | 1.04 ± 0.001            | NA                     |"}}, "id": "IlGocWVYCc", "forum": "kxzYGDL4fY", "replyto": "kxzYGDL4fY", "signatures": ["ICLR.cc/2026/Conference/Submission11860/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11860/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11860/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763653064362, "cdate": 1763653064362, "tmdate": 1763653064362, "mdate": 1763653064362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Clarification on the ``general observability properties'' and the PROBE benchmark"}, "comment": {"value": "Reading reviewers' comments, we noticed a source of confusion deriving from an insufficient elaboration on what we mean by ``general observability properties'', why current benchmarks do not have them, and why we designed a new benchmark with those properties.\n\nIn the paper, we mention the ``general observability properties'' we aim to tackle with a reference to Zamboni et al 2024b. Their main result (Th. 4.1) shows that  maximizing the observation entropy is enough whenever the maximum singular value of the emission matrix $\\sigma_{\\text{max}} (O)$ and the Hadamard inverse (having elements $O_{ij}^{\\circ -1} = 1 / O_{ij}$) of the emission matrix $\\sigma_{\\text{max}} (O^{\\circ - 1})$ are both small. Coarsely, $\\sigma_{\\text{max}} (O)$ is large when a state can emit various distinct observations, while $\\sigma_{\\text{max}} (O^{\\circ - 1})$ is large when a single observation can be emitted by several distinct states. In our paper, we aim to target domains where either $\\sigma_{\\text{max}} (O)$ or $\\sigma_{\\text{max}} (O^{\\circ - 1})$ is large. \n\nThe most common benchmark in prior works, the DeepMind Control suite (Tassa et al 2018), $\\sigma_{\\text{max}} (O^{\\circ - 1})$ and $\\sigma_{\\text{max}} (O)$ are both relatively small. $\\sigma_{\\text{max}} (O)$ is small because every state can emit just one observation. $\\sigma_{\\text{max}} (O^{\\circ - 1})$ is not necessarily small as every observation can be emitted by several distinct states (e.g., same position with different velocities), but it becomes small once you stack a few consecutive observations (frames), which is the default setup of prior works. In this kind of settings, Zamboni et al. 2024b shows that maximizing the observation entropy is enough. \n\nIn our paper, we aim to target settings in which either $\\sigma_{\\text{max}} (O)$ or $\\sigma_{\\text{max}} (O^{\\circ - 1})$ is not small, which we informally describe as ``general observation properties''. To this end, we develop a new benchmark with those properties: In *Delusional Pusher* every state can emit several distinct observations (large $\\sigma_{\\text{max}} (O)$), in *Masked Pendulum* and *Vertically Blind Ant* the observations of the masked regions are emitted by multiple states (large $\\sigma_{\\text{max}} (O^{\\circ - 1})$).\n\nFinally, to address the concern that the proposed method is only evaluated in a new benchmark, we conducted additional experiments in the most common DeepMind Control suite (Tassa et al. 2018). \nWe summarize in the table below the results obtained by LatEnt against the maximum observation entropy (solution proposed by prior works) and the maximum state entropy (ideal target). As the reviewers can see, LatEnt and maximum observation entropy achieve nearly identical performance across all metrics. This is in line we the explanation above and the results of Zamboni et al. 2024b.\n\n**Table: DMC Walker pretraining (10 seeds, mean ± std)**\n\n|                              | **State entropy**     | **Observation entropy** | **Latent entropy**    |\n|------------------------------|------------------------|--------------------------|-------------------------|\n| **Max state entropy**        | 2.61 ± 0.008          | 1.528 ± 0.011           | 0.847 ± 0.013          |\n| **Max observation entropy**  | 2.35 ± 0.027          | 1.589 ± 0.012           | 0.85 ± 0.01             |\n| **LatEnt**                   | 2.36 ± 0.012          | 1.58 ± 0.005            | 0.863 ± 0.012           |\n| **Random policy (baseline)** | 2.26 ± 0.001          | 1.49 ± 0.001            | NA                      |\n\n\n**Table: DMC Hopper pretraining (10 seeds, mean ± std)**\n\n|                              | **State entropy**     | **Observation entropy** | **Latent entropy**    |\n|------------------------------|------------------------|--------------------------|-------------------------|\n| **Max state entropy**        | 1.77 ± 0.056          | 1.19 ± 0.039            | 0.757 ± 0.042          |\n| **Max observation entropy**  | 1.73 ± 0.03           | 1.3 ± 0.01              | 0.81 ± 0.01            |\n| **LatEnt**                   | 1.71 ± 0.03           | 1.28 ± 0.024            | 0.818 ± 0.017          |\n| **Random policy (baseline)** | 1.39 ± 0.002          | 1.04 ± 0.001            | NA                     |"}}, "id": "IlGocWVYCc", "forum": "kxzYGDL4fY", "replyto": "kxzYGDL4fY", "signatures": ["ICLR.cc/2026/Conference/Submission11860/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11860/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11860/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763653064362, "cdate": 1763653064362, "tmdate": 1763659438909, "mdate": 1763659438909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work concerns unsupervised pretraining of (exploration) policies for partially observable environments. The work discusses in detail the difference between maximum state entropy and maximum observation entropy to motivate why the former is more informative. Still as it this would require knowledge of true states, the work introduces a surrogate in the form maximum latent entropy as objective. In this setting the objective is to maximize the entropy over the latent states that are induced by the current policy under the current dynamics model. The work then discusses all relevant design decisions to derive the LatEnt algorithm for on-policy pretraining using PPO.\nTo assess the quality of the proposed method the work introduces the PROBE benchmark, which provides partially observable variants of existing benchmarks, such as Pendulum or Ant. The work empirically verifies that the maximum latent entropy objective outperforms the maximum observation entropy objective."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The work tackles an important problem by proposing a novel solution. I am a big fan of code being available already during the review phase and want to highlight it. The work does a good job discussing the different pretraining objectives and explaining the proposed solution approach and positioning itself in the related research. I only have minor questions related to the approach:\n* Isn't the choice of null-action for $a_{-1}$ (as stated on line 228) potentially very environment specific? Would it make sense to sample this value from the full action range to enable potential different initializations?\n* Is the choice of PPO for policy learning crucial or is LatEnt agnostic to the policy learning mechanism, as long as dynamics model learning updates happen less frequently than policy learning? I am wondering if the constraint updates of PPO are particularly benefitial for the way LatEnt is learning the representations.\n* Lines 295- 297 claims that a larger batch size is required for the entropy objective but it's not substantiated how much bigger this is required and I do not immediately see the reasoning behind this statement.\n\nMinor comments: \n* Line 237 missing space after colon symbol\n* Footnote 1: Shouldn't the reward still be part of the POMDP definition, even if you consider the reward free setting or differing tasks?"}, "weaknesses": {"value": "The introduction of the PROBE benchmark seems like the biggest weakness to me. I) The environment descriptions are confusing. For example it is not fully clear from the text what exactly it means that something is unobservable in Pendulum if $y=0$ (line 319). It is also not well motivated why Pendulum requires both an unobservable region and the occlusion of the velocity from the observation vector. Similarly, the statement that Ant's z-axis is fully observable only when the ant is on the ground (line 337) requires a bit more explanation. Does that mean that at least one of the legs needs to touch the ground or is there the same cutoff as in Pendulum? (Fiugre 1 b suggests the latter). Directly after, it is stated that extrinsic forces are unobserved at all times in Ant but it is not clarified what exactly that means. I intuitively understood it as a form of \"random wind\" being applied to the environment that is not directly observed by the agent but has to be inferred. Lastly in Pusher it is not clear if the noisy puck position is just on the observation or if it is noise on the true state.\nThese details need to be clarified to better justify the choice of introducing and using these benchmarks.\nI understand that there are more descriptions of the environments in Appendix C, but those do not provide descriptions of why the environment modifications were chosen. In any case, it is unlikely that most readers will immediately look into Appendix C when reading the paper and would be more confused by the unclear environment description of the main paper.\n\nFurther, I understand that the authors feel that existing POMDP benchmarks are inadequate for their purposes (Line 308-309). Still, I believe it would be good to evaluate LatEnt on at least one such benchmarks to better quantify the advantages/disatvantages that stem from the novel algorithm. Solely evaluating on a newly proposed benchmark seems questionable to me.\n\nAs it stands, I believe this is a very interesting paper with a lot of interesting ideas that should spark good discussions at the conference. However, the above shortcomings in experimental design make me hesitant to vote for acceptance. In particular the sole focus on the newly introduced environments seems indefensible to me. For now I vote borderline leaning towards rejection. I am happy to increase my score if the authors provide a better justification why only the PROBE benchmark results should be deemed sufficient or provide some small-ish experiments that show LatEnts behavior on established benchmarks."}, "questions": {"value": "Please see the sections above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OmdEIL7E6W", "forum": "kxzYGDL4fY", "replyto": "kxzYGDL4fY", "signatures": ["ICLR.cc/2026/Conference/Submission11860/Reviewer_LwEB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11860/Reviewer_LwEB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665599242, "cdate": 1761665599242, "tmdate": 1763672015464, "mdate": 1763672015464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends maximum entropy unsupervised RL for MDPs to POMDPs. It formulates a new training objective and proposes a training algorithm called LatEnt, which is evaluated on a new benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Pretraining a policy for POMDP to improve sample efficiency is an important problem\n* The paper is well-organized at a high level."}, "weaknesses": {"value": "I find this paper's work insufficiently or misleadingly motivated. The popular state entropy maximization approach for pretraining or encouraging exploration for MDPs has previously been extended to POMDPs, as cited by the authors, which shows interest in doing such extension. When it comes to the paper's work, there was just a brief reference to Zamboni et al. (2024b) that \"these approaches are bound to fail on POMDPs with more general observability properties\", but there is no discussion and example of what the \"general observability properties\" could be, and why these cases are important; in addition, Zamboni et al. (2024a) is one of \"these approaches\", but as far as I can see, Zamboni et al. (2024b)'s analysis is not applicable to this work. Furthermore, Zamboni et al. (2024a) actually does maximize state entropy, instead of observation entropy.\n\nAnother major concern I have is that the technical writing lacks sufficient clarity. I'll mention some vague definitions, claims and theorems below.\n* Definition 1: What does it mean to say that IS2 can be replaced by IS2a and IS2b? Presumably doing this leads to a different notion of information state, but the wording is confusing.\n* Theorem 1: This theorem shows that the concept of information state in Definition 1 is the \"an information state for a POMDP with a convex objective\", but the latter has not been defined yet.\n* Assumption 1: I find it rather confusing to assume that it is possible to access the state of a POMDP for a constant number of times.  If it is possible to access the state of a POMDP, then it is not a POMDP any more, at least not in the standard sense. In addition, I find the claim \"Assumption 1 rules out the possibility to maximize the state entropy directly\" vague and hand-wavy.\n* Definition 2: $\\ell$ is not used in IS2a and IS2b.\n\nSome of these may be easily fixable, but with a general lack of precision in technical writing, it is very difficult to assess the correctness of the claims.\n\nThe proposed benchmarks appear to be somewhat contrived, as there is no discussion on what kind of \"general observability properties\" they are designed to capture.\n\nAlso, is there a reason why there is no empirical comparison with Zamboni et al.  (2024a)? And some other more recent approaches as cited in Zamboni et al. (2024b)?"}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9EF988dnme", "forum": "kxzYGDL4fY", "replyto": "kxzYGDL4fY", "signatures": ["ICLR.cc/2026/Conference/Submission11860/Reviewer_cjki"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11860/Reviewer_cjki"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919363081, "cdate": 1761919363081, "tmdate": 1762922878511, "mdate": 1762922878511, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an exploration technique in POMDPs. It consists of maximizing the frequency with which an information state of the POMDP is visited."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Exploration in POMDPs is a complex problem that, to my understanding of the literature, has been little studied.\n2. The proposed method is intuitive and seems correct.\n3. The experiments are comprehensive and clear."}, "weaknesses": {"value": "1. The presentation is rather heavy, and the paper is difficult to follow.\n2. From what I understand, there is an incompleteness in the formalization. The distribution $d^\\pi_L(l)$ is never explicitly defined. However, this object is at the heart of the method. I think it is important to clarify the definition of this object. \n3. From my understanding of section 4.1, a deterministic latent model is used and learned by minimizing the mean squared error (L2 reconstruction). If this is indeed the case, we are trying to learn a very specific type of information state that is \"deterministic\" predictive, where the definition of Subramanian et al. (2022) only required \"stochastic\" predictiveness. From my understanding, such IS only exists if the history allows the state to be reconstructed deterministically, which is a very specific case of POMDP (like memory POMDP)."}, "questions": {"value": "1. Could the authors clarify what $d^\\pi_L(l)$ is? Could authors also clarify what it would equal in the particular case of using the belief as IS?\n2. Could the authors clarify if the latent space is deterministic or not? If it is, what guarantee do we have that the learned statistic is indeed an IS?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mc1osEbL0Q", "forum": "kxzYGDL4fY", "replyto": "kxzYGDL4fY", "signatures": ["ICLR.cc/2026/Conference/Submission11860/Reviewer_jeJm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11860/Reviewer_jeJm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928912600, "cdate": 1761928912600, "tmdate": 1762922878091, "mdate": 1762922878091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}