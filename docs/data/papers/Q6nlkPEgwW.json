{"id": "Q6nlkPEgwW", "number": 3351, "cdate": 1757406846111, "mdate": 1763695449875, "content": {"title": "Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts", "abstract": "The Mixture of Experts (MoE) architecture is a cornerstone of modern state-of-the-art (SOTA) large language models (LLMs). \nMoE models facilitate scalability by enabling sparse parameter activation.\nHowever, traditional MoE architecture uses homogeneous experts of a uniform size, activating a fixed number of parameters irrespective of input complexity and thus limiting computational efficiency.\nTo overcome this limitation, we introduce Grove MoE, a novel architecture incorporating experts of varying sizes, inspired by the heterogeneous big.LITTLE CPU architecture.\nThis architecture features novel adjugate experts with a dynamic activation mechanism, enabling model capacity expansion while maintaining manageable computational overhead.\nBuilding on this architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model during mid-training and post-training.\nGroveMoE models dynamically activate $3.14 \\text{-} 3.28$B parameters based on token complexity and achieve performance comparable to SOTA open-source models of similar or even larger size.", "tldr": "", "keywords": ["Large Language Models", "Mixture of Experts"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/65e9480499d7a932db97bac24802138e408295fd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes adding adjugate experts MoE model architecture to introduce adaptive computation. To be specific, authors first split experts to fewer groups, and add one more smaller expert into each group. The representation is added to the output representation once any of the normal experts in the group is activated. As a results, the model achieves better performance than baseline models with additional but adaptive inference FLOPs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The idea is interesting. introducing adaptive compute by grouping experts and adding an additional expert is cool.\n2) It is great to have one baseline with \"qwen+midtrain\". This is a fair enough baseline with controlled training data.\n3) The writing in Sec 3.2 is clear. It's helpful to understand the reason why the computational cost is adaptive."}, "weaknesses": {"value": "1) One very important weakness, authors should report the real throughput/efficiency of their model. Since the low-rank layer is not that hardware friendly. There might be an MFU drop.\n2) I think it is not very clear that whether the adaptive flops is really working. Authors should have one more baseline using g=128, h=64. As shown in Table 1, smaller h and larger g is keep improving the model. Then how about just giving up adaptive compute? This will make the model a bit more similar to LoRA, but if it really works better, it is still something worthy study and report. But without that, it's very hard to make a clean conclusion based on the current results.\nMinor: \n1) It would be informative to mention that the h (intermediate hidden) of qwen baseline is 768. It implies the added expert is always smaller than the naive expert, aka a low rank expert.\n2) Similarly, authors should show this in Fig 2\n3) Authors should also somehow show or mention that A would not be computed if all experts are not activated."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ctVluyzj9X", "forum": "Q6nlkPEgwW", "replyto": "Q6nlkPEgwW", "signatures": ["ICLR.cc/2026/Conference/Submission3351/Reviewer_jATE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3351/Reviewer_jATE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761588670124, "cdate": 1761588670124, "tmdate": 1762916683925, "mdate": 1762916683925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a modification of the Mixture of Experts (MoE) architecture, where experts are divided into disjoint groups. Then, instead of having one shared expert (like in Deepseek architecture), we will have a separate shared expert (called Grove expert) within each group. For each token, whenever multiple experts from the same group are chosen, this group's shared expert output will need to be computed only once.\nThe authors then perform upcycling of the Qwen3-30B-A3B model to the proposed architecture, performing the mid-training and post-training phases. The resulting model compares favorably to other similarly-sized and even larger models, when measuring quality using a set of commonly-used LLM evaluation benchmarks.\nUnfortunately, the reproducibility of the experiments is currently limited. The authors do not detail the construction of the mid-training data mixture. For this stage of training, there is no comparison against a model with the same architecture, trained on the same data mixture. Therefore, it is hard to tell to what extent can the benchmark score improvements be attributed to the data mixture, and what is the effect of the architecture. \nI like the main idea for the change in the architecture. However, given the reproducibility issues mentioned above, I cannot recommend acceptance, due to limited experimental evidence. If I missed any important details in the paper regarding reproducibility and experimental comparisons, I would be happy to reconsider my score."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes an interesting change in the architecture, with the potential to improve MoE quality.\n2. The scale of experiments is big.\n3. The authors perform multiple stages of training, which resemble modern LLM training pipeline."}, "weaknesses": {"value": "1. It is unclear how much of the improvement comes from the datasets used.\n2. Limited reproducibility without the details regarding the dataset.\n3. Limited comparison with architecture trained in the same, controlled setup."}, "questions": {"value": "1. Do the authors provide comparison between the GroveMoE model after mid-training and Qwen30B-A3B-Base trained with the same mid-training setup?\n2. Could the authors elaborate a bit more on the motivation and computational benefits of the GroveMoE architecture, compared to more standard MoE?\n3. In section 5.4, the authors compare GroveMoE-Base to Qwen3-30B-A3B-Base, after both models were trained with the same post-training setup. In this experiment, was GroveMoE-Base already after the mid-training phase - and was the corresponding Qwen3-30B-A3B-Base also after mid-training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s3vzN57IGh", "forum": "Q6nlkPEgwW", "replyto": "Q6nlkPEgwW", "signatures": ["ICLR.cc/2026/Conference/Submission3351/Reviewer_onL8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3351/Reviewer_onL8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864529517, "cdate": 1761864529517, "tmdate": 1762916683639, "mdate": 1762916683639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Grove MoE that improves computational efficiency through dynamic parameter activation inspired by the big.LITTLE CPU design. The key innovation is organizing experts into disjoint groups, where each group shares an adjugate expert that is computed only once when multiple experts from the same group are activated, enabling dynamic computation (3.14-3.28B activated parameters) while maintaining controllable overhead. Building on this architecture, the authors develop GroveMoE-Base and GroveMoE-Inst (33B total parameters) by upcycling Qwen3-30B-A3B-Base through mid-training on 400B tokens and post-training with synthetic data. The models demonstrate strong empirical performance comparable to or exceeding SOTA open-source models of similar or larger scale, with particularly notable improvements on challenging mathematical reasoning tasks (e.g., 44.4% on AIME25 vs. 10.0-28.1% for baselines) and coding benchmarks, while maintaining compatibility with standard top-k routing mechanisms."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-  The Grove MoE architecture presents an interesting design where experts are organized into groups sharing adjugate experts, offering a fresh perspective on dynamic computation allocation inspired by heterogeneous computing principles, which provides a new direction for improving parameter efficiency in MoE models.\n\n\n- The paper demonstrates promising empirical results on a wide range of tasks spanning general knowledge, mathematical reasoning, and coding (e.g., notable improvements on AIME25 and OlympiadBench), suggesting the potential effectiveness of the proposed approach for challenging reasoning scenarios.\n\n- The manuscript is well-organized with clear architectural illustrations (Figure 2), systematic exploration of design choices including group configurations and scaling factors (Tables 1-2), insightful routing distribution analysis (Figure 3), and comprehensive reporting of hyperparameters and evaluation protocols across multiple stages."}, "weaknesses": {"value": "- The core mechanism of grouping experts and sharing adjugate experts is essentially a parameter-sharing strategy reminiscent of existing work on parallel blocks (e.g., AltUp), and the connection to big.LITTLE architecture appears more metaphorical than technically substantive, as the \"dynamic\" allocation is merely a byproduct of static grouping rather than adaptive routing.\n\n- The paper lacks ablation studies on critical components such as the loss-free load balancing strategy (Equation 8) and the decoupled routing mechanism (Equation 9-10), making it unclear which components actually contribute to performance gains versus inherited capabilities from the Qwen3-30B-A3B base model.\n\n- The evaluation conflates the benefits of the Grove MoE architecture with advantages from (1) additional mid-training on 400B tokens, (2) superior post-training data quality through extensive synthetic data generation, and (3) upcycling from an already strong base model, making it impossible to isolate the architectural contribution."}, "questions": {"value": "- The 30% inference slowdown reported in Section B.3 contradicts the claimed efficiency advantages. Could you provide: (a) detailed FLOPs analysis comparing theoretical vs. actual computational costs, (b) profiling results showing where the overhead originates, and (c) evidence that a custom kernel can realistically achieve the theoretical efficiency? Without this, the practical applicability of Grove MoE remains unclear.\n\n- Could you provide ablation experiments that isolate the contribution of each component (adjugate experts, loss-free load balancing, decoupled routing) by comparing against: (a) baseline Qwen3-30B-A3B with the same 400B mid-training tokens, (b) Grove MoE without the load balancing strategy, and (c) Grove MoE with coupled routing? This would clarify which design choices are essential for the observed performance gains. I understand such experiments are resource-intensive, so even partial results or analysis on smaller-scale settings would be valuable.\n\n- Figure 3 shows routing distributions, but could you provide more detailed analysis on: (a) whether certain adjugate experts become over-specialized or underutilized during training, (b) how the group assignments affect expert diversity and specialization, and (c) whether the routing patterns change meaningfully across different task types (math vs. coding vs. general knowledge)? I recognize that comprehensive analysis across all tasks may be extensive, but insights on even a subset of these questions would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7Rc9eKC353", "forum": "Q6nlkPEgwW", "replyto": "Q6nlkPEgwW", "signatures": ["ICLR.cc/2026/Conference/Submission3351/Reviewer_dxRa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3351/Reviewer_dxRa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979874199, "cdate": 1761979874199, "tmdate": 1762916683475, "mdate": 1762916683475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Grove MoE, a MoE architecture with experts of different sizes and a shared adjugate expert within each group to improve parameter efficiency. The model aims to balance capacity and cost by activating smaller experts more frequently and sharing computation across groups."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is generally well-written and clearly structured, making the proposed method easy to follow. The experimental evaluation is fairly comprehensive, covering multiple benchmarks and including relevant ablation studies to analyze the impact of key design choices."}, "weaknesses": {"value": "* **Prior Works Comparison**: Allocating different expert capacities via structural scaling is related to a line of recent works [1,2] that explore heterogeneous expert sizes within MoE layers. However, the paper does not compare against or discuss these prior approaches. \n\n* **Loading Balance Sensitivity**: While Grove MoE incorporates a load-balancing mechanism, the paper does not sufficiently address whether its heterogeneous expert structure is more sensitive to imbalance compared to standard MoE. In particular, when routing is skewed, large experts may impose disproportionate compute and memory burdens. It would strengthen the work to include controlled experiments or quantitative analysis that evaluates the sensitivity under different levels of routing imbalance, and compares its behavior to standard MoE.\n\n* **Expert Grouping Strategy**: Grove MoE is built upon Qwen3-30B-A3B and partitions its experts into disjoint groups, each associated with a shared adjugate expert. However, the grouping strategy is not thoroughly analyzed or discussed in the paper. Given that expert co-activation patterns or functional similarity may significantly influence the effectiveness, a more detailed examination of the grouping rationale is warranted. \n\n* **Performance Comparisons**: In Tables 1 and 2, Grove MoE is compared to a 30B MoE baseline using the same 50B training tokens. However, Grove MoE introduces an additional ~3B parameters and exhibits slightly higher activated parameter counts. This raises potential fairness concerns regarding the claimed gains.  \n\n\n[1] Wang A, Sun X, Xie R, et al. Hmoe: Heterogeneous mixture of experts for language modeling[J]. arXiv preprint arXiv:2408.10681, 2024.\n\n[2] Sun M, Liu W, Luan J, et al. Mixture of diverse size experts[J]. arXiv preprint arXiv:2409.12210, 2024."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YWpPMSuoPi", "forum": "Q6nlkPEgwW", "replyto": "Q6nlkPEgwW", "signatures": ["ICLR.cc/2026/Conference/Submission3351/Reviewer_AAf9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3351/Reviewer_AAf9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3351/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982192157, "cdate": 1761982192157, "tmdate": 1762916683276, "mdate": 1762916683276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}