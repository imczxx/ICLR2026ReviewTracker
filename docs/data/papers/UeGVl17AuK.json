{"id": "UeGVl17AuK", "number": 20803, "cdate": 1758310373483, "mdate": 1759896958041, "content": {"title": "SLogic: Subgraph-Informed Logical Rule Learning for Knowledge Graph Completion", "abstract": "Logical rule-based methods offer an interpretable approach to knowledge graph completion by capturing compositional relationships in the form of human-readable inference rules. However, current approaches typically treat logical rules as universal, assigning each rule a fixed confidence score that ignores query-specific context. This is a significant limitation, as a rule's importance can vary depending on the query. To address this, we introduce SLogic (Subgraph-Informed Logical Rule learning), a novel framework that assigns query-dependent scores to logical rules. The core of SLogic is a scoring function that utilizes the subgraph centered on a query's head entity, allowing the significance of each rule to be assessed dynamically. Extensive experiments on benchmark datasets show that by leveraging local subgraph context, SLogic consistently outperforms state-of-the-art baselines, including both embedding-based and rule-based methods.", "tldr": "SLogic is a novel neuro-symbolic framework that improves knowledge graph completion by using a GNN on local subgraphs to learn dynamic, context-aware scores for logical rules.", "keywords": ["knowledge graph", "logical rules learning", "knowledge graph completion", "knowledge graph reasoning", "neuro-symbolic"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d6a5993579ca28e0ad4518d715707bfeb45e25b8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SLogic, a framework for knowledge graph completion (KGC) that aims to improve upon traditional logical rule-based methods. The core idea is to move beyond static, global rule confidences by learning a dynamic, query-dependent scoring function. This function leverages the local subgraph context of a query's head entity, extracted and encoded by a Relational Graph Convolutional Network (R-GCN). The model combines this subgraph representation with a GRU-based rule embedding and static rule features to predict a context-specific score for each rule. Experiments on WN18RR, FB15k-237, and YAGO3-10 show that SLogic outperforms baselines on two of the three datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Intuitive Core Idea: The central premise of using the local subgraph context around a query entity to dynamically re-weight the importance of logical rules is sensible and intuitively appealing.\n\n- Detailed Methodology: The paper provides a comprehensive description of its framework, detailing the offline \"instance creation\" pipeline (rule mining, subgraph extraction, feature engineering) and the hybrid neural architecture (R-GCN + GRU + MLP) used for scoring.\n\n- Clear Case Study: The case study in Section 5.3 effectively illustrates the model's intended mechanism, showing how SLogic's rule preferences change for the same relation (isLocatedIn) based on two different query entities (one film-related, one-geography-related), demonstrating its context-aware capability."}, "weaknesses": {"value": "- Reliance on Heuristic Engineering: A very large part of the proposed contribution (Section 4.1) is a complex, multi-stage pipeline of heuristic-driven data and feature engineering. This includes DFS for path finding, k-hop BFS with neighbor sampling for subgraph extraction, and a hand-crafted set of topological node features (e.g., distance to head, global degree). This makes the framework feel less like a novel learning paradigm and more like a complicated feature engineering effort.\n\n- Misleading Novelty Claims: The paper's primary motivation, stated in the abstract, is that \"current approaches typically treat logical rules as universal, assigning each rule a fixed confidence score that ignores query-specific context.\" This statement is factually incorrect. A vast body of prior work (e.g., Markov Logic Networks, pLogicNet, and many others) has focused on learning weights or confidences for logical rules for decades. The paper fails to properly differentiate its specific contribution (using subgraph context) from this extensive literature, making its novelty unclear. The related work section is insufficient in this regard.\n\n- Limited Scope of Rules: The method only handles chain-like, compositional rules (i.e., relational paths). This is explicitly stated in the definition in Section 3 ($r_h(X,Y) \\leftarrow r_1(X,Z_1) \\wedge \\cdot\\cdot\\cdot \\wedge r_L(Z_{L-1},Y)$) and confirmed by the DFS-based rule mining process (Section 4.1.1). This ignores all other, more complex rule structures (e.g., rules with multiple atoms, rules with constants) that are crucial for reasoning. This severe limitation on the form of logic used makes the framework's practical utility and generality questionable.\n\n- Mixed Experimental Results: The method does not consistently outperform baselines. It notably performs worse than several baselines, including RLogic and RotatE, on the FB15K-237 dataset. The paper offers no substantive explanation for this failure, which undermines the general applicability of the SLogic framework."}, "questions": {"value": "- Could the authors please justify the novelty claim from the abstract (\"current approaches typically treat logical rules as universal...\") against prior work like pLogicNet, Markov Logic Networks, or other methods that learn rule weights/confidences? How is the subgraph-based context proposed here fundamentally different from other forms of contextual or dynamic rule scoring in the literature?\n\n- Why does the SLogic model perform poorly on FB15K-237 compared to other methods? What specific properties of this dataset (e.g., graph density, rule types) might cause a subgraph-informed approach to fail or underperform?\n\n- The method is strictly limited to relational paths. What are the main conceptual or technical barriers to extending this framework to support more complex rule structures, such as rules with multiple branches (e.g., $r_h(X,Y) \\leftarrow r_1(X,Z_1) \\wedge r_2(X,Z_2)$)? How would the subgraph-based scoring and grounding mechanisms need to be adapted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "O4khHUo1Ar", "forum": "UeGVl17AuK", "replyto": "UeGVl17AuK", "signatures": ["ICLR.cc/2026/Conference/Submission20803/Reviewer_orxV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20803/Reviewer_orxV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706131105, "cdate": 1761706131105, "tmdate": 1762935478452, "mdate": 1762935478452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key limitation in existing logical rule-based methods for Knowledge Graph Completion (KGC)—the reliance on static, query-agnostic rule confidence scores—by proposing SLogic, a framework introducing query-dependent, context-aware rule scoring. SLogic uses a GNN-based subgraph encoder to capture local structural context around the query head entity, enabling more precise assessment of candidate rule importance. The framework integrates symbolic rules with neural representations, combining offline rule mining with a contrastive learning-based scoring model. Experiments on WN18RR, FB15k-237, and YAGO3-10 show state-of-the-art performance on WN18RR and YAGO3-10."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper clearly identifies a fundamental limitation of static rule confidences and proposes a principled solution. The core concept of query-dependent, context-aware rule scoring represents a meaningful paradigm shift from global prevalence to local relevance in rule learning.\n\nThe proposed SLogic framework effectively integrates symbolic and neural approaches. It maintains the interpretability of symbolic rules by building upon a mined rule base while harnessing the representational power of GNNs to encode rich local subgraph context. This hybrid design successfully balances transparency and predictive performance."}, "weaknesses": {"value": "The authors argue that \"query-dependent scoring is more reasonable than static rules,\" but they lack a clear delineation of the specific categories of KGs or relation types for which this conclusion holds (e.g., scenarios with high/low relation counts, strong/weak hub structures, high/low rule coverage). Currently, this is supported only by post-hoc experimental observations (effectiveness on certain datasets), and there is no theoretical guidance or well-defined quantitative metrics (e.g., rule coverage thresholds, relation-sparsity indicators) to guide when to adopt this method.\n\nThe paper employs k-hop BFS with a fixed neighbor sampling threshold (α), but it lacks a theoretical or empirical analysis justifying this specific choice (e.g., why random sampling is preferable to degree-weighted or importance sampling). The method's first step selects only \"locally applicable rules ranked top-N by Wilson score\" as candidates. However, the Wilson score itself is influenced by body-count, and the definition of \"local applicability\" is contingent upon the subgraph extraction parameters (α, k). If different methods were to use different subgraph extraction strategies, the resulting \"comparable candidate sets\" could be biased, thereby affecting the fairness of the re-ranking comparison.\n\nThe results tables report only single-run values (point estimates), without means ± standard deviations across multiple random seeds or significance tests. This is particularly crucial when performance improvements are marginal or mixed across datasets, and confidence intervals are necessary for robust evaluation.\n\nDespite the use of LLMs for polishing, formatting, and grammatical errors remains. For instance: \"The instances generated in this step... comprise rule-enriched triplets, ...\" Here, the subscript i is inconsistent and should be 1. The paper exhibits inconsistent referencing of equations."}, "questions": {"value": "1. Can the authors provide quantitative or theoretical analysis clarifying which KG types (e.g., relation count, node degree, rule coverage) benefit most from SLogic? Any correlation analyses between KG structure and \nperformance?\n\n2. Were alternative neighbor sampling strategies (degree-weighted, importance sampling) evaluated? \n\n3 .NCRL shows discrepancies between reported scores (NCRL†) and scores under the authors’ protocol (NCRL*). What explains this? Were all baselines evaluated under identical candidate rule sets and inference procedures?\n\n4. Table 1 and the ablation study present results from a single run. Did the authors conduct multiple runs with different random seeds? If not, please supplement the results with the mean ± standard deviation from 3–5 independent runs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A38SVFpx5O", "forum": "UeGVl17AuK", "replyto": "UeGVl17AuK", "signatures": ["ICLR.cc/2026/Conference/Submission20803/Reviewer_4PdS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20803/Reviewer_4PdS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904269121, "cdate": 1761904269121, "tmdate": 1762935422839, "mdate": 1762935422839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SLogic, a subgraph-informed logical rule learning framework for Knowledge Graph Completion (KGC). SLogic could  incorporate query-dependent rule scoring, where the significance of each rule is dynamically recalculated using a subgraph centered on the query’s head entity. Extensive experiments on three datasets demonstrate that SLogic outperforms several embedding-based and rule-based baselines, while maintaining interpretability through explicit reasoning paths."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper’s key contribution of assigning dynamic rule weights conditioned on query subgraphs is well-motivated. It effectively bridges symbolic and neural reasoning.\n\n2. The overall pipeline, including rule mining, subgraph extraction, and query-specific scoring, is clearly explained and internally coherent. The use of the Wilson confidence score and contextual GNN embeddings reflects thoughtful design choices that strengthen robustness.\n\n3. SLogic demonstrates competitive or superior results to state-of-the-art rule-based and embedding methods on two of three benchmark datasets. The case study convincingly illustrates that the model adapts rule importance to distinct local contexts."}, "weaknesses": {"value": "1. The performance improvement is inconsistent, strong on WN18RR and YAGO3-10, but weaker on FB15k-237. The discussion attributes this to graph density and relation diversity, but a deeper diagnostic such as rule quality distribution, subgraph connectivity analysis would better substantiate the explanation.\n\n2. Training and negative sampling costs are significantly higher than baselines. While the cause is identified, potential optimizations such as subgraph pruning, caching, mini-batch rule evaluation are not explored.\n\n3. The ablation studies focus on inference components, but ignore analyses of relation embedding, subgraph encoder, rule encoder, and negative sampling strategy. Demonstrating their individual contributions would clarify the necessity of each module.\n\n4. Minor stylistic and typographic inconsistencies (such as some equations are numbered while others are not) slightly detract from readability, especially in the methodology section.\n\n5. Some significant and typical related works are neglected, such as joint rule and embedding-based models IterE [1] and RPJE [2]. These models are suggested to be added and compared.  \n[1] Iteratively Learning Embeddings and Rules for Knowledge Graph Reasoning. WWW 2019.  \n[2] Rule-Guided Compositional Representation Learning on Knowledge Graphs. AAAI 2020."}, "questions": {"value": "1. How does subgraph size (hop number) interact with rule length L in determining performance? Is there a trade-off between local and global reasoning depth?\n2. Could the authors elaborate on why SLogic underperforms on FB15k-237 despite its relatively rich relational structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "38UXJllB86", "forum": "UeGVl17AuK", "replyto": "UeGVl17AuK", "signatures": ["ICLR.cc/2026/Conference/Submission20803/Reviewer_fMyx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20803/Reviewer_fMyx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20803/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964423969, "cdate": 1761964423969, "tmdate": 1762935406518, "mdate": 1762935406518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}