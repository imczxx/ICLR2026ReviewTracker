{"id": "HhThhjKyfw", "number": 5740, "cdate": 1757930901022, "mdate": 1759897957230, "content": {"title": "WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training", "abstract": "Recent advances in learning rate~(LR) scheduling have demonstrated the effectiveness of decay-free approaches that eliminate the traditional decay phase while maintaining competitive performance. Model merging techniques have emerged as particularly promising solutions in this domain. We present Warmup-Stable and Merge (WSM), a general framework that establishes a formal connection between learning rate decay and model merging. WSM provides a unified theoretical foundation for emulating various decay strategies—including cosine decay, linear decay and inverse square root decay—as principled model averaging schemes, while remaining fully compatible with diverse optimization methods. Through extensive experiments, we identify merge duration—the training window for checkpoint aggregation—as the most critical factor influencing model performance, surpassing the importance of both checkpoint interval and merge quantity. Our framework consistently outperforms the widely-adopted Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving significant improvements of +3.5\\% on MATH, +2.9\\% on HumanEval, and +5.5\\% on MMLU-Pro. The performance advantages extend to supervised fine-tuning scenarios, highlighting WSM's potential for long-term model refinement.", "tldr": "", "keywords": ["llm pre-training", "learning rate schedule", "checkpoint merging", "decay-free approach"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ccbff71f6c7d8b3972c5c293af896f05de25d13.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Warmup-Stable and Merge (WSM), a novel and compelling learning rate (LR) scheduling framework for pre-training large language models. The core idea is to replace the conventional, inflexible LR decay phase with a \"decay-free\" approach. In WSM, after a standard warmup, the model is trained with a constant peak LR (the \"stable\" phase), during which checkpoints are periodically saved. The final performant model is then produced by applying a weighted average (merging) to a series of these saved checkpoints.\nThe authors make a significant theoretical contribution by establishing a formal connection between checkpoint merging and LR decay. They demonstrate that a weighted average of checkpoints is mathematically equivalent to applying a synthetic, effective decay schedule to the gradient updates accumulated since the first checkpoint in the merge window. This framework allows for the emulation of various decay curves (e.g., linear, cosine, 1-sqrt) by simply choosing the appropriate checkpoint weights, without ever modifying the live learning rate during training.\nThrough extensive experiments on a 16.3B parameter MoE model, the paper shows that WSM consistently and significantly outperforms the strong Warmup-Stable-Decay (WSD) baseline on a wide range of benchmarks, including MATH, HumanEval, and MMLU-Pro. The key empirical finding is that the \"merge duration\" (i.e., the time window over which checkpoints are aggregated) is the most critical factor for performance. The benefits of WSM are shown to persist even after supervised fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is the formal connection it establishes between LR decay and checkpoint merging (Section 3.1), which is insightful. \n\n- The experiments are conducted at a scale that is highly relevant to the LLM community (a 16.3B parameter model trained on trillions of tokens). \n\n- The paper provides a thorough empirical analysis of the WSM schedule. The authors methodically investigate the impact of the merging algorithm (Table 3), merge duration (Figure 4), and merge granularity (Table 4)."}, "weaknesses": {"value": "- **Significant Computational and Storage Overhead:**\n  - While the paper suggests transitioning to an online, sliding-window approach after identifying an optimal strategy, the initial exploration phase itself represents a significant, non-trivial cost. Can auther give a quantified results？\n  - The merging process also incurs computational overhead, specifically in terms of I/O for loading multiple large checkpoints and memory consumption, which is not fully quantified in the paper.\n\n- **Insufficient Baselines for Model Averaging:**\n  - While the paper compares its theoretically-derived merging weights against mean averaging and EMA, it overlooks other well-established trajectory averaging methods. A direct comparison with classic techniques like Stochastic Weight Averaging (SWA) would be highly valuable. \n\n- **Shift in Hyperparameter Complexity:**\n  - The paper claims that WSM simplifies LR scheduling by eliminating the decay phase. However, it arguably replaces one set of hyperparameters (decay function, duration) with another, potentially more complex set: merge duration/window size, checkpoint interval, and merge algorithm.\n  - The finding that \"merge duration\" is the most critical factor highlights that WSM does not eliminate the need for extensive tuning; it merely shifts the tuning process from an online LR schedule design to an offline search for the optimal merging strategy. This trade-off should be more explicitly discussed."}, "questions": {"value": "- see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y4woGg1sEc", "forum": "HhThhjKyfw", "replyto": "HhThhjKyfw", "signatures": ["ICLR.cc/2026/Conference/Submission5740/Reviewer_852L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5740/Reviewer_852L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760531329797, "cdate": 1760531329797, "tmdate": 1762918232175, "mdate": 1762918232175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes merging checkpoints as an alternative to learning rate annealing.\nIn particular, warmup-stable-merge (WSM) is proposed as a replacement for the warmup-stable-decay (WSD) learning rate schedule,\nwhich is common for LLM pre-training.\nThere is some theory to provide intuition. Results are good.\nSome analysis experiments investigate which factors are more important."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* WSM is a simple method which provides sufficient improvement over WSD and is convenient enough to implement and use that I support the paper's proposal to adopt it instead of WSD.\n\n* The experiments are comprehensive."}, "weaknesses": {"value": "* This work has limited novelty.\nThe connection between LR schedules and weight averaging is well-known and has more complete treatment elsewhere [1,2].\nThis includes almost the entirety of Section 3 and forms the main contribution of the paper.\nFurther, the theoretical treatment in this paper is rather hand-wavy.\n\n* The proposed methodology is also not surprising, although maybe it hasn't appeared in this packaging before.\nWeight averaging is a common practice, often applied with annealing [3].\n\n* The authors identify the \"merge duration\" as the most significant factor and suggest an offline computation to compute the merged model. However, from the experiments it is not clear why or if merging along the entire training trajectory would hurt. That would be much easier to implement online without memory overhead by maintaining a running average. For example, this is the recommendation in [4].\n\n[1] Straight to zero: Why linearly decaying the learning rate to zero works best for LLMs. ICLR 2025.\n\n[2] How to set AdamW's weight decay as you scale model and dataset size. ICML 2025.\n\n[3] The Llama 3 Herd of Models. See Section 3.4.3.\n\n[4] Post-Hoc Reversal: Are We Selecting Models Prematurely? NeurIPS 2024. Search for SWA."}, "questions": {"value": "Some concerns / questions / comments:\n\n* Section 5.2: What you are calling model merging in this paper is more commonly termed as Stochastic Weight Averaging (SWA). To my knowledge, \"model merging\" is used in the context where the models come from different training/fine-tuning runs. i.e. (1) in L460 is \"model merging\" but (2) in L463 is SWA. At the very least, SWA should be mentioned and discussed in the paper as this is the official name used by the 2018 paper (which you have cited under model merging in L458).\n\n* WSD is not exactly schedule-free as it has been found that optimal LR depends on the decay start time [1]. By extension WSM is also not schedule-free. Please fix this claim, or at least add a caveat and discussion.\n\n* L115: You still need / will benefit from LR tuning, so please remove this line.\n\n* L257: To help the readers, please clarify that this corresponds to a linear decay LR schedule.\n\n* What is the intuition for why WSM is better than WSD? Can you do some empirical analysis to support the intuition?\n\n* Why is there a systematic dip in all plots towards the end in Figure 4?\n\n* L348: \"this finding reinforces the theoretical connection\"... I believe you need Figure 5a style curves to rigorously establish this.\n\n* L429: Is \"language modeling loss\" column the train loss or test loss?\n\n* L483: The theory is not exactly optimizer-agnostic with a more careful analysis as in [2]. For example, the weight decay constant $\\lambda$ is an important part of the SWA weight terms.\n\n[1] Straight to zero: Why linearly decaying the learning rate to zero works best for LLMs. ICLR 2025.\n\n[2] How to set AdamW's weight decay as you scale model and dataset size. ICML 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Uix1J3qDod", "forum": "HhThhjKyfw", "replyto": "HhThhjKyfw", "signatures": ["ICLR.cc/2026/Conference/Submission5740/Reviewer_fGtp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5740/Reviewer_fGtp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961768259, "cdate": 1761961768259, "tmdate": 1762918231961, "mdate": 1762918231961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a theoretically grounded approach to model merging, which they suggest could fully replace LR decay schedules for WSD. This would be great! The basic idea is that, since every theta_k is a weighted combination of the gradients so far, you can choose a particular weighted average of the theta_k to emulate any decay schedule you please. They show the algebra to justify this and then a bunch of experiments at a more-than-reasonable scale that very convincingly show that you can, on tasks, do every bit as well as a proper decay (if not better)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Overall I really like this paper! It's well written and argued, and the empirical results are quite strong. In terms of significance, this has a non-trivial chance of becoming the standard way people produce final model checkpoints.\n\nThe theoretical derivation of the equivalence is simple in a wonderful way (though I worry a bit specious). It's of course just algebra, but I have no problem with that. The intuition is the same one that motivates EMA and SWA etc.\n\n The empirical results are very strong, with across the board accuracy gains relative to EMA (another decay-free strategy) and even over WSD itself. \n\nThey also do a fairly good job defending the proposed equivalence: averaging schemes derived from stronger decay schedules (1-sqrt, linear) outperform those derived from weaker schedules (exponential decay--> EMA); and they show little to no benefit of combining a real decay with averaging, strengthening the claim that they are in some sense equivalent."}, "weaknesses": {"value": "I have two categories of concerns:\n\n### Experiments don't really demonstrate connection between theory and practice.\n\nMy biggest concern is that, algebra-aside, the theory is a bit specious. The derivation assumes that gradients would be ~the same in a decay versus non-decay setup, and that can't be true. So, really I think this is \"decay-inspired averaging\" or \"empirical substitute\" more than an equivalent/replacement.\n\nTo that end, there aren't any experiments directly comparing the supposedly equivalent decay schedules with merges, except for the linear decay schedule, and even that experiment isn't fully convincing. I get that you can't afford to do this for a model trained to 10T tokens, but I would expect it wouldn't be too hard to do this for a smallish model?\n\nFor instance, if you were to really believe the theory strongly enough, you could, after warmup, keep the LR hot the entire time and recover a cosine decay via appropriate weighting. Right? Do experiments support this? If not, where does the theory/reality connection break down?\n\nI would expect that the losses wouldn't be that similar, because this approach assumes the gradients you get are similar enough in a decay vs non-decay setup that the averaging is ~equivalent to decay.\n\nI'd expect these to show up in loss plots rather than accuracy plots. I note that loss plots are conspicuously absent from the paper, which to me suggests that WSM isn't so much \"equivalent to WSD\" but more \"just as good as\" (which is more than good enough, given the advantages!). I just think we should see these results and adjust the positioning of the paper as appropriate.\n\n### Can't avoid data scheduling\n\nAnother, smaller weakness is that, in the age of \"mid-training\", you still have to decide when to start including higher quality data. In typical WSD setups, that is also when you start the decay. So you don't really have the nice any-time property that you might want from a truly decay-free world. I'm not sure how to work around this. It just (slightly) diminishes the promise of this approach."}, "questions": {"value": "As noted above, it would be nice if you were able to conduct some preliminary experiments comparing other decay schedules at a modest scale 150M/3B or something. (A different scale is fine too.)\n\nAgain, if you fully believe the theory, you should be able to approximately recover a 100% cosine schedule by starting your averaging/merging from the end of warmup. I think?\n\nI recognize that your pipeline isn't set up for this (and I think it was a good choice for your experiments), and I don't expect you to run a large experiment here, but in my experience with WSD with EMA merging, we just update the merged theta at every step. This is obviously a lot more merges, but it's cheap and online. One could also do this with mean averaging. And Figure 4 indicates that it's definitely better to merge more often! how does maintaining a running average every step compare?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eb7bAoa6UC", "forum": "HhThhjKyfw", "replyto": "HhThhjKyfw", "signatures": ["ICLR.cc/2026/Conference/Submission5740/Reviewer_kALL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5740/Reviewer_kALL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020649635, "cdate": 1762020649635, "tmdate": 1762918231739, "mdate": 1762918231739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an alternative approach to WSD (Warmup-Stable-Decay) learning rate schedule. It proposes to use a constant learning rate and then apply weight merging on the saved checkpoints. The authors establish an informal correspondence between the two methods and show empirically some of the weight merging schedules can result in higher downstream performance than the WSD method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The experiment in this paper is large-scale and detailed. The models are evaluated over a range of hard benchmarks to show the improvement of the methods.\n\n2. The proposed method is conceptually clean and can be useful in many settings.\n\n3. The theoretical justification of the method, while not rigorous, provides intuitions in guiding the design of the algorithm."}, "weaknesses": {"value": "1. In Table 5, the authors show that WSM has a higher ending loss compared to WSD. This seems to be counterintuitive, especially given the higher downstream accuracy. This also brings questions regarding the validity of the correspondence discussed in Section 3.1.\n\n2. Regarding continual pretraining, the authors propose to continual pretrain from the constant learning rate checkpoints before weight merging. It is unclear how this will compare with re-warming up the decay checkpoint in a standard WSD setting."}, "questions": {"value": "1. The authors mention sensitivity regarding the weight merging schedules. Is there similar sensitivity in the decay schedule in WSD? Also, is the ranking between the weight merging schedules consistent with the corresponding learning rate schedule?\n\n2. Do the authors observe differences in the ranking of weight merging schedules when choosing different downstream evaluation tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Siap8ELz0X", "forum": "HhThhjKyfw", "replyto": "HhThhjKyfw", "signatures": ["ICLR.cc/2026/Conference/Submission5740/Reviewer_hHGj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5740/Reviewer_hHGj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5740/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762035873379, "cdate": 1762035873379, "tmdate": 1762918231348, "mdate": 1762918231348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}