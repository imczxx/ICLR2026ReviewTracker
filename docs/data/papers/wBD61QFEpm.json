{"id": "wBD61QFEpm", "number": 24333, "cdate": 1758355578067, "mdate": 1763102612239, "content": {"title": "Multimodal Fusion of RGB and Complementary Modalities for Semantic Segmentation", "abstract": "Multi-modal semantic segmentation augments RGB imagery with an auxiliary sensing stream X (RGB+X), e.g., thermal, event, LiDAR, polarization, or light field, to improve robustness under adverse illumination and motion blur. We target two coupled bottlenecks in RGB+X segmentation: selecting the most predictive modality at each location and aligning semantics across different modalities. The proposed framework performs token-wise auxiliary selection to activate a single, reliable auxiliary stream per token and applies style-consistent, polarity-aware cross-modality fusion that transfers auxiliary appearance statistics to RGB features while preserving both supportive and contradictory evidence. We evaluate across five modality pairings: RGB+Thermal, RGB+Event, RGB+LiDAR, RGB+Polarization, and RGB+Light Field and obtain new state of the art on each. Representative results include 76.89% mIoU on MFNet (RGB-Thermal) and 52.54% mIoU on MCubeS (RGB+A+D+N) and other combinations, surpassing recent fusion frameworks under comparable backbones and training protocols. Overall, this selective, alignment-aware fusion design provides a robust path to better RGB+X segmentation without sacrificing efficiency.", "tldr": "", "keywords": ["Semantic segmentation", "multimodal fusion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e566276487dea50118f4cef924fbfa2124b604fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a bidirectional polarity-aware cross-modality fusion method that effectively captures complementary cues while enhancing feature alignment. The proposed method is evaluated on multiple benchmarks and achieves large gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed solution is verified to be effective on different RGB-X segmentation benchmarks.\n2. The proposed solution achieves large accuracy gains on the MFNet dataset.\n3. Extensive ablation studies and analyses are presented."}, "weaknesses": {"value": "1. It would be nice to compare the proposed fusion module against a list of existing fusion modules to better illustrate its superiority and computational efficiency.\n2. The ablation study helps to show the gains of the architecture on MFNet. It would be nice to add more qualitative or interpretable analyses (e.g., feature visualizations, t-SNE visualizations) to help better understand the effectiveness of the proposed method. Why did existing methods fail to achieve high performance on MFNet? Why does BPLCA have a great benefit for RGB-T segmentation? These behaviors should be discussed and analyzed in detail.\n3. More recent state-of-the-art general multimodal segmentation models like OmniSegmenter, MemorySAM, and AnySeg could be compared and discussed."}, "questions": {"value": "1. How about RGB-D segmentation? This could be discussed, as RGB-D segmentation is one of the main benchmarks in RGB-X vision.\n2. Would you consider presenting the detailed results in different modality-degradation scenarios on the DeLiVER benchmark?\n3. Would you consider some experiments using different backbones? This would better verify the generalization capacity of your method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HVi4Moj8an", "forum": "wBD61QFEpm", "replyto": "wBD61QFEpm", "signatures": ["ICLR.cc/2026/Conference/Submission24333/Reviewer_dFie"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24333/Reviewer_dFie"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760604688943, "cdate": 1760604688943, "tmdate": 1762943046677, "mdate": 1762943046677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "YdVIyNDabs", "forum": "wBD61QFEpm", "replyto": "wBD61QFEpm", "signatures": ["ICLR.cc/2026/Conference/Submission24333/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24333/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763102611473, "cdate": 1763102611473, "tmdate": 1763102611473, "mdate": 1763102611473, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for multi-modal semantic segmentation, designed to fuse RGB data with various complementary modalities. The central contribution is the Bidirectional Polarity-aware Cross-modality Fusion (BPCF) module, which aims to address feature misalignment and effectively integrate complementary information from heterogeneous sensors. The BPCF module consists of two main components: a Bidirectional Polarity-aware Linear Cross-Attention (BPLCA) mechanism to capture both positively and negatively correlated signals, and a Dual Feature Consistency Constraint (DFCC) with a stage-wise loss to enforce feature alignment. The overall architecture is a dual-encoder system that uses ScoreNet to dynamically select the most informative auxiliary modality at the patch level. The authors conduct extensive experiments across eight public datasets and five modality pairings to validate the method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized and easy to follow.\n2. The proposed method is evaluated on eight distinct datasets covering five different RGB+X combinations, supporting the framework's versatility and generalizability."}, "weaknesses": {"value": "1. The overall framework heavily relies on prior works like CMNeXt, adopting its dual-encoder structure. The main novelty is confined to the BPCF module. However, the BPCF module itself appears to be a complex amalgamation of existing techniques without a clear, simple motivation for why this specific combination is necessary.\n2. The use of ScoreNet for patch-level modality selection is a coarse approach. The fixed patch grid does not align with semantic content, potentially splitting a single object and forcing an arbitrary modality choice for its constituent parts.\n3. The paper reports a remarkable 17% absolute mIoU improvement on the MFNet dataset, but the gains on other datasets are far more modest. Whether the method is over-tuned to this dataset? There should be a detailed analysis explaining this phenomenon."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bHeM7Oc1fk", "forum": "wBD61QFEpm", "replyto": "wBD61QFEpm", "signatures": ["ICLR.cc/2026/Conference/Submission24333/Reviewer_k5UA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24333/Reviewer_k5UA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761294549596, "cdate": 1761294549596, "tmdate": 1762943046362, "mdate": 1762943046362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for multimodal semantic segmentation that combines RGB images with other complementary sensing modalities, including thermal, LiDAR, event, polarization, and light-field data. The main technical contribution is a bidirectional polarity-aware cross-modality fusion (BPCF) module, which includes a polarity-aware linear cross-attention (PLCA) mechanism and a dual feature consistency constraint (DFCC). The authors evaluate their approach on eight different benchmarks covering five auxiliary modalities, and report strong performance, often achieving state-of-the-art results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Figures throughout the paper (e.g., Fig. 1, 2, 3, 4, 5) provide clear visualizations of results, architecture, and information flow, aiding in the comprehension of nontrivial design choices.\n2. This paper is easy to follow."}, "weaknesses": {"value": "1. The proposed polarity-aware attention mechanism (Eq. 11, Page 5) adds extra computation compared to standard linear attention because of its explicit decomposition and gating operations. Although the authors claim that it operates in linear time and memory (Page 4), the paper does not provide any wall-clock runtime or memory usage analysis. This lack of practical efficiency evaluation could make it difficult to assess the method’s feasibility for resource-constrained deployment or for scenarios involving more than two modalities.\n2. In Section 3.1 and Eqs. 1–2 (Page 3), ScoreNet is used to dynamically select the most informative patches from the auxiliary modalities. However, the paper provides little guidance on hyperparameter choices, threshold settings, or how ScoreNet handles ambiguous cases such as ties. It is also unclear how the method behaves if ScoreNet consistently suppresses a particular modality, which could lead to modal dominance or imbalance.\n3. On the DELIVER dataset (Table 4), the method is evaluated at a reduced input resolution (512 × 512) due to resource constraints, whereas some competing methods use higher resolutions (1024 × 1024). While this limitation is acknowledged, the resulting score differences may be less meaningful. The paper also does not provide a detailed error analysis to quantify how this downscaling affects performance.\n4.  The main novelty of the paper lies in the architectural design for feature fusion. However, the authors do not provide any analysis of latency, throughput, or real-time feasibility. This is particularly important given the use of Transformers and multiple input modalities, which are relevant for time-sensitive applications such as autonomous driving.\n5. While ScoreNet (Eqs. 1–2) is justified as a mechanism for dynamically selecting the most informative patches, the paper provides little visualization or explanation of what features it actually selects in ambiguous or complex multi-modal regions. Including an analysis—such as heatmaps or selection frequency statistics—would strengthen the claims regarding the importance and effectiveness of adaptive modality selection."}, "questions": {"value": "Please refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sttcegkeL7", "forum": "wBD61QFEpm", "replyto": "wBD61QFEpm", "signatures": ["ICLR.cc/2026/Conference/Submission24333/Reviewer_groU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24333/Reviewer_groU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959692915, "cdate": 1761959692915, "tmdate": 1762943046103, "mdate": 1762943046103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to solve RGB-X semantic segmentation, where RGB is complemented by one of several modalities (thermal, LiDAR, event, polarization, light field). The authors propose a transformer-based, dual-encoder architecture with stage-wise fusion and introduce a Bidirectional Polarity-aware Cross-modality Fusion (BPCF) module. BPCF itself is made of (i) a Bidirectional Polarity-aware Linear Cross-Attention (BPLCA) that splits features into positive/negative parts and does cross-modal attention in both directions, and (ii) a Dual Feature Consistency Constraint (DFCC) that normalizes the two branches and enforces a stage-wise consistency loss. The proposed network is evaluated on five modality parings (RGB+T, RGB+Event, RGB+Lidar, RGB+Polarization, RGB+Light Field) on eight public datasets (MFNet, KITTI-360, DDD17, DSEC, DELIVER, UrbanLF-real/syn, MCubeS, ZJU). it shows performance improvements over CMX, CMNeXt, and a few recent modality-agnostic methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Unified evaluation across many RGB+X pairs.\n* Testing on RGB+Thermal, RGB+Event, RGB+LiDAR, RGB+Polarization, and RGB+Light Field in one framework is valuable. Because many existing fusion papers only show one or two pairs. This makes the claim of “modality-agnostic” a bit more convincing."}, "weaknesses": {"value": "1. Novelty on fusion architecture over existing RGB+X works is weak.\n* The core claim is that BPCF/BPLCA “captures complementary cues and aligns heterogeneous features.” But this idea is generally shared over all RGB+X frameworks, such as CMX (cross-modal rectification + fusion), CMNeXt (self-query hub + parallel pooling), and Any2Seg (modality-agnostic fusion). They have two branches and cross-attention in both directions, and a feature-alignment/rectification step. Here, “polarity-aware” is mostly a kernelized linear attention with positive/negative decomposition on top of it. That is an incremental change, not a qualitatively new fusion paradigm. This is still a “dual encoder + cross-attention + consistency” network, like many RGB-X papers in 2022–2025. \n\n2 Performance gains are not significant enough to be the contribution.\n* The authors emphasize the MFNet jump (≈59.9 → 76.9 mIoU) as a “17% absolute” gain. But that is the best case. On other datasets, the performance improvements are quite marginal, making it difficult to consider the performance improvement itself a standalone contribution."}, "questions": {"value": "Please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "G0kpV1f8op", "forum": "wBD61QFEpm", "replyto": "wBD61QFEpm", "signatures": ["ICLR.cc/2026/Conference/Submission24333/Reviewer_myaP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24333/Reviewer_myaP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24333/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979579544, "cdate": 1761979579544, "tmdate": 1762943045760, "mdate": 1762943045760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}