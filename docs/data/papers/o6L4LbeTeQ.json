{"id": "o6L4LbeTeQ", "number": 12547, "cdate": 1758208502378, "mdate": 1759897502462, "content": {"title": "Agent-Centric Personalized Multiple Clustering with Multi-Modal LLMs", "abstract": "Personalized multiple clustering aims to generate diverse partitions of a dataset based on different user-specified aspects, rather than a single clustering. It has recently drawn research interest for accommodating varying user clustering preferences.\nRecent approaches primarily use CLIP embeddings with proxy learning to extract representations biased toward user interests. However, CLIP primarily focuses on coarse image-text alignment, lacking a deep contextual understanding of user interests.\nTo overcome these limitations, we propose an agent-centric personalized clustering framework that leverages multi-modal large language models (MLLMs) as agents to comprehensively traverse a relational graph to search for clusters based on user interests. Due to the advanced reasoning mechanism of MLLMs, the obtained clusters align more closely with user-defined criteria than those obtained from CLIP-based representations. To reduce computational overhead, we shorten the agents' traversal path by constructing a relational graph using user-interest-biased embeddings extracted by MLLMs. \nA large number of weak edges can be filtered out based on embedding similarity, facilitating an efficient traversal search for agents. Experimental results show that the proposed method achieves NMI scores of 0.9667 and 0.9481 on the Card Order and Card Suits benchmarks, respectively, largely improving the SOTA model by over 140%. Code is available at \\url{https://anonymous.4open.science/r/Agent-Centric-Clustering-BEA5/}.", "tldr": "", "keywords": ["Personalized Multiple Clustering", "Multi-Modal LLMs", "Agent"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd2326b59bd5cc0d461e623b58dcea93152b7090.pdf", "supplementary_material": "/attachment/bb4299e46f0d6a4babc950f3fc4f4eee01074f01.pdf"}, "replies": [{"content": {"summary": {"value": "This paper focuses on personalized multiple clustering and proposes a framework with multi-modal large language models (MLLMs): it first extracts user interest-biased embeddings via MLLMs to construct a weak edge-filtered sparse relational graph, then deploys multiple MLLMs for parallel graph traversal to search interest-aligned clusters, and finally uses a global MLLM to merge redundant clusters; with MLLMs fine-tuned lightweight via LoRA and GPT-4 generating supervised pseudo-labels, the framework achieves good performance on some multiple clustering datasets.While the method demonstrates strengths in fine-grained user interest modeling, efficiency optimization via sparse graph structures, and rigorous experimental validation, it is constrained by several limitations, including high computational costs, limited coverage of datasets and baselines, and insufficient algorithmic innovation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Fine-Grained User Interest Capture: The method leverages Multimodal Large Language Models (MLLMs) to supplant conventional CLIP-based alignment, enabling a deeper, reasoning-based understanding of context. This is achieved through customized prompts that elicit fine-grained semantic embeddings, thereby capturing nuanced user preferences.\n\n2. Sparse Relational Graph Design: To mitigate the computational overhead from excessive agent traversal in fully-connected relational graphs—a consequence of redundant weak edges—the proposed approach constructs a sparse graph. This is done by computing edge weights from MLLM-generated user interest embeddings and subsequently pruning weak edges using a fixed threshold (τ = 0.6)."}, "weaknesses": {"value": "1. High Computational Cost: High Computational Cost: The use of Qwen2-7B (7B parameters) requires 4 NVIDIA A100 GPUs for 3 days, limiting practicality for resource-constrained teams. But in practice, the improvement on the dataset is not significant.\n\n2. Limited Dataset & Baseline Coverage: Experiments focus on high-quality datasets but lack tests on real-world or cross-domain tasks (e.g., datasets of different art styles and those on food ripeness levels). Comparisons to low-cost concurrent deep clustering methods are also missing. Several recent clustering works are very relevant to the proposed method. The authors are encouraged to include them in the related works, and if possible, make some comparisons with them.\n\tInteractive Deep Clustering via Value Mining, NeurIPS 2024\n\tPersonlized Clustering via Targeted Representation Learning, AAAI 2025\n\tImage Clustering with External Guidance, ICML 2024\n\n3. Lack of Algorithmic Innovation: The method relies heavily on supervisory signals from GPT-4 and the feature extraction capabilities of the MLLM rather than achieving good performance through innovative algorithm design. The algorithm itself lacks novel contributions, as its core components build on existing paradigms."}, "questions": {"value": "1. Real-World Applications: Where can this personalized clustering method be applied in real-world scenarios? Could the authors provide specific examples or domains where user interest-based clustering is critical?\n2. Learning Without Supervisory Signals from Large Models: For tasks where large models cannot provide supervision signals (e.g., determining whether a portrait is beautiful), how should the learning process be designed?\n3. Supervised Classification vs. Clustering: Since GPT-4 can provide supervisory signals, why not directly label some images with GPT-4 and use supervised classification methods to achieve the same goal, rather than constructing clusters?\n4. Future of Vision Models: Incidentally, does the author believe that all image-related tasks (e.g., classification, clustering, segmentation) eventually will be solved by large vision models, similar to how large language models have addressed many language-related tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cAEKJBV6Pr", "forum": "o6L4LbeTeQ", "replyto": "o6L4LbeTeQ", "signatures": ["ICLR.cc/2026/Conference/Submission12547/Reviewer_KbNJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12547/Reviewer_KbNJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830659701, "cdate": 1761830659701, "tmdate": 1762923407762, "mdate": 1762923407762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper liberates multiple clustering from the CLIP-based representation model and transforms it into a retrieval and query task, avoiding the inherent problem of mismatch between representations and downstream tasks. This is the most essential difference between this method and the existing ones."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1- The proposed model seems simple and easy to implement, and has relatively high flexibility for real-world scenarios.\n\nS2- As far as I know, this is a relatively new paradigm, which is different from the previous representation learning-based methods, because there is an upper performance limit for moving the representation distribution from a universal feature space to a user-specific feature space.\n\nS3- The performance on many datasets has been significantly improved compared to existing methods."}, "weaknesses": {"value": "W1- The writing of this paper before the fifth page is clear. From the sixth page on, some expressions are difficult to understand. I will clarify this point in the questions, and hope that the author will further clarify it in the rebuttal stage.\n\nW2- The definition of connected components in Eq. 3 should be unclear; a more accurate description would be the largest connected component; The $w(u, v)$ in Eq. 2 is confusing to read. It is not until I continue reading Eq. 8 that I know its definition.\n\nW3- The author's code links are empty, but the appendix provides code examples for each module.\n\nW4- The processes in Figures 3b and 4b and the corresponding main text descriptions are not clear. It took me some time to understand them. I suggest improving this part of the content.\n\nW5- I think the author did not clearly explain the essential drawbacks of CLIP-based models, why CLIP's alignment mode hinders the learning ability of fine-grained, user specific features.\n\nW6- How about the performance of an MLLM without fine-tuning? This point requires supplementary experiments to prove."}, "questions": {"value": "Q1- In line 304, \"The generated descriptions are aligned with GPT-4's outputs using cross-entropy loss.\" How does it correspond to Figure 3b? What does it mean when \"Embedding: <embedding>\" and \"Generation\" both point to \"Append\"? Whose Embedding is \"Embedding: <embedding>\"?\n\nQ2- Similarly, in Figure 4, is the “Supervision” trained by entering the entire text or a binary label of“No”?\n\nQ3- It is necessary to discuss whether fine-tuning the MLLM is necessary. Inputting images into the existing MLLM webpage-based model seems to be able to distinguish the categories of images very well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yThVBryKRi", "forum": "o6L4LbeTeQ", "replyto": "o6L4LbeTeQ", "signatures": ["ICLR.cc/2026/Conference/Submission12547/Reviewer_b5gz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12547/Reviewer_b5gz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929566586, "cdate": 1761929566586, "tmdate": 1762923407435, "mdate": 1762923407435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an agent-centric personalized multiple clustering framework that leverages multi-modal large language models (MLLMs) to traverse a relational graph and identify clusters based on user preferences. The proposed method aims to overcome the limitations of CLIP-based embeddings by using MLLMs to provide a more context-aware and user-interest-driven approach to clustering. The paper demonstrates the framework on various datasets, reporting improvements in performance metrics like NMI and RI, surpassing state-of-the-art models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The framework introduces an interesting agent-based approach to multiple clustering. Leveraging MLLMs for embedding generation and graph traversal is an innovative step to incorporate context-aware embeddings.\n- The results are promising."}, "weaknesses": {"value": "- The theoretical foundation for using MLLMs for clustering is weak. I recommend authors to provide a deeper analysis of why MLLMs are particularly suited for this task compared to other methods.\n- Why is the threshold for graph density set at 0.6, and what are the effects of different thresholds on the performance of the method?\n- The method’s explanation could be more structured, and the steps in the algorithm require further clarification. In particular, the paper does not explain how the clustering performance improves with the agent-centric approach versus traditional methods in a manner that ties back to the user-defined preferences."}, "questions": {"value": "- Can you provide a more detailed theoretical justification for why MLLMs, specifically, are well-suited for clustering tasks over other methods like CLIP?\n- How sensitive is the algorithm to the graph density threshold?\n- The paper claims to improve on prior methods by 140%, but the reported NMI and RI scores do not show such drastic improvements in many cases. Can you provide more context to understand the significance of these improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FugoNogLJx", "forum": "o6L4LbeTeQ", "replyto": "o6L4LbeTeQ", "signatures": ["ICLR.cc/2026/Conference/Submission12547/Reviewer_Gwhj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12547/Reviewer_Gwhj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970752941, "cdate": 1761970752941, "tmdate": 1762923407138, "mdate": 1762923407138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to generate image clustering results according to user-specified interest by taking benefit of the rich information in multi-modal large language models (MLLM). Specifically, according to the user's interest, a MLLM will be fine-tuned using pairs of images pseudo labeled by GPT with the user's interest specified. Then, the trained MLLM can be used to extract the corresponding image embedding. Those embeddings would be used to measure the similarity between data points to build graphs. Thereafter, each agent would be assigned to each connected component in the graph to search clusters using the similarities based on the embeddings. Moreover, these agents are fine-tuned by cluster vs candidate membership using GPT's assessment. The proposed method provides better performance no most multiple clustering datasets compared to existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) Multiple clustering according to user's preference is an important and interesting problem. It is also sound to take the benefits of the MLLM to gather interest-based embeddings for personalized clustering. Similarity-graph based cluster search using the agent is also interesting and reasonable.\n\n2) The proposed method provides better performance (sometimes significantly) over various multiple clustering datasets. The contribution of the MLLM embeddings and the graph search way are both demonstrated in the ablation study."}, "weaknesses": {"value": "1) Some technical details are missing in the presentation. For example, the similarity between embeddings are calculated with a learnable logit scale. It is not clear how this is learned and in which step? Secondly, clusters with similar semantic will be merged in the end. However, this merging step was not clearly discussed or evaluated.\n\n2) For the proposed method, it seems that both the embedding extractor and the membership assessment agent need fine-tuning for different use interest on different datasets. Thereafter, the additional training cost is significant. Moreover, compared to end-to-end deep multiple clustering or decoupled clustering using k-means or so, the cost of additional graph search using agents is also way significant. The discussion on this is necessary. \n\n3) For the above point in 2), it is necessary to conduct experiment to demonstrate that fine-tuning of those two models are necessary. For example, without fine-tuning, what is the performance. Similarly, without the membership agent, how the performance is degenerated?\n\n4) In Table 1, some baselines like MCV or DDMC were not discussed at all without any references. What are these?"}, "questions": {"value": "Specific questions can be found in the above weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6b4J5lTHU5", "forum": "o6L4LbeTeQ", "replyto": "o6L4LbeTeQ", "signatures": ["ICLR.cc/2026/Conference/Submission12547/Reviewer_Wcy9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12547/Reviewer_Wcy9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12547/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117619946, "cdate": 1762117619946, "tmdate": 1762923406884, "mdate": 1762923406884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}