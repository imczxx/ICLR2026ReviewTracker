{"id": "EFYb8SsRi7", "number": 11770, "cdate": 1758203651936, "mdate": 1763741962906, "content": {"title": "Error Analysis of Discrete Flow with Generator Matching", "abstract": "Discrete flow models offer a powerful framework for learning distributions over discrete state spaces and have demonstrated superior performance compared to the discrete diffusion model. However, their convergence properties and error analysis remain largely unexplored. In this work, we develop a unified framework grounded in stochastic calculus theory to systematically investigate the theoretical properties of discrete flow. Specifically, we derive the KL divergence of two path measures regarding two continuous-time Markov chains (CTMCs) with different transition rates by developing a novel Girsanov-type theorem, and provide a comprehensive analysis that encompasses the error arising from transition rate estimation and early stopping, where the first type of error has rarely been analyzed by existing works. Unlike discrete diffusion models, discrete flow incurs no truncation error caused by truncating the time horizon in the noising process. Building on generator matching and uniformization, we establish non-asymptotic error bounds for distribution estimation. Our results provide the first error analysis for discrete flow models.", "tldr": "", "keywords": ["Discrete Flow", "Error Analysis", "Distribution Estimation", "Error Bound"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3b209aca03cf2c98cbfd40d77feb685e2f9f12c8.pdf", "supplementary_material": "/attachment/303085e69e83576cb66d36c2d1c23cce5c2bf531.zip"}, "replies": [{"content": {"summary": {"value": "This paper theoretically investigates the Discrete Flow Model (DFM). Specifically, the authors establish a change-of-measure framework between two Continuous-Time Markov Chains (CTMCs) with different transition rates and subsequently provide a formula on the Kullback-Leibler (KL) divergence of their path measures. Based on this framework, they provide a non-asymptotic convergence analysis for discrete flow models, which further considers the estimation error through a function class (e.g., neural networks). They partition the total distribution error into three distinct terms: stochastic error caused by the randomness of the data samples, approximation error that stems from the expressive limitations of the function class, and, finally, early-stopping error."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper presents the first theoretical analysis for the recently proposed discrete flow model.\n2. The error decomposition presented in Section 5 is clearly written and highly intuitive, and the stochastic error bound (Theorem 3) for the estimation is a significant contribution that has been less explored in existing theoretical works.\n3. The authors provide results for both general function classes and specific ReLU networks, which successfully demonstrates the generality of their theory."}, "weaknesses": {"value": "1. The results on the general KL divergence bound (Theorem 1 and Theorem 2) are very similar to existing work for discrete diffusion models. I believe the authors should further elaborate on the novelty of this part; otherwise, it may appear to be a simple extension of previous works.\n2. Although this is primarily a theoretical work, it would be beneficial to include empirical validations, such as numerical experiments, to demonstrate how the theory translates to practice."}, "questions": {"value": "1. What is the main technical difficulty or contribution in Theorem 1 and Theorem 2 when compared to existing results for discrete diffusion models?\n2. Although the uniformization algorithm provides exact simulation for CTMCs, it has a random running time. Is it possible, within this work's framework, to further analyze the error bound for algorithms with a deterministic running time (e.g., through truncation in the uniformization or by using other sampling algorithms)?\n\n**Other Suggestions:**\n\n1. Line 033: \"Discrete flow models...**provide**...\";\n2. Line 077: \"**Hamming** distance\";\n3. Line 277: \"...two marginal distributions **regarding** two CTMCs...\";\n4. Page 23 in Appendix contains significant blank space; please consider reformatting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cc0Fg0P3lT", "forum": "EFYb8SsRi7", "replyto": "EFYb8SsRi7", "signatures": ["ICLR.cc/2026/Conference/Submission11770/Reviewer_X6xy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11770/Reviewer_X6xy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11770/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761029208498, "cdate": 1761029208498, "tmdate": 1762922794892, "mdate": 1762922794892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides with error bounds for a class of discrete flow-matching models. Starting from conditional transition rates $ u_t(y,x_t|x_1) $, and an empirical data distribution $ (Z_i)_{i \\leq N} $ the models considered here work in two phases as usual. In the first phase, a family of Markovian mimicking rates $ \\hat{u}_t(z,x) $ is learned through empirical risk minimization. If the Bregman divergence used in this phase, the empirical risk is an approximation of the KL on path space between the Markov chain with rates $ u_t(z,x) $ and the law of the non-Markov process with conditional transition rates $ u_t(y,x_t|x_1) $. In the second phase, the terminal distribution of Markov chain with rates $ \\hat{u}_t(z,x) $ is sampled, starting from the initial distribution applying the so-called uniformization technique. Importantly, this sampling algorithm avoids time discretization.\n\nThe main result of this paper is a bound between the sampled distribution and the data distribution which scales almost linearly in the underlying dimension $\\mathcal{D}$. Three sources of errors are taken into account here: an early stopping error, the approximation error, accounting for the error introduced by the the fact that $\\hat{u}$ is chosen within a given class of functions, and the stochastic error which originates from the fact that averages are taken with respect the n-samples empirical distribution of the data distribution rather than the data distribution itself.\n\nIn fact most of the paper's efforts and emphasis is put on the estimation of the stochastic error. The main result of the paper, Theorem 3, is indeed a bound on the stochastic error that requires the following assumptions\n\n- The conditional transition rates are upper bounded. The optimal Markov rates, called oracle rates,  are uniformly lower bounded.\n\n- That the function class in which $ \\hat{u} $ is chosen is made of functions that are controlled from above and below by the oracle rates up to multiplicative constants"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of the paper are the stochastic error estimates that scale linearly in $ \\mathcal{D} $. This is the first result of this kind I am aware of in the context of discrete flow models. Given that discrete flow-matching techniques are becoming increasingly popular in applications, these results are definitely intereting for the community of researchers working in the are of generative modeling. Moreover, they align with other theoretical results known for continuous (diffusion) flow matching models. It is likely that the finding of this paper will motivate further research efforts on error bounds for discrete flow models. The presentation of the main results is fairly clear."}, "weaknesses": {"value": "There are three mean weaknesses\n\n- The assumptions are not directly imposed on the source and target data but rather on the transition rates, which are not available in closed form. This makes it hard to check them in practice and  very difficult to understand how the regularity of the data distribution determines the size of the constants appearing in the assumptions. The authors show that the boundedness of the conditional rates can be ensured under reasonable assumptions in Appendix A. However, when bounding from below the optimal Markov rates, they appeal to the mixing constant of the data distribution $\\alpha$ and seem to impose that the marginals of the data distribution have some kind of uniform log-Lipschitz property, quantified through the constant $ \\beta $. Both $ \\alpha $ and $ \\beta $ could be very large, and it is not clear whether or not $ \\alpha $ and $ \\beta $ would depend on the dimension when computed on relevant examples. \n\n- The authors claim that one of the main contributions is to develop stochastic calculus and Girsanov theory for continuous time Markov chains. In my opinion, this not a novelty of this paper. Girsanov theory for jump processes has a long history starting with works in the 1970s by Jacod, see also book by Applebaum cited in this work for a Girsanov theorem for Lévy processes, the book by Jacod and Shiryaev for abstract results and many others. Specifying and making these abstract results explicit for the family of Markov chains studied in the area generative modeling is interesting in its own right, but it appears that this is not the first paper doing this. For example, the authors can check Appendix F in *\"Discrete Markov Probabilistic Models: An Improved Discrete Score-Based\nFramework with sharp convergence bounds under minimal assumptions\"* by Pham et. al, available on Open Review. \n\n- The proof of Theorem 3 is involved and very difficult to follow. I wanted to read it line-by-line as the bounds of Theorem 3 constitute the main strength of the paper, and had to give up and content myself with just getting an idea about is going on. There are literally pages of mathematical formulas without any intermediate comment about the overarching strategy. In some parts, there are some clear formatting issues, see [ages 23 and 24. Please avoid situations like the one at page 25 where you write 'following the proof of Theorem 11.6 in...' without actually saying what the proof ideas are, and then giving a page-long chain of inequalities.\n\nTo make a stronger case for publication, I recommend that the authors\n\n- Exhibit a reasonably wide class of distributions and a choice of conditional rates for which the constants appearing in Assumptions 1 and 2 can be explicitly computed. If possible, show that certain forms of regularity on the data distribution imply bounds on these constants.\n\n- Rethink section 4 by acknowledging previous results on Girsanov theory for CTMCs and their use in the context of discrete GMs. It would be beneficial to briefly compare the use of Girsanov Theorem in this work and the use of Girsanov theorem in the paper by Pham et al. Moreover, I feel like it's fair to recognize that the usage of Girsanov theorem in the context of diffusion models in $\\mathbb{R}^d$ has been pioneered by Chen et al. in the paper *\"Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions\"*.\n\n- Make the proof of Theorem 3 more reader friendly, by introducing some notation that avoids repeating the same long expressions over and over and by explaining what is the idea behind each of the steps. Also, think about including a proof outline for Thm, 3 right after its statement."}, "questions": {"value": "- Please explain what do you mean by \"marginalization trick\". As far as I can see, it boils down to using the property of conditional expectation but I may be missing something\n\n- I understand that the uniformization technique has the advantage of avoiding time-discretization. However I wonder about its interest in application. Are there existing works on discrete diffusion models that make use of this technique? What are the experimental results that have been obtained with this technique so far?\n\n- I find the choice of denoting $u_t(z,x)$ the rate at which we jump from $x$ to $z$ very unorthodox and in contrast with the classical conventions for Markov chains. Is there a reason behind it?\n\n- In the display corresponding to equation (6) there are some typos; in particular there is an odd number of parentheses in some of the equations.\n\n- In the display in the statement of Theorem 3: E should be $\\mathbb{E}$\n\n- The definition of $\\mathcal{N}(1/2n,\\mathcal{G}_n,L^{\\infty}(\\mathbb{P}_n))$ should be given at some point in the paper. Recalling to the reader what is the covering number would also help\n\n- In the statement of Theorem 3 can you clarify the (qualitative) dependence of $K_1$ on the constants appearing in Assumption 1 and 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o4ohhS21oP", "forum": "EFYb8SsRi7", "replyto": "EFYb8SsRi7", "signatures": ["ICLR.cc/2026/Conference/Submission11770/Reviewer_8JzY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11770/Reviewer_8JzY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11770/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761080805998, "cdate": 1761080805998, "tmdate": 1762922794146, "mdate": 1762922794146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Using rigorous stochastic analysis, this paper establishes the first non-asymptotic error bound for distribution error of discrete flow models with uniformization technique.\nThe total error can be decomposed to estimation error and early stopping error.\nTo analyze the estimation error, they develop a Girsanov change of measure theorem for CTMCs, which can transform the KL between two path measures of CTMCs to some expectation of Bregman divergence between rate functions of CTMCs.\nBased on it, the estimation error can be decomposed to two terms: stochastic error and approximation error, which can be dealt with using standard statistical learning theory. \nAnd they analyze the early stopping error to get the final result."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The writing of this paper is great. Even though I am not a researcher in the field of generative models, I can still grasp the core content and contributions of the work through its clear presentation.\nThis paper employs rigorous stochastic analysis theory to establish a theoretical framework for analyzing the non-asymptotic errors of discrete flow matching algorithms. In particular, the authors clearly demonstrate a Girsanov-type change of measure theorem for CTMCs, which I believe will be highly valuable for subsequent related research.\nFurthermore, the authors conduct a careful analysis of each type of error, complemented by intuitive explanations. They also provide an analysis of how to balance these errors.\nIn summary, I consider this to be a highly polished and well-executed work."}, "weaknesses": {"value": "Given that the authors emphasize the Girsanov-type change of measure theorem for CTMCs as one of the contributions in Contribution 1, it is necessary for the paper to elaborate on the related work of Girsanov's theorem that is currently missing. Additionally, the paper should clarify the technical novelty or differences of the proposed theorem compared to previous works of this kind.\n\nMinor Points:\n\nIn Proposition 2, there are typos regarding s.\n\nIn equation (3), the first p_t^d -> p_{t|1}^d\n\nIn line 248, Xi(ti) ∼ pt|1(·|Xi(1)) -> Xi(ti) ∼ pt_i|1(·|Xi(1))"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uKkyVLLE0a", "forum": "EFYb8SsRi7", "replyto": "EFYb8SsRi7", "signatures": ["ICLR.cc/2026/Conference/Submission11770/Reviewer_9zVq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11770/Reviewer_9zVq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11770/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799399903, "cdate": 1761799399903, "tmdate": 1762922793580, "mdate": 1762922793580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a rigorous theoretical analysis of discrete flow models, a class of generative models that operate on discrete state spaces using continuous-time Markov chains (CTMCs). While prior theoretical work has focused primarily on discrete diffusion models, this paper develops a complete error analysis framework for discrete flow matching.\n\nThe main technical inovation is a Girsanov-like theorem for CTMCs, which allows the authors to compute the KL divergence between two path measures of Markov chains with different transition rate matrices, which is a useful tool.The total error then decomposes  into:\n1) Stochastic error (due to finite data and empirical estimation)\n2) Approximation error (due to model capacity)\n3) Early stopping error (due to singular behavior near t = 1).\n\nCentral is the use of uniformization, which represents a CTMC as a Poisson process with constant rate M. This makes simulation exact—eliminating the discretization error that plagues tau-leaping or Euler schemes—and simplifies the theoretical treatment of sampling and generalization.\nThe analysis provides explicit convergence rates for total variation distance between the learned and target distributions, matching those known for continuous flow matching in the limit. The work is purely theoretical but conceptually unifies diffusion, flow, and discrete generative modeling frameworks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Theoretical contribution: The paper develops the first formal stochastic analysis of discrete flow models, extending continuous-time flow matching theory to discrete state spaces.\n- The derivation of a Girsanov-type theorem for CTMCs is technically elegant and significant; it bridges stochastic calculus and discrete dynamics.\n- The Error decomposition (stochastic, approximation, early stopping) provides a clear framework for understanding training behavior and generalization."}, "weaknesses": {"value": "Chief among the weaknesses is the lack of experiments: The paper is entirely theoretical. While the derivations are elegant, ICLR remains a machine learning venue, and some empirical validation, however small, would substantiate the claims. \nWhat useful information can machine learners in practice derive from these bounds. This is not clear to me.\nFor example, a toy CTMC experiment comparing uniformization vs tau-leaping would illustrate the zero-discretization error advantage.\nDemonstrating the predicted error scaling (with n, D, and tau) on a synthetic discrete dataset would strengthen the empirical relevance.\nThese weaknesses link, for me personally, to a limited connection to real-world use: Although discrete flows could apply to language, graph, or combinatorial data, the assumptions (bounded, smooth transition rates; fixed generator structures) are far from practical neural architectures.\nThis indicates for me that the manuscript might be better suited for a more technical publication outfit like TMLR. While the theoretical insights are definitely nontrivial, it’s not obvious how they translate into actionable design or training principles for practitioners."}, "questions": {"value": "1) Is the uniformization technique essential for your analysis, or could similar results hold for non-uniformized CTMCs (where jump rates are not globally bounded by M)?\n2) In practice, generator matching is optimized via stochastic gradient descent with minibatches. Do your stochastic error bounds extend to this setting, or do they assume exact gradient estimates? An experiment would be welcome here demonstrating the applicability of your approach to real world setups.\n3) How does the error bound scale with the discrete system’s dimensionality D? Are your near-linear dependencies empirically realistic, or do they rely on sparsity in the transition structure?\n4) In prior discrete diffusion work, tau-leaping introduced discretization error. Could you quantify, even empirically, how much uniformization improves accuracy for comparable computational cost?\n5)How could the theoretical insights here guide model design or training? For example, does the early stopping trade-off suggest a concrete way to choose tau in practice?\n6) How do you expect these bounds to hold for modern architectures used in discrete diffusion?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MsdfoMSlnz", "forum": "EFYb8SsRi7", "replyto": "EFYb8SsRi7", "signatures": ["ICLR.cc/2026/Conference/Submission11770/Reviewer_gwnn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11770/Reviewer_gwnn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11770/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845677150, "cdate": 1761845677150, "tmdate": 1762922793164, "mdate": 1762922793164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}