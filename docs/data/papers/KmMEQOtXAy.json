{"id": "KmMEQOtXAy", "number": 23887, "cdate": 1758349844050, "mdate": 1759896792285, "content": {"title": "Metric $k$-clustering using only Weak Comparison Oracles", "abstract": "Clustering is a fundamental primitive in unsupervised learning. However, classical algorithms for $k$-clustering (such as $k$-median and $k$-means) assume access to exact pairwise distances---an unrealistic requirement in many modern applications.  We study clustering in the \\emph{Rank-model (R-model)}, where access to distances is entirely replaced by a  \\emph{quadruplet oracle} that provides only relative distance comparisons. In practice, such an oracle can represent learned models or human feedback, and is expected to be noisy and entail an access cost.\n\nGiven a metric space with $n$ input items, we design randomized algorithms that, using only a noisy quadruplet oracle, compute a set of $O(k \\cdot \\mathsf{polylog}(n))$ centers along with a mapping from the input items to the centers such that the clustering cost of the mapping is at most constant times the optimum $k$-clustering cost. Our method achieves a query complexity of $O(n\\cdot k \\cdot \\mathsf{polylog}(n))$ for arbitrary metric spaces and improves to $O((n+k^2) \\cdot \\mathsf{polylog}(n))$ when the underlying metric has bounded doubling dimension. When the metric has bounded doubling dimension we can further improve the approximation from constant to $\\varepsilon$, for any arbitrarily small constant $\\varepsilon\\in(0,1)$, while preserving the same asymptotic query complexity.\nOur framework demonstrates how noisy, low-cost oracles, such as those derived from large language models, can be systematically integrated into scalable clustering algorithms.", "tldr": "", "keywords": ["clustering", "$k$-center", "$k$-median", "$k$-means", "comparison oracles", "learned rankers", "learning-augmented algorithms"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/20d21c89b230a12a4154e4e2bc7c7e91c928e4b8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies approximation algorithms for the $(k,p)$-clustering problem in the Rank-model (R-model), where instead of having direct access to pairwise distances, we have access to a (weak) oracle. This oracle takes two pairs of points, $(a,b)$ and $(c,d)$, as input and returns the pair that is closer. Furthermore, the oracle behaves probabilistically: it returns the correct answer with high probability (e.g., 99%) and is persistent. The task is to find a good solution to the $(k,p)$-clustering problem using as few oracle queries as possible.\n\nThe paper presents a bicriteria algorithm for the problem that opens $O(k (\\log n)^{O(1)})$ centers with an $O(1)$ approximation guarantee, using $O(nk (\\log n)^{O(1)})$ queries to the oracle in general metrics. Furthermore, the authors present improved results for metrics of bounded doubling dimension. To achieve their results, they design $O(1)$-“coresets” of size $O(k (\\log n)^{O(1)})$, also constructed with $O(nk (\\log n)^{O(1)})$ queries, which yields a bicriteria solution. The main technique appears to build on Har-Peled and Mazumdar (STOC ’04), which uses a recursive sampling subroutine to construct such coresets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It is known that no constant-factor approximation is possible for $k$-median based solely on the R-model. In this context, the paper presents a promising approach that still achieves a constant-factor approximation, albeit by opening more than $k$ centers.\n\n2. Most of the results appear to extend to the adversarial oracle model, where the oracle is no longer probabilistic but adversarial, giving correct responses only when $d(a,b)$ and $d(c,d)$ are not very close to each other.\n\n3. The presentation is clear and well-organized, especially the technical section that highlights the main ideas of the algorithms.\n\n4. The experimental results are interesting and support the practical relevance of the proposed methods."}, "weaknesses": {"value": "1. Although the paper achieves a constant-factor approximation using $O(k (\\log n)^{O(1)})$ centers, it is unclear whether the $\\log n$ term is necessary. Ideally, it would be preferable to obtain $O(k)$ centers, or alternatively, provide a lower bound showing that this is not possible.\n2. It is not clear how technically different their approach is compared to Har-Peled and Mazumdar, apart from the adaptation to the R-model.\n3. It is unfortunate that not a single proof appears in the main paper."}, "questions": {"value": "1. This relates to Weakness #2. I am curious to understand how different the techniques are compared to existing work, particularly what new methods were required to adapt them to the R-model.\n2. It would be helpful to include a table summarizing the current state-of-the-art results for the problem (both in the R-model and RM-model) and to clearly highlight the contributions of this paper.\n3. On line 67: what is meant by “sublinear approximation”? Could you clarify this term?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1sfzORnmbN", "forum": "KmMEQOtXAy", "replyto": "KmMEQOtXAy", "signatures": ["ICLR.cc/2026/Conference/Submission23887/Reviewer_yJGX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23887/Reviewer_yJGX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761128863215, "cdate": 1761128863215, "tmdate": 1762942841267, "mdate": 1762942841267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the metric k-clustering problem when only a weak comparison-based oracle (noisy oracle) is available. It introduces new representative centers construction method (with size $O(k poly\\log n)$) together with a mapping strategy. Using only quadruplet queries, this paper gives randomized algorithms that achieve: 1) in general metrics, an O(1)-Coreset+ (an extended notion of coreset with tighter guarantees) of size $O(kpoly\\log n)$ with $O(nk poly\\log  n)$ queries; 2) in bounded-doubling metrics, the same-size Coreset+ construction with $O((n+k^2) poly\\log n)$ queries; 3) for $k$-median/means objectives in doubling metrics, an $\\epsilon$-Coreset+ under the same query bound. The key techniques include probabilistic sorting under persistent noise with $O(logn)$ dislocation and an adversarial-oracle emulation using “kernel/guard” samples to produce 4-sorted orders and enable ANN selection. Overall, the framework shows how low-cost and noisy oracles can be integrated into scalable clustering algorithms designs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "One of the paper’s key strengths is that it eliminates distance oracles entirely, which provides a clear breakthrough from RM-model work (e.g., Raychaudhury et al., 2025). This makes the proposed methods more practical in settings where exact pairwise distances are costly to compute, especially for high-dimensional or graph-based data settings. Building on the Coreset+ framework and using only quadruplet queries, the proposed method reduces computational complexities. It achieves query complexities of $O(nk poly\\log n)$ in general metrics and $O((n+k^2) poly\\log n)$ in doubling metrics, which are competitive with prior results while avoiding any distance computations.\n\nAdditionally, the algorithms are proved to remain within logarithmic factors of optimal bounds. Their techniques used, including probabilistic sorting and adversarial-oracle emulation, are new and interesting, which could have broader applications beyond clustering."}, "weaknesses": {"value": "The weakness can be summarized as follows.\n\n1.Limited Experiments. While the authors present results on a synthetic 2D dataset, the empirical evaluation is limited to a single dataset with fixed parameters. There is no exploration of how the algorithm performs with varying parameters, dimensions, and noise levels. The paper would benefit from additional experiments with different datasets, noise rates, and metric spaces.\n\n2. Assumptions on noise. The paper assumes that noise across edge pairs is persistent and independent, which may not hold in practical settings. Some discussion of how to handle noise inconsistencies would improve the robustness of the proposed framework. \n\n3.Practical Efficiency: The constants involved in the query complexity could be high for large-scale datasets. More discussion on how to choose parameters (e.g., m_win, kernel sizes) for practical applications would strengthen the paper's relevance to practitioners."}, "questions": {"value": "1. What are the tight query lower bounds for this problem under weak (noisy quadruplet) oracles? Whether the upper bounds provided in this paper can match these limits (up to logarithmic factors) for k-median/means, and if not, where is the gap?\n\n2. Since the paper focuses on general metrics, can the algorithms be further extended to low-dimensional Euclidean spaces—for example, achieving faster query complexities or stronger accuracy guarantees by leveraging geometric structure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cxjDN714em", "forum": "KmMEQOtXAy", "replyto": "KmMEQOtXAy", "signatures": ["ICLR.cc/2026/Conference/Submission23887/Reviewer_6phy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23887/Reviewer_6phy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722671221, "cdate": 1761722671221, "tmdate": 1762942841030, "mdate": 1762942841030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies metric k-clustering problems in the so-called Rank-model (R-model). In this model, we only have access to a noisy quadrupled oracle, i.e. an oracle that returns a correct answer to the question “is point A closer to B, or is C closer to D?” with probability phi. Since it it known that it is impossible to get any sublinear approximation for k-median and k-means problems in this setting, the paper presents algorithms that computes O(k polylog n) centers (and the corresponding mapping of the other points), whose corresponding k-clustering cost is a constant approximation of the optimal k-clustering cost, using only O(nk poly log n) oracle calls. \n\nThe paper further presents an improved algorithm with the same number of centers, but with a run-time of O((n+k^2)polylog n) for metrics whose doubling dimension is bounded. Then, an even better (1+epsilon) approximation algorithm is possible to obtain for k-median and k-means problems in these metrics. Finally, using these obtained centers from each of the above algorithms, one can also easily compute coresets for these clustering problems. \n\n** Technical Overview **\n\nThe generic algorithm follows a standard clustering approach by Mettu-Plaxton: we start by sampling a set S of O(k polylog n) points from the input set, and then order the remaining points by their distance to S. We then remove the closer half of these points, and recourse on the remaining ones. Of course, the biggest challenge is to find this ordering of the distances to S, since we don’t have an exact distance oracle. \n\nThe way to deal with this difficulty is to approximately sort the points in each step. Namely, if one uses a stronger notion of oracle, the so-called quadruplet oracle with adversarial noise, Raychaudhury (2025) showed that we can approximately sort these points by their distance from S. This oracle is similar to our weak oracle, but it returns the correct solution if the points are not too close, while it can return a false answer if the points are close. However, to be able to emulate this stronger oracle with our weaker quadruplet oracle, certain conditions must hold. Therefore, in all three algorithms, our goal is to find these conditions, and then exploit the above result. \n\nIn the general algorithm ALG-G, the algorithm first finds sets Kernel and Guard for each s in S. Guard points filter out the remaining input points that are too close to S, while Kernel points help us emulate the quadruplet oracle with adversarial noise. \n\nIn algorithm ALG-D we cannot afford to compute sets Kernel and Guard for each s in S. Instead, we first partition points in S into smaller parts, such that no two points in the same class are too close. With a slightly different filtering approach compared to ALG-G, we proceed in a similar manner to obtain the desired centers, now with respect to these parts. Finally, centers returned by ALG-D can be used to obtain clustering with better approximation by growing concentric balls around each of the centers, and exploiting the fact that the doubling dimension is bounded."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and easy to follow.\nThe weak-oracle model is well motivated, and oracle-based modes in general have gained significant attention in the last few years."}, "weaknesses": {"value": "While it is clear why ALG-DI required bounded doubling dimension, it is not entirely clear why ALG-D cannot be used in the general setting\nMinor technical comments (see Detailed technical overview)"}, "questions": {"value": "What is the (intuitive) explanation why ALG-D fails in the general setting, i.e. where is the doubling dimension exploited?\n\n** Detailed technical comments **\n\nLine 24: from constant to epsilon -> to 1+epsilon?\nPage 3: definition of O(1)- and epsilon-coreset not consistent, the second condition missing in line 120?\nLemma 2.1: |E|=m missing\nBounds on phi not consistent, is phi < 1/4 or < 1/2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dQYrLx1wBl", "forum": "KmMEQOtXAy", "replyto": "KmMEQOtXAy", "signatures": ["ICLR.cc/2026/Conference/Submission23887/Reviewer_DLzD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23887/Reviewer_DLzD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813540983, "cdate": 1761813540983, "tmdate": 1762942840664, "mdate": 1762942840664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studied coreset algorithms with a noisy quadruplet distance oracle. Here, we are given $n$ data points in a metric space, and the distance metric $d$ is hidden from the algorithm. Instead, the algorithm could only make *comparison queries* in the form of ‘whether the distance $d(x,y)$ is smaller than the distance $d(u,v)$’. The oracle answers correctly with probability $1-p$ for some $p\\in(0,1/4)$, and the answer could be adversarial with probability $p$. The noise is persistent, meaning that if the oracle returns a wrong answer, we cannot query multiple times to get the correct answer. We want to understand what we can do for $k$-clustering with information from the noisy quadruplet oracle answers.\n\nThe motivation for this model is from a recent line of work using weak and strong oracle models, where the information obtained by the weak oracle is usually deemed as ‘cheap but inaccurate’ and the strong oracle simply returns the correct answer, but it is considered expensive to query. In many cases, the presence of a strong oracle is necessary, as shown by Galhotra, Raychaudhury, and Sintos [PODS’24], that we cannot hope to obtain $O(1)$-approximation for $k$-clustering with only the weak quadruplet oracle. \n\nThis paper, in contrast, shows that it is possible to construct *coresets* of size $k \\text{polylog}(n)$ using only the weak oracle. The catch here is that we might be able to output a subset of points that contains the information for $O(1)$-approximation $k$-clustering *without* knowing the actual clustering. On this front, the paper obtained an algorithm that achieves $O(1)$-approximation coreset of size $k \\text{polylog}(n)$ using only $nk \\text{polylog}(n)$ weak oracle queries. Furthermore, when the doubling dimension of the metric is bounded, the number of queries could be further decreased to $(n+k^2)\\text{polylog}(n)$.\n\n**Techniques.** The starting point of the techniques is quite simple: we know that if we sample $k \\text{polylog}(n)$ points in each iteration, filter out a tiny constant fraction of the vertex pairs with the longest distances, then the rest of the points would preserve the distances by an $O(1)$ factor. Therefore, if we simply want a coreset, we could just sample recursively and order the distances from the quadruplet oracle with some noisy sorting algorithm. The process terminates in $O(\\log{n})$ rounds, so we have coresets of size $k \\text{polylog}(n)$. However, if we want to generate a mapping between the points and their coreset points, then we will need to be more careful. In particular, the distances sorted by the noisy algorithm are not trustworthy, so the paper designed a more careful’’guard’’ set that basically serves as ‘distance witnesses’ to filter out points that are too close to the sampled points. For the rest of the points, since we could tolerate the radius of the kernel set distance, we could aggregate the information to compute an approximate neighbor for $v$ in the sampled set. This constitutes the main idea used in the paper."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Overall, I like this paper. It follows the recent line of work for weak-strong oracle models. However, different from existing results that almost exclusively showed the necessity of the strong oracle, this paper is the first to show that we could do something without the strong oracle at all (as far as I know). I think this is a nice conceptual contribution. The paper is well-written, and although I did not get the time to check all the steps, the technical overview is easy to follow. Therefore, I would want to see the paper accepted into the conference."}, "weaknesses": {"value": "I do not see any major weakness in the paper. One thing I want the author to emphasize is that one can know the coreset without knowing the clustering, since I was confused for a moment about whether there is a contradiction with GRS [PODS’24]. \n\nSome of the technical overview is a bit wordy and dense with math. I understand it’s a bit hard to present it in a cleaner manner. Maybe you can add some figures for the guard and kernel sets, plus the filtering process. I believe that’ll help readers understand."}, "questions": {"value": "I have some slightly technical questions to help me understand the results better:\n- If we do not care about the mapping between points and the coreset, but we are still given the noisy oracle (as opposed to the perfect one), does it make the situation drastically easier? I believe it is the case since the dislocation error is at most $O(\\log{n})$, and the recursive process that eliminates O(1) fraction from the sampled sets is going to succeed with high probability. Is this correct, or did I miss anything?\n- For the points that do not get assigned to any sampled coreset point, e.g., the points in the kernel, do they simply survive for the next round of sampling? I guess this is the reason you want $|Kernel|<= V_i/\\text{polylog}(n)$, but I want to confirm.\n- Also, how large is the exponent on the logarithm terms? Do you have an upper bound?\n- Finally, line 264 says ‘For simplicity, we focus on the k-median objective (p = 1), but our approach extends naturally to $p>1$’ —-- I don’t really buy this. $k$-median costs have some nice linear properties that other clustering objectives might not have. Do you want to use generalized triangle inequality? You’ll lose a factor of $2^p$, though. Then, you’ll need to say that you assume $p=O(1)$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vvlNPRVju9", "forum": "KmMEQOtXAy", "replyto": "KmMEQOtXAy", "signatures": ["ICLR.cc/2026/Conference/Submission23887/Reviewer_y7gL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23887/Reviewer_y7gL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953294192, "cdate": 1761953294192, "tmdate": 1762942840436, "mdate": 1762942840436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies metric $k$-clustering when instead of distances an oracle is provided that given 2 pairs of points $(a,b)$ and $(c,d)$ returns if $a$ is closer to $b$ or $c$ is closer to $d$. Previous works has establish that with only access to oracle and no distance information, no constant approximation algorithm exists with $k$ centers. This work instead considers constructing coresets, and give and algorithm that outputs a $O(1)$ error coreset with $O(k\\text{poly}\\log{n})$ points and $O(nk\\text{poly}\\log{n})$ oracle queries. If the underlying metric has bounded doubling dimension, the query complexity improves to $O((n+k^2)\\text{poly}\\log{n})$, and $(1+\\varepsilon)$-coreset for $k-$means and $k-$median."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The problem setting is very natural, for cases where distance computation might be hard or expensivem, performing comparisions using machine learning models might be efficient. The proofs are clear to the best of my knowledge. This work removes the requirement of distance oracle and provides coreset construction with only the oracle queries. The results obtained are near-optimal."}, "weaknesses": {"value": "The experimental setting is very limited, with them being run on only one synthetic dataset. But the algorithmic contributions and results obtained are non-trivial and significant contribution so this is not really a major concern for me. Maybe a minor typo - probsort (in Lemma 2.1) requires noise parameter $\\leq 1/4$ but appendix A says $\\leq 1/2$."}, "questions": {"value": "N/A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OdaiZVRLzQ", "forum": "KmMEQOtXAy", "replyto": "KmMEQOtXAy", "signatures": ["ICLR.cc/2026/Conference/Submission23887/Reviewer_8Vye"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23887/Reviewer_8Vye"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962196923, "cdate": 1761962196923, "tmdate": 1762942840224, "mdate": 1762942840224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}