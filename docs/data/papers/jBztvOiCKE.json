{"id": "jBztvOiCKE", "number": 3330, "cdate": 1757403254018, "mdate": 1763020161030, "content": {"title": "Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield", "abstract": "Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that the primary driver of few-step generation is not the distribution matching term, but a previously overlooked component we identify as \\textit{\\textbf{C}FG \\textbf{A}ugmentation} (\\textbf{CA}). We demonstrate that this term acts as the core \"engine\" of distillation, while the \\textbf{D}istribution \\textbf{M}atching (\\textbf{DM}) term functions as a \"regularizer\" that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor between CA and DM also allows a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains.", "tldr": "Conventional wisdom holds that DMD works by matching distributions. We decouple the objective and find this is incorrect. The true \"engine\" of distillation is CFG Augmentation (CA), while Distribution Matching (DM) is a \"regularizer\" for stability.", "keywords": ["Diffusion Model", "Diffusion Distillation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/94d0dc041fa4ffb3ff64a7e9de17bab80c40bae1.pdf", "supplementary_material": "/attachment/1a897bc5bc8cdc3bc345d868d61a16dbd4ceaf57.zip"}, "replies": [{"content": {"summary": {"value": "This work revisits what makes DMD effective. While DMD’s success is usually credited to matching the student’s output distribution with a teacher model, the authors show that the true driver is an overlooked component called CFG Augmentation (CA). They find that CA acts as the core engine of distillation, while the Distribution Matching (DM) term mainly serves as a stabilising regularizer. They also show that recognising this separation enables a clearer understanding of DMD and allows improvements such as decoupling noise schedules for better performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I really like this research topic and believe the distribution-matching distillation is an under-explored topic, and only from a divergence perspective, it can't answer why it works or why it doesn't work in some scenarios, so I think the topic of this paper is very valuable.\n\nThe experiments are also sound, which can support the argument."}, "weaknesses": {"value": "My major concern with this paper is that I found the conclusion a little bit conclusive.\n\nThe argument is CFG Augmentation is the engine for dilatation, and Distribution Matching is the regularizer for stability.  \n\nHowever, many CIFAR experiments don't use label-conditioned and can achieve one-step distillation, e.g. the original diff-intruct paper or more recent paper: https://arxiv.org/pdf/2502.08005. In this case, the pure driven engine is only the distribution matching term, which couldn't be explained by the hypotheses introduced in the paper.\n\nIt may be possible that CFG can play a key role in the conditional generation, but it is hard to say DM is not the engine.\n\nMinor: The distillation also relates to the student model score estimation quality, initialisation, teacher model's score quality, etc... It would be good to add some analysis on that."}, "questions": {"value": "See above, why the unconditional CIFAR works with only DM term?\n\nHappy to increase the score if the concern could be solved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tp2YnkYobh", "forum": "jBztvOiCKE", "replyto": "jBztvOiCKE", "signatures": ["ICLR.cc/2026/Conference/Submission3330/Reviewer_rsWL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3330/Reviewer_rsWL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725294863, "cdate": 1761725294863, "tmdate": 1762916674679, "mdate": 1762916674679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the conventional understanding of the underlying mechanisms of Distribution Matching Distillation (DMD) for distilling pre-trained diffusion models into one/few-step student models. While it might be tempting to think that DMD's success mainly stems from matching the student's output distribution to the teacher's output distribution, the authors decompose the DMD loss into a distribution matching (DM) term and a CFG augmentation (CA) term, arguing that it is the CA term that plays the primary role in the distillation process. Surprisingly, the DM term functions more like a stabilizing regularizer and could be replaced by other regularization terms with different trade-offs. Leveraging this insight, a decoupled noise schedule is proposed for CA and DM to improve the model performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper identifies a discrepancy between theory and practice in DMD that CFG is only used in the teacher model but not the student model. This is an interesting observation and a natural motivation for this important research topic.\n2. The decomposition of the DMD loss into the DM and CA terms provide novel and valuable insights towards a better and principled understanding of the underlying mechanism of DMD.\n3. The arguments and hypotheses in the paper are supported by extensive experiments with ablation studies, demonstrating impressive empirical results.\n4. The paper is well-written and easy to understand. It also acknowledges the limitations of the current understanding of the CA term and provides some preliminary discussions."}, "weaknesses": {"value": "Overall, I like the paper very much. My only concern is the paper's claim about the CA term being the engine for DMD, which is a bit strong to me. For example, early DMD papers achieved great distillation performance on unconditional generation for CIFAR images, which is not discussed or explored in this paper."}, "questions": {"value": "Could the authors comment on the issue in the weakness section? One way to address this issue is to reduce the claim to \"the CA term is the engine for DMD **in conditional generation**\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zQW7R2L7fz", "forum": "jBztvOiCKE", "replyto": "jBztvOiCKE", "signatures": ["ICLR.cc/2026/Conference/Submission3330/Reviewer_uRhT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3330/Reviewer_uRhT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814597367, "cdate": 1761814597367, "tmdate": 1762916674517, "mdate": 1762916674517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the respective roles of the two loss components—CFG augmentation and distribution matching—in the DMD framework. Through carefully controlled experiments, the authors conclude that CFG augmentation serves as the primary driver for few-step or one-step conversion, while distribution matching acts mainly as a regularizer. They further argue that, although alternative regularizers could be used, distribution matching remains the best fit. Finally, the paper observes that assigning different $\\tau$ values to the two loss terms yields additional performance improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Provides a timely and insightful analysis of the functional roles of DMD’s two loss terms, addressing the open question of why DMD excels in few-step or one-step generation.\n* The authors design careful and hypothesis-driven experiments to isolate and test the contribution of each loss term, leading to well-supported conclusions. \n* Based on these insights, the paper proposes using distinct $\\tau$ values for the two terms, leading to measurable performance gains."}, "weaknesses": {"value": "Most experiments rely primarily on qualitative evaluation (visual inspection of generated images). While visualization is valuable for illustrating effects, heavy reliance on qualitative judgments risks confirmation bias—highlighting supportive examples while overlooking contradictory ones. A more scientifically rigorous approach would involve defining quantitative metrics and validating observations across the entire test set, to ensure statistical robustness and reproducibility."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2917QO8ycq", "forum": "jBztvOiCKE", "replyto": "jBztvOiCKE", "signatures": ["ICLR.cc/2026/Conference/Submission3330/Reviewer_vHFL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3330/Reviewer_vHFL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3330/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964793562, "cdate": 1761964793562, "tmdate": 1762916674225, "mdate": 1762916674225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}