{"id": "5AXO7z4XLz", "number": 5586, "cdate": 1757921375621, "mdate": 1759897966220, "content": {"title": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing", "abstract": "Despite the rapid progress of instruction-based image editing, its extension to video remains underexplored, primarily due to the prohibitive cost and complexity of constructing large-scale paired video editing datasets. To address this challenge, we introduce a low-cost pretraining strategy for instruction-based video editing that leverages in-context learning from unpaired video clips. We show that pretraining a foundation video generation model with this strategy endows it with general editing capabilities, such as adding, replacing, or deleting operations, according to input editing instructions. The pretrained model can then be efficiently refined with a small amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our framework first pretrains on approximately 1M real video clips to learn basic editing concepts, and subsequently fine-tunes on fewer than 150k curated editing pairs to extend more editing tasks and improve the editing quality. Comparative experiments show that our method surpasses existing instruction-based video editing approaches in both instruction alignment and visual fidelity, achieving a 12\\% improvement in editing instruction following and a 15\\% improvement in editing quality.", "tldr": "We propose a low-cost pretraining strategy for instruction-based video editing using unpaired clips plus limited paired data, enabling general editing abilities with improved alignment and quality.", "keywords": ["Video Editing", "Video Generation", "Instruction-based"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/179b4462edd5fc60b6da495f9316b709f9b4deab.pdf", "supplementary_material": "/attachment/06bf40062abde25dd66bd45d5745fc57b571c832.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a data-efficient training strategy for instruction-based video editing that reduces reliance on large paired datasets. The method first pretrains on unpaired video clips to learn general editing concepts, then fine-tunes on a small set of high-quality editing pairs. Built upon HunyuanVideoT2V, the approach achieves significant gains in instruction alignment and visual quality over existing methods (e.g., Senorita-2M, InsViE-1M), with 12% improvement in instruction following and 15% in editing quality. The paper includes detailed ablations demonstrating that pretraining on clip data provides strong editing priors and enables effective fine-tuning with limited paired data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important and timely problem in instruction-based video editing, where collecting large-scale paired datasets is prohibitively expensive.\n- The proposed idea of using unpaired video clips for pretraining is novel, practical, and conceptually simple, yet it leads to strong empirical improvements.\n- The experiments are comprehensive and include comparisons with several baselines, detailed ablations, and qualitative results that convincingly demonstrate the method’s effectiveness.\n- The paper is clearly written and well-structured, with strong visuals that help the reader understand the data curation pipeline and model design."}, "weaknesses": {"value": "- The main innovation lies in the data strategy rather than the model architecture, which is only moderately modified from HunyuanVideoT2V.\n- The evaluation relies heavily on automated metrics (such as CLIP similarity and GPT-5-based scoring), without human studies to verify perceptual quality or instruction alignment. Pairwise comparison with ELO rating would be helpful.\n- The paper does not thoroughly analyze the computational cost of pretraining on one million clips, which could still be resource-intensive in practice.\n- The generalization ability of the approach to other domains, such as stylized or synthetic videos, remains unclear and could have been explored further."}, "questions": {"value": "1. How does the performance change when the model is pretrained on smaller subsets of clip data (for example, 100k or 200k clips)?\n2. Could the same pretraining and fine-tuning strategy be applied to other backbones beyond HunyuanVideoT2V?\n3. Have you evaluated how well the model performs on non-natural video domains such as animation, cartoons, or synthetic datasets?\n4. Could the authors provide an estimate of the compute or training time required for the 1M-clip pretraining stage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dNEUzz6S8t", "forum": "5AXO7z4XLz", "replyto": "5AXO7z4XLz", "signatures": ["ICLR.cc/2026/Conference/Submission5586/Reviewer_aSfy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5586/Reviewer_aSfy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760726293345, "cdate": 1760726293345, "tmdate": 1762918148744, "mdate": 1762918148744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of data scarcity for instruction-based video editing by proposing a novel two-stage training strategy. The approach first pretrains a foundation video generation model (based on Hunyuan VideoT2V) using In-Context Learning (ICL) with unpaired video clips. The pretraining stage leverages approximately 1 million real video clips, treating clips sampled from the same scene segment but different temporal intervals as pseudo-original and pseudo-edited pairs. An instruction is automatically generated to describe the difference between the two clips. This stage teaches the model basic editing concepts (e.g., addition, replacement, deletion) and strengthens its ability to preserve original video content. The second stage involves Supervised Fine-Tuning (SFT) on a small, high-quality synthetic dataset of fewer than 150k curated editing pairs to extend editing tasks and improve quality. The model architecture uses an in-context approach where the original video tokens (with timestep $t=0$) are concatenated with the noised video tokens. The proposed method achieves superior performance against existing instruction-based video editing models, with reported improvements of 12% in instruction following and 15% in editing quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The combination of large-scale, low-cost pretraining on real-world clips (learning basic concepts and preservation) followed by targeted SFT on a small, high-quality synthetic set (learning complex edits) is highly effective and data-efficient.\n2. The method achieves state-of-the-art results, showing significant gains (12% and 15%) in instruction following and editing quality compared to existing methods.\n3.  The model achieves superior results using only $\\sim$1M video clips and $<150k$ paired editing samples, a fraction of the data required by comparable SOTA models."}, "weaknesses": {"value": "1. The data curation process heavily relies on powerful external models (Step3 for instruction generation/filtering, GroundedSAM2 for masking, VACE for inpainting, Qwen2.5-VL for filtering). This dependency raises questions about the generalizability of the pipeline if these auxiliary models change or are unavailable.\n2. While the ablation study is mentioned, more granular detail on the performance difference between: a) No pretraining + SFT, b) Pretraining only, and c) Full two-stage training is crucial to quantify the specific gain from the ICL pretraining stage. (The current abstract only mentions the final performance ).\n3. Built upon Hunyuan VideoT2V, the model size and hardware requirements are implicitly high. A brief discussion on the computational resources needed for training (not just data generation) compared to other SOTA models would provide a more complete picture of the \"low-cost\" claim."}, "questions": {"value": "1. Can the authors characterize the instructions generated from the unpaired clips (e.g., using a distribution plot or semantic clustering) and compare their nature (e.g., motion, camera work, lighting changes) to the instructions in the SFT and testing datasets? This would help justify the claim that these \"basic editing concepts\" are effectively transferable.\n2. Given the heavy reliance on an in-context token concatenation approach, how is temporal consistency explicitly addressed beyond the base DiT architecture? Did the authors find that the separation of original and noised tokens (via $t=0$ and $t=T$) impacts the temporal self-attention in the DiT blocks, and if so, how was this mitigated?\n3. The SFT uses \"fewer than 150k\" samples for only \"one epoch\". This is an extremely low training budget. Please confirm if the entire Hunyuan VideoT2V backbone is updated during SFT, or if only a subset of parameters (e.g., attention layers or a LoRA adapter) is fine-tuned. This detail is crucial for assessing the true efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TJ1kLtHFAr", "forum": "5AXO7z4XLz", "replyto": "5AXO7z4XLz", "signatures": ["ICLR.cc/2026/Conference/Submission5586/Reviewer_86hF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5586/Reviewer_86hF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761403135306, "cdate": 1761403135306, "tmdate": 1762918148487, "mdate": 1762918148487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a two-stage training strategy to addresses the challenge of data scarcity for instruction-based video editing. They first pretrains a foundation video generation model (Hunyuan T2V) using In-Context Learning with unpaired video clips. During the pretraining stage, they use clips sampled from the same scene segment but different temporal intervals as pseudo-original and pseudo-edited pairs, while allow them to leverage a large amount video clips without too much extra process. The instruction is generated to describe the difference between the two clips. This stage teaches the model basic editing concepts and strengthens its ability to preserve original video content. In the second stage, they collect a small, high-quality synthetic dataset (fewer than 150K) to conduct Supervised Fine-Tuning which can extend editing tasks and improve quality. The model architecture uses an in-context condition injection approach where the original video tokens (with timestep $t=0$) are concatenated with the noised video tokens. Evaluation results show that the final model can achieve superior performance against existing instruction-based video editing models, with reported improvements of 12% in instruction following and 15% in editing quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Use clips sampled from the same scene segment but different temporal intervals as pseudo-original and pseudo-edited pairs as the training pairs can greatly reduce the data collection burden in the first stage. After acquire the editing capacity through large-scale pretraining, model can quickly adapt to complex editing through conducting SFT on a small, high-quality synthetic set.\n2. The method achieves SOTA performance in editing scenario, showing significant gains (12% and 15%) in instruction following and editing quality compared to previous methods.\n3.  Compared with previous work which relies on large-scale training data and lengthy data collection pipelines, The model achieves superior results using only $\\sim$1M video clips and $<150k$ paired editing samples."}, "weaknesses": {"value": "1. The data curation process heavily relies on powerful external models (Step3 for instruction generation/filtering, GroundedSAM2 for masking, VACE for inpainting, Qwen2.5-VL for filtering). This dependency raises questions about the generalizability of the pipeline if these auxiliary models change or are unavailable.\n2. While the ablation study is mentioned, more granular detail on the performance difference between: a) No pretraining + SFT, b) Pretraining only, and c) Full two-stage training is crucial to quantify the specific gain from the ICL pretraining stage. (The current abstract only mentions the final performance ).\n3. Built upon Hunyuan VideoT2V, the model size and hardware requirements are implicitly high. A brief discussion on the computational resources needed for training (not just data generation) compared to other SOTA models would provide a more complete picture of the \"low-cost\" claim."}, "questions": {"value": "1. Can the authors characterize the instructions generated from the unpaired clips (e.g., using a distribution plot or semantic clustering) and compare their nature (e.g., motion, camera work, lighting changes) to the instructions in the SFT and testing datasets? This would help justify the claim that these \"basic editing concepts\" are effectively transferable.\n2. Given the heavy reliance on an in-context token concatenation approach, how is temporal consistency explicitly addressed beyond the base DiT architecture? Did the authors find that the separation of original and noised tokens (via $t=0$ and $t=T$) impacts the temporal self-attention in the DiT blocks, and if so, how was this mitigated?\n3. The SFT uses \"fewer than 150k\" samples for only \"one epoch\". This is an extremely low training budget. Please confirm if the entire Hunyuan VideoT2V backbone is updated during SFT, or if only a subset of parameters (e.g., attention layers or a LoRA adapter) is fine-tuned. This detail is crucial for assessing the true efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TJ1kLtHFAr", "forum": "5AXO7z4XLz", "replyto": "5AXO7z4XLz", "signatures": ["ICLR.cc/2026/Conference/Submission5586/Reviewer_86hF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5586/Reviewer_86hF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761403135306, "cdate": 1761403135306, "tmdate": 1763647668905, "mdate": 1763647668905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a two-stage training strategy to addresses the challenge of data scarcity for instruction-based video editing. They first pretrains a foundation video generation model (Hunyuan T2V) using In-Context Learning with unpaired video clips. During the pretraining stage, they use clips sampled from the same scene segment but different temporal intervals as pseudo-original and pseudo-edited pairs, while allow them to leverage a large amount video clips without too much extra process. The instruction is generated to describe the difference between the two clips. This stage teaches the model basic editing concepts and strengthens its ability to preserve original video content. In the second stage, they collect a small, high-quality synthetic dataset (fewer than 150K) to conduct Supervised Fine-Tuning which can extend editing tasks and improve quality. The model architecture uses an in-context condition injection approach where the original video tokens (with timestep $t=0$) are concatenated with the noised video tokens. Evaluation results show that the final model can achieve superior performance against existing instruction-based video editing models, with reported improvements of 12% in instruction following and 15% in editing quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Use clips sampled from the same scene segment but different temporal intervals as pseudo-original and pseudo-edited pairs as the training pairs can greatly reduce the data collection burden in the first stage. After acquire the editing capacity through large-scale pretraining, model can quickly adapt to complex editing through conducting SFT on a small, high-quality synthetic set.\n2. The method achieves SOTA performance in editing scenario, showing significant gains (12% and 15%) in instruction following and editing quality compared to previous methods.\n3.  Compared with previous work which relies on large-scale training data and lengthy data collection pipelines, The model achieves superior results using only $\\sim$1M video clips and $<150k$ paired editing samples."}, "weaknesses": {"value": "1. The data curation process heavily relies on powerful external models (Step3 for instruction generation/filtering, GroundedSAM2 for masking, VACE for inpainting, Qwen2.5-VL for filtering). This dependency raises questions about the generalizability of the pipeline if these auxiliary models change or are unavailable.\n2. While the ablation study is mentioned, more granular detail on the performance difference between: a) No pretraining + SFT, b) Pretraining only, and c) Full two-stage training is crucial to quantify the specific gain from the ICL pretraining stage.\n3. A brief discussion on the computational resources and training budget are needed to support the \"low-cost\" claim."}, "questions": {"value": "1. Can the authors characterize the instructions generated from the unpaired clips (e.g., using a distribution plot or semantic clustering) and compare their nature (e.g., motion, camera work, lighting changes) to the instructions in the SFT and testing datasets? This would help justify the claim that these \"basic editing concepts\" are effectively transferable.\n2. Given the heavy reliance on an in-context token concatenation approach, how is temporal consistency explicitly addressed beyond the base DiT architecture? Did the authors find that the separation of original and noised tokens (via $t=0$ and $t=T$) impacts the temporal self-attention in the DiT blocks, and if so, how was this mitigated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TJ1kLtHFAr", "forum": "5AXO7z4XLz", "replyto": "5AXO7z4XLz", "signatures": ["ICLR.cc/2026/Conference/Submission5586/Reviewer_86hF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5586/Reviewer_86hF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761403135306, "cdate": 1761403135306, "tmdate": 1763731885579, "mdate": 1763731885579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an \"in-context learning\" approach for training an instructional video editing model through a two-stage paradigm: unpaired pre-training followed by paired supervised fine-tuning. Compared to existing methods, the proposed approach achieves reasonable performance improvements. While I acknowledge the systematic effort invested in large-scale data processing and model training, the work currently offers limited novel insights or technical contributions beyond the engineering effort. I consider this a borderline paper at this stage and look forward to the authors' response."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed pre-training stage on unpaired clip data mitigates the data scarcity issue inherent in instructional video editing, where obtaining strictly paired data is challenging. This approach enables the model to establish fundamental instructional editing capabilities even without strict pairing, which is a practical and valuable contribution.\n\n2. Through large-scale pre-training followed by supervised fine-tuning (SFT), the proposed model demonstrates consistent and reasonable performance improvements on instructional video editing tasks compared to existing baselines."}, "weaknesses": {"value": "1. While I acknowledge that pre-training on large-scale unpaired data helps the model shift from understanding descriptive text to instructional text, which benefits instructional comprehension, I'm concerned about its impact on video preservation. Since unpaired data typically contains videos with significant differences where very few elements are strictly preserved, this pre-training stage may harm the model's ability to preserve the original video content—an important aspect of video editing. I would suggest including more automatic metrics in the ablation study to evaluate video preservation under different training paradigms, rather than relying solely on the O_P score from GPT-5, which would strengthen the analysis.\n\n2. I'm not convinced that the term \"in-context learning\" is appropriate here, given that the original video is essentially used as a condition through sequence concatenation. Could the authors elaborate on why this framing is justified?\n\n3. I'm curious about the design choice of setting t=0 for the original video tokens during training. Intuitively, both t=0 and t=T seem viable. I understand this design might help the model distinguish the original video from noisy frames, but do the authors have experimental results comparing these choices? How much does this specific design contribute to performance, and what's the deeper reasoning behind it?\n\n4. What is the SFT dataset used in this paper? I couldn't find detailed information about it in the manuscript."}, "questions": {"value": "See wekanesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wASTF5CS76", "forum": "5AXO7z4XLz", "replyto": "5AXO7z4XLz", "signatures": ["ICLR.cc/2026/Conference/Submission5586/Reviewer_T1N6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5586/Reviewer_T1N6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668153761, "cdate": 1761668153761, "tmdate": 1762918148278, "mdate": 1762918148278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, it is demonstrated that pretraining a foundation model on unpaired video clips, followed by fine-tuning on supervised fine-tuning (SFT) data, can significantly improve the model’s performance. Using HunyuanVideoT2V as the base framework, the model is first pretrained on approximately one million real video clips to acquire general video editing capabilities, and then fine-tuned on fewer than 150K carefully curated editing pairs to further enhance performance and broaden editing abilities. The experimental results show strong performance and provide comprehensive comparisons with previous approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper effectively leverages unpaired video–text clips to train a model that can be easily fine-tuned into an instruction-based framework, demonstrating flexibility and scalability.\n\n2. The editing results are visually impressive and show clear improvements in realism and semantic consistency.\n\n3. Compared with existing video editing approaches, the proposed model achieves competitive or superior performance across several qualitative and quantitative evaluations, highlighting its potential practical value."}, "weaknesses": {"value": "1. Some fine-grained details are lost after editing. For example, in Figure 6, the hair in the first row becomes noticeably blurred, indicating a limitation in preserving texture details.\n\n2. The model occasionally fails to fully follow the given instructions. In Figure 6, fifth row, the necklace remains visible despite the instruction to remove it, suggesting incomplete semantic alignment.\n\n3. The model has not been evaluated on alternative architectures such as WAN2.1 1.3B, so the generalization capability of the proposed method across different backbones remains unclear.\n\n4. The paper does not include comparisons with specialized models designed for specific editing purposes, such as Style Master or Minimax-Remover, which could provide a more comprehensive evaluation.\n\n5. The ablation study is insufficient. It would be valuable to clarify whether, under identical training steps, a model based on Senorita would yield similar editing performance, helping to verify the true contribution of the proposed method."}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eMgzvXa69W", "forum": "5AXO7z4XLz", "replyto": "5AXO7z4XLz", "signatures": ["ICLR.cc/2026/Conference/Submission5586/Reviewer_WZm9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5586/Reviewer_WZm9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977476318, "cdate": 1761977476318, "tmdate": 1762918148078, "mdate": 1762918148078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}