{"id": "R2FH4wrVk2", "number": 18925, "cdate": 1758292050617, "mdate": 1759897072902, "content": {"title": "Rethinking LLM Ensembling from the Perspective of Mixture Models", "abstract": "Model ensembling is a well-established technique for improving the performance of machine learning models. Conventionally, this involves averaging the output distributions of multiple models and selecting the most probable label. This idea has been naturally extended to large language models (LLMs), yielding improved performance but incurring substantial computational cost. This inefficiency stems from directly applying conventional ensemble implementation to LLMs, which require a separate forward pass for each model to explicitly compute the ensemble distribution. In this paper, we revisit this conventional assumption and find that ensembling in the context of LLMs is fundamentally different. Unlike conventional models, LLMs typically generate tokens by sampling from the output distribution rather than selecting the top prediction via argmax. This key distinction enables us to reinterpret LLM ensembling as a mixture model. Under this perspective, one can sample from the ensemble distribution by simply selecting a single model at random and sampling from its output, which avoids the need to compute the full ensemble distribution explicitly. We refer to this approach as the **Mixture-model-like Ensemble** (ME). ME is mathematically equivalent to sampling from the ensemble distribution, but **requires invoking only one model**, making it **1.78×-2.68×** faster than conventional ensemble. Furthermore, this perspective connects LLM ensembling and token-level routing methods, suggesting that LLM ensembling is a special case of routing methods. Our findings open new avenues for efficient LLM ensembling and motivate further exploration of token-level routing strategies for LLMs. Our code is available at https://anonymous.4open.science/r/Mixture-model-like-Ensemble/.", "tldr": "We revisit LLM ensembling from the perspective of mixture models and observe that: (1) it can be implemented by invoking only one model, and (2) it can be interpreted as the simplest form of token-level routing.", "keywords": ["LLM Ensembing", "Mixture models", "Token-level routing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cbd08194501917c12e6aaeb5d9d17e2193d345ce.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a way to efficiently implement LLM ensemble. The method is based on a very simple math observation: Sampling from the mixture distribution can be implemented through ancestral sampling, i.e. first sampling a mixture index, then sampling from the mixture. In the setting of LLM ensembling, this corresponds to first sampling a LLM, then sampling a token from the picked LLM.\n\nThe paper then considers evaluation on different types of ensembling e.g. LLMs of similiar structure/vocabulary, or not. on standard benchmark tasks. The proposed ensemble method shows performance similiar to standard ensemble method (Table 2) but shows efficiency improvment (Table 4)\n\nOverall I find the method technically sound, the presentation very clear, the paper is nicely written. However I have a couple of concern about the claim and contribution, see weakness and question section."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The idea is simple and technically sound.\n\n- The paper is well written and the presentation is very clear!\n\n- The problem studied is of great importance.\n\n- The experiments consider a wide range of models and tasks."}, "weaknesses": {"value": "## Correctness of the claim\n\nDoes the equivalence between Alg.1 and Alg.2 still hold when top-P / top-K / temperature is used?\n\nI think temperature would not affect the equivalence much but the usage of top-P / K would break the equivalence since the top-P / K of the weighted avearge distribution is different from that of the single model's distribution. \n\n## Actual runtime and memory overhead\n\nThere are several things unclear about the efficiency gain of the proposed method compared with CE (Parallel).\n\n- The proposed method would have the GPU-CPU load / unload overhead, which CE (Parallel) does not have.\n\n- It is unclear how the proposed method manages KV cache: The KV cache needs to be loaded / unloaded from the memory to GPU everytime a model is picked; Many tokens' KV cache is not there for certain models when the models are not selected, so the corresponding kv caceh needs to be generated freshly. \n\n- The CE(Parallel)'s actualy communication cost is unclear, and how exactly does the inter-GPU communication decompose is also unclear. Does the overhead mainly come from the exchange of next token prediction probability vector or does it come from the synchronization of different models on different machines due to their different sizes?\n\n## Contribution\n\nIt also seems to me that many parts of the papers are not talking about the contribution of the proposed method in this paper, for example, the ablation study in Fig.2 can be applied to both CE and ME, and it is nothing particular about the proposed method."}, "questions": {"value": "Is the efficiency difference between CE v.s. ME (parallel) mainly from\n- At each token, CE (parallel)'s time is dominated by the slowest model, since each sampling step needs to wait for each model to finish the compute before aggregation.\n- ME's time is roughtly the average of all models.\nTherefore ME would be faster than CE. And the speed up would be more significant if the model sizes different are larger for all ensemble components?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vKSAcy42Ex", "forum": "R2FH4wrVk2", "replyto": "R2FH4wrVk2", "signatures": ["ICLR.cc/2026/Conference/Submission18925/Reviewer_2mRJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18925/Reviewer_2mRJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760467640786, "cdate": 1760467640786, "tmdate": 1762930912967, "mdate": 1762930912967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel approach, termed Mixture-model-like Ensemble. It argues that conventional model ensemble approaches typically require forwarding n different models to generate one token, which results in significant computation overhead. The authors pointed out that with fixed ensemble weights, sampling from a weighted average of different models' output probabilities is mathematically equal to selecting one model and then sampling from its output probability. The new process would significantly reduce the computation overhead. Experiments across different models with diverse tasks demonstrate the effectiveness and efficiency of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is important. Computation efficiency is indeed the main obstacle for the practical application of model ensemble approaches.\n2. The idea is novel and effective. By converting sampling from a mixed distribution to first sampling the model, then generating a token, it largely reduces the need to forward n different models."}, "weaknesses": {"value": "The main limitation of this work lies in its reliance on pre-defined ensemble weights. One of the most promising directions for token-level ensemble methods is the ability to dynamically adjust ensemble weights based on each model’s confidence at every step. Prior studies, such as EVA and SweetSpan, have shown that such adaptive weighting is key to achieving effective ensembling. Because a model’s performance can vary across different generation steps. An effective ensemble requires deciding the weights case by case, using the confidence or PPL as a proxy for model performance.\n\nBy contrast, the proposed approach fixes the ensemble weights in advance, which significantly constrains this potential and results in only marginal performance improvements. As shown in Appendix A.4, the optimal ensemble weights often converge toward 0 or 1, suggesting that the method may reduce to sampling from only the best-performing model in many cases. \n\nAlthough the proposed approach does improve computational efficiency, it does so at the cost of limiting the performance gains that can be achieved through ensembling multiple models. The need to know the model's performance for deciding the ensemble weights in advance is also unrealistic in practice."}, "questions": {"value": "1. What is the selected ensemble weight for the results reported? \n2. What are the results for ensemble approaches that can dynamically adjust the ensemble weights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "snXYFNAxHM", "forum": "R2FH4wrVk2", "replyto": "R2FH4wrVk2", "signatures": ["ICLR.cc/2026/Conference/Submission18925/Reviewer_XkeJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18925/Reviewer_XkeJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760696437908, "cdate": 1760696437908, "tmdate": 1762930912305, "mdate": 1762930912305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an alternative to standard LLM ensembling: instead of running all $N$ models $f_1, \\dots, f_N$ and averaging their next-token distributions, it samples one model at each step from a multinomial with weights $\\boldsymbol{\\alpha}$, and uses only that model’s output. The authors show this is faster than averaging distributions, with similar accuracy on their model pools as averaging. \n\nMy main concern is that it is underspecified. The method is presented as a training free approach compared to token routing, however it is a routing policy, just the input agnostic one. The paper calls this \"training free\" but does not explain how a user should actually choose these weights. Without a clear way to set , it is hard to evaluate how practical the method really is.\n\nMoreover authors do not compare proposed forms of ensembling with prior ensembling works as well as token level routing (see Weaknesses). Overall, I have an impression that it is a token level routing method without a proper router but instead with predefined weights for each model. I think the approach would be much more convincing if the  coefficients were not fixed but learned as a function of model properties (e.g., size, performance, domain strength, latency, cost)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "It is an interesting direction to reduce ensemble cost by sampling a single model instead of running all models at each step. The paper shows that this leads to speedups compared to averaging the output distributions."}, "weaknesses": {"value": "1. Positioning\n\n> While both achieve similar inference speeds, token-level routing offers the potential for better performance by making informed routing decisions based on input content. However, this benefit comes at the cost of training a router, which adds computational overhead. In contrast, LLM ensembling requires no additional training—once multiple models are available, they can be used directly—making it a training-free, plug-and-play alternative.\n\n   This framing feels misleading. Training a lightweight router is typically negligible compared to training the expert models themselves, so it is unclear why router training is actually the bottleneck. If the authors believe it is, a citation would help. In addition, the proposed method is not really router-free: it still defines global mixture weights $\\alpha_1,\\dots,\\alpha_n$ and samples an index from $\\mathrm{Multinomial}(\\alpha)$, which is itself a routing policy, just an input-agnostic one. The paper does not explain how a user is supposed to choose these $\\alpha$ values in practice. Given that, it is not obvious why one should prefer fixed global weights over even a simple learned router that could account for task type, model size, latency/overhead constraints, and load balancing.\n\nAuthors do provide some ablations regarding the alpha parameter, but it is only for a pool of 2 models that were trained on the same data just different size (Llama 3b and 8b).\n\n2. Lack of baseline comparison\n\nAs noted above, the paper positions the method relative to prior work, but the only quantitative baseline used is Conventional Ensembling (CE) -- plain averaging of next-token distributions. However, prior work on probability-level model ensembling (DeepEn, GAC, LLM-Blender, Pair-Ranker) applies additional post-processing, reweighting, or reranking over model outputs rather than naive averaging. These approaches are much closer to how ensembles are actually deployed and how the output distributions are used. The paper does not compare to these baseline. On the other hand, proposed method uses predefined input agnostic alphas (for me it is a form of routing) but do not compare with token level routing methods. Including such baselines, or explicitly explaining why they are not applicable here, would make the empirical claims significantly stronger.\n\nSmall notes:\n1. I found the claim (“conventional machine learning models output scores, normalize with softmax, and then pick the highest-scoring label”) in the introduction vague and too broad. It is not clear what “conventional machine learning models” refers to. Are the authors specifically talking about multiclass classification? If so, this should be stated explicitly. More importantly, the contrast the authors set up between “softmax → argmax” and “sampling from a probability distribution” is not well motivated. Generative models (including language models, but also beyond LLMs) do treat the output as a probability distribution and sample from it. Conversely, taking argmax after softmax can be seen as sampling from a delta distribution. I would suggest tightening or removing this contrast, since it distracts from the core technical point."}, "questions": {"value": "1. > An interesting finding on the RTX 3090 was that parallel CE was slower than sequential CE. This is likely due to the slower inter-GPU communication speed of the RTX 3090, which introduces significant overhead and negates the benefits of parallelization.\n\nI suppose it depends on the type of interconnect that is used. Can authors clarify on that?\n\n2. If I understand correctly, the proof of equivalence (Eq. (1) vs Eq. (2)) assumes you’re operating with the full distributions, but authors also use Top-k union (select the most probable k tokens). Then, if I understand correct, the equivalence breaks. Can authors clarify this?\n\n3. What is $k$ that was used for the number in the Table 4?\n\n4. Why in Table 2 the set of used models is different from Table 4? Could authors provide the downstream performance results for the models in Table 4? Also the models used in Table 4 are base model + math / code fine-tuned. I am not sure that this is the best use case for ensembling.\n\n5. What alphas are used for the pool of 3 models in  Table 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mGzzkJ8ztO", "forum": "R2FH4wrVk2", "replyto": "R2FH4wrVk2", "signatures": ["ICLR.cc/2026/Conference/Submission18925/Reviewer_tjZs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18925/Reviewer_tjZs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761614051685, "cdate": 1761614051685, "tmdate": 1762930911630, "mdate": 1762930911630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits large language model ensembling from a theoretical and computational standpoint. \nThe authors argue that traditional ensemble methods—where multiple models’ output distributions are averaged before sampling—are unnecessarily expensive for LLMs. \nThey observe that because LLMs generate text via probabilistic sampling rather than argmax selection, the ensemble distribution can be equivalently realized by randomly choosing one model at each generation step and sampling from it.\nBuilding on this observation, they propose a new algorithm, the Mixture-model-like Ensemble, which samples one model per token rather than performing multiple forward passes. \nThe paper proves that ME is mathematically equivalent to conventional ensembling, while being 1.78×–2.68× faster. Experiments on standard benchmarks (GSM8K, MMLU, BBH, ARC) demonstrate comparable performance between ME and traditional ensembles. \nThe authors further connect ME to token-level routing and mixture-of-experts methods, arguing that ensembling can be seen as a special case of routing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper offers an reinterpretation of LLM ensembling as a mixture model, yielding a mathematically simple yet computationally efficient alternative to the standard approach. This theoretical reframing is insightful and connects two important areas—ensembling and token-level routing—in a unified probabilistic view.\n2. The proposed Mixture-model-like Ensemble method is lightweight, requires no retraining, and provides substantial efficiency gains in inference, making it potentially appealing for applied settings where ensemble methods are otherwise too slow.\n3. Experiments span diverse model families (Qwen, Llama, Mistral, etc.) and show consistent equivalence between ME and conventional ensembles across datasets, supporting the theoretical claim. The authors also evaluate speed on multiple GPUs, strengthening the empirical rigor.\n4. The paper is well-structured and accessible. Figures and pseudo-code effectively illustrate the differences between conventional and mixture-model-like ensembles. The motivation is intuitive, and the empirical sections are easy to follow."}, "weaknesses": {"value": "1. The equivalence between ME and conventional ensembling only holds when sampling is used for decoding. Most real-world LLM applications (QA, summarization, code generation) rely on deterministic decoding (greedy or beam search). The paper’s method thus applies to a narrower class of scenarios than implied, and no results are provided under deterministic conditions.\n2. The theoretical proof guarantees equality of token-level distributions, but generation is autoregressive—errors or random choices can compound. The paper does not analyze how ME diverges from CE over long sequences or measure variance across multiple runs, leaving potential stability issues unaddressed.\n3. While the authors draw conceptual parallels between ensembling and routing, no experiments compare ME with even simple token-level routing methods or mixture-of-experts variants. This weakens the claimed connection and leaves readers uncertain about ME’s relative merits beyond computational efficiency.\n4. All benchmarks focus on factual or reasoning accuracy (e.g., GSM8K, MMLU). Since ensembles often facilitate diversity and alignment in generation, the omission of open-ended evaluations limits the significance of the findings."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xvfPpFLMZy", "forum": "R2FH4wrVk2", "replyto": "R2FH4wrVk2", "signatures": ["ICLR.cc/2026/Conference/Submission18925/Reviewer_NTcK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18925/Reviewer_NTcK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18925/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722518053, "cdate": 1761722518053, "tmdate": 1762930910908, "mdate": 1762930910908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}