{"id": "uBThjlbzxS", "number": 4163, "cdate": 1757617342765, "mdate": 1759898049571, "content": {"title": "Doxing via the Lens: Revealing Location-related Privacy Leakage on Multi-modal Large Reasoning Models", "abstract": "Recent advances in multi-modal large reasoning models (MLRMs) have shown significant ability to interpret complex visual content. While these models possess impressive reasoning capabilities, they also introduce novel and underexplored privacy risks. In this paper, we identify a novel category of privacy leakage in MLRMs: Adversaries can infer sensitive geolocation information, such as users' home addresses or neighborhoods, from user-generated images, including selfies captured in private settings. To formalize and evaluate these risks, we propose a three-level privacy risk framework that categorizes image based on contextual sensitivity and potential for geolocation inference. We further introduce DoxBench, a curated dataset of 500 real-world images reflecting diverse privacy scenarios divided into 6 categories. Our evaluation across 13 advanced MLRMs and MLLMs demonstrates that most of these models outperform non-expert humans in geolocation inference and can effectively leak location-related private information. This significantly lowers the barrier for adversaries to obtain users' sensitive geolocation information. We further analyze and identify two primary factors contributing to this vulnerability: (1) MLRMs exhibit strong geolocation reasoning capabilities by leveraging visual clues in combination with their internal world knowledge; and (2) MLRMs frequently rely on privacy-related visual clues for inference without any built-in mechanisms to suppress or avoid such usage. To better understand and demonstrate real-world attack feasibility, we propose GeoMiner, a collaborative attack framework that decomposes the prediction process into two stages consisting of clue extraction and reasoning to improve geolocation performance. Our findings highlight the urgent need to reassess inference-time privacy risks in MLRMs to better protect users' sensitive information.", "tldr": "", "keywords": ["Privacy Leakage"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aed7512676a38fea63e1cfd91d36337f1e76cbd7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identifies a new privacy risk in multimodal large reasoning models (MLRMs): geolocation inference leakage, where models can deduce users’ home locations from images like selfies. The authors propose a three-level privacy risk framework and introduce DOXBENCH, a 500-image dataset to evaluate such risks. They also develop GEOMINER, an attack framework that demonstrates how MLRMs can effectively infer locations using visual clues, underscoring the urgent need for privacy safeguards in multimodal AI systems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper presents a comprehensive study, covering the motivation, evaluation of existing methods, as well as attack and defense aspects.\n\n- I particularly like Table 1, which maps the identified risks to specific legal and regulatory provisions.\n\n- The experiments are also quite thorough and well-conducted.\n\n- threat model defined is aligned with the practice."}, "weaknesses": {"value": "First, I would like to clarify that I have read the entire main body of the paper carefully, including some of the appendices that are relevant to my interests, as well as the figures and tables in those appendices. I may have skipped certain parts of the appendix that are either unrelated to the main text or not of direct interest to me. Therefore, if any of my questions have already been addressed in the appendix, please kindly point that out.\n\n- The prompt structures used in the benchmark evaluation may reflect different risk capabilities. How do you evaluate the potential capability or upper bound of risk represented by each prompt?\n\n- I noticed that in Table 2, Claude’s VRR is quite low, which seems to significantly affect the overall evaluation outcome. Can your evaluation strategy mitigate or correct this imbalance caused by low VRR values?\n\n- Many of the result explanations in the paper do not clearly indicate which figure or table they refer to. For example, the statement “Prediction difficulty increases with the annotated levels”, which figure is this referring to?\n\n- How was Figure 25 generated? From which dataset or classification process did it originate, and what method was used for the classification?\n\n- Regarding Figure 4, could there be potential information leakage, since the “clue” might have been extracted from the same dataset used for evaluation?"}, "questions": {"value": "- Can your evaluation strategy mitigate or correct this imbalance caused by low VRR values?\n\n- Many of the result explanations in the paper do not clearly indicate which figure or table they refer to. For example, the statement “Prediction difficulty increases with the annotated levels”, which figure is this referring to?\n\n- How was Figure 25 generated? From which dataset or classification process did it originate, and what method was used for the classification?\n\n- Regarding Figure 4, could there be potential information leakage, since the “clue” might have been extracted from the same dataset used for evaluation?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The main issue with this paper lies in its **ethical concerns**, which likely require the **intervention of the ICLR Ethics Committee**.\nSpecifically, there are several points that need careful evaluation:\n\n1. The paper employed **268 unique workers participating in Mtruck**, but it does not specify the **ethical procedures**, **payment details**, or whether **ethical approval** was obtained. The claimed IRB exemption seems to cover only **dataset collection**, not human participation.\n2. In the **Ethics Statement**, the authors mention that data collection received an **IRB exemption**, but there is **no reference ID or supporting documentation** provided. These materials should be **strictly reviewed**.\n3. The paper **should not be published** until all ethical issues are **properly addressed and verified**."}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Rhw2CwnyZS", "forum": "uBThjlbzxS", "replyto": "uBThjlbzxS", "signatures": ["ICLR.cc/2026/Conference/Submission4163/Reviewer_gxPb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4163/Reviewer_gxPb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760931511314, "cdate": 1760931511314, "tmdate": 1762917207651, "mdate": 1762917207651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a novel privacy risk associated with recent multi-modal large reasoning models (MLRMs). It finds that these models possess sophisticated reasoning capabilities enabling them to infer sensitive geolocation information, such as home addresses or neighborhoods, from user-generated images like selfies, even those taken in private settings. To evaluate this risk, the authors propose a three-level privacy risk framework and introduce DOXBENCH, a benchmark dataset of 500 real-world images representing various privacy scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper focuses on evaluating the risk of geolocation privacy leakage in large models and introduces a novel benchmark dataset for this purpose.\n\n2. The use of the GLARE metric is well-justified, offering a more comprehensive assessment than simple accuracy measures alone.\n\n3. The experiments conducted are extensive, covering a wide range of mainstream large models."}, "weaknesses": {"value": "1. The paper anchors its geographic scope almost exclusively to California, rendering the dataset incomplete and limiting its persuasiveness. This raises concerns about potential bias, possibly stemming from an overrepresentation of California data in the large models' training sets. While the authors acknowledge this limitation in Appendix F, the discussion provided is far from sufficient to address the concern.\n\n2. Critical questions regarding defense mechanisms and the utility of geolocation data remain unanswered. How can this identified privacy vulnerability be effectively mitigated? Furthermore, what is the quantifiable benefit or utility gained by large models from leveraging geographic coordinates?\n\n3. The distinction between \"Privacy Space\" and \"Personal Imagery\" lacks clarity. What are the precise operational differences between these two concepts? They appear highly similar and ultimately converge on the fundamental issue of personal privacy, making their practical differentiation ambiguous."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0iUHRIWu4H", "forum": "uBThjlbzxS", "replyto": "uBThjlbzxS", "signatures": ["ICLR.cc/2026/Conference/Submission4163/Reviewer_WVq6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4163/Reviewer_WVq6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656091255, "cdate": 1761656091255, "tmdate": 1762917207476, "mdate": 1762917207476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reveals that multi-modal large reasoning models (MLRMs) can infer users’ private locations from ordinary photos, including selfies in personal spaces. The authors build DOXBENCH, a 500-image dataset of real-world scenes annotated into three legal privacy-risk levels, and evaluate 13 MLRMs and MLLMs, showing that most outperform non-expert humans in geolocation inference. The paper further proposes CLUEMINER to identify which visual clues drive location reasoning, and GEOMINER, a two-stage attack combining clue extraction and reasoning to amplify leakage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The problem studied in this paper is interesting and well-motivated. \n\nS2: The paper introduces a purpose-built dataset of 500 privacy-sensitive, real-world images representing personal spaces rather than public landmarks, making the evaluation realistic and legally grounded.\n\nS3: The paper benchmarks 14 leading multimodal models using reproducible metrics. \n\nS4: The paper develops CLUEMINER (to identify visual clue categories) and GEOMINER (a two-stage clue-assisted attack) that together reveal how and why leakage occurs. \n\nS5: Overall, the paper is well written, well-organized, and easy to follow."}, "weaknesses": {"value": "W1: DOXBENCH primarily includes images from California and nearby areas, which may limit geographic, cultural, and environmental diversity; generalization to other regions remains unclear.\n\nW2: It seems that the experiments focus on image-based inputs; the approach and findings may not fully extend to other modalities (e.g., video or text-image pairs)."}, "questions": {"value": "Q1: Can the authors briefly discuss whether their findings generalize to regions beyond California, and what additional geographic or environmental factors might influence model performance?\n\nQ2: Can the proposed analysis and metrics be extended to other modalities, such as video or text–image pairs, and what challenges might arise in doing so?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nyd4Y6bMEJ", "forum": "uBThjlbzxS", "replyto": "uBThjlbzxS", "signatures": ["ICLR.cc/2026/Conference/Submission4163/Reviewer_APLg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4163/Reviewer_APLg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774479842, "cdate": 1761774479842, "tmdate": 1762917207097, "mdate": 1762917207097, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents the first systematic study of location-related privacy leakage in Multi-modal Large Reasoning Models. The authors identify a novel privacy risk where adversaries can infer sensitive geolocation information from user-generated images. Key contributions include:They constructed DoxBench, the first benchmark dataset specifically designed to evaluate this risk. They also introduced a three-tier privacy risk taxonomy grounded in legal frameworks. Furthermore, it innovatively proposes GLARE, an information-theoretic metric to quantify the extent of privacy leakage, and develops analytical tools named ClueMiner and GeoMiner to trace the root causes of the risk and demonstrate attack feasibility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors have meticulously constructed a privacy dataset containing real-world scenarios, proposed a highly innovative evaluation metric, and demonstrated the pervasiveness and severity of the risks through extensive experiments. They even showcased how their attack tools could enable ordinary users to achieve this with ease. The entire research framework is comprehensive, progressing logically from problem definition and analysis to verification, with robust evidence throughout."}, "weaknesses": {"value": "The current dataset primarily focuses on California, USA, which naturally raises the question: would this methodology remain equally effective when applied to European or Asian streetscapes and architectural styles? Furthermore, in the defense section, while several methods were tested, the underlying reasons for their failures haven't been thoroughly explored. A deeper analysis of how the models circumvent blurring and noise-based defenses would provide more valuable insights."}, "questions": {"value": "Could you validate your findings on images from other geographic regions？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dtgHBRKDEz", "forum": "uBThjlbzxS", "replyto": "uBThjlbzxS", "signatures": ["ICLR.cc/2026/Conference/Submission4163/Reviewer_qp28"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4163/Reviewer_qp28"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4163/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914363429, "cdate": 1761914363429, "tmdate": 1762917206884, "mdate": 1762917206884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}