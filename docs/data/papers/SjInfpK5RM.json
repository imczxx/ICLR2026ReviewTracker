{"id": "SjInfpK5RM", "number": 3604, "cdate": 1757486025422, "mdate": 1759898078995, "content": {"title": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching", "abstract": "Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d$^2$Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d$^2$Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The anonymous evaluation codes are available at \\url{https://anonymous.4open.science/r/d2Cache-5538}.", "tldr": "", "keywords": ["Diffusion-based large language models", "Key-value caching", "Inference acceleration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/77f13d04d1023bfdebe0213b84a2c76a6a6ff3bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes D2Cache, a training-free approximate key-value (KV) cache framework designed to accelerate inference in diffusion-based large language models (dLLMs). Unlike autoregressive models (ARMs), dLLMs rely on bidirectional attention and cannot directly reuse KV states efficiently.\n\nD2Cache introduces a two-stage fine-grained token selection process:\n1. Certainty Prior-Guided Selection: Selects masked tokens to update based on both their prediction confidence and local token density, yielding a more reliable quasi-left-to-right decoding order.\n2. Attention-Aware Selection: Updates KV states only for tokens that receive higher attention weights, identified using attention rollout.\n\nThrough this adaptive caching mechanism, D2Cache reduces redundant computations and achieves significant speedups (3–4×) over baselines such as dLLM-Cache and Fast-dLLM, while maintaining or slightly improving task performance across math datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a dual adaptive caching scheme that fine-tunes token-level updates for dLLMs. This is a clear step forward compared to previous coarse-grained cache methods like Fast-dLLM and dLLM-Cache.\n2. D2Cache demonstrates up to 4× acceleration without significant performance loss. The method achieves competitive or better results than baselines across multiple reasoning and programming datasets."}, "weaknesses": {"value": "1. The baseline (confidence-based decoding) performs much worse than the proposed certainty prior-guided method, even when acceleration isn’t applied. This large performance gap raises doubts about whether the comparison is fair or due to confounding factors. It would be useful to include random sampling or alternative decoding methods (e.g., semi-autoregressive decoding) for stronger baselines. \n\n2. There’s ambiguity between speedup runs (Table 1) and default non-autoregressive runs (Table 2). Where Table 1 evaluates speedup under different methods, and Table 2 evaluates decoding schemes (confidence-based vs certainty-prior). However, both report exacly same performance scores, which makes it unclear whether the performance metrics are decoupled from the acceleration setting or derived from identical runs.\n3. The experiments focus only on math and code-related tasks. It remains unclear how the proposed method generalizes to general tasks. For example, perplexity should be evaluated as a basic metric."}, "questions": {"value": "1. Since the proposed decoding strategy (certainty prior-guided) enforces a quasi-left-to-right order, how does it differ from simply using an autoregressive model? If the pipeline behaves like a “quasi-AR” system, does the original advantage of dLLMs (e.g., bidirectional modeling, parallel decoding) still hold?\n2. Were the baseline methods (dLLM-Cache, Fast-dLLM) tuned optimally for the same tasks? Given the performance gap, could hyperparameter choices or block sizes explain some of the observed differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gwr72QII10", "forum": "SjInfpK5RM", "replyto": "SjInfpK5RM", "signatures": ["ICLR.cc/2026/Conference/Submission3604/Reviewer_E2JM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3604/Reviewer_E2JM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760695447229, "cdate": 1760695447229, "tmdate": 1762916864124, "mdate": 1762916864124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To solve diffusion-based LLMs’ (dLLMs) inference inefficiency (cannot use standard KV caches), the paper proposes d2Cache, a training-free KV cache framework. It leverages two insights: (1) masked tokens’ KV states only need updates in the \"rapid-change\" phase; (2) dLLM attention concentrates on a small token subset. d2Cache uses two-stage selection (certainty prior for masked tokens, attention rollout for prompt/decoded tokens) and guided decoding to avoid premature termination. Experiments on 2 dLLMs (LLaDA-8B/Dream-7B) and 4 benchmarks show 3.2×–4.0× speedup over vanilla dLLMs, outperforming baselines while improving accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Empirically grounded design with measurable evidence: The framework  relies on fine-grained analyses of dLLM behavior. For example, PCA visualizations confirm masked tokens’ three-phase KV evolution (e.g., LLaDA-8B-Inst/GSM8K shows rapid changes in steps 64–98), and attention rollout proves 80% of attention concentrates on 20% of prompt/decoded tokens. These insights show the \"phase-based updates\" and \"high-attention selection\" core strategies.\n2. Most dLLM efficiency methods trade accuracy for speed, but d2Cache improves both. Its certainty prior-guided decoding mitigates vanilla dLLMs’ \"U-shaped\" bias (tokens at sequence boundaries decoded first, leading to premature, boosting accuracy while accelerating inference. For instance, on Dream-Inst/HumanEval, it raises throughput from 4.39 to 14.06 and accuracy from 56.7 to 61.6."}, "weaknesses": {"value": "1. The paper only tests 7B/8B parameter models and sequences ≤512 tokens. For larger models (e.g., 13B/70B dLLMs), the computational overhead of attention rollout may scale sharply with model size, offsetting speed gains. It is interesting to discuss the proposed method on the experiments at scale.\n2. The paper ignores scenarios where d2Cache underperforms. For example, on LLaDA-Inst/MBPP, its accuracy (12.4) is lower than Fast-dLLM (13.8) and dLLM-Cache (12.8)， although the proposed method is better at throughput and latency. It is interesting to explain whether code generation requires more frequent masked token updates (k=32 is too small), attention rollout misses code-specific critical tokens (e.g., function call syntax), or the certainty prior fails to capture code structure dependencies."}, "questions": {"value": "1. Scalability to large models: How does d2Cache’s performance (speedup, accuracy) change when applied to 13B/70B dLLMs?  Can lightweight optimizations (e.g., sparse attention rollout, only using top-5 layers) reduce this overhead without losing accuracy?\n2. Adaptation to long sequences: For sequences longer than 512 tokens (e.g., 1k PubMed abstracts, 2k legal documents), how should the Gaussian function’s σ be adjusted? Does a dynamic σ (e.g., σ = sequence_length × 0.02) maintain accuracy while avoiding excessive updates?\n3. Failure case root cause: On LLaDA-Inst/MBPP, why does d2Cache’s accuracy (12.4) lag behind Fast-dLLM (13.8)? Are there specific code patterns (e.g., nested loops, import statements, function definitions) where the two-stage selection fails?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4zlhR3myt9", "forum": "SjInfpK5RM", "replyto": "SjInfpK5RM", "signatures": ["ICLR.cc/2026/Conference/Submission3604/Reviewer_1oge"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3604/Reviewer_1oge"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760951836593, "cdate": 1760951836593, "tmdate": 1762916863943, "mdate": 1762916863943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors proposes d2Cache, a training-free approximate cache that updates only a small subset of tokens’ KV states and reuses cached KV for the rest. Token selection is two-stage: Certainty-prior for masked tokens and attention-aware selection. The accuracy and efficieny are improved by this method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. KV cache for dLLM inference is new, where standard KV cache doesn’t apply.\n\n2. Simple design with limited moving parts and clear hyperparameters.\n\n3. consistent speedups and several accuracy improvements."}, "weaknesses": {"value": "1. Please explore hyperparameters of k  and p. these likely trade speed vs. quality and are important for deployment.\n\n2. Results use gen lengths 256–512; authors may do experiement on L=1-4k. The rollout cost and cache efficacy under long prompts should be evaluated\n\n3. How are layer norms, residuals, and MLP activations treated when some tokens are stale, as freezing only K V may mismatch with new hidden states."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kHe0XNQkgA", "forum": "SjInfpK5RM", "replyto": "SjInfpK5RM", "signatures": ["ICLR.cc/2026/Conference/Submission3604/Reviewer_KmGv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3604/Reviewer_KmGv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853371190, "cdate": 1761853371190, "tmdate": 1762916863390, "mdate": 1762916863390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Diffusion-based large language models (dLLMs) outperform autoregressive models (ARMs) in mitigating the \"reversal curse\" and capturing global semantics but suffer from low inference efficiency—they cannot use standard KV caches due to bidirectional attention, and existing coarse-grained approximate KV solutions lack flexibility or cause redundancy. To address this, the authors propose D2Cache, a training-free framework built on two insights: masked tokens’ KV states only need updates in the \"rapid-change phase\" before decoding, and dLLMs’ attention concentrates on a small subset of prompt/decoded tokens. D2Cache uses a two-stage strategy: 1) selecting masked tokens via a \"certainty prior\" (combining prediction confidence and nearby known token density) for KV updates, enabling quasi left-to-right generation; 2) choosing high-impact prompt/decoded tokens via attention rollout. Experiments on LLaDA-8B and Dream-v0-7B show D2Cache achieves 3.2×–4.0× average speedups (e.g., 4.7× on Dream-Inst/GSM8K) while maintaining/improving generation quality, outperforming baselines like dLLM-Cache and Fast-dLLM. It advances dLLMs’ practical deployment by bridging performance and inference efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **Originality**: Breaks from prior coarse-grained (segment/block-level) KV caching for dLLMs, proposing a token-level adaptive strategy. It’s driven by two dLLM-specific insights (masked tokens’ KV states only need updates in the \"rapid-change phase\"; dLLMs’ attention concentrates on prompt/decoded tokens) and introduces \"certainty prior-guided decoding\" to fix NAR’s premature overconfidence bias.  \n\n- **Quality**: Rigorous validation covers 2 dLLMs (LLaDA-8B, Dream-v0-7B), 4 tasks (reasoning, code generation), and key metrics (throughput, latency, accuracy). Comprehensive ablations (decoding schemes, update phases, hyperparameters) isolate design impacts, with mathematical formalization (Gaussian-weighted density, attention rollout) ensuring theoretical soundness.  \n\n- **Clarity**: Follows a logical \"problem→insight→solution→validation\" flow. Visualizations and plain-language formula explanations enhance accessibility, while full parameter specs, public datasets, and planned code release support reproducibility.  \n\n- **Significance**: Achieves 3.2×–4.0× average inference speedups (up to 4.7×) while maintaining accuracy, resolving dLLMs’ deployment bottleneck. The token-level caching paradigm inspires optimization for other bidirectional models, and its benchmarks set a reference for future work."}, "weaknesses": {"value": "+ **Narrow validation scope**: Only tests mid-sized dLLMs (8B/7B) and 4 reasoning/code tasks—lacks large-scale dLLMs (17B+/70B) and non-technical tasks (dialogue/creative writing). If it is possible, add experiments on large dLLMs and conversational datasets (e.g., DailyDialog).  \n\n+ **Fixed hyperparameters**: Uses static k=32/p=0.1/σ=10 across all scenarios, no adaptive tuning. If it is possible, add hyperparameter sensitivity analysis (e.g., long vs. short sequences) and lightweight dynamic adjustment (as a supplement to ablation study, beyond section D.4).  \n\n+ **Unreported memory overhead**: Ignores peak GPU memory usage—critical for edge deployment. If it is possible, add memory metrics and analyze speed-accuracy-memory trade-offs."}, "questions": {"value": "+ **Discussion 1**: How do you think your method is compatible with token sparsification techniques [1], which have been proven useful for LLMs [2]? Could you add related discussion?\n\n+ **Discussion 2**: While the paper presents the method as a training-free framework, it raises the question of its potential trainability. It would be interesting to investigate whether the framework can be made trainable and how fine-tuning might affect its performance.\n\n**I would be happy to increase my rating if my views are given a thorough discussion.**\n\n[1]: Wu X, Zeng F, Wang X, et al. PPT: Token pruning and pooling for efficient vision transformers[J]. arXiv preprint arXiv:2310.01812, 2023.\n\n[2]: Wan Z, Wu X, Zhang Y, et al. D2o: Dynamic discriminative operations for efficient generative inference of large language models[J]. arXiv preprint arXiv:2406.13035, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6MWwtBBF7n", "forum": "SjInfpK5RM", "replyto": "SjInfpK5RM", "signatures": ["ICLR.cc/2026/Conference/Submission3604/Reviewer_4Cqp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3604/Reviewer_4Cqp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3604/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911417040, "cdate": 1761911417040, "tmdate": 1762916862861, "mdate": 1762916862861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}