{"id": "oFC7fUsGje", "number": 15665, "cdate": 1758253693796, "mdate": 1763717786251, "content": {"title": "On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs", "abstract": "Reinforcement learning (RL) fine-tuning has become a key technique for enhancing large language models (LLMs) on reasoning-intensive tasks, motivating its extension to vision language models (VLMs). While RL-tuned VLMs improve on visual reasoning benchmarks, they remain vulnerable to weak visual grounding, hallucinations, and over-reliance on textual cues. We show that simple, controlled textual perturbations—misleading captions or incorrect chain-of-thought (CoT) traces—cause substantial drops in robustness, and that these effects are more pronounced when CoT consistency is taken into account across open-source multimodal reasoning models. To better understand these vulnerabilities, we further analyze RL fine-tuning dynamics and uncover an accuracy–faithfulness trade-off: fine-tuning raises benchmark accuracy, but can simultaneously erode the reliability of the accompanying CoT and its robustness to contextual shifts.\nAlthough adversarial augmentation improves robustness, it does not by itself prevent faithfulness drift. Incorporating a faithfulness-aware reward can restore alignment between answers and reasoning, but when paired with augmentation, training risks collapsing onto shortcut strategies and robustness remains elusive. Together, these findings highlight the limitations of accuracy-only evaluations and motivate training and assessment protocols that jointly emphasize correctness, robustness, and the faithfulness of visually grounded reasoning.", "tldr": "", "keywords": ["vision language model", "visual reasoning", "chain-of-thought", "robustness", "reinforcement learning", "RL finetuning", "chain-of-thought consistency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f3c60a8d745a6aee528009e7cbfabfb7abba6847.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the robustness and chain-of-thought (CoT) consistency of reinforcement-finetuned vision-language models (VLMs). The authors introduce controlled textual perturbations (e.g., Wrong-Caption and Wrong-Think) to test whether these models can resist misleading text and remain visually grounded. Results show that although RL fine-tuning improves benchmark accuracy, it reduces reasoning faithfulness and heightens sensitivity to misleading context. To mitigate these effects, the authors propose data augmentation and faithfulness-based reward methods that enhance both robustness and reasoning coherence."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on an important question—how Reinforcement Finetuning affects the reasoning faithfulness of large vision-language models."}, "weaknesses": {"value": "1. The overall contribution of this paper is limited. It lacks technical innovation, does not present a solid benchmark, and offers no particularly insightful experimental findings.\n\n    a. The paper primarily analyzes existing RL-trained VLMs through text perturbations. However, such perturbation-based evaluation has been extensively explored in prior works, such as [1].\n\n    b. The benchmark proposed in this paper is mostly an extension of existing datasets with additional annotations of incorrect text or captions. This extension is incremental and does not provide substantial novelty or methodological advancement. Besides, it is not clear whether the additional annotations will be released.\n\n    c. The proposed data augmentation strategy resembles approaches introduced in previous studies, such as [1]. Furthermore, the faithfulness-based reward function is conceptually similar to previously explored *process reward models* used in prior work, such as [2]\n\n[1] Chen et al. PerturboLLaVA: Reducing Multimodal Hallucination with Perturbative Visual Training.\n\n[2] Zhang et al. The Lessons of Developing Process Reward Models in Mathematical Reasoning.\n\n\n2. The results in Table 1 show only marginal improvements or even degradations. Moreover, the evaluation relies on a single prompt configuration, making it difficult to assess the robustness and reliability of the reported results. To improve experimental validity, multiple prompt settings should be tested to evaluate consistency across prompt variations.\n\n\n3. The assessment of faithfulness and reasoning trace quality depends solely on a single large-model judge. The absence of human evaluation or inter-annotator validation undermines the reliability of the reported metrics. It remains unclear whether the automated judgments accurately reflect true reasoning quality.\n\n4. The used datasets assess simple spatial patterns and do not reflect complex multimodal reasoning or real-world understanding. It is unclear whether improvements or degradations on such tasks can generalize to more complex real world visual understanding.\n\n5. The implementation details of experiments in Section 3 are unclear. Though the author provide details on how to generate captions and initial think, it is unclear how these annotations are used in training."}, "questions": {"value": "1. How does your method differ from previous perturbation-based evaluations and previous design on process reward (e.g., [1], [2])?\n2. Could you describe more clearly how the “initial think” and caption data are used in training?\n3. Are the findings consistent when tested on more complex used, multimodal reasoning datasets, such as MMBench, V* Bench, MME, GQA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4jN2OMvMCw", "forum": "oFC7fUsGje", "replyto": "oFC7fUsGje", "signatures": ["ICLR.cc/2026/Conference/Submission15665/Reviewer_j4PE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15665/Reviewer_j4PE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638241156, "cdate": 1761638241156, "tmdate": 1762925921897, "mdate": 1762925921897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically evaluates the robustness and COT faithfulness of RL-tuned vision language models on basic visual reasoning, including counting, identity, and 2D and 3D spatial relations on static images. The authors augment established benchmarks: 3DSRBench, CV-Bench, Spatial-MM, and WhatsUp and add controlled textual perturbations that keep the image unchanged:: Stop-Think, Wrong-Think, and Wrong-Caption. Across five RL-finetuned models derived from Qwen2.5-VL-7B-Instruct, the study measures accuracy, robustness, and CoT consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Focusing on robustness and faithful, visually grounded reasoning, the paper uses simple, controlled textual perturbations to effectively probe modality conflict.\nBy analyzing training dynamics, the paper indicates an accuracy–faithfulness tradeoff, shows that augmentation improves robustness while faithfulness continues to drift, and finds that adding faithfulness to the reward aligns CoT with answers yet becomes unstable when combined with augmentation, yielding limited robustness gains."}, "weaknesses": {"value": "The paper mainly reveals the accuracy–faithfulness disconnect and sensitivity to textual perturbations, but does not provide training or inference method that can be readily reused.\nThe augmentation strategies with wrong-think and wrong-caption yield clear in-distribution improvements, but evidence for transfer across datasets and tasks is limited.\nOut-of-distribution performance is under-reported, including results on different data sources and task types\nFormatting error: “n Appendix D.1 we show that” seems it should be “In Appendix D.1 we show that.”"}, "questions": {"value": "Please report results on  spatial-relation datasets from different sources, outside the training mix, to quantify performance under distribution shift\nEvaluate transfer without changing the training method (e.g., spatial relations, spatial relations → geometric reasoning) to distinguish answer accuracy from visually grounded reasoning.\nReport out-of-distribution variants for Wrong-Think and Wrong-Caption separately."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NJ8edC9x0j", "forum": "oFC7fUsGje", "replyto": "oFC7fUsGje", "signatures": ["ICLR.cc/2026/Conference/Submission15665/Reviewer_h5wX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15665/Reviewer_h5wX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789065307, "cdate": 1761789065307, "tmdate": 1762925921393, "mdate": 1762925921393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic empirical study of the robustness and chain-of-thought (CoT) consistency of RL-finetuned vision-language models (VLMs). By introducing controlled textual perturbations—misleading captions and reasoning prefixes—the authors reveal that RL-tuned models are often misled by textual cues and exhibit an accuracy–faithfulness trade-off. They further analyze the effects of adversarial augmentation and faithfulness-aware rewards. The findings provide valuable diagnostic insights into how RL fine-tuning affects multimodal reasoning reliability."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "+ Proposes a clear and reproducible textual perturbation framework to probe VLM robustness.\n\n+ Identifies a consistent accuracy–faithfulness trade-off during RL fine-tuning.\n\n+ Covers a wide range of recent RL-based multimodal reasoning models and benchmarks.\n\n+ Analysis is careful, and the findings are both timely and practically relevant."}, "weaknesses": {"value": "+ The paper remains primarily empirical, without a formal theoretical explanation or principled model-level intervention to mitigate the observed trade-off.\n\n+ The faithfulness-as-reward experiments, though conceptually interesting, are underexplored; their instability and optimization dynamics merit deeper quantitative analysis.\n\n+ The study relies on a single large-language-model judge (Qwen3-32B) to assess reasoning faithfulness, which may introduce evaluation bias; cross-validation with other judgment models would strengthen the claims.\n\n+ While the work discusses potential solutions (e.g., richer reward signals, uncertainty modeling), these remain qualitative suggestions rather than systematically validated interventions."}, "questions": {"value": "+ Have the authors considered developing a formal metric or theoretical framing for “faithfulness drift”? For instance, could the trade-off between accuracy and CoT consistency be modeled via reward attribution entropy or causal influence metrics?\n\n+ Can the authors provide quantitative evidence—such as variance across seeds, reward gradients, or convergence plots—to substantiate the claim of unstable training dynamics?\n\n+ Have alternative evaluators been tested to confirm that the faithfulness judgments are not artifacts of Qwen3’s inductive biases?\n\n+ Could the authors empirically evaluate one or more of the suggested remedies (e.g., uncertainty-aware reward, contrastive consistency loss) to demonstrate their potential effectiveness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FpvaYnhaoN", "forum": "oFC7fUsGje", "replyto": "oFC7fUsGje", "signatures": ["ICLR.cc/2026/Conference/Submission15665/Reviewer_2c19"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15665/Reviewer_2c19"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908117346, "cdate": 1761908117346, "tmdate": 1762925920752, "mdate": 1762925920752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We’d like to thank all reviewers for taking the time to provide careful feedback of our work. We have updated our submission with edits and additional results, namely the addition of three new benchmarks and two new evaluator models to validate our approach. The edits are visible in blue font. To summarize, we have made the following changes:\n\n* Section 2: Added three new datasets (MMBench, MME-RealWorld-Lite, V*-Bench) which emphasizes more general VQA and real-world understanding rather than primarily spatial relations and addresses concerns raised by Reviewers h5wX and j4PE. The full ablations are given in **Appendix D.1** (Wrong-Think/Wrong-Caption, Stop-Think, Faithfulness and interannotator agreement). We generally see the same trends and provide additional exposition in the pdf.\n* Section 2: Added two evaluator models as LLM-as-judges (Llama3.1-70B-Instruct, GPT-OSS-120B) and report interannotator agreement metrics in **Table 2**. We chose these models to span different model families and training data distributions. Reviewers 2c19 and j4PE raised this valid concern about possible evaluation bias introduced by using Qwen3-32B. We put the table below as well, reporting on agreement across judge models (A = GPT-OSS-120B, B = Qwen3-32B, C = Llama3.1-70B-Instruct). Quantities are grouped by dataset and averaged across all open-source VLMs and perturbation settings. Strict 3-way agreement reports the percentage of examples where all three judges agree. Across datasets, we observe consistently high inter-annotator agreement among the three judge models. \n\n| Dataset            | Strict 3-way agree (%) | Cohen's κ(A,B) | κ(A,C) | κ(B,C) | Fleiss' κ |\n|--------------------|------------------------|----------------|--------|--------|-----------|\n| 3DSRBench          | 92.3                   | 0.880          | 0.846  | 0.823  | 0.850     |\n| CVBench            | 94.2                   | 0.899          | 0.883  | 0.870  | 0.884     |\n| SpatialMM-Obj      | 93.4                   | 0.889          | 0.868  | 0.865  | 0.874     |\n| SpatialMM-Multihop | 94.0                   | 0.876          | 0.860  | 0.823  | 0.853     |\n| WhatsUp            | 89.2                   | 0.845          | 0.801  | 0.783  | 0.809     |\n\n* Section 3.1: Expanded and clarified our data augmentation setup for the RL fine-tuning.\n* Conclusion: More explicitly discussing why simply incorporating the text perturbations directly in the RL training data and adding a faithfulness-reward doesn’t achieve the desired behavior, and takeaways from this analysis.\n* Related Work: Added citations as per discussion with Reviewer j4PE.\n\nWe again thank the reviewers for their thoughtful and constructive feedback, which has significantly helped us refine and strengthen this work. We hope that the additional datasets, evaluator models, and ablations clarify our main claims and address the key concerns raised in the reviews. We are also actively working to incorporate additional dataset evaluations for our RL checkpoints (of which there are many more models than we could fully cover in this initial response), but we expect the qualitative trends reported in the current version to hold in these extended experiments as well. We look forward to further interaction during the discussion period, and please let us know if there is anything additional we can provide or clarify to improve your assessment of our work."}}, "id": "RSYluGDkzT", "forum": "oFC7fUsGje", "replyto": "oFC7fUsGje", "signatures": ["ICLR.cc/2026/Conference/Submission15665/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15665/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission15665/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763717744522, "cdate": 1763717744522, "tmdate": 1763717744522, "mdate": 1763717744522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}