{"id": "05PqjBzN6S", "number": 23926, "cdate": 1758350444098, "mdate": 1763635758080, "content": {"title": "When to Retrain after Drift: A Data-Only Test of Post-Drift Data Size Sufficiency", "abstract": "Sudden concept drift makes previously trained predictors unreliable, yet deciding when to retrain and what post-drift data size is sufficient is rarely addressed. We propose CALIPER —a detector- and model-agnostic, data-only test that estimates the post-drift data size required for stable retraining. CALIPER exploits state dependence in streams generated by dynamical systems: we run a single-pass weighted local regression over the post-drift window and track a one-step proxy error as a function of a locality parameter $\\theta$. When an effective sample size gate is satisfied, a monotonically non-increasing trend in this error with increasing a locality parameter indicates that the data size is sufficiently informative for retraining.\nWe also provide a theoretical analysis of our CALIPER, and we show that the algorithm has a low per-update time and memory. Across datasets from four heterogeneous domains, three learner families, and two detectors, CALIPER consistently matches or exceeds the best fixed data size for retraining while incurring negligible overhead and often outperforming incremental updates. CALIPER closes the gap between drift detection and data-sufficient adaptation in streaming learning.", "tldr": "", "keywords": ["Concept drift", "Stream learning", "Data sufficiency", "Time series"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/30f0a0a91d353a098d8ad4c780716d8d507cf708.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of determining when sufficient data is available to safely retrain a model after a sudden concept drift. The authors propose CALIPER, a model-agnostic and data-only test to estimate this required post-drift data size. The core idea is grounded in the concept of \"state dependence\" in dynamical systems. CALIPER employs a lightweight weighted local regression (WLR) to probe the local predictability of the post-drift data window. A retraining trigger is issued when the WLR's prediction error exhibits a monotonically non-increasing trend as the locality parameter increases, conditioned on a sufficient effective sample size (ESS). The authors provide theoretical analysis linking this trigger to state dependence and learnability, and empirical results across four datasets and three model families show that CALIPER outperforms fixed-window and incremental update strategies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper formalizes the problem of \"post-drift data sufficiency,\" skillfully identifying the gap between drift detection and effective model adaptation. Focusing on when to retrain, rather than just if a drift occurred, is a profound and highly practical contribution to the streaming learning community.\n\n2. The core idea of leveraging state dependence—an intrinsic data property—to infer learnability is interesting. It reframes a complex model-dependent question into a simple data-driven test. Using a lightweight WLR as a proxy is an efficient and well-justified choice for streaming environments.\n\n3. The paper provides a theoretical foundation and well-designed experiment."}, "weaknesses": {"value": "1. This method hinges on the assumption that the data stream is generated by a dynamical system of the form x(t+1) = f(x(t)) + noise, This assumption may not hold in many complex dynamical systems where the next state x(t+1) depends on an extended history of past states (x(t-k), ..., x(t)) [1, 2] or is affected by significant external latent factors. The paper does not provide an analysis of its robustness when this core assumption is violated, thus restricting the method's general applicability.\n    [1] Learning robust spectral dynamics for temporal domain generalization, arXiv preprint arXiv:2505.12585\n    [2] Continuous Temporal Domain Generalization, NeurIPS 2024.\n\n2. While the method is model-agnostic, it implicitly assumes that data sufficiency is a property independent of the downstream model. Intuitively, however, models with varying complexity and characteristics (e.g., a linear model vs. a deep Transformer) will need different amount of samples for stable converge.  The paper does not provide an analysis of how CALIPER's estimate correlates with the true convergence points of different models, nor does it discuss how it mitigates the influence of these model-dependent data requirements.\n\n3. The method relies on distance-based neighborhoods, which are notoriously unreliable in high-dimensional settings due to the curse of dimensionality. This could impair key components: the ESS check may require an impractically large window to meet the C(d+1) threshold, and the performance of WLR itself degrades. The experiments are conducted on relatively low-dimensional data, leaving its high-dimensional scalability unverified.\n\n4. The post-drift window X_t is re-normalized at every time step. This dynamic scaling could itself introduce non-stationarity or mask the underlying data dynamics, especially in the presence of outliers. A more stable streaming normalization scheme might be more appropriate.\n\n5. The ESS check is performed only at the tightest locality (θ_max), but ESS is a function of θ. This single check does not guarantee sufficient samples across the entire range of θ values tested for monotonicity, creating a potential logical inconsistency.\n\n6. The proof sketches for Propositions 1 and 2 rely on high-level intuitive arguments (e.g., \"if many pairs... a decrease could not persist\") rather than formal mathematical derivations. For instance, the link between the probability of violating state dependence and the expected change in localized error needs to be made explicit."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eNjd7SQMz6", "forum": "05PqjBzN6S", "replyto": "05PqjBzN6S", "signatures": ["ICLR.cc/2026/Conference/Submission23926/Reviewer_NZ6X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23926/Reviewer_NZ6X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761522312937, "cdate": 1761522312937, "tmdate": 1762942860216, "mdate": 1762942860216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for determining the right time to retrain/adapt a model after concept drift has occurred. The proposed method is computational efficient because it only uses the data from the data stream together with some hyperparameters."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n- Clear contribution with potentially high impact, well-grounded in the literature\n- Formal analysis of the proposed algorithm\n- Overall, the paper is well structured. Mostly easy to read, with some exceptions (see \"Weaknesses\" for suggestions on how to improve it)"}, "weaknesses": {"value": "The state dependence looks like a pretty strong assumption, as it assumes continuity of the state transition function. I am not sure if this is safe to assume. In particular, in high-dimensional settings. This is also reflected in Section 2.3. I miss a critical discussion on those assumptions, apart from Appendix C. Maybe one could also run experiments on datasets that differ in their dimensionality to get a better feeling for how the method performs in such cases.\n\nProblem 1: I do not understand why minimising the generalisation loss links to the data-side stopping criterion R. If I understand it correctly, they are supposed to model the same thing -- however, the generalisation error is difficult to compute in an online approach, right? If so, it is just a change of notation. I suggest clarifying this to avoid any potential confusion on the reader's side.\n\nLine 216: If I understood it correctly, the following steps have to be executed for every theta in the locality grid, right? If so, please state this explicitly!\n\nWhat is the reasoning behind line 223 \"Proceed only if ...\"? Do you want to ensure that the data set contains enough samples (see statement in line 238)? I suggest adding a short clarification. In general, it is important to ensure that all steps in the algorithm, the reader understands why certain things are done.\n\nA few comments on the GitHub:\n- Please provide more information in the README, in particular, describing the outline of the repository (i.e., where to find what).\n- Please put more comments and docstrings in the code\n\nMinor:\n- Acronym WLR in Figure 1.c not introduced -- becomes obvious later in the paper. However, I recommend introducing it when it is first used.\n- Problem 1: While I am familiar with epsilon-delta notions, it might be good to add a brief explanation to make the notation more accessible.\n- I suggest moving the proof sketches into the appendix -- they kind of interrupt the reading flow"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QohygaGUI4", "forum": "05PqjBzN6S", "replyto": "05PqjBzN6S", "signatures": ["ICLR.cc/2026/Conference/Submission23926/Reviewer_U28n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23926/Reviewer_U28n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761556518126, "cdate": 1761556518126, "tmdate": 1762942859864, "mdate": 1762942859864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for determining the right time to retrain/adapt a model after concept drift has occurred. The proposed method is computational efficient because it only uses the data from the data stream together with some hyperparameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths\n- Clear contribution with potentially high impact, well-grounded in the literature\n- Formal analysis of the proposed algorithm\n- Overall, the paper is well structured. Mostly easy to read, with some exceptions (see \"Weaknesses\" for suggestions on how to improve it)"}, "weaknesses": {"value": "The state dependence looks like a pretty strong assumption, as it assumes continuity of the state transition function. I am not sure if this is safe to assume. In particular, in high-dimensional settings. This is also reflected in Section 2.3. I miss a critical discussion on those assumptions, apart from Appendix C. Maybe one could also run experiments on datasets that differ in their dimensionality to get a better feeling for how the method performs in such cases.\n\nProblem 1: I do not understand why minimising the generalisation loss links to the data-side stopping criterion R. If I understand it correctly, they are supposed to model the same thing -- however, the generalisation error is difficult to compute in an online approach, right? If so, it is just a change of notation. I suggest clarifying this to avoid any potential confusion on the reader's side.\n\nLine 216: If I understood it correctly, the following steps have to be executed for every theta in the locality grid, right? If so, please state this explicitly!\n\nWhat is the reasoning behind line 223 \"Proceed only if ...\"? Do you want to ensure that the data set contains enough samples (see statement in line 238)? I suggest adding a short clarification. In general, it is important to ensure that all steps in the algorithm, the reader understands why certain things are done.\n\nA few comments on the GitHub:\n- Please provide more information in the README, in particular, describing the outline of the repository (i.e., where to find what).\n- Please put more comments and docstrings in the code\n\nMinor:\n- Acronym WLR in Figure 1.c not introduced -- becomes obvious later in the paper. However, I recommend introducing it when it is first used.\n- Problem 1: While I am familiar with epsilon-delta notions, it might be good to add a brief explanation to make the notation more accessible.\n- I suggest moving the proof sketches into the appendix -- they kind of interrupt the reading flow"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QohygaGUI4", "forum": "05PqjBzN6S", "replyto": "05PqjBzN6S", "signatures": ["ICLR.cc/2026/Conference/Submission23926/Reviewer_U28n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23926/Reviewer_U28n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761556518126, "cdate": 1761556518126, "tmdate": 1763715973926, "mdate": 1763715973926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on handling the sudden drift in streaming data and tries to explore when to retrain after drift. A method called CALIPER has been developed for detecting concept drift occurrence and stable retraining. And a theoretical analysis of the proposed method has been given for fundamental support. The experiment on several datasets and benchmarks has been conducted, and the experiment results show the performance of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The idea of \"when to retrain after drift\" is interesting, the previous works usually retrain directly when drift is detected. This work focuses on identifying the right time to retrain the model to help enhance the learning stability.\n\n2. The proposed method is well designed with a detailed theoretical analysis, and the experiment is sufficient and reflects the learning performance of the proposed method."}, "weaknesses": {"value": "1. In the experiment, models like MLP and transformer have been chosen for comparison. I think tree-based models should also be chosen for comparison, since they are commonly used in traditional concept drift learning.\n\n2. The author only compares the proposed method with ADWIN, which is a traditional drift detection method, more comparisons with recently proposed drift detection methods are required.\n\n3. A parameter analysis is needed to show the robustness of the proposed method."}, "questions": {"value": "1. The author should clarify the difference between the proposed method with traditional concept drift learning methods to enhance the novelty.\n\n2. The experiment should not only focus on MLP and Transformer, but also should focus on tree-based model, which performs excellently in concept drift learning.\n\n3. More comparison on the recently proposed concept drift detection method is required."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "wuX6eItFHt", "forum": "05PqjBzN6S", "replyto": "05PqjBzN6S", "signatures": ["ICLR.cc/2026/Conference/Submission23926/Reviewer_JXVC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23926/Reviewer_JXVC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23926/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992897401, "cdate": 1761992897401, "tmdate": 1762942859468, "mdate": 1762942859468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}