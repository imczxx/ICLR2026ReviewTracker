{"id": "PUuAzsAprm", "number": 10557, "cdate": 1758175508469, "mdate": 1759897643626, "content": {"title": "ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition", "abstract": "LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks.\nHowever, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. \nConversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings.\nWe propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model’s capacity intact.\nWe evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46\\% fewer trainable parameters than the standard LoRA.\nIn multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54\\% of the trainable parameters demanded by the conventional LoRA.", "tldr": "", "keywords": ["Parameter-Efficient Fine-Tuning", "LoRA", "Multi Task"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8104542c183ea9f73f2743a8cfd5e048b4ce80e4.pdf", "supplementary_material": "/attachment/06558bc06784244ee033cb162e18a3248c9c5589.zip"}, "replies": [{"content": {"summary": {"value": "ID-LoRA is a novel parameter-efficient fine-tuning (PEFT) method for LLMs that reduces trainable parameters by reusing clustered weight groups to form multiple low-rank components with a single shared trainable matrix. It outperforms full fine-tuning and existing PEFT methods (LoRA, DoRA, HydraLoRA) across diverse benchmarks—including reasoning, code generation, and multi-task settings—while using up to 46% fewer trainable parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. ID-LoRA reuses frozen pretrained weights as low-rank bases and trains only a single shared matrix, removing extra parameters.\n2. Clustered decomposition provides tighter error bounds and enhanced pivot robustness for reliable multi-task performance.\n3. Achieves LoRA-level or better accuracy while reducing trainable parameters by up to 46%."}, "weaknesses": {"value": "1. ID-LoRA introduces some computational and memory overhead, which may compromise the inference-time advantages of standard LoRA.\n2. The authors could provide a comparison of fine-tuning costs between ID-LoRA and other LoRA variants. While it uses fewer trainable parameters, this does not necessarily imply reduced training cost or improved training efficiency.\n3. The authors could provide more details on how ID-LoRA operates during inference which would be helpful."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Oh6EyAeQ3a", "forum": "PUuAzsAprm", "replyto": "PUuAzsAprm", "signatures": ["ICLR.cc/2026/Conference/Submission10557/Reviewer_jCmp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10557/Reviewer_jCmp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629649804, "cdate": 1761629649804, "tmdate": 1762921832196, "mdate": 1762921832196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a PEFT framework, ID-LoRA, that cleverly reuses parts of the pretrained model's weights to reduce the number of trainable parameters. The core idea of clustering frozen weights to form task-specific bases is well-motivated and is a significant departure from conventional LoRA-based methods (see Figure 1). The empirical results are strong, demonstrating superior parameter efficiency and performance across a diverse set of single-task and multi-task benchmarks (see Tables 1 & 2). However, the experimental evaluation, while broad, lacks depth in several areas, such as hyperparameter sensitivity, the impact of specific design choices, and analysis on larger-scale models. Furthermore, the clarity of certain methodological details and the connection between the theoretical analysis and practical implementation could be improved (see Section 4.2, Section 4.3)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel Parameter Efficiency**  \n   The core idea of reusing clustered pretrained parameters as frozen bases is innovative. It reduces trainable parameters by up to 46% compared to standard LoRA while maintaining competitive performance.\n\n2. **Solid Theoretical Grounding**  \n   The paper provides rigorous theoretical guarantees (Theorems 1-2) showing tighter error bounds for cluster-based decomposition compared to global low-rank adaptation.\n\n3. **Comprehensive Multi-Task Evaluation**  \n   Extensive experiments across 5 diverse benchmarks (Math, Code, MMLU, CommonsenseQA, Safety) demonstrate consistent advantages over LoRA variants in multi-task scenarios.\n\n4. **Practical Inference Efficiency**  \n   ID-LoRA achieves minimal inference overhead (only 0.5% latency increase) compared to methods like MoELoRA (+61.6%) and HydraLoRA (+36.5%), while reducing extra memory usage by 45%."}, "weaknesses": {"value": "1. **Ambiguity in Mathematical Formulation**\n\nThe description of the Rank Boosting (RB) mechanism is unclear, and the associated formula exhibits dimensional inconsistencies. Evidence from Section 4.2, Equation (4) shows the term `fc([x1_i, x2_i]B)`, where `x1_i` and `x2_i` are partitions of a vector of size `r`, resulting in a concatenated vector of size `r`. However, matrix `B` is defined as having dimensions `R^(r/2) x (d/2)`, making standard matrix-vector multiplication impossible. This ambiguity hinders understanding and reproducibility of the core method, undermining its practicality.\n\n2. **Limited Experimental Scope and Rigor**\n\nThe experiments, while broad, lack depth in validation: model scale is confined to 7B/8B parameters without extension to larger models (e.g., 8B+), failing to demonstrate parameter efficiency in critical scenarios; results are reported as single-run scores without error bars or confidence intervals from multiple runs, raising doubts about statistical significance; training time analysis is omitted, neglecting quantification of k-means clustering overhead; clustering choices lack justification, with no comparison to alternative algorithms (e.g., cosine similarity) or validation of distance metrics; hyperparameters like minimum cluster size are unspecified, and pivot selection strategies (e.g., centroid-based) are not ablated against alternatives; ablation on cluster count `k` is only on 3B models with marginal differences, insufficient for robust conclusions; RB motivation is vaguely described (e.g., \"balanced interactions\"), lacking technical explanation. These limitations reduce the robustness and persuasiveness of the experimental findings.\n\n3. **Disconnect Between Theory and Practice**\n\nThe theoretical analysis, based on CUR decomposition and pivot selection (e.g., Assumption 3 and Theorem 2), is loosely connected to the method's implementation, which uses k-means clustering and centroid-distance-based row selection. Evidence indicates that Theorem 2 discusses \"cluster-local pivots\" stability, but the algorithm does not select pivots in a CUR-typical manner, making the theoretical guarantees less directly applicable. This gap weakens the claim that the method's success stems from its theoretical foundations.\n\n4. **Missing Qualitative Analysis**\n\nThe paper lacks qualitative insights into the method's behavior, such as analyzing whether clusters correspond to specific linguistic or reasoning capabilities, or how the router parameters α activate clusters for different tasks or inputs. This omission limits opportunities to understand internal mechanisms beyond quantitative metrics, reducing the depth of methodological validation."}, "questions": {"value": "1. **Rank Boosting Scaling Factor**  \n   The RB method currently uses a division factor of 2. Could it theoretically be extended to /4 or even extreme cases where B becomes d×1 for greater parameter reduction? Ablation studies should explore the impact of different scaling factors on performance to determine the optimal value.\n\n2. **Router Mechanism Analysis**  \n   The router is essentially a learnable alpha parameter. Could fixed alpha values be used? Statistical analysis of alpha values across experiments should be conducted—what are their average values? Are there observable patterns?\n\n3. **Mergeability and Inference Latency**  \n   Due to the alpha-based router, can the adapter be merged with pretrained weights during inference? If not, does this significantly increase inference latency?\n\n4. **Comparison with LoRA-FA**  \n   LoRA-FA [5] also trains only B while keeping A fixed (Gaussian initialization). How does ID-LoRA compare comprehensively with LoRA-FA to further validate PMRC's effectiveness?\n\n5. **Baseline Completeness**  \n   This work aims to reduce trainable parameters through sharing, but relevant baselines are missing (e.g., VeRA [1], Tied-LoRA [2], BS-LoRA [3], VB-LoRA [4]). Related work should be discussed, and experiments should include comparisons with other parameter-sharing methods.\n\n6. **Figure 1(c) Misleading Representation**  \n   The x-axis in Figure 1(c) increases non-linearly, exaggerating parameter savings exponentially. For fair comparison, ranks should grow linearly—the current plot overstates ID-LoRA’s gains. Since savings are fixed (~1/8), a linear plot may be more appropriate.\n\n7. **Diagram Consistency**  \n   Figure 1 uses K=3 clusters, but experiments use K=4. Shouldn’t these be consistent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uetALsDYuI", "forum": "PUuAzsAprm", "replyto": "PUuAzsAprm", "signatures": ["ICLR.cc/2026/Conference/Submission10557/Reviewer_7pCV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10557/Reviewer_7pCV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894932224, "cdate": 1761894932224, "tmdate": 1762921831863, "mdate": 1762921831863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the author proposed the ID-LoRA method, which aims to reduce the trainable parameters during efficient fine-tuning. On five diverse benchmarks, the author validates the effectiveness of ID-LoRA on both single- and multi-task scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tSection 4.3 provides a theoretical foundation for the proposed method, which is interesting.\n-\tWith fewer trainable parameters, ID-LoRA achieves better results in both single-task and multi-task scenarios."}, "weaknesses": {"value": "-\tThe reasoning behind using MID for initializing matrices $A_i$ is unclear. Please compare/justify against alternatives like SVD or random initialization.\n-\tThe writing is messy and unclear.\n    -    Numerous abbreviations (e.g., PMRC, RB, Pivot) are used without definition, hindering comprehension. And the Equation numbering is inconsistent; some have equation numbers, some do not.\n    - The terms \"good pivot\" and \"bad pivot\" are undefined, making Assumption 3 resemble a definition rather than a theoretical premise. And the max/min inequality relationship needs explanation.\n    - In Assumption 2, why is it $A_iB^T$ rather than $BA_i$\n-\tNeed additional ablation studies.\n    - Main results are based on Llama-3-8B, but ablation studies use Llama-3.2-3B. This is weird. Ablations should be replicated on Llama-3-8B for consistency.\n    -  In Table 3, besides PMRC/SS/RS, include comparisons with non-MID initializations (e.g., random or SVD).\n    - In Table 3, why does \"w/o RB\" perform worse than \"w/ RB\"? This is counterintuitive since \"w/ RB\" has more parameters, and \"w/o RB\" (equivalent to duplicated B matrices) is a subset of the \"w/ RB\" solution.\n    - Need to compare with other efficient LoRA methods (e.g., LoRA-XS, VeRA)\n-\tWhy do some benchmark metrics in Figure 4 degrade as trainable parameters increase?\n-\tThere is a gap between the theory in Section 4.3 and ID-LoRA's practice. The theory approximates optimal matrices $W_i$, but ID-LoRA decomposes the pretrained weight $W$. This implicitly assumes $W$ contains $W_i$, which is questionable: How can pretrained weights encompass downstream task-specific optima? And why can MID approximate this?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eCxj2NhOIz", "forum": "PUuAzsAprm", "replyto": "PUuAzsAprm", "signatures": ["ICLR.cc/2026/Conference/Submission10557/Reviewer_ztga"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10557/Reviewer_ztga"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142736997, "cdate": 1762142736997, "tmdate": 1762921831518, "mdate": 1762921831518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ID-LoRA, a new PEFT framework designed to improve upon LoRA. The approach, inspired by Matrix Interpolative Decomposition, allows the model to achieve a high effective rank while reducing the number of trainable parameters. The authors evaluate ID-LoRA on a diverse set of benchmarks (math, code, MMLU, etc.) and show that it outperforms LoRA and its variants (like DoRA) while using up to 46% fewer trainable parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The rank-vs-parameter trade-off is a well-known limitation of LoRA.\n- The method demonstrates clear and consistent performance gains.\n- The authors provide a theoretical analysis (Theorems 1 & 2) to back their method."}, "weaknesses": {"value": "- The method is inherently more complex to implement than vanilla LoRA. The cost of the pre-processing step is not discussed.\n- The paper does not compare ID-LoRA against LoRI in either single-task or multi-task settings, even though several of the result tables are structurally similar to those in LoRI. Since LoRI is more parameter-efficient than ID-LoRA, including this comparison would provide a clearer picture of the trade-offs.\n- The method introduces a new and important hyperparameter, $k$ (the number of clusters). The ablation in the appendix (Table 8) shows performance is sensitive to this. \n- The authors rightly admit that the performance gains on mathematical reasoning (GSM8K) are \"less pronounced\". This suggests ID-LoRA may not be a universal improvement for all task types.\n- The RB component feels somewhat separate from interpolative decomposition. \n- The experiments are conducted on LLaMA-3-8B and Mistral-7B, which are models from 2023 and 2024. Evaluating the method on more current base models would be necessary to demonstrate its relevance for a 2026 conference."}, "questions": {"value": "- What is the practical, one-time cost (in terms of time and compute) of running k-means clustering?\n- The parameter savings seem to come from two places: (1) reusing W, sharing B and (2) the RB technique. Could you provide a clearer parameter breakdown in Table 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Du6t0eHgji", "forum": "PUuAzsAprm", "replyto": "PUuAzsAprm", "signatures": ["ICLR.cc/2026/Conference/Submission10557/Reviewer_Gd5V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10557/Reviewer_Gd5V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762169249725, "cdate": 1762169249725, "tmdate": 1762921831128, "mdate": 1762921831128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}