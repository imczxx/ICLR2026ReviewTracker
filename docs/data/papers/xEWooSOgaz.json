{"id": "xEWooSOgaz", "number": 8794, "cdate": 1758098426070, "mdate": 1759897763520, "content": {"title": "A Noise is Worth Diffusion Guidance", "abstract": "Diffusion models have demonstrated remarkable image generation capabilities, but their performance heavily relies on classifier-free guidance (CFG). While CFG significantly enhances image quality, evaluating both conditional and unconditional models at every denoising step leads to substantial computational overhead. Existing approaches mitigate this cost through distillation, training a student network to learn the guided predictions. In contrast, we take an orthogonal approach by refining the \\textit{initial Gaussian noise}, a critical yet under-explored factor in the diffusion-based generation pipelines. Recent studies have explored noise optimization for specific tasks such as layout-conditioned generation and human preference alignment. However, whether refined noise alone can enable guidance-free high-quality image generation remains an open question. We introduce a noise refinement framework where a refining network is trained to minimize the difference between images generated by unguided sampling from the refined noise and those produced by guided sampling from the input Gaussian noise. Our method achieves CFG-like quality without modifying the diffusion model, preserving its prior knowledge and compatibility with techniques like DreamBooth LoRA. Additionally, the learned refining network generalizes across domains without retraining and seamlessly integrates with existing distilled models, further improving sample quality. Beyond its practical benefits, we provide an in-depth analysis of refined noise, offering insights into its role in the denoising process and its interaction with guidance. Our findings suggest that structured noise initialization is key to efficient and high-fidelity image synthesis. Code and weights will be publicly released.", "tldr": "To improve initial noise for diffusion inference, we train a noise-refining network that injects guidance signals (e.g., classifier-free guidance) into noise", "keywords": ["Classifier Free Guidance", "Diffusion Guidance", "Guidance Distillation", "Text to Image Synthesis"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/94e728d7194b4c2682bc6f66f245dab6dbf59170.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces NoiseRefine, a new method to eliminate the computational overhead of classifier-free guidance (CFG) during diffusion model inference. Instead of applying guidance at each sampling step, NoiseRefine employs a refining network to perform a one-time transformation on the initial noise vector. This network, created by fine-tuning a pre-trained diffusion model with LoRA, maps random noise to a \"guidance noise\". By using this refined noise as the starting point, an off-the-shelf diffusion model can generate high-quality, guided images without the need for iterative guidance, effectively halving the inference cost."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a new approach to guidance distillation. The core idea of refining the initial noise vector instead of performing iterative guidance is interesting. The work is well-motivated by the clear and practical goal of reducing the inference cost of classifier-free guidance, a significant bottleneck in modern diffusion models."}, "weaknesses": {"value": "Though the idea of adopting noise refinement in distilling guidance is interesting, the manuscript in its current form has several major issues related to contradictory motivation, methodological clarity, and experimental evaluation that weaken its conclusions. My primary concerns are detailed below.\n\n### Contradictory Motivation and Methodological Justification.  \nThe introduction motivates the method by showing that \"guidance noise\" (inverted from a high-quality guided image) can be successfully mapped back to a similar high-quality image by the diffusion model (Figure 2). This suggests that such a guidance noise target is stable and learnable. However, in Section 3.2, the authors argue against directly learning this mapping. They claim that inversion errors make this approach infeasible, leading to low-quality reconstructions (Figure 4). This directly contradicts the initial motivation. If the inversion error is as severe as Figure 4 suggests, the premise illustrated in Figure 2, which underpins the entire approach, is undermined. Could the authors please clarify this discrepancy?  \n\n### Ambiguity in the Proposed Loss Function and Training.   \nThe discussion around the loss functions ($L_{denoise}$ v.s. $L_{MSD}$) is confusing and raises questions.  \n**a. Training Cost**: The paper claims that training with the full-gradient loss $L_{denoise}$ is significantly more expensive than with the proposed score-matching approximation $L_{MSD}$. Could the authors elaborate on this? Since the diffusion model's weights are frozen during the refiner's training, the primary cost should be the forward/backward passes of the refining network itself. It's not immediately clear why the diffusion model incurs expensive training cost.  \n**b. Approximation Outperforming the Target**: In Figure 6, the proposed $L_{MSD}$ loss not only converges to a lower value but also produces visually superior results compared to the full-gradient objective. This is counter-intuitive. If $L_{MSD}$ is a theoretical approximation, why does it empirically outperform the objective it is meant to approximate? This suggests either the approximation is not just an approximation but introduces a beneficial regularization, or there is an issue with the implementation or interpretation of the $L_{denoise}$ objective.  \n\n### Experimental Evaluation Weaknesses\n1. Missing cost analysis in the main paper.  \nThe paper is motivated by reducing the high cost of guidance sampling and claims the commonly adopted guidance distillation is computationally expensive, yet it fails to demonstrate the efficiency gain or provide a quantitative comparison of its own training costs against related methods. Please provide a direct comparison of the training cost (e.g., in GPU hours) of NoiseRefine against the key guidance distillation baselines mentioned.  The inference process is not clearly described. Is the noise refined once before the sampling process begins, or at every step? The primary contribution is claimed to be efficiency, so a detailed breakdown of the inference speed-up (e.g., wall-clock time, FLOPs) compared to the standard CFG baseline is essential.  \n2.  Under-reported and puzzling baseline results.  \n**a. CFG baseline**: The CFG performance for SiT-XL/2 in Table 2 is substantially worse than what is reported in the original paper. This weakens the claimed improvements of NoiseRefine.  \n**b. Approximation surpassing full method**: It is highly unexpected that NoiseRefine, which is trained to approximate CFG, would achieve better FID scores than full CFG itself. This result is counter-intuitive and may point to a suboptimal implementation of the CFG baseline or an unfair comparison setup.  \n3. Incomplete baselines and flawed user study.  \n**a. Missing baselines**: For conditional image generation, the comparison is incomplete. Methods like auto-guidance, which also aim to reduce guidance costs with a lightweight model, are highly relevant and should be included as a baseline.\n**b. User study design**: The user study is flawed. It compares images generated without any guidance to images generated with NoiseRefine. A meaningful comparison must be between the standard guidance method (CFG) and the proposed method (NoiseRefine) to evaluate if the approximation preserves perceptual quality.\n4. Questionable application and generalizability.  \n**a. Experiments on SD-Turbo**: The experiments on SD-Turbo are conceptually questionable. NoiseRefine is designed to alleviate the cost of iterative guidance. In a one-step model like SD-Turbo, there is no iterative cost to reduce. Instead, NoiseRefine adds the overhead of an extra network pass, making it inherently less efficient than the native one-step model. Furthermore, could the authors explain the surprising result in Table 4 where 2-step sampling yields a worse FID than 1-step sampling?  \n**b. Generalizability**: The claim of superior generalizability in Figure 8 is supported only by limited qualitative examples. To make this claim convincing, quantitative metrics evaluating performance on out-of-distribution prompts or styles are needed.  \n\nWhile the core idea of refining initial noise is interesting, the manuscript requires a major revision. Addressing the concerns regarding baseline performance, cost analysis, and experimental design is critical to substantiating the contributions of this work."}, "questions": {"value": "All my concerns and questions are listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "klkwlKfnV4", "forum": "xEWooSOgaz", "replyto": "xEWooSOgaz", "signatures": ["ICLR.cc/2026/Conference/Submission8794/Reviewer_AzoV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8794/Reviewer_AzoV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761379355885, "cdate": 1761379355885, "tmdate": 1762920567234, "mdate": 1762920567234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes _NoiseRefine_, a guidance distillation method that learns to refine the initial Gaussian noise into one that can generate high-quality images without classifier-free guidance. When performing inversion _without_ guidance on an image generated _with_ guidance, the resulting \"inversion noise\" is generally non-Gaussian, yet it can still recreate the image without guidance. Based on this observation, the authors propose training a lightweight refinement network—a LoRA of the pretrained diffusion model—to learn the mapping between the original Gaussian noise and the inversion noise. This effectively distills guidance into the noise.\n\nTo achieve this, the method introduces three main contributions:\n- **Image-space supervision**: Training is supervised in image space, which is shown to be more robust to inversion errors.\n- **Residual connection**: Instead of learning the full mapping, the refinement network is parameterized to learn the residual.\n- **Backpropagation**: Similar to score distillation sampling (SDS), the gradient is not propagated through the network during training. This has been shown to lead to faster convergence and higher-quality results.\n\nTo validate the approach, the paper ablates the method on several conditional image diffusion models and thoroughly analyzes what the refinement network learns, which seems to be primarily adding low-frequency structures to the noise. Additionally, the authors compare their method with other guidance distillation approaches and argue that distilling guidance into the noise is more efficient and generalizes better to other fine-tuned models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles an important topic in the diffusion community, which is related to guided generation. The main strength of this paper is in the idea it proposes. To the best of my knowledge, this is the first work to look into guidance distillation into the noise input, which seems like a simple yet brilliant idea. \n\nThe execution of the method is also in itself very well done: the authors validate each contribution that make the method work using small experiments. The manuscript is easy to read and the method clearly explained. \n\nFurthermore, the appendix is very exhaustive and provides many additional details and experiments that makes the submission very strong."}, "weaknesses": {"value": "No major weaknesses, the appendix addresses many of my concerns already."}, "questions": {"value": "Again, not many questions, as the exhaustive appendix addressed most of my interrogations already.\n\n- The user study is purely done against no guidance in the main paper, and against with guidance in the appendix. Have the authors tried to compare the method with other guidance distillation methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rVHBZLIIYT", "forum": "xEWooSOgaz", "replyto": "xEWooSOgaz", "signatures": ["ICLR.cc/2026/Conference/Submission8794/Reviewer_a4W6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8794/Reviewer_a4W6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761594998978, "cdate": 1761594998978, "tmdate": 1762920566866, "mdate": 1762920566866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a novel and orthogonal approach that \"distills\" the effect of guidance into the initial noise vector rather than into the diffusion model itself. The core idea is to train a lightweight \"noise refining network\" (implemented as a parameter-efficient LoRA on the diffusion model's UNet) that maps a standard Gaussian noise vector ($x_T$) to a \"refined\" noise vector ($\\hat{x}_T$). Specifically, this paper propose Multistep Score Distillation (MSD), a technique that stops the gradient during backpropagation through the denoising network at each step."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This idea is novel and plug-and-play.\n2. It's orthogonal to other acceleration methods."}, "weaknesses": {"value": "1. I'd like to know if optimizing Eq4 is difficult and costly, since it requires go through the network many times.\n2. The result after refinement was still not good enough, significantly worse than the result generated by CFG.\n3. For me, it's difficult to understand why noise-to-noise mapping has generalization properties. The paper lacks discussion and analysis on this aspect. For example, are the generated results of training samples has better quality than validation samples? For refined noise, can it be combined with CFG to obtain better results? If so, how does it compare to the results obtained using CFG with initial noise?"}, "questions": {"value": "Can we directly learn the mapping from the initial XT to X0_guide using a bridge model initialized from the original diffusion network? For example, DDBM or I2ISB. What are the advantages of MSD compared to this approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cYZvcAh3Qw", "forum": "xEWooSOgaz", "replyto": "xEWooSOgaz", "signatures": ["ICLR.cc/2026/Conference/Submission8794/Reviewer_ScWm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8794/Reviewer_ScWm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816482017, "cdate": 1761816482017, "tmdate": 1762920566554, "mdate": 1762920566554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to address the reliance of Diffusion models on sampling guidance (e.g., CFG), which effectively doubles inference cost. It proposes to learn a (external) noise refining network $g_\\phi$ to modify the initial Gaussian noise $x_T$, for the correspondence with the effects of CFG. It also uses methods including training in image space for better performance and using Multistep Score Distillation (MSD) for efficient training, with rounded analysis and sound experiments.\n\nTotally, this work is simple in the method but interesting in the perspective. The core idea (noise refinement for guidance) is actually like a special (but clever) variant of guidance distillation approaches, and some comparisons are unfair or not comprehensive. But it does give an insightful perspective from the initial noise, with convincing (at least for me) motivation, not bad results, and the potential for wider application in various scenarios. \n\nSome questions need to be clarified and solved, but overall, this paper is well-written, easy to follow, rich in content, and deserves a positive rating."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Totally, it is well-written and easy to follow.\n2. The overall motivation is clear. Optimizing the guidance to accelerate the inference of Diffusion models is useful.\n3. The perspective from the initial noise is quite interesting. The idea of distilling guidance information into the initial noise, rather than into the denoising network itself, is also insightful and provides a fresh perspective on tackling the guidance overhead problem (with potential and compatibility for wider application in various scenarios).\n4. Sound experiments and rounded analysis make the results convincing. The method significantly outperforms the unguided baseline, with an acceptable gap to CFG."}, "weaknesses": {"value": "1. A Special Form of Distillation: While the paper claims their method is \"orthogonal\" to guidance distillation, it is more accurately a clever variant rather than a new paradigm. The framework fits the knowledge distillation paradigm, the difference from guidance distillation is the locus of learning (the noise refiner $g_\\phi$ v.s. the denoising network). This positioning overstates the fundamental novelty, more discussion/comparison/clarification should be made.\n2. Loss of Critical Controllability: The method encodes the effect of a *fixed* guidance scale (e.g., CFG scale 7.0) into the network, thereby losing the native, zero-cost flexibility of the CFG scale $w$ at inference time. The workarounds proposed (scaling the residual or conditioning the training) are either heuristics with unverified behavior or add complexity to the training process, undermining the method's simplicity. Especially, the presented results are not intuitive to understand (Fig. 13; Fig. 23): Changing $w$ seems to have no effect on the \"richness\" of the image at all, but merely affects the degree of distortion, failing to reach the original function of $w$ in the CFG.\n3. The Theoretical Soundness of MSD: The entire framework's viability hinges on Multistep Score Distillation (MSD) for gradient approximation. However, its theoretical justification (Proposition 2) relies on a strong assumption ($\\partial\\epsilon_\\theta^{(t)} / \\partial x_t \\propto I$). This assumption is empirically/visually supported in the appendix but lacks rigorous quantitative evidence.\n4. Additional Complexity and Overhead: The training process requires running the expensive guided sampling to generate the teacher signal $x_0^{Guide}$ for every training sample. As for the inference overhead, unlike standard LoRA (which can be merged into the base model's weights to eliminate inference overhead), the noise refining network $g_\\phi$ implemented with LoRA cannot be merged, meaning an extra model forward pass is also always required at inference time."}, "questions": {"value": "1. The Study/Evaluation of Controllability: The NoiseRefine is used to replace the role of CFG, but the study of the controllability seems not to be sufficient. Can the details of the training-based approach of controllability be further illustrated? Curious about how the guidance scale $w$ is incorporated as an additional input for training, and how to guarantee the range of $w$ (e.g., show the num of samples of different $w$). Also, have you done contrast experiments of different fixed $w$ of CFG during the training of NoiseRefine, and quantitatively compared the FID/CLIP score?\n2. Cost and Performance Comparison with Guidance Distillation: Given that training NoiseRefine also requires running guided sampling, how does its total training budget (e.g., in GPU-hours) compare directly to training a standard guidance distillation model [1] to achieve similar performance?\n3. The Architectural Choice for $g_\\phi$: Is implementing $g_\\phi$ as a LoRA attached to $\\epsilon_\\theta$ a limitation? Have you experimented with a completely separate, smaller network for $g_\\phi$? How would this impact performance and the leveraging of \"pretrained knowledge\"?\n4. The Jacobian Approximation in MSD: The proof of Proposition 2 critically depends on the assumption that $\\partial\\epsilon_\\theta^{(t)} / \\partial x_t \\propto I$. Does this assumption still hold for Transformer architectures (e.g., DiT, SiT) where attention mechanisms might induce more complex, long-range Jacobian structures? Could you provide a quantitative error bound or an ablation study (e.g., detaching gradients for only a subset of steps) to more rigorously validate this approximation? This theoretical/experimental verification may not be so easy to do, so it is OK as the current results are great, just a comment. But if you can provide an intuitive understanding, or if the validity of this Jacobian Approximation can be verified, it would be really helpful to the research community.\n\n\n[1] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14297–14306, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OmKzAqm3AQ", "forum": "xEWooSOgaz", "replyto": "xEWooSOgaz", "signatures": ["ICLR.cc/2026/Conference/Submission8794/Reviewer_86wZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8794/Reviewer_86wZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8794/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850753778, "cdate": 1761850753778, "tmdate": 1762920566240, "mdate": 1762920566240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}