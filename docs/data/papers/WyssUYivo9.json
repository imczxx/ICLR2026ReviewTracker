{"id": "WyssUYivo9", "number": 19715, "cdate": 1758298653721, "mdate": 1759897023656, "content": {"title": "XPro-Design: Rational Protein Engineering Framework Using Explainable AI", "abstract": "Protein engineering seeks to rationally tailor proteins to achieve specific structural and functional objectives. These objectives encompass enhancing catalytic efficiency, modifying substrate specificity, improving binding affinity, reducing immunogenicity, and increasing stability under adverse conditions. A major bottleneck is protein instability, as elevated temperatures often drive degradation and compromise activity. Developing thermostable proteins is therefore a key objective in engineering efforts. Here, we present XPro-Design, an explainable AI driven framework for protein optimization that integrates amino acid-level explanations of functional impact into generative modeling. Our method captures epistatic interactions and the mutational landscape by training a low-rank matrix, which biases the generative model toward high-scoring regions of sequence space. This enables targeted generation of candidate variants optimized for thermostability, while remaining extensible to other objectives. XPro-Design further uses distribution tempering and annealing to effectively balance exploration vs exploitation without compromising on structural integrity. We demonstrate rational, causality driven design of protein variants with melting temperatures nearly 2x that of their wild-type counterparts, while preserving binding pocket integrity and domain architecture. Moreover, engineered variants show up to 38% lower folding free energy relative to wild-type, indicating significantly enhanced thermodynamic stability. XPro-Design establishes a generalizable strategy for explainable and controllable protein design, enabling multi-objective optimization beyond thermostability.", "tldr": "XPro-Design is a novel explainable AI framework towards structure aware protein engineering", "keywords": ["protein", "engineering", "thermostability", "biotech", "enzyme", "optimization"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4560827f2275553cd7455ada2bbb5cec9c11949d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors claim to contribute:\n- a method to shift a pre-trained generative model toward some function of interest with a low-rank (?) per-position scoring matrix \n- designed proteins for increased thermostability; method is extensible to other objectives\n- tries to address bias (present in most pre-trained models) toward mesophilic proteins by increasing the default temperature of pre-trained models, because they want to design proteins for extreme temperatures."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Results of applying the PSSM to pre-trained inverse folding models leads to an appreciable increase in the distribution mean PTM (Fig. 2).\n- Refolding results (Table 2) suggest that protein sequences generated by their procedure fold near the target structure (though not under 2 Angstroms) and that AlphaFold is pretty confident about them.\n- Authors try to validate their method in a variety of ways, including some packing analyses.\n- Doesn’t seem to require training anything, just combines a pre-trained generative model with a PSSM computed from $f(x)$."}, "weaknesses": {"value": "- I’m confused as to why the authors primarily used interpretable gradients since for protein design, the properties one tries to maximize are not generally differentiable because they’re defined as $f(x)$ with respect to discrete $x$ (protein sequence). One could decide to take gradients of discrete functions if they exist, as they do for neural networks [1], but that should be explained and justified. Moreover, it’s odd that in the explicitly non-differentiable predictor case, they use correlations. One might more naturally consider fitting an epistatic linear model up to order two (position-wise and pairs). An explanation of how integrated gradients and correlations used correspond to rigorous notions of epistasis [2] is lacking.\n- Authors explain how to get PSSM given integrated gradients but not for correlations. Also, the PSSM seems to only use the single site IG, and not any pairs — so the claim that the method accounts for epistasis seems inappropriate.\n- Authors say PSSM is learned, but I don’t see anywhere describing learning or training. They also say that it’s low-rank, but I don’t see anything about that either.\n- It seems odd to use AlphaFold-based refolding metrics (Table 2) if they’re trying to design proteins for extreme conditions; should AlphaFold do a good job on these out-of-distribution proteins?\n- Results for only one protein are shown in the main text, making it difficult to evaluate whether this works generally. Their claims about their method seem too strong for the evidence presented. It’s also not evaluated against any of the methods mentioned in the intro, just the base pre-trained models.\n- It’s difficult to understand novelty of method because the explanation of the method isn’t well situated within existing methods. It seems to sort of come out of nowhere. The idea seems really simple, which isn’t necessarily a bad thing, but makes me wonder if some other paper has already done something similar, and if this method really achieves SOTA performance for taking a pre-trained generative model and shifting its distribution according to some desired property $f(x)$. There are many generic methods for doing this like fine-tuning, RL post-training, SMC, etc. that aren’t mentioned.\n- Organization and clarity of the writing is significantly lacking:\n    - No separate related works\n    - Figure 1 doesn’t have a caption and is pretty complicated. Fonts in Figure 2 are oddly sized, and the dashed lines aren’t explained in the legend or in the caption. In general, captions aren’t very descriptive or helpful.\n    - Grammar hasn’t been thoroughly checked throughout and there’s missing capitalization (e.g., Table 1), typos, etc.\n    - The methodology goes straight into experimental details without providing any real roadmap\n    - Writing is overly specific in places (e.g., why is a particular length, $L=320$, specified for the first equation, which one would expect to be generic?)\n    - Equations aren’t written carefully (e.g., in Eq.2, $\\tilde{p}_{i,l}$ should take $T$ as an argument)\n    - Text and figures aren’t consistent; e.g., Sec. 3.3 says that Table 2 shows PAE but it does not.\n\nI recommend rejecting this paper for two primary reasons — their idea might be good, but there isn’t sufficient evidence of it working beyond one test protein, and the paper overall is not well written. I think substantial revision effort is needed to make this paper understandable.\n\nA few small pieces of advice: It would be clearer to the reader if you had a separate related works section, as opposed to interspersing it throughout your introduction. Correspondingly, it would also be easier to understand your paper if the introduction primarily focused on the motivation for your method and its big picture idea with intuition. As it stands, the introduction is a bit scattered and claims that your method will do many things without making it clear to the reader what the (at most 3) critical contributions you make are.\nThere’s also a red squiggly typo line in your Figure 1. An algorithm box, or at least a clear outline somewhere, would help the reader to understand what exactly your general method is, separate from the specific analyses that you do throughout. And please, please, structure your paper (and especially your methods section) clearly, as if you're trying to teach someone what you did. \n\n[1] Grathwohl, Will, et al. \"Oops i took a gradient: Scalable sampling for discrete distributions.\" International Conference on Machine Learning. PMLR, 2021.\n[2] Poelwijk, Frank J., Vinod Krishna, and Rama Ranganathan. \"The context-dependence of mutations: a linkage of formalisms.\" _PLoS computational biology_ 12.6 (2016): e1004771."}, "questions": {"value": "The details of the method are overall pretty unclear, I'd like clarity on that primarily. More explanation of how your method is different from the closest related works would help to position it in the field. Comparison with methods other than just pre-trained model would also be informative so readers can understand if your method is achieving SOTA, or how close if not. Comparisons on more proteins or properties other than thermostability would be compelling if this method really is applicable generally."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YEPpAwh4s5", "forum": "WyssUYivo9", "replyto": "WyssUYivo9", "signatures": ["ICLR.cc/2026/Conference/Submission19715/Reviewer_feGP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19715/Reviewer_feGP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761785796118, "cdate": 1761785796118, "tmdate": 1762931552517, "mdate": 1762931552517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes the XPro-Design framework, which combines generative inverse folding models with explainable AI to rationally engineer proteins for improved thermostability and other functional objectives. It begins by sampling amino acid sequences from backbone-conditioned models such as ProteinMPNN or HyperMPNN and applies temperature tempering to broaden sequence diversity. Each generated sequence is then evaluated using differentiable thermostability predictors, where Integrated Gradients quantify the residue-level contributions to melting temperature. These attribution scores guide iterative updates of a position-specific scoring matrix (PSSM), biasing future sampling toward stabilizing mutations while maintaining structural fidelity. Through this explainable optimization process, XPro-Design efficiently explores the mutational landscape, refines favorable regions of sequence space, and produces protein variants with enhanced folding stability and preserved functional domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1 Compared with previous generative models such as ProteinMPNN or HyperMPNN, XPro‑Design incorporates explainable AI methods like Integrated Gradients to identify the contribution of each residue to target properties. This not only improves optimization efficiency but also allows researchers to understand why specific mutations enhance stability, enabling truly rational design rather than black‑box generation.\n\n2 The method employs temperature‑controlled sampling and annealing to balance exploration and exploitation in sequence space. Instead of directly finetuning model parameters, XPro-Design updates a biasing matrix and PSSM iteratively, achieving distribution‑level optimization without catastrophic forgetting or overfitting.\n\n3 The framework demonstrates robust performance across different test templates (e.g., CalB and SOR), generating variants with significantly improved melting temperatures, free energy profiles, and packing entropies. Structural validation using Boltz‑2 and AlphaFold3 confirms correct folding, proving that XPro-Design can generalize effectively and support multi‑objective protein optimization tasks."}, "weaknesses": {"value": "1 XPro-Design employs low‑rank bias matrices and Integrated Gradients to guide explainable optimization. While this provides residue‑level interpretability, the framework relies entirely on gradient‑based signals from thermostability predictors, which may carry noise or bias from models such as TemBERTure or DeepSTABp. This could lead to unstable optimization directions. Please explain why you didn’t incorporate more robust attribution approaches (e.g., SHAP or causal inference) to reduce susceptibility to gradient noises, and add relative baselines.\n\n2 The study mainly compares results with ProteinMPNN and HyperMPNN, which can be a bit simple and make it hard to situate the model’s performance relative to broader state‑of‑the‑art design frameworks. Please include more baselines such as RFdiffusion, MapDiff, ADFLIP.\n\n3 The current metrics may not be enough for comprehensive evaluations. The author may expand the experimental validation dataset or integrate existing high‑throughput stability databases like FireProtDB; use broader biophysical measurements (e.g., unfolding rate, ΔH, or kinetic assays) to support multi‑dimensional validation; and introduce uncertainty quantification metrics (e.g., variance of attribution scores) to assess the reliability of explainability signals."}, "questions": {"value": "Same as weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "byiYCd5TJd", "forum": "WyssUYivo9", "replyto": "WyssUYivo9", "signatures": ["ICLR.cc/2026/Conference/Submission19715/Reviewer_5WzG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19715/Reviewer_5WzG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807933923, "cdate": 1761807933923, "tmdate": 1762931551865, "mdate": 1762931551865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose XPro-Design, a protein optimization framework that leverages pre-trained inverse folding methods and predictors to propose mutants with enhanced functionality. Additionally, Integrated Gradient is used to create the bias to temper the distributions used to choose positions and amino acids to mutate. Results for two proteins used as case studies show enhanced performance in different surrogate metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Using the feedback from IG to bias inverse folding methods is interesting.\n2. The authors validate the designs using various methods, ranging from traditional energy, predictors, structure predictors, and molecular dynamics.\n3. Results show that for the case studies analyzed (one in the main text, one in Appendix), most metrics are enhanced for the candidates generated by the proposed method."}, "weaknesses": {"value": "1. The methodology section is confusing, as it is hard to grasp how the framework in Fig. 1 can be implemented.\n2. Only two case studies are investigated, and the setting might be unfair to baseline methods.\n3. The writing has contradicting sentences. Additionally, the use of the term Explainable AI is arguable.\n4. Code is not available."}, "questions": {"value": "My initial recommendation is rejection due to the following reasons: (i) additional clarification is needed regarding the methodology, (ii) the writing has contradicting sentences throughout the manuscript, and (iii) the evaluation is limited and needs clarification regarding fairness.\n\nComments:\n\n1. Figure 1 shows a loop that is repeated N times, but there are no details about this crucial part to understand the methodology in the main text.\n2. In Table 1, there is a sequence recovery metric, but how many average mutations from the wild-type are for the candidates generated?\n3. Given the framework in Figure 1, it seems that the PSSM matrix is learned in an active learning setting (with in silico predictors substituting wet lab experiments), but the baselines are just inverse folding methods, which raises questions about the fairness of the evaluation.\n4. In Eq. 1 is P calculated by masking the residues individually and checking the probabilities given by ProteinMPNN and HyperMPNN?\n5. (lines 181-183) There are no mathematical equations or references that support this statement.\n6. How is Eq. 4 used by the proposed method?\n7. Which predictor in Section 2.2 is used? Which are differentiable and which are non-differentiable?\n8. In Table 2 is Boltz-2 or AF used? Compared to ProteinMPNN and HyperMPNN, the proposed method leads to the worst confidence metrics. Why do you think the proposed method is worse in these refoldability metrics?\n9. How is the ddG in Fig. 4 calculated?\n10. The conclusion has parts that contradict the characteristics of the proposed method. For example, “in a gradient free setting” and “while guiding optimization without task-specific predictors”. Additionally, the sentence in lines 483-485 does not reflect the current state in protein engineering and protein design research.\n\nMinor Comments (that did not impact the score):\n\n1. Many captions are missing a dot at the end. Also in line 199.\n2. Using the term Explainable AI in this context, where IG is used to bias a distribution, needs proper support. This application seems more related to the definition of Interpretable ML in Murdoch et al “Definitions, methods, and applications in interpretable machine learning”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dYu2eRxhMM", "forum": "WyssUYivo9", "replyto": "WyssUYivo9", "signatures": ["ICLR.cc/2026/Conference/Submission19715/Reviewer_Mbn2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19715/Reviewer_Mbn2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19715/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903215344, "cdate": 1761903215344, "tmdate": 1762931551112, "mdate": 1762931551112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}