{"id": "0JLUFJMo5p", "number": 25449, "cdate": 1758368192077, "mdate": 1759896720791, "content": {"title": "Dynamic Task-Embedded Reward Machines for \\\\ Adaptive Code Generation and Manipulation \\\\ in Reinforcement Learning", "abstract": "We introduce Dynamic Task-Embedded Reward Machine (DTERM), a new machine learning approach for reinforcement learning on tasks of code generation and code manipulation. Conventional reward models tend to be based on fixed weightings or manual tuning, which is not flexible enough for many different coding tasks, such as translation, completion and repair. To overcome that, DTERM dynamically modulates reward components using a hypernetwork-driven architecture, which can balance the task-aware configuration of syntactic correctness, semantic correctness, and computational efficiency. The framework combines three key modules, including a transformer-based task embedding generator, a modular reward decomposer, and a hypernetwork to generate context-dependent weights of sub-rewards.", "tldr": "", "keywords": ["Reinforcement Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa6de8f172967f9988c29abcc16091879272bcd0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The manuscript strongly resembles AI-generated content and may have been produced as an internal test for prospective AI researchers. If so, it suggests that the current state of such roles remains immature and requires further development."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I believe this paper was generated by AI. If not, please let me know."}, "weaknesses": {"value": "I believe this paper was generated by AI. If not, please let me know."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nKMdFIiiCf", "forum": "0JLUFJMo5p", "replyto": "0JLUFJMo5p", "signatures": ["ICLR.cc/2026/Conference/Submission25449/Reviewer_oG3A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25449/Reviewer_oG3A"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792676897, "cdate": 1761792676897, "tmdate": 1762943437868, "mdate": 1762943437868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dynamic Task-Embedded Reward Machines (DTERM), a novel framework for reinforcement learning (RL) in code generation and manipulation tasks. Unlike traditional reward models that rely on fixed or manually tuned weights, DTERM employs a hypernetwork-driven architecture to dynamically adjust the contributions of various reward components - such as syntactic correctness, semantic correctness, and computational efficiency - based on task embeddings. The framework integrates a transformer-based task embedding generator, a modular reward decomposer, and a hypernetwork to produce context-aware reward weightings. Experiments across multiple benchmarks (e.g., CodeXGLUE, APPS, DeepFix, HumanEval) demonstrate consistent improvements over static reward baselines and strong generalization to unseen tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The one potential strength of the work is the identification of a valid problem: static reward functions are indeed a limitation in RL for code generation. The idea of making them adaptive is a worthwhile direction to explore."}, "weaknesses": {"value": "The weaknesses are severe and fundamental.\n\nFatally Compromised Presentation: The numerous grammatical errors, incoherent sentences, and missing content (like Eq. 4 and Fig. 1) make the paper unreadable and un-reviewable in its current state. This alone warrants rejection.\n\nUnsupported Claims: All major claims regarding performance and generalization are made without the necessary statistical evidence or rigorous experimental design to support them.\n\nTechnical Debt: The model is complex (hypernetwork, task encoder, multiple reward modules, FiLM layers, prototype attention), yet the paper provides no analysis of computational cost, training stability, or sensitivity to hyperparameters."}, "questions": {"value": "The paper is riddled with grammatical errors and incomplete sentences (e.g., Sec 4.6, Sec 6). Can the authors provide a coherent, fully proofread version that accurately represents their work?\n\nWhere is Equation 4 and the detailed architecture diagram (Figure 1)? The current manuscript is incomplete without them.\n\nWhere are the results of statistical significance tests (e.g., p-values) for the performance improvements reported in Table 1? Can the authors prove their method's advantage is not due to variance?\n\nThe \"unseen tasks\" in the generalization experiment are not defined. What are these tasks, and how do they semantically differ from the training tasks? Please provide concrete examples and task-level success metrics, not just normalized reward.\n\nThe conclusion (Section 6) describes a completely different model (\"Dual Selfular-Acting Machine\"). Has the manuscript been compromised during submission? Please clarify this critical discrepancy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "xxVpR7yuLY", "forum": "0JLUFJMo5p", "replyto": "0JLUFJMo5p", "signatures": ["ICLR.cc/2026/Conference/Submission25449/Reviewer_NmKe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25449/Reviewer_NmKe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904156215, "cdate": 1761904156215, "tmdate": 1762943436532, "mdate": 1762943436532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DTERM, a framework for RL in code generation and manipulation tasks. DTERM combines transformer-based task embeddings, modular decomposition of reward components, and a hypernetwork that produces context-dependent weights over these components. Experiments across four prominent code-generation benchmarks show that DTERM outperforms static and manually tuned reward baselines, particularly in cross-task generalization and adaptability."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses a relevant problem in RL for code generation, proposing a principled yet straightforward framework for dynamic reward weighting."}, "weaknesses": {"value": "The paper lacks sufficient comparison with prior adaptive reward modeling work, making the novelty claims less convincing. The approach is heavily dependent on CodeBERT, with minimal analysis of robustness or generality. Moreover, there are noticeable writing and editing issues."}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7TmB1fIdjw", "forum": "0JLUFJMo5p", "replyto": "0JLUFJMo5p", "signatures": ["ICLR.cc/2026/Conference/Submission25449/Reviewer_p2jD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25449/Reviewer_p2jD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928365838, "cdate": 1761928365838, "tmdate": 1762943435552, "mdate": 1762943435552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dynamic Task-Embedded Reward Machines (DTERM), a hypernetwork-based framework for dynamically weighting reward components in reinforcement learning for code generation and manipulation tasks. Instead of using static weights for sub-rewards (e.g., syntax, functionality, style), DTERM employs task embeddings derived from transformer encoders (e.g., CodeBERT) to generate adaptive weighting through a hypernetwork. The method is tested on several code generation benchmarks such as CodeXGLUE, HumanEval, APPS, and DeepFix, reporting modest improvements over static baselines."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "While the motivation of dynamic reward composition is reasonable, the technical soundness is weak. The method lacks clear theoretical grounding, ablation rigor, and reproducibility. Several equations are poorly defined, notation inconsistent, and experimental details insufficient to support the claims. The “cross-task prototype” and “FiLM modulation” mechanisms are mentioned but not rigorously formulated or justified. The last section (“Dual Selfular-Acting Machine”) seems unrelated and possibly mistakenly copied."}, "weaknesses": {"value": "The idea of task-conditioned dynamic reward weighting via hypernetworks is moderately interesting, but not novel. Prior work on meta-RL, reward machines, and multi-objective RL (e.g., Icarte et al., 2022; Yang et al., 2019a,b) already explore similar concepts with stronger theoretical and experimental grounding. The paper lacks a substantial new insight or methodological advance. The improvements in Table 1 (~2–4%) are minor and may be within variance."}, "questions": {"value": "What exactly is the learning objective of the hypernetwork? Is it trained jointly with the policy or separately?\n\nHow are task embeddings obtained—are they frozen or fine-tuned during RL training?\n\nWhy does the “Dual Selfular-Acting Machine” appear in the conclusion—was this an editing error?\n\nHow does DTERM differ in practice from meta-learning reward-weight modulation (e.g., GradNorm or MAML-style adaptation)?\n\nWere any of the experiments verified for statistical significance or reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hDyMiHcIpR", "forum": "0JLUFJMo5p", "replyto": "0JLUFJMo5p", "signatures": ["ICLR.cc/2026/Conference/Submission25449/Reviewer_YZfp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25449/Reviewer_YZfp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762911982305, "cdate": 1762911982305, "tmdate": 1762943434802, "mdate": 1762943434802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}