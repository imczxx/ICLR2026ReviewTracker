{"id": "gu9yu4FTkI", "number": 24622, "cdate": 1758358688758, "mdate": 1759896757975, "content": {"title": "RegexPSPACE: A Benchmark for Evaluating LLM Reasoning on PSPACE-Complete Regex Problems", "abstract": "Large language models (LLMs) show strong performance across natural language processing (NLP), mathematical reasoning, and programming, and recent large reasoning models (LRMs) further emphasize explicit reasoning. Yet their computational limits$\\textemdash$particularly spatial complexity constrained by finite context windows$\\textemdash$remain poorly understood. While recent works often focus on problems within the NP complexity class, we push the boundary by introducing a novel benchmark grounded in two PSPACE-complete regular expression (regex) problems: equivalence decision (RegexEQ) and minimization (RegexMin). PSPACE-complete problems serve as a more rigorous standard for assessing computational capacity, as their solutions require massive search space exploration. We perform a double-exponential space exploration to construct a labeled dataset of over a million regex instances with a sound filtering process to build the benchmark. We conduct extensive evaluations on 6 LLMs and 5 LRMs of varying scales, revealing common failure patterns such as verbosity and repetition. With its well-defined structure and quantitative evaluation metrics, this work presents the first empirical investigation into the spatial computational limitations of LLMs and LRMs, offering a new framework for evaluating their advanced reasoning capabilities.", "tldr": "", "keywords": ["PSPACE-complete", "Regex Minimization", "Regex Equivalence"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2a13d65ae732f2c678a69d5b802f847485371ad.pdf", "supplementary_material": "/attachment/a4b652946776919fc38b165555a30595392399c5.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors investigate the ability of LLMs to solve algorithmic problems in the complexity class PSPACE, in particular, the two PSPACE-complete problems **regex equivalence** (i.e., determining if two regexes are equal) and **regex minimization** (i.e., deciding the minimal size of a regex). They motivate their investigation in the beginning by claiming that \"LLMs are often claimed to be Turing-complete, yet such arguments rely on unrealistic assumptons, such as infinite context length..\"  and that while some recent studies investigate NP-hard problems, \"the actual limitations of LLMs are more closely tied to context length, which represents limited memory, and from this spatial perspective, analyses of their computational boundaries remain underexplored\". They claim that PSPACE problems are well suited to study LLMs from this \"spatial perspective\" and that regexes are a natural application to look at given their ubuquity throughout computer science. \n\nFirst, it doesn't seem to be the case LLMs are *often* claimed to by Turing-complete and that \"anlyses of their computational boundaries remain underexplored\". In fact, there is a rich contemporary literature on the computational expressivity of the transformer architecture, as noted in the studies below, where it is generally understand that transformers theoretically express a comparatively inexpressive class of functions, certainly well below PSPACE (I would ask the authors to carefully consider this work and consider citing it):\n> [**Merrill and Sabharwal 2025**] *Exact Expressive Power of Transformers with Padding*.  (**see citations within**)\n> [**Merrill and Sabharwal 2025**] *A little depth goes a long way: the expressive power of log-depth transformers*\n> [**Li and Cotterell 2025**] *Characterizing the Expressivity of Transformer Language Models* \n\nWhile the focus of this paper is on empirically testing transformers on PSPACE proboems, which is tangential to the theoretical studies cited above, they also claim that \"there is no direct prior work\" that they can compare to and \"no labeled datasets of benchmarks for PSPACE and beyond\", which is not quite the case. For example, the paper below builds testing datasets for problems in the complexity classes NLOGSPACE to NEXPTIME: \n> [**Madusanka, Pratt-Hartmann, Batista-navarro 2023**] *Identifying the limits of transformers when performing model-checking\nwith natural language*\nand is worth citing (see some prior work along these things cited inside). \n\nGiven the high computational overhead of creating data instances for both problems, the dataset construction procedure for both subsets is not straightforward, and most details are included in the appendix (Appendix B). The way that this is written is the main text is hard to follow, e.g., they write (line 205) \"We construct three resources for regex problems: a labeled dataset, an unlabeled dataset and a benchmark\" (it remains unclear exactly what the unlabeled dataset is for). Later they write that they \"Based on this [i.e., their regex dataset, we augment this dataset with a regex equivalence task] and curate 1,685 non-trivial regex problems ... to construct the Regex Problem benchmark\". The details of what is being shown on the far left of Figure 2 is hard to discern, I would ask the authors to make the dataset size and structure more explicit in a table and provide an easier to read summary of the the dataset construction procedure and formal results in the appendix. \n\nThey experiment with 6 LLMs and 5 LRMs (exclusively open weight models such as Qwen, Phi, Deepseek and gpt-oss) and their main results are reported in Table 1. Specifically, they compare zero-shot versus five-shot performance and find (as expected) large gradations in performance based on model size, with GPT-oss achieving the highest accuracy of ~87% on the regex equivalence task (the results on the minimization task are harder to interpret, given that a special metric is needed which is hard to understand based on the description starting on line 248). They perform further error analysis at the end."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "-- A new suite of hard reasoning challenge datasets, specifically focus on PSPACE-complete algorithmic problems involving regex manipulation."}, "weaknesses": {"value": "-- **Essential details are difficult to follow**. Specifically, the dataset construction procedure is very hard to understand, especially given that virtually all of the details are in the appendix. I would encourage the authors to move some of this content into the main paper and show clearly the size and the structure of the two datasets in a table. \n\n-- As discussed above, the **main justification** for why it is useful to study PSPACE problems, or other problems beyond NPTIME, needs to be better motivated and considered in light of the missing citations provided above. \n\n-- The **choice of models is not very well motivated* and the results are difficut to comprehend (see specific questions below)."}, "questions": {"value": "-- How big are the resulting two datasets, and what is the role on the unlabeled examples you discuss at the begining of Section 4? \n\n-- What are the 3D plots in Figure 2 showing exactly? \n\n-- Are the resulting instances you construct computationally hard in any formal sense? I'm particularly struct by the high performance of GPT-oss-low on regex equivalence (86% acc.). One might naively thing that this shows evidence that the model can solve PTIME-complete problems, however, a more likely explanation is that the examples sampled are cover the simplest problems in that class. \n\nAre there any known phase-transitions for this problem that you might consider exploiting in order to get at the really hard cases? This is how hard instances are constructed for NP-complete problems in the following papers, which warrants some consideration:\n> [**Richardson, Sabharwal**] *Pushing the Limits of Rule Reasoning in Transformers through Natural Language Satisfiability*  [**Madusanka, Pratt-Hartmann, Batista-navarro 2023**] *Identifying the limits of transformers when performing model-checking\nwith natural language*"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4yddKYL9rK", "forum": "gu9yu4FTkI", "replyto": "gu9yu4FTkI", "signatures": ["ICLR.cc/2026/Conference/Submission24622/Reviewer_MA8R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24622/Reviewer_MA8R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761269718653, "cdate": 1761269718653, "tmdate": 1762943139026, "mdate": 1762943139026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RegexPSPACE, a benchmark focused on PSPACE problems that includes regex equivalence and regex minimization tasks to test LLMs’ reasoning under space constraints. The authors report empirical findings from evaluating LLMs and LRMs on their benchmark: (1) models tend to perform worse on regex minimization than on equivalence; (2) common failure modes include verbosity, invalid outputs, and hitting token limits; and (3) performance drops as inputs grow longer."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The authors evaluated their benchmark on 12 models and conducted a thorough failure-case analysis, reporting the percentage of each failure type, including repetition, invalid outputs, and stopping due to token limits.\n- The evaluation metrics chosen for the two tasks are reasonable."}, "weaknesses": {"value": "- I’m mainly concerned that the introduction motivates a benchmark to demonstrate the spatial computational limitations of LLMs (as opposed to existing theoretical work), yet the empirical findings do not clearly answer this question or convincingly achieve the goal of using PSPACE problems to quantify the need for more space complexity. The failure modes for RegexMin—such as verbosity and repetition—may relate to space pressure, but they are not unique to it. Likewise, hitting the output limit reflects an insufficient output budget, which is not necessarily caused by the spatial complexity of the benchmark’s problems. Without a causal demonstration, the claim that empirical findings necessitate evaluating \"under spatial complexity constraints” is not very convincing; instead, the results indicate that longer inputs and limited output budgets correlate with worse outcomes, which is not surprising. I did a very quick literature review and could find quite a few papers with experiments showing \"context length hurting the performance of LLMs\" or \"diminishing performance with longer prompts\".  \n- There are quite a few typos throughout the paper. While typos are not major issues, they were noticeable during reading. A more thorough proofreading would help in a future version (including notation consistency, e.g., RegexMin vs. RegexMinimization). Some examples:  \n   - line 45: “a challenging and essential [task]”\n   - line 83: missing a period\n   - line 229: \"process\" misspelled\n   - line 465: \"focus\" misspelled"}, "questions": {"value": "- Is there a way to explicitly quantify how much the implicit space requirements of these PSPACE tasks contribute to LLM failures?\n- Beyond the reported metrics, are there any novel insights or qualitative analyses that emerge from the empirical results of evaluating this benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NMUMsp8Qqy", "forum": "gu9yu4FTkI", "replyto": "gu9yu4FTkI", "signatures": ["ICLR.cc/2026/Conference/Submission24622/Reviewer_3xY6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24622/Reviewer_3xY6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964701353, "cdate": 1761964701353, "tmdate": 1762943138797, "mdate": 1762943138797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We would like to thank all reviewers for their constructive comments.\n\nHowever, there seem a gap in understanding the fundamental necessity of our problem's PSPACE-completeness or imprecise feedbacks on related works (e.g.; the paper “Identifying the limits of transformers...” addresses a PTIME problem, representing a significantly lower complexity bound than our PSPACE-complete task .) due to various reasons.\n\nWe will revise our paper to cope with these comments and clarify our contributions compared with current works more clearly."}}, "id": "2Sw6PtxOnV", "forum": "gu9yu4FTkI", "replyto": "gu9yu4FTkI", "signatures": ["ICLR.cc/2026/Conference/Submission24622/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24622/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24622/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763520440060, "cdate": 1763520440060, "tmdate": 1763520440060, "mdate": 1763520440060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "this paper proposes a complexity-based benchmark based on PSPACE complexity, extending beyond traditional NP-complexity complexity hierarchy"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. introduce PSPACE in evaluation, which is not included or considered in any previous benchmarks\n2. solid experiment results with open-source models\n3. detailed failure case analysis"}, "weaknesses": {"value": "1. there are plenty works trying to evaluate LLMs based on complexity hierarchy, as discussed related work, and thus I am not sure why adding this P-SPACE complexity class experiment is essentially interesting. What's inherently new about it and what different observations can we derive compared with eval results using time-complexity classes? \n2. the paper only evaluates open-source models without close-source models"}, "questions": {"value": "why PSPACE? a lot of models can't work well on NP-complete problems yet, why we need this extra benchmark?\n\nWhat new observations or conclusions can be made by evaluating on PSPACE that cannot be found when evaluating on NP-complete etc?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Z2wdy7dGrX", "forum": "gu9yu4FTkI", "replyto": "gu9yu4FTkI", "signatures": ["ICLR.cc/2026/Conference/Submission24622/Reviewer_KbuW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24622/Reviewer_KbuW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24622/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009287116, "cdate": 1762009287116, "tmdate": 1762943138414, "mdate": 1762943138414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}