{"id": "aa14rlfR6k", "number": 2095, "cdate": 1756987308534, "mdate": 1759898169854, "content": {"title": "UNIVERSAL AND EFFICIENT LOADING BALANCING FOR RL TRAINING OF LARGE MULTIMODAL MODELS", "abstract": "Reinforcement learning (RL) is crucial for aligning Vision-Language Models (VLMs), but its practical application is hampered by significant system-level bottlenecks. The typical RL pipeline, encompassing data loading, inference-based rollouts, and model updates, suffers from severe inefficiencies when applied to VLMs due to the extreme heterogeneity of multimodal data. Centralized data loading creates I/O bottlenecks with large media files, while variations in sequence length across text, image, and video inputs lead to critical load imbalance during computation, leaving expensive GPU resources underutilized. Existing systems either focus on text-only RL or employ general load-balancing techniques that are incompatible with the small-batch, iterative nature of RL training.\n\nTo address these challenges, we present FlexRL, a holistic system designed to optimize the end-to-end VLM RL pipeline. FlexRL introduces two core contributions: (1) a \\textbf{Decentralized Data Pipeline} that parallelizes data fetching and preprocessing across worker nodes, facilitates metadata-only scheduling on the single controller, eliminating the central bottleneck and accelerating data-intensive stages; and (2) a novel \\textbf{Hybrid Sequence Sharding} mechanism that partitions sequences into fine-grained chunks. This enables sub-sequence level load balancing for both inference and training, effectively mitigating workload skew. Our evaluation on a 128-GPU cluster shows that FlexRL significantly improves training efficiency by up to 4.2$\\times$ in long video training scenarios compared to state-of-the-art baselines, enabling more efficient and scalable RL for large multimodal models.", "tldr": "FlexRL removes data and computation bottlenecks in RL for Vision-Language Models by finely sharding sequences and decentralizing data loading, achieving up to 4.2× faster training on large clusters.", "keywords": ["RL Training;Load Balancing;Seuqence Parallelism;Distributed Training"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d8bcb7dda9a7349b3b310f239a47f0221b34940.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents FlexRL, a system designed to address data loading bottlenecks and computational load imbalance in RL training for large multimodal models. Its core contributions are a decentralized data pipeline and a novel hybrid sequence sharding mechanism. The authors claim up to 4.2× speedup on a 128-GPU cluster."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Accurately identifies key systemic bottlenecks in multimodal RL training.\n\n- The proposed hybrid sequence sharding mechanism is ingenious and offers a promising direction for handling workload skew from heterogeneous sequences.\n\n- The design of a dynamic execution engine to manage the scheduling complexity introduced by hybrid sharding is a significant engineering step."}, "weaknesses": {"value": "- [Mandatory] There are minor typos, formatting inconsistencies, and grammatical errors. The authors should carefully proofread the manuscript.\n\n- [Mandatory] The paper does not separately evaluate the individual contributions of the two core components: the \"Decentralized Data Pipeline\" and the \"Hybrid Sequence Sharding.\" It is unclear which component drives the performance gains.\n\n- [Mandatory] The hybrid sharding introduces complex All-to-All communication. While overlap strategies are qualitatively mentioned, a quantitative analysis of the communication overhead's impact on end-to-end performance under different cluster scales and network topologies is missing.\n\n- [Mandatory] Key details regarding the scheduling heuristic and cost estimation model are missing, hindering reproducibility."}, "questions": {"value": "Please refer to Weaknesses. Btw, I have some optional questions:\n\n- [Optional] Dynamic sequence packing techniques, which concatenate short sequences into longer ones during training, have emerged recently. How does FlexRL fundamentally compare to such methods in terms of load balance efficiency and memory utilization? What are the relative advantages and disadvantages?\n\n- [Optional] The title claims \"Universal,\" but the method heavily relies on All-to-All communication within the attention mechanism. If future VLM architectures shift towards SSMs or other non-attention-based mechanisms, would FlexRL's core sharding mechanism remain effective? What is your view on this architectural dependency risk?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "n1vX2WC8C7", "forum": "aa14rlfR6k", "replyto": "aa14rlfR6k", "signatures": ["ICLR.cc/2026/Conference/Submission2095/Reviewer_uQAx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2095/Reviewer_uQAx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760607839490, "cdate": 1760607839490, "tmdate": 1762916020309, "mdate": 1762916020309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a distributed training pipeline for RL training stage of large-scale VLMs involving multimodal data (images, videos, texts). The key challenge in the multimodal RL training is the highly diverse data length (short text, long text, long image&video tokens), which is hard to be properly scheduled in a distributed training system. It mainly proposes a decentralized data pipeline to properly schedule the data with a single controller, and a hybrid sequence sharding technique to partition sequences into finegrained chunks to enable sub-sequence level load balancing. Existing technique Ulysses Sequence Parallelism is used to enable sequence parallel training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tThe proposed hybrid sharding technique is novel and alleviate the issue of imbalanced loading.\n-\tExperiments show the proposed approach outperform the speed of existing approach such as verl on video understanding tasks."}, "weaknesses": {"value": "-\tThe speed of running a batch on a gpu should be clarified more. How does the gpu handles sequence with different length in a batch? From figure2, a gpu will pack samples with different lengths into groups and conduct their attention operation separately. While in some implementations using generic sequence packing and masked attention, the running time is irrelevant to the sequence length of each sample since a global masked attention of all tokens in a batch is conducted. How much speed up does the proposed approach have compared to the global-attention method?\n-\tThe paper only provides numbers for speed, while accuracy numbers are not provided. The accuracies of using the proposed approach is also required to show the method’s robustness for different applications."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vn3XbWRZcf", "forum": "aa14rlfR6k", "replyto": "aa14rlfR6k", "signatures": ["ICLR.cc/2026/Conference/Submission2095/Reviewer_cr9u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2095/Reviewer_cr9u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930764131, "cdate": 1761930764131, "tmdate": 1762916019176, "mdate": 1762916019176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "FlexRL is an end-to-end optimization system built on the verl framework to improve the efficiency of RL training for MLLMs. It addresses two primary bottlenecks: (1) a Decentralized Data Pipeline that distributes multimodal data loading and preprocessing across worker nodes while the control node handles only lightweight metadata, eliminating centralized I/O bottlenecks; and (2) a Hybrid Sequence Sharding mechanism that partitions sequences into fine-grained chunks to achieve subsequence-level load balancing, mitigating uneven GPU utilization caused by extreme length disparities across modalities such as text, images, and video."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper systematically analyzes practical bottlenecks across the entire RL training pipeline for MLLMs rather than focusing on a single stage, and it demonstrates strong system-level completeness."}, "weaknesses": {"value": "I must first note that I am not very familiar with mlsys, while I only offer a limited perspective on this paper.\n\n1. Why not compare against other verl-based optimized frameworks, such as [1], which also targets long-video scenarios?\n2. The paper lacks concrete ablations; for example, it does not separately quantify the contributions of the Decentralized Data Pipeline and the Hybrid Sequence Sharding components.\n3. Is the framework primarily intended for highly imbalanced workloads? The design appears to degenerate to conventional parallelism, but comparisons under balanced workloads (e.g., image-only or pure-text) are missing.\n4. Performance under different settings is not reported, e.g., varying batch size and tensor/pipeline/sequence parallelism (TP/PP/SP) sizes.\n5. As an mlsys work, the paper does not provide an engineering code release or community usage feedback, which I consider a weakness.\n\n[1] Scaling RL to Long Videos. NeurIPS 2025."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yQk9s5JhlK", "forum": "aa14rlfR6k", "replyto": "aa14rlfR6k", "signatures": ["ICLR.cc/2026/Conference/Submission2095/Reviewer_apGj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2095/Reviewer_apGj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2095/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160203903, "cdate": 1762160203903, "tmdate": 1762916019015, "mdate": 1762916019015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}