{"id": "CSIo4D7xBG", "number": 16394, "cdate": 1758264186071, "mdate": 1763536204211, "content": {"title": "WebArena Verified", "abstract": "Autonomous web agents have been growing in interest within the research community and are increasingly operating in more complex multistep browser workflows. However, widely used benchmarks can misestimate performance due to underspecified goals and fragile evaluators—challenges typical of normal benchmark maturation rather than flaws in the paradigm.\nThis hinders accurate performance assessment and limits benchmark utility for guiding method development.\nWe present WebArena Verified, a reproducible re‑evaluation of WebArena that retains its containerized environments while strengthening measurement.\nWe audit all 812 tasks, repair misaligned checks and clarify ambiguous instructions, replace substring matching with type‑aware exact matching with semantic normalization, verify backend state for mutation tasks, and adopt a structured JSON schema with explicit status codes for deterministic scoring.\nFor reporting, we provide per‑template macro‑averaged metrics, 95\\% confidence intervals, and failure‑mode breakdowns.\nOur new evaluator reduces the false‑negative rate by 11.3 percentage points compared to the original scoring pipeline.\nTo reduce evaluation overhead, we introduce WebArena Verified Hard, a 258‑task subset that retains difficult tasks, reduces runtime by 68.2\\%, and maintains discriminative power and coverage.\nWebArena Verified remains drop‑in compatible with existing agents, requiring only minimal changes and enabling faithful comparisons. Code, data, and evaluation tools will be released to support reproducibility.", "tldr": "", "keywords": ["web agent", "agent evaluation", "webarena", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c69e5d0c4b706363978d60c23aadd74e0a8135c3.pdf", "supplementary_material": "/attachment/a6d927fd94f28f07d6b2e386831a527bda8c2219.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces WebArena Verified, a revised and enhanced version of the WebArena benchmark. The authors audited all 812 original tasks and their failure patterns, identifying flaws such as fragile evaluation logic, ambiguous instructions, and knowledge contamination. They propose a new version, WebArena Verified, that addresses these flaws. They also introduce WebArena Verified Hard, a smaller and more challenging subset of the verified dataset. Using the OpenAI Operator, they show that the new dataset strengthens evaluation while preserving WebArena's realism."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors conducted a rigorous study of both the original WebArena benchmark and their revised version, as well as a detailed analysis of agent failure patterns. The lessons learned are valuable for the designers of future benchmarks.\n\n2. The authors proposed several new verification methods that successfully reduce many of the false positives and false negatives present in the original benchmark's design.\n\n3. The authors provide a newly revised version of the WebArena dataset and a carefully selected \"hard\" subset, which will be valuable to the community."}, "weaknesses": {"value": "1. While \"Type-Aware Exact Matching\" is an improvement over naive string comparison, I am not convinced it can fully replace LLM-based judging. It is difficult to normalize dates, currencies, and coordinates in a way that covers all possible edge cases.\n\n2. The \"Playwright network monitoring\" feature aims to detect knowledge contamination, where agents use pre-existing knowledge to complete a task without interacting with the website. However, this may unfairly penalize two legitimate agent designs: a) agents that persist memory across tasks, and b) agents that use API calls to interact with websites.\n\n3. The new requirement for a \"structured action field\" gives the harness a greater capability to assess task understanding, but it also introduces the additional challenge of formatting structured responses. The ability to formulate these responses goes beyond the original testing scope of WebArena.\n\n4. Finally, the paper's contribution seems somewhat limited. While the systematic analysis of WebArena's failure patterns is valuable, this contribution is partially attributed to the *Establishing Best Practices for Building Rigorous Agentic Benchmarks* paper that this work cites. The primary contribution is the new dataset, which is an incremental improvement on an existing benchmark."}, "questions": {"value": "1. Why not open-source the code, data, and tools during the review phase via an anonymous repository? This would help reviewers better understand the contribution.\n\n2. Did you reach out to the original WebArena team? If so, have they acknowledged the flaws that you systematically discovered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mU4fB4znmC", "forum": "CSIo4D7xBG", "replyto": "CSIo4D7xBG", "signatures": ["ICLR.cc/2026/Conference/Submission16394/Reviewer_KeUD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16394/Reviewer_KeUD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760854287524, "cdate": 1760854287524, "tmdate": 1762926517008, "mdate": 1762926517008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "BswN6QcXSi", "forum": "CSIo4D7xBG", "replyto": "CSIo4D7xBG", "signatures": ["ICLR.cc/2026/Conference/Submission16394/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16394/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763536202573, "cdate": 1763536202573, "tmdate": 1763536202573, "mdate": 1763536202573, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors develop WebArena Verified, a modification of WebArena that addresses issues in the current benchmark, including repairing checks and clarifying instructions. They demonstrate that this leads to a significant reduction in the false negative rate when evaluating agents. They additionally release a smaller hard subset of WebArena Verified, reducing evaluation time but retaining discriminatory power."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Improves robustness of agent evaluation\n- Removal of many LLM-as-judge requirements improves determinism"}, "weaknesses": {"value": "- Benchmarks and modifications thereof are useful when they tell us something new about models and their capabilities. It would be interesting to compare the accuracy achieved on WebArena versus WebArena Verified for more models, and to determine if there is a substantive change in the relative rankings of the models. Currently, evaluation is lacking, which limits our understanding of the benefits of WebArena Verified vs WebArena.\n- Lack of novelty: this paper is at most an incremental change over existing WebArena, and I am unsure whether data-cleaning of an existing benchmark merits its own paper.\n- Claims of discriminatory power of the hard subset are not supported in the main text, as there are far too few models compared."}, "questions": {"value": "- Could you perform evaluations to see the scores and relative rankings of more models on WebArena vs WebArena Verified?\n- On what grounds are you claiming that the hard subset retains discriminative power?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xPHyGsLmWM", "forum": "CSIo4D7xBG", "replyto": "CSIo4D7xBG", "signatures": ["ICLR.cc/2026/Conference/Submission16394/Reviewer_FSHA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16394/Reviewer_FSHA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829674213, "cdate": 1761829674213, "tmdate": 1762926516574, "mdate": 1762926516574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper revisits the WebArena benchmark and proposes WebArena Verified, a re‑evaluation that preserves the original containerized environments while substantially redesigning the measurement pipeline. Through a full audit of 812 tasks, the authors fix several problems (e.g., misalignment and ambiguities, brittle string matching) . Authors also release a difficulty‑focused 258‑task subset (WebArena Verified Hard) claimed to retain discriminative power while reducing runtime."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper makes a useful contribution by addressing reliability issues in WebArena. The authors conduct a full audit of 812 tasks and introduce type-aware matching, backend state checks, and structured JSON outputs. These changes reduce false positives and negatives and improve determinism in evaluation. The work is timely and aligns with best practices from other verified benchmarks like SWE-bench Verified and OSWorld-Verified. Overall, the design improvements are well motivated and clearly documented.\n\nIn summary, \n- The paper directly tackles important reliability issues in widely used WebArena benchmark.\n- All 812 tasks are examined under a documented protocol with double annotation and good inter‑rater agreement, yielding 81 reference alignment fixes and 218 ambiguity resolutions. This shows depth and rigor rather than anecdotal corrections"}, "weaknesses": {"value": "**Summary:**\n- The work is incremental and the impact is narrow.\n- The evaluation relies mainly on a single agent and one seed. This limits generalizability and makes claims about ranking stability and performance improvements weak. Multi-agent, multi-seed experiments are essential for a benchmark revision.\n-  Improvements target string‑verifiable or state‑verifiable tasks within five sites; limits include English‑only tasks, desktop web conventions, and missing state traces in historical logs—constraining generalization to other environments and modalities\n- The proposed “Verified Hard” subset fails pre-registered stability thresholds (Kendall τ and MAE). This undermines its intended role as a reliable low-cost proxy. Without stability, the subset risks misleading comparative studies.\n- While network‑activity checks remove trivial no‑browsing successes, the paper admits agents can still navigate to a page and answer from model memory without grounding in retrieved content; no page‑content attestation or citation‑based grounding is implemented\n\n**Details:**\nThe paper makes a useful contribution by addressing reliability issues in WebArena. The authors conduct a full audit of 812 tasks and introduce type-aware matching, backend state checks, and structured JSON outputs. These changes reduce false positives and negatives and improve determinism in evaluation. The work is timely and aligns with best practices from other verified benchmarks like SWE-bench Verified and OSWorld-Verified. Overall, the design improvements are well motivated and clearly documented.\n\nHowever, the contribution of the paper is narrow. It proposes a better version of WebArena, which is only one of the many benchmarks out there now. The work does not benefit users of other benchmarks. It would be more interesting if authors could show how their methodology could be generalized, with (semi-) automated techniques, to other benchmarks as well.\n\nAnother major concern is the narrow scope of empirical validation. Most results rely on a single agent (OpenAI Operator) and one seed, as noted in Section 5 and Appendix A. This limits confidence in claims about ranking stability and generalizability. Related work such as SWE-bench Verified emphasizes multi-agent, multi-seed evaluations to ensure robustness. I recommend expanding experiments to include diverse agent families and multiple seeds to strengthen the evidence base.\n\nThe proposed “Verified Hard” subset does not meet its stated stability goals. Table 8 shows Kendall τ and MAE thresholds are not satisfied, and Table 10 reports high selection variability (mean selection probability ≈0.30). This undermines the claim that the subset preserves rankings and could mislead comparative studies. To improve, consider increasing subset size, applying stratified sampling, or reframing the subset as a diagnostic stress test rather than a ranking proxy.\n\nThe paper acknowledges that agents can still answer from memory without using retrieved content (Section 5.2). While network-activity checks prevent trivial no-browsing successes, they do not guarantee evidence-based answers. This gap is critical because contamination and hallucination remain major concerns in web-agent evaluation. We suggest adding DOM span or citation-based attestation, similar to pointer-based evaluation in open-domain QA, to enforce grounding and improve reliability.\n\nSome methodological choices lack sufficient justification. For example, the survival-style GLMM for hardness and policy caps (κ_default, τ_hard/easy) appear hyperparameter-driven, and sensitivity analyses in Appendix B show variability in selection outcomes. This raises questions about reproducibility and robustness. We recommend providing clearer rationale for these choices and adding ablation studies or alternative models to demonstrate that conclusions are not overly dependent on specific parameter settings."}, "questions": {"value": "1. Could you explain why the “Verified Hard” subset fails Kendall τ and MAE thresholds? Do you plan to adjust the subset size or selection method to improve stability?\n2. Why were most evaluations conducted with a single agent and one seed? Do you have results for other agent families or multiple seeds to confirm robustness?\n3. How does the benchmark ensure that answers are based on retrieved content rather than model memory? Are there plans to add DOM span or citation-based checks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MaZAEjbn0e", "forum": "CSIo4D7xBG", "replyto": "CSIo4D7xBG", "signatures": ["ICLR.cc/2026/Conference/Submission16394/Reviewer_QCGH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16394/Reviewer_QCGH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860384295, "cdate": 1761860384295, "tmdate": 1762926516114, "mdate": 1762926516114, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper introduces WebArena Verified, a re-evaluation of WebArena that reduces the false-negative rate by 11.3 percentage points compared to Web Arena.\n- The authors also introduce WebArena Verified Hard, a subset of the full tasks that retains difficult tasks and reduces runtime."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper conducts a systematic audit of WebArena tasks in terms of task specification and evaluation reliability.\n- Significant engineering effort seems to have been done to turn WebArena into WebArena Verified, by introducing several concrete technical improvements to address issues in Web Arena"}, "weaknesses": {"value": "- The work focuses exclusively on the WebArena benchmark. As such, it is unclear how the methodology could be systematically applied to other benchmarks. A clearer framework for generalization would strengthen its broader impact.\n- The experimental validation only has 2 agents: OpenAI Operator and a simple baseline ensemble. Evaluating multiple diverse agents or LLMs would better demonstrate WebArena Hard and WebArena Verified Hard discriminative power and stability across different architectures.\n- Constructing a “hard” version is not novel (e.g., https://arxiv.org/abs/2406.11939). The authors could more clearly state how their approach is similar or different from others."}, "questions": {"value": "Please address each of the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Glce34Awvb", "forum": "CSIo4D7xBG", "replyto": "CSIo4D7xBG", "signatures": ["ICLR.cc/2026/Conference/Submission16394/Reviewer_8zd3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16394/Reviewer_8zd3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951584668, "cdate": 1761951584668, "tmdate": 1762926515619, "mdate": 1762926515619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}