{"id": "tVnml9Q4XW", "number": 5334, "cdate": 1757902170176, "mdate": 1763732309216, "content": {"title": "Turning Internal Gap into Self-Improvement: Promoting the Generation-Understanding Unification in MLLMs", "abstract": "Although unified MLLMs aim to unify generation and understanding, they are considered to exhibit an internal gap, with understanding outperforming generation. Through large‑scale evaluation across multiple MLLMs and tasks, we confirm the widespread non‑unification of MLLMs, and demonstrate that it indeed stems from weak generation rather than misunderstanding. This finding motivates us to propose a simple yet effective internal gap-based self-improvement framework, which mitigates internal gaps by leveraging stronger understanding to guide weaker generation without relying on any external signals. We validate this strategy through comprehensive experiments: scoring generations with understanding to construct image data for post-training (e.g., SFT and DPO) significantly improves generation while promoting unification. Furthermore, we empirically discover a co-improvement effect of such self-improvement, a phenomenon well known in pre-training but underexplored in post-training. Specifically, as generation improves, understanding becomes more effective at detecting false positives that were previously misclassified as prompt‑aligned. To explain this effect, we extend learning dynamic theory to the MLLM setting, showing that the shared empirical neural tangent kernel between generation and understanding encourages aligned learning dynamics, thereby driving co-improvement. This interplay between generation and understanding further motivates a curriculum learning approach for stronger self‑improvement: progressively enhanced understanding and generation revisit samples underutilized by pre‑trained MLLMs, dynamically expanding post‑training data and leading to improved performance and unification.", "tldr": "We systematically explore the internal gap in unified MLLMs, which typically manifests as understanding being stronger than generation, covering empirical validation, mitigation methods, mechanistic analysis, and the design of improved approaches.", "keywords": ["MLLM", "Self-improvement", "Unification"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/535d7d14c4f475b8a25ccb8ef681fcdcc5bc4171.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the \"internal gap\" in unified Multimodal Large Language Models (MLLMs), where understanding consistently outperforms generation. The authors propose a self-improvement framework that leverages stronger understanding to guide weaker generation through standard post-training (SFT/DPO), achieving up to 20% generation improvement without external signals."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides systematic empirical validation of the generation-understanding gap across multiple MLLMs with clear methodology and reproducible experiments. The theoretical framework extending learning dynamics to multimodal settings offers mathematical formalization, though it essentially explains an expected outcome—that shared parameters naturally lead to correlated improvements. The work is technically sound with detailed ablations."}, "weaknesses": {"value": "The work is technically sound with detailed ablations, but the core insight that two capabilities in a closed system can mutually improve through iterative training is conceptually straightforward, limiting originality. The self-improvement approach applies standard techniques (SFT/DPO) without novel algorithmic contributions, and still underperforms methods using external rewards, constraining its practical significance. Critically, the theory fails to address fundamental limitations: in a closed system without external supervision, two learners cannot indefinitely improve through mutual training—the paper lacks analysis of performance ceilings or convergence bounds.  Overall, the paper competently documents a predictable phenomenon rather than introducing fundamentally new concepts.\nMore specifically, the theoretical analysis (Propositions 1-2) provides mathematical formalization but offers limited conceptual depth beyond expected outcomes. Experimental scope is narrow: only two models tested, consistently underperforming external reward baselines, undermining practical value. The non-unification metric relies on potentially unreliable binary judgments, and using Qwen as ground truth introduces unexamined biases. Curriculum learning adds minimal gains without principled design."}, "questions": {"value": "no more."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oOl2OPoEIh", "forum": "tVnml9Q4XW", "replyto": "tVnml9Q4XW", "signatures": ["ICLR.cc/2026/Conference/Submission5334/Reviewer_fCxY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5334/Reviewer_fCxY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761460237930, "cdate": 1761460237930, "tmdate": 1762918012595, "mdate": 1762918012595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the generation–understanding internal gap in unified multimodal large language models (MLLMs), where the understanding branch consistently outperforms the generation branch.\nTo address this, the authors propose a simple internal gap–based self-improvement framework that leverages the model’s own understanding capability to guide and enhance generation, without any external rewards or supervision.Key contributions include:\n\n1. Quantitative diagnosis of generation–understanding non-unification, introducing an internal Non-Unification Score to measure intra-model consistency and empirically verifying the widespread internal gap across six MLLMs (Sec. 3).\n\n2. Internal gap–based self-improvement framework, leveraging the stronger understanding branch to score and filter generations for post-training (SFT/DPO), effectively improving generation quality and reducing non-unification without external signals (Sec. 4.1–4.2).\n\n3. Theoretical analysis of the co-improvement effect, extending learning dynamics to multimodal models and revealing that shared empirical neural tangent kernels (eNTKs) drive aligned updates between generation and understanding (Sec. 5).\n\n4. Curriculum-based self-improvement strategy, progressively reintroducing previously underutilized or difficult samples through curriculum replay to further enhance both branches and surpass external-reward baselines (Sec. 6)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper propose an internal gap–based self-improvement framework, which is conceptually simple but novel, requiring no external supervision or reward models.  The authors also introduce a new internal evaluation metric (Non-Unification Score) to quantify intra-model consistency.\n\n2. The work provides strong empirical evidence through large-scale experiments on six unified MLLMs and three task difficulty levels (Figure 2).\n\n3. Figures and algorithms are clearly presented — e.g., Algorithm 1 succinctly formalizes the self-improvement process, and Figure 5 (a) intuitively illustrates the co-improvement effect with side-by-side visual examples.\n\n4. The discovery of a shared empirical NTK between generation and understanding offers a novel theoretical perspective on multimodal model coupling ( Eq. 2–3). The curriculum-based self-improvement strategy demonstrates a scalable path to strengthen MLLMs without external data or rewards, surpassing reward-model–based baselines such as T2I-R1 and HermesFlow."}, "weaknesses": {"value": "1. Theory–practice gap: While the shared eNTK explanation is conceptually interesting, its empirical validation remains limited, and the theoretical section is notation-heavy, reducing accessibility for non-theoretical readers.\n\n2. Limited model diversity: Main experiments focus on Janus-Pro and Show-o. Although six models were initially analyzed, most in-depth post-training results come from only two, limiting the generality of conclusions."}, "questions": {"value": "1. How stable is the self-improvement process when the understanding branch provides noisy or incorrect judgments?\n\n2. Can the proposed framework generalize to other modalities (e.g., video or audio) or to non-generative MLLM tasks?\n\n3. How sensitive are the results to the number of generated candidates N and the selection threshold in the understanding branch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KtrPQVy61J", "forum": "tVnml9Q4XW", "replyto": "tVnml9Q4XW", "signatures": ["ICLR.cc/2026/Conference/Submission5334/Reviewer_Qp5p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5334/Reviewer_Qp5p"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834794404, "cdate": 1761834794404, "tmdate": 1762918012384, "mdate": 1762918012384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically investigates the prevalent \"generation-understanding non-unification\" phenomenon in Multimodal Large Language Models (MLLMs), where understanding capabilities typically outperform generation capabilities. The authors propose an internal gap-based self-improvement framework that leverages the model's own stronger understanding branch to guide its weaker generation branch. Without relying on external signals, this approach significantly enhances generation quality and reduces the internal gap through post-training methods like SFT or DPO. Experiments reveal that this method also induces a \"co-improvement\" effect, where improvements in generation simultaneously enhance understanding, particularly in identifying misaligned generated samples. Furthermore, drawing from learning dynamics theory, the paper identifies the shared empirical Neural Tangent Kernel (eNTK) between generation and understanding as the key mechanism behind co-improvement. Based on this, a curriculum learning strategy is proposed to dynamically expand the training data, further boosting model performance and unification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Rigorous Problem Verification: The paper first confirms the internal gap in MLLMs where \"generation is weaker than understanding\" through large-scale evaluation (across 6 models and tasks of varying difficulty). To achieve this, the authors innovatively propose a \"non-unification score\" that does not rely on external evaluators. \n- Simple and Effective Solution: An \"internal gap-based self-improvement\" framework is proposed, which leverages the model's own stronger understanding capability to filter generated data (for SFT or DPO). This method requires no external reward models, achieving closed-loop self-improvement.\n- Discovery and Explanation of the \"Co-improvement\" Effect: this paper introduces that \"generation-targeted training\" also enhances \"understanding capability\" (particularly in correcting false positive samples). From a learning dynamics perspective, the paper attributes this to the shared eNTK (empirical Neural Tangent Kernel) between the generation and understanding branches.\n- Clear Logical Structure Throughout the Paper: The paper exhibits a clear and coherent logical structure from beginning to end."}, "weaknesses": {"value": "- Experimental Results Heavily Rely on a Single, Insufficiently Strong Judge Model:Two of the paper's key conclusions—that the \"internal gap stems mainly from weak generation\" and the \"co-improvement effect\"—rely heavily on using Qwen2.5-VL-72B-Instruct as the sole external judge.\n    However, Qwen2.5-VL-72B is not a strong enough multimodal model to serve as a reliable evaluator, especially when dealing with complex or \"hard tasks.\" To enhance the credibility of the experimental results, it is recommended to adopt more advanced SOTA models or incorporate multiple models as evaluators.\n- Limited Innovation and Effectiveness of the Curriculum Learning Part:\n    The curriculum learning (C-SFT/C-DPO) strategy proposed in Section 6, whose core idea is to reuse \"difficult\" samples discarded earlier due to the model's limited capability, is lacking innovation and Effectiveness.\n    This method is a conventional application of curriculum learning and lacks significant methodological innovation.Based on the experimental results (Table 1 and Table 5), the improvement of C-SFT over standard SFT is very marginal. For example, on the \"Overall\" metric for T2I-CompBench++ (Table 5), the generation score for Janus-Pro only increased from 43.29 to 44.18, and for Show-o, only from 52.67 to 52.82. This marginal gain seems insufficient to justify the introduction of this strategy.\n- Insufficient Experimental Evidence and Lack of Generalization for \"Co-Improvement Effect\":The \"co-improvement effect\" (Finding 2), a key contribution, claims that self-improvement targeted at generation also enhances understanding. As mentioned, the effect is primarily shown via the custom, Qwen-dependent \"win rate\" metric.The improvement on standard benchmarks is Negligible(Table 8). For example, after SFT, Janus-Pro-7B's score on POPE \\textit{decreased} from 89.04 to 88.45, the improvement on MMB was only 0.74 (76.23 $\\rightarrow$ 76.97), and on GQA, only 0.1 (56.02 $\\rightarrow$ 56.12).This significant disconnect between the custom metric (Win Rate) and standard benchmarks (Table 8) suggests that the so-called \"co-improvement\" might just be overfitting to the internal understanding task or the Qwen judge's preferences, rather than a genuine, generalizable improvement in understanding ability for standard VQA or hallucination detection tasks."}, "questions": {"value": "A critical ambiguity exists regarding potential data leakage in the T2I-CompBench++ experiments. The authors state in Section 4.2.1 that post-training data was constructed using ``about 6000 text prompts as post-training candidates'' from T2I-CompBench++. Subsequently, the model's performance is evaluated on the T2I-CompBench++ evaluation set (as shown in Table 1 and Table 5) . However, the paper fails to explicitly state whether the 6000 prompts used for post-training (SFT/DPO) were drawn from the benchmark's designatedtraining split. If any overlap exists between these post-training prompts and the prompts in the evaluation set, the reported performance gains on this benchmark would be invalid due to data contamination. This lack of clarity undermines the reliability of these key experimental results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B1DAYg0PkB", "forum": "tVnml9Q4XW", "replyto": "tVnml9Q4XW", "signatures": ["ICLR.cc/2026/Conference/Submission5334/Reviewer_fxim"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5334/Reviewer_fxim"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886974473, "cdate": 1761886974473, "tmdate": 1762918012168, "mdate": 1762918012168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors let the understanding branch of MLLMs judge the correctness of images generated by the generation branch, as a reflection of the model's internal gap, across multiple models and tasks. They leverage this gap to improve the models' generation ability using SFT and DPO with data generated by the generation branch and verified by the understanding branch, resulting in improved generation, understanding, and internal unification. They explain the enhancement of the understanding ability with a learning dynamics theory and support it with empirical analysis. They further propose to incorporate data that could not be utilized initially in their data-collection pipeline into the post-training process, after the model's ability has improved, thereby further enhancing the training outcome."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The self-improvement approach, which uses the understanding ability to guide the generation branch, is intuitive and intriguing in concept and effective in practice.\n2. The work is generally solid and logically rigorous, with a motivation validated across multiple models, comprehensive experiments conducted on two models evaluated by both the authors’ proposed metrics and existing benchmarks, theoretical interpretation, and empirical validation. One of the relatively unconvincing and inelegant aspects—using Qwen as an external judge—is compensated for by a human verification experiment.\n3. The learning dynamics analysis provides theoretical insights into the mechanism of the co-improvement effect.\n4. The paper is generally well written, and most details are explained clearly."}, "weaknesses": {"value": "The empirical evidence for some of the paper's claims is less conclusive than stated, which lacks further clarification:\n\n1. In Fig.2(c), the claimed \"trend of increasing with task difficulty\" for the non-unification score is not obvious or monotonic. The variation between models seems to dominate any difficulty-based trend. \n2. In Fig.7, the difference in similarity between improved samples and random samples is also not clear, especially for image pairs."}, "questions": {"value": "1. What was the detailed setup of the human evaluation in Fig. 9? The appendix mentions the human check but omits details on the number of annotators, the interface used, the specific instructions given. \n2. Could the authors evaluate the self-improved Show-o model on the understanding benchmarks, similar to Janus-Pro-7B in Tab. 8?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "D4bwmSD1g0", "forum": "tVnml9Q4XW", "replyto": "tVnml9Q4XW", "signatures": ["ICLR.cc/2026/Conference/Submission5334/Reviewer_iJU7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5334/Reviewer_iJU7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934408343, "cdate": 1761934408343, "tmdate": 1762918011888, "mdate": 1762918011888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}