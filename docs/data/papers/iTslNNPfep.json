{"id": "iTslNNPfep", "number": 12548, "cdate": 1758208505667, "mdate": 1762961073006, "content": {"title": "TaskForce: Cooperative Multi-agent Reinforcement Learning for Multi-task Optimization", "abstract": "Multi-task learning (MTL) involves the simultaneous optimization of multiple task-specific losses, often leading to gradient conflicts and scale imbalances that result in negative transfer. While existing multi-task optimization methods attempt to mitigate these challenges, they either lack the stochasticity needed to escape poor local minima or fail to explicitly address conflicts at the gradient level. In this work, we propose TaskForce, a novel multi-task optimization framework incorporating cooperative multi-agent reinforcement learning (MARL), where agents learn to find an effective joint optimization strategy based on their respective task gradients and losses. To keep the optimization process compact yet informative, agents observe a summary of the training dynamics that consists of the gradient Gram matrix—capturing both gradient magnitudes and pairwise alignments—and task loss values. Each agent then predicts the balancing parameters that determine the weight of their contribution to the final gradient update. Crucially, we design a hybrid reward function that incorporates both gradient-based signals and loss improvement dynamics, enabling agents to effectively resolve gradient conflicts and avoid poor convergence by considering both direct gradient information and the resulting impact on loss reduction. TaskForce achieves consistent improvements over state-of-the-art MTL baselines on NYU-v2, Cityscapes, and QM9, demonstrating the promise of cooperative MARL in complex multi-task scenarios.", "tldr": "We propose a TaskForce, the multi-task optimization framework leveraging cooperative multi-agent reinforcement learning.", "keywords": ["Multi-task learning", "Multi-objective optimization", "Cooperative multi-agent reinforcement learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/de85f9366d5430ca093defd9e58a4e4acdb84f62.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method to address the open challenge of training in multi-objective learning, a topic that has gained considerable attention in recent years. The authors introduce a cooperative multi-agent reinforcement learning (MARL) framework, where agents learn to develop an effective joint optimization strategy. Each agent predicts balancing parameters that determine the weight of its contribution to the final gradient update. The proposed algorithm is evaluated against multi-task learning (MTL) baselines on the NYU-v2, Cityscapes, and QM9 datasets."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is overall well-written and well-motivated within the context of existing literature. \nThe proposed method is innovative and relevant to the field. \nThe approach is conceptually sound and could have a meaningful impact on the multi-objective learning community.\n\nThis paper presents an interesting and original idea that connects cooperative multi-agent reinforcement learning with multi-task optimization. Conceptually, the approach is promising and could inspire further research in this direction."}, "weaknesses": {"value": "The paper lacks theoretical guarantees or even an intuitive justification for the proposed framework. For instance, a conceptual figure similar to Figure 1 in previous works (e.g., Aligned-MTL or NashMTL) could help illustrate the underlying intuition.\nThe experimental results are not sufficiently convincing: \n(a) The range of experiments is limited compared to prior works such as Aligned-MTL (Table 3) and NashMTL (10 seeds in Table 4). \n(b) In Table 1, results are reported with only two significant digits, whereas NashMTL reports four. This discrepancy can meaningfully affect the interpretation of Δm. \n(c) No results are provided for the 11 tasks of the QM9 dataset. \n(d) The paper does not include comparisons of runtime or computational efficiency with previous algorithms. \n(e) Overall, the reported improvements do not appear significant enough to support the claim that their algorithm achieves consistent improvements.\n\nThe current version lacks sufficient theoretical grounding and empirical rigor to fully support its claims. Strengthening the theoretical motivation, expanding the experimental analysis, and improving the presentation of results would substantially enhance the paper’s impact and credibility."}, "questions": {"value": "Please address the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TYEFXi1N97", "forum": "iTslNNPfep", "replyto": "iTslNNPfep", "signatures": ["ICLR.cc/2026/Conference/Submission12548/Reviewer_b8s2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12548/Reviewer_b8s2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761419626636, "cdate": 1761419626636, "tmdate": 1762923408315, "mdate": 1762923408315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "7Y7pqTSmmo", "forum": "iTslNNPfep", "replyto": "iTslNNPfep", "signatures": ["ICLR.cc/2026/Conference/Submission12548/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12548/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762961071613, "cdate": 1762961071613, "tmdate": 1762961071613, "mdate": 1762961071613, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the problem of multi-task learning (MTL) through the lens of reinforcement learning (RL). The authors employ an actor–critic framework to dynamically predict task weights during training. Specifically, the observation space consists of the concatenation of the Gram matrix and task losses, while the action corresponds to the predicted task weights. The reward is designed to encourage minimization of all task losses, thereby guiding the learning policy toward balanced optimization across tasks. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Studying MTL from the perspective of RL is novel and provides a fresh direction for adaptive task weighting.\n\n2. The paper is generally well-organized.\n\n3. The proposed method achieves competitive performance under certain experimental settings, demonstrating its potential effectiveness."}, "weaknesses": {"value": "1. The proposed RL-based paradigm may face scalability issues in many-task scenarios, as the number of agents grows linearly with the number of tasks, making it difficult to generalize efficiently.\n\n2. The actor-critic framework introduces additional trainable parameters for task weight prediction, which could lead to an unfair comparison with conventional MTL baselines. It would be helpful to investigate whether employing a simpler neural network to predict task weights could achieve comparable results.\n\n3. The experimental comparison is somewhat limited. Including more recent and competitive MTL approaches[1][2] would strengthen the evaluation.\n\n4. Additional evaluations on more diverse benchmarks (e.g., CelebA) are expected to better demonstrate generalization.\n\n5. The paper lacks theoretical insights or analysis to justify the effectiveness and convergence of the proposed RL-based optimization.\n\n6. The claim that “existing methods either lack the stochasticity needed to escape poor local minima or fail to explicitly address conflicts at the gradient level” is not sufficiently supported.\n\nReference:\n\n[1] Fair resource allocation in multi-task learning. ICML 2024.\n\n[2] Towards consistent multi-task learning: Unlocking the potential of task-specific parameters. CVPR 2025."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2O4qdHunGr", "forum": "iTslNNPfep", "replyto": "iTslNNPfep", "signatures": ["ICLR.cc/2026/Conference/Submission12548/Reviewer_5YFF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12548/Reviewer_5YFF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837451171, "cdate": 1761837451171, "tmdate": 1762923408026, "mdate": 1762923408026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TaskForce introduces a novel framework for multi-task learning (MTL) that formulates the multi-task optimization (MTO) problem as a cooperative multi-agent reinforcement learning (MARL) game. Each task is modeled as an agent that learns a policy to adaptively balance multiple tasks based on training feedback, instead of relying on heuristic rules. The method leverages the Gram matrix of task gradients to reduce dimensionality while retaining information about gradient magnitudes and inter-task alignments. A hybrid reward function combines gradient- and loss-based optimization signals, and each agent outputs a weight for its task’s gradient. The proposed approach is evaluated on three standard benchmarks. Overall the manuscript presents an interesting idea and a step away from fixed heuristics for MTL balancing and avoidance of interference to a learnable policy. However, theoretical grounding is thin, and I have some questions regarding the experiments and baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths\n\nThe paper proposes a distinctive formulation that sets it apart from the large body of heuristic-based task-weighting and gradient-aggregation methods.\n\nThe method is clearly motivated and well-documented, and several ablation studies are included (see Appendix A).\n\nThe use of a cooperative MARL framework is creative and potentially extensible to broader optimization settings."}, "weaknesses": {"value": "Weaknesses\n\nThe paper repeatedly frames r_G as “grounded in provably convergent criteria', but there is no convergence guarantee for the learned stochastic policy with mixed rewards and non-stationary dynamics. Otherwise the theoretical grounding is fairly thin to make this a fundamental progress.\n\nThe proposed method introduces many more degrees of freedom and higher computational overhead for relatively limited empirical gains.\n\nThe theoretical link between cooperative MARL and general MTL optimization remains somewhat underdeveloped. \n\nThe precise set-up of the baselines in function of gradient normalization is unclear for the QM9 benchmark.\n\nI detail the latter three points more extensively in the \"questions' section."}, "questions": {"value": "Detailed Comments and Questions\n\nCan you show that r_g + r_l avoids failure modes where pure ∥∑wg∥ minimization collapses?\n\nInformation loss in Gram matrix compression: The Gram matrix discards directional information beyond pairwise inner products, meaning distinct gradient constellations can share the same Gram matrix. Have you observed failure modes due to this compression—for instance, in degenerate or near-collinear task settings?\n\nSensitivity to λ_G: Performance appears to hinge on a very small λ_G. How sensitive are results to this hyperparameter, and what happens if λ_G is increased by an order of magnitude?\n\nTable 3 inconsistencies: Table 3 mixes relative normalizations, includes a “rough calculation” for the non-Gram variant, and lists two rows with identical “×1.00” costs despite different configurations, while one DE variant shows “×3.21” contradicting claims that DE improves efficiency. Please clarify this with consistent baselines and real wall-clock measurements, or move these results to an appendix with clearer methodology.\n\nQM9 underperformance vs. STL: All MTL methods—including TaskForce—perform worse than single-task training on QM9. Could this stem from architectural mismatch or excessive parameter sharing in relatively low-dimensional task heads? Clarifying this would help interpret the results. (A public code link would also be appreciated for verification.)\n\nGradient normalization fairness: You mention applying gradient normalization as part of TaskForce’s setup, but it’s unclear whether the same normalization was applied to baselines. Since TaskForce agents operate on normalized Gram matrices and losses (after normalization), ensuring fair comparison is important—especially on QM9, where performance differences are partly attributed to task-scale imbalances.\n\nChoice of MARL technique: How sensitive is performance to the selection of MARL algorithm (e.g., MADDPG vs. alternatives) or to the update frequency of the agents? A brief discussion would help contextualize robustness.\n\nInterpretation of weight dynamics (Fig. 4): Figure 4 shows evolving task weights, but no analysis is given. Can you interpret these dynamics—e.g., are they correlated with Gram-matrix entries, loss trends, or signs of conflict resolution among tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kqoVnaOoNT", "forum": "iTslNNPfep", "replyto": "iTslNNPfep", "signatures": ["ICLR.cc/2026/Conference/Submission12548/Reviewer_KK4j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12548/Reviewer_KK4j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995060444, "cdate": 1761995060444, "tmdate": 1762923407714, "mdate": 1762923407714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TaskForce, a novel framework that reformulates multi-task optimization as a cooperative Multi-Agent Reinforcement Learning (MARL) problem.\nTaskForce models each task as an agent that observes compact summaries of task gradients and losses and outputs a weight determining its contribution to the shared update. And the hybrid reward function combining loss reduction and gradient alignment further boost the performance on NYU-v2, Cityscapes, and QM9 datasets, achieving the SOTA results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well organized with a good writing.\n - It is novel to solve multi-task optimization task in Multi-Agent Reinforcement Learning way.\n - The use of centralized critic and shared cooperative reward stabilizes multi-agent learning and provides consistent convergence\n - The evaluation across diverse benchmarks achieves SOTA and shows robustness"}, "weaknesses": {"value": "- Benchmarks focus on homogeneous visual or molecular tasks. Cross-modality tasks remain untested.\n - Implementing MADDPG with multiple agents and critics increases hyperparameter sensitivity and tuning cost, there are concerns about **reproducibility**.\n - Learned weight policies lack interpretability; it is unclear which task dominates or how cooperation evolves during training."}, "questions": {"value": "1. How are the two weights in the reward function determined? Are there any adaptive methods?\n2. Nowadays, MLLM also face many multi-task scenarios, so why are experiments still only conducted on traditional models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HBigD9Pmwy", "forum": "iTslNNPfep", "replyto": "iTslNNPfep", "signatures": ["ICLR.cc/2026/Conference/Submission12548/Reviewer_inyc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12548/Reviewer_inyc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999108744, "cdate": 1761999108744, "tmdate": 1762923407300, "mdate": 1762923407300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}