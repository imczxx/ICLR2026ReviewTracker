{"id": "1FnCrZtBNQ", "number": 16473, "cdate": 1758264929853, "mdate": 1759897238469, "content": {"title": "LAMDA: A Longitudinal Android Malware Benchmark for Concept Drift Analysis", "abstract": "Machine learning (ML)-based malware detection systems often fail to account for the dynamic nature of real-world training and test data distributions. In practice, these distributions evolve due to frequent changes in the Android ecosystem, adversarial development of new malware families, and the continuous emergence of both benign and malicious applications. Prior studies have shown that such concept drift—distributional shifts in benign and malicious samples, leads to significant degradation in detection performance over time. Despite the practical importance of this issue, existing datasets are often outdated and limited in temporal scope, diversity of malware families, and sample scale, making them insufficient for the systematic evaluation of concept drift in malware detection.\n\nTo address this gap, we present LAMDA, the largest and most temporally diverse Android malware benchmark to date, designed specifically for concept drift analysis. LAMDA spans 12 years (2013–2025, excluding 2015), includes over 1 million samples (approximately 37\\% labeled as malware), and covers 1,380 malware families and 150,000 singleton samples, reflecting the natural distribution and evolution of real-world Android applications. We empirically demonstrate LAMDA's utility by quantifying the performance degradation of standard ML models over time and analyzing feature stability across years. As the most comprehensive Android malware dataset to date, LAMDA enables in-depth research into temporal drift, generalization, explainability, and evolving detection challenges.", "tldr": "This paper presents the largest and most diverse Android malware benchmark dataset for studying concept drift with a comprehensive temporal analysis, feature stability, and explainability.", "keywords": ["Machine Learning", "Android Malware", "Concept Drift Analysis", "Explainability", "Dataset Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/286d61ef0d974561099a4078f3d86d820d7719c4.pdf", "supplementary_material": "/attachment/a86d74bd2cacda72c39c7afdd52b6ddf8761253e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces LAMDA, a large-scale Android malware dataset spanning 12 years (2013–2025, excluding 2015) with over 1 million APK samples, designed specifically to study concept drift in malware detection. The dataset comprises approximately 37% malware samples across 1,380 families and includes static Drebin features. The authors empirically demonstrate performance degradation of standard ML models over time and analyze feature stability, providing a temporal benchmark substantially larger and more diverse than existing datasets. The paper includes comprehensive drift analysis using multiple techniques including Jeffreys divergence, t-SNE visualization, SHAP-based explanations, and label drift analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "\tScale and Temporal Scope: Over 1 million samples across 12 years with 1,380 families and 150K singleton samples provide unprecedented temporal coverage and diversity for Android malware research. This addresses a genuine gap in existing datasets. \n\tComprehensive Drift Analysis: The multi-faceted approach (supervised learning degradation, feature distribution shifts via Jeffreys divergence, feature stability scores, SHAP-based explanation drift, label drift) provides rich evidence for concept drift. The integration of multiple complementary methods strengthens the analysis. \n\tReproducibility and Scalability: Publication of feature matrices, variance threshold objects, and code supports reproducibility. The design enables extensibility to new samples, which is valuable for long-term use. \n\tRigorous Experimental Validation: The comparison between LAMDA and APIGraph on identical evaluation protocols effectively demonstrates that LAMDA exhibits stronger, more realistic drift. High standard deviations in LAMDA results versus APIGraph's stability support the claim of pronounced drift."}, "weaknesses": {"value": "Unclear Scan Consistency： The paper does not specify whether VirusTotal labels were obtained from single-pass or repeated scans. Since detection outcomes can vary across rescans, this ambiguity may introduce label inconsistency. \n \nLack of Intra-Sample Drift Analysis： The study analyzes global and family-level drift but does not consider intra-sample temporal variation—how the same APK’s features might change across time. Such analysis could better capture longitudinal behavior shifts. \n \nStatic Feature Limitation： LAMDA focuses exclusively on Drebin-style static features. While this ensures comparability, it may overlook runtime or dynamic behaviors that evolve differently, slightly limiting ecological completeness."}, "questions": {"value": "Collection Procedure and Label Stability: During dataset construction, were APKs scanned once or multiple times on VirusTotal? If repeated scans occurred, how were temporal discrepancies in detection counts handled—by selecting the earliest, latest, or majority label? Clarifying this would help assess label stability across the 12-year span. \n \nIntra-Sample Temporal Drift: Has the team examined how features or VirusTotal labels for the same APK hash change across years? This could quantify intra-sample drift and distinguish it from population-level concept drift. \n \nDynamic and Hybrid Features: Given the exclusive use of static Drebin features, do the authors plan to include dynamic runtime features (e.g., API invocation traces, network behaviors) or hybrid representations in future LAMDA versions? This would enrich longitudinal analysis and reflect real-world adaptive threats. \n \nTemporal Label Validation: Considering that VirusTotal engines evolve over time, did the authors fix specific engine versions or cross-engine consensus thresholds to mitigate version-induced label drift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "o4PARaru0S", "forum": "1FnCrZtBNQ", "replyto": "1FnCrZtBNQ", "signatures": ["ICLR.cc/2026/Conference/Submission16473/Reviewer_dzpd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16473/Reviewer_dzpd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657199832, "cdate": 1761657199832, "tmdate": 1762926579653, "mdate": 1762926579653, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present LAMDA, a malware dataset that spans 12 years and therefore is aimed at capturing classifier drop in performance due to representational drift."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work is in an area that is now not near my current area of research, thus my lower confidence score.\n\nThe dataset is large and to the best of my knowledge the longest longitudinal malware dataset collected to date. The analysis is very thorough."}, "weaknesses": {"value": "See questions"}, "questions": {"value": "In Figure (2) for LAMDA, could the authors explain why there is a large performance drop in 2017 and 2018?\n\nDrebin-style features are rather old (from 2014), could the authors support the choice for this feature set? \n\nIn Table 4 in the appendix, it seems like there are extremely low malware samples from 2023-2025 compared to the train set. This seems to coincide in Figure (2) LAMDA with a very big performance drop. I wonder if the authors have any comments on this? It seems strange to keep especially the years of 2024 and 2025 given the huge class imbalance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Fjq9eZVT0U", "forum": "1FnCrZtBNQ", "replyto": "1FnCrZtBNQ", "signatures": ["ICLR.cc/2026/Conference/Submission16473/Reviewer_GAMZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16473/Reviewer_GAMZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754844540, "cdate": 1761754844540, "tmdate": 1762926579063, "mdate": 1762926579063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce LAMDA, a temporally diverse dataset meticulously crafted to tackle the challenges of concept drift in Android malware detection. This benchmark spans 12 years and includes over 1 million samples. Additionally, the authors assess state-of-the-art concept drift adaptation methods, revealing their limitations when applied to LAMDA. This highlights the urgent need for more robust approaches in the field."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-organized.\nThe research topic is significant.\nThe experiments are sufficient."}, "weaknesses": {"value": "It lacks a clear comparison with relevant datasets.\nIt lacks specific guidance for future work."}, "questions": {"value": "1. It is suggested  to conduct a tabular comparison of the existing data to display information such as the number of malware samples, family types, and distribution over the years. This will clarify the innovative aspects of this study.\n\n2. Has the data been deduplicated? Given that many malware samples exhibit identical features at the characteristic level (even if their hash codes differ), it is crucial to know if any deduplication efforts have been made to ensure diversity in software feature.\n\n3. Please provide feasible research directions for future concept drift adaptation work based on the results of your dataset collection. For instance, does the dataset exhibit characteristics that differ from other datasets, making concept drift adaptation more challenging? While it is still recommended to continuously expand the dataset, simply increasing the amount of data is not sufficient for innovation. It is also necessary to introduce fresh perspectives and methodologies.\n\n4. I am particularly curious about the decision to submit this work to ICLR instead of a conference or journal focused on software engineering or security."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rl00hXFOxB", "forum": "1FnCrZtBNQ", "replyto": "1FnCrZtBNQ", "signatures": ["ICLR.cc/2026/Conference/Submission16473/Reviewer_x7WH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16473/Reviewer_x7WH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16473/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756745109, "cdate": 1761756745109, "tmdate": 1762926578410, "mdate": 1762926578410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}