{"id": "faxcxKINBC", "number": 17757, "cdate": 1758280165615, "mdate": 1759897155821, "content": {"title": "Sparse Imagination for Efficient Visual World Model Planning", "abstract": "World model based planning has significantly improved decision-making in complex environments by enabling agents to simulate future states and make informed choices.\nThis computational burden is particularly restrictive in robotics, where resources are severely constrained.\nTo address this limitation, we propose a Sparse Imagination for Efficient Visual World Model Planning, which enhances computational efficiency by reducing the number of tokens processed during forward prediction. \nOur method leverages a sparsely trained vision-based world model based on transformers with randomized grouped attention strategy, allowing the model to flexibly adjust the number of tokens processed based on the computational resource.\nBy enabling sparse imagination during latent rollout, our approach significantly accelerates planning while maintaining high control fidelity.\nExperimental results demonstrate that sparse imagination preserves task performance while dramatically improving inference efficiency. \nThis general technique for visual planning is applicable from simple test-time trajectory optimization to complex real-world tasks with the latest VLAs, enabling the deployment of world models in real-time scenarios.", "tldr": "To overcome the high computational cost of world models, our method uses a sparse imagination approach to achieve faster planning for real-time applications while maintaining high task performance.", "keywords": ["World Model", "Planning", "Computational Efficiency", "Model Predictive Control", "Vision Transformer"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b15345f94d34620bfed1ed8be91fb928865e0639.pdf", "supplementary_material": "/attachment/889045e7282e3d40a57c847996fe8d757aed922e.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Sparse Imagination, a simple yet effective approach for improving computational efficiency in visual world-model-based planning.The key idea is to perform model predictive control (MPC) rollouts on only a random subset of ViT patch tokens, rather than processing all of them.To make this feasible, the authors train a transformer world model with a randomized grouped-attention strategy, enabling robustness to missing visual tokens at inference time.\nDuring planning, a random dropout mask is applied dynamically at each iteration, which substantially reduces the quadratic computational cost of self-attention.Extensive experiments on seven simulated environments (e.g., Pointmaze, PushT, Rope, LIBERO-10) and a real-world LeRobot setup show that the method achieves comparable task success to the full-token baseline while reducing planning time by more than 50%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear motivation and simplicity**.\nThe paper tackles an important bottleneck of visual world-model planning (quadratic attention cost) with a conceptually simple and practical solution.\n\n2. **Strong empirical validation**. \nExperiments are extensive, covering both simulation and real robotic control tasks. Results convincingly demonstrate significant efficiency gains with minimal performance loss."}, "weaknesses": {"value": "1. **Limited algorithmic novelty**.\nWhile effective, the proposed approach mainly combines known components (token dropout + grouped attention) and lacks deeper theoretical or architectural innovation.\n\n2. **Shallow theoretical grounding**. \nThe work frames the idea as “redundancy reduction,” yet offers little formal analysis or justification of when and why sparse imagination preserves sufficient task information.\n\n3. **Empirically chosen design choices**.\nThe dropout ratio and grouping strategy are empirically chosen; no adaptive mechanism or general rule for balancing performance and speed is proposed."}, "questions": {"value": "1. Can the dropout ratio be made adaptive to uncertainty or task complexity?\n2. Does grouped attention reduce long-range spatial reasoning ability?\n3.  What is the real-world end-to-end latency improvement on hardware?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ciIBTOhaws", "forum": "faxcxKINBC", "replyto": "faxcxKINBC", "signatures": ["ICLR.cc/2026/Conference/Submission17757/Reviewer_gLWK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17757/Reviewer_gLWK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572952044, "cdate": 1761572952044, "tmdate": 1762927599250, "mdate": 1762927599250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Sparse Imagination, a simple yet effective method designed for transformer-based world models. The key idea is to accelerate planning by applying random dropout on visual tokens, significantly improving inference speed while largely preserving predictive performance. Extensive experiments and analyses demonstrate that the random dropout strategy, along with the corresponding attention mechanism used during training, is effective on a variety of robotics tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces Sparse Imagination, a surprisingly simple yet highly effective approach for accelerating world models. Counter-intuitively, such a straightforward method outperforms importance-based sampling strategies, offering both practical value and novel insight that the full patch tokens are redundant.\n\n2. The authors conduct extensive experiments to demonstrate the superiority of random dropout–based Sparse Imagination. Comparisons across multiple dropout ratios and different dropout strategies provide strong empirical support for the central claim."}, "weaknesses": {"value": "1. The real-world evaluation is limited to only a single task, which raises concerns about the robustness of the reported results. In general, I would suggest using at least three tasks, or at a minimum two different embodiments. This is particularly important because the workspace of the LeRobot arm is quite small, meaning that the effective action chunk workspace is very limited.\n\n2. I have some concerns regarding the comparative experimental settings. In the case of random sampling, does fixed imply that the same subset of tokens is dropped consistently across different frames? Concerning the blind spot analysis, the paper suggests that blind spots may originate from static aspects of the scene during initialization. In this context, are the important tokens primarily those associated with the end-effector (EEF)? Moreover, I believe the analysis would be more intuitive and convincing if the authors could provide additional visualizations and concrete examples of blind spots as part of a quality assessment.\n\n3. A minor suggestion: It would be helpful to report an average score across multiple tasks in the experiments. This would make it more intuitive to see the overall superiority of Sparse Imagination compared to other baselines or ablations."}, "questions": {"value": "1. From my understanding of this work, actions are sampled either from MPC or from a VLA policy. The world model then imagines the future states resulting from these actions. My question is: during execution, how are the candidate actions selected based on the imagined states, especially in open-ended robotics tasks where no golden state is available?\n\n2. On line 258, the authors mention that the world model can evaluate and refine trajectories. How exactly is the refinement performed? I could not find a detailed description of this process in the paper.\n\n3. On line 141, the authors mention splitting the visual tokens into two groups. What is the motivation for this design? How are the groups specifically divided — for example, first half vs. second half, interleaved, or one group being dropped out? Why does such a partition help strengthen subtoken-level patterns?\n\n4. To address the issue of full-patch visual tokens being overly redundant for world model learning, there is also the concept of latent action [1] in robotic manipulation, which models the state transition a -> s using only a small number of tokens. I would like to ask how the authors consider the relationship between sparse imagination and latent action. Have you considered making a comparison, since both approaches largely fall within the scope of robotics tasks with action annotations?\n\nReferences:\n\n[1] Latent Action Pretraining from Videos."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CZ5XaOMr2a", "forum": "faxcxKINBC", "replyto": "faxcxKINBC", "signatures": ["ICLR.cc/2026/Conference/Submission17757/Reviewer_HGmx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17757/Reviewer_HGmx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783119392, "cdate": 1761783119392, "tmdate": 1762927598827, "mdate": 1762927598827, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method for accelerating visual world-model planning by processing only a randomly selected subset of vision tokens during latent rollouts. The approach trains a transformer-based world model with randomized grouped attention so that, at test time, the planner can flexibly reduce token count without modifying the architecture. During planning, a fraction of tokens is randomly retained and used for MPC/CEM rollouts, yielding substantial computational savings with minimal degradation in performance. The work compares against learned token-importance and compression baselines, showing that random token selection matches or outperforms these more complex strategies. Experiments across serveral simulated domains and a real-world robotic task demonstrate that dropping 10–50% of tokens preserves task success while significantly reducing planning time. The authors argue that static importance metrics suffer from “blind spots” in dynamic planning settings, while random sampling avoids such failures with negligible overhead. Overall, the paper positions random token dropout as a simple and practical baseline for efficient visual planning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is generally well-written and easy to follow, with a clear problem formulation, motivation, and method description. The overall narrative is coherent and the technical contributions are communicated effectively.\n\n2. The core idea of training a transformer-based world model with randomized grouped attention such that it can perform test-time planning by randomly dropping visual patch tokens is conceptually simple, architecture-agnostic, and broadly applicable to visual world-modeling settings.\n\n3. Experiments across multiple simulated control environments (e.g., PointMaze, Wall, PushT, Rope) demonstrate that dropping approximately 50% of visual tokens yields substantial planning-time reductions (40–60%) while preserving control performance. These results show that the approach delivers consistent and meaningful efficiency gains with minimal performance degradation.\n\n4. The method is compared against learned or attention-based token selection or merging techniques (e.g., LTRP, STAR, ATC), and consistently matches or outperforms them. This provides compelling evidence that simple random sampling can avoid failure modes associated with static importance-scoring heuristics and offers a strong, low-overhead baseline for token reduction in visual planning.\n\n5. Real-robot results and integration with a Vision-Language-Action model (SmolVLA) further strengthen the empirical validation. The technique transfers effectively to real-world robotic control and maintains competitive task success while reducing real-world episode execution time by roughly half, demonstrating practical benefit in realistic deployment scenarios."}, "weaknesses": {"value": "1. The writing, particularly in the Experiments section, needs improvement for readability. Important details and baselines are placed in Appendix B.6 instead of the main text, which makes it difficult to follow comparisons and understand experimental context. Additionally, some methods (e.g., Latin Hypercube Sampling, McKay et al., 2000) are referenced without explanation, making it harder for readers unfamiliar with these techniques to interpret results.\n\n2. The paper argues that training the world model with random patch dropout induces robustness to token sparsification at test time, but this claim is supported primarily by empirical evidence. A simple formal or conceptual analysis clarifying when and why random token subsets preserve planning quality would strengthen the contribution. For instance, discussing conditions related to scene observability, task complexity, visual redundancy, and environmental structure would help clarify the boundaries of the method’s effectiveness. Explicitly connecting these factors to the observed planning success patterns would provide a stronger theoretical grounding and a clearer understanding of the method’s limitations and applicability.\n\n3. The experiments focus on goal-conditioned manipulation tasks, and it is unclear whether the approach extends to broader embodied settings (e.g., DMControl, Meta-World, RLBench, BEHAVIOR-1K, Habitat 3.0, or long-horizon household environments). Likewise, the evaluation uses a narrow set of visual encoders. Given recent advances in visual backbones (e.g., DINOv3, FastVLM, OpenVision-2, SAIL-VL2), testing across a wider set of pretrained representations would strengthen the claim that random patch sparsification generalizes beyond the chosen model family.\n\n4. The proposed approach is evaluated primarily within an MPC/CEM planning framework, with an additional VLA-guided sampling variant for real-robot experiments. However, it remains unclear whether the computational and performance benefits of this method extend to other planning paradigms. In particular, it would be useful to see results under implicit planners used in model-based RL (e.g., actor-critic methods) and learned planning approaches (e.g., transformer-based sequential planners or diffusion-policy rollouts). Because different planners have varying tolerance to approximation errors and representation sparsity, demonstrating consistent benefits would help establish that random token dropout is a generally applicable mechanism rather than one primarily suited to CEM-style sampling-based planning.\n\n5. The method relies on a manually chosen token-drop ratio, tuned per task. While the simplicity of random dropout is appealing, the lack of adaptive control limits its practicality in settings where computation budgets or scene complexity change over time (e.g., dynamic camera views or occlusions). An adaptive mechanism that adjusts sparsity based on model confidence, rollout divergence, uncertainty estimates, or planning consistency could potentially offer better performance–efficiency trade-offs. The paper briefly acknowledges this possibility, but does not explore even lightweight heuristics (e.g., annealing token sparsity or entropy-based token retention). As a result, it is unclear whether token sparsity could be automatically optimized online, which would be important for real-time deployment and applications with fluctuating computational constraints.\n\n6. Although the reported runtime savings are substantial, the compute analysis remains relatively coarse-grained. The paper primarily reports end-to-end planning time, but does not clearly decompose where efficiency gains arise (e.g., savings in vision attention layers, latent rollout computation, sampling loops, and planning iterations). Providing latency per module across token-drop ratios would help understand where the speedups come from and how to tune the method for different hardware and latency budgets."}, "questions": {"value": "1. How sensitive is the approach to the choice of token-drop ratio across environments? Did you observe cases where higher or lower sparsity meaningfully changed success rates or planning stability?\n\n2. Can the authors formalize or provide intuition for the conditions under which random token subsets preserve planning quality? For example, how do factors like scene observability, task complexity, and visual redundancy influence performance?\n\n3. Do you expect similar efficiency gains on non-manipulation tasks such as navigation, locomotion, or long-horizon household planning? Have you tested (or can you comment on) performance in these settings?\n\n4. Would this method apply to other planning paradigms beyond CEM-based MPC, such as actor-critic agents, or transformer-based learned planners?\n\n5. Can you consider providing some insight into adaptive mechanisms for adjusting the token-drop ratio at test time (e.g., confidence- or uncertainty-based heuristics)? Do you foresee a principled approach for online adaptation?\n\n6. Can the authors provide a breakdown of computation cost across components (e.g., attention layers, latent rollouts, sampling iterations, VLA calls) to clarify where efficiency gains originate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "snV3vGZLgo", "forum": "faxcxKINBC", "replyto": "faxcxKINBC", "signatures": ["ICLR.cc/2026/Conference/Submission17757/Reviewer_C7kW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17757/Reviewer_C7kW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903589657, "cdate": 1761903589657, "tmdate": 1762927598427, "mdate": 1762927598427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method called sparse imagination, which aims to increase the efficiency for test-time planning with patch token-based transformer world models while maintaining planning performance. The method works by randomly dropping half of the patch tokens when calculating attention, ensuring robust predictions on any subset of tokens. During MPC planning, an arbitrary percentage of tokens can be dropped in each iteration to accelerate world model rollouts. Experiments on seven simulated environments and one real-world environment show that the method achieves comparable planning performance to the full-patch world model, with significantly reduced planning time."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea is well-motivated, as planning time and inference cost are major concerns for patch token-based world models that encode rich information about the environment.\n- The reduction in inference and planning time is significant, while achieving comparable or even better planning success rates across various benchmarks.\n- The analysis of token information in Section 5.3 is quite interesting, providing insights into the information content and redundancy of patch features."}, "weaknesses": {"value": "- The application of the proposed method seems somewhat limited, as it only applies to world models with patch tokens, a transformer backbone, and MPC as the planning algorithm. However, the idea of randomly dropping tokens during training and inference appears more general. Could this approach be extended to other use cases?\n- At planning time, the method relies on resampling different tokens across MPC iterations to capture the full task information. For open-loop CEM planning, however, information is lost since token sampling occurs only once. How are the CEM-only results in Table 1 affected by this limitation?\n- The paper could be strengthened by evaluating patch token-based world models using features other than DINO-v2. Does the analysis of patch token information in Section 5.3 generalize to other pre-trained patch features?\n- There is no evaluation of the world model’s prediction quality when using full patches while training with random token dropout. Does this training mechanism improve or degrade the world model’s quality?"}, "questions": {"value": "- In Table 1, why does a nonzero drop ratio sometimes outperform planning with full patches? Why isn’t the success rate strictly decreasing as the drop ratio increases?\n- For the LIBERO and LeRobot tasks, the dataset used to train the world model appears to consist solely of expert trajectories, which provides limited state and action space coverage. How does the world model generate the counterfactual information essential for planning with the policy?\n- How does the token drop ratio during training affect the world model’s prediction quality?\n-  In Table 2, what does the change in time for CLS mean, given that no tokens are dropped?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Fyv0tTT1Ae", "forum": "faxcxKINBC", "replyto": "faxcxKINBC", "signatures": ["ICLR.cc/2026/Conference/Submission17757/Reviewer_ASzo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17757/Reviewer_ASzo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066539944, "cdate": 1762066539944, "tmdate": 1762927598066, "mdate": 1762927598066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}