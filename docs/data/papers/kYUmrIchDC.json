{"id": "kYUmrIchDC", "number": 15370, "cdate": 1758250678679, "mdate": 1763095821582, "content": {"title": "The Sparse Matrix-Based Random Projection: Exploring Optimal Sparsity for Classification", "abstract": "In the paper, we study the sparse $\\\\{0,\\pm1\\\\}$-matrix-based random projection, a technique extensively applied in diverse classification tasks for dimensionality reduction and as a foundational model for each layer in the popular deep ternary networks. For these  sparse matrices, determining the optimal sparsity level, namely the minimum number of nonzero entries $\\pm1$ needed to achieve the optimal or near-optimal classification performance, remains an unresolved challenge.  To investigate the impact of matrix sparsity on classification, we  propose to analyze the mean absolute deviation (MAD) of projected data points, which quantifies their dispersion. Statistically, a higher degree of dispersion is expected to improve classification performance by capturing more intrinsic variations in the original data.  Given that the MAD value depends   not only on the sparsity level of random  matrices but also on the distribution of the original data,  we  evaluate two  representative  data distributions for generality: the Gaussian mixture distribution, widely used to model complex real-world data; and the two-point distribution, available for modeling discretized data. Our analysis reveals that sparse matrices with only \\textit{one} or \\textit{a few} nonzero entries per row can achieve MAD values comparable to, or even exceed, those of denser matrices,   provided  the  matrix size satisfies $m\\geq\\mathcal{O}(\\sqrt{n})$, where $m$ and $n$ denote the projected and original dimensions, respectively. These extremely sparse matrix structures imply significant computational savings.  This finding  is further validated through classification experiments on diverse real-world datasets, including images, text, gene data, and binary-quantized data, demonstrating its broad applicability.", "tldr": "", "keywords": ["random projection", "sparse matrices", "dimensionality reduction", "mean absolute deviation", "classification"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e8d379981bec73aeedc6622b18395fccc74fdb92.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies sparse $\\{0,\\pm1\\}$ random projection matrices and investigates the optimal sparsity level for classification tasks. By analyzing the mean absolute deviation (MAD) of projected data under two representative distributions (two-point and Gaussian mixtures), the authors show that extremely sparse matrices---with only one or a few nonzero entries per row---can achieve comparable or even superior classification performance to denser matrices when $m \\ge O(\\sqrt{n})$. The results imply significant computational savings without loss of accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "S1 (Quality): Theoretical derivations are detailed, with convergence bounds and conditions for optimal sparsity explicitly characterized. Empirical results cover diverse data modalities and scales.\n\nS2 (Clarity): The motivation, assumptions, and implications are clearly stated, and the paper is generally well written.\n\nS3 (Efficiency): The results imply potentially large computational savings for random-projection-based pipelines, and offer insights relevant to large models and quantized deep networks."}, "weaknesses": {"value": "W1 (Limited Novelty): The conceptual contribution appears incremental. Extremely sparse random projections and ternary/binary structures have been extensively explored in the context of model compression and quantized neural networks, raising concerns about whether the paper provides sufficiently new theoretical insight beyond existing literature.\n\nW2 (Strong Distributional Assumptions): The analysis relies on two idealized data models—Gaussian mixture and two-point distributions. It remains unclear how well the theoretical conclusions hold under more realistic, heavy-tailed, multimodal, or correlated distributions commonly observed in large-scale datasets.\n\nW3 (Weak Link Between MAD and Accuracy): MAD is used as a surrogate for classification performance, but the paper does not rigorously justify why maximizing MAD directly correlates with improved accuracy. The connection is empirical and intuitive, lacking a formal link to decision boundaries, margin analysis, or downstream classifier behavior.\n\nW4 (Limited Baselines): Experiments only compare against Gaussian random projection. Without comparisons to other state-of-the-art dimensionality reduction or projection techniques, it is difficult to evaluate the practical advantage and competitiveness of the proposed approach."}, "questions": {"value": "Q1 (Novelty Clarification):\nPrior work has extensively investigated ternary/binary projections and extreme sparsity in the context of compression and quantized neural networks. Could the authors more clearly distinguish what new theoretical insight this paper provides beyond existing analyses? \n\nQ2 (Distributional Robustness):\nThe theoretical analysis assumes Gaussian mixture and two-point distributions. How robust are the results if data deviates from these assumptions (e.g., heavy tails, multimodal density, correlated features)? Can the authors provide empirical evidence, theoretical arguments, or discussion on how far the conclusions generalize under real-world distributional shifts?\n\nQ3 (Justification of MAD as a Surrogate):\nMAD is used as a proxy for classification accuracy, but the paper currently offers only intuitive justification. Can the authors provide a more rigorous argument or reference connecting MAD to downstream accuracy—such as its relationship to class separability, classifier margins, or error bounds? Under which conditions might maximizing MAD not translate into improved accuracy?\n\nQ4 (Baseline Coverage):\nExperiments only compare against Gaussian random projection. To better evaluate practical competitiveness, could the authors include additional baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A3ZSyD8qxF", "forum": "kYUmrIchDC", "replyto": "kYUmrIchDC", "signatures": ["ICLR.cc/2026/Conference/Submission15370/Reviewer_Cdb8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15370/Reviewer_Cdb8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729121885, "cdate": 1761729121885, "tmdate": 1762925653682, "mdate": 1762925653682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Dear Chairs and Reviewers, \n\nWe would like to thank you for taking  the time to review our paper. Unfortunately, yet fortunately, two reviewers have identified an error in our Property 1. While we can devise a solution to resolve this problem, it would require substantial modifications to the entire manuscript. In light of this, we wish to withdraw the current submission and publish it in the future.\n\nBest regards,\n\nAuthors"}}, "id": "LDPiUWrMQM", "forum": "kYUmrIchDC", "replyto": "kYUmrIchDC", "signatures": ["ICLR.cc/2026/Conference/Submission15370/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15370/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763095820419, "cdate": 1763095820419, "tmdate": 1763095820419, "mdate": 1763095820419, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new random projection method, where the projection matrix is constructed by $\\{0, \\pm 1\\}$. The authors establishes a connection between matrix sparsity and classification performance via MAD analysis, providing a new perspective beyond traditional distance preservation. It seems that the analysis in this work relies heavily on specific data distributions. Therefore, I recommend weak accept."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors conduct numerical experiments using six datasets (images, text, genes, binary) to support the theoretical claim."}, "weaknesses": {"value": "The limitation of the assumption for the original data: The analysis relies heavily on specific data distributions (e.g. Gaussian mixture in the manuscript)"}, "questions": {"value": "How to generalize your framework to more general original data distributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3y8YQvKqzx", "forum": "kYUmrIchDC", "replyto": "kYUmrIchDC", "signatures": ["ICLR.cc/2026/Conference/Submission15370/Reviewer_W43D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15370/Reviewer_W43D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918803073, "cdate": 1761918803073, "tmdate": 1762925653144, "mdate": 1762925653144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript suggests that sketching with sparse matrices tends to preserve classification accuracy in practice. Motivated by this observation, the authors study the maximum absolute deviation (MAD) of the projected data and argue that this framework explains the effectiveness of sparse sketching compared to dense matrices, such as Gaussian ones."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper reads well."}, "weaknesses": {"value": "I think certain parts of the paper should be explained better, and some statements appear to be incorrect:\n\n- In lines 58–61, the authors suggest that sketching with sparse matrices preserves accuracy well and often performs comparably to or better than dense sketching. However, no citation is provided. A citation should be provided, as this is the motivation of the paper.  \n- I could not follow why MAD is a good metric for understanding classification performance. The authors suggest that MAD is a more robust metric for quantifying dispersion; however, neither the text nor the examples involve heavy-tailed models or outliers. As written, the claim that MAD is a better alternative is not convincing.  \n- There are some incorrect statements in the paper that should be addressed:  \n  - In line 175, it is argued that when the original data $h$ follow the Gaussian mixture distribution described above, the projected data $z$ remain Gaussian. This appears to be incorrect.  \n  - In line 179, it is argued that this relationship also holds approximately for original data $h$ drawn from other distributions, since by the Central Limit Theorem, the projected data $z \\in \\mathbb{R}^m$ can be approximated by a Gaussian distribution. This claim is too broad, since it depends heavily on the dimension and on the tail properties of the data in high-dimensional settings.  \n  - In line 364, the authors suggest that MNIST follows a two-point distribution. On what basis? Please clarify."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wrg1EMEUT5", "forum": "kYUmrIchDC", "replyto": "kYUmrIchDC", "signatures": ["ICLR.cc/2026/Conference/Submission15370/Reviewer_9s5R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15370/Reviewer_9s5R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762213745951, "cdate": 1762213745951, "tmdate": 1762925652712, "mdate": 1762925652712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the optimal sparsity level for $\\{0, \\pm 1}$-matrix random projections in classification tasks by analyzing the dispersion of projected data points under two representative data distributions. The main finding is that extremely sparse matrices with only one or a few nonzero entries per row can match or exceed the classification performance of much denser alternatives, offering significant computational savings. This result is validated experimentally across diverse real-world datasets including images, text, gene expression data, and binary data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles why extremely sparse random projections can still perform well in classification despite violating traditional distance-preservation guarantees, which is an interesting problem to tackle.\n- The proposal to use Mean Absolute Deviation (MAD) as a proxy for classification performance seems to be a novel departure from standard $l_2$ or variance-based analyses. It potentially provides a new lens through which to analyze the problem.\n- The theoretical findings are tested against a wide and diverse range of datasets, including images (YaleB, CIFAR100, ImageNet1000), text , and gene data."}, "weaknesses": {"value": "1. **Connection between Disperson and Classification:** A core concern I have about the proposed view is the connection between dispersion and classification. The authors mention in the abstract that, *\"Statistically, a higher degree of dispersion is expected to improve classification performance by capturing more intrinsic variations in the original data.\"*  However, I could not find anywhere in the paper where the author theoretically examines this. Since, this is a theory paper proposing this central hypothesis of MAD being a proxy for classification, I think this should be theoretically explored. \n    - I am also not convinced that PCA is a good example when supporting this view, as the goal of PCA is reconstruction and not class separability. \n   - If dispersion is the metric, it means everything is dispersed by the projection. I think inter/between-class variance relative to intra/within-class variance would be the proper metric for classification? How does MAD capture these nuances and how do the authors study this point?\n\n2. **Claim about $z$ being Gaussian:**  The paper invokes the closure of Gaussian distributions under linear transformations to claim that the projected variable $z = Rh$ “remains Gaussian.” However, in section 2.2.2 $h$ is modeled as a Gaussian mixture (not Gaussian). Hence, $z$ is, in general, a *mixture* of Gaussians rather than a single Gaussian. This misapplication of the closure property undermines the validity of later derivations that rely on i.i.d. Gaussian assumptions (e.g., the MAD identity). Can the authors clarify?\n\n3. In terms of proof presentation, the central point of the proof could be summarized much more neatly in the main paper and more of the technical details could be stowed away in the appendix."}, "questions": {"value": "Please, see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pLSlubub35", "forum": "kYUmrIchDC", "replyto": "kYUmrIchDC", "signatures": ["ICLR.cc/2026/Conference/Submission15370/Reviewer_H7au"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15370/Reviewer_H7au"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15370/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762237153586, "cdate": 1762237153586, "tmdate": 1762925652228, "mdate": 1762925652228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}