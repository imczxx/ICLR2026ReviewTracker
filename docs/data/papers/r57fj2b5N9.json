{"id": "r57fj2b5N9", "number": 18491, "cdate": 1758288270548, "mdate": 1759897099882, "content": {"title": "Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think", "abstract": "Large Language Models (LLMs) leverage step-by-step reasoning to solve complex problems. Standard evaluation practice involves generating a complete reasoning trace and assessing the correctness of the final answer presented at its conclusion. In this paper, we challenge the reliance on the final answer by posing the following two questions: Does the final answer reliably represent the model's optimal conclusion? Can alternative reasoning paths yield different results? To answer these questions, we analyze intermediate reasoning steps, termed subthoughts, and propose a method based on our findings. Our approach involves segmenting a reasoning trace into sequential subthoughts based on linguistic cues. We start by prompting the model to generate continuations from the end-point of each intermediate subthought. We extract a potential answer from every completed continuation originating from different subthoughts. We find that aggregating these answers by selecting the most frequent one (the mode) often yields significantly higher accuracy compared to relying solely on the answer derived from the original complete trace. Analyzing the consistency among the answers derived from different subthoughts reveals characteristics that correlate with the model's confidence and correctness, suggesting potential for identifying less reliable answers. Our experiments across various LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025) show consistent accuracy improvements, with gains reaching up to 13% and 10% respectively. Implementation will be made publicly available.", "tldr": "", "keywords": ["reasoning; efficiency; llms"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/149da3cda398a17d620772557ce367a9bba66236.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how the answer of an LLM evolves across different reasoning stages. To do so, the authors segment the reasoning trace of an LLM into a bunch of subthoughts, based on a set of handcrafted delimiters. Then the authors obtain the answers for intermediate stages by prompting the LLM with partial reasoning traces that are constructed from concatenation of subthoughts. By visualizing the distribution of answers over intermediate stages, the authors found three distinct patterns for the final answer and the majority of intermediate answers. It is observed that correct predictions generally have a lower entropy for the intermediate answer distribution than incorrect ones. Finally, the authors show that the majority of intermediate answers, in most cases, perform better than the final answer for several LLMs on AIME 2024 and AIME 2025."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper provides an interesting analysis of the intermediate reasoning stages of LLMs, and obtains several insights.\n2. The writing of this paper is easy to follow."}, "weaknesses": {"value": "1. Most contributions of this paper has already been done in previous papers. The technique of prompting models to continue partial reasoning traces has already been studied in [1, 2] for reward modeling, and [3] for understanding LLM reasoning behaviors. The lesson that correct and incorrect solutions can be distinguished by consistency patterns have also been found in [3]. Moreover, [3] made one step further by showing this property can be used to build a verifier to detect errors, which this paper claims as a potential but doesn’t have any experiment. Besides, the conclusion that majority voting outperforms greedy decoding has been known for a few years since self-consistency[4]. Unless you can show the majority of answers from intermediate stages outperform self-consistency under ISO tokens or ISO time, the conclusion of this paper isn’t significant to the community.\n2. Experiments are not quite enough to support the conclusions of this paper. For example, when comparing the distribution of intermediate answers and the final answer, the authors only show 3 qualitative samples for each model. It’s necessary to have quantitative results showing the portion of each pattern. The authors claimed that entropy has the potential for detecting errors, but this claim is not well supported by experiments. It’s better to have an explicit experiment on error detection, or verifier like in [3]. The conclusion on majority voting performance is obtained only on two datasets, AIME 2024 and AIME 2025, which are even highly correlated. This level of experimental validation falls short of the current standards for LLM research. Finally, the performance in Figure 4 conflicts with the conclusion in Line 365.\n3. Some goals or claims in the introduction are not fully addressed. Line 48-49 discusses a more reliable assessment, but the rest of this paper doesn’t directly investigate the assessment of LLM reasoning ability. Line 109 mentions respective advantages for greedy and non-greedy sampling, but I didn’t find any advantage of greedy sampling in this paper.\n\n[1] Wang et al. Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations. ACL 2024.\n\n[2] Ton and Taufiq et al. Understanding Chain-of-Thought in LLMs through Information Theory. ICML 2025.\n\n[3] Zhou, Zhu and Li et al. Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models. arXiv 2025.\n\n[4] Wang et al. Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR 2023."}, "questions": {"value": "1. Related work: The term training-based reasoning is not very common in the community. Better rephrase it as finetuning models or post-training models for reasoning.\n2. Citations in the related work section should use \\citep, unless they serve as the subject of a sentence. The authors used many \\citet there.\n3. What’s the relationship and positioning of your method against related work? This hasn’t been well discussed in the related work section.\n4. Sec 3.1: Why must the initial reasoning traces generated by greedy sampling?\n5. The subthought transition markers are likely to be model dependent. Do you have an automatic way of detecting markers?\n6. Line 227-229: Do you ask the model to continue partial thoughts (i.e. unclosed </thought>) before getting an answer, or directly generate the answer (i.e. closed </thought>)?\n7. What’s the average number of generations and tokens you need? How are they compared to self-consistency?\n8. Line 298-299: For \\boxed{} format, why not use math_verify?\n9. Could you try other type of datasets, such as multi-choice datasets? [3] used multi-choice datasets and their results on entropy (termed as uncertainty in their paper) seem to be different."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fODgxORJ1h", "forum": "r57fj2b5N9", "replyto": "r57fj2b5N9", "signatures": ["ICLR.cc/2026/Conference/Submission18491/Reviewer_WMNG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18491/Reviewer_WMNG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18491/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603466677, "cdate": 1761603466677, "tmdate": 1762928186518, "mdate": 1762928186518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how much information is hidden inside a single reasoning trace from an LLM. The authors split a greedy trace into sequential subthoughts using fixed linguistic markers such as Wait, Now, and Let me double-check, then restart the model from each cumulative prefix to produce a full continuation and a final answer. They extract these answers and aggregate them by the most frequent value, that is the mode. Experiments on AIME2024 and AIME2025 with seven models show that mode aggregation often raises accuracy compared to judging only the last answer from the original trace."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The core idea is clearly communicated and easy to follow. Splitting a single trace into subthoughts, restarting from each prefix, and aggregating the resulting answers gives a simple way to use signals that the final answer alone ignores.\n\n- The paper provides a careful quantitative evaluation. Across two challenging math reasoning datasets and seven LLMs, the method improves accuracy over the standard final-answer-only baseline, showing the effectiveness of the proposed method."}, "weaknesses": {"value": "- he generalization of the subthought procedure is unclear beyond models that emit explicit step-by-step traces. The method depends on surface markers like Wait and Now to define cut points. The experiments cover seven models trained on step-by-step data that already produce rich traces, but models that are not trained for stepwise reasoning may not expose reliable markers, which would limit the ability to segment or to restart from meaningful states.\n\n- Attribution of gains between subthought restarts and answer voting is unclear. The pipeline restarts from many intermediate states and then selects the mode of the resulting answers. Prior work [1] has shown that sampling many full CoT solutions and selecting the most consistent final answer can improve accuracy, even without subthought segmentation. The paper compares last-answer accuracy to mode aggregation over subthought restarts, but it does not isolate how much of the lift comes from exposing multiple partial start points versus simply increasing the number of votes.\n\n- The analysis is limited. For instance, Figure 4 reports a few negative or zero-gain instances (such as DeepScaleR-1.5B-Preview’s -6.66\\% on AIME2024 Greedy), but these are hand-waved as “noise” rather than examined for pattern or causes.\n\n\n[1] Fu et al., \"Deep think with confidence.\" arXiv preprint arXiv:2508.15260, 2025.\n\n[2] Wang et al., \"Self-consistency improves chain of thought reasoning in language models.\" ICLR, 2023"}, "questions": {"value": "- Following up on W2, a controlled baseline could be considered, for example a self-consistency setting that uses the same total number of continuations, the same decoding settings, and a single start state. Without that control, it is hard to conclude that subthought restarts add value beyond voting.\n\n- Typos: Page 3: The most straingtforward approach… -> straightforward; Page 12: subthough -> subthought; Page 12: highlighing -> highlighting"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4WCFLaPT8l", "forum": "r57fj2b5N9", "replyto": "r57fj2b5N9", "signatures": ["ICLR.cc/2026/Conference/Submission18491/Reviewer_kvVE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18491/Reviewer_kvVE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18491/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945934163, "cdate": 1761945934163, "tmdate": 1762928186098, "mdate": 1762928186098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a decoding method for improving LLMs' performance in math reasoning. Similar to self-consistency/majority-voting, the proposed method aggregates multiple sampled answers instead of relying on a single answer -- by performing a separate rollout starting from each sub-step in an originally sampled complete reasoning trace. The empirical evaluations show that the proposed method outperforms sampling just once on AIME2024 and AIME2025."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed decoding method shows better performance than sampling only once.\n\n2.  The analysis conducted shows a correlation between the model's performance and confidence on the different math reasoning problems."}, "weaknesses": {"value": "1. An important baseline is missing for comparison, which is self-consistency decoding. Since the proposed method requires multiple (partial) rollouts and answer aggregation (i.e., taking a majority vote), conceptually it is very similar to self-consistency. However, there is no empirical comparison with self-consistency decoding with a similar inference budget. A potential argument might be that the proposed method is more computational efficient than self-consistency decoding. However, this would need to be empirically validated.\n\n2. More datasets should be considered, especially considering the limited sample size of AIME2024 and AIME2025. Moreover, datasets on other reasoning domains other than MATH should also be included to increase the comprehensiveness of the empirical evaluations.\n\n3. The proposed method relies on certain hardcoded phrases (\"Subthought Transition Markers\"), which raises concerns regarding the generalizability of the proposed method w.r.t. both the datasets and the models used. Further discussion and empirical examination should be conducted."}, "questions": {"value": "Please see the Weaknesses section. How does the proposed method compare to self-consistency decoding under a similar inference budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gC8u5x9ytR", "forum": "r57fj2b5N9", "replyto": "r57fj2b5N9", "signatures": ["ICLR.cc/2026/Conference/Submission18491/Reviewer_cNcg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18491/Reviewer_cNcg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18491/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971719856, "cdate": 1761971719856, "tmdate": 1762928185607, "mdate": 1762928185607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The method:\n\n1. Generate a full reasoning trace via greedy decoding.\n2. Split the trace into subthoughts using linguistic markers (e.g., “Wait,” “Now,” “Let me”).\n3. Restart reasoning from each subthought, prompting the model to complete the solution.\n4. Extract an answer from each continuation.\n5. Aggregate these answers—most effectively via mode voting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I feel the paper is written well and also motivated well. The internal mechanisms of reasoning traces is worth looking into, although I have doubts regarding the experimental sections. Please see weaknesses."}, "weaknesses": {"value": "1. Just AIME 2024/25 is too small for conclusion to be drawn\n2. The overall method seems like a variant of self-consistency combined with MCTS and majority voting. SC is already known to be better than vanilla CoT. I think just comparing with A_last does not give enough insights, one needs to look at other strong baselines such as SC-CoT\n3. a relatively less concerning weakness is heuristic based segmentation"}, "questions": {"value": "1. Have you tried other datasets ? \n2. please see the weaknesses raised"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MaoG5TWsRo", "forum": "r57fj2b5N9", "replyto": "r57fj2b5N9", "signatures": ["ICLR.cc/2026/Conference/Submission18491/Reviewer_gZXx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18491/Reviewer_gZXx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18491/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018913641, "cdate": 1762018913641, "tmdate": 1762928184556, "mdate": 1762928184556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}