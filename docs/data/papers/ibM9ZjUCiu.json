{"id": "ibM9ZjUCiu", "number": 15340, "cdate": 1758250402684, "mdate": 1762925795353, "content": {"title": "SEM: Reinforcement Learning For Search-Efficient Large Language Models", "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated their capabilities not only in reasoning but also in invoking external tools, particularly search engines.\nHowever, teaching models to discern when to invoke search and when to rely on their internal knowledge remains a significant challenge. \nExisting reinforcement learning approaches often lead to redundant search behaviors, resulting in inefficiencies and over-cost. \nIn this paper, we propose SEM, a novel post-training reinforcement learning framework that explicitly trains LLMs to optimize search usage.\nBy constructing a balanced dataset combining Musique and MMLU, we create scenarios where the model must learn to distinguish between questions it can answer directly and those requiring external retrieval.\nWe design a structured reasoning template and employ Group Relative Policy Optimization (GRPO) to post-train the model’s search behaviors. \nOur reward function encourages accurate answering without unnecessary search while promoting effective retrieval when needed. \nExperimental results demonstrate that our method significantly reduces redundant search operations while maintaining or improving answer accuracy across multiple challenging benchmarks.\nThis framework advances the model's reasoning efficiency and extends its capability to judiciously leverage external knowledge.", "tldr": "", "keywords": ["Agent", "Reinforcement Learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c79417067bc1bc038d7cecf36ea39e1effc89112.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The topic of the paper is interesting. It addresses an important question of how to enable large language models (LLMs) to decide when to act as a research agent that performs external searches versus when to directly answer a question using their internal knowledge. \nHowever, the overall quality of the submission falls below the expected standard. The paper suffers from poor presentation — figures, equations, and templates are presented in a disorganized and visually unappealing way. The evaluation is limited, covering only two datasets and a very small number of baselines, which makes it difficult to assess the claimed effectiveness. Moreover, the novelty is weak, as the approach closely resembles prior work in this area without clear conceptual advancements. Finally, the paper lacks essential implementation details such as training configuration and data statistics, which severely impacts reproducibility. Overall, while the topic itself is promising, the current version of the paper does not meet the bar for publication."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic is interesting."}, "weaknesses": {"value": "1. Poor Presentation Quality\nThe presentation quality is below the expected standard, giving the impression that the paper was hastily prepared rather than carefully polished. For instance, in Section 2.2 (Reward Formulation), the equation unnecessarily occupies almost half a page, which significantly reduces readability. Similarly, in Section 2.3 (Training Template), the format is presented in raw text, which looks unprofessional and visually unappealing. A structured table or figure would convey the information more clearly and effectively.\n\n2. Limited Evaluation Datasets\nThe paper can be categorized as developing a research agent, similar to prior work such as [1]. In this line of research, it is standard to evaluate on multiple multi-hop QA datasets (e.g., HotpotQA, MuSiQue, 2Wiki, and Bamboogle). However, the paper only reports results on the first two, which makes the evaluation incomplete and prevents fair comparison with existing work.\n\n3. Insufficient Baseline Models\nGiven the large number of recent papers on this topic, the choice of only two baseline models is inadequate. A more comprehensive comparison — including recent and competitive baselines — is essential to demonstrate the effectiveness and relevance of the proposed method.\n\n4. Lack of Novelty\nThe overall novelty of the work is limited. The method appears to follow a similar framework to recent research agents, without introducing substantial conceptual or algorithmic innovations.\n\n5. Missing Implementation Details\nThe implementation details are very unclear. Key information, such as the learning rate, number of training steps, number of training examples, and data split proportions, is not provided. It seems like the authors don't know we can write unlimited pages in the Appendix. \n\n\n\nReferences:\n[1] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen. R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning. 2025"}, "questions": {"value": "- What are the training details of the framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qD2GRmClQq", "forum": "ibM9ZjUCiu", "replyto": "ibM9ZjUCiu", "signatures": ["ICLR.cc/2026/Conference/Submission15340/Reviewer_GEzQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15340/Reviewer_GEzQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761688806477, "cdate": 1761688806477, "tmdate": 1762925631033, "mdate": 1762925631033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "t7eKOKnjKu", "forum": "ibM9ZjUCiu", "replyto": "ibM9ZjUCiu", "signatures": ["ICLR.cc/2026/Conference/Submission15340/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15340/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762925794200, "cdate": 1762925794200, "tmdate": 1762925794200, "mdate": 1762925794200, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SEM, a post-training reinforcement learning framework for optimizing search behavior in Large Language Models. The authors address the problem of redundant search operations by training models to distinguish between questions they can answer directly using internal knowledge and those that require external retrieval. The approach uses a balanced dataset combining MuSiQue (unknown questions) and MMLU (known questions), and employs Group Relative Policy Optimization (GRPO) to optimize the search behavior."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Well-Motivated Problem**. The dynamic reasoning/tool-calling issue that this paper aims to solve is a good, practical question.\n* **Good performance**. The method shows consistent improvements across multiple benchmarks."}, "weaknesses": {"value": "1. **Limited Technical Novelty**. The paper fails to discuss or compare with established methods that address similar adaptive retrieval problems. Notable omissions include Self-RAG, Adaptive-RAG, and subsequent works that have proposed various solutions for determining when retrieval is necessary. The absence of these discussions makes it difficult to assess the novelty and relative merits of the proposed approach.\n2. **Incomplete Experimental Analysis**. For agentic search agents, recent strong baselines such as Search-R1 and related follow-up work are not included in comparisons. For adaptive retrieval methods, Self-RAG, Adaptive-RAG, and similar approaches are absent from the evaluation; The experiments use Qwen2.5 models while the latest Qwen-3 series models are not evaluated.\n3. **Missing Ablation Studies**. No ablation experiments are provided to understand the contribution.\n4. **Notation and Typography**. The paper contains several typographical errors, particularly in punctuation."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5ySccpBKK2", "forum": "ibM9ZjUCiu", "replyto": "ibM9ZjUCiu", "signatures": ["ICLR.cc/2026/Conference/Submission15340/Reviewer_Y7Am"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15340/Reviewer_Y7Am"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954150179, "cdate": 1761954150179, "tmdate": 1762925630505, "mdate": 1762925630505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces new post-training framework based on reinforcement learning that encourages the model to adaptively search for questions that require external knowledge and do not search when external knowledge is not needed. To do this, they design a custom rollout style where the model first predicts the response and then based on the initial response decides if search is needed or not. To encourage models to do this, they utilize a custom reward model where encourages the model to first correctly answer the question, then invoke search if the initial response is wrong, and also considers the format. Their experimental results show that they perform well in comparison with the included baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper studies an important problem because if they do this successfully, it can decrease the cost of training and inference. \n- The reward model designed in this paper is reasonable."}, "weaknesses": {"value": "This paper has a very limited contribution that also doesn’t perform well compared to other studies. For example, [1] achieve much better results on the same datasets without including this adaptive scoring in their reward model. Additionally, many baselines for RAG is not provided that can be further included to make comparison fair. These baselines can be find in [1] paper. Another baseline that studies adaptive retrieval is [2] that should be included. In general, the introduced methods work better than included baselines, but compared to literature (even those with very similar idea) it is not effective.\n\n\n[1] Search-r1: Training llms to reason and leverage search engines with reinforcement learning\n[2] Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity"}, "questions": {"value": "Why does your method not work as well as [1] or [2] (cited in the weaknesses section) despite using a more complicated reward function that encourages this adaptive retrieval behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2dTF0hMX0M", "forum": "ibM9ZjUCiu", "replyto": "ibM9ZjUCiu", "signatures": ["ICLR.cc/2026/Conference/Submission15340/Reviewer_jdqJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15340/Reviewer_jdqJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049565412, "cdate": 1762049565412, "tmdate": 1762925629760, "mdate": 1762925629760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SEM, a reinforcement learning (RL) framework that train LLMs to efficiently use a search engine to answer open-domain queries. The authors observe that existing models often perform redundant, unnecessary searches despite LLMs can already answer with their embedded knowledge. Therefore, SEM addresses this by training the model on a balanced dataset combining MMLU & MuSiQue, SEM uses GRPO with a reward function that have higher values with correct predictions and less search calls. The experimental results show that SEM can reduces redundant search operations while maintaining or improving answer accuracy across datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed SEM significantly reduces redundant search operations by training the model to exploit its internal knowledge on simple queries while perform searching when external knowledge is required.\n\n2. The paper introduces a reward function that explicitly penalizes unnecessary searches and rewards effective retrieval, successfully improving search LLMs to distinguish queries it knows and those it doesn't, and only search for those complex ones."}, "weaknesses": {"value": "1. This paper essentially combines reasoning and QA datasets to train a search agent that prioritieze answer correctness followed by the number of searches, with no novel technique / settings introduced.\n\n2. Lack of datasets & baselines. Many baselines like IRCot and Search-R1 are not adopted as baselines in this paper, and further reasoning / QA datasets like MATH & 2Wiki should also be considered to improve the evaluation.\n\n3. Limited training, analysis & observations to demonstrate the efficacy of SEM. For instance, the paper only compares limited baseline methods in Figure 2, whic does not reveal any intrisic advantages / stability of SEM over ReSearch. The authors should also provide more insights / discussions on why SEM may be better than other RL-based search agents and provide potential savings on the API costs."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A6wcqYGIJ1", "forum": "ibM9ZjUCiu", "replyto": "ibM9ZjUCiu", "signatures": ["ICLR.cc/2026/Conference/Submission15340/Reviewer_DMkk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15340/Reviewer_DMkk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15340/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138136655, "cdate": 1762138136655, "tmdate": 1762925628962, "mdate": 1762925628962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}