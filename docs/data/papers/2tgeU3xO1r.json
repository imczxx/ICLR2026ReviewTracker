{"id": "2tgeU3xO1r", "number": 23914, "cdate": 1758350289824, "mdate": 1759896790663, "content": {"title": "Origins and roles of world representations in neural networks", "abstract": "While neural representations have been extensively studied in large practical models, the controlled conditions that govern their emergence and their downstream role in model adaptation remain poorly understood. We introduce a World–Data–Model framework that separates the underlying world (fixed city coordinates) from data generation (seven geometric tasks) and model training, enabling causal study of how tasks shape representations and how those representations mediate fine-tuning. We show that training on single tasks yields divergent representational geometries despite an identical underlying world, whereas multi-task training drives alignment, providing controlled evidence for the Platonic Representation Hypothesis. Building on this, we investigate adaptability by adding 100 synthetic “Atlantis” cities and fine-tuning on subsets of tasks. Surprisingly, representational divergence measured in single-task pretraining predicts downstream failure: tasks with low representational alignment not only fail to support cross-task generalization but actively hinder it by isolating new entities from the shared manifold. Overall, our framework establishes a model system for holistically studying the origins, alignment, and adaptability of neural world representations.", "tldr": "We introduce a synthetic setup to study how emergent world representations affect generalization.", "keywords": ["World Representation", "Fine-tuning", "Generalization", "Transformers"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11216858e6865c51116e24c640c0eef9fa64c257.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The present paper proposes a framework for studying representations in neural networks. In particular, the authors separate input data (\"world\") from task (\"data generation\"), and then study how training on single and multiple tasks effects the emerging representations. They find that training on single tasks yields divergent representational geometries, whereas multi-task training drives alignment. Furthermore, their results indicate that representational divergence measured in single-task pretraining predicts downstream failure during finetuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper was overall easy to follow and read. The broader question (how to make sense of neural network representations) is of both intellectual and practical interest. From a conceptual perspective, I liked the proposed seperation between data and task. In terms of results, the paper showed very cleanly that multi-task learning leads to more aligned model internal representations."}, "weaknesses": {"value": "While the authors argue that their work represents evidence for the platonic representation hypothesis, that is at best only weakly the case. True evidence would require studying representations of different model architectures, which is not done presently. \n\nRelated work is not much discussed. I am not too familar with this line of work, but surely people have studied neural network representations in multi-task settings before. How does the present work connect to this and in which aspects does it differ?\n\nPersonally, I found most of the results not very surprising and somewhat limited in impact. While the methodology is sound and the analysis thorough, the findings largely align with my prior expectations.\n\nThe authors argue that \"generalization performance correlates with the CKA values from single-task pretraining.\" While that is true, the relationship is fairly weak.\n\nLimiting the analysis to seven tasks seems constraining, especially if one eventually wants to transfer the insights to realistic settings.\n\nMinor:\n* Figure labels generally very small.\n* Figure 1 not referenced in the text.\n* Figure 4 abbreavtions not defined.\n* Figure 6 not referenced in the text.\n* Figure 6 dual axis very confusing without color coding."}, "questions": {"value": "What is the actual input to the transformer? I assume it is just the entire string? Is it the same for the single- and multi-task settings? This is never explicitly mentioned. How are strings tokenized? Everything on the character-level except for city ids?\n\nHow consistent in their representations are single-task models across mutliple runs? That seems like an important control condition.\n\nCrossing fails to train alone. Why? That seems strange given that this is a fairly simple setup.\n\n\"Despite these differences, we can still linearly decode (x,y) coordinates from most tasks, as shown in the second row of Fig. 4.\" Where can I see this in the figure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZbVjeGOtCE", "forum": "2tgeU3xO1r", "replyto": "2tgeU3xO1r", "signatures": ["ICLR.cc/2026/Conference/Submission23914/Reviewer_taAU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23914/Reviewer_taAU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761486801788, "cdate": 1761486801788, "tmdate": 1762942852567, "mdate": 1762942852567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a study investigating how neural networks (transformers specifically) learn convergent representations of a single latent data manifold through different tasks, or combinations thereof. They use real world cities to define a set of latent coordinates, and come up with seven function learning tasks, mapping city tuples to outputs that the networks are trained to predict. The authors show that, when trained on single tasks, the models learn representations that tend to by similar (measured by CKA), but also show structural differences. By constraining the representation space by training the models on more tasks, the representations start to align more. This nicely demonstrates a principle often formalized as the Platonic Representation Hypothesis. Lastly, the authors analyze representations when models are fine-tuned to incorporate a novel (fictitious) city."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The authors conduct simple and diagnostic experiments. The results make sense and support the conclusions made in the paper\n* Nice diagnostic test of the platonic representation hypothesis in a toy setting"}, "weaknesses": {"value": "* The introduction meanders on very general questions related to representation learning and neural networks, not easy to see how all of these are related to the questions the paper actually studies. I would recommend making the introduction more succinct and to-the-point.\n\nOverall, the paper provides good evidence for a simple and interesting question. While it's not very surprising that multi-task training constrains the model representations, giving rise to alignment, the evidence presented is solid so I'm happy to recommend accept.\n\nFormatting:\n* On line 383 there seems to be a reference missing (see the question mark)"}, "questions": {"value": "* Were other model training factors tested? For instance, regularization (L1 or L2) on the residual stream representations might speed up alignment, as constraints are put on the representations.\n* Does multi-task alignment interact with model size? Were differently sized transformers trained?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UMGN6LZgkC", "forum": "2tgeU3xO1r", "replyto": "2tgeU3xO1r", "signatures": ["ICLR.cc/2026/Conference/Submission23914/Reviewer_oLe1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23914/Reviewer_oLe1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766487698, "cdate": 1761766487698, "tmdate": 1762942852317, "mdate": 1762942852317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a simple framework to analyze how different training objectives influence the learned representations when the underlying world model is known. To make this analysis tractable, the authors construct a synthetic setup where the “world” consists of 2D city coordinates. Data are generated using seven different geometric tasks based on these coordinates. This controlled environment allows the authors to systematically study several aspects of representation learning: First, they show that models trained on most of these tasks can learn representations that are linearly mappable to the underlying 2D coordinates, thereby capturing the correct world model. Second, they demonstrate that combining multiple tasks improves this alignment with the true world representation; for some tasks, such multi-task training is even necessary for success. Lastly, they investigate fine-tuning effects: when a general-purpose model is fine-tuned on a subset of tasks with additional data, the choice of fine-tuning task is crucial. In some cases, new “cities” embed seamlessly into the existing latent geometry, while in others they occupy a separate region of the space. This behavior also determines whether the fine-tuned model generalizes across tasks or becomes specialized to the task it was fine-tuned on"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a well-designed framework for systematically analyzing how training objectives affect learned representations when the underlying world model is known. The idea of generating tasks based on a shared, low-dimensional representation is elegant and enables controlled, interpretable experiments. The seven tasks are well-chosen and require different forms of geometric reasoning. The overall problem setup is clearly explained, and Figure 2 effectively illustrates the environment and task construction. By isolating the training objective from other factors such as architecture or data complexity, the findings become easier to interpret.\nThe visualization in Figure 3 provides an intuitive view of how the world model emerges during training, demonstrating that this emergence is not necessarily directly correlated with task performance. Overall, the proposed framework serves as a valuable analytical tool for studying and bench-marking representation learning methods."}, "weaknesses": {"value": "- The study appears to rely on a single random seed for training. It would be important to evaluate whether the learned representations vary more across different tasks than across different random initializations of the same task. Without such analysis, it is difficult to assess the stability of the reported findings.\n- The character-based city encoding is somewhat unconventional and insufficiently justified. It is unclear how the cities are indexed or numbered—if the numbering follows geographic order, this could inadvertently leak coordinate information into the model. Furthermore, the authors note in the appendix that city coordinates starting with “0”, “00”, or “000” fail to work and were excluded from all experiments. This exclusion raises concerns about potential implementation artifacts or biases in the input representation.\n- In the fine-tuning experiments, the new coordinates are concentrated in a small region of the space, leading to clustering of new cities. This represents a special case of localized additional information, while experiments using more spatially distributed new points would help evaluate whether the conclusions generalize.\n- The city manifold used in this framework is a flat 2D plane, while real-world data often lie on more complex manifolds. Extending the approach to non-Euclidean geometries, such as a spherical globe, would be an interesting next step and could test the robustness of the proposed framework.\n- The paper provides only a limited explanation of what constitutes “divergent tasks.” A deeper discussion of the specific geometric reasoning required by each task and why certain tasks diverge would help clarify this concept.\n- Minor comments:\n   - The caption of Figure 3 mentions “top,” “middle,” and “bottom” panels, but the figure appears as a single mixed layout. \n   - The caption and description of Figure 5 refer to a 21×21 CKA matrix, while the plot shows only 7×7."}, "questions": {"value": "- As the Crossing task did not succeed on its own, did it work in combination with any other tasks, or only with Distance and Perimeter? In addition, while Distance appears to perform well in early experiments, it is later described as a divergent task. Could this divergent behavior be an artifact of the fine-tuning data, where the new cities (“Atlantis”) are concentrated in a single coordinate region?\n- What are the accuracies of the linear probe used for coordinate prediction across different settings—models trained on single tasks, combined tasks, and the fine-tuned variants? Including these results would help quantify how well each representation captures the true world geometry.\n- How is Normalized Improvement defined? Please clarify how it is calculated. Also, it is unclear, whether the deviation from max-model is in percentage or absolute units."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0Xoznmnwyl", "forum": "2tgeU3xO1r", "replyto": "2tgeU3xO1r", "signatures": ["ICLR.cc/2026/Conference/Submission23914/Reviewer_8dPS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23914/Reviewer_8dPS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919934981, "cdate": 1761919934981, "tmdate": 1762942851996, "mdate": 1762942851996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper performs a controlled analysis of how world models are formed in autoregressive models. 'World representations' are not fixed by the world itself but by the data‐generation tasks: single tasks carve up internal geometry very differently; training on diverse tasks forces these geometries to align ('Platonic' convergence). \n\nThe authors find:\n\n-World representations emerge under autoregressive training: models first cluster nearby cities, then form a world‑aligned geometry; (x,y) becomes linearly decodable before task accuracy jumps.\n\n-Single‑task training induces distinct geometries (e.g., distance yields a thread‑like structure, angle a 2‑D manifold)\n\n-Multi‑task pretraining aligns representations: average CKA increases with task count (1→2→3), including between models that share no tasks"}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-The controlled setup for the experiments makes the results presented in this work convincingly support the idea of Platonic Representations presented in earlier work. By decoupling the world, data, and model, the authors control exactly what changes (tasks vs. world) and show models learn only from task outputs, never coordinates, so any alignment effects can be attributed to task diversity rather than data confounds.\n\n-Setup is straightforward and training is possible at the level of academic resources. (i.e a 6‑layer, 128‑hidden, 4‑head Transformer with a 98‑symbol ASCII vocabulary, trained autoregressively). Fine‑tuning likewise uses manageable data.\n\n-The addition of the Atlantis fine-tuning is a creative illustration of how downstream performance is impacted by the learned 'world representation' during pretraining. This provides a clean test of whether the 7‑task‑pretrained world manifold can absorb new entities and generalize them across tasks. Fine‑tuning on a single task with Atlantis yields a generalization matrix whose gains vary by task and correlate with pretraining CKA, directly tying the geometry learned during pretraining to downstream performance."}, "weaknesses": {"value": "Since this paper focuses on analysis, most of the issues I encounter in this paper are focused on the clarity of presentation:\n\n-Many figures are way too small (e.g. Figure 3-7), as a rule, the figure text should be sized similarly to the paper text\n\n-Citations are missing in some parts of the paper (e.g. ? citations)\n\n-Colors shown on World Map are not indicated with a legend.\n\nFigure 8 is hard to tell the difference or what should be noticed in the contrast between the PCA/Linear probe subfigures for non-divergent vs divergent tasks. Are there quantitative measures that quantify how the new entities are different in the non-divergent vs divergent conditions?\n\nIf the goal is to demonstrate that the coordinates are placed in an orthogonal subspace and lie close to the origin, it would be more helpful to quantify it numerically rather than showing it visually.\n\nFigure 8a is also hard to interpret. Without a clear way to interpret what the x-axis means, it's hard to understand what each entry in the matrix denotes and, consequently, what the vertical bands indicate."}, "questions": {"value": "\"This suggests that divergent tasks cause optimization to encode new entities in orthogonal subspaces rather than integrating them into the existing world manifold—explaining their failure to support cross-task generalization.\"\nCan the authors make a statement (admittedly extrapolative) about how this is handled in the real-world by current models (e.g. LLMs) trained on data that may encode divergent tasks? Presumably, the data in the real world will not be as consistent as in the idealized setting posed in this paper. \n\n\"we do not claim that interventions to increase single-task CKA would necessarily improve fine-tuning generalization.\"\nWhat are the author's thoughts on the intervention and how that would impact generalization? Was this intervention tried?\n\n\"Even excluding models with shared tasks, we find substantially higher CKA compared to single-task models\" Can these pairs be isolated better from Figure 5c? Perhaps the matrix can be structured in a way where the partial overlap entries can be localized. I think non-overlapping tasks having higher alignment is an important result because it shows the common anchor is the World Map (i.e. the 'Platonic Space')"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aBV4Cx47Er", "forum": "2tgeU3xO1r", "replyto": "2tgeU3xO1r", "signatures": ["ICLR.cc/2026/Conference/Submission23914/Reviewer_PuHx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23914/Reviewer_PuHx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23914/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010028525, "cdate": 1762010028525, "tmdate": 1762942851720, "mdate": 1762942851720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}