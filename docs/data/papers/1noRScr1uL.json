{"id": "1noRScr1uL", "number": 20908, "cdate": 1758311586970, "mdate": 1759896952608, "content": {"title": "MARché: Fast Masked Autoregressive Image Generation with Cache-Aware Attention", "abstract": "Masked autoregressive (MAR) models unify the strengths of masked and autoregressive generation by predicting tokens in a fixed order using bidirectional attention for image generation. While effective, MAR models suffer from significant computational overhead, as they recompute attention and feed-forward representations for all tokens at every decoding step, despite most tokens remaining semantically stable across steps. We propose a training-free generation framework MARché to address this inefficiency through two key components: cache-aware attention and selective KV refresh. Cache-aware attention partitions tokens into active and cached sets, enabling separate computation paths that allow efficient reuse of previously computed key/value projections without compromising full-context modeling. But a cached token cannot be used indefinitely without recomputation due to the changing contextual information over multiple steps. MARché recognizes this challenge and applies a technique called selective KV refresh. Selective KV refresh identifies contextually relevant tokens based on attention scores from newly generated tokens and updates only those tokens that require recomputation, while preserving image generation quality. MARché significantly reduces redundant computation in MAR without modifying the underlying architecture. Empirically, MARché achieves up to 1.7x speedup with negligible impact on image quality, offering a scalable and broadly applicable solution for efficient masked transformer generation. The code is available at https://anonymous.4open.science/r/MARche-26F0.", "tldr": "", "keywords": ["Efficient AI", "Image generation", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a757d5fe58ea64fc9718674773460f26d5487d68.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This manuscript introduces \"MARché,\" a training-free inference acceleration framework specifically designed to optimize the image generation process of Masked Autoregressive (MAR) models. MAR models exhibit significant computational redundancy during inference: attention and feed-forward network (FFN) computations are recalculated for all tokens at each step, even though most tokens remain unchanged. MARché addresses this by leveraging KV Cache through two key components:\n\n1. Cache-aware Attention: Partitions tokens into \"Active\" and \"Cached\" sets, designing distinct computation paths (Active tokens recompute QKV and pass through FFN; Cached tokens only provide KV and skip FFN) while maintaining mathematical equivalence.\n\n2. Selective KV Refresh: Recognizes that caches cannot be used indefinitely. This mechanism analyzes attention scores from the 2nd decoder layer to identify and select the Top-K cached tokens most relevant to the currently generating tokens as \"Refreshing Tokens,\" adding them to the active set for forced updates. It also incorporates periodic full refresh (default: every 3 steps) to prevent error accumulation.\n\nWithout modifying the MAR model structure or requiring retraining, MARché significantly reduces inference computation. Experiments on the ImageNet 256x256 dataset show that MARché provides up to 1.7x (approaching 1.8x for MAR-H) inference speedup for MAR models, with minimal impact on generation quality (FID/IS)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Inference Acceleration**: The core contribution is practical and impactful, achieving speedups of 1.57x to nearly 1.8x for MAR models (Table 1).\n\n2. **Training-Free**: As a pure inference-time optimization, MARché is easy to adopt and applicable to existing pretrained MAR models without costly retraining.\n\n3. **Preservation of Generation Quality**: The method maintains high fidelity, with only slight increases in FID while significantly reducing latency (Table 1, Fig 9).\n\n4. **Clever Mechanism Design**: The combination of cache-aware attention (with efficient kernel implementation) and attention-guided selective KV refresh is well-motivated and technically sound.\n\n5. **Clear Exposition and Thorough Experimentation**: The paper effectively explains the redundancy problem in MAR models and clearly details the MARché design. It provides comprehensive experimental validation, including extensive ablation studies, strongly supporting the methodology and the role of each component."}, "weaknesses": {"value": "1. **Dependence on MAR Architecture**: MARché is specifically optimized for MAR models (with fixed generation order and bidirectional attention). It's unclear if this optimization can be extended to other types of masked generative models (like MaskGIT with potentially different generation strategies) or models without a fixed order. Its generality might be limited by the MAR architecture itself.\n\n2. **Hyperparameter Sensitivity**: The effectiveness of selective KV refresh seems dependent on several key hyperparameters, such as the layer chosen for selecting refresh tokens, the number K of refresh tokens (dynamic strategy vs. fixed value), and the frequency of periodic full refresh. While ablations are provided, choosing these hyperparameters might still require careful tuning for specific applications.\n\n3. **Insufficient Comparison with Similar Methods**: The paper mentions another MAR acceleration work, LazyMAR (Yan et al., 2025), and notes differences (reusing hidden states vs. KV cache, changing vs. preserving generation order), but lacks a direct performance comparison (speedup vs. FID). This makes it difficult for readers to assess the relative advantages of MARché among similar approaches."}, "questions": {"value": "**Generality of the Refresh Token Selection Layer:**\n\nQuestion: The paper chooses the 2nd layer's attention scores to determine refresh tokens, showing it's a good speed-quality balance (Fig 5). Is this choice optimal across different model scales (MAR-L, MAR-H) and potentially other datasets or tasks? For instance, might deeper models benefit from scores derived from a deeper layer (e.g., Layer 3, whose Top-K selection aligns more closely with even deeper layers, see Fig 4) to maintain quality? Discussion or experiments on generality are suggested.\n\n**Comparison of Dynamic K vs. Fixed K Refresh Strategies:**\n\nQuestion: The current strategy dynamically adjusts the number of refresh tokens (K) to fill a batch size of 64. How does performance (speed vs. FID) change if a strategy with a fixed K (e.g., always refreshing the Top-50 tokens) is adopted? Does the dynamic K strategy lead to significant fluctuations in computation per step? Which strategy is preferable for practical deployment?\n\n**Potential for Adaptive Periodic Full Refresh Frequency:**\n\nQuestion: The current approach uses a fixed 3-step cycle for full refresh. Considering that token changes might differ across generation stages (e.g., early stages generating outlines vs. late stages filling details), could an adaptive full refresh strategy (e.g., dynamically adjusting frequency based on token change rate or generation stage) further optimize performance?\n\n**Applicability of MARché to Other Bidirectional Attention Models:**\n\nQuestion: The core idea of MARché (identifying and caching less-changing KVs) doesn't seem strictly limited to MAR models with a fixed order. To what extent do the authors believe this method could be applied to other iterative generation models employing bidirectional attention (e.g., certain types of image editing models, non-autoregressive iterative refinement for language models, or MaskGIT models with non-fixed generation orders)? What would be the main challenges?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yXmEYabnQh", "forum": "1noRScr1uL", "replyto": "1noRScr1uL", "signatures": ["ICLR.cc/2026/Conference/Submission20908/Reviewer_L52V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20908/Reviewer_L52V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761463385846, "cdate": 1761463385846, "tmdate": 1763000005240, "mdate": 1763000005240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MARché a training-free acceleration framework for Masked Autoregressive (MAR) image generation models. The method addresses computational redundancy in MAR models through two core components: (1) cache-aware attention that partitions tokens into active and cached sets with independent computational paths, and (2) selective KV refresh that identifies context-relevant tokens for recomputation based on attention scores. Experimental results demonstrate that MARch achieves 1.7× inference speedup on ImageNet 256×256 with negligible impact on image quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The KV caching mechanism is innovatively adapted from autoregressive language models for masked generation tasks, effectively solving the cache staleness issue via selective refresh. The approach is elegantly designed, with its implementation details clearly articulated.\n2. The method is architecture-agnostic and training-free, making it easily integrable into existing MAR frameworks."}, "weaknesses": {"value": "1. The method incorporates multiple empirical hyperparameters (e.g., fixed active set size of 64, full refresh every 3 steps, use of Layer 2 attention scores). Although ablation studies demonstrate their effectiveness, deeper theoretical analysis or principled design guidance is lacking. For instance, is a fixed size of 64 always optimal across varying image complexities or generation steps? It is suggested to discuss the robustness of these hyperparameters in different scenarios or explore an adaptive mechanism for determining the active set size.\n2. Missing analysis of memory overhead: While KV caching accelerates computation, it inevitably increases memory usage. The paper does not quantify or discuss the additional memory overhead introduced by MARché.\n3. Experiments are conducted only on ImageNet 256×256, lacking validation on higher-resolution datasets such as 512×512 or 1024×1024.\n4. Insufficient comparison with recent works: The paper mentions LazyMAR in the related work section but does not include it as a baseline in the experiments. A direct performance (speed/quality) comparison with LazyMAR is needed.Additionally, while the proposed method achieves a 1.7× speedup, the IS metric decreases significantly, which is not observed with LazyMAR. Please explain the reason for this discrepancy.\n5. The discussion of limitations in the conclusion is relatively brief. For example, during stages where the generated content changes abruptly (e.g., transitioning from background to foreground objects), the stability of KV projections may decrease. Could this affect the efficiency and quality of MARché?"}, "questions": {"value": "1. Although ablation studies demonstrate their effectiveness, deeper theoretical analysis or principled design guidance is lacking. For instance, is a fixed size of 64 always optimal across varying image complexities or generation steps?\n2. The discussion of limitations in the conclusion is relatively brief. For example, during stages where the generated content changes abruptly (e.g., transitioning from background to foreground objects), the stability of KV projections may decrease. Could this affect the efficiency and quality of MARché?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y4hLrVCcq9", "forum": "1noRScr1uL", "replyto": "1noRScr1uL", "signatures": ["ICLR.cc/2026/Conference/Submission20908/Reviewer_54c7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20908/Reviewer_54c7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742661748, "cdate": 1761742661748, "tmdate": 1762938412780, "mdate": 1762938412780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MARché, a training-free decoding scheme that speeds up masked autoregressive (MAR) image generation by avoiding redundant recomputation. It splits tokens each step into an active set (newly generated + a few contextually relevant “refresh” tokens) and a cached set whose key/value projections are reused via cache-aware attention; relevance is decided by attention from newly generated tokens (selective KV refresh). This preserves full-context modeling (using an online-softmax merge that’s equivalent to standard attention) while skipping unnecessary FFN work. On ImageNet 256×256, MARché reports up to ~1.7× latency speedup vs. MAR with only minor FID/IS changes, and requires no architecture changes or retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Training-Free Speedup: MARché offers a significant speedup in masked autoregressive (MAR) image generation without requiring any retraining, making it highly efficient for real-time applications.\n\nEfficient Use of Cache: By reusing key/value projections from previous steps through cache-aware attention, the method avoids redundant calculations, which reduces computational load while preserving high-quality generation.\n\nNo Architecture Modifications: It doesn't require any changes to the underlying model architecture, making it easy to integrate into existing systems.\n\nPerformance Gains: The paper reports a 1.7× latency reduction with only minor decreases in image quality metrics like FID (Fréchet Inception Distance) and IS (Inception Score), showing that it delivers efficiency without significantly compromising performance.\n\nBroad Applicability: The approach can be applied to other autoregressive models without much modification, offering potential for wide adoption in various image generation tasks."}, "weaknesses": {"value": "1 The acceleration gains from the proposed MARché method are modest. Despite achieving a 1.7× speedup, the improvement may not be significant enough for certain applications.\n\n2 The method was primarily tested on the ImageNet 256×256 dataset. Its performance and acceleration effect on larger-scale tasks, such as high-resolution images, video generation, or multi-modal data, remain unclear. If the method does not perform well in these scenarios, its general applicability could be limited.\n\n3 The method lacks significant novelty, as it primarily builds on existing techniques like cache management and selective KV refresh, and its scope is limited to optimizing a specific approach in the rapidly advancing field of autoregressive image generation."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gqBkxvWA3m", "forum": "1noRScr1uL", "replyto": "1noRScr1uL", "signatures": ["ICLR.cc/2026/Conference/Submission20908/Reviewer_5nNx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20908/Reviewer_5nNx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777060654, "cdate": 1761777060654, "tmdate": 1762938396805, "mdate": 1762938396805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MARCHÉ, a training-free acceleration method for masked autoregressive (MAR) image generation. The key idea is to reuse KV representations that remain stable across decoding steps by maintaining cached tokens and selectively refreshing active ones via attention signals and online softmax. The method reports up to ~1.7× speedup on MAR models with minor quality changes and requires no model modification."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Training-free & plug-and-play: No architectural change, no retraining, practical for deployment.\n- Clear and practical idea: Efficient KV reuse tailored to masked autoregressive generation."}, "weaknesses": {"value": "1. Lack of evaluation across different MAR schedules\n  - The KV-reuse assumption is tested under a single MAR step schedule.\n  - It is unclear whether the method generalizes to different decoding steps.\n\n2. KV-cache strategy generality remains uncertain\n  - Only evaluated on MAR.\n  - It is unknown how well the approach applies to other iterative masked Autoregressive generative frameworks.\n\n3. Comparison to LazyMAR needs clarification\n  - LazyMAR is also a training-free MAR acceleration method. However, the paper does not clearly demonstrate superior performance over LazyMAR (e.g., matched-speed or matched-quality comparisons). A direct quality-speed Pareto comparison would be necessary to substantiate claims of advantage."}, "questions": {"value": "1. KV Memory Layout Question\nHave you explored pre-allocating a single contiguous KV buffer and directly writing both cached tokens and newly active tokens into it during loading, instead of handling them in separate buffers and fusing the output? In principle, constructing a contiguous KV block upfront could eliminate the concatenate step and enable one attention kernel call on a regularized sequence length, which may yield higher efficiency.\n\n2. Question on the “generation order” claim\nI am not a domain expert, but from reading LazyMAR, my understanding is that it keeps the predefined MAR decoding schedule (i.e., which tokens are decoded at step t) and only dynamically decides which tokens can reuse features within that step to avoid recomputation. In other words, LazyMAR seems to reuse features across layers but does not change which tokens are revealed at each decoding step.\nGiven this, I am not fully understanding the statement that LazyMAR “departs from MAR’s predefined generation order.” Could you clarify in what sense the generation order is altered? If LazyMAR truly modifies the token decoding order, it would be helpful to reference the specific mechanism or evidence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QstkOjTfUY", "forum": "1noRScr1uL", "replyto": "1noRScr1uL", "signatures": ["ICLR.cc/2026/Conference/Submission20908/Reviewer_TpJn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20908/Reviewer_TpJn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055087553, "cdate": 1762055087553, "tmdate": 1762938338200, "mdate": 1762938338200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}