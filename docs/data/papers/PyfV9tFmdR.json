{"id": "PyfV9tFmdR", "number": 8754, "cdate": 1758097176191, "mdate": 1763772398880, "content": {"title": "SpectralGCD: Spectral Concept Selection and Cross-modal Representation Learning for Generalized Category Discovery", "abstract": "Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes. Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information. However, they treat modalities independently and incur high computational cost. We propose SpectralGCD, an efficient and effective multimodal approach to GCD that uses CLIP cross-modal image-concept similarities as a unified cross-modal representation. Each image is expressed as a mixture over semantic concepts from a large task-agnostic dictionary, which anchors learning to explicit semantics and reduces reliance on spurious visual cues. To maintain the semantic quality of representations learned by an efficient student, we introduce Spectral Filtering which exploits a cross-modal covariance matrix over the softmaxed similarities measured by a strong teacher model to automatically retain only relevant concepts from the dictionary. Forward and reverse knowledge distillation from the same teacher ensures that the cross-modal representations of the student remain both semantically sufficient and well-aligned. Across six benchmarks, SpectralGCD delivers accuracy comparable to or significantly superior to state-of-the-art methods at a fraction of the computational cost.", "tldr": "SpectralGCD uses CLIP similarity scores as a unified cross-modal representation, expressing images as a mixture of concepts for GCD. It exploits a teacher to select relevant concepts, and knowledge distillation to preserve semantic quality.", "keywords": ["Generalized Category Discovery", "Spectral Filtering", "Semi-Supervised Representation Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68f59013eaf2eafc3e9ee541a61eaf0c9e43315e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes SpectralGCD, a multimodal approach for Generalized Category Discovery (GCD) that leverages CLIP’s cross-modal image–concept similarities as a unified representation. Instead of treating visual and textual modalities independently, SpectralGCD represents each image as a mixture over a large task-agnostic concept dictionary, which is then filtered via a novel Spectral Filtering mechanism based on eigendecomposition of a cross-modal covariance matrix derived from a frozen teacher model. The method further employs forward and reverse knowledge distillation to preserve semantic fidelity during student training. Evaluated across six benchmarks, SpectralGCD achieves state-of-the-art or competitive performance with significantly lower computational cost than existing multimodal GCD methods, while maintaining efficiency comparable to unimodal baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) The core idea of using CLIP’s cross-modal similarities as a sufficient representation for GCD is both conceptually elegant and practically effective, grounding classification in explicit semantics and reducing overfitting to spurious visual cues.  \n(2) Spectral Filtering provides an automated, unsupervised way to prune irrelevant concepts from a large dictionary without relying on LLM-generated descriptions or manual curation, improving both representation quality and computational efficiency.  \n(3) The combination of forward and reverse knowledge distillation ensures strong alignment between student and teacher cross-modal representations, which is empirically validated through Spearman correlation and ablation studies.  \n(4) The method achieves state-of-the-art results across diverse benchmarks (fine- and coarse-grained) while being significantly faster to train than other multimodal approaches like GET and TextGCD, making it suitable for real-world deployment scenarios requiring repeated discovery."}, "weaknesses": {"value": "(1) It is hard to figure out the novelty as there is many works that constructs hierarchal fine-grained knowledge when performing tasks. Also, the paper assumes access to a “Tags” dictionary derived from benchmark datasets, but it is unclear how generalizable this dictionary is to truly out-of-domain tasks (e.g., medical or satellite imagery). While OpenImages-v7 is tested, both dictionaries are still vision-centric and curated from existing classification datasets. Could the authors clarify whether SpectralGCD would still perform well with a generic, non-vision-specific concept set (e.g., WordNet or Wikipedia titles), and what minimum coverage or semantic alignment is required between the dictionary and the target domain?\n(2) Spectral Filtering relies on computing the full cross-modal covariance matrix G ∈ ℝ^{M×M}, where M is the dictionary size (~20K). For very large dictionaries (e.g., 100K+ concepts), this becomes memory-prohibitive (O(M²) storage). The paper mentions efficiency but does not discuss scalability limits of Spectral Filtering. Did the authors explore approximations (e.g., randomized SVD, Nyström) for larger M, and what is the practical upper bound on dictionary size given current GPU memory constraints?\n(3) The distillation loss uses softmax-normalized similarities σ(zˆi) and σ(zˆi∗), but CLIP’s original logit scaling already includes a temperature τ. The paper sets τ = 0.01 for both teacher and student (Appendix A), yet the distillation loss applies another softmax. This may over-smooth or distort the relative concept rankings. Could the authors justify this design choice and provide ablation results comparing raw cosine similarities vs. softmax-normalized logits in the distillation objective?\n(4) The student only fine-tunes the last transformer block of ViT-B/16, while the teacher is ViT-H/14. This architectural mismatch raises questions about the fairness of distillation: the student has far fewer parameters and less capacity. Would the performance gap between SpectralGCD and TextGCD shrink if both used the same backbone size? Also, why not use a ViT-B/16 teacher for a more direct comparison of the representation learning strategy alone?\n(5) The evaluation protocol follows standard GCD practice, but all benchmarks assume that the unlabeled set contains a known split of Old and New classes (e.g., 50/50 in CIFAR100). How sensitive is SpectralGCD to imbalanced Old/New ratios in the unlabeled data? For instance, if New classes dominate (>80%), does the method still avoid collapsing New clusters into Old prototypes, and how does entropy regularization interact with such shifts?\n(6) The paper claims that cross-modal representations reduce overfitting to Old classes, but Figure 3 and Table 7 show that on Stanford Cars, the “Image Features” variant actually achieves higher Old accuracy (93.4 vs. 92.6) than the cross-modal version. This contradicts the stated benefit. Could the authors explain this anomaly and clarify under what conditions cross-modal representations might sacrifice Old performance for New gains—or vice versa?\n(7) The preparation phase for SpectralGCD (194s on CUB) includes precomputing teacher representations and performing Spectral Filtering. However, if new unlabeled data arrive incrementally (as mentioned in the introduction), does the entire filtering step need to be recomputed? If so, this could undermine the claimed efficiency in dynamic settings. Please clarify whether the filtered dictionary Cˆ is fixed after initial filtering or must be updated with new data.\n(8) The reverse distillation term L_rd = −σ(zˆi) log σ(zˆi∗) penalizes the student for assigning high probability to concepts the teacher deems unlikely. However, if the teacher itself is biased or misaligned with the true semantics of a novel class (e.g., mislabeling a “sparrow” as “eagle”), wouldn’t reverse distillation reinforce this error? How robust is the method to teacher mistakes, especially on fine-grained novel categories where even strong CLIP models struggle?"}, "questions": {"value": "Please see Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "j3MGbBrOQa", "forum": "PyfV9tFmdR", "replyto": "PyfV9tFmdR", "signatures": ["ICLR.cc/2026/Conference/Submission8754/Reviewer_hK7S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8754/Reviewer_hK7S"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761625247980, "cdate": 1761625247980, "tmdate": 1762920540444, "mdate": 1762920540444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of changes"}, "comment": {"value": "We thank all reviewers for their comments and constructive criticism of our submission. Their invaluable input has helped us improve our work thanks to their insights and probing questions. In summary, on the basis of reviewer comments, we have:\n\n+ enhanced motivations for why spectrally filtered cross-modal representations are discriminative with additional empirical evidence (**Main Paper Section 4.2** and **Appendix J**);\n+ incorporated new experiments evaluating spectral filtering under partial splits (an incremental scenario) and under new/old class imbalance (**Appendix I** and **Appendix H**);\n+ expanded computational analysis, including inference-time efficiency (**Appendix F**) and scalability in terms of number of concepts of our approach (**Appendix D**).\n+ added additional ablations on student backbone capacity (**Main paper**, **Section 5.3**);\n+ included additional analysis on the distillation weight (**Main paper**, **Section 5.3**);\n+ extended dictionary evaluation, incorporating experiments using WordNet (**Appendix D**);\n+ performed a fairness assessment of using a strong teacher through experiments on the NEV dataset unseen by CLIP (**Appendix E.1**); and\n+ included new experiments jointly fine-tuning the text encoder together with the vision encoder (**Appendix G**).\n\n**Note on References and Revisions:** All tables, figures, and sections cited throughout this rebuttal refer to the **revised manuscript**. To facilitate review, all additions and modifications to the PDF manuscript are highlighted in *blue* for easy identification."}}, "id": "YIUKebcOxe", "forum": "PyfV9tFmdR", "replyto": "PyfV9tFmdR", "signatures": ["ICLR.cc/2026/Conference/Submission8754/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8754/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8754/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763772545267, "cdate": 1763772545267, "tmdate": 1763772545267, "mdate": 1763772545267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses Generalized Category Discovery, aiming to find novel categories using limited labeled data. It introduces SpectralGCD, which builds a unified cross-modal representation by expressing images as mixtures over CLIP-derived semantic concepts. A teacher-guided Spectral Filtering selects relevant concepts via a cross-modal covariance matrix, and bidirectional knowledge distillation keeps the student’s representations aligned and semantically sufficient. On six benchmarks, SpectralGCD matches or surpasses SOTA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The idea of using the cross-modal representations is interesting.\n\n(2) The paper is clearly written and easy to follow.\n\n(3) The performance is promising."}, "weaknesses": {"value": "(1) Using VLMs (e.g., CLIP) for GCD risks data leakage, as these models may have been exposed to images or names of the “unknown” classes. Prior work (e.g., GET) evaluates on splits unseen by CLIP to mitigate this. Please discuss this issue and, if possible, include experiments on CLIP-unseen splits or provide a robustness analysis addressing the leakage problem.\n\n(2) What is the performance when using ViT-B/16 as the teacher or using ViT-H/14 as student? To what extent do the gains stem from distillation from a larger teacher rather than the proposed components? An ablation varying teacher and student capacity (e.g., ViT-B vs ViT-H) would help isolate the contribution.\n\n(3) Please report or elaborate on the zero-shot performance of the CLIP models in Table 1, to contextualize the improvements over zero-shot.\n\n(4) The KD component seems fairly standard and lacks technical novelty. Please clarify the insight beyond common KD practices.\n\n(5) What are the total inference costs compared with multimodal methods (GET, TextGCD) and unimodal baselines (SimGCD)? Latency would clarify efficiency trade-offs, as the proposed approach appears quite complex.\n\n(6) Have you evaluated fine-tuning the text encoder? Reporting this result would be informative.\n\n(7) The abstract states: “Training a parametric classifier solely on image features often leads to overfitting to old classes.” Is this primarily due to the absence of labeled images for the novel classes during training, which biases the classifier toward seen (old) categories? Please clarify more about this issue."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "viO7k3VTTJ", "forum": "PyfV9tFmdR", "replyto": "PyfV9tFmdR", "signatures": ["ICLR.cc/2026/Conference/Submission8754/Reviewer_GDAc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8754/Reviewer_GDAc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000181351, "cdate": 1762000181351, "tmdate": 1762920539895, "mdate": 1762920539895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of Generalized Category Discovery (GCD), where models often overfit to known classes. The authors propose SpectralGCD, a multimodal approach that represents images not by their visual features directly, but as a mixture over semantic concepts from a large dictionary. This \"cross-modal representation\" is derived from CLIP-based image-concept similarities. The core technical contribution is \"Spectral Filtering,\" a method that automatically prunes the concept dictionary by performing an eigendecomposition on a cross-modal covariance matrix derived from a strong teacher model, retaining only concepts deemed most informative. The learning process for a smaller student model is then guided by a combination of standard GCD losses and a forward-and-reverse knowledge distillation objective to align its representations with the teacher's."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses the Generalized Category Discovery  task, focusing on the common problem where models overfit to the labeled \"Old\" classes and perform poorly on unlabeled \"New\" classes.\n\n2. Proposed Core Idea: It introduces a novel \"cross-modal representation\" for each image. Instead of using raw image features, it represents an image as a vector of similarity scores against a large, task-agnostic dictionary of semantic concepts, computed using a pre-trained CLIP model.\n\n3. To refine this representation and reduce noise from irrelevant concepts, the paper proposes \"Spectral Filtering.\" This technique uses a strong teacher model to compute a cross-modal covariance matrix across the entire dataset. Through eigendecomposition (PCA), it identifies and retains concepts that contribute most to the principal components (i.e., high-variance directions) of the concept-similarity space."}, "weaknesses": {"value": "1. The primary weakness lies in the justification for \"Spectral Filtering\". The motivation is to select \"task-relevant\" concepts. However, the mechanism (performing PCA on the global cross-modal covariance matrix) selects concepts that explain the most variance across the dataset. High variance does not necessarily equate to high discriminative power or task relevance. For example, a common background (e.g., 'sky', 'grass') present across many different classes could easily form a principal component with high variance. The method might then prioritize these non-discriminative concepts. \n\n2. The paper makes a conceptual leap by equating \"high contribution to dataset variance\" with \"semantic relevance for classification\" without providing a strong theoretical or empirical argument to support this crucial link.\n\n3. The paper feels more like a report on a successful engineering recipe than a deep scientific inquiry. This lack of insight limits the paper's contribution. An outstanding paper should not only present a method that works but also provide the understanding that allows the community to build upon its core ideas. SpectralGCD in its current form feels more like a well-tuned heuristic than a principled approach, making it less inspiring for future exploration.\n\n4. In the task of discovering general categories, there has already been similar work [1] that decomposes objects into combinations of various attributes (textual or visual). I believe there needs to be more comparative discussion with the current work.\n\n5. In addition, there has been progress in the discussion on the information represented by the covariance matrix of features in general category discovery. A comparison with those works [2,3] should be made.\n\n\n[1] Dissecting Generalized Category Discovery: Multiplex Consensus under Self-Deconstruction. In ICCV, 2025.\n\n[2] Generalized Category Discovery via Token Manifold Capacity Learning. In Arxiv, 2025.\n\n[3] Continual Generalized Category Discovery: Learning and Forgetting from a Bayesian Perspective. In ICML, 2025."}, "questions": {"value": "1. The core assumption of Spectral Filtering is that concepts contributing most to the variance of the cross-modal covariance matrix are the most \"relevant\". Could you provide a more rigorous justification for this? How does this method distinguish between concepts that are genuinely discriminative and those that are simply common or part of a shared background, which could also lead to high variance?\n\n2. The paper frames the problem as representing an image as a \"mixture over semantic concepts.\" This is an appealing analogy to topic modeling. However, the current implementation simply uses a linear projection on the similarity vector. Did you explore enforcing a probabilistic constraint (e.g., ensuring the representation is a valid probability distribution over concepts) to more closely follow this analogy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XHbycbnzOI", "forum": "PyfV9tFmdR", "replyto": "PyfV9tFmdR", "signatures": ["ICLR.cc/2026/Conference/Submission8754/Reviewer_YwSE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8754/Reviewer_YwSE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058553413, "cdate": 1762058553413, "tmdate": 1762920539385, "mdate": 1762920539385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new GCD method inspired by probablistic models.\n\nThey model images as mistures over semantic concepts in CLIP embedding space.\n\nThey use spectral filtering to filter out irrelevant concepts from a large task agnostic dictionary.\n\nThey achieve better accuracy on standard benchmarks than existing unimodal and multimodal approaches to GCD."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of spectral filtering on a dictionary of concepts is cute.\n- The paper is quite complex. There are multiple stages to the proposed method. Yet the proposed method is still efficient."}, "weaknesses": {"value": "- The authors claim that the dictionary of concepts is task agnostic. But this isn't really true right? There must be some overlap between the dictionary and the concepts of the target dataset. Otherwise it would not work.\n- GCD is useful for discovering concepts in a dataset that do not fit neatly into the existing label set. However, I would argue that the new assumptions that the authors are introducing render the task of GCD meaningless.\n  - In particular, the authors use a Teacher model CLIP H/14 that has been pretrained on all the concepts (both new and old) across all the benchmarks. So the \"novel classes\" can't really be considered novel. \n  - It would be more impressive if the authors test their method on datasets that have a smaller conceptual overlap with LAION. e.g. bacteria species classification based on cell cultures."}, "questions": {"value": "Minor:\n- It may be helpful to clarify how \"New\" accuracy is defined for those not familiar with the literature."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eiCNMoSjFy", "forum": "PyfV9tFmdR", "replyto": "PyfV9tFmdR", "signatures": ["ICLR.cc/2026/Conference/Submission8754/Reviewer_ei3h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8754/Reviewer_ei3h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8754/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132367094, "cdate": 1762132367094, "tmdate": 1762920538995, "mdate": 1762920538995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}