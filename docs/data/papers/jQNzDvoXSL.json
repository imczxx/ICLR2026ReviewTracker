{"id": "jQNzDvoXSL", "number": 6903, "cdate": 1758001045129, "mdate": 1759897884858, "content": {"title": "GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric Consistency Regularization", "abstract": "Prior ReLoc3R achieves breakthrough performance with fast 25ms inference and state-of-the-art regression accuracy, yet our analysis reveals subtle geometric inconsistencies in its internal representations that prevent reaching the precision ceiling of correspondence-based methods like MASt3R (which require 300ms per pair).\nIn this work, we present GeLoc3r, a novel approach to relative camera pose estimation that enhances pose regression methods through Geometric Consistency Regularization (GCR). \nGeLoc3r overcomes the speed-accuracy dilemma by training regression networks to produce geometrically consistent poses without inference-time geometric computation. During training, GeLoc3r leverages ground-truth depth to generate dense 3D-2D correspondences, weights them using a FusionTransformer that learns correspondence importance, and computes geometrically-consistent poses via weighted RANSAC. This creates a consistency loss that transfers geometric knowledge into the regression network. Unlike FAR method which requires both regression and geometric solving at inference, GeLoc3r only uses the enhanced regression head at test time, maintaining ReLoc3R's fast speed and approaching MASt3R's high accuracy. On challenging benchmarks, GeLoc3r consistently outperforms ReLoc3R, achieving significant improvements including 40.45% vs. 34.85% AUC@5° on the CO3Dv2 dataset (16% relative improvement), 68.66% vs. 66.70% AUC@5° on RealEstate10K, and 50.45% vs. 49.60% on MegaDepth1500. By teaching geometric consistency during training rather than enforcing it at inference, GeLoc3r represents a paradigm shift in how neural networks learn camera geometry, achieving both the speed of regression and the geometric understanding of correspondence methods.", "tldr": "GeLoc3r enhances direct pose regression for visual localization by injecting geometric consistency via training-time RANSAC supervision from depth maps, yielding SOTA accuracy at fast inference speeds.", "keywords": ["Computer Vision", "Pose Estimation", "Visual Localization", "Vision Transformers", "RANSAC"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c9a50e45afbac8f3b1f4328f4bb7736a67e957fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents GeLoc3r, a relative camera pose estimation method that enhances regression-based localization through Geometric Consistency Regularization (GCR). Unlike prior ReLoc3R, which achieves fast inference but suffers from geometric inconsistencies, GeLoc3r trains regression networks with dense 3D–2D correspondences and a weighted RANSAC consistency loss to embed geometric knowledge."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a clear motivation and insightful attempt to push the accuracy ceiling of regression-based localization methods. The proposed geometric consistency regularization is conceptually sound and an interesting direction for improving pose regression models."}, "weaknesses": {"value": "Although the authors evaluated their method on several datasets, the reported improvements over ReLoc3R appear very marginal. For example, the gains shown in Tables 2, 3, 6, and 7 are relatively small and do not convincingly demonstrate a substantial advancement as the author claimed in the paper. In addition, the inference speed of Geloc3R is slower than that of ReLoc3R. While the motivation is strong, the current results do not seem to fully support the claimed effectiveness of the proposed approach."}, "questions": {"value": "Could the authors include experiments on additional datasets or on longer trajectories involving more than three images to better highlight the benefits/improvements of their method? Demonstrating clearer and more substantial improvements over ReLoc3R would strengthen the paper considerably. If such evidence can be provided, I would be inclined to raise my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jo0p8ooBYl", "forum": "jQNzDvoXSL", "replyto": "jQNzDvoXSL", "signatures": ["ICLR.cc/2026/Conference/Submission6903/Reviewer_RVQF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6903/Reviewer_RVQF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761236209940, "cdate": 1761236209940, "tmdate": 1762919145160, "mdate": 1762919145160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GeLoc3R, a method which aims to enhance the accuracy of existing Relative Pose Regression techniques while preserving fast inference speed. GeLoc3R is framed as a specific extension of ReLoc3R(a prior state-of-the-art RPR method from CVPR 2025) through the integration of a geometric consistency regularization mechanism during training. The motivation for this development stems from the authors' empirical analysis, which identifies a fundamental limitation in ReLoc3R, trained exclusively with pose loss: the presence of pixel-level geometric inconsistencies. This means that deep network descriptors for points that are geometrically consistent in 3D, and should therefore possess high descriptor similarity, instead exhibit high cosine similarity errors. The authors argue that pose-only training is insufficient to enforce the necessary geometric constraint that predicted camera poses must align accurately with the underlying 3D scene structure.\n\nTo address this, the authors introduce two specific regularization losses designed to inject geometric knowledge and enforce consistency. The network architecture is augmented with dedicated heads to estimate the final pose, dense features, and confidence maps. The dense features are then processed by a Fusion Transformer to estimate correspondence weights, which subsequently inform a RANSAC-weighted solver that utilizes the regressed pose as a prior. Supervision is achieved through three distinct losses: (a) a standard pose loss common to RPR methods; (b) an angular loss between the network's directly regressed pose and the refined pose estimated by the solver; and (c) descriptor loss that enforces consistency for features corresponding to the same 3D point (using ground truth depth for supervision). At inference time, GeLoc3R operates purely as a regression method. Only the pose regressor head is executed, allowing the method to maintain the fast runtime characteristic of RPR. \n\nThe method is trained on a collection of 10 indoor and outdoor datasets which provide ground truth depth for supervision. Evaluation is focused on the relative pose estimation task across test splits of the training environments (ScanNet1500, RealEstate10K, MegaDepth1500, and CO3Dv2), comparing against state-of-the-art RPR methods and selected non-regression approaches. The visual localization task is separately evaluated on the 7Scenes and CambridgeLandmark datasets against RPR methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a data-driven justification for the proposed method based on the identified geometric inconsistency weakness in existing RPR models.\n2. The method successfully introduces explicit geometric constraints during training without incurring penalty on the desired fast inference time of the RPR architecture.\n3.  GeLoc3R surpasses state-of-the-art RPR methods on the relative pose estimation task and demonstrates competitive runtimes."}, "weaknesses": {"value": "1. Since the training and test data environments overlap, it is essential to explicitly state whether the compared baseline methods were also trained on these same environments to ensure a fair comparison of their performance.\n2. The reported results on the visual localization task are less compelling. For example, the average performance on the 7Scenes dataset appears to be outperformed by the simpler ReLoc3R. This discrepancy requires more detailed discussion and analysis.\n\nMinor:\n1. The method requires ground truth depth for supervision during training, which limits its applicability to datasets where this information is available. This constraint should be explicitly discussed as a limitation compared to depth-agnostic RPR baselines.\n2. The visual representation of the architecture (e.g., in Figure 1) is not sufficiently clear. It is difficult to distinguish precisely which components (e.g., only the heads, or other modifications) constitute the proposed extension beyond the original ReLoc3R architecture. The figure should clearly delineate the novel contributions of GeLoc3R.\n3. Use of confidence maps: it is not clear from the figures and the text how the confidence maps are being used."}, "questions": {"value": "1. Could the authors explicitly clarify the training regimens used for all compared baselines on the evaluation environments to ensure the comparison is conducted under identical or well-justified conditions?\n2. What is the authors' analysis of where the geometric regularization degrades performance in smaller-scale, visual localization settings (e.g. 7Scenes) compared to the relative pose task?\n3. While the training requires ground truth depth, can the authors speculate or provide preliminary results on the model's robustness or accuracy when trained with synthesized or noisy depth estimates, to broaden the method's applicability?\n4. Architectural Visualization: Could the authors clearly distinguish the new contributions (heads, losses, solver) \n5. Usage of Confidence maps: could the authors explain how these are used during training.\n6. What is the reason for the 8ms gap in runtime between Reloc3R and GeLoc3R? (Table 1)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aTKU6VQ3Ix", "forum": "jQNzDvoXSL", "replyto": "jQNzDvoXSL", "signatures": ["ICLR.cc/2026/Conference/Submission6903/Reviewer_xvn5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6903/Reviewer_xvn5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654296540, "cdate": 1761654296540, "tmdate": 1762919144692, "mdate": 1762919144692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method for relative pose prediction. Given two input images, a pose regression network estimates the relative pose between them. This task is fundamental to visual localization.\n\nThe work extends the previous approach, Reloc3r, with a newly designed training strategy. Its core innovations include correspondence weight prediction, weighted RANSAC, and a loss function that evaluates projected descriptors.\n\nStrength: The paper offers valuable insights into the problem of correspondence mismatching observed in Reloc3r.\n\nWeakness: The performance gain over Reloc3r is modest. Given the small improvement, the authors should include error bars in all evaluations to demonstrate whether the improvement is statistically significant."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The analysis of descriptor inconsistency in relative pose estimation, presented in Figure 1 and Section 3.1, is insightful.\n3. The additional loss terms and training procedures are technically sound."}, "weaknesses": {"value": "1. Marginal improvement: While the proposed additional loss terms and training steps are reasonable, the performance gain is marginal. This would be acceptable if the improvement were shown to be statistically significant. At the current level of improvement, it is difficult to determine whether the gain truly results from the proposed training scheme.\n2. The paper omits evaluation on the commonly used ACID dataset."}, "questions": {"value": "Show error bars in the experiments to verify the effectiveness of the proposed training procedure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dprBdtPnx3", "forum": "jQNzDvoXSL", "replyto": "jQNzDvoXSL", "signatures": ["ICLR.cc/2026/Conference/Submission6903/Reviewer_dRwe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6903/Reviewer_dRwe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956828131, "cdate": 1761956828131, "tmdate": 1762919144349, "mdate": 1762919144349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a key limitation in fast pose regression methods like ReLoc3R: their internal representations lack geometric consistency. The authors propose GeLoc3r, which introduces a training-only module called Geometric Consistency Regularization (GCR), leveraging ground-truth depth to generate 3D-2D correspondences. GeLoc3r is fast at inference ~25-33ms, while achieving accuracy that approaches correspondence-based methods, showing significant gains on benchmarks like CO3Dv2."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-motivated. The core contribution—a training-only geometric regularization framework—is a clever and effective solution to the speed-accuracy dilemma. The method demonstrates significant quantitative improvements over its baseline, while maintaining similar inference speed."}, "weaknesses": {"value": "1. The method's primary limitation is its dependency on ground-truth depth maps during training. This restricts its applicability to in-the-wild data sources.\n2. The paper does not explore the method's sensitivity to depth quality. It is unclear how Geometric Consistency Regularization would perform if trained with estimated and noisy pseudo-ground-truth depth from a monocular network, which would be a critical step toward training on more diverse datasets."}, "questions": {"value": "Can the Geometric Consistency Regularization be adapted to work with noisy estimated depth? How robust is the weighted RANSAC solver to the significant noise that would be introduced, and does the consistency loss provide a meaningful signal in this scenario?\n\nMissing related works:\n1. Kefan Chen et al. Wide-Baseline Relative Camera Pose Estimation with Directional Learning, CVPR 2021.\n2. Meng Xu et al. Regression-Based Camera Pose Estimation through Multi-Level Local Features and Global Features, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EgFwX5kihf", "forum": "jQNzDvoXSL", "replyto": "jQNzDvoXSL", "signatures": ["ICLR.cc/2026/Conference/Submission6903/Reviewer_2BHc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6903/Reviewer_2BHc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762566521911, "cdate": 1762566521911, "tmdate": 1762919143893, "mdate": 1762919143893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}