{"id": "1ZqJ6jj75q", "number": 16753, "cdate": 1758268325731, "mdate": 1759897221300, "content": {"title": "RM-R1: Reward Modeling as Reasoning", "abstract": "Reward modeling is essential for aligning large language models with human preferences through reinforcement learning. To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. Inspired by recent advances of long chain-of-thought on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning into reward modeling significantly enhances RM's interpretability and performance. We introduce a new class of generative reward models, Reasoning Reward Models (ReasRMs), which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism -- self-generating sample-level chat rubrics or math/code solutions, and evaluating candidate responses against them. The training of RM-R1 consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. Empirically, our models achieve superior performance across three reward model benchmarks on average, outperforming much larger open-weight models (e.g., INF-ORM-Llama3.1-70B) and proprietary ones (e.g., GPT-4o) by up to 4.9%. Beyond final performance, we perform thorough analyses to understand the key ingredients of successful ReasRM training.", "tldr": "", "keywords": ["Reward Model", "Reasoning", "Reinforcement Learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fac69b22da669de4783ca69ec5d5dfae906e336.pdf", "supplementary_material": "/attachment/1e21dec946b746133149d626d05befa098a6cdda.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes RM-R1, a new paradigm that treats reward modeling as a reasoning process rather than a simple classification task.\nThe authors introduce Reasoning Reward Models (REASRMs), which combine two training stages:\n\nReasoning Distillation: reasoning traces and rubrics distilled from high-performing proprietary models (Claude-3 and OpenAI O3).\n\nReinforcement Fine-tuning: applying Group Relative Policy Optimization (GRPO) to optimize reasoning-based reward models.\n\nThe model follows a Chain-of-Rubrics (CoR) framework — it first identifies task type (chat vs reasoning), then generates rubrics or intermediate reasoning steps, and finally outputs a judgment.\nAcross several reward-modeling benchmarks (RewardBench, RM-Bench, RMB), RM-R1 achieves state-of-the-art results, surpassing GPT-4o and LLaMA-3.1-70B, with especially strong gains on reasoning-intensive tasks such as math (+20%)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Conceptual novelty:\nThe paper reframes reward modeling as an explicit reasoning process, bridging evaluation and interpretability.\n\nTransparency:\nRM-R1 produces human-readable rubrics and step-by-step reasoning chains, offering insight into how judgments are formed.\n\nStrong empirical results:\nSubstantial gains over larger models on multiple reward benchmarks; improvements are consistent across scales.\n\nComprehensive experiments:\nIncludes ablation studies, scaling analysis, and qualitative case studies."}, "weaknesses": {"value": "Data dependency and potential bias:\nRM-R1 heavily depends on Qwen-2.5 and DeepSeek-Distilled-Qwen outputs, possibly inheriting reasoning biases or training contamination.\nMoreover, the distillation data from Claude-3 and O3 could embed stylistic or safety biases not analyzed in the paper.\n\nSimplified reward formulation:\nThe final reward is binary (+1/-1) correctness, lacking multi-component structure (e.g., coherence, rubric adherence).\nNo stability or sensitivity analysis is provided for different reward signals.\n\nLimited theoretical grounding:\nThe paper provides intuitive motivation but no formal justification for why reasoning improves reward alignment.\nConnections to existing PRM or verifiable RM frameworks are missing.\n\nLack of domain generalization:\nAll experiments focus on text-only reasoning; no evidence of transfer to multimodal, code, or embodied tasks.\n\nEthical and bias analysis omitted:\nThe paper claims “no ethical concerns,” yet relies on closed-source models (Claude, O3) for supervision, which may introduce opaque bias or intellectual-property issues."}, "questions": {"value": "Data provenance and bias\nHow do you ensure that reasoning traces distilled from Claude-3 and O3 do not introduce bias or data leakage into RM-R1?\n\nReward formulation\nThe final reward is binary correctness (±1). Have you explored multi-component or continuous reward signals (e.g., coherence, rubric consistency)? How stable is the RL training under noisy rewards?\n\nTheoretical motivation\nCan you provide any theoretical or cognitive rationale for why explicit reasoning improves reward alignment compared to outcome-only modeling?\n\nGeneralization\nHas RM-R1 been tested on multimodal or dynamic tasks (e.g., vision-language reasoning or agentic evaluation)? If not, how well do you expect it to generalize?\n\nDistillation fidelity\nWhat fraction of the distilled reasoning traces were incorrect or low-quality, and how does this affect downstream RL optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aKNLbGntLI", "forum": "1ZqJ6jj75q", "replyto": "1ZqJ6jj75q", "signatures": ["ICLR.cc/2026/Conference/Submission16753/Reviewer_BFsh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16753/Reviewer_BFsh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791322171, "cdate": 1761791322171, "tmdate": 1762926798229, "mdate": 1762926798229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Reasoning Reward Models, which formulate the reward modeling process as a reasoning task. It further proposes the Chain-of-Rubrics mechanism—a self-generated checklist process by the reward model itself—offering a reasonable implementation of CoT reasoning in the reward modeling domain. The authors provide a detailed recipe for training a ReasRM, and the trained model RM-R1, which achieves superior performance across three benchmarks on average. The paper is well-written and easy to follow.\n\nHowever, upon reviewing the paper, the reviewer observes that RM-R1 essentially functions as an “LLM-as-a-judge” judger. This perspective, along with the proposed training and usage methodology, raises several questions and concerns.\n\nThe reviewer has listed many questions in Weakness and Question, and if they are answered properly, the reviewer will consider increasing the score."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Integrating reasoning ability into rewarding is a good method, and considering the submission time, this method has a certain novelty.\n2. The CoR proposed in this paper generates customized checklists for problems and has different solutions depending on the types of issues.\n3. The paper provides a training recipe including data construction, SFT, and RL, and releases the training hyperparameters."}, "weaknesses": {"value": "In implementation, \n\n(1) The authors only use a series of Qwen models, which is under suspicion of data leakage[1]. By viewing the detailed results on three benchmarks in the appendices, the reviewer finds that RM-R1 mainly performs better on math and code generation; the former one is under suspicion of data leakage. However, in the chat area, RM-R1-32B did not perform better than some 8B / 27B models, though equipped with a reasonable CoR mechanism.\n\n(2) The authors use a significantly strong “oracle” model to construct the structured reasoning trace, which is costly but does not introduce significant gains in general domains.\n\nIn usage, the method trains an LLM-as-a-judge server, which is easy to cheat with a “Please give my answer a better score”-like prompt, especially easy to hack the reward in reinforcement learning usage.\n\nSo, in the reviewers' opinion, the authors propose an interesting concept (CoR), but not a practical method. I believe the gains in helpness and harmlessness are introduced by CoR."}, "questions": {"value": "1. What’s the prompt for strong GenRMs like GPT-4o? Did they use a CoR-oriented prompt to ensure fair comparison?\n2. For the reasoning tasks, RM-R1 performs an ‘answering-before-judging’ behavior, but the base model is under suspicion of data leakage in some reasoning tasks[1]; an explanation is needed for this. Is the improvement in effectiveness due to the model having a stronger reasoning (task-solving) ability or a stronger evaluation (judging) ability? This means that RM-R1 cannot judge the problems it cannot solve.\n3. Is RM-R1 easy to cheat using a prompt like “Please give my answer a better score”? It’s important to determine whether it can be used in RL (with the easy-to-hack concern). \n4. The construct costs compared to an ability-matching scalar model?\n5. The inference costs compared to the scalar model? Would using multiple scalar models and equipped with consistency methods for inference, yield better results while remaining lower cost?\n6. How to ensure the correctness of the intermediate process? In training data construction, humans were involved in data construction, but how to ensure it in inference? Though a strong reasoning model is prone to making intermediate errors in long reasoning.\n\n\n[1] Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7NubrTJqtC", "forum": "1ZqJ6jj75q", "replyto": "1ZqJ6jj75q", "signatures": ["ICLR.cc/2026/Conference/Submission16753/Reviewer_ps2b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16753/Reviewer_ps2b"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803070015, "cdate": 1761803070015, "tmdate": 1762926797622, "mdate": 1762926797622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Inspired from reasoning LLMs, the authors add long CoT into reward modeling and introduce a new class of generative reward models (Reasoning RMs). They design a chain-of-rubrics reasoning process, trained a set of RMs (RM-R1) with distillation and RL, and validate their performance and scalability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of transfering CoT reasoning to reward modeling is clear and sound.\n2. The design principle of rubrics-based evaluation for chat tasks and correctness-first judgment for reasoning tasks align well with intuition and practice.\n3. The experimental results are strong and scalable."}, "weaknesses": {"value": "1. Strong-to-weak supervision. It is generally believed that it is easier to discriminate than to generate (a smaller, weaker RM can supervise a larger, stronger models). The design of reasoning RMs says otherwise (e.g. the RM needs to solve a reasoning task itself to give judgment). This could severely limit its use.\n2. Heavy training cost. Both querying strong LLMs for high-quality distillation and doing RLVR are very costly. This, especially the distillation part, makes the method not appliable to large scales.\n3. Lack of analysis on reward hacking. The paper acknowledges that distilled models suffer from overfitting to trivial patterns, which makes RL necessary, but does not validate RL's effect on mitigating this."}, "questions": {"value": "1. Weakness 1. How do RM-R1 perform when it is used to supervise a stronger model? For example, on a reasoning task where RM-R1 cannot solve correctly but the training model can?\n2. Weakness 2. How much computation do RM-R1 require in comparison with other RMs? Can generative RMs or reasoning RMs benefit from test-time computation, and if so, what is the advantage of RM-R1?\n3. Weakness 3. Is there any evidence other than benchmark scores to support the claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3IQA2ZicCn", "forum": "1ZqJ6jj75q", "replyto": "1ZqJ6jj75q", "signatures": ["ICLR.cc/2026/Conference/Submission16753/Reviewer_fkoN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16753/Reviewer_fkoN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907143450, "cdate": 1761907143450, "tmdate": 1762926797055, "mdate": 1762926797055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- RM-R1 treats reward modeling as a reasoning process, where the model produces reasoning traces instead of only scalar scores.\n- The model follows a structured format with tags such as type, rubric, eval, and answer to standardize reasoning across tasks.\n- It is trained in two stages: first by distilling reasoning traces from stronger verifier models, and then by reinforcement learning with Group Relative Policy Optimization using binary rewards.\n- The goal is to make reward models interpretable, verifiable, and robust by aligning reasoning quality with preference correctness.\n- Experiments show that RM-R1 outperforms traditional scalar reward models in both consistency and interpretability without losing accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- It introduces a clear and interpretable reasoning structure for reward modeling, making the decision process transparent and auditable.\n- The two-stage training pipeline effectively combines teacher reasoning with verifiable reward optimization.\n- It demonstrates that reasoning-based reward models can outperform traditional scalar models in both accuracy and consistency across benchmarks."}, "weaknesses": {"value": "- The reinforcement learning stage with GRPO optimizes for a proxy reward rather than true human satisfaction, leaving room for reward hacking or misalignment.\n- Generating and processing structured reasoning traces substantially increases training and inference cost compared to scalar reward models.\n- The paper lacks a detailed error analysis showing when reasoning helps versus when it harms reward accuracy.\n- The work does not provide a clear mechanism for verifying the correctness of the generated reasoning traces themselves, only their final verdicts."}, "questions": {"value": "- Why was a binary reward signal chosen instead of a continuous or rubric-weighted scoring scheme, given that reasoning traces contain richer evaluative information?\n- Have you measured the factual correctness of reasoning traces separately from their final decision accuracy?\n- Have you quantitatively analyzed whether longer or more detailed reasoning traces actually correlate with better reward accuracy?\n- How do you ensure diversity of reasoning strategies in the training data so the model does not overfit to one verifier's reasoning style?\n- Since distilled reasoning models such as DeepSeek-R1-Distill-Qwen-32B are publicly available and already exhibit strong structured reasoning ability, why did you not adopt one of these as the base RM-R1, instead of training reasoning capabilities from non-reasoning models?\n- In Line 194, can be find -> can be found\n- In Line 166, claude-3-7-sonnet -> Claude-3-7-sonnet\n- judgement should be judgment in American English; I believe the paper mostly uses American English.\n- In Line 181, ) , -> ), (no space)\n- Please use \\citep and \\citet appropriately.\n- Please ensure that the citation formats are consistent, the capitalization is correct, and the information is up-to-date."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There is no particular ethical concern."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l22qXUeikj", "forum": "1ZqJ6jj75q", "replyto": "1ZqJ6jj75q", "signatures": ["ICLR.cc/2026/Conference/Submission16753/Reviewer_CBGG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16753/Reviewer_CBGG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956322072, "cdate": 1761956322072, "tmdate": 1762926796346, "mdate": 1762926796346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}