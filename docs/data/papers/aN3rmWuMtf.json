{"id": "aN3rmWuMtf", "number": 12282, "cdate": 1758206840322, "mdate": 1759897520358, "content": {"title": "KerZOO: Kernel Function Informed Zeroth-Order Optimization for Accurate and Accelerated LLM Fine-Tuning", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across numerous NLP tasks. Nevertheless, conventional first-order fine-tuning techniques impose heavy memory demands, creating practical obstacles to real-world applications. Zeroth-order (ZO) optimization has recently emerged as a promising memory-efficient alternative, as it circumvents the need for backpropagation by estimating gradients solely through forward passes, making it particularly suitable for resource-limited environments. Despite its efficiency, ZO optimization suffers from gradient estimation bias, which significantly hinders convergence speed. To address this, we analytically identify and characterize the lower-order bias introduced during ZO-based gradient estimation in LLM fine-tuning. Motivated by tools in mathematical physics, we introduce a kernel-function-based ZO framework aimed at mitigating this bias and improving optimization stability.  KerZOO achieves comparable or superior performance to existing ZO baselines in both full-parameter and parameter-efficient fine-tuning settings of LLMs, while significantly reducing the number of iterations required to reach convergence. For example, KerZOO reduces total GPU training hours by as much as 74% and 44% on WSC and MultiRC datasets in fine-tuning OPT-2.7B model and can exceed the MeZO baseline by 2.9% and 2.6% in accuracy.  We show that the kernel function is an effective avenue for reducing estimation bias in ZO methods.", "tldr": "", "keywords": ["Zeroth-order optimization", "Efficient AI", "LLM"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c9c6fbecbe10e9b790a5079773baa7749377d1e.pdf", "supplementary_material": "/attachment/812049634f5c8592af295840be4534ba9f9886c4.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces KerZOO, a kernel function-based zeroth-order (ZO) optimization framework for efficient fine-tuning of large language models (LLMs). KerZOO uses a Taylor expansion to identify the lower-order bias in ZO gradient estimation and introduces a kernel function satisfying specific moment conditions to eliminate this bias. Extensive empirical results show higher accuracy and faster convergence of KerZOO compared with existing baselines across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes an interesting observation that the lower-order bias in ZO gradient estimation can be removed by incorporating an additional scalar random variable $r$ and applying a kernel function weighting. This approach is promising and has the potential to address the slow convergence commonly observed in existing ZO optimization methods."}, "weaknesses": {"value": "The paper is not clearly written. There is limited discussion or intuition provided to explain the proposed kernel function and rationale behind Algorithm 1. Some experimental results are also difficult to interpret (e.g., Tables 4, 5, and 6). Overall, the writing needs to be significantly improved to meet publication standards. Additionally, the effect of kernel weighting on variance should be explored in more depth."}, "questions": {"value": "- In Section 3.4, why is $K_\\beta(r)$ chosen in the specific form given in Eq (17)? The proposed kernel function seems to appear from nowhere. A discussion clarifying the reasoning behind this particular choice would help readers better understand the kernel design (also $K_\\beta(r)$ and $K(r)$ in Eq (17) appear redundant, remove one).\n\n- It would be helpful to discuss whether there exist other kernel functions satisfying the moment conditions and how to compare different design choices. Are there any general strategies for constructing the kernels satisfying the moment conditions?\n\n- KerZOO removes the lower-order bias, but it introduces another random variable $r$ which could increase the estimator's variance. The paper briefly discusses variance in the appendix, but the explanation is very heuristic. Can the authors provide a more rigorous analysis of how the kernel design influences variance, or at least empirical evidence showing that variance does not increase significantly?\n\n- The constant $C$ seems to be an important hyperparameter in the kernel design, but it is not clearly discussed. Table 6 gives some experimental results for different $C$ values, but it gives no intuition or guidance on how to select it. Since $C$ may affect higher-order bias and possibly variance, the paper should discuss the theoretical and/or practical guidances for choosing $C$.\n\n- Algorithm 1 uses an exponential moving average at every iteration, but the paper gives no explanation or motivation for this algorithm choice. Since Algorithm 1 is presented as the main fine-tuning procedure, a detailed discussion is needed. Also $\\theta_t^{md}$ doesn't seem to be used in Algorithm 1?\n\n- What does \"percentage\" represent in Table 4, and what does \"loss at iteration\" mean in Tables 5 and 6?\n\nMinor comments:\n\n- Line 128: perturbation. (period)\n\n- Eq (4): replace $\\nabla^2 f(x)$ with $\\nabla^2 \\mathcal{L}(\\theta)$.\n\n- Change the numbering of Figure 3 and Figure 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1CMaEA98Jr", "forum": "aN3rmWuMtf", "replyto": "aN3rmWuMtf", "signatures": ["ICLR.cc/2026/Conference/Submission12282/Reviewer_HQgz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12282/Reviewer_HQgz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569298315, "cdate": 1761569298315, "tmdate": 1762923216681, "mdate": 1762923216681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **KerZOO**, a kernel-guided zeroth-order (ZO) optimization method for LLM fine-tuning. It analyzes the low-order bias introduced by random perturbations in ZO gradient estimation and designs kernels satisfying moment conditions (e.g., $\\mathbb{E}[rK(r)] = C$, $ \\mathbb{E}[r^3K(r)] = 0 )$ to cancel the dominant bias term. The resulting estimator is simple to implement (few directions, e.g., $n=3$ and drops into standard ZO-SGD. Experiments on RoBERTa-large, OPT-2.7B/6.7B, and LLaMA-3 3B/8B across classification and generation tasks show faster convergence and competitive or better accuracy than MeZO/HiZOO, with sizable reductions in GPU hours; the method remains compatible with LoRA. Ablations on kernel order $(\\beta)$ and scaling constant $C$ support the design choices."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Precisely targets ZO’s low-order bias and provides principled kernel conditions with expectation-level analysis.\n* Kernel weighting and scalar perturbations integrate cleanly; works with very few directions $(n  \\approx 3)$.\n* Substantial cuts in training steps/GPU hours while maintaining or improving accuracy.\n* Results span multiple model families/sizes and tasks, under both full-finetuning and PEFT (LoRA).\n* Sensitivity to $\\beta$ and $C$, plus memory/time comparisons, aid reproducibility and deployment."}, "weaknesses": {"value": "1. **Near-duplicate figures/tables.** The paper’s plotting/table template and ordering are *highly similar* to submission **#12350**, with only color changes: **#12282 Fig.1 / Fig.2 / Fig.3 ≈ #12350 Fig.3 / Fig.2 / Fig.4** (same axes styles, legend shapes, and layout).\n\n2. **If a shared template is acceptable, how do you explain drifting baselines?** Under ostensibly comparable settings, baselines differ across the two papers in ways that **systematically favor each paper’s own method**.\n\n   * Example (**Table 2 · DROP**): **#12282** MeZO-LoRA=13.4 / HiZOO-LoRA=13.9 / KerZOO-LoRA=14.7 **vs.** **#12350** MeZO-LoRA=19.2 / HiZOO-LoRA=18.3 / P-GAP-LoRA=22.5.\n   * Example (**Table 3**): the same cross-paper baseline discrepancy appears **column-wise** under matched configurations.\n\n3. **Even within this paper, baselines are selectively changed without explanation.** In **Table 1**, the **Zero-shot** row mirrors the MeZO paper, but **MeZO-LoRA** departs markedly. The paper does not clarify which baselines are re-runs vs. prior quotes, nor why numbers were altered—changes consistently benefit the proposed method.\n\n4. **Reproducibility gap.** The released code lacks **requirements.txt / environment specs, hyperparameters/seeds, making the results non-reproducible and preventing verification of the baseline inconsistencies."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oMeCAFPNXJ", "forum": "aN3rmWuMtf", "replyto": "aN3rmWuMtf", "signatures": ["ICLR.cc/2026/Conference/Submission12282/Reviewer_5uGb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12282/Reviewer_5uGb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968464893, "cdate": 1761968464893, "tmdate": 1762923216180, "mdate": 1762923216180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes KerZOO, a kernel-function–informed zeroth-order (ZO) optimizer for LLM fine-tuning. The core idea is to attach a random scalar r to each two-point ZO query and weight it with a kernel K(r) whose moments enforce $E[rK(r)]=C$ and $E[r^{3}K(r)]=0$, thereby canceling the leading $O(\\epsilon^{2})$ bias term in the ZO gradient estimator and leaving only higher-order bias. Algorithm 1 implements this estimator with multiple perturbation directions per step and uniform $r\\sim[-1,1]$ sampling. Experiments on RoBERTa-large, OPT, and LLaMA report faster convergence and accuracy gains over MeZO/HiZOO, with large reductions in normalized GPU hours (e.g., up to 74% on WSC) at comparable memory cost. Overall, the paper targets a concrete weakness of ZO (biased estimates) with a clear analytic prescription (kernel moment conditions) and backs it with broad empirical evidence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Clear and targeted theory: the derivation isolates the lower-order bias in two-point ZO and gives explicit kernel moment conditions ($E[rK(r)]=C$, $E[r^{3}K(r)]=0$) that remove it in expectation, which is simple to check/implement.\n- Practical algorithmic wrapper: KerZOO fits the standard two-point estimator with minimal changes (draw $u$, draw $r$, apply $K(r)$) and uses a small number of directions (default $n=3$), making adoption straightforward.\n- Broad empirical wins with efficiency: consistent improvements over MeZO/HiZOO across GLUE/SuperGLUE-style tasks and models, plus sizable reductions in training iterations/GPU hours at similar memory footprints (e.g., Tables/Figs for RoBERTa/OPT, and the SQuAD time-memory table)."}, "weaknesses": {"value": "- Theoretical clarity/notation: the Taylor expansions mix notations (e.g., $∇^{2}f(x)$ vs. $∇^{2}L(θ)$) and use $D^{3}∇L$ (which suggests a 4-th order derivative) without a clean justification; please tighten the derivation around Eqs. (4)–(8) and state smoothness/independence assumptions precisely.\n- Assumption–implementation gap: the theory relies on $r \\in [-1,1]$ with moment constraints, but the method later “shrinks” the range of $r$ over training to reduce variance, breaking $E[rK(r)]=C$ and $E[r^{3}K(r)]=0$; the paper acknowledges this qualitatively but lacks a bias bound under shrinking-$r$. The convergence theorem also hides constants ($\\kappa,\\kappa_\\beta$) and leaves the practical regime unclear.\n- Fairness and accounting: core results set KerZOO to $n=3$ directions by default, but it is not always explicit whether MeZO/HiZOO use the same number of directions or the same forward-pass budget; key tables highlight “iterations/GPU hours” but not queries per update. Although Table 15 adds a 3-direction ablation, the main tables would benefit from strict normalization by total forward evaluations. Also, Algorithm 1’s $\\beta_t$ schedule is ambiguous (typo?), complicating reproduction."}, "questions": {"value": "- Evaluation protocol & reporting: For Tables 1–3, please match MeZO/HiZOO to the same number of perturbation directions (e.g., n=3) and identical forward-pass budgets, so that results are fully reproducible.\n- Shrinking-$r$ schedule: since you shrink the support of $r$ during training, what is the resulting residual bias term (in place of $E[r^{3}K(r)]=0$)? Can you provide a bound that depends on the current interval and show convergence with this controlled bias? Ideally add a figure/table showing accuracy and loss vs. shrink schedule.\n- Budget normalization & hyperparameters: for Tables 1–3, can you report results where each method is matched on total forward-pass queries (or function evaluations), and clarify the exact perturbation count for MeZO/HiZOO in those tables? Also specify the intended $\\beta_t$ formula in Algorithm 1 and provide sensitivity to $β$, $C$ (you use $C=4$), and $n$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4NRvgw5Imb", "forum": "aN3rmWuMtf", "replyto": "aN3rmWuMtf", "signatures": ["ICLR.cc/2026/Conference/Submission12282/Reviewer_v2NT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12282/Reviewer_v2NT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986029589, "cdate": 1761986029589, "tmdate": 1762923215650, "mdate": 1762923215650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "KerZOO addresses the slow convergence problem in zeroth-order (ZO) optimization for LLM fine-tuning by introducing a kernel function to reduce gradient estimation bias. The paper analytically identifies that standard ZO methods suffer from lower-order bias due to random perturbations, which significantly hinders convergence speed. By designing kernel functions based on Legendre polynomials that satisfy specific moment conditions, KerZOO eliminates this second-order bias term and improves estimation accuracy to higher-order terms only. Experiments across RoBERTa, OPT, and LLaMA models demonstrate that KerZOO achieves up to 74% reduction in GPU training hours while improving accuracy by 2-3% compared to the MeZO baseline. The method is compatible with parameter-efficient fine-tuning techniques like LoRA and maintains the memory efficiency advantages of zeroth-order optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The research problem in this paper is very important and interesting. ZO methods usually need more iterations to converge than first-order methods, so accelerating the training process will be important and interesting.\n\n2. The proposed method in this paper is very easy to follow and the kernel-based method is very interesting.\n\n3. The paper provides thorough experiments to verify the performance gain of the proposed method KerZOO."}, "weaknesses": {"value": "1. I think the main concern is from the experiments. I noticed the paper provides the results on LLaMA3, but most experiments focus on RoBERTa and OPT. I hope the authors can provide more results on the state-of-the-art pre-trained models. Because I also think a strong pre-trained model can narrow the performance gap between zeroth-order based methods and first-order based methods. \n\n2. I think the paper focused on accelerating the ZO training process, ans some related paper also focused on this direction, the authors can also provide these comparison results, such as SensZOQ, Sparse MeZO, ZO-SVRG. \n\n[1] Zeroth-Order Fine-Tuning of LLMs with Transferable Static Sparsity. \n[2] Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning. \n[3] Zeroth-order stochastic variance reduction for nonconvex optimization."}, "questions": {"value": "1. In my opinion, ZO method is very sensitive to the selection of hyper-parameters, like learning rate. I would like to ask whether the authors fully tune the hyper-parameters."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nZlMVkI0ZV", "forum": "aN3rmWuMtf", "replyto": "aN3rmWuMtf", "signatures": ["ICLR.cc/2026/Conference/Submission12282/Reviewer_PJ43"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12282/Reviewer_PJ43"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762054914129, "cdate": 1762054914129, "tmdate": 1762923215098, "mdate": 1762923215098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}