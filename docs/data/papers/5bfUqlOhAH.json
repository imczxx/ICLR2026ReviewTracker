{"id": "5bfUqlOhAH", "number": 13982, "cdate": 1758226458712, "mdate": 1759897398539, "content": {"title": "Efficient Autoregressive Inference for Transformer Probabilistic Models", "abstract": "Transformer-based models for amortized probabilistic inference, such as neural processes, prior-fitted networks, and tabular foundation models, excel at single-pass marginal prediction. However, many real-world applications -- from signal interpolation to multi-column tabular predictions -- require coherent joint distributions that capture dependencies between predictions. While purely autoregressive architectures efficiently generate such distributions, they sacrifice the flexible set-conditioning that makes these models powerful for meta-learning. Conversely, the standard approach to obtain joint distributions from set-based models requires expensive re-encoding of the entire augmented conditioning set at each autoregressive step. We introduce a causal autoregressive buffer that preserves the advantages of both paradigms. Our approach decouples context encoding from updating the conditioning set. The model processes the context once and caches it. A dynamic buffer then captures target dependencies: as targets are incorporated, they enter the buffer and attend to both the cached context and previously buffered targets. This enables efficient batched autoregressive generation and one-pass joint log-likelihood evaluation. A unified training strategy allows seamless integration of set-based and autoregressive modes at minimal additional cost. Across synthetic functions, EEG signals, cognitive models, and tabular data, our method matches predictive accuracy of strong baselines while delivering up to $20\\times$ faster joint sampling. Our approach combines the efficiency of autoregressive generative models with the representational power of set-based conditioning, making joint prediction practical for transformer-based probabilistic models.", "tldr": "We accelerate autoregressive inference of transformer probabilistic models such as prior-fitted networks and transformer neural processes.", "keywords": ["probabilistic machine learning", "neural processes", "probabilistic meta-learning", "amortized inference"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b80b6dcfb128e81db0ba866b40b26019f9a6d0d.pdf", "supplementary_material": "/attachment/2e3d5c604fb60735ed7d5acddfdbfb7040b8ac31.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a causal autoregressive buffer for transformer-based amortized probabilistic models (NPs, PFNs, tabular FMs). The key idea is to encode the context once and keep it in a static cache; generated targets enter a causal buffer that attends to the context cache and prior buffer tokens, enabling joint dependencies without re-encoding the augmented set each step. Complexity drops from (O(K(N!+!K)^2)) to (O(N^2 + KN + K^2)), supporting batched AR sampling and single-pass joint likelihood evaluation. Experiments across synthetic, EEG, cognitive, and tabular tasks match standard AR accuracy while achieving speedups."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This is a practical architectural improvement that preserves set-conditioning benefits while unlocking efficient joint predictions typical of AR models. The idea of a role-aware attention mask (immutable context, causal buffer, no writes back to context) feels simple yet powerful, and widely applicable to TNPs/PFNs/tabular FMs. The potential for large efficiency gains in meta-learning scenarios with repeated sampling is significant.\n\n- The factorization and masking constraints (R1–R4) are clearly stated; buffering semantics are sound. The complexity analysis matches the masking structure.\n- Training uses a buffer-size curriculum with structured masks that let targets attend to context or a variable prefix of the buffer, aligning training and inference. The link to minimizing KL to posterior predictive under varying conditioning sets is consistent with PFN theory.\n- The claims of accuracy comparison with standard AR and speedups are supported on small/medium contexts and (K< 32) (as per reported settings). The authors also acknowledge a degradation risk when target counts exceed the trained buffer size."}, "weaknesses": {"value": "In the experiments, they are rather limited to moderate sizes of K and small/medium-sized contexts. I have a feeling that this may be limiting the characterization of the approach. Also, residual order dependence for likelihood evaluation requires averaging; I think an analysis on the order sensitivity is missing."}, "questions": {"value": "- Is it possible to have tests on larger (K) regimes and very large contexts to map out failure boundaries, plus ablation on positional embeddings inside the buffer and ordering effects when approximating permutation invariance via order-averaging?\n - Is it possible to quantify order-averaging required to stabilize joint likelihoods with a metric measuring their cost/benefit?\n- How do the authors decide on the size of (N)s? Is it there a experiment for emprically showing for large values of N, proposed method has compounding advantages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qm9439MPnw", "forum": "5bfUqlOhAH", "replyto": "5bfUqlOhAH", "signatures": ["ICLR.cc/2026/Conference/Submission13982/Reviewer_agRA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13982/Reviewer_agRA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914060386, "cdate": 1761914060386, "tmdate": 1762924481814, "mdate": 1762924481814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a conflict between two types of models: set-based probabilistic models and autoregressive models. When models (like Neural Processes) try to generate a sequence of predictions (e.g., filling in a signal), they have to re-process all the original data and all the new predictions at every single step. The core idea is to decouple the static context from the dynamic predictions by introducing a new architectural component called the \"causal autoregressive buffer\". The advantage is (i) on the computational advantage: The expensive $\\mathcal{O}(N^2)$ cost (encoding the context) is paid only once at the beginning and (ii) preserved accuracy as shown in the experiments section."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and novel. The problem of repeated, expensive re-encoding is visualized in Figure 1, which provides an intuitive understanding of the entire paper's premise. The proposed solution is an elegant architectural fix that decouples the static context from the dynamic predictions, supported by a clear complexity analysis ($\\mathcal{O}(N^{2}+NK+K^{2})$). \n\n2. The empirical evidence is convincing. It delivers a speedup (up to 20x) with small additional training cost and without sacrificing predictive accuracy."}, "weaknesses": {"value": "I'm not an expert in this field (neural processes, probabilistic meta-learning, amortized inference), so I'll leave my comments in the following Questions sections."}, "questions": {"value": "1. What happens when the target sequence length M is larger than the buffer size? I saw a discussion on Appendix E.2 and also the limitation in the final discussion section, which suggests that to achieve better performance, we need more computational time. I'm wondering what the final complexity is if including the operation \"we evaluate every $K$ targets once and perform AR for $M/K$ steps\". Will this still lead to the 20 times speed up? How often does the situation \"target count exceeds training bounds of the buffer\" happen in real-world analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HB7FycKLHd", "forum": "5bfUqlOhAH", "replyto": "5bfUqlOhAH", "signatures": ["ICLR.cc/2026/Conference/Submission13982/Reviewer_Jq2K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13982/Reviewer_Jq2K"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921887725, "cdate": 1761921887725, "tmdate": 1762924481271, "mdate": 1762924481271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a “causal autoregressive buffer” to accelerate joint inference in transformer-style probabilistic models such as Transformer Neural Processes (TNPs) and Prior-Fitted Networks (PFNs). The idea is: instead of repeatedly re-encoding the entire context set at every autoregressive step, the model encodes the context once, caches it, and then maintains a lightweight causal buffer of generated targets. This is claimed to both (i) avoid recomputing attention over the full growing set, and (ii) still model target-to-target dependencies during sequential rollout. The authors argue this reduces complexity from  \n$O(K (N+K)^2) \\) to \\( O(N^2 + NK + K^2) $ \nand yields up to 20× faster joint sampling while matching predictive quality of stronger autoregressive baselines, across synthetic regression, EEG forecasting, and tabular regression tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### 1 Practical inference speedup for probabilistic transformers  \nThe work directly targets a real bottleneck: autoregressive joint inference in set-conditioned probabilistic models is extremely expensive if you have to repeatedly re-encode the entire (context ∪ generated targets) set at every step. The proposed causal autoregressive buffer reduces the need to rerun the full encoder every time by caching the context representation once and then only extending a lightweight causal buffer of generated targets. This reduces the stated complexity from \\(O(K (N+K)^2)\\) to \\(O(N^2 + NK + K^2)\\) for K rollout steps, which the authors claim translates into 3×–20× wall-clock speedups in their experiments.\nThis is a meaningful systems win for anyone who wants to deploy TNP/PFN-style models in real-time or interactive settings.\n\n### 2 Keeps target-to-target dependency modeling (vs. fully independent heads)  \nA common cheap fallback in meta-regression / neural process style models is to just predict each target independently conditioned on the same context encoding. That is fast but discards correlations across targets.  \nHere, the buffer still allows each new target to attend causally to previously generated targets in the rollout, so you capture sequential statistical structure across predicted outputs without paying the full re-encode cost. This is especially relevant for tasks like trajectory / time-series forecasting (e.g. EEG forecasting) where temporal consistency matters.  \nIn other words, you get something closer to coherent joint samples instead of just pointwise fits.\n\n### 3 Unification: one model that can act both “independently” and “autoregressively”  \nThe paper proposes a training curriculum in which, during training, some targets are forced to condition only on the static context block (independent prediction mode), and others are allowed to attend to a prefix of previously generated targets through the buffer (autoregressive mode). The same set of parameters is trained to handle both regimes.  \nIf this actually works robustly, it’s attractive from an engineering perspective: you don’t need to maintain two separate heads (one batchable + independent, one fully autoregressive). You get a single model that can cheaply do fast batched prediction or slower-but-more-coherent sequential rollout.\n\n### 4 Conceptual bridge between permutation-invariant context encoders and standard AR decoding  \nArchitecturally, the paper is trying to marry two worlds:\n- permutation-invariant / set-encoder style context processing (as in Neural Processes, PFNs, etc.), and\n- standard causal decoding with KV caching (as in language models)."}, "weaknesses": {"value": "## 2. Major Concerns\n\n### 2.1 Claimed novelty vs. standard KV caching  \nThe proposed “causal autoregressive buffer” is essentially a block-structured attention mask plus caching of keys/values:\n\n- A *static* context block (permutation-invariant encoder over the observed context set) is computed once and cached.\n- A *growing* autoregressive buffer stores keys/values for previously generated targets and is updated step by step with a causal mask.\n- During generation, new target tokens attend to (a) the frozen context block and (b) previous targets in the buffer, but the buffer cannot “write back” into the context block.\n\nThis is very close in spirit to ordinary autoregressive Transformer inference with KV caching: you prefill a prefix once, then incrementally extend the cache with each new token under a causal mask. \n\nThe paper repeatedly describes this as a general architectural mechanism that “decouples context encoding from sequential prediction,” but standard decoder-only Transformers already do this: prefix context is cached, new tokens causally attend to the prefix and prior tokens. The only real twist here is that the “prefix” came from a permutation-invariant encoder instead of from plain tokens, and that the mask enforces a hard separation between context and generated targets (“R1–R4”).\nRight now, the paper oversells this as conceptually new. It reads more like applying known KV-caching style inference to set-conditioned meta-learners. The authors need to justify why this is more than “cache the context encodings and use a causal mask for the generated block.”\n\n---\n\n### 2.2 Theory is bookkeeping, not analysis of approximation error  \nThe main “theoretical” contribution is a complexity argument: naive autoregressive deployment of a TNP-like model requires re-running attention over the enlarged context-plus-buffer at every step, costing \\( O(K (N+K)^2) \\), while the buffer method pays \\( O(N^2) \\) once for the context and then \\( O(NK + K^2) \\) to roll out K targets.\n\nThis is essentially computational accounting, not a theoretical guarantee about inference quality. The paper makes a strong claim that the buffered rollout “preserves model quality,” because each new prediction can still attend to all prior targets via the buffer and to the cached context. But there is no formal analysis of when this approximation matches the behavior of the “true” fully-updated model that would have re-encoded the augmented context set after each new target is added.\n\nIn fact, once you stop re-encoding, you’ve thrown away strict permutation invariance over the *union* of context and generated targets. After generation begins, the model is no longer allowed to revisit and re-symmetrize the combined set. The paper acknowledges degradation when the number of autoregressive targets exceeds the buffer size K used during training.\nThis is a core limitation, but it’s treated as an implementation detail rather than a fundamental modeling gap.\n\n---\n\n### 2.3 Empirical evaluation seems arranged to flatter the method  \nMost of the storytelling is about runtime: joint sampling speed, likelihood evaluation speed, etc. The new buffer method is reported as 3–20× faster than autoregressive baselines like TNP-A or TNP-D-AR, which repeatedly re-encode context at each step, especially at large N.\n\nConcerns:\n\n- The paper admits their implementation uses heavy engineering (FlashAttention-2, Triton kernels, KV caching), and also says baselines were “optimized beyond their public versions.” But we never see an ablation that isolates the architectural contribution from pure systems tuning. We can’t tell if the 20× number is “new idea” vs. “better CUDA.”\n- There is no memory comparison. The buffer maintains per-layer KV caches for both the frozen context block and the autoregressive buffer tokens, which can blow up for long rollouts and large batch sizes. Only wall-clock time is emphasized, which is convenient but incomplete.\n- When you actually look at predictive quality (e.g. EEG forecasting), the buffered model with K=16 can underperform the slow full autoregressive baseline (TNP-D-AR), and sometimes is even worse than extremely small-buffer variants (effectively K=1).\n  So the tradeoff is not “same accuracy, way faster.” It’s “sometimes noticeably worse, but faster.”\n\nDespite that, the abstract and intro still assert that the method “matches predictive accuracy … while delivering up to 20× faster joint sampling,” which is too strong given these cases.\n\nIn short, the evaluation is narrated as “little to no quality loss, huge speedup,” but the actual numbers show a real quality vs. speed tradeoff.\n\n---\n\n### 2.4 Generality claims are speculative and weakly supported  \nThe paper claims broad applicability: Perceiver-like architectures with pseudo-tokens, probabilistic neuroscience modeling, and “tabular foundation models” such as PFN-style inference.\n\nBut:\n\n- The Perceiver extension is hypothetical; there is no experiment demonstrating the buffer with learned pseudo-tokens.\n- The “tabular foundation model” experiment is actually quite small. The model is trained from scratch on synthetic structural causal model data, then evaluated on a few UCI-like tabular tasks with N=128 context / M=32 targets.\n  That’s nowhere near the scale implied by the phrase “foundation model.”\n- In those tabular results, “AR w/ buffer (K=32)” performs roughly on par with a standard AR baseline (K=1) and modestly better than fully independent predictions.\n  That mostly shows that causal conditioning across targets helps, which is expected. It does not prove that the specific buffering trick is uniquely enabling.\n\nSo the claim that this mechanism is a general, broadly applicable inference upgrade feels speculative. The evidence is narrow, and in some cases purely suggestive.\n\n---\n\n### 2.5 Missing baselines / missing ablations  \nThe baselines are primarily TNP variants (TNP-D, TNP-D-AR, TNP-A, etc.).\nHowever, to really argue “our method is necessary,” I would expect at least:\n\n1. **A simple two-tower baseline**  \n   - Tower A encodes the context set once (frozen).\n   - Tower B is a causal decoder over targets that cross-attends into Tower A, with standard KV caching.\n   - This is, in spirit, what the proposed model is.  \n   Without ablations on masking structure and curriculum, we don’t know if the fancy buffer rules (R1–R4) are actually critical or if a trivial cross-attend-decoder would do the same.\n\n2. **A plain autoregressive Transformer treating the context as a prefix**  \n   - Just linearize the context into a token sequence, feed it as a prefix prompt, and then autoregress over targets exactly like a language model, ignoring permutation invariance entirely.\n   - The paper insists that permutation invariance of the context block is crucial, but it never quantifies how much you lose if you drop that constraint and just go full decoder-style."}, "questions": {"value": "see in weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n1YDqaQXb4", "forum": "5bfUqlOhAH", "replyto": "5bfUqlOhAH", "signatures": ["ICLR.cc/2026/Conference/Submission13982/Reviewer_G5io"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13982/Reviewer_G5io"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930524648, "cdate": 1761930524648, "tmdate": 1762924480778, "mdate": 1762924480778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Neural processes are finding applications across a range of areas, including in tabular foundation models like TabPFN and in weather modelling e.g. in Aardvark Weather. In many of these applications, it’s important to be able to model joint predictive distributions that capture the dependencies between target variables at multiple locations e.g. when imputing multiple missing values into a row of a tabular dataset or when predicting precipitation across a region to assess flood risk in weather forecasting.  Autoregressive transformer neural processes are arguably the go-to method for solving such tasks.\n\nThis paper makes a neat contribution: it addresses one of the central limitations of autoregressive transformer neural processes and sequence models -- their high computational cost at inference time. It introduces a lightweight causal buffering mechanism that preserves the expressive power and calibration benefits of full autoregression while dramatically reducing computation and latency. This makes autoregressive probabilistic models viable for real-time and large-scale applications, from time-series forecasting to neural operator learning. In doing so, the paper bridges the gap between expressive but slow autoregressive methods and fast but less flexible parallel inference, providing a practical path toward scalable, uncertainty-aware inference in scientific and foundation-model settings.\n\nI liked the paper and would vote for acceptance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper has at its core a simple but neat idea that works well. I liked the fact that it had a wealth of experimental results including\n\n1. Synthetic function modelling (Gaussian Process (GP) functions and sawtooth synthetic functions.)\n2. EEG forecasting and interpolation including real EEG recordings from 20 human subjects\n3. Multisensory causal inference modelling involving simulated and real human behavioural data from a multisensory integration study (Liu et al., 2025).\n4. Tabular foundation model experiments\n5. Further Efficiency and ablation studies\n\nThe results were strong -- the method really works.\n\nThe presentation and the writing were very clear and well polished. Figures were very well presented. Generally, I thought it was a well executed piece of work."}, "weaknesses": {"value": "The idea is quite simple which could be viewed as a weakness, but I actually view this as a strength for multiple reasons: it's easy to understand, it's simple to implement, and because of this it could be deployed widely. \n\nThe idea might also seem niche as AR-TNPs are not super well-known, but this is also not correct. TNPs in the sim2real setting were rebranded by the impactful TabPFN line of work and the contributions here are directly aligned with this breed of tabular foundation model as the final experiment shows. Moreover, NPs have been deployed in another Nature paper in Aardvark weather, which has also been impactful and which could leverage these results to produce scalable weather forecasts. I expect more and more examples of applied NPs will emerge over the coming years."}, "questions": {"value": "Small point, but I’m not a huge fan of the phrase “joint likelihood” since in many contexts in the paper the primary focus is really producing a \"joint predictive density” rather than the likelihood of the parameters need for e.g. learning. Indeed, if you trained a model only using likelihoods derived from univariate predictives, you could then immediately use this to produce joint predictive densities (c.f. the AR CNP paper). So I'd place the emphasis on joint distributions most of the time, rather than on the likelihood functions.\n\nline 49: \"However, this breaks the set-based structure”  I think that, at this stage in the paper, this is a bit ambiguous. E.g. in standard mode, adding the generated targets back into the context and recomputing everything is arguably as good as it gets in terms of permutation invariance. Perhaps say “However, this involves significant computational overhead…”\n\nline 161: \"In practice, Eq. (2) is not exact for likelihood evaluation as it breaks permutation invariance of the model.” I don’t agree with this. This is really a modelling choice, or something that is determined by the task itself, rather than being wrong or right. You can choose to model the order as latent (and therefore something that you need to average over) or known and fixed (in which case you don’t). For time-series, for example, there is often an implied order over the targets. For spatial data, there might not be. \n\nline 291: you could point out here that discrete diffusion is really an any order AR model in disguise and is very closely related to a NP with a discrete input set (arguably it is an NP) and as such could leverage your ideas."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rkIpRXGsJk", "forum": "5bfUqlOhAH", "replyto": "5bfUqlOhAH", "signatures": ["ICLR.cc/2026/Conference/Submission13982/Reviewer_jkX4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13982/Reviewer_jkX4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13982/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762114155702, "cdate": 1762114155702, "tmdate": 1762924480339, "mdate": 1762924480339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}