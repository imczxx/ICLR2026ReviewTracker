{"id": "u8QXExmfgq", "number": 626, "cdate": 1756757056095, "mdate": 1763694590229, "content": {"title": "Towards Second-Order Optimization in Learned Image Compression: Faster, Better, and More Deployable", "abstract": "Training learned image compression (LIC) models entails navigating a challenging optimization landscape defined by the fundamental trade-off between rate and distortion. Standard first-order optimizers, such as SGD and Adam, struggle with gradient conflicts arising from competing objectives, leading to slow convergence and suboptimal rate–distortion performance. In this work, we demonstrate that a simple switch to a second-order quasi-Newton optimizer, SOAP, dramatically improves both training efficiency and final performance across diverse LIC architectures. Our theoretical and empirical analyses reveal that SOAP’s Newton preconditioning inherently resolves the intra-step and inter-step update conflicts intrinsic to the R–D objective, facilitating faster, more stable convergence. Beyond acceleration, we uncover a critical deployability benefit: SOAP-trained (non-diagonal) models exhibit significantly fewer activation and latent outliers. This improves entropy modeling and substantially enhances robustness to post-training quantization. Together, these results establish second-order optimization—achievable as a seamless drop-in replacement of the imported optimizer—as a powerful, practical tool for advancing the efficiency and real-world readiness of LICs. Code will be publicly available.", "tldr": "", "keywords": ["Learned Image Compression", "Training Efficiency"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/af121c5deb4bc5faa6beef86cf93bb306220a177.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes using a second-order optimizer, Shampoo with Adam in the Conditioner (SOAP), instead of first-order optimizers, like Adam, to train Learned Image Compression (LIC) models. The paper finds that this optimizer results in faster training, better rate-distortion (RD) curves, and models that are more robust to quantization. Experiments include comparisons of state-of-the-art LIC models with Adam and with SOAP optimization as well as analysis of how gradients change during training."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method shows strong empirical results. Specifically, it is great to see both convergence time and RD metrics improving simultaneously.\n2. The paper includes strong empirical evidence that this improvement occurs over a wide variety of LIC models and test datasets. Furthermore, this method works better than other recently proposed methods to solve training convergence issues.\n3. The paper is clearly written and most claims are clearly validated by empirical or theoretical evidence. The concept of inter-step and intra-step scores was neat and useful for understanding how gradients change during training.\n4. The section on evaluating the effect of this optimizer on quantized LIC models is neat and useful for potential deployment of compressed LIC models."}, "weaknesses": {"value": "1. SOAP is an existing optimizer, so replacing Adam with SOAP has limited novelty.\n2. The theoretical results rely on strong assumptions, like $L_H$-Lipschitz and local smoothness. It'd be interesting to know whether these hold for the models under investigation."}, "questions": {"value": "1. Have you considered training the SOAP models longer than 300 epochs? From Figure 1, it looks like the loss might still be decreasing. I'm curious if you could see even more improvements in RD curves with longer training?\n2. How do other optimizers perform? For example, Shampoo, SGD, AdamW."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1KT3PDwv1x", "forum": "u8QXExmfgq", "replyto": "u8QXExmfgq", "signatures": ["ICLR.cc/2026/Conference/Submission626/Reviewer_DAjx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission626/Reviewer_DAjx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761418532177, "cdate": 1761418532177, "tmdate": 1762915569113, "mdate": 1762915569113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the advantages of the existing second-order optimizer SOAP for training learned image codecs, including (1) Assist in avoiding conflicts between different optimization objectives of RD within mini batches; (2) Help avoid gradient conflicts between different samples across mini-batches. (3) Help eliminate outliers in the latent and activation layers, and facilitate PTQ. In terms of final results, validation on models such as ELIC, TCM, LALIC, and DCAE found that the training appear more stable, resulting in a BD-Rate gain of about 3%. The performance after PTQ training was 1% -2% higher than the prior Adam training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- From the results reported in the paper, it can be seen that using the SOAP optimizer has indeed saved training time, improved convergence performance, and assisted with the implementation of PTQ"}, "weaknesses": {"value": "- The paper only analyzes the advantages of using SOAP in LIC, without making any methodological improvements or targets analysis for the Compression task\n- Some of the details in the paper are unclear, e.g., if the performance comparison conditions are consistent, and if the 300 epoch model fully converges, etc. See \"Questions\".\n- A large number of \"See Appendix\" jumps, but there is no concise explanation of the relevant content in the main text, resulting in poor readability and logical fragmentation"}, "questions": {"value": "- We want to know if this 3% performance gain is compared to the original performance in models' paper or based on the final convergence performance of the Adam optimizer under the same training conditions. In addition, how does the performance of the original model compare according to the training configuration in the paper(such as EMA, Batch size set to, etc.)\n- The paper claims that all models are trained for 300 epochs to ensure complete convergence. However, according to Fig.1, the model did not fully converge at 300 epochs.\n- I wonder if the SOAP optimizer is robust enough to alleviate gradient conflicts: (1) For multiple optimization objectives within a mini batch, such as R&D, whether this effect is universally applicable when the amplitude and convergence speed differences of different optimization objectives are more diverse, such as with different R-D balance (lambda). (2) For gradient consistency between mini batches, whether it can be equally effective in small batch sizes, as batch size set to 64 in the paper requires too much GPU memory (H100 is used in the paper, which is too expensive for training learned image codecs). And also when the batch size is large, the randomness of samples is smaller, and the differences between samples between mini batches are more stable, these will also help mini batch gradients tend towards smaller sizes.\n- I wonder if the EMA used in the paper is necessary for SOAP, as many codecs, especially lightweight CNN based ones, have weak performance gains with EMA, which slows down their training speed. At the same time, I also wonder how wild the training without the average effect of the EMA, and what is the effect of SOAP at this time.\n- In Figure 3, after more training steps, does the gradient consistency still maintain the current trend, especially in the first 10000 steps where inter-step score fluctuates but seems to show an overall upward trend? Will Adam's performance be similar to SOAP after this initial training period?\n- For the outliers in Latent, I think more in-depth analysis and visualization are needed, including the changes in this outlier during the training process, comparing the distribution of this outlier with the same training loss alignment, and so on.\n- The paper only compares with Adam optimizer, there are also some learned image/video codecs that use AdamW for optimization. I wonder if there are any relevant experiments or analyses\n- Regarding the application of SOAP to other types of data compression models, such as videos, gradient conflicts/error accumulation may also occur temporally. However, it also involves an allocation optimization problem trained on this dimension (such as frame level bitrate allocation in videos). Does the SOAP optimizer have any advantages in solving such problems?\n- This is an innovative and practical research, but I have some doubts about the practicality and actual performance of this new optimizer, which is a fundamental component in network training. If the author can solve these problems, I will increase my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JxFpyVo34V", "forum": "u8QXExmfgq", "replyto": "u8QXExmfgq", "signatures": ["ICLR.cc/2026/Conference/Submission626/Reviewer_8Sdb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission626/Reviewer_8Sdb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795529648, "cdate": 1761795529648, "tmdate": 1762915568999, "mdate": 1762915568999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed SOAP, a second-order optimizer for training Learned Image Compressor (LIC). Applying this optimizer is simple, and only requires a few lines of change. Empirically, the paper demonstrate that SOAP is superior compared to Adam, in terms of step-efficiency, time-efficiency, and achieves a better BD-rate after convergence. The paper provide theoretical and intuitive explanation that why SOAP achieves a better performance. The first explanation is that SOAP \"smooth\" the training process and better aligns conflicting gradients given by the rate and distortion factor. Both theoretical explanation and empirical evidence that supports the theory are provided. The second explanation is that SOAP suppresses latent and activation outliers, which are negative for entropy coding. Also both theory and empirical evidence are provided."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is very well structured and easy to follow. Placing the empirical results ahead of theoretical explanation makes it easy to focus on the main results and contributions without being absorbed into too much details, while providing the theoretical explanation afterwards answers the questions naturally raised when reading the empirical results.\n2. This paper provides very good insights into why second-order optimizer works better than first-order methods. It manages to define quantitative scores to proof the point of theory, and provide additional empirical evidence to support the theory.\n3. The empirical results (table 1) is rich and sound. Benchmarking is performed using multiple baseline models and datasets."}, "weaknesses": {"value": "1. I think showing algorithm 1 is kind of trivial.\n2. I am not sure \"step-efficiency\" and \"time-efficiency\" are flawless metrics. It is clear that SOAP introduce computation overhead as it needs to compute (even just an approximation) of Hessian. So that each step of SOAP is significantly more expensive than a step of Adam. Therefore I am not sure it is fair to use step-efficiency to compare. Hopefully the authors can address this concern in their response. Using \"time-efficiency\" is also a bit questionable in some sense, as given the same amount of time SOAP and Adam consumes, it makes lead to e.g. different CPU usage. Hopefully there are better metrics to more fairly quantitatively compare SOAP and Adam."}, "questions": {"value": "1. Question on the validness of using \"step-efficiency\" and \"time-efficiency\" to compare Adam and SOAP is already mentioned in the \"Weakness\" session.\n2. It is not fully convincing to me that why having a larger intra-step similarity score is a good thing. It is intrinsically conflicting to optimize rate and distortion, so that a small cosine similarity between p_R and p_D is expected. As both terms are used, it makes sure the model parameters are updated toward the right direction. Why having a larger cosine similarity translates to a good thing in parameter updating?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8TKLMrFRUf", "forum": "u8QXExmfgq", "replyto": "u8QXExmfgq", "signatures": ["ICLR.cc/2026/Conference/Submission626/Reviewer_RnuL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission626/Reviewer_RnuL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889119580, "cdate": 1761889119580, "tmdate": 1762915568798, "mdate": 1762915568798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a second-order optimization method for training learned image compression models. Experimental results demonstrate faster convergence and improved coding efficiency compared with first-order optimization. Furthermore, suppressing activation outliers is shown to facilitate effective post-training quantization of learned image compression networks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Overall, the paper makes a meaningful contribution to the learned image coding community.\n\n2. The experimental results appear solid and well supported.\n\n3. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "The paper claims three primary contributions, however, each has certain weaknesses, as discussed below.\n\n1. Although this may be a common concern, I am afraid the paper lacks strong novelty. The main contribution appears to be the application of an existing second-order optimization method (SOAP) to learned image compression (LIC). While the plug-and-play nature of the method demonstrates its versatility, it also highlights the limited originality of the work, especially since SOAP itself was not proposed by the authors. Besides, the proposed approach does not show a significant performance improvement compared with previous studies such as Zhang et al. (2025c).\n\n2. The theoretical part is relatively weak. The core section (Section 4.2) mainly reproduces the SOAP derivation, which cannot be considered a novel contribution of this paper.\n\n3. Regarding the PTQ part, it seems that the outlier reduction is performed only for activations (as shown in Fig. 4) rather than for weights. If this is the case, considering that AdaRound is designed solely for weight quantization, I wonder why the authors employed AdaRound in the experiments of Section 5.3. I am also curious why weight quantization is included in Table 2."}, "questions": {"value": "1. In Section 5, the distinction between latent and activation is unclear. Does latent refer to the output activation of the final encoding layer?\n\n2. From Fig. 4, it is difficult to observe a clear difference in the dynamic range of values.\n\n3. In Table 3, it is unclear what the final results would be if the model were trained for more epochs (e.g., 1000 epochs)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "No specific ethics concerns"}}, "id": "L3nOIElmEF", "forum": "u8QXExmfgq", "replyto": "u8QXExmfgq", "signatures": ["ICLR.cc/2026/Conference/Submission626/Reviewer_Nu8g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission626/Reviewer_Nu8g"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008907307, "cdate": 1762008907307, "tmdate": 1762915568679, "mdate": 1762915568679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates second-order optimization for training Learned Image Compression (LIC) models. The authors argue that common first-order optimizers (SGD, Adam) suffer from gradient conflicts between rate and distortion terms in the rate–distortion (R–D) loss, causing slow and unstable convergence. They propose replacing Adam with a quasi-Newton optimizer (SOAP). With this simple two-line substitution the authors are able to show the acceleration in training by 57–70% in wall-time and steps and improvement in BD-Rate by ~3% across different learned image compression models. Furthermore, they showed empirically that the SOAP optimizer reduces the outliers in the activations and latents, improving entropy modeling and post-training quantization (PTQ) robustness. Finally, they also present theoretical analysis to show why the SOAP optimizer is able to overcome the conflicting gradient updates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple yet useful approach in the Learned image compression. Replacing Adam with SOAP is a minimal, drop-in change requiring no architectural modification. Despite its simplicity, it yields significant practical benefits — faster training, better BD-Rate.\n\n2. The paper evaluates on four advanced LICs (ELIC, TCM, LALIC, DCAE) across three datasets (Kodak, Tecnick, CLIC2022). The improvements are consistent and robust.\n\n3.  The authors provide an elegant geometric interpretation of how Newton preconditioning aligns gradients from rate and distortion objectives (intra-step) and across consecutive iterations (inter-step). I did not check the accurateness of the theorems.\n\n4. Section 5 provides an interesting secondary contribution: showing that second-order updates reduce activation outliers, thus tightening entropy models and enhancing quantization robustness. This bridges optimization dynamics with hardware-friendly deployment — a fresh perspective."}, "weaknesses": {"value": "1. The main idea—using a second-order optimizer—is not new in deep learning. SOAP (Vyas et al., 2024) is external work, and applying quasi-Newton updates to LICs is a contextual extension, not a fundamental algorithmic advance. The novelty lies more in the application and analysis than in the optimizer itself. For a top-tier acceptance, stronger conceptual framing (e.g., a LIC-specific curvature model or adaptive R–D Hessian decomposition) would strengthen the contribution.\n\n2. The comparison is primarily between Adam and SOAP. The authors mention Shampoo and AdaHessian but do not present direct comparisons, even though those are obvious baselines.\nEmpirical results would be more convincing if SOAP’s advantages were shown against other second-order or preconditioned methods (e.g., Shampoo, Muon, AdaHessian).\n\n3. Proposition 1 assumes that the rate and distortion Hessians are locally proportional or jointly diagonalizable — a strong assumption that is unlikely to hold in practice for deep networks. The analysis is therefore qualitative rather than rigorous, though still informative."}, "questions": {"value": "1) Can the authors empirically verify the “shared curvature structure” assumption (e.g., via eigenvalue overlap or gradient covariance between rate and distortion losses)?\n2)How does SOAP interact with other training stabilizers (e.g., gradient clipping, EMA, learning-rate warmup)?\n3) Is SOAP’s benefit additive to other acceleration techniques (CMD-LIC, Balanced-RD)?\n4) Are the preconditioners reused across layers, or is SOAP applied per-layer independently?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1JsX8G1miG", "forum": "u8QXExmfgq", "replyto": "u8QXExmfgq", "signatures": ["ICLR.cc/2026/Conference/Submission626/Reviewer_xcTm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission626/Reviewer_xcTm"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762427492729, "cdate": 1762427492729, "tmdate": 1762915568551, "mdate": 1762915568551, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}