{"id": "9QQ3Kc2hj6", "number": 6757, "cdate": 1757994724674, "mdate": 1759897896019, "content": {"title": "No Pixel Left Behind: A Detail-Preserving Architecture for Robust High-Resolution AI-Generated Image Detection", "abstract": "The rapid growth of high-resolution, meticulously crafted AI-generated images poses a significant challenge to existing detection methods, which are often trained and evaluated on low-resolution, automatically generated datasets that do not align with the complexities of high-resolution scenarios. A common practice is to resize or center-crop high-resolution images to fit standard network inputs. However, without full coverage of all pixels, such strategies risk either obscuring subtle, high-frequency artifacts or discarding information from uncovered regions, leading to input information loss. In this paper, we introduce the **H**igh-Resolution **D**etail-**A**ggregation Network (**HiDA-Net**), a novel framework that ensures no pixel is left behind. We use the Feature Aggregation Module (FAM), which fuses features from multiple full-resolution local tiles with a down-sampled global view of the image. These local features are aggregated and fused with global representations for final prediction, ensuring that native-resolution details are preserved and utilized for detection. To enhance robustness against challenges such as localized AI manipulations and compression, we introduce Token-wise Forgery Localization (TFL) module for fine-grained spatial sensitivity and JPEG Quality Factor Estimation (QFE) module to disentangle generative artifacts from compression noise explicitly. Furthermore, to facilitate future research, we introduce **HiRes-50K**, a new challenging benchmark consisting of **50,568** images with up to **64 megapixels**. Extensive experiments show that HiDA-Net achieves state-of-the-art, increasing accuracy by over **13%** on the challenging Chameleon dataset and **10%** on our HiRes-50K.", "tldr": "We propose HiDA-Net, a detector for high-resolution AI-generated images that leverages all input pixels by integrating global context with local tile features, achieving 13% gain on challenging Chameleon benchmark.", "keywords": ["AIGI detection", "High-Resolution detection", "Featrue aggregation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a896a5c4c14765d91bdc9de793c201f3f8cdff8d.pdf", "supplementary_material": "/attachment/765a437fb5fb56c5cc54beb6e1c733fa9b91e2ee.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents HiDA-Net, a high-resolution AI-generated image detection framework designed to retain fine-grained details by aggregating information from full-resolution local tiles and a global downsampled view. The model integrates Token-wise Forgery Localization (TFL) and JPEG Quality Factor Estimation (QFE). The authors further introduce HiRes-50K dataset. Experimental results demonstrate that HiDA-Net achieves state-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Structure of Paper; The paper is technically sound and well-executed, with comprehensive ablation studies that explore the effects of different loss functions, numbers of tiles, and other architectural choices. These analyses effectively demonstrate the contribution of each component to the overall performance. The proposed framework is both conceptually clear and empirically validated."}, "weaknesses": {"value": "1. Visual Analysis on process: The paper could provide a deeper analysis of the interaction between local detail tokens and global tokens. For example, it would be valuable to examine HiDA-Net’s behavior on AI-inpainted images, where some tiles contain real content while others contain generated regions. Understanding how local detail tokens respond in such mixed cases and how the local aggregator fuses these heterogeneous signals would offer stronger insight into the model’s robustness and interpretability."}, "questions": {"value": "1. Since tiling is a core aspect of the method, a more comprehensive ablation would strengthen the contribution. Could the authors explore overlapping tile strategies or compare padding-based approaches instead of simply resizing patches smaller than 224 pixels?\n\n2. Is the CLS token for local detail tokens rotation-invariant? For instance, would identical patches with a 90° rotation produce the same output representation ($t_{out}$)?\n\n3. How are the ratios (weights) among different loss functions determined?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fmz5RIyLu2", "forum": "9QQ3Kc2hj6", "replyto": "9QQ3Kc2hj6", "signatures": ["ICLR.cc/2026/Conference/Submission6757/Reviewer_MJAJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6757/Reviewer_MJAJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811212852, "cdate": 1761811212852, "tmdate": 1762919039017, "mdate": 1762919039017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HiDA-Net and HiRes-50K benchmark to prevent performance drops of AIDI on high-resolution images brought by detail loss and limited generalization. Specifically, HiDA-Net aggregates multiple information (local details, global structure, JPEG) with FAM to provide a better understanding of the AIGC image. Also, the introduces HiRes-50K to enable realistic high-resolution evaluation under varied resolution and image quality. Overall, the HiDA-Net achieves SoTA performance in the experimental comparison."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The motivation is clear and intuitive. The detailed information loss will introduce misunderstanding for the detection model, especially for the AIGC scene, where most generated models produce well-structured but detail-failed images.\n- The model design is reasonable and theoretically proven.\n- The experiment is comprehensive to demonstrate the effectiveness of the proposed network and benchmarks."}, "weaknesses": {"value": "- The open-source AIGC image dataset and real-image dataset are innumerable. The paper does not sufficiently explain the core basis for HiRes-50K to surpass existing data resources in terms of irreplaceability or value increment.\n- The experiments primarily rely on outdated models (e.g., SD v1.4, SDXL) for generating AI-synthesized images with limited coverage of other mainstream high-resolution generative models—especially advanced models updated after 2024 (e.g., SD3.5, FLUX, Qwen-Image). \n- The analysis on input degradation focuses on JPEG artifacts; however, the real-world imaging encounters noise, blur, defocus, and moire patterns. Extensive studies on more complicated degradation would improve the integrity of the discussion.\n-  More recent model have employed multiple strategies, e.g., RL and adversarial learning to improve their capability of \"actively evading detection\". The experimental design of the paper does not cover such scenarios."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bR1NvABtio", "forum": "9QQ3Kc2hj6", "replyto": "9QQ3Kc2hj6", "signatures": ["ICLR.cc/2026/Conference/Submission6757/Reviewer_LuBd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6757/Reviewer_LuBd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832382336, "cdate": 1761832382336, "tmdate": 1762919038487, "mdate": 1762919038487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a High-Resolution Detail-Aggregation Network (HiDA-Net) for detecting AI-generated images in high-resolution inputs. To effectively capture both global context and local details, HiDA-Net processes the input image through two complementary paths — a global path and a local path — where global and local features are extracted for classification. \n\nTo aggregate these features effectively, the authors introduce three key modules: the Feature Aggregation Module (FAM), Token-wise Forgery Localization (TFL), and JPEG Quality Factor Estimation (QFE). Specifically, FAM aggregates the [CLS] tokens from local detail tiles and the global token to form a discriminative feature representation for classification. TFL introduces a Random Patch Swap (RPS) augmentation to create partially manipulated images, providing token-level supervision that enhances spatial sensitivity to localized forgeries. QFE, implemented as a lightweight MLP, predicts the JPEG quality factor to help disentangle compression artifacts from generative ones.\n\nIn addition, the paper presents a new dataset, HiRes-50K, which provides a large number of high-quality images for evaluating detection performance. Experimental results show that the proposed method achieves superior performance on both the Chameleon and HiRes-50K datasets. However, several questions and concerns remain, as listed below:"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper convincingly identifies and quantifies how resizing harms detection by losing high-frequency details.\n2. It introduces a new and high-quality dataset, named HiRes-50K, for evaluation."}, "weaknesses": {"value": "1. The proposed method includes both a global and a local path, but uses the same transformer blocks to process global and local images. How do the proposed FAM, TFL, and QFE modules adaptively extract and distinguish local and global features for classification? It is unclear why these modules can effectively capture both types of features simultaneously.\n\n2. The computational complexity of transformer blocks is quadratic. When more local patches are used, this inevitably increases the processing time and computational cost. How is this issue addressed or mitigated?\n\n3. The paper claims that local details are crucial for detection performance, implying that the size of the local image patches plays an important role. How does the patch size affect overall performance?\n\n4. Important ablation studies are missing. Since the proposed method includes both a global and a local path, an ablation study should be provided to evaluate the effectiveness of this dual-path design. How do the global and local paths individually contribute to the final performance?\n\n5. For a fair comparison, the paper should clearly provide implementation details, including the training dataset and the model complexity (e.g., number of parameters, FLOPs)."}, "questions": {"value": "The questions I concern have been posted in the Weakness section. The authors can prepare their rebuttal with reference to the review comments in Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E5mpf8QVbk", "forum": "9QQ3Kc2hj6", "replyto": "9QQ3Kc2hj6", "signatures": ["ICLR.cc/2026/Conference/Submission6757/Reviewer_L8Yu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6757/Reviewer_L8Yu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911449046, "cdate": 1761911449046, "tmdate": 1762919037989, "mdate": 1762919037989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of detecting high-resolution AI-generated images. The proposed framework HiDA-Net processes images in original resolution enabling increased quality of predictions and achieves SOTA performance. The paper also proposes a new dataset HiRes-50K that consists of >50k images of high resolution."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation for the method is absolutely clear and supplemented with math and illustrations.\n2. The proposed method achieves SOTA performance on several datasets.\n3. The paper proposed a novel high-resolution HiRes-50K dataset that may be valuable for the community.\n4. The paper includes extensive ablation on the proposed method."}, "weaknesses": {"value": "Major weaknesses:\n1. The proposed method is not compared with recent AI-generated image detection methods, like [1 - 3].\n2. The proposed method has an increased inference time for high resolution images compared to the other approaches. But what is the difference in speed between the HiRes-50K and the other methods on standard resolutions, like 224 $\\times$ 224?\n3. I have not found the explicit list of models that are used in creating the HiRes-50K dataset. It is important to include the relevant models in the dataset.\n4. Lines 333-334 describe the resizing applied to the real part of the proposed dataset. What was the distribution of the original sizes of it? The resizing operations on the real part of the dataset may have led to models learning resizing artifacts instead of the synthetic images signs.\n\n[1] Karageorgiou, Dimitrios, et al. \"Any-resolution ai-generated image detection by spectral learning,\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n[2] Koutlis, Christos, and Symeon Papadopoulos. \"Leveraging representations from intermediate encoder-blocks for synthetic image detection,\" European Conference on Computer Vision, 2024.\n[3] Guillaro, Fabrizio, et al. \"A bias-free training paradigm for more general ai-generated image detection.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\nMinor weaknesses:\n1. The quotes could be isolated from the text to improve readability (e.g. “generative models (Karras et al. (2017))”)\n2. The text mentions that the dataset contains only images with resolutions below 1K to over 10K pixels, however Figure 4 shows that the biggest resolution is below 6K. Does the dataset contain higher resolutions and what is the percentage of it compared to the other dataset?"}, "questions": {"value": "The authors should clarify issues described in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sOz0UBoOqc", "forum": "9QQ3Kc2hj6", "replyto": "9QQ3Kc2hj6", "signatures": ["ICLR.cc/2026/Conference/Submission6757/Reviewer_Uopj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6757/Reviewer_Uopj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926613964, "cdate": 1761926613964, "tmdate": 1762919037582, "mdate": 1762919037582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}