{"id": "7gLNA6nT5d", "number": 19389, "cdate": 1758295880190, "mdate": 1759897042102, "content": {"title": "N-Gram Induction Heads for In-Context RL: Improving Stability and Reducing Data Needs", "abstract": "In-context learning allows models like transformers to adapt to new tasks from a few examples without updating their weights, a desirable trait for reinforcement learning (RL). However, existing in-context RL methods, such as Algorithm Distillation (AD), demand large, carefully curated datasets and can be unstable and costly to train due to the transient nature of in-context learning abilities. In this work, we integrated the n-gram induction heads into transformers for in-context RL. By incorporating these n-gram attention patterns, we considerably reduced the amount of data required for generalization and eased the training process by making models less sensitive to hyperparameters. Our approach matches, and in some cases surpasses, the performance of AD in both grid-world and pixel-based environments, suggesting that n-gram induction heads could improve the efficiency of in-context RL.", "tldr": "We propose using n-gram induction heads to reduce data requirements and improve the stability of in-context reinforcement learning training", "keywords": ["reinforcement learning", "in-context reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87685ed9d9b1d27df29e74f3116750411e2887d5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes to explicitly add n-gram induction heads to transformer model for in-context reinforcement learning. It can improve the data efficiency, address the simplicity bias and make the model more stable and less sensitive to hyperparameters. Experiments show that the proposed method can outperform the algorithm distillation baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of introducing n-gram induction head is simple and easy to implement.\n2. It can improve the data efficiency and reduce the hyperparameter sensitivity.\n3. The proposed n-gram head can also be applicable to visual environment settings."}, "weaknesses": {"value": "1. Section 2.2: the description of the whole model structure is unclear. The formula in line152 needs more detailed explanation, like how the attention score of n-gram head is normalized, what if there are no n-grams in the history, if there is only one head in each NG layer. The authors should better provide a pseudo-code or the architecture figure.\n\n2. What is the architecture of your base transformer model? What is the number of layers and the total size of the model? Do you try using a powerful pretrained model to see if the benefit of 2-gram information still helps? There should also be experiments on different model size to further validate the effectiveness of the n-gram head. \n\n3. The limited generalizability. The motivation of the 2-gram induction head  is to mitigate the simplicity bias and accelerate the emergence of in-context learning. \nWhile effective at small scales and with limited data, this architectural improvement is fundamentally a hardcoded bias for simple, short-term pattern recognition (specifically, 2-gram statistics in the experiments, 3-gram will hurt the performance). In the era of increasingly large transformers and datasets, the attention itself can emerge the complex induction heads and even more sophisticated learning algorithms, which makes the 2-gram information redundant. Moreover, in real-world scenarios, the basic in-context learning capability is already strong, we do not need to train the transformer from scratch for the emergence of in-context RL, and the n-gram information is insufficient for distilling complex RL algorithms that require abstract reasoning instead of the local sequential patterns."}, "questions": {"value": "Typos: The title of the paper is wrong."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w2piq8pLsg", "forum": "7gLNA6nT5d", "replyto": "7gLNA6nT5d", "signatures": ["ICLR.cc/2026/Conference/Submission19389/Reviewer_yhM1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19389/Reviewer_yhM1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19389/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761278546539, "cdate": 1761278546539, "tmdate": 1762931312881, "mdate": 1762931312881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes N-gram attention in ICRL to reduce the data required in training process but achieve the similar performance of AD. Evaluations on some visual-based scenarios further show its edge."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to follow.\n2. The figure is well-situated and clear."}, "weaknesses": {"value": "As I'm not the expert in this area, I cannot provide some professional review here. Below are my comments:\n\n1. The work lacks formal analysis explaining why N-gram heads yield stability gains and reduced data requirement. Maybe the authors should provide some theoretical perspective to elaborate this.\n2. Could the authors add more experiments on some diverse visual tasks to further showcase the superiority ?"}, "questions": {"value": "See weakness above"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Format issue: The title in pdf is not corresponding with the title on openreview."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WFRHYl3Hij", "forum": "7gLNA6nT5d", "replyto": "7gLNA6nT5d", "signatures": ["ICLR.cc/2026/Conference/Submission19389/Reviewer_vnMf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19389/Reviewer_vnMf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19389/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552238559, "cdate": 1761552238559, "tmdate": 1762931312456, "mdate": 1762931312456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper integrates n-gram introduction heads into transformers for In-Context RL. This integration aims to resolve two challenges in Algorithm Distillation: (1) the low data efficiency in ICRL; (2) the training instability in emerge the in-context capability. The authors apply explicit n-gram introduction heads to the transformer backbone and test n-gram heads in Dark Room and Key-to-Door environments. The authors also propose a scheme to extend the environments to image states. The results show that the n-gram heads can significantly reduce the data requirement and improves hyperparameter robustness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a very useful improvements to AD by integrating n-gram heads to transformer. This architectural improvement significantly mitigates the disadvantage of AD of data overhead, hyperparameter tuning, and training instability\n1. The empirical study is very comprehensive that answers all my concerns towards the n-gram heads architecture application in AD. Section 4 strengths my confidence in applying this technique to ICRL research.\n1. The VQ technique provides a feasible approach to image-based environments which extends the n-gram heads architecture to broader application scenarios.\n1. The authors give detailed description on how to sweep and choose the hyperparameters. This information is rather important to such empirical papers for reproduction."}, "weaknesses": {"value": "1. This paper demonstrates the benifits of n-gram heads by expriments in two environments and their variants in Miniworld. However, these environments are relatively simple. I suggest the authors test n-gram heads in more complicated continuous control tasks (e.g., MuJoCo, Meta world) to show the generalization.\n1. In addition to the experiments, I would expect theoretical analysis for why explicit n-gram heads can help improve data efficiency and traning stability in Algorithm Distillation, as n-gram has been studied for decades.\n1. As VQ and ResNet are used in image-based tasks, I suggest the authors also analyze the training cost sensitivity for these modules. They are important to prove the adaptation and efficiency in pixel-based environment.\n\nMinor issue:\n1. The title is missing in the paper.\n1. The figure positions mismatch the text. For example, I have to jump pages to find Fig. 2 and Fig. 4 when I read the paper. It would be nice to rearrange the paper so that the figures are right in the place where they are referred."}, "questions": {"value": "1. Could the authors compare with other baselines, for example, Noise Distillation [1] mentioned in the paper? This will help clarify whether the observed data-efficiency and stability gains stem specifically from the proposed n-gram inductive bias, or whether similar improvements could be achieved through alternative data-centric approaches.\n1. Could the authors test the method in high dimensional continuous control environments, e.g., MuJoCo and Meta world? These are widely used benchmarks to evaluate the generalization and scalability of RL algorithm, and would help demonstrate whether the proposed n-gram heads can extend the scope to more complex, real-world control tasks.\n\n[1] Zisman, Ilya, et al. \"Emergence of In-Context Reinforcement Learning from Noise Distillation.\" International Conference on Machine Learning. PMLR, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vVhx4p3M8O", "forum": "7gLNA6nT5d", "replyto": "7gLNA6nT5d", "signatures": ["ICLR.cc/2026/Conference/Submission19389/Reviewer_ULwF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19389/Reviewer_ULwF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19389/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720725102, "cdate": 1761720725102, "tmdate": 1762931312051, "mdate": 1762931312051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines whether augmenting the architecture with n-gram attention heads improves the performance of Algorithm Distillation [17]."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "The scope of this work is narrow, as it focuses solely on improving Algorithm Distillation [17], and it is unclear whether the findings generalize beyond this specific approach. Given the limited experimental scale, its relevance to understanding in-context learning in large transformer models is also unclear."}, "questions": {"value": "The title in the PDF file (\"FORMATTING INSTRUCTIONS FOR ICLR 2026 CONFERENCE SUBMISSIONS\") should be corrected."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TR0Taqx92y", "forum": "7gLNA6nT5d", "replyto": "7gLNA6nT5d", "signatures": ["ICLR.cc/2026/Conference/Submission19389/Reviewer_vyuv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19389/Reviewer_vyuv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19389/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774346067, "cdate": 1761774346067, "tmdate": 1762931311646, "mdate": 1762931311646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper integrates n-gram induction heads into transformer architectures for in-context reinforcement learning (ICRL). Building on Algorithm Distillation, the authors show that explicitly incorporating n-gram attention patterns reduces data requirements (up to 27Ã— fewer transitions) and makes hyperparameter search easier, and works across both discrete and pixel-based environments. The method is validated on Dark Room, Key-to-Door, and Miniworld environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "S1: The paper addresses a real practical problem in ICRL: Algorithm Distillation's demand for large datsets, and sometime slow convergence or getting stuck.\n\nS1: The paper tests their method on different setups, ones with more grid based states and another with rgb 64x64 pixel. A diverse set of tasks reinforces the findings.\n\nS3: Clear presentation with intuitive figures."}, "weaknesses": {"value": "W1: This is my main concern. This work seems to not disentangle three questions: 1) how does n-gram heads general help the training of transformers 2) why this is espeically important (or not) in ICRL tasks 3) is the gain via n-gram heads really not achievable by other hyperparameter tuning or different architectural tricks.\n\nW2: It is very hard to takeaway generalizable insights on why n-gram heads help and when we should be using these.\n\nW3: Lack of serious comparison. The method is only compared against vanilla AD. There has been many variants on the algorithmic side, and at the same time many improvements on the transformer architecture and initialization.\n\nW4: Overselling of generality. The title and abstract suggest n-grams solve ICRL's data problems broadly, but it is hard to understand whether these gains are precisely attributable to n-gram heads or will be simply addressed by data scale or better optimization."}, "questions": {"value": "Q: What is the positional encoding used in the transformer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CgHBJm2zBL", "forum": "7gLNA6nT5d", "replyto": "7gLNA6nT5d", "signatures": ["ICLR.cc/2026/Conference/Submission19389/Reviewer_Csny"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19389/Reviewer_Csny"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19389/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951283695, "cdate": 1761951283695, "tmdate": 1762931311210, "mdate": 1762931311210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}