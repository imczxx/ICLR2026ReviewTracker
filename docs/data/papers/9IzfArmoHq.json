{"id": "9IzfArmoHq", "number": 10460, "cdate": 1758172363770, "mdate": 1759897649500, "content": {"title": "Unlearning Evaluation through Subset Statistical Independence", "abstract": "Evaluating machine unlearning remains challenging, as existing methods typically require retraining reference models or performing membership inference attacks—both rely on prior access to training configuration or supervision label, making them impractical in realistic scenarios. Motivated by the fact that most unlearning algorithms remove a small, random subset of the training data, we propose a subset-level evaluation framework based on statistical independence. Specifically, we design a tailored use of the Hilbert–Schmidt Independence Criterion to assess whether the model outputs on a given subset exhibit statistical dependence, without requiring model retraining or auxiliary classifiers. Our method provides a simple, standalone evaluation procedure that aligns with unlearning workflows. Extensive experiments demonstrate that our approach reliably distinguishes in-training from out-of-training subsets and clearly differentiates unlearning effectiveness, even when existing evaluations fall short.", "tldr": "", "keywords": ["Machine Unlearning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a17ed1a4bcde7f0f8b667b03ae5c37881337e8a2.pdf", "supplementary_material": "/attachment/c0619f468167e6685f34d04993698abd407f1699.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes to use Hilbert–Schmidt Independence Criterion to evaluate the effectiveness of unlearning methods. This novel, statistic-based method facilitates evaluation without a retrained reference model or shadow models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The proposed method is well-motivated, novel, clear, and effective. This paper is well-presented and easy to read.\n2. The research questions studied in 4.1 are important.\n3. The discussion in Section 5 is comprehensive and valuable."}, "weaknesses": {"value": "1. Previously, unlearning methods usually played with the metrics, including accuracies and MIA results, via cherry picking hyperparameters with the best performance. It seems HSIC won't stop this game, but just provides one more metric to fit.\n2. I think HSIC might be extended for unlearning in generative models, but the authors did not discuss it."}, "questions": {"value": "The authors claim that HSIC evaluation does not require a retrained reference model. But I'm wondering: Is it possible that one day a new unlearning method achieves higher OTR than a retrained model's? If so, how do we analyze this phenomenon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WFRBxQZ7DV", "forum": "9IzfArmoHq", "replyto": "9IzfArmoHq", "signatures": ["ICLR.cc/2026/Conference/Submission10460/Reviewer_QQ14"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10460/Reviewer_QQ14"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875918834, "cdate": 1761875918834, "tmdate": 1762921757471, "mdate": 1762921757471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines machine unlearning evaluation at the subset level. Motivated by insufficiencies in existing evaluation methods such as retraining (which is computationally expensive) and membership inference attacks (MIA) (which lack effectiveness), the authors propose a tailored metric based on the Hilbert-Schmidt Independence Criterion (HSIC) to measure statistical dependence between model representations and the unlearned subset with a proxy. Experiments demonstrate that the proposed method serves as a reliable evaluation tool for machine unlearning."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. I find the paper very well written and pleasant to read. It is sufficiently motivated by an important problem in machine unlearning and proposes an elegant solution. The presentation is clear and easy to follow.\n\n2. The adoption of subset-level statistical dependence as an evaluation metric is both interesting and clever. It provides a well-defined tool for assessing model-data interactions in machine unlearning (and potentially for measuring neural network memorization more broadly). I believe the proposed method has strong potential to become a standard evaluation tool in the unlearning literature.\n\n3. The paper potentially opens many promising future research directions. I would be particularly interested to see its adaptation to other foundation models, such as those based on contrastive learning methods and generative models."}, "weaknesses": {"value": "1. It appears that the size of the unlearning subset plays an important role in the effectiveness of the proposed method. The experiments demonstrate that SDE works well for unlearning sets comprising 5-20% of the training data, which represents a relatively large proportion. However, consider a scenario where a user requests the unlearning of only a few samples: would SDE remain effective in this case? In other words, is it possible to quantify the minimum subset size at which SDE provides a reliable evaluation?\n\n2. The experiments show that Unroll obtains very low OTR. Is there any explanation for this phenomenon? Moreover, I am curious whether it is possible to attribute unlearning effectiveness at the individual sample level within the subset. While this may seem overly ambitious from a statistical perspective, I wonder if the authors have any insights or preliminary thoughts on this direction.\n\nNote that neither question necessarily requires additional experiments. The authors are welcome to include a discussion of these points and propose reasonable directions for future work."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zLMxrhWvs8", "forum": "9IzfArmoHq", "replyto": "9IzfArmoHq", "signatures": ["ICLR.cc/2026/Conference/Submission10460/Reviewer_3Ap9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10460/Reviewer_3Ap9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941876300, "cdate": 1761941876300, "tmdate": 1762921757031, "mdate": 1762921757031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces *Split-half Dependence Evaluation (SDE)*, a new way to measure how well a model has “forgotten” data during machine unlearning. Instead of retraining models or using membership attacks, it checks the statistical independence of model outputs on data subsets using the Hilbert–Schmidt Independence Criterion (HSIC)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear presentation and good illustration.\n2. Conceptually new evaluation framework."}, "weaknesses": {"value": "1. Unclear motivation: The benefits of the proposed method should be better explained and carefully compared with other proposed methods. For a proposal of an \"evaluation\" method, it is especially important to measure/prove/validate its properties, such as robustness, consistency, among others.\n2. Unjustified design choice: Some of the design choices are not well-explained, e.g., HSIC with specific RBF kernels, Mann-Whitney U-test, Jensen-Shannon Divergence, among others. To me, it feels artificial to see these components being introduced to the method without a clear justification."}, "questions": {"value": "1. Missing reference: Recently, there have been many related works focusing on unlearning evaluation, e.g., [1], where the idea of split sets is also explored.\n2. Weakness 1: Can you explain or provide additional experiments that demonstrate some desirable properties for the proposed evaluation metric, compared to others? Also, can you explain why your evaluation metric is better compared to the existing literature?\n3. Weakness 2: Can you provide some brief justification for each of the mentioned components, besides your main conceptual novel proposal (SDE)?\n\n[1]: Tu, Yiwen, Pingbang Hu, and Jiaqi Ma. Towards Reliable Empirical Machine Unlearning Evaluation: A Cryptographic Game Perspective."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4o2yoBFhmm", "forum": "9IzfArmoHq", "replyto": "9IzfArmoHq", "signatures": ["ICLR.cc/2026/Conference/Submission10460/Reviewer_UFX4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10460/Reviewer_UFX4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965126027, "cdate": 1761965126027, "tmdate": 1762921756591, "mdate": 1762921756591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new metric for evaluating unlearning methods, leveraging a clever connection to statistical independence. The main idea behind the paper is to measure the statistical dependence (using a method called the Hilbert-Schmidt Independence Criterion) between different subsets of examples, and use the computed (in)dependence scores as a proxy to evaluate unlearning efficacy. This allows one to evaluate unlearning without needing ground-truth retrained models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper identifies an interesting and important problem (improving the efficiency of unlearning evaluations)\n- The idea to use statistical dependence testing as a proxy for retraining is quite interesting and novel. It also seems quite convenient to implement, and thus practical to run in actual unlearning setups.\n- The paper conducts a thorough ablation study to understand the effect of different design choices on the efficacy of their approach"}, "weaknesses": {"value": "- The theoretical analysis seems quite handwavy and makes lots of approximations without really justifying why we might expect them to be true. To me this is the main concern with the paper, and one that permeates throughout the rest of the weaknesses (see also Q1).\n- In Table 4, it's seems concerning that the retrained model does not get 100% according to the metric - doesn't this suggest that the metric is either overly sensitive or otherwise misspecified with respect to the unlearning objective?\n- The fact that the method hinges on the selection of good reference sets also limits the practical applicability of the algorithm (this limitation is explicitly acknowledged by the authors)."}, "questions": {"value": "- Is there a simple theoretical setting where this evaluation is exactly the right thing to do?\n- Did the authors try other kernel functions outside of RBF?\n- Can the authors provide a computational cost analysis of their method? What are the main computational steps, and how does the cost scale with the various dataset sizes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hWo3FvHPjC", "forum": "9IzfArmoHq", "replyto": "9IzfArmoHq", "signatures": ["ICLR.cc/2026/Conference/Submission10460/Reviewer_kGt2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10460/Reviewer_kGt2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10460/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762163116993, "cdate": 1762163116993, "tmdate": 1762921756082, "mdate": 1762921756082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}