{"id": "baP3Dw7bWO", "number": 6593, "cdate": 1757989996269, "mdate": 1763635978484, "content": {"title": "Primal-Dual Policy Optimization for Adversarial Linear CMDPs", "abstract": "Existing work on linear constrained Markov decision processes (CMDPs) has primarily focused on stochastic settings, where the losses and costs are either fixed or drawn from fixed distributions. However, such formulations are inherently vulnerable to adversarially changing environments. To overcome this limitation, we propose a primal-dual policy optimization algorithm for online finite-horizon {adversarial} linear CMDPs, where the losses are adversarially chosen under full-information feedback and the costs are stochastic under bandit feedback. Our algorithm is the \\emph{first} to achieve sublinear regret and constraint violation bounds in this setting, both bounded by $\\widetilde{\\mathcal{O}}(K^{3/4})$, where $K$ denotes the number of episodes. The algorithm introduces and runs with a new class of policies, which we call weighted LogSumExp softmax policies, designed to adapt to adversarially chosen loss functions. Our main result stems from the following key contributions: (i) a new covering number argument for the weighted LogSumExp softmax policies, and (ii) two novel algorithmic components---periodic policy mixing and a regularized dual update---which allow us to effectively control both the covering number and the dual variable. We also report numerical results that validate our theoretical findings on the performance of the algorithm.", "tldr": "", "keywords": ["Safe Reinforcement Learing", "Adversarial Linear Constrained MDP", "Policy Optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/313a40e8aaf5f3377a7719f3d9950b2d4ef4d855.pdf", "supplementary_material": "/attachment/dd7fc5c65280af019aade0329e17c77451531008.zip"}, "replies": [{"content": {"summary": {"value": "The paper initiates the study of adversarial linear CMDPs by proposing an efficient primal-dual policy optimization method for linear CMDPs with adversarial losses and stochastic constraints. The algorithm assumes full feedback on the loss and bandit feedback on the constraints. This method attains $\\widetilde{O}(K^{3/4})$ regret and violation where $K$ is the number of episodes of the learning dynamic. Slater's condition is assumed, while the algorithm does not need the knowledge of the Slater's parameter $\\gamma$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main contribution of the work is the setting studied. Indeed, the literature on adversarial CMDPs, up to now, mainly focuses on tabular CMDPs, while linear ones may be more useful in many real world applications. Moreover, while I am not an expert on linear MDPs settings, the algorithm seems to have sufficient technical novelties. To conclude, the overall contribution of the work seems sufficient for acceptance, even if there are many weaknesses that I will highlight in the following."}, "weaknesses": {"value": "1. The different feedback for losses and constraints is not particularly reasonable in practice. I understand that the choice is made since bandit feedback and adversarial settings were too challenging while full feedback and stochastic settings are generally trivial. Still, I see it as a weakness of the work.\n2. A similar reasoning can be done for the nature of losses and constraints. Nonetheless, given the well known impossibility result on learning with adversarial constraints, this is not the first work on CMDPs focusing on the adversarial losses and stochastic constraints setting. \n3. No lower bounds are provided. Thus, the tightness of the bounds cannot be inferred.\n4. In tabular CMDPs, $K^{3/4}$ regret and violation can be attained without the dependence on the Slater's parameter $\\gamma$. Differently, in this work, the algorithm attains $K^{3/4}/\\gamma$ violation."}, "questions": {"value": "See weaknesses.\n\nMoreover, I am curious about what would happen to the bounds of the algorithm assuming the knowledge of the Slater's parameter $\\gamma$. Specifically, assuming $\\gamma$ is known, I believe that the space of the dual variables che be optimally bonded to something of order $H/\\gamma$, and no policy mixing/fixed shared approaches are needed. In such a case, can $\\sqrt K$ regret and violation bounds be attained by the algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r8Z1QmGMeu", "forum": "baP3Dw7bWO", "replyto": "baP3Dw7bWO", "signatures": ["ICLR.cc/2026/Conference/Submission6593/Reviewer_Nuny"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6593/Reviewer_Nuny"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760691069210, "cdate": 1760691069210, "tmdate": 1762918920532, "mdate": 1762918920532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel primal-dual policy optimization algorithm for learning in finite-horizon adversarial CMDPs with linear function approximation, where the reward (loss) functions may vary adversarially and the cost functions are stochastic but constrained. The proposed method is model-free and policy-based, operating directly in the parameter space of policies (e.g., softmax). The authors provide rigorous theoretical guarantees: the algorithm achieves $\\widetilde{O}(K^{3/4})$ regret and $\\widetilde{O}(K^{3/4})$ constraint violation against the best fixed feasible policy in hindsight. This work claims to be the first to provide sublinear regret and hard violation guarantees for adversarial CMDPs with generalization via linear features and policy-based learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel setting: The paper addresses a previously open setting—adversarial losses with linear CMDPs using policy optimization—for which no prior sublinear regret and violation guarantees existed.\n\nTheoretical rigor: The analysis is sound and carefully done, with clear derivation of the regret and violation bounds under realistic assumptions (e.g., smoothness, realizability).\n\nPolicy-based algorithm: Unlike prior works that rely on occupancy measure optimization or LPs, this work develops a computationally efficient policy-based method applicable to large-scale problems.\n\nNo Slater assumption: The algorithm does not require prior knowledge of a strictly feasible policy, increasing practical applicability.\n\nHard constraint violation: The results hold for non-compensated violation metrics, making the guarantee stronger than many earlier results."}, "weaknesses": {"value": "W1: Partial adversariality: While the rewards are adversarial, the costs are assumed to be i.i.d. stochastic. The title and claims could more explicitly reflect this partial adversariality to avoid potential misinterpretation.\n\nW2: Gap to optimal rate: The $\\widetilde{O}(K^{3/4})$ regret and violation are not optimal; prior works in simpler settings (e.g., tabular CMDPs) achieved $\\widetilde{O}(\\sqrt{K})$. A discussion of whether this gap is fundamental in the linear function setting is missing.\n\nW3: Limited empirical evaluation (if any): The submission does not include experiments, which makes it difficult to assess the practical effectiveness and convergence behavior of the algorithm, though the paper focuses on theory.\n\nW4: Suggestion for the preliminary: the structure that overviews the strength in the earlier part is quite understandable, however, there appears some terms without mathmatical definition such as dual variables and primal-dual algorithm. It would be better to explain them in the preliminary to enhance the readability of the non-experts."}, "questions": {"value": "Q1: Could the proposed approach be extended to fully adversarial cost functions (not just stochastic)? What obstacles arise in doing so?\n\nQ2: Is the $\\widetilde{O}(K^{3/4})$ bound tight under your assumptions, or do you believe a $\\widetilde{O}(\\sqrt{K})$ rate is possible with more sophisticated analysis or algorithm design?\n\nQ3: Would the method generalize to nonlinear function approximation (e.g., neural network policies) under similar assumptions?\n\nQ4: Have you considered empirical validation to support the theoretical findings? other than synthetic numerical task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jtJVLwIxys", "forum": "baP3Dw7bWO", "replyto": "baP3Dw7bWO", "signatures": ["ICLR.cc/2026/Conference/Submission6593/Reviewer_6Wqt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6593/Reviewer_6Wqt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706707188, "cdate": 1761706707188, "tmdate": 1762918919854, "mdate": 1762918919854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies adversarial linear CMDPs. The authors propose a primal-dual policy optimization algorithm that achieves $O(K^{3/4})$ regret bounds for both the objective and constraint violations. (The constraint-violation bound allows error cancellation, though.)\n\nSection 3 presents three technical challenges and their solutions, primarily focused on controlling the dual variable.\n\nNote: I am not an expert in adversarial linear CMDPs, although I am familiar with stochastic linear CMDPs. My comments below may therefore contain misunderstandings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper obtains the first sublinear regret bounds for adversarial linear CMDPs. The authors provide a rigorous analysis of their algorithm; while I did not check every proof detail, the high-level arguments appear reasonable."}, "weaknesses": {"value": "The writing has some room to be improved. Particularly, the motivation behind the technical challenges and the algorithm is not sufficiently clear. It would be nice if the paper briefly describes natural or \"naive\" approaches for linear CMDPs, and then explain why those approaches fail in the adversarial setting.\n\nFor example,\n- It is nice to state what existing algorithms (e.g., stochastic linear CMDPs) can achieve and why they are inadequate here. Then, it becomes easier for readers to appreciate the contributions. Section 3 currently assumes familiarity with prior work and jumps into technical fixes.\n- Novelty 1 introduces a policy-mixing technique to control the dual variable. It would be nice to explain in Section 3 why mixing is necessary instead of simply using the upper bound implied by a Slater condition (e.g., Ghosh 2022). This point becomes clearer later (around page 7), but until then it was unclear.\n- It would be nice to briefly explain what \"drift analysis\" mentioned in Novelty 3 is. The authors refer to it as a \"well-known method for bounding dual variables,\" but readers would appreciate if it is explained how the method can be applied here or why bounding the dual variable is essential for their regret bounds.\n\nIt appears that many technical challenges and the main contributions stem from the mixing technique. Because the paper does not provide an early, intuitive explanation of that technique, its significance is hard to assess.\n\nOverall, due to the lack of clear motivation and context, I score \"weak reject\" for this paper. However, I'm happy to reconsider my score if the authors can address these concerns."}, "questions": {"value": "1. Would you explain why controlling the dual variable is necessary for regret analysis in this setting?\n2. Would you describe how naive approaches fail to control the dual variable in adversarial CMDPs?\n3. Would you provide an intuitive explanation of why the mixing technique addresses these failures?\n\nWould you provide a high-level motivation for the algorithm? For example, how it differs from standard primal-dual algorithms for linear CMDPs and which components are critical for handling adversarial environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1anwhkHKa3", "forum": "baP3Dw7bWO", "replyto": "baP3Dw7bWO", "signatures": ["ICLR.cc/2026/Conference/Submission6593/Reviewer_j4Sg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6593/Reviewer_j4Sg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982038575, "cdate": 1761982038575, "tmdate": 1762918919415, "mdate": 1762918919415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies online finite-horizon adversarial linear CMDPs where per-episode losses are chosen adversarially with full-information feedback, costs are i.i.d. stochastic with bandit feedback, and the dynamics, loss, and cost all admit a linear feature structure as in linear MDPs. The authors propose a policy-based primal–dual algorithm that, for the first time in this setting, attains simultaneous sublinear regret and sublinear constraint violation, both of order $O(K^{\\frac{3}{4}})$ where $K$ is the number of episodes. To get there, they introduce a nonstandard weighted LogSumExp softmax policies, which arises because they must occasionally mix the current policy with a uniform policy to keep the dual variable under control, and that mixing destroys the simple softmax recursion. The main technical work is (1) proving a new covering-number bound for this mixed policy class, and (2) combining periodic policy mixing with a regularized dual update so that the policy class does not blow up too fast, and the dual variable stays $O(1)$ rates, which together leads to $O(K^{\\frac{3}{4}})$ regret. Experiments on a job-scheduling CMDP with adversarial losses confirm sublinear growth of both metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper gives the first sublinear result in this exact setting. Prior CMDP work with linear function approximation handled stochastic losses/costs, prior adversarial CMDP work handled tabular state spaces. This paper is the first to get both linear function approximation and adversarial losses and constraints with provable sublinear regret and violation. \n\nTechnically, the paper propose analysis to the covering number of weighted logSumExp softmax policy class and periodic mixing to control the dual. Through this way, it is possible to balance statistical complexity and constraint control.\n\nA lot of adversarial/safe CMDP papers fall back to occupancy-measure mirror descent but this paper is policy-based."}, "weaknesses": {"value": "The paper consider full-information losses, leaving more challenging bandit feedback setting unsolved. \n\nThe regret bound is still suboptimal due to the mixing technique but this is the cost for the additional constraint."}, "questions": {"value": "Can you show in the synthetic CMDP that (1) removing periodic mixing makes the dual blow up, or (2) forcing mixing every episode makes the regret curve worse? Right now the experiment doesn’t validate the two key algorithmic ideas."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Bgyt6MJpEo", "forum": "baP3Dw7bWO", "replyto": "baP3Dw7bWO", "signatures": ["ICLR.cc/2026/Conference/Submission6593/Reviewer_Zk7f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6593/Reviewer_Zk7f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6593/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989943221, "cdate": 1761989943221, "tmdate": 1762918919073, "mdate": 1762918919073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}