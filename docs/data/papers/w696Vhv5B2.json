{"id": "w696Vhv5B2", "number": 22483, "cdate": 1758331713140, "mdate": 1763715509448, "content": {"title": "OptiVer: Unleashing the Power of LLMs for Optimization Modeling via Dual-Side Verification", "abstract": "Building mathematical optimization models is critical in operations research (OR), while it requires substantial human expertise. Recent advancements have utilized large language models (LLMs) to automate this modeling process. However, existing works often struggle to verify the correctness of the generated optimization models, without checking the rationality of the constraints and variables or the validity of solutions to the generated models. This hampers the subsequent verification and correction steps, and thus it severely hurts the modeling accuracy. To address this challenge, we propose a novel LLM-based framework with Dual-side Verification (OptiVer) from both structure and solution perspectives, thereby improving the modeling accuracy. The structure-side verification ensures that the modeling structure of the generated optimization models aligns with the original problem description, accurately capturing the problem's constraints and requirements. Meanwhile, the solution-side verification interprets and evaluates the validity of the solutions, confirming that the optimization models are logically and mathematically sound. Extensive experiments on several popular benchmarks demonstrate that our approach significantly outperforms the state-of-the-art, achieving over 20% improvement in accuracy.", "tldr": "", "keywords": ["Optimization Modeling", "Operations research", "Large language models"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa01b30c102803a063a48d6a0f3e966823e34e7f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- The paper proposes Optiver, an LLM-based framework to improve the formulation of optimization models through dual-side verification - checking both the model structure and the solution\n- It uses a multi-agent approach: one set of agents performs structure-based consistency verification (via back-translation-type approaches) and another handles solution-side validity verification (by interpreting the actual solution itself for logical consistency)\n- Results on benchmarks show notable improvements compared to prior methods"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a timely problem, namely the verification issue of LLM-based optimization modeling\n- The empirical results are comprehensive, considering multiple benchmarks and ablation studies to indicate the importance of each algorithmic component\n- The approach of dual-side verification, and its inspiration in mutual information, is well-explained"}, "weaknesses": {"value": "- While the dual-side verification framing is novel, much of the structure (multi-agent framework, back-translation, verification prompts) resembles existing self-correction or multi-agent LLM paradigms. The paper may be more of an incremental refinement than a conceptual breakthrough.\n- The mutual information arguments are superficial—they’re not formally derived or validated empirically.\n- The performance analysis focuses on solving accuracy (SA), while the main focus of the paper is on verification. It would benefit from a more rigorous demonstration of verification performance, failure modes, and types of errors that are detected\n- Also, it is not clear what proportion of the reported gains comes from structure vs solution-side verification"}, "questions": {"value": "- Since GPT-4o-mini is used for both generation and evaluation, how do you avoid circular bias (the model verifying its own output)?\n- How would this generalize beyond simple OR tasks (e.g., LP/MIP) on stochastic/combinatorial problems\n- The paper feels more engineering, although this is not a major weakness IMO, it would be worth highlighting more clearly the methodological contributions.\n- I might have missed this, but are all baselines evaluated using the same underlying LLM?\n- Can the authors also report the token consumption to get a sense for the normalized performance improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b2oARovfki", "forum": "w696Vhv5B2", "replyto": "w696Vhv5B2", "signatures": ["ICLR.cc/2026/Conference/Submission22483/Reviewer_SBAA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22483/Reviewer_SBAA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652239514, "cdate": 1761652239514, "tmdate": 1762942236533, "mdate": 1762942236533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Dual-side Verification (OptiVer) framework to improve the modeling accuracy from both structure and solution perspectives."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Dual-side verification is a good idea to improve the solving accuracy of LLMs.\n- This paper proposes the OptiVer framework, which novelly implements bidirectional verification and improves the accuracy of optimization problem modeling."}, "weaknesses": {"value": "- Repeated interactions and calls between multi-agents may result in excessively long call times.\n- Similarly, when the problem is complex (long descriptions, many numerical values, etc.), repeated interactions with LLMs may introduce data errors. Have the authors considered this issue? And what is the maximum scale of problems OptiVer can handle?\n- For simple problems, such as simple MILP problems with only two variables, overly complex validation is unnecessary and may even introduce more errors."}, "questions": {"value": "As described in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TtbbN6FWPh", "forum": "w696Vhv5B2", "replyto": "w696Vhv5B2", "signatures": ["ICLR.cc/2026/Conference/Submission22483/Reviewer_FVPi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22483/Reviewer_FVPi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895068680, "cdate": 1761895068680, "tmdate": 1762942236304, "mdate": 1762942236304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the unreliability in the process of modeling optimization problems with large language models and emphasizes the crucial role of process supervision and verification. This paper proposes OptiVer, a method that performs effective verification at both the structural and solution aspects to enhance the reliability of the modeling process. Experimental results demonstrate that OptiVer surpasses all Reasoning Models, Fine-tuning Methods, and Prompt-based Methods in terms of solving accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Proposes verification of the modeling process from both structural and solution interpretation perspectives, introducing a novel approach for validating modeling.\n\n - Describes the measurement of reliable modeling from the perspective of mutual information, providing a theoretical metric for the semantic consistency of modeling structures.\n\n - The experiments in this paper successfully validate the effectiveness of both multi-level modeling and multi-perspective modeling."}, "weaknesses": {"value": "- Conflicting Use of Baseline: The paper says the old NL4OPT and ComplexOR benchmark have some mistakes, but uses it as the main proof that the new one is better. This weakens the argument.\n\n- The proposed Mutual Information metric lacks a specified estimation method and does not play a functional role in the methodology or experiments.\n\n- Although an ingenious framework is designed to verify the reliability of the modeling process, the reliability of the verification process itself may not be guaranteed under the Prompt-based Multi-Agent framework."}, "questions": {"value": "- The Challenges section in Chapter 3 mentions that NL4OPT and ComplexOR contain 36% and 12.6% erroneous cases, respectively. Does this potentially compromise the reliability of the reported 96.5% Solving Accuracy achieved by OptiVer(Full) on NL4OPT and 78.9% on ComplexOR?\n- The paper frequently mentions the statistical measure of mutual information. However, the mutual information mentioned herein merely serves a descriptive role in the proposed framework, why is its value not quantitatively estimated to measure the consistency between natural language descriptions and modeling structures, as well as the consistency between hierarchical structures and their reconstructed counterparts?\n- During the inference process for a single sample, how many times is the Refinement module executed? Is there a maximum triggering limit?\n- In the results shown in Table 2, the ablation results for medium-level and low-level settings on NL4OPT and ComplexOR respectively show no difference. What might be the reason for this? Are these experimental results statistically stable after multiple repetitions?\n- Regarding the results in Table 3, the ORLM outcomes are cited from the original or reproduced papers, while the ORLM+OptiVer results are obtained from experiments conducted in this paper. Does this potentially introduce inconsistencies in experimental conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "145TQwqtiA", "forum": "w696Vhv5B2", "replyto": "w696Vhv5B2", "signatures": ["ICLR.cc/2026/Conference/Submission22483/Reviewer_j6Mz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22483/Reviewer_j6Mz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996169112, "cdate": 1761996169112, "tmdate": 1762942236045, "mdate": 1762942236045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors proposed a multi-agent framework for formulating optimization problems from natural-language problem descriptions. Compared to a standard pipeline from problem descriptions to mathematical models, the authors added \"dual-side verification\" that mainly consists of (1) identifying structures from the problem description (by the structure distillation agent), (2) comparing the structure of the mathematical model (obtained by a structure interpretation agent) with the identified structures (done by a structure evaluation agent), and (3) verifying the solution (by a solution interpretation agent and a solution evaluation agent).\n\nI think that the idea of verifying the structures of and the solution to the formulated model makes sense and is somewhat novel. The effectiveness is also confirmed by the improved accuracy across benchmarks.\n\nMy main concerns are the correctness and the purposes of the two propositions, and the overhead of having to solve the problem (for solution verification)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The idea of verifying the structures of the formulated model is interesting and new.\n  * It is not a new idea to identify the structures from the problem description and incorporate them into the prompt. This has been done in OptiMUS (Ahmaditeshnizi et al., 2024).\n  * But the idea of verifying the structures of the formulated mathematical model with the identified structures is new.\n* The idea of interpreting and verifying the solution is interesting and new.\n* The overall pipeline seems to be \"simpler\", measured by the modeling time in `Table 6` of `Appendix B.1`. \n  * I do have comments on the efficiency, as described in details in \"Weaknesses\".\n* The accuracy of the proposed solution is higher than existing works, including fine-tuned models, on all benchmarks except OptMATH. For OptMATH, it is still the best among the prompt-based methods, and is only slightly worse than the fined-tuned model in OptMATH."}, "weaknesses": {"value": "* **Computational Overhead.** While the proposed method achieves state-of-the-art accuracy, I think it comes at a cost.\n  * Instead of passing the structures of the problem description into the prompt like in OptiMUS, the proposed method has an extra step of verifying the structures *after* the mathematical model is formulated. This means more agents and more computation.\n  * The proposed solution verification requires the solver to solve the problem *before* the formulation process is completed. This additional computational cost can be high for large-scale problems. The solution verification also requires more agents and more computation.\n  * In `Appendix B.1`, the authors compare the proposed methods and baselines in solving time (in seconds). The comparison is only done on two datasets. Based on the authors' comments that \"the solver execution time is short (under 0.01 seconds)\", I assume that all the tested problems are easy to solve. So this is not a fair comparison for the methods that do not require solving the mathematical problems.\n  * I also think comparing the solving time in seconds may not be the best metrics. Some models (e.g., GPT4o-mini used by the authors) might be faster than others. It may be more reasonable to compare the number of tokens used.\n* **Correctness and Purposes of Propositions.** I do not get the purposes of `Proposition 4.1` and `Proposition 4.2`.\n  * How and why is mutual information an indicator of the modeling accuracy?\n  * Even if mutual information was the correct indicator, the propositions just prove that the structure-side verification and solution-side verification *reduces* the mutual information. It was not proved that they *optimize* the lower bound. So the statements are not rigorous.\n  * I am not sure if the proof to Proposition 4.1 is correct. I assumed that the authors used the [Data Processing Inequality](https://en.wikipedia.org/wiki/Data_processing_inequality) for the inequality $I(X,Y) \\geq I(X, f(Y))$. Here the key assumption is that $f(Y)$ is conditional independent of $X$ when conditioned on $Y$. I am not sure if $S=\\mathtt{Distillation Agent}(D)$ is conditional independent of $M$ because $M=\\mathtt{Formulation Agent}(D,S)$ according to `Eqn. (2)`.\n  * I did not find the proof to Proposition 4.2."}, "questions": {"value": "Based on the points raised in \"Weaknesses\", I would like to see:\n* a more nuanced and more comprehensive study on the computational overhead;\n* a clarification on the proofs, the statements, and purposes of the two propositions.\n\nMinor typos to fix:\n1. Typo in `Line 081`: \"whether Othe solution\".\n2. Typo in `Appendix B.1`, \"DeVet\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "57Y66umGqK", "forum": "w696Vhv5B2", "replyto": "w696Vhv5B2", "signatures": ["ICLR.cc/2026/Conference/Submission22483/Reviewer_NAaT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22483/Reviewer_NAaT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22483/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762210117433, "cdate": 1762210117433, "tmdate": 1762942235806, "mdate": 1762942235806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}