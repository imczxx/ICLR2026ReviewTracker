{"id": "mIO3kVDC2L", "number": 14969, "cdate": 1758246314835, "mdate": 1759897338591, "content": {"title": "Not All Documents Are What You Need for Extracting Instruction Tuning Data", "abstract": "Instruction tuning improves the LLMs performance but depends on high-quality training data. Recently, LLMs have been used to synthesize data, enhancing training with seeds like question-answer (QA) pairs. However, this synthesis often results in instruction examples similar to the seeds, lacking diversity and biasing real applications. Thus, we propose to extract instruction tuning data from web corpus with much rich knowledge. The most straightforward strategy is to quickly retrieve domain specific documents from the corpus and then extract all QA pairs of these documents for tuning LLMs, which has two main limitations. (1) Extracting all QA pairs using LLMs is prohibitively expensive; and (2) These extracted pairs are not all beneficial for the downstream applications, and incorporating all of them for tuning may even hurt the model performance. To overcome the limitations,  we introduce $\\texttt{EQUAL}$, an $\\textbf{E}$ffective and scalable data extraction framework that iteratively interleaves  document selection and extract high-$\\textbf{QUAL}$ity QA pairs to optimize instruction tuning. $\\texttt{EQUAL}$ first clusters the document set based on the embeddings generated by contrastive learning. Then it leverages the multi-armed bandit based strategy to quickly identify document clusters where can extract high-quality QA pairs for training. This iterative framework significantly reduces computational costs while improving model performance much. Experiments on AutoMathText, KnowledgePile and StackOverflow across 9 downstream tasks demonstrate that $\\texttt{EQUAL}$ reduces computational costs by 5–10$\\times$ while improving accuracy by 2.5\\% on LLaMA-3.1-8B and Mistral-7B. Code and data is available at https://anonymous.4open.science/r/EQUAL-5CC6.", "tldr": "", "keywords": ["data extraction", "data efficient", "instruction tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/97ac87790ec363b368edcae20c7ef6a6804ed010.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper focuses on efficient instruction-tuning data curation from unstructured text in the documents. Specifically, it argues that the existing methods generate many QA pairs from all the documents using LLMs which is computationally prohibitive. In addition, many of them suffer from low-quality data problems. To fix this, the paper proposes an iterative solution, posed as a multi-arm bandit problem, where they first cluster all documents in the embedding space and quantify their quality based on the optimal transport distance between the QA pairs extracted from them and reference datasets. Further, they compute the diversity score of each cluster so that the QA extraction is focused on exploration first and exploitation later. They perform several experiments to show the effectiveness of the method on llama-3.1-8b and mistral models on math and code tasks. Further, they perform several ablations to show the usefulness of diverse design choices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The limitation pointed out by the work makes sense, and the proposed solution is tailored to fix them. In particular, the algorithm is reasonable which poses document selection and QA pair generation as a multi-bandit problem. \n\n2. The paper also proposes a contrastive learning approach to ensure that the features extracted from the documents and extracted QA pairs are similar in the representation space while they are far apart for original documents and negative QA pairs. \n\n3. The experimental results are quite extensive in terms of baselines and ablation studies. It definitely shows the general usefulness of the approach."}, "weaknesses": {"value": "1. Line 46-48 argues that the synthetic data generation LLMs suffer from lack of diversity because of their proximity to the seed examples. The proposed method may also suffer from the same problem since the algorithm is optimized to reduce the optimal transport distance between the QA pairs from the documents and SEED datasets (MATH/GSM-8K or MBPP). \n\n2. The choice of models and evaluation datasets is somewhat old. For instance, it is quite common to show some quantitative experiments on Qwen models nowadays. In addition, there are more challenging datasets such as AIME24, 25 for math and LiveCodeBench or CodeElo for Code.\n\n3. There is no comparison against state-of-the-art synthetic data generation methods such as OpenThoughts, s1k etc."}, "questions": {"value": "Mentioned above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NTesLkWWn6", "forum": "mIO3kVDC2L", "replyto": "mIO3kVDC2L", "signatures": ["ICLR.cc/2026/Conference/Submission14969/Reviewer_wiHJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14969/Reviewer_wiHJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702657319, "cdate": 1761702657319, "tmdate": 1762925303054, "mdate": 1762925303054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents EQUAL, a new framework for efficiently extracting instruction-tuning data (QA pairs) from large web-scale corpora. The core idea is to avoid the \"extract-all\" approach, which is prohibitively expensive. Instead, EQUAL first uses a \"warm-up\" phase (sampling 5% of the data) to train a contrastive learning model that aligns document embeddings with the QA pairs they contain. It then clusters the entire corpus and uses a Multi-Armed Bandit strategy to iteratively sample from the most promising clusters. The \"reward\" for the MAB is based on the Optimal Transport distance to a small, trusted reference set. The authors show this method significantly cuts costs (5-10x) while also improving downstream model performance on math, code, and open-ended tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper applied an innovative application of the Multi-Armed Bandit framework to sample from the most promising clusters iteratively. \n2. The paper's key insight is interesting: don't cluster raw documents, cluster them by the data you can get out of them. \n3. The paper is easy to follow and authors have done quite a lot of experiments including comprehensive downstream benchmarks. My core issues with this paper are that its impressive results seem to rest on a few critical assumptions that don't hold up to scrutiny, making me question its real-world practicality."}, "weaknesses": {"value": "My core issues with this paper are that its impressive results seem to rest on a few critical assumptions that don't hold up to scrutiny, making me question its real-world practicality.\n1. The entire framework is anchored to the Optimal Transport distance to a reference set, D_r. The paper's claim (in Appendix P) that using just 20 reference samples is nearly as effective as 1500 is not credible to me. Estimating a stable OT distance for high-dimensional embeddings from 20 samples is statistically dubious. More importantly, this setup assumes the reference set is a perfect, unbiased oracle. In reality, a small D_r is likely biased. The current framework doesn't just use this bias; it optimizes for it. \n2. The \"cold start\" cost is still prohibitive. The paper frames this as a low-cost solution, but the \"warm-up\" requires processing 5% of the corpus with a 72B-parameter model. For their 1.4M document dataset, that is 70,000 documents. This is not a \"warm-up\"; it's a massive, expensive inference job in itself. This barrier to entry is extremely high and is not adequately acknowledged.\n3. There might be a risk of cascading bias. Any bias in the reference set D_r gets learned into the contrastive learner, which in turn skews the entire clustering. Since the MAB only explores at the cluster level, it has no way to find high-quality, novel documents if they were already buried in a \"bad\" cluster. The method risks locking onto a local optimum very early in the process."}, "questions": {"value": "1. Can you run an experiment that proves this isn't just a \"bias amplifier\"? For instance, what happens if you use a D_r for the math task that only contains simple, 1-step arithmetic? Does EQUAL (as I suspect) fail to select clusters with complex, multi-step proofs?\n2. The 5% warm-up cost is a major barrier. How does the entire pipeline's performance (end-to-end) degrade if you only use a 1% or 0.1% warm-up budget? What is the minimum compute needed for this to actually work?\n3. How much of this success is just an artifact of using a giant 72B model for extraction? What happens if I use a smaller, less-capable 7B model? Does EQUAL still find the best data, or does it just find the data that is easiest for the weak extractor to parse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ikFOPVF2fJ", "forum": "mIO3kVDC2L", "replyto": "mIO3kVDC2L", "signatures": ["ICLR.cc/2026/Conference/Submission14969/Reviewer_5USS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14969/Reviewer_5USS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813462052, "cdate": 1761813462052, "tmdate": 1762925302781, "mdate": 1762925302781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to explore a scalable and cost-effective approach for automatically extracting high-quality instruction data from large-scale document corpora. To this end, the authors propose the EQUAL framework, which integrates contrastive learning–based embedding alignment with a multi-armed bandit–driven adaptive sampling strategy. By iteratively sampling documents and updating distributional similarity based on optimal transport, EQUAL effectively identifies and extracts high-quality instruction data. Experimental results demonstrate that EQUAL significantly reduces computational costs and outperforms multiple strong baselines across various tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a clear research motivation, and the proposed EQUAL method achieves a favorable trade-off between cost and performance, providing valuable insights for future work.\n2. The paper is well-organized, and the experiments in this paper cover three major datasets and multiple tasks, providing comprehensive evidence for the effectiveness of EQUAL. \n3. The authors conducted comprehensive ablation studies on the proposed framework and provided thorough analyses of the extracted data, further enhancing the persuasiveness of the proposed EQUAL framework."}, "weaknesses": {"value": "1. In lines 144–148, the authors emphasize that the main goal of this paper is to reduce the number of documents to be extracted in order to lower data construction costs while maintaining high model performance. However, the paper does not clearly show the performance gap between the instruction data extracted under high-cost settings and that obtained using the proposed method. This could be an important issue, as a large gap would call into question the practical significance of the proposed cost optimization.\n2. The main experiments do not report the amount of data used, making it difficult to assess the specific impact of data scale on the experimental results. The superior performance of the proposed method might be partially attributed to training on a larger amount of data."}, "questions": {"value": "1. Why did the authors not conduct experiments on the Qwen model and instead choose the relatively outdated Mistral-7B model? Including experiments on a more advanced model could further strengthen the persuasiveness of the paper.\n1. How large is the performance gap between EQUAL and high-cost instruction data extraction methods, such as directly using closed-source model APIs? If this gap is small, the proposed method would represent a highly valuable contribution by maintaining strong performance while significantly reducing data extraction costs.\n1. In the limitations section, the authors briefly note that EQUAL “may still contain imbalanced or culturally skewed content.” However, it remains unclear what proportion of the final dataset consists of such biased content and how it may affect model performance. It is recommended that the authors provide a brief analysis  of these biases.\n1. In the anonymized code repository provided by the authors, the following statement appears: “This repository contains the code for our paper [EQUAL: Efficient Scalable Data Extraction Framework for Instruction Tuning] published at ICML 2025.” Even if the paper was not actually accepted by ICML 2025, such a statement could serve as a unique identifier and potentially reveal the authors’ identities. I am not sure whether this violates the double-blind review policy of ICLR 2026, and I would like the authors to clarify this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JnnzbDWuly", "forum": "mIO3kVDC2L", "replyto": "mIO3kVDC2L", "signatures": ["ICLR.cc/2026/Conference/Submission14969/Reviewer_kjNN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14969/Reviewer_kjNN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14969/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896182569, "cdate": 1761896182569, "tmdate": 1762925302464, "mdate": 1762925302464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}