{"id": "Hxzjxu4y6A", "number": 1496, "cdate": 1756887526426, "mdate": 1763037125877, "content": {"title": "UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D Generation", "abstract": "High-fidelity 3D asset generation is crucial for various industries. While recent 3D pretrained models show strong capability in producing realistic content, most are built upon diffusion models and follow a two-stage pipeline that first generates geometry and then synthesizes appearance. Such a decoupled design tends to produce geometry–texture misalignment and non-negligible cost. In this paper, we propose UniLat3D, a unified framework that encodes geometry and appearance in a single latent space, enabling direct single-stage generation. Our key contribution is a geometry–appearance Unified VAE, which compresses high-resolution sparse features into a compact latent representation -- UniLat. UniLat integrates structural and visual information into a dense low-resolution latent, which can be efficiently decoded into diverse 3D formats, e.g., 3D Gaussians and meshes. Based on this unified representation, we train a single flow-matching model to map Gaussian noise directly into UniLat, eliminating redundant stages. Trained solely on public datasets, UniLat3D produces high-quality 3D assets in seconds from a single image, achieving superior appearance fidelity and geometric quality.", "tldr": "We propose UniLat3D, a novel one-stage 3D generation framework, which unifies geometry and appearance in a compact latent representation, achieving superior performance than common two-stage 3D generation models.", "keywords": ["3D Generation", "Diffusion Model", "Unified 3D Representation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/6cebd83ea64eca34159d06ca0e31551124c74e79.pdf", "supplementary_material": "/attachment/5f8aff1fd0e35e3db80407bd53ddf85440e8347f.zip"}, "replies": [{"content": {"summary": {"value": "UNILat3D proposes a unified framework for single-stage 3D generation by encoding both geometry and appearance into a single latent space called UniLat. Unlike previous methods like TRELLIS, which use a two-stage pipeline (first geometry, then appearance), UNILat3D uses a Unified VAE to compress sparse 3D features into a dense, low-resolution latent representation. This allows a single flow-matching model to generate 3D assets directly from noise, without intermediate steps."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Uni-VAE: By designing a Unified VAE that encodes both structural and visual information into a single, dense latent space (UniLat), the model is forced to learn the intrinsic correlation between shape and appearance. During generation, the single flow-matching model F_uni makes coherent decisions about what exists where and what it looks like simultaneously. This joint decision-making process ensures that the generated 3D structure is, by design, compatible with its texture.\n\nEfficiency: The unification of representation naturally enables a more efficient generation process, providing a significant practical advantage over more complex, multi-stage counterparts. By fusing geometry and appearance into UniLat, UNILat3D reduces the entire generative process to a single step: mapping noise to the unified latent. This is achieved with one flow-matching model. This architectural simplicity translates directly into speed. The model generates 3D Gaussians in ~8 seconds and meshes in ~36 seconds on an A100 GPU.\n\nThe method is trained on public datasets (e.g., Objaverse, ABO) and evaluated on Toys4K and a custom complex dataset. It shows competitive results in both qualitative and quantitative comparisons."}, "weaknesses": {"value": "1. Limited Conceptual Novelty and Heavy Architectural Dependence on TRELLIS\n\nThe most significant weakness of UNILat3D lies in its lack of fundamental innovation, as it largely builds upon the existing TRELLIS framework without a substantial conceptual leap.\n\nArchitectural Inheritance, Not Revolution: The core pipeline of UNILat3D is nearly identical to that of TRELLIS. Both methods begin by lifting multi-view images into a sparse 3D feature volume. Both rely on sparse Transformers as a core building block for processing these features. The design of the decoders for 3D Gaussians and meshes is also directly inherited. This high degree of architectural overlap positions UNILat3D less as a novel paradigm and more as a significant modification or an extension of the TRELLIS architecture.\n\nIncremental Contribution: The primary proposed innovation is the \"UniLat\" representation, which is created through a \"Sparse Feature Densification\" and \"Densified Feature Compression\" process. While this is a valid technical contribution, it can be perceived as an engineering refinement of the representation rather than a groundbreaking new idea. It essentially converts TRELLIS's explicit, sparse slat into an implicit and dense slat. The core idea of using a unified latent space for 3D generation is appealing, but the implementation here is heavily reliant on repurposing and modifying the components of its predecessor. A truly novel approach might have proposed a more radical architectural departure to achieve unification.\n\n2. Marginal and Ambiguous Performance Improvements\n\nThe empirical results fail to demonstrate a decisive advantage over existing methods, particularly its direct baseline, TRELLIS, which undermines the claim that unification is a significantly superior paradigm.\n\nLack of a impressive factor in quantitative metrics: Examining Table 1 reveals that the performance gains are minimal. For 3D Gaussian generation, UNILat3D achieves a CLIP score of 90.87 versus 90.70. These are modest improvements that do not constitute a clear breakthrough. In mesh-based generation, the ULIP and Uni3D scores are virtually identical to those of TRELLIS and Hunyuan3D-2.1."}, "questions": {"value": "1. The author should clarify the novelty of the paper\n2. More experimental results should be included to demonstrate the effectiveness of the proposed method"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "k6RktgvyVq", "forum": "Hxzjxu4y6A", "replyto": "Hxzjxu4y6A", "signatures": ["ICLR.cc/2026/Conference/Submission1496/Reviewer_omD2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1496/Reviewer_omD2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760456308035, "cdate": 1760456308035, "tmdate": 1762915785748, "mdate": 1762915785748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "t8QhQoCTMM", "forum": "Hxzjxu4y6A", "replyto": "Hxzjxu4y6A", "signatures": ["ICLR.cc/2026/Conference/Submission1496/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1496/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763037125113, "cdate": 1763037125113, "tmdate": 1763037125113, "mdate": 1763037125113, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed UniLat3D, a single-stage 3D generation framework that aims to unify geometry and appearance generation within a shared latent space, called UniLat. Unlike prior two-stage methods such as TRELLIS that separately model geometry and texture, UniLat3D employs a Unified VAE (Uni-VAE) that first densifies sparse voxel features, compresses them via 3D convolutions, and decodes them into dense latent representations supporting both mesh and 3D Gaussian outputs. A rectified flow model then maps Gaussian noise directly to the unified latents for end-to-end 3D generation. Experiments on Toys4K and other datasets suggest moderate improvements in appearance alignment, though geometric fidelity lags behind leading baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Practical motivation: addresses the inefficiency and misalignment of two-stage 3D pipelines.\n* Unified latent representation: enables joint modeling of geometry and appearance, and simplifies pipeline integration.\n* Open-data training: uses only public datasets, aiding reproducibility."}, "weaknesses": {"value": "* Limited novelty: the unification mechanism is essentially a dense voxel compression pipeline, and does not offer a clear theoretical or empirical breakthrough. The contribution is more an engineering variant of TRELLIS than a fundamentally new paradigm.\n* Dense voxel inefficiency: converting sparse voxels to dense grids defeats the main efficiency advantage of sparse voxel representations and limits scalability to high resolutions (e.g., 1024³+).\n* Missing or inconsistent visual comparisons: Fig.4 lacks mesh results of the proposed method, weakening fairness.\n* Computational cost: despite claims of efficiency, dense representation and large flow models still demand substantial resources (64 GPUs for 2 weeks).\n* Less rigorous baseline selection: TripoSR is a 3D reconstruction model, which is fundamentally mismatched with the task of 3D generation. Furthermore, the comparison omits several native 3D generation pipelines, such as TripoSG [1] and Hi3DGen [2].\n\n\n[1] TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models. ArXiv 2025.\n\n[2] Hi3DGen: High-fidelity 3D Geometry Generation from Images via Normal Bridging. ICCV 2025."}, "questions": {"value": "1. The mesh results in Fig. 4 are missing, please include them in the rebuttal to ensure fair visual comparison.\n2. How does the latent resolution influence the reconstructed geometry? The current ablation (Tab. 2) only reports the quantitative results on appearance rendering, but lacks metrics like Chamfer Distance or F-score to evaluation the quality of reconstructed geometry."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9Yg7cBdcpn", "forum": "Hxzjxu4y6A", "replyto": "Hxzjxu4y6A", "signatures": ["ICLR.cc/2026/Conference/Submission1496/Reviewer_VvUU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1496/Reviewer_VvUU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760601697350, "cdate": 1760601697350, "tmdate": 1762915785607, "mdate": 1762915785607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose UniLat3D, a framework that generates 3D assets in a single stage by unifying geometry and appearance into a compact latent representation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The method achieves good appearance fidelity while using only publicly available training data, unlike competitors that rely on proprietary datasets.\n\n2. The paper is well-written and easy to follow."}, "weaknesses": {"value": "The paper's central claim is that unifying geometry and appearance generation offers significant advantages. However, this claim requires substantial clarification and stronger justification. The paper does not adequately address why unified generation is fundamentally better than the established two-stage paradigm. Separate generation offers several well-documented advantages that are dismissed without proper discussion:\n\n- Independent control over geometry and texture enables flexible editing and iterative refinement\n- Compatibility with existing 2D diffusion models and traditional rendering pipelines\n\nThe claimed benefits of unification are underwhelming:\n- The visual improvements over Hunyuan3D and other baselines are marginal in the provided examples, making it unclear whether unified representation truly solves alignment issues.\n- The efficiency gains appear primarily attributable to the smaller model size (1.55B vs. 5.3B) and lower latent resolution (16³ vs. 64³), rather than the unified paradigm itself. Notably, TRELLIS achieves 5s generation for 3DGS compared to the proposed 8s.\n\nThe unification itself lacks innovative design. The core contribution reduces to: (1) densifying sparse features via zero-padding (Eq. 7), (2) applying standard 3D convolutions for compression (Eq. 8), and (3) training a diffusion model on the resulting latents. This is a straightforward engineering solution rather than a conceptual breakthrough."}, "questions": {"value": "1. Can you provide a more convincing speed comparison with two-stage models isolating the influence of model size and grid resolution?\n2. Demonstrate specific failure cases where two-stage methods produce geometry-texture misalignment that your unified generation resolves."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eNgHdoaGZL", "forum": "Hxzjxu4y6A", "replyto": "Hxzjxu4y6A", "signatures": ["ICLR.cc/2026/Conference/Submission1496/Reviewer_X7Zy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1496/Reviewer_X7Zy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751756631, "cdate": 1761751756631, "tmdate": 1762915785214, "mdate": 1762915785214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniLat3D, a framework for 3D asset generation from a single image. The method is presented as a \"single-stage\" solution to address the purported geometry-texture misalignment and high costs of two-stage pipelines (which generate geometry then appearance).\n\nThe core idea is to modify the VAE from prior work (TRELLIS) to \"unify\" geometry and appearance into a single compressed latent representation (UniLat). This is done by taking sparse 3D features, densifying them, and then compressing them with a VAE. A single flow-matching model is then trained to generate this UniLat from noise, which is subsequently decoded into 3D Gaussians (GS) or meshes.\n\nThe authors claim this unified approach achieves superior performance, particularly in appearance fidelity, while remaining efficient. However, the work appears to be a highly incremental modification of TRELLIS that introduces new bottlenecks and relies on questionable experimental comparisons."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses a Known Problem: The paper attempts to tackle a recognized limitation (geometry-appearance misalignment) in two-stage 3D generation models.\n\n2. Competitive Metric on GS: The 3DGS variant achieves a strong FD_DINOv2 score (Table 1), suggesting the unified latent may be beneficial for this specific metric, although this comes at the cost of slower inference speed."}, "weaknesses": {"value": "1. The paper's efficiency claims are undermined by its own data. The mesh generation comparison (36s at 512³ vs. 21s at 256³) is invalid due to mismatched resolutions, while the 3DGS comparison (8s vs. 5s) shows a clear regression in speed.\n\n2. The \"single-stage\" approach introduces a new, severe bottleneck. The authors admit their unified flow model cannot scale beyond a $16^3$ latent resolution without prohibitive computational cost. This directly contradicts the goal of solving the \"cost\" of two-stage models.\n\n3. The framework is a highly incremental modification of TRELLIS. The core change—the \"sparse-to-dense-to-compress\" VAE design—is counter-intuitive and presented without any ablation study to justify it over simpler alternatives.\n\n4. In qualitative comparisons (Fig 4, 7), the unified model's results are noticeably smoother and lack the fine-grained texture details visible in competing two-stage models, suggesting the unified latent forces a compromise that sacrifices appearance quality."}, "questions": {"value": "1. Can the authors provide a fair experimental comparison for mesh generation by running their method at the same 256³ resolution as TRELLIS? The current $512^3$ vs $256^3$ comparison is misleading and invalidates any claims about efficiency.\n\n2. The paper admits that a $32^3$ latent space is computationally prohibitive for the flow model. This suggests a severe scalability bottleneck. How can this be considered an \"improvement\" over a two-stage model, which can scale its geometry and appearance components independently? Doesn't this unified approach make the problem worse?\n\n3. The qualitative results (Fig 4, 7) consistently show a loss of high-frequency texture detail compared to other methods like Hunyuan3D. Is this loss of detail an inevitable consequence of forcing geometry and appearance into a single, compact latent space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZEm3xJgyQh", "forum": "Hxzjxu4y6A", "replyto": "Hxzjxu4y6A", "signatures": ["ICLR.cc/2026/Conference/Submission1496/Reviewer_Gsxt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1496/Reviewer_Gsxt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1496/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013235597, "cdate": 1762013235597, "tmdate": 1762915784983, "mdate": 1762915784983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}