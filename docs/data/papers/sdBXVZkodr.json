{"id": "sdBXVZkodr", "number": 8160, "cdate": 1758071326783, "mdate": 1759897802722, "content": {"title": "A Disentangled Low-Rank RNN Framework For Uncovering Neural Connectivity and Dynamics", "abstract": "Low-rank recurrent neural networks (lrRNNs) are a class of models that uncover low-dimensional latent dynamics underlying neural population activity. Although their functional connectivity is low-rank, it lacks disentanglement interpretations, making it difficult to assign distinct computational roles to different latent dimensions. To address this, we propose the **Disentangled Recurrent Neural Network (DisRNN)**, a generative lrRNN framework that assumes group-wise independence among latent dynamics while allowing flexible within-group entanglement. These independent latent groups allow latent dynamics to evolve separately, but are internally rich for complex computation. We reformulate the lrRNN under a variational autoencoder (VAE) framework, enabling us to introduce a partial correlation penalty that encourages disentanglement between groups of latent dimensions. Experiments on synthetic, monkey M1, and mouse voltage imaging data show that DisRNN consistently improves the disentanglement and interpretability of learned neural latent trajectories in low-dimensional space and low-rank connectivity over baseline lrRNNs that do not encourage partial disentanglement.", "tldr": "", "keywords": ["low-rank RNN", "partial disentanglement", "neural latent subspace"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4947f56d5a5323563af3de75ff9e3df765f6ad3c.pdf", "supplementary_material": "/attachment/bb3c2fbf35c6f09989612131db8866c0b3b8e389.zip"}, "replies": [{"content": {"summary": {"value": "In this paper, the authors propose DisRNN, a method for learning latent representations of neural dynamics that also obey groupwise statistical independence. After assessing the efficacy of their method on a synthetic dataset with known ground truth, they then apply it to two neural datasets, and argue that their method provides more interpretible fits, both in terms of recovered latent dynamics, and in terms of inferred connectivity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The formulation of low-rank RNNs in terms of a VAE bridges different perspectives and may not be well known. \n- The authors not only test their method with in synthetic settings with known ground truth, but also apply it to real neural data."}, "weaknesses": {"value": "- The motivation for the need for such a method was rather weak, and the presentation of the method felt confusing and disorganized at times. For instance, the history convolution kernel used in the formulation of DisRNN seems like an unnecessary complication that is unrelated to the stated goal of the method, and unless I'm mistaken it was also not clarified as to what was actually used for it in experiments (I'm assuming the trivial kernel with the full support at $\\tau=0$ was chosen?). \n- The partial correlation penalty proposed in equation 10 seems like the real crux of the paper, and that could in principle be applied posthoc with any RNN, including a (noisy) standard low-rank RNN. \n- Following on the previous point, for the first experiment, if all methods get approximately similarly good reconstruction of the observation sequences, and latent activities of DisRNN are simply an affine transform of the observation predictions, then surely other methods could yield equally good reconstruction of the \"true\" latents following an affine transform, no?\n- With regards to experiments involving real neural data, the authors seem to conflate interpretible predictions with accurate predictions. For instance, in Fig. 4, even if one accepts the premise that the inferred connectivity predicted by DisRNN is more interpretible (which is certainly up for debate), that is irrelevant if it turns out to be incorrect, and there is no reason a priori to think \"separable low-rank channels\" is more biologically realistic compared to a \"single entangled circuit\". Nonetheless, I understand that not having a notion of ground truth here makes it difficult to say more."}, "questions": {"value": "1. With regards to comparisons to other models: what's the difference between LINT and lrRNN in terms of experiment implementation? My understanding is both methods parameterize the weight matrix in a low-rank factorized form.\n2. Regarding Fig. 3, I'm not understanding how latent $R^2$ could be affected by rotations of the coordinate system. Rotations are contained within the set of linear transforms, so simply rotating the ground truth trajectories shouldn't affect their linear predictivity. Unless what is happening here is different models are being fit to different rotated versions of the dataset, but the writing seems to imply otherwise."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zAhvy8kRaK", "forum": "sdBXVZkodr", "replyto": "sdBXVZkodr", "signatures": ["ICLR.cc/2026/Conference/Submission8160/Reviewer_Lyob"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8160/Reviewer_Lyob"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683570015, "cdate": 1761683570015, "tmdate": 1762920125835, "mdate": 1762920125835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a variational inference framework for fitting low-rank RNNs to neural data. In low-rank networks the $N\\times N$ recurrent weight matrix $\\mathbf{W}$ can be decomposed by two smaller matrices  $\\mathbf{W=AB}$ of size $N\\times R$ and $R\\times N$. Previous (LINT and SMC; also a VAE) methods fit unconstrained $\\mathbf{A}$ and $\\mathbf{B}$ matrices and apply a post-hoc orthogonalisation step for visualising the latent dynamics (defined as a projection in the space spanned by the weights). In this submission, an additional regularisation term is applied during training the RNN that promotes a different decomposition of the $\\mathbf{W}$ matrix such that the resulting latents are disentangled."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The disentanglement term in the loss potentially leads to more interpretable latents compared to previous approaches.\n\nS2. The method shows improvement over baselines on a specific disentanglement $r^2$ metric. \n\nS3. The figures are generally informative and clear."}, "weaknesses": {"value": "W1. The authors write as their first contribution (line 064) that they obtain a generative RNN model. However as far as I understand the authors never show any generation / simulations of the fitted RNN (unlike previous LINT and SMC methods, which did show adequate simulations). In my opinion the author should either adjust their text to reflect they are interested in inference / $p(z|x)$ only, or to also show RNN simulations.\n\nW2. I think the paper requires some clarification of the methods:\n - W2.1 In sections 2 and 3 a history filter was included in the RNN equations. I could not find whether or not this history filter was also applied for the main experiments. If it was indeed applied, I am unable to see how we can disentangle whether the improvements over baseline methods come from the history filter or from the regularisation term. If it was not applied, my suggestion would be to exclude it from the main text (resulting in an easier to read methods section, and less confusion later on), and move the full equations with history filter to the supplementary.\n\n- W2.2. For the inferred latent trajectories, what exactly are you plotting here? In previous works (LINT, SMC and other work on low-rank RNNs) the latents are generally defined as projections of $\\mathbf{x}$ on the column space of the weights  / $\\mathbf{A}$, here it seems you are (at least for your method) considering latents as projections of $\\sigma(\\mathbf{x})$on the row-space / $\\mathbf{B}$ (equation (4)). Again it would be crucial to verify that the improvements over baselines come from the proposed regularisation, and not from a different definition of how the latents are computed. \n\n- W2.3 There are a couple of other methods related unclear to me, see Question section."}, "questions": {"value": "Q1. What is the difference between lrRNN and LINT, whether or not a Post-Hoc SVD orthogonalisation step was applied to the recurrent weight matrix before computing evaluation metrics?\n\nQ2. The learned recurrent matrices seem very similar between all methods (see e.g., A.2.2) - in fact, if I understand the method correctly, the improvements just comes from how we decompose the recurrent weights? If so could we not also find the desired decomposition easier after training, i.e., find a decomposition of the weights that minimises the disentanglement?\n\nQ3.   You also use your method to fit spiking data, but don't show any plots. Do you indeed learn a good generative model of spiking data when simulating the RNN?\n\nQ4. For the SMC posterior, did you plot the mean of the posterior (with a reasonable number of particles)? The explanation (line 320) for the lower performance in any case seem not right, in the original paper, also fits without using direction labels reached good $r^2$ values - but the authors of that paper seemed to have used a different dataset than here (of a similar task), so we can't make a direct comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QlZLeQHBvO", "forum": "sdBXVZkodr", "replyto": "sdBXVZkodr", "signatures": ["ICLR.cc/2026/Conference/Submission8160/Reviewer_EiA8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8160/Reviewer_EiA8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743824933, "cdate": 1761743824933, "tmdate": 1762920125500, "mdate": 1762920125500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to fit recurrent neural network models that have low-dimensional latent dynamics, which are divided into disentangled groups. Through applications to a few example sets of dynamics, the authors argue that their method outperforms prior approaches, both in terms of accuracy and interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is definitely timely, and the method seems interesting and (up to the issues of uncited work mentioned below) novel. The example applications are mostly convincing."}, "weaknesses": {"value": "The manuscript misses references to closely-related previous work. Most importantly, this is not the first paper to propose a VAE-inspired framework for learning disentangled representations in RNNs. Indeed, there is a previous work by [Miller and colleagues (NeurIPS 2023)](https://proceedings.neurips.cc/paper_files/paper/2023/file/c194ced51c857ec2c1928b02250e0ac8-Paper-Conference.pdf) that in fact adopts the same \"DisRNN\" acronym! So far as I can see, that prior art goes uncited in this work. See also my **Questions** about other uncited related works. \n\nI also have some more specific questions about benchmarking, which I detail under **Questions**."}, "questions": {"value": "- The authors must compare their method to the existing \"DisRNN\" method of [Miller et al. (NeurIPS 2023)](https://proceedings.neurips.cc/paper_files/paper/2023/file/c194ced51c857ec2c1928b02250e0ac8-Paper-Conference.pdf). Probably they also should adopt a different acronym to clearly distinguish their method. \n\n- The goal of discovering circuit-level hypothesis (*i.e.*, at the level of parceling the overall observed network into subcircuits) is similar to the goal of the so-called \"current-based decomposition\" method proposed by [Perich et al.](https://www.biorxiv.org/content/10.1101/2020.12.18.423348v2) in 2020. That work also passes uncited. One limitation - as far as I remember - of the results in Perich et al is that they assume the subnetwork structure is known, which the present method does not. Their method also seeks a post-hoc simplification of a trained, unconstrained network. It'd be interesting to at least comment on the relative merits of these methods. \n\n- I wish the authors did more to explore the effect of the convolution kernel $\\psi$. How does the choice of window size $L$ affect the results? In principle, within a linear network one could trade off latent dynamics generated through recurrence with filtering by this kernel. What choices of prior are reasonable here?\n\n- I did not find the choice of synthetic dataset well-motivated. Why did you choose this example? It could be interesting to benchmark your method on a vanilla multi-region RNN, as studied for instance by [Beiran and Clark (2025)](https://www.pnas.org/doi/abs/10.1073/pnas.2404039122). \n\n- It would be helpful if you could provide some investigation of how robust your method is, depending on the rank of dynamics, to subsampling measured neurons within each disentangled group. This is related to the fact experiments will likely be biased in which regions are well-sampled."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yceGyIaQQH", "forum": "sdBXVZkodr", "replyto": "sdBXVZkodr", "signatures": ["ICLR.cc/2026/Conference/Submission8160/Reviewer_8SDC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8160/Reviewer_8SDC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759626026, "cdate": 1761759626026, "tmdate": 1762920125082, "mdate": 1762920125082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript presents a new method to fit recurrent neural networks to experimentally measured neural population responses. Such RNN hold the promise to reveal key features of the mechanisms underlying the responses, both at the level of neural population dynamics and connectivity. The main innovation in the method proposed by the authors is that is identifies solutions for which dynamics factorizes into a small set of “disentangled” components that evolve in separate subspaces of the dynamics. The authors compare their method to several previously proposed methods, both on simulated responses and on experimental data from various neuroscience experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses a timely question in computational neuroscience\n\nThe authors put forward what appears to be a novel approach to fit neural population dynamics.\n\nThe proposed methods appear to the technically sound."}, "weaknesses": {"value": "I found the paper quite challenging to parse. The description of the methods and findings in the main text is dense, making an evaluation of the findings challenging.\n\nOverall, the authors could have done much more to demonstrate in what settings and how well their method works in retrieving dynamics and ground-truth parameters, and in what settings and how it fails. \n\nThe main simulations used to demonstrate the validity of the approach are based on a ground truth that by design factorizes into disentangled dynamical components. In this setting, it seems plausible that a method that explicitly incorporates such disentangled representations may fare better than alternative methods that do not make this assumption.\n\nConsidering the motivation of the work, the authors could have done much more to characterize to what extent various components of their model are identifiable or not. The authors limit themselves to fitting models without inputs, even though in the fits of experimental data such inputs arguably would be required to explain the responses, but would increase issues with non-identifiability.\n\nThe premise of the work is also somewhat questionable. In general, why would it be desirable to find solutions that have disentangled dynamics of the kind retrieved by their method? The goal of such RNN fits, as discussed by the authors themselves, it to get insights into the possible mechanisms and connectivity underlying the measured responses. To this end, the ultimate goal should be to retrieve the dynamics, mechanisms, connectivity that most closely resemble the ground truth, not those that result in disentangled representations."}, "questions": {"value": "How well does their method work in cases where the ground truth does not match their hypothesis of disentangled dynamics? Are there cases where their method does “hallucinate” disentangled dynamics (e.g. because it allows a description of the dynamics with overall fewer parameters) when the ground truth is not disentangled? What kind of dynamics are recovered in cases when the ground truth includes external inputs, but their fit does not?\n\nWhat is the role of the history convolution kernel? Why is it necessary, i.e. could its effect no be effectively mimicked by the recurrent dynamics itself? Does the inclusion of this kernel not result in issues with identifiability of the recurrent dynamics vs. the kernel?\n\nCan the authors independently validate the existence of the inferred subspaces of disentangled dynamics? For example, should the these subspaces not have a reflection in the structure of the covariance matrix of the data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xaLCwOWb6N", "forum": "sdBXVZkodr", "replyto": "sdBXVZkodr", "signatures": ["ICLR.cc/2026/Conference/Submission8160/Reviewer_jvnH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8160/Reviewer_jvnH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8160/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996657326, "cdate": 1761996657326, "tmdate": 1762920124677, "mdate": 1762920124677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}