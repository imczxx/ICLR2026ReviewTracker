{"id": "J04D9xBUCi", "number": 25233, "cdate": 1758365534490, "mdate": 1759896728690, "content": {"title": "Bridging the Preference Gap: Post-Training Input Rewriting with Large Language Models", "abstract": "Pre-trained language models, such as BERT and RoBERTa, have achieved remarkable performance in semantic classification tasks. Yet, their effectiveness varies with different textual expressions due to inherent preferences developed during training. To address this limitation, we propose a framework that leverages large language models (LLMs) to rewrite input texts in ways that better align with a target classifier's preferences, thereby enhancing its performance. To achieve this, we introduce a training process for the LLM and an automated method for constructing training data that encapsulates the classifier-specific preferences. Furthermore, we present a multi-sampling and filtering strategy to address instability in LLM outputs. Empirical evaluations on semantic classification datasets demonstrate that our framework significantly improves classifier’s performances.", "tldr": "", "keywords": ["textual entailment", "natural language inference"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a299b35537faee98a7172e8ec6160f4a245f083d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a post-training input rewriting framework that leverages large language models (LLMs) to rewrite input texts at inference time, aligning them with the “preferences” of a fixed downstream classifier (e.g., RoBERTa, BART) to boost its performance on GLUE. The method involves: (1) automatically constructing preference-aligned training data, (2) fine-tuning the LLM via SFT and DPO, and (3) applying a multi-sampling + classifier-embedding-based filtering strategy during inference."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The experimental pipeline is relatively complete, including ablation studies, generalization analysis, and comparisons with hand-crafted prompts.  \n- It presents a post-hoc, inference-time approach that avoids retraining the classifier.  \n- The paper empirically challenges the common assumption that reducing textual complexity improves model performance."}, "weaknesses": {"value": "- **Motivation is weak and conceptually muddled**: The term “preference” is used loosely without clear definition or causal analysis. The paper conflates model limitations (e.g., poor generalization to paraphrases) with inherent “preferences,” then proposes a complex workaround instead of addressing root causes.  \n- **Performance gains are marginal**: The method improves RoBERTa-Large by only **+1.08** GLUE points and BART-base by **+0.72**. The large gain on MRPC (+3.4) is isolated and likely stems from dataset noise rather than a robust mechanism, as noted by the authors themselves.  \n- **Excessive engineering complexity**: The pipeline requires SFT + DPO training of an LLM, construction of a separate classifier-embedding-based filter, and multi-sample inference—yet yields sub-1% gains. This makes the approach impractical for real-world deployment.  \n- **Missing key baselines**: No comparison with standard test-time augmentation (TTA), self-consistency decoding, or even simple prompt ensembling—methods that are far cheaper and often equally effective.  \n- **Limited scientific insight**: The work offers no analysis of what linguistic features the classifier actually “prefers” or how the LLM learns them. It remains a black-box performance patch with little theoretical or practical generalizability."}, "questions": {"value": "1. How do you formally define “model preference”? Can you provide evidence that it is a stable, intrinsic property of the classifier—not an artifact of training data bias or insufficient robustness?  \n2. Why not compare against standard test-time augmentation or ensemble-based inference, which are simpler and more widely adopted?  \n3. Are the reported gains (e.g., +1.08 on GLUE for RoBERTa) statistically significant? Were multiple random seeds or runs used to rule out variance?  \n4. Given the high computational and latency overhead of your pipeline (LLM rewriting + filtering), how do you justify its practical utility over simply fine-tuning the classifier further or using a stronger base model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nXqRHGZknh", "forum": "J04D9xBUCi", "replyto": "J04D9xBUCi", "signatures": ["ICLR.cc/2026/Conference/Submission25233/Reviewer_53au"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25233/Reviewer_53au"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760516983485, "cdate": 1760516983485, "tmdate": 1762943375887, "mdate": 1762943375887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work propose a framework that trains an LLM to rewrite input texts to assist a classifier to make better classification in semantic classification tasks according to its preference. The LLM rewriter is trained via SFT and DPO with data labeled by the classifier to align with the classifier's preference. A filter module is also trained using the classifer's embeds to filter inferior rewrites from the LLM rewriter's multiple samplings. Experiments show that the framework helps improve the performance of bert-based classifiers and LLMs on GLUE benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper propose a framework that leverage classifier's inherent preference to improve its accuracy rather than forcefully correct the preference, providing a new prospective for enhancing the performance of classifiers.\n2. The training process is concise and easy to conduct, which enhances the applicability of the framework."}, "weaknesses": {"value": "1. The experiment lacks a comparison with the effect of directly fine-tuning the classifier with diverse rewriting formats, which is necessary to prove that leveraging the preference of classifier is better than directly correcting it.\n2. The research field of this work is limited. This work focuses only on the semantic classification tasks, and GLUE is a relatively simple benchmark for current models. Since the authors said their work \"focuses on unlocking the upper capability boundaries of task models through preference-guided input rewriting\" in the conclusion, it is necessary to validate this method on more tasks, such as instruction following.\n3. The base models selected in the experiments are out-dated. Stronger baselines should be taken into account, such as reasoning models like Qwen3-8B and other LLM-based classifiers."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B60OVHR2lj", "forum": "J04D9xBUCi", "replyto": "J04D9xBUCi", "signatures": ["ICLR.cc/2026/Conference/Submission25233/Reviewer_7n88"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25233/Reviewer_7n88"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888564979, "cdate": 1761888564979, "tmdate": 1762943375678, "mdate": 1762943375678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework that employs large language models (LLMs) to rewrite input texts to align with the preferences of a target classifier. The authors propose a training paradigm for the LLM, accompanied by an automated data construction pipeline that encapsulates classifier-specific characteristics. A multi-sampling and filtering strategy is further introduced to mitigate the inherent instability of LLM-generated outputs. Empirical evaluations on semantic classification datasets demonstrate that the proposed framework yields improvements in classifier performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper empirically validates the distinction of model preferences from human linguistic cognition, demonstrating that traditional text complexity metrics, such as sentence length and lexical rarity, cannot reliably predict model behavior.\n2. This paper proposes a method to train LLMs to capture the preferences of the Classifier, along with a technique for the automated construction of training data.\n3. This paper presents a method that combines multiple sampling with a filtering strategy to address the issue of instability in the outputs generated by the LLM."}, "weaknesses": {"value": "1. The practicality of the approach appears limited. In the title of the paper, it seems that the authors propose an empirical paradigm; however, the scope of the study is restricted to text classification tasks, specifically the MRPC, MNLI, and SST-2 datasets. These benchmarks have already achieved near-saturated performance (e.g., 96.20% accuracy with RoBERTa-Large on SST-2). Hence, doubts remain about the meaningfulness of the work, and it is unclear whether the proposed framework can generalize to more diverse and complex scenarios, such as text generation or reasoning tasks.\n2. Although the authors claim that \"*Empirical evaluations on semantic classification datasets demonstrate that our framework **significantly** improves classifiers' performances,*\" the reported performance gains are relatively small and may fall within the range of random variation (e.g., +0.45% on QNLI with BART-Base). To substantiate the claim of significant improvement, multiple experimental runs and statistical significance tests should be conducted. Moreover, the efficiency of the proposed approach is questionable, as the framework requires multiple sampling runs to obtain the final results, which could substantially increase computational overhead.\n3. The experimental evaluation lacks an ablation study or a detailed analysis of the proposed filtering module. It remains unclear how much the filtering component performs and how it compares to the conventional RoBERTa classification. A more comprehensive examination of this module would strengthen the empirical validity of the work."}, "questions": {"value": "See \"Weaknesses.\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0w7ZWcnELN", "forum": "J04D9xBUCi", "replyto": "J04D9xBUCi", "signatures": ["ICLR.cc/2026/Conference/Submission25233/Reviewer_ShwC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25233/Reviewer_ShwC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971089368, "cdate": 1761971089368, "tmdate": 1762943374745, "mdate": 1762943374745, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a post-training framework that uses LLMs to rewrite classification inputs according to a classifier’s inherent preferences. It includes SFT, DPO, and a filtering stage based on classifier embeddings. Experiments on GLUE show small but consistent gains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The shift from \"eliminating preferences\" to \"adapting to preferences\" is interesting.\n2. The three-stage training from SFT, DPO to Filter, is technically sound in isolation.\n3. In the current experiments, the effectiveness on the selected baselines and benchmarks is demonstrated, and through ablations, the authors effectively show the necessity of each component."}, "weaknesses": {"value": "1. The approach is conceptually interesting but not a strict or theoretically grounded post-training method. Training stability and convergence are not statistically validated.\n\n2. The selection of datasets and baselines is limited, lacking generalization analysis.\n\n3. Empirical improvements are minor; significance not verified.\n\n4. The observed performance improvements might stem from the LLM memorizing task-specific linguistic patterns rather than capturing genuine preference alignment. I suggest adding evaluations on out-of-domain datasets to verify the robustness  of the proposed method.\n\n4. Minor presentation errors – e.g., (a.1) and (a.4) are mentioned but missing in Figure 2."}, "questions": {"value": "1. How stable is the DPO-based training process? Have you observed consistent convergence across multiple random seeds?\n\n2. Could the filter generalize across classifiers, or must it be retrained for each new target model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "USRJ9ysNgw", "forum": "J04D9xBUCi", "replyto": "J04D9xBUCi", "signatures": ["ICLR.cc/2026/Conference/Submission25233/Reviewer_oBL7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25233/Reviewer_oBL7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25233/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978834717, "cdate": 1761978834717, "tmdate": 1762943374473, "mdate": 1762943374473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}