{"id": "6UaXec8aUt", "number": 19293, "cdate": 1758295122668, "mdate": 1763715195463, "content": {"title": "HLStrans: Dataset for C-to-HLS Hardware Code Synthesis", "abstract": "High-Level Synthesis (HLS) enables hardware design from C/C++ kernels but requires extensive transformations, such as restructuring code, inserting pragmas, adapting data types, and repairing non-synthesizable constructs, to achieve efficient FPGA implementations. While large language models (LLMs) show promise in automating these transformations, progress has been limited by the absence of large-scale, well-structured datasets. Existing HLS datasets focus primarily on resource estimation, lack paired C/HLS examples with testbenches, and cover only a narrow set of optimizations.\nWe introduce HLStrans, the first benchmark-scale dataset for LLM-driven C-to-HLS synthesis. HLStrans contains over 124K paired C/HLS programs for real-world applications, with full testbenches and synthesis-based annotations of latency and resource usage. The dataset systematically captures five categories of transformations and is enriched by an automated augmentation pipeline combining LLMs, Monte Carlo Tree Search (MCTS), and Design Space Exploration (DSE). We benchmark state-of-the-art LLMs on HLStrans, demonstrating that retrieval and fine-tuning significantly improve success rates and performance.", "tldr": "", "keywords": ["Large language model", "Dataset", "Hardware Codes"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1260ebf7f9eb2e40040b67c72b8a2462971949ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces HLStrans, a dataset containing 124K paired C/HLS programs for training LLMs to transform C code into optimized High-Level Synthesis (HLS) code for FPGAs. The dataset is created by collecting 309 base programs from various sources and augmenting them using an automated pipeline combining LLMs, Monte Carlo Tree Search (MCTS), and Design Space Exploration (DSE). The authors benchmark various LLMs showing improvements from retrieval and fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Target is real and timely: LLMs for C→HLS need paired before/after code plus testbenches.\n(2) Dataset is well-scaffolded around real HLS transformations, which is closer to what actual HLS engineers do than “just insert a pragma.”\n(3) They try to close the loop with EDA feedback (Vitis), which is the right direction for LLM-in-the-loop hardware optimization."}, "weaknesses": {"value": "(1) The augmentation relies entirely on DeepSeek-R1 and automated synthesis. There is no mention of expert validation of the generated samples' quality or correctness.\n(2) All synthesis on Xilinx Alveo U55C, and thus it is unclear if insights transfer to other FPGAs (Intel, Lattice, etc.)."}, "questions": {"value": "(1) Please make an explicit comparison to Forgebench (2025): what does HLStrans offer that Forgebench doesn’t, besides testbenches? Do you cover more transformation types, or just organize them differently?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P1p84N2loK", "forum": "6UaXec8aUt", "replyto": "6UaXec8aUt", "signatures": ["ICLR.cc/2026/Conference/Submission19293/Reviewer_HLdE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19293/Reviewer_HLdE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873232185, "cdate": 1761873232185, "tmdate": 1762931247884, "mdate": 1762931247884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HLStrans, a large-scale benchmark dataset for C-to-HLS (high-level synthesis) code generation, targeting the automatic transformation of standard C/C++ kernels into synthesizable, hardware-optimized HLS code. The dataset comprises over 124,000 paired C/HLS programs, complete with testbenches and synthesis-based performance/resource annotations. The authors present a three-stage pipeline for dataset construction involving open-source collection, automated augmentation using LLMs, Monte Carlo Tree Search (MCTS), and design space exploration (DSE). The paper benchmarks multiple LLMs and demonstrates that fine-tuning and retrieval-augmented techniques using HLStrans result in clear improvements in synthesis rates, optimization, and code quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. HLStrans provides a benchmark-scale, well-structured dataset for C-to-HLS transformation that includes paired pre/post-HLS code, testbenches, and synthesis-based resource/latency feedback. Compared to previous datasets (Table 1), HLStrans has greater diversity, size, and supports a broader range of transformation tasks.\n\n2. Figure 1 concretely illustrates the transformation space covered (T1–T5), moving beyond mere pragma insertion (the focus of most prior works) to include code restructuring, data-type adaptation, algorithm repair, and function remapping."}, "weaknesses": {"value": "1. While empirical results are solid, there is a lack of explicit theoretical analysis linking the diversity/scope of the dataset to expected generalization properties of LLMs trained/fine-tuned on it. There is little quantitative discussion on statistical diversity or representativeness of the underlying C/HLS patterns, which is crucial if the dataset is to become a benchmark standard.\n\n2. The paper overlooks Wan et al. (2024), which introduced the Chrysalis dataset — an LLM-aided framework for HLS defect generation and functional verification. While Chrysalis focuses on bug injection rather than optimization, it is one of the earliest datasets coupling LLMs with HLS code transformation, synthesis feedback, and verification. Failing to acknowledge or contrast with this work substantially weakens the claimed novelty of HLStrans as the “first LLM-oriented C-to-HLS dataset.”\n\n3. Figure 5 reports extremely large “speedup (×)” values, but the paper does not specify how latency was measured or controlled. Since HLS latency varies across synthesis runs and tool settings, the lack of a fixed evaluation protocol or success-rate reporting makes these results difficult to interpret or reproduce."}, "questions": {"value": "1. Can the authors provide more quantitative evidence about the dataset’s diversity/coverage—e.g., statistical measures on code structure, transformation frequency, or functional domain clustering?\n\n2. How was the latency in Figure 5 obtained—were multiple synthesis runs averaged, and were failed or unsynthesizable cases excluded from the reported speedups?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RJKHVHdIPT", "forum": "6UaXec8aUt", "replyto": "6UaXec8aUt", "signatures": ["ICLR.cc/2026/Conference/Submission19293/Reviewer_AMsZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19293/Reviewer_AMsZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929982018, "cdate": 1761929982018, "tmdate": 1762931247479, "mdate": 1762931247479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents HLStrans, a dataset for LLM-driven C-to-HLS synthesis. The main contributions of HLStrans include: open-source HLS examples with testbenches, an augmentation pipeline to generate diverse designs, and evaluation of LLMs on success rates and performance."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Datasets for HLS are important and urgently needed for the EDA community\n* Well-written background about HLS toolflows"}, "weaknesses": {"value": "* This work specifically evaluates a single HLS tool; how about other HLS tools, such as Bambu HLS, Cadence Stratus HLS, and Siemens Catapult HLS?\n* The paper is missing reasoning about functional failure and synthesis failure. Are they caused by the same problem? It would be helpful to have classifications of failures and break down the importance of these failures over the benchmarks.\n* This work tries to cover the whole HLS flow but misses a lot of ablation studies at each HLS pipeline stage.\n* I am not sure if the data augmentation evaluation is sound. Why is a higher percentage better in this case? For example, 100% of the programs involve T2 - not all programs need to be unrolled. It seems less diverse to me.\n* The paper is missing details about test bench generation. How are the test data generated for the test bench? What is the coverage over different hardware interfaces?"}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3NNX37YMo4", "forum": "6UaXec8aUt", "replyto": "6UaXec8aUt", "signatures": ["ICLR.cc/2026/Conference/Submission19293/Reviewer_jjU3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19293/Reviewer_jjU3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947425815, "cdate": 1761947425815, "tmdate": 1762931246922, "mdate": 1762931246922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents HLStrans, a large-scale dataset for C-to-HLS code transformation. The dataset includes over 124K paired C and HLS programs with testbenches, covering varying transformation types. It introduces an automated augmentation framework that combines LLMs, MCTS, and DSE to generate optimized HLS variants. Experiments demonstrate that retrieval-based prompting and finetuning on HLStrans can notably improve synthesis success rates and latency reduction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It proposes the first large-scale benchmark for C-to-HLS transformation, filling a missing piece in the LLM-assisted EDA field.\n\n2. The automated augmentation framework demonstrates good soundness, and the integration of MCTS and DSE may provide insights for other LLM-assisted EDA tasks.\n\n3. It conducts comprehensive benchmarking of multiple models and prompting strategies using meaningful synthesis-level metrics."}, "weaknesses": {"value": "1. The major concern lies in the limited generalizable insights and scientific contributions this work provides to the community. Although it successfully demonstrates the application of LLMs, MCTS, and DSE tools in crafting an EDA dataset, it remains unclear how the findings can generalize to other problems or advance the broader fields of LLM and EDA research.\n\n2. There already exist LLM4EDA datasets created through strategic prompting or design space search. The authors are expected to provide a methodological comparison to clarify whether their proposed approach represents a superior or more scalable pipeline for future LLM4EDA dataset construction.\n\n3. It is also unclear whether C-to-HLS is a sufficiently meaningful task for researchers and practitioners in this domain. Given that HLS closely resembles C, HLS experts can readily add pragmas to convert C to HLS, while algorithm developers typically do not work on such conversions. Therefore, the C-to-HLS task, due to the strong similarity between the two languages, may be less impactful compared to more practical tasks like Verilog generation."}, "questions": {"value": "My questions have been included in the weakness section. In addition, I would like the authors to address the following two questions:\n\n1. Do the authors think that future LLM4EDA datasets should be crafted using the pipeline proposed in this work?\n\n2. Which group of users would most benefit from the proposed C-to-HLS benchmark and the finetuned LLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hhgA8V33sO", "forum": "6UaXec8aUt", "replyto": "6UaXec8aUt", "signatures": ["ICLR.cc/2026/Conference/Submission19293/Reviewer_qdH6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19293/Reviewer_qdH6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963835894, "cdate": 1761963835894, "tmdate": 1762931246487, "mdate": 1762931246487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}