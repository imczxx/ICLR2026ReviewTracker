{"id": "q2VKSHWhAS", "number": 22464, "cdate": 1758331430412, "mdate": 1759896864705, "content": {"title": "SecFPP : Secure Federated Prompt Personalization for Vision Language Models", "abstract": "Prompt learning has emerged as an effective and widely-adopted approach for customizing pre-trained vision language models (VLMs) to user-specific downstream tasks. To tackle data shortage and heterogeneity across multiple users, federated prompt personalization (FPP) has received significant attention as an effective method to harmonize customized performance and pre-trained model generalization capability. However, user-specific prompts, as valuable intellectual assets, face increasing privacy risks such as prompt stealing attacks. Though conventional privacy-preserving techniques such as differential privacy can mitigate these risks by adding noise masks to prompt parameters, they can incur severe performance degradation due to prompt sensitivity. In this work, we propose SecFPP, a secure federated prompt personalization protocol, that reconciles the trade-off among model generalization, local personalization, and privacy preservation. SecFPP delivers state-of-the-art performance under severe data heterogeneity, while using secure multiparty computation primitives to provide formal privacy guarantees without utility loss. The proposed protocol employs a decoupled prompt adaptation strategy by decomposing user prompts into federated and local components, thereby improving personalization performance in multi-granular unbalanced data distributions. We develop a privacy-preserving adaptive clustering algorithm for federated prompts to capture different domains or dataset heterogeneity while using the local prompts to adapt downstream tasks and capture the class heterogeneity. We validate the security of SecFPP theoretically and empirically. Extensive experiments comparing SecFPP with non-private and privacy-preserving baselines demonstrate its superior personalization accuracy. Moreover, comparisons with existing privacy-preserving frameworks highlight that SecFPP significantly improves the privacy-performance trade-off in FPP, simultaneously delivering strongest privacy guarantees and enhanced personalization.", "tldr": "This paper presents a novel protocol to achieve federated prompt personalization for VLMs with provable privacy guarantees and state-of-the-art performance under severe data heterogeneity.", "keywords": ["Prompt Personalization", "Federated Learning", "Privacy Protection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff1d150603bf383d51b000e5716be430b16ad147.pdf", "supplementary_material": "/attachment/4e1d3c38edc4a03e2720395306695a7b5a5c274e.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes SecFPP, a federated prompt personalization framework that provide formal privacy. SecFPP operates as a distributed k-mean clustering algorithm and provides privacy via secure aggregation protocol. In SecFPP framework, each user maintains a local prompt and a shared federated prompt that is updated via k-mean clustering. In each federated communication round, each user secret shares their truncated personalized prompt and federated prompt using Lagrange coded computation. The server maintains and updates a cluster assignment which is distributed to all users every round."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Motivation is well stated, and the problem the paper tries to address is important in multimodal LLM framework.\n* Experiment setting is clear and described in detail. The performance results support the author's claims."}, "weaknesses": {"value": "* While the idea is interesting, the proposed method is just a combination of federated k-mean clustering with secure aggregation, both of which have been extensively studied in the past decades, limiting the novelty.\n\n* Algorithm 1 is vague with a lot of missing details. What are $\\nabla_{G,i} \\mathcal{L}$ and $\\nabla_{L,i} \\mathcal{L}$ and how are they computed? They are not introduced anywhere in the paper. Step 7 is very confusing and incomplete. What exactly does $PCA(.)$ compute? How is $P_i^{(t+1)}$ computed? It seems that $P_i^{(t+1)}$ is not known until step 12.\n\n* The author points out the existing challenge of gradient-based privacy attacks in federated learning setting (line 150), however, the proposed method does not prevent such attacks. The shared components, i.e. personalized prompt and federated gradients, may have information about the gradient of the local components. This is a serious gradient leakage which is susceptible to gradient-based attacks.\n\n* I disagree with the author's claim that the proposed method has negligible overheads (line 90). There are lots of communication overheads in SecFPP compared to a standard federated learning framework and other FPP baselines. First, users share both the personalized prompt $\\bar{P_i}$ (Algorithm 1 step 9) and the federated gradients (Algorithm 1 step 10) to the server, while previous FPP baselines only need to share the latter. Second, the secure aggregation protocol requires user-to-user in addition to user-to-server communication as shown in Algorithm 2, while other FPP methods only require user-to-server communication. Furthermore, the protocol is applied twice to the personalized prompt and the federated gradients, adding extra overheads.\n\n* While dimension reduction may reduce some communication overhead, per-round SVD computation is expensive. A detailed communication-computation cost tradeoff should be discussed to justify if the use of truncated SVD brings any benefit."}, "questions": {"value": "* What does $F(.)$ compute mathematically? How is the loss computed when $P_i$ involves two separate components?\n\n* What are the dimensions of $P_i$ before and after dimension reduction?\n\n* How is the accuracy in Table 2 computed? How can one interpret the personalization and generalization based on the results, especially for single datasets (class heterogeneity)?\n\n* What are the communication and computation overheads of the proposed method compared to other baselines? It would be helpful if the author can show the overhead comparison theoretically and empirically.\n\n* Does SecFPP provide equivalent privacy guarantee to DP-FPL? How does one quantify the privacy provided by secure aggregation and differential privacy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bWZE5pMtwb", "forum": "q2VKSHWhAS", "replyto": "q2VKSHWhAS", "signatures": ["ICLR.cc/2026/Conference/Submission22464/Reviewer_q7fU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22464/Reviewer_q7fU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761189097990, "cdate": 1761189097990, "tmdate": 1762942229564, "mdate": 1762942229564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SecFPP, a framework for secure federated prompt personalization that combines secret sharing and Lagrange coded computation. Prompts are decomposed into federated and local parts, and a privacy-preserving clustering method (SecPC) aggregates prompts within domains. Mutual information analysis supports the theoretical privacy claims, and experiments under multi-domain and non-IID settings show strong performance even under strict privacy constraints."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work addresses a timely and practically important problem by integrating privacy-preserving computation with prompt personalization.\n\n2. The proposed design is modular and interpretable, with each component clearly targeting a specific type of heterogeneity.\n\n3. Theoretical analysis and empirical results complement each other, providing a balanced and convincing evaluation."}, "weaknesses": {"value": "1. The security analysis relies on the honest-but-curious assumption, leaving stronger adversarial scenarios such as collusion or data manipulation insufficiently discussed.\n\n2. The protection mechanism in the clustering stage is not fully clear;  small or imbalanced clusters could expose additional information, which warrants further analysis.\n\n3. Quantization and finite-field operations may introduce nontrivial numerical errors, yet their effects on model performance and security are not systematically evaluated.\n\n4. A more comprehensive exploration of hyperparameters would be valuable, including systematic experiments and analysis of how different settings affect overall performance.\n\n5. The scalability discussion remains brief;  the communication and computation overhead of large-scale deployments should be further examined."}, "questions": {"value": "1. How does SecFPP handle adversarial settings involving collusion or active attacks, and what is the tolerable number of compromised participants?\n\n2. How are very small or imbalanced clusters treated in SecPC, and could they cause additional leakage risks?\n\n3. What guidelines exist for choosing the quantization parameters (Î», q), and how sensitive is the performance to these settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "csufJwindr", "forum": "q2VKSHWhAS", "replyto": "q2VKSHWhAS", "signatures": ["ICLR.cc/2026/Conference/Submission22464/Reviewer_ZHqX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22464/Reviewer_ZHqX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923087046, "cdate": 1761923087046, "tmdate": 1762942229280, "mdate": 1762942229280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new federated algorithm for prompt personalization for vision-language models. The algorithm, SecFPP, uses a novel secure federated clustering method to group clients, and each client refines its global prompt within its own cluster. The experimental results show, in general, higher accuracy than a number of non-private baselines and one private baseline."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presentation is overall clear and easy to follow.\n- The SecPC method and its analysis are novel and may have broader applications.\n- SecFPP demonstrates impressive accuracy in experiments."}, "weaknesses": {"value": "- The paper compares SecFPP to a differentially private method DP-FPL as a privacy baseline, however, SecFPP does not provide the same privacy guarantees. The DP method provides quantifiable privacy guarantees for the training data over the entire training process. While SecFPP gives a statistical measure of the privacy of the training data in SecPC, this is not as strong a guarantee as DP. More importantly, SecFPP leaks information about the training data though the aggregated gradient of the global prompts in line 10 of Algorithm 1. The secure aggregation does protect the inputs to the aggregate, but the aggregated value is still a privacy leak. This is a major issue as the method fails at its main claim of privacy.\n- Even if SecFPP does not protect privacy, the experiment results indicate it may still be of value for personalized prompt learning. The results do not really give a good indication of the balance between personalization and generalization. It would be helpful to break up the results in the multi-domain setting to differentiate performance on own domain vs. other domain.\n- Minor: there appears to be something wrong with the citations that are not used inline in sentences. \n- Minor: For Table 1, described in Section 2.2, please include the dataset details."}, "questions": {"value": "- What is the intuition behind the benefit of the clustering. Is there any way to demonstrate this through the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yc6uIxNSGK", "forum": "q2VKSHWhAS", "replyto": "q2VKSHWhAS", "signatures": ["ICLR.cc/2026/Conference/Submission22464/Reviewer_PpxW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22464/Reviewer_PpxW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22464/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951539909, "cdate": 1761951539909, "tmdate": 1762942228789, "mdate": 1762942228789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}