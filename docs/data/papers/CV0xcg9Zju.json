{"id": "CV0xcg9Zju", "number": 1266, "cdate": 1756868442604, "mdate": 1759898218095, "content": {"title": "DSTA4D: Rethinking Adaptive Spatio-Temporal Decoupling for Dynamic Point Cloud Videos", "abstract": "Understanding 4D point cloud videos is crucial for intelligent agents to perceive the dynamic changes in their external environment. However, due to the inter-frame time inconsistency and spatial disorder inherent in long-sequence point clouds, designing a unified 4D global model faces significant challenges. Existing methods primarily rely on static, monolithic network architectures that apply a uniform computational pipeline to all input data. This approach neglects the differences in spatio-temporal complexity across videos, resulting in inefficient resource allocation and limiting model's performance. To address these issues, we present a novel content-aware 4D point cloud processing approach, termed DSTA4D, which leverages dynamic spatio-temporal decoupling via adaptive modules. We first propose decoupling temporal and spatial features within the embedding layer, which avoids the complexity of full-process long-term modeling. Second, we introduce a innovative lightweight module: Dynamic Spatio-Temporal Adapter(DST-Adapter). This module dynamically generates gating weights based on the global spatio-temporal features of the input sequence and adaptively fuses features from three parallel streams: identity path, spatial enhancement path, and temporal enhancement path. This content-aware mechanism allows the model to intelligently allocate its computational focus to the most critical feature dimensions. Our experiments on mainstream benchmarks MSR-Action3D (\\textbf{+5.23\\%} accuracy), NTU RGBD (\\textbf{+1\\%} accuracy) and Synthia4D (\\textbf{+1.36\\%} mIoU) show significant performance gains, offering a more efficient and intelligent adaptive modeling paradigm for point cloud video understanding.", "tldr": "We propose a content-aware 4D point cloud modeling framework that combines pre-embedding spatio-temporal decoupling with a dynamic adapter to intelligently allocate computation, achieving significant gains on MSR-Action3D and Synthia4D benchmarks.", "keywords": ["4D Point Cloud; Spatio-Temporal Decoupling; Content-Aware Processing; Dynamic Adapter; Video Understanding; Action Recognition; Temporal-Biased Convolution; Spatio-Temporal Attention; Adaptive Feature Fusion; Point Cloud Video"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0b0a443e2599d5c83c91f5f2c59c47535d8405a.pdf", "supplementary_material": "/attachment/0208158d247c10b0008a21c0d568f2243ff926ef.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel light-weight spatial temporal decoupling strategy with respective adaptation. Three parallel paths including original path, spatial-enhancement path, and temporal enhancement one are designed. Gating mechanism and FiLM-based methods are also introduced. Even though this method achieves state-of-the-art performance on 4D datasets, I doubt the novelty, overclaim advantages, and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The proposed module is lightweight, and is effective on 4D tasks.\n\n2.Authors leverage several detailed designs in spatial enhancement, temporal enhancement, and gating to provide better spatio-temporal modeling capabilities.\n\n3.Strong performance: The method achieves state-of-the-art performance on three commonly-used datasets like MSRA3D, NTU, and Synthia4D."}, "weaknesses": {"value": "1.Limited Technique Novelty: Even though authors propose a lightweight but effective method with higher performance, many designs resort to existing techniques like token-mixing, channel-mixing, gating, and scaling&shifting. These incremental contributions have limited influence on this field. Furthermore, this pipeline retains the original point 4D conv and transformer backbones, which only add the designed components in between. \n\n2.Overclaim: Authors claim ‘DST-Adapter o intelligently allocate computational resources, prioritizing the most critical spatio-temporal features’. However, in both experimental and method sections, there are no corresponding theory analysis or experimental proof. More explanations and experiments should be added to prove this point.\n\n3.Similarly, authors say ‘Fixed computational graphs and processing strategies in Mamba-based methods can become a performance bottleneck with large-scale, high-frame-rate inputs.’ in the introduction section. But there are no corresponding analysis in the experiment section either.\n\n4.Inconsistent Contents in Pipeline Figure. In the pipeline, what is the meaning of ‘symmetric’ part? There are no corresponding descriptions in the text. Also, it is quite confusing why the outputs of context encoder are $\\alpha$, $\\beta$, etc. In the text description, the output is ctx. \n\n5.Efficiency: I have concerns on the additional computational resources since the methods keep the original 4D conv and transformer backbones. More analysis on how much additional overhead this method will incur should be added. Also, how authors balance the trade-off between accuracy and efficiency?\n\n6.Results on HOI4D dataset: HOI4D is a newly-proposed dataset, which is more commonly used for metric evaluation in recent works. Could you provide the results on this dataset?\n\n7.Insufficient ablation studies: There are no ablation studies for content extractor part and gating, fusion module. I think these are also core novelties in this paper, which should be validated. Also, most hyper-parameter settings should be compared.\n\n8.In Figure 2, could authors compare the qualitative results with other methods? \n\n9.Inconsistent tenses: In the related work section, the first and third paragraph use the common tense while past tense is used in the second one. \n\n10.Grammars: The overall grammar is great. But there are some missed space before (. For example, ‘Dynamic Spatio-Temporal Adapter(DST-Adapter)’ in the abstract."}, "questions": {"value": "Please refer to the weakness part. Also, since authors claim this adaptive decoupling is a universal design, could you provide more plug-and-play results on other baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "989JPvUYS1", "forum": "CV0xcg9Zju", "replyto": "CV0xcg9Zju", "signatures": ["ICLR.cc/2026/Conference/Submission1266/Reviewer_cGvm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1266/Reviewer_cGvm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761145753266, "cdate": 1761145753266, "tmdate": 1762915722993, "mdate": 1762915722993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses dynamic 4D point cloud video understanding, noting that existing models use static, uniform pipelines that poorly handle varying spatio-temporal complexity. It proposes DSTA4D, a content-aware architecture that first decouples spatial and temporal features at the embedding layer, then applies a Dynamic Spatio-Temporal Adapter (DST-Adapter). The DST-Adapter uses a global context encoder to generate FiLM-style gating weights that fuse three parallel feature streams (identity, spatial-enhanced, temporal-enhanced) in a data-dependent way. This mechanism adapts the computational focus to the scene complexity. Experiments on MSR-Action3D, NTU RGB+D, and Synthia4D show notable gains: +5.23% action accuracy on MSR-Action3D, +1.0% on NTU RGB+D, and +1.36% mIoU on Synthia4D over previous state-of-the-art. These results establish new benchmarks, validating the approach’s effectiveness on classification and segmentation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strong empirical gains. The method achieves state-of-the-art performance on all evaluated tasks. For example, it sets a new record on MSR-Action3D (96.17% vs. ~94.8% prior), and improves segmentation mIoU on Synthia4D by ~0.8 points. These consistent improvements on diverse benchmarks indicate the approach’s effectiveness.\n\n- Rigorous evaluation. The experiments are comprehensive: multiple datasets (small to large, action recognition and segmentation), standard splits, and many baselines are compared. Ablations in Table 4 show each component’s importance (removing FiLM or decoupling hurts accuracy), giving confidence in the design choices. The paper also analyzes hyperparameters (e.g., temporal kernel size) and compares to baselines using more frames.\n\n- Theoretical backing. The inclusion of theoretical results (Appendix Theorem A.1) provides a solid foundation, explaining why decoupling and conditional gating can improve risk bounds. This adds depth and credibility beyond empirical claims."}, "weaknesses": {"value": "- No efficiency metrics: Along similar lines, the paper lacks any evaluation of resource usage (GPU time, memory, etc.). It would strengthen the work to know if the adapter adds significant overhead compared to a baseline Transformer. Without this, the practicality of “lightweight” gating is speculative.\n\n- Limited long-range modeling: The temporal adapter is a depthwise conv with kernel size 3, which captures only very short-term motion. Although the paper claims “long-term dynamic perception” via decoupling, it is actually the downstream Transformer layers that must capture longer dependencies. The text could better clarify this interaction. As is, one might question whether decoupling truly addresses long-term dynamics or mainly local enhancements.\n\n- Dataset scope: The evaluation, while diverse, is still limited to a small action dataset (MSR-Action3D), a large human-action set (NTU), and one synthetic driving dataset. No real-world outdoor or multi-object benchmarks (e.g. SemanticKITTI sequence modeling) are tested. It would be useful to see how DSTA4D generalizes to more complex or varied point-cloud video domains."}, "questions": {"value": "- Effect of gating on compute: Since the gating weights merely re-weight the three branches, have the authors considered sparsifying them (e.g. setting small weights to zero) to actually skip computation? Can you clarify what resource savings (if any) are achieved by the adapter? Is there a runtime or FLOP count comparison to a baseline Transformer?\n\n- Temporal modeling scope: The Temporal Adapter uses a small 1D conv (kernel size 3). Do you rely on the subsequent Transformer layers for longer-range temporal dependencies? Would it help to use a larger temporal window or learnable RNN/SSM in that branch? What was the reasoning for this design?\n\n- Context vector details: What is the dimensionality of the context vector $ctx$? Did you experiment with different sizes or pooling strategies? How sensitive is performance to this choice, or to the structure of the gating MLP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YWuIZQv3Zv", "forum": "CV0xcg9Zju", "replyto": "CV0xcg9Zju", "signatures": ["ICLR.cc/2026/Conference/Submission1266/Reviewer_GSr5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1266/Reviewer_GSr5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761560845664, "cdate": 1761560845664, "tmdate": 1762915722857, "mdate": 1762915722857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a dynamic spatio-temporal decoupling model (DSTA4D) for 4D point cloud video understanding. It separates spatial and temporal features at the embedding layer and employs a Dynamic Spatio-Temporal Adapter (DST-Adapter) to achieve content-adaptive fusion through context gating and FiLM modulation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper clearly identifies a real limitation of existing 4D models, i.e., the static and uniform computation graphs, and argues for the need of content-aware computation allocation. The idea sounds appealing.\n\n2.\tThe modular design (embedding decoupling + DST-Adapter + FiLM) is easy to understand and implement."}, "weaknesses": {"value": "1.\tThe model claims to adapt computation based on input content, using a global context vector (ctx) that generates gating weights through an MLP and fuses them via FiLM modulation. However, there’s no actual analysis or visualization showing how these gates behave across different samples or datasets. We don’t see whether the gating weights really change in a meaningful way, or how this adaptivity connects to the reported accuracy gains.\n\n2.\tAlthough the paper emphasizes “intelligent computation allocation” and “adaptive efficiency,” it does not report FLOPs, memory consumption, inference latency, or throughput. \n\n3.\tThe proposed “decoupled spatio-temporal embedding” is presented as a key contribution, but similar ideas have been widely used in previous 4D models. .Besides, there is no ablation comparing the decoupled and non-decoupled variants to justify its impact in this paper.\n\n4.\tThe ablation only removes FiLM and the Adapter. It does not analyze the relative importance or complementarity of the three branches (identity/spatial/temporal), nor test the sensitivity to context statistics (mean/max pooling) or gating parameters.\n\n5.\tSome figures, equations, and tables are not well standardized."}, "questions": {"value": "1. How do the gating distributions differ across categories, scenes, or datasets? Can this be visualized?\n\n2. What are the FLOPs, parameter counts, memory usage, inference latency, and throughput?\n\n3. What differentiates this decoupling scheme from similar 4D spatio-temporal embeddings in prior work?\n\n4. Have you conducted an ablation comparing the coupled vs. decoupled variants to quantify its contribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oo0lwB1fsi", "forum": "CV0xcg9Zju", "replyto": "CV0xcg9Zju", "signatures": ["ICLR.cc/2026/Conference/Submission1266/Reviewer_QQPt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1266/Reviewer_QQPt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652379807, "cdate": 1761652379807, "tmdate": 1762915722700, "mdate": 1762915722700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DSTA4D, a content-aware framework for 4D point cloud video understanding designed to address the inefficiencies of static, monolithic models. The core contribution is the Dynamic Spatio-Temporal Adapter (DST-Adapter), a lightweight module that dynamically computes a global context vector from the input sequence. This vector is then used to generate gating weights that adaptively fuse features from three parallel streams: an identity path, a spatial enhancement path, and a temporal enhancement path. Experiments across multiply benchmarks demonstrate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper presents a novel and well-motivated framework, DSTA4D, which tackles the inefficiency of static 4D point cloud models. This content-aware mechanism well addresses the one-size-fits-all limitation of prior work.\n2.\tExtensive experiments validate the effectiveness of the proposed method. \n3.\tThis paper is written and organized well."}, "weaknesses": {"value": "1.\tThe paper claims to solve inefficient resource allocation, but provides no efficiency metrics (FLOPs, latency, parameters). It will be more convincing to provide some efficiency metrics.\n2.\tThe ST Transformer backbone is not defined in the main paper. Without knowing its architecture (details are hidden in Appendix A.2), it's impossible to discern how much performance gain comes from the novel adapter versus a potentially strong, custom backbone. \n3.\tThe DST-Adapter is proposed as a general module but is only tested on its own Transformer backbone. It will be more interesting to plug it into other architectures."}, "questions": {"value": "Refer to the Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4rdnDmpitT", "forum": "CV0xcg9Zju", "replyto": "CV0xcg9Zju", "signatures": ["ICLR.cc/2026/Conference/Submission1266/Reviewer_L1Fq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1266/Reviewer_L1Fq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1266/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744252000, "cdate": 1761744252000, "tmdate": 1762915722516, "mdate": 1762915722516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}