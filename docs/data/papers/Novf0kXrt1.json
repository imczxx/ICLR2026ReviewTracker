{"id": "Novf0kXrt1", "number": 20193, "cdate": 1758303528142, "mdate": 1759896992977, "content": {"title": "TOAST : Transformer Optimization using Adaptive and Simple Transformations", "abstract": "Foundation models achieve  state-of-the-art (SOTA) performance across different tasks, but their size and computational demands raise concerns about accessibility and sustainability. Existing efficiency methods often require additional retraining or fine-tuning, limiting their practicality.   Recent findings suggest that deep neural networks exhibit internal representation similarities. While such similarities across different models have been exploited for enabling techniques such as model stitching and merging, intra-network redundancy remains underexplored as a source for efficiency gains. In this paper, we introduce Transformer Optimization using Adaptive and Simple Transformations (TOAST), a framework that exploits these redundancies to approximate entire transformer blocks with lightweight closed-form mappings, such as linear transformation or even the identity, without any additional training. Across SOTA pretrained vision models (e.g., ViT, DINOv2, DeiT) and datasets ranging from MNIST to ImageNet-1k, TOAST reduces parameters and computation while preserving, and in some cases improving, downstream performance. These results show that large portions of transformer depth can be replaced by trivial functions, opening a new perspective on efficient foundation models.", "tldr": "We propose TOAST, a framework that approximates transformer blocks using simple transformations, reducing parameter count and GFLOPs while increasing throughput, with minimal to no impact on downstream task performance.", "keywords": ["latent representations", "representation learning", "neural network similarities", "classification", "foundation models", "transformers"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2bed138c863e86365c5be156ea43c42f12820c78.pdf", "supplementary_material": "/attachment/f548cf1765f87adf4ba5620962c9a0853246de8f.zip"}, "replies": [{"content": {"summary": {"value": "This paper explores intra-network redundancy and introduces Transformer Optimization using Adaptive and Simple Transformations (TOAST), a framework that exploits these redundancies to approximate entire transformer blocks with lightweight, closed-form mappings, such as a linear transformation or even the identity function."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "(1) The paper's approach to improving model efficiency from the perspective of intra-network redundancy is a promising and valuable area of research."}, "weaknesses": {"value": "(1) Insufficient Comparison with Related Fields: The paper's connection to well-established fields like layer-wise pruning and Stitchable Neural Networks (SN-Nets) is not adequately discussed. Replacing a block with an identity or linear transformation is conceptually similar to these methods, yet the paper lacks a clear comparison, making it difficult to assess the relative advantages and novelty of TOAST.\n\n(2) Lack of Methodological Clarity: The mechanism for selecting the appropriate approximation (identity or linear) for a given block is unclear. The paper does not provide a formal algorithm or a sufficiently detailed explanation of the decision process. This ambiguity hinders the reproducibility and a deeper understanding of the core mechanism.\n\n(3) Limited Experimental Scope: The experimental evaluation is confined to the vision domain. While results on ViT are presented, there is no evidence to suggest that the proposed method is generalizable to other important domains, particularly Large Language Models (LLMs). This narrow scope limits the perceived impact and applicability of the work.\n\n(4) Unconvincing Empirical Performance: The empirical evidence supporting the method's effectiveness is not compelling. As shown in Tables 1 and 2, the method often leads to significant performance degradation with only modest parameter reduction. The claimed benefits appear to be isolated to very large-scale models (ViT-L) or DEiT-S, and are not consistently demonstrated across different settings, which questions the practical utility of the approach.\n\n(5) Inadequate Baseline Selection: The set of baselines used for comparison is insufficient. Given the conceptual overlap with model pruning, the paper should have included comparisons against relevant pruning methods, especially those from the vision literature. Without these comparisons, the claimed efficiency and performance benefits of TOAST are not well-contextualized."}, "questions": {"value": "(1) The paper suggests replacing a full block with an identity transformation. Could this be interpreted as pruning the entire block? If so, how does this method differ from the well-established technical line of layer-wise pruning? Similarly, if a linear transformation is used, does this effectively replace the block with a single linear layer, thereby \"stitching\" blocks together? If so, what are the advantages of this approach compared to more efficient methods like Stitchable Neural Networks (SN-Nets)? Given that SN-Nets are known for their resource efficiency and flexibility, a more detailed discussion and comparison would be highly beneficial.\n\n(2) Regarding the \"Approximating Transformer Blocks\" section, could you clarify the selection process between a linear transformation and an identity mapping? Is this choice optimized via a loss function, or is it determined by directly comparing which mapping results in a smaller loss? A formal algorithm or pseudocode describing this process would greatly improve clarity.\n\n(3) The experiments are primarily conducted on vision backbones and tasks. Have the authors investigated whether the TOAST framework is equally effective for Large Language Model (LLM) backbones? such as Qwen and Llama.\n\n(4) Based on Tables 1 and 2, the empirical results do not appear consistently strong. The method seems to maintain a slight performance advantage only on the very large ViT-L model and Deit-S while reducing parameters by about 10%. In most other cases, performance degrades substantially even with a parameter reduction of less than 10%. Does this suggest that the practical effectiveness of the method may be limited?\n\n(5) In the baseline comparisons, would it be more appropriate to include baselines from the model pruning literature, particularly methods well-established in the vision domain? This seems especially relevant given the conceptual similarity between replacing a block with an identity mapping and block-level pruning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QWRaxMfeCg", "forum": "Novf0kXrt1", "replyto": "Novf0kXrt1", "signatures": ["ICLR.cc/2026/Conference/Submission20193/Reviewer_dwJk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20193/Reviewer_dwJk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760538170268, "cdate": 1760538170268, "tmdate": 1762933698510, "mdate": 1762933698510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Based on the observation that intermediate features of pretrained vision transformers are highly similar, the paper proposes Transformer Optimization using Adaptive and Simple Transformations (TOAST), which replace redundant blocks with lightweight transformations, e.g., identity operations or linear mappings. It uses a small number of training samples, e.g., 500, to find the optimal linear transformation that replaces the original transformer blocks. Experiments show that TOAST reduces parameters and computations while preserving the downstream performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and easy to follow. The proposed method is naturally based on existing observations. The authors provide strong empirical analysis including CKA visualizations.\n2. The experiments include many architectures including DINO, ViT, and DeiTs."}, "weaknesses": {"value": "1. The first concern of the paper is that TOAST is extremely sensitive to the hyper-parameters, i.e., which blocks to be replaced. For example, in Table 2 DeiT-S with Linear Approximation, a good choice (10->11) leads to only -0.09% degradation while a bad choice (3->4, 9->11) leads to -7.39% degradation. This suggests that block selection is non-trivial. The paper may be improved with a general theoretical grounded/heuristic guidelines for quickly choosing the optimal layers to replace. In general, based on the results, the last layer may be a good choice, while in Table 3, choosing the last layer seems not to be a good choice as the results are not shown.\n2. The second concern of the paper is that since the proposed method is targeting reducing inference efficiency with a few samples, which overlaps strongly with classical compression techniques. However, critical relevant baselines like neural network pruning or quantization are not compared."}, "questions": {"value": "1. In Table 2, DINO-B baseline rows (original) has different accuracy and throughput for identity and linear transformations, is this a typo?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iqGbjj5TTe", "forum": "Novf0kXrt1", "replyto": "Novf0kXrt1", "signatures": ["ICLR.cc/2026/Conference/Submission20193/Reviewer_sH5D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20193/Reviewer_sH5D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547903138, "cdate": 1761547903138, "tmdate": 1762933698050, "mdate": 1762933698050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TOAST, a training-free framework that replaces spans of transformer blocks with lightweight closed-form mappings-either the identity or a single linear map-estimated from a small subset of activations (≈500 images) between a “source” block s and a later “end” block e, motivated by block-wise representational redundancy measured via CKA. TOAST is applied to pre-trained vision transformers (ViT/DeiT/DINOv2) for classification, showing modest accuracy drops (sometimes gains) alongside parameter/GFLOPs reductions and throughput increases; the approach often works best when approximating late blocks, and scales to ViT-L; a small sample ablation suggests performance plateaus by ~500 samples; and a zero-shot OpenCLIP experiment reveals that late-layer approximation can be catastrophic in multimodal settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n\n2. Simple, training-free idea with closed-form estimator. Clear formulation with identity/linear translators; easy to implement and deploy."}, "weaknesses": {"value": "1. **Limited experimental scope.** The method is evaluated only on classification tasks, which may rely more heavily on the classification head rather than the transformer backbone itself. Broader evaluation on more complex tasks (e.g., segmentation or detection) is needed to verify whether a single translator is sufficient. \n\n2. **Evaluation protocol may favor TOAST.** Although the appendix includes runs with original heads, these appear only for certain models and datasets, and their reporting format is inconsistent with the main experiments. This inconsistency makes it unclear how performance varies across all configurations. Reporting both protocols-using the original head and retrained head-side by side would better isolate the backbone’s contribution.\n\n3. **Inconsistent layer selection.** The paper does not clearly explain how layers are chosen for reporting. While the focus seems to be on early and late layers, the selections appear somewhat arbitrary, making comparisons difficult. Including more results for middle layers and providing a clear rationale for the chosen spans would improve transparency and interpretability.\n\n4. **Limited interpretability of when and why it works.** CKA-based similarity is used to justify redundancy between layers, but CKA is known to be sensitive to representation scaling and other factors. The observed correlations between similarity heatmaps and downstream performance remain mostly observational. A stronger causal or ablation-based analysis would provide greater confidence in the method’s underlying mechanisms."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HdDuxBDf0s", "forum": "Novf0kXrt1", "replyto": "Novf0kXrt1", "signatures": ["ICLR.cc/2026/Conference/Submission20193/Reviewer_RS29"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20193/Reviewer_RS29"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635116020, "cdate": 1761635116020, "tmdate": 1762933697731, "mdate": 1762933697731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Transformer Optimization using Adaptive and Simple Transformations (TOAST) which looks at simple ways to approximate entire transformer blocks in vision transformers. Parts of the model can then be replaced, reducing parameter counts and making the models smaller and more efficient. The method was examined using MNIST, F-MNIST, CIFAR-10, CIFAR-100, and ImageNet 1k. In general, the method has only slight degredation in task performance for DEiT-S and ViT-L with efficiency and speed games, but suffers more for DiNO-B. Overall, this is a potentially interesting paper to the community."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A novel method that can make vision transformer models that are already trained smaller and more efficient."}, "weaknesses": {"value": "I'm not as familiar with vision tasks as NLP tasks so it is a bit harder for me to evaluate this. However, my understanding is that MNIST and CIFAR-10 are easier tasks. ImageNet-1K is hard, but is quite old. A potential weakness might be the difficulty of the test sets. However, I'll state again to the other reviewers and area chairs that I'm not as familiar as with other modalities."}, "questions": {"value": "You argue 500 examples are enough and show some plots to justify this. However, many tasks in AI have a long-tailed distribution. Did you look at parts of the longer tails to see if this is the case? For instance, you might only lose a small percentage of absolute performance, but have something that is less robust and only works on the dominant class.\n\nAny idea how this would perform on other tasks? For instance, a text or audio task which also have pre-trained transformer architectures where your method could be applied."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "y2qYyIPUQY", "forum": "Novf0kXrt1", "replyto": "Novf0kXrt1", "signatures": ["ICLR.cc/2026/Conference/Submission20193/Reviewer_NEKG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20193/Reviewer_NEKG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20193/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762319805058, "cdate": 1762319805058, "tmdate": 1762933697389, "mdate": 1762933697389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}