{"id": "1xbIF3Kc0l", "number": 23390, "cdate": 1758343072742, "mdate": 1759896817454, "content": {"title": "Distributional Reinforcement Learning for Large Language Models", "abstract": "Actor-critic reinforcement learning for large language models (LLMs) typically relies on a scalar value function, discarding crucial information about potential returns. We propose a distributional actor-critic framework that learns the full distribution of returns to guide exploration more effectively. We find that in deterministic reasoning tasks, the spread of this learned distribution directly measures the model's confidence in its own value estimates. Our method harnesses this signal through an optimistic exploration bonus derived from the distribution's upper-tail variance, guiding the policy toward promising yet uncertain reasoning paths. This uncertainty-guided exploration promotes the discovery of diverse correct solutions, leading to substantial gains in pass@k across challenging benchmarks. This result demonstrates a significant enhancement of the model's upper-bound reasoning capacity over strong baselines, which is complemented by consistent, albeit more modest, improvements in single-answer correctness.", "tldr": "", "keywords": ["Large Language Models", "Distributional Reinforcement Learning", "Actor–Critic Methods"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da1c444bb8d12e8650f4bcc0f406a8cd5dfee16b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a distributional actor-critic framework for enhancing the reasoning capabilities of LLMs. The authors build upon the following observation: in deterministic environments the spread of a learned value distribution ceases to be a conflation of environmental and model-based uncertainty. Instead, it becomes a pure measure of the model's own parametric uncertainty, i.e., its confidence in its value estimates. The authors operationalize this insight by introducing an exploration bonus which they call the Decaying Left Truncated Variance (DLTV), which is derived from the upper tail of the value distribution to guide the policy towards promising but uncertain reasoning paths. This mechanism is integrated into an actor-critic framework. The authors show via their experiments that their approach improves pass@k metrics across a variety of mathematical reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The insight that the deterministic nature of LLM environments isolates parametric uncertainty is a clean and valuable contribution to the conceptual understanding of RL in this domain.\n2. The authors use multiple strong baselines, comprehensive and challenging benchmarks, and a principled ablation study"}, "weaknesses": {"value": "1. The core exploration mechanism is functionally almost identical to that of Mavrin et al. (2019), a critical point that is not adequately acknowledged, thereby overstating the methodological novelty of the work.\n2. The paper's claim of \"minimal\" overhead is directly contradicted by its own data, which shows a 65% increase in training time over the state-of-the-art value-free baseline, DAPO. This is a significant practical limitation that is not properly discussed.\n3. The AIME 2025 case study in the appendix demonstrates incorrect reasoning from the proposed method, which severely undermines the paper's central claim of enhancing the model's reasoning ability."}, "questions": {"value": "1. The DLTV exploration bonus appears functionally analogous to the combination of the Left Truncated Variance (LTV) bonus and the decaying schedule introduced by Mavrin et al. (2019). Could you elaborate on the methodological distinctions beyond the simplification of the theoretical grounding (i.e., isolating parametric uncertainty in a deterministic environment)? What specific algorithmic innovations differentiate DLTV?\n\n2. Could you provide a more detailed justification for the significant performance-compute trade-off? How do you position the practical viability of reintroducing a critic in a field that has largely moved toward more efficient value-free methods like GRPO and DAPO for scalability?\n\n3. The experiments demonstrate substantial improvements in pass@k metrics, especially for large k, but more modest gains in avg@k. This suggests the method is highly effective at improving the model's ability to find at least one correct solution given many attempts. How do you reconcile this finding with the claim of enhancing the \"model's intrinsic reasoning capacity\"?  Could the results be interpreted as enhancing the efficiency of exploratory search over the base model's latent capabilities, a distinction which was also highlighted in recent work by Yue et al. (2025a)?\n\n4. The analysis in Figure 4 claims that the exploration bonus shifts from targeting \"computational fragments\" to \"strategic\" meta-reasoning instructions over time. What evidence can you provide to rule out the alternative that the model is engaging in a form of reward hacking—learning that generating these strategic-sounding phrases is statistically correlated with positive final outcomes, irrespective of the logical coherence of the subsequent steps?\n\n5. Given the clear exploration benefits of the uncertainty signal but the high computational cost of the full actor-critic framework, have you considered or experimented with a hybrid approach? For instance, could the uncertainty signal from a lightweight or intermittently trained distributional critic be integrated as an intrinsic reward within a more efficient value-free framework like GRPO, potentially capturing the benefits of both?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nWvELD4kxq", "forum": "1xbIF3Kc0l", "replyto": "1xbIF3Kc0l", "signatures": ["ICLR.cc/2026/Conference/Submission23390/Reviewer_UmDv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23390/Reviewer_UmDv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760732427045, "cdate": 1760732427045, "tmdate": 1762942639334, "mdate": 1762942639334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a distributional actor-critic framework to enhance the reasoning capabilities of large language models (LLMs). The core insight is that, in deterministic reasoning tasks, the intrinsic stochasticity of the environment vanishes. As a result, the spread of the learned value distribution primarily reflects the model's parametric uncertainty (i.e., its estimation error). The authors leverage reinforcement learning (RL) techniques to improve the current LLM approaches, including exploration methods and advantage estimation techniques.\n\nHowever, after reading the full paper, I found that it contains fundamental errors and typo issues, giving the impression that it was hastily written. The paper would benefit from refinement."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "This reframing of distributional RL for LLMs is a good idea."}, "weaknesses": {"value": "- The writing is poor, and several expressions are inaccurate. For instance：\n\n  Line 171： The claim \" uniformly sampled\" is misleading in the context of QR-DQN, as this is specific to IQN. \n\n  Line 189： The statement \"for typical values like N=51, the computational overhead compared to a standard scalar critic is minimal (Dabney et al., 2018b)\" is not supported by the cited work. The selection of the number of quantiles is highly environment-dependent.\n\n  Line 221: The notation $\\delta_t^{(i)}$ in equation (3) is not consistent with the pairwise type defined in equation (1). \n\n  Additionally, there are many other typos, such as $T$ in line 261.........\n\n- In equation (6), the expectation is unclear. The notation $\\tau$ is not used in the inner terms. Moreover, the target $\\hat{y}$ and quantile $q_{\\theta}$ should depend on distinct parameter, but this dependency is not clearly stated.\n\n- The paper introduces the concept of non-crossing quantiles but borrows network architecture ideas from prior work ([1], [2]) without properly citing these sources. Similarly, Section 3.2 introduces the DLTV exploration method but fails to cite the relevant work ([3]). **The authors are encouraged to ensure that proper citations are included to appropriately acknowledge the contributions of others, rather than overlooking them.**\n\n\n\n[1] Non-crossing quantile regression for deep reinforcement learning. NIPS 2020.\n\n[2] Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning. IJCAI, 2021\n\n[3] Distributional reinforcement learning for efficient exploration. ICML, 2019"}, "questions": {"value": "1. Could you elaborate on the claim that \"the environment dynamics are deterministic in LLM reasoning tasks\"?\n2. The core insight about deterministic environments applies to many LLM reasoning tasks. Have you conducted any preliminary experiments on benchmarks like GPQA (for scientific QA) or ARC-Challenge (for commonsense reasoning)? If not, do you anticipate the same benefits, or are there domain-specific challenges?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical concerns are apparent in the paper."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ivkHkw9T8I", "forum": "1xbIF3Kc0l", "replyto": "1xbIF3Kc0l", "signatures": ["ICLR.cc/2026/Conference/Submission23390/Reviewer_6oj7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23390/Reviewer_6oj7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761227494337, "cdate": 1761227494337, "tmdate": 1762942639071, "mdate": 1762942639071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a distributional actor-critic architecture with a Distributional Generalized\nAdvantage Estimation (D-GAE) that encourages the agent to explore uncertain regions where high reward is expected by prioritizing upper tail variance. The authors claim that with the transitions being deterministic, the only uncertainty comes from the model itself. This insight is used to model the state value (not state action) as a random variable. The upper tail of this random variable is then used as an exploration bonus called Decaying Left-Truncated Variance (DLTV), an operationalization of the *optimism under uncertainty* principle. Specifically, the quantiles are estimated via the quantile Huber loss, and are then used to calculate the optimistic advantage estimate. The effectiveness of the method is demonstrated on math benchmarks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Interesting and novel architecture using the powerful and oftern overlooked quantile regression methods\n\n- Impressive design approach for the method\n\n- Clear formalisms and useful operationalization of the exploration bonus\n\n- Very interesting qualitative examples (albeit in a very tiny difficult to read font) \n\n- Strong results in **Qwen2.5-Math-7B** and good results in **Qwen2.5-7B** general"}, "weaknesses": {"value": "## Minor \n\n*The following do not affect my score but would be really important to follow the highest standards of clarity and style*\n\n1. The fonts in every figure (other than Figure 2) are very difficult to read, espoecially the qualitative results in Figure 4. Please fix.\n\n2. Do you need for Figure 2 to be 3D and using Comic Sans? I would strongly advise the authors to revisit the design choices. \n\n## Major \n\n3. I have a few issues with the way the contributions are outlined. It is fine to list things that have been done on the paper as a checklist. What is misleading is listing normal steps of the scientific process as separate contributions. There is no utility to inflating impact. The **first point** is an insight as the authors themselves describe it to be. The **third point** is necessary analysis to support the contribution in the **second point**, not a separate contribution itself. If the **third point** was not completed, the **second point** would not be a contribution to begin with.\n\n 3a. In **line 125**, does this \"insight\" hold even when the temperature is not 0? How does this relate to the sampling temperature and the usual way reasoning model are used which is with a temperature greater than 0.\n\n4. For the claims the paper makes, an exclusive focus on math questions is not sufficient. At least one coding benchmark experiment on LiveCodeBench [1] or SWEBench is necessary. You can use Qwen-2.5-Coder for the time being.\n\n5. Missing relevant baselines. See Question 5. There are a lot of simpler to implement, effective algorithms. Please motivate why this work is a worthwhile addition in light of that.\n\n[1] Jain, Naman, et al. \"Livecodebench: Holistic and contamination free evaluation of large language models for code.\" arXiv preprint arXiv:2403.07974 (2024)."}, "questions": {"value": "1. Is Figure 1 needed? Seems superfluous given that the caption has all the necessary information.\n\n2. What is the difference between your ontology of **intrinsic** and **parametric** and the common **aleatoric** and **epistemic** uncertainty framework?\n\n3. **Paragraph in line 187**: why is the typical value N=51?\n\n4. Regarding point 3 in the weaknesses, how does the claimed determinism relate to temperature, batch variance and GPU non-determinism? [1]\n\n5. This work is very similar to Max Entropy RL in its purpose and motivation. How does it compare to the method introduced by [2][3]. Should be added as baselines.\n\nHappy to increase the score if the questions are answered and my concerns addressed with additional results.\n\n[1] He, Horace and Thinking Machines Lab, \"Defeating Nondeterminism in LLM Inference\", \nThinking Machines Lab: Connectionism, Sep 2025.\n\n[2] Yao, Jian, et al. \"Diversity-Aware Policy Optimization for Large Language Model Reasoning.\" arXiv preprint arXiv:2505.23433 (2025).\n\n[3] He, Andre, Daniel Fried, and Sean Welleck. \"Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening.\" arXiv preprint arXiv:2506.02355 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8v0ZnNHg9G", "forum": "1xbIF3Kc0l", "replyto": "1xbIF3Kc0l", "signatures": ["ICLR.cc/2026/Conference/Submission23390/Reviewer_Z9jB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23390/Reviewer_Z9jB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950225114, "cdate": 1761950225114, "tmdate": 1762942638893, "mdate": 1762942638893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to improve RL for LLM post-training by developing a distributional version of PPO that consists of two key modifications: 1. maintaining a distributional state-value function, and computing advantages using a distributional version of GAE, and 2. adding an optimistic exploration bonus enabled by the distributional value estimate. The proposed method, DistRL, is benchmarked on standard RL for math reasoning setups and demonstrates improvements in pass@1 and particularly pass@k."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a generally important problem: are there better algorithms than GRPO or PPO for RL fine-tuning of LLMs? The authors' approach consists of directly bringing ideas from previous distributional RL literature (especially DLTV from Mavrin et. al 2019.), and is therefore naturally sound. The main technical difference is considering a state-value function instead of a Q-function, which is reasonable considering the LLM token MDP is noise-free. Experimentally, the results are fairly impressive, and many (but not all, see below) of the details look great (models, benchmarks, implementation)."}, "weaknesses": {"value": "I found the paper to be weak in the following respects:\n\n1. **Lack of algorithmic baselines**: Only GRPO and PPO are benchmarked, and not any other exploration methods for LLM RL (of which many have been proposed in recent months).\n2. **Lack of ablations**: There are no ablations presented for what are the relative contributions of the method: is it distributional advantage estimation, or is it the exploration bonus that comprise most of the benefit?\n3. **Lack of insight into the method**: The paper simply presents the method and shows experimental performance of the method. There are no other quantitive analyses into why distributional RL should be superior. For example, I would like to see an analogous figure to Fig. 4 in the original QR-DQN paper.\n4. **Novelty and motivation**: The method consists of relatively straightforward modifications from prior distributional RL literature. This is not a significant issue by itself, but there is not sufficient motivation provided for this idea (beyond \"distributional RL is good\").\n\nIn summary: the algorithm and results seem plausible, but this version of the paper lacks scientific contribution. I'm willing to raise my score if these concerns are addressed."}, "questions": {"value": "1. Is it correct that the max response length used in RL training is 3k, and inference is 4k? This seems far shorter than what is typically used in RL training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f4r9UqyanO", "forum": "1xbIF3Kc0l", "replyto": "1xbIF3Kc0l", "signatures": ["ICLR.cc/2026/Conference/Submission23390/Reviewer_M6c7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23390/Reviewer_M6c7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23390/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762204267983, "cdate": 1762204267983, "tmdate": 1762942638645, "mdate": 1762942638645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}