{"id": "Ej1DYLYzFU", "number": 13058, "cdate": 1758213190495, "mdate": 1759897468416, "content": {"title": "NoLoCo: No-all-reduce Low Communication Training Method for Large Language Models", "abstract": "Training large language models is generally done via optimization methods on clusters containing tens of thousands of accelerators, communicating over a high-bandwidth interconnect. Scaling up these clusters is expensive and can become impractical, imposing limits on the size of models that can be trained. \nSeveral recent studies have proposed training methods that are less communication intensive, avoiding the need for a highly connected compute cluster.  These state-of-the-art low communication training methods still employ a synchronization step for model parameters, which, when performed over all model replicas, can become costly on a low-bandwidth network.\n\nIn this work, we propose a novel optimization method, NoLoCo, that does not explicitly synchronize all model parameters during training and, as a result, does not require any collective communication. NoLoCo implicitly synchronizes model weights via a novel variant of the Nesterov momentum optimizer by partially averaging model weights with a randomly selected other one. We provide both a theoretical convergence analysis for our proposed optimizer as well as empirical results from language model training.\n\nWe benchmark NoLoCo on a wide range of accelerator counts and model sizes, between 125M to 6.8B parameters. Our method requires significantly less communication overhead than fully sharded data parallel training or even widely used low communication training method, DiLoCo. The synchronization step itself is estimated to be one magnitude faster than the all-reduce used in DiLoCo for few hundred accelerators training over the internet. We also do not have any global blocking communication that reduces accelerator idling time. Compared to DiLoCo, we also observe up to $4\\%$ faster convergence rate with wide range of model sizes and accelerator counts.", "tldr": "A novel training method for low bandwidth distributed training.", "keywords": ["Decentralized Training", "Low Bandwidth Training", "Large Language Models"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c9347298df2e8e628d8634bd65fb1c31fcc74547.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper try to eliminating global collective communication to reduce the e2e training communication overhead. This can be useful in extreme cases where cross-node network bandwidth is limited.\n\nThe authors proposed NoLoCo method, which does dynamic routing on PP dimension (similar as dynamic routing in MoE expert tokens), but do need model synchronization on DP dimension. However, loss divergence issue has been observed."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed idea is interesting (i.e. doing dynamic routing in PP stages). Also picking PP is practical since it has minimum communication volumes compared to other parallelism strategies. \n\n2. A detailed convergence analysis in appendix is a plus."}, "weaknesses": {"value": "1. In general, eliminating global communication will lead to model divergence. In practice, training a model has much worse model serving accuracy, this training itself will be a complete waste of compute resources. Therefore, only try to optimizing/eliminating communcation overhead is not a good strategy. \n\n2. For experiments, the paper does not mention any details on which LLama vesion is used? e.g. LLama2? llama3? which can be a huge difference on perplexity or loss.\n\n3. the results of Figure 2, it shows there is noticeable and significant loss/PPL gaps between FSDP and proposed NoLoCo strategy, which makes the proposed method not practical in real model training scenarios.  \n\n4. Although the paper has some \"proof\" of model convergence with dynamic PP routing, as shown in later result (e.g. Table2 and Figure 2), the model divergence issue is quite severe. Therefore, it makes the whole NoLoCo scheme less practical, mainly because the loss divergence issue."}, "questions": {"value": "1. In Figure 2, why larger models and more PP stages will make PPL differnce less compared wth FSDP?\n\n2. In practice, nowadays it is very rare to see communication bandwidth is less than 1GB/s. So the Table 3 comparison number on 100Mb/s ( 12.5MB/s) and 1Gb/s (125MB/s) are less practical and less convincing. \n\n3. Theoretically, doing dynamic routing on PP stages will definitely cause loss divergence issue. The high level idea is, difference PP stages of a single PP group are training on different input data at the same iteration, this will definitely introduce errors and wrong momentum."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "enVGgUHhRT", "forum": "Ej1DYLYzFU", "replyto": "Ej1DYLYzFU", "signatures": ["ICLR.cc/2026/Conference/Submission13058/Reviewer_KLKc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13058/Reviewer_KLKc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536541042, "cdate": 1761536541042, "tmdate": 1762923788614, "mdate": 1762923788614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NoLoCo, a decentralized training method for large language models that eliminates global all-reduce synchronization. Building on DiLoCo, which reduces communication by performing infrequent global synchronizations between local training phases, NoLoCo further minimizes overhead by synchronizing only random pairs of workers using a modified Nesterov momentum optimizer that maintains model stability. Combined with dynamic pipeline routing, this approach enables efficient large-scale training on low-bandwidth or high-latency networks. Experiments on different sizes of LLaMA models show that NoLoCo achieves up to 4% faster convergence than DiLoCo while greatly reducing communication cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tThe key innovation of this work is to remove global all-reduce synchronization by introducing local, pairwise averaging.\n•\tThe authors propose a new variant of Nesterov momentum with an additional local averaging term to prevent divergence, supported by theoretical convergence analysis.\n•\tThe authors provide mathematical proofs showing convergence and variance behavior, linking stability to the inner learning rate."}, "weaknesses": {"value": "•\tOnly empirically compare NoLoCo against the original DiLoCo baseline, despite acknowledging multiple improved variants.\n•\tConvergence proof assumes independence between replicas and IID data, which may not hold in real heterogeneous clusters.\n•\tLargely reused learning rates and batch sizes from prior FSDP studies and fixed outer-loop settings without systematic or sensitivity analysis.\n•\tThe authors may want to separately test pairwise averaging, modified Nesterov, and random routing to show each component’s contribution."}, "questions": {"value": "•\tThe paper references multiple improved versions but compares only with the original DiLoCo; including these baselines would better show whether NoLoCo truly advances beyond recent low-communication optimizers. If cannot make the comparison, the authors may want to provide a detailed explanation of why these variants were excluded and how NoLoCo differs from them in design or communication efficiency.\n•\tThe paper integrates three innovations: pairwise averaging, modified Nesterov momentum, and random pipeline routing. However, do not disentangle their effects. The authors may want to conduct ablation studies that isolate each component if possible to clarify which element primarily drives convergence speed, stability, and communication efficiency.\n•\tNoLoCo introduces several new hyperparameters (like outer momentum, outer learning rate, local averaging strength, subgroup size, and outer step frequency), but the paper provides little analysis of how these parameters influence stability, convergence, or communication cost. Could the authors discuss which hyperparameters are most critical, how sensitive the method is to their choice, and how much tuning effort practitioners should expect compared to standard distributed training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sj30V0ys7r", "forum": "Ej1DYLYzFU", "replyto": "Ej1DYLYzFU", "signatures": ["ICLR.cc/2026/Conference/Submission13058/Reviewer_fHc3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13058/Reviewer_fHc3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970403362, "cdate": 1761970403362, "tmdate": 1762923788106, "mdate": 1762923788106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NoLoCo, a novel optimization method designed to eliminate the need for collective communication during distributed training of large language models (LLMs). Instead of explicitly synchronizing model parameters, NoLoCo achieves implicit synchronization through a variant of the Nesterov momentum optimizer. Specifically, model weights are partially averaged with those of a randomly selected peer, allowing for communication-efficient and scalable model training. The authors provide both theoretical convergence guarantees for the proposed algorithm and empirical experiments demonstrating its efficiency in large-scale language model training.  \n\nWhile the idea is innovative and the theoretical aspect is solid, the experimental studies are incomplete, especially regarding the ablation and sensitivity analyses needed to confirm the importance of the paper’s key design choices. I will raise my score to positive if the authors address my concerns."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides both theoretical and empirical validation for the proposed optimization method, which strengthens its overall contribution and credibility.  \n2. NoLoCo effectively reduces communication overhead compared to state-of-the-art methods, particularly DiLoCo, while achieving faster convergence in language model training.  \n3. The idea of replacing global synchronization with randomized local interactions is novel and practically meaningful, offering a potential improvement for large-scale distributed training systems."}, "weaknesses": {"value": "1. Ablation studies are insufficient. The paper lacks experiments isolating the impact of the “outer optimizer step with modified Nesterov momentum,” which is one of the paper’s core contributions. Specifically, it is unclear how performance would change if the original (unmodified) Nesterov momentum were used instead of the modified version described in Equation (2). Without further ablation to isolate the effect of the modified Nesterov momentum, it remains unclear whether this component is indeed critical to NoLoCo’s improved performance.  \n2. The paper omits ablations on critical hyperparameters such as:  \n   - The group size (n) in section 3.2, which likely influences both communication cost and model performance (e.g., perplexity).  \n   - The number of inner optimizer steps, which differs between NoLoCo (50) and DiLoCo (100) in the main comparisons, but the rationale for this choice is not adequately explained.  \n     Including these analyses would provide deeper insight into NoLoCo’s behavior and fairness in comparison.  \n3. The implementation details in Section 3 are not sufficiently clear, making it challenging to fully grasp how NoLoCo is practically realized."}, "questions": {"value": "1. What would happen if the modified Nesterov momentum (Equation 2) were replaced with the original Nesterov momentum? How does this change impact convergence and model performance?  \n2. How does group size (n) affect communication efficiency and model perplexity? Can the authors provide an ablation study?  \n3. How does varying the number of inner optimizer steps influence both communication cost and performance?  \n4. During the outer optimizer step, are the local groups fixed or re-sampled randomly at each round?  \n5. Since each step only performs partial synchronization within groups, is there a final global synchronization among all subgroups at the end of training? If not, wouldn’t this result in $\\frac{N}{n}$ slightly different model replicas?  \n6. In Figure 4, the variable *n* is annotated as “world size,” but in Section 3.2 it denotes “group size.” Are these the same variable or different? The notation may be confusing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uYEV1T800Z", "forum": "Ej1DYLYzFU", "replyto": "Ej1DYLYzFU", "signatures": ["ICLR.cc/2026/Conference/Submission13058/Reviewer_2LMt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13058/Reviewer_2LMt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13058/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762410042287, "cdate": 1762410042287, "tmdate": 1762923787554, "mdate": 1762923787554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}