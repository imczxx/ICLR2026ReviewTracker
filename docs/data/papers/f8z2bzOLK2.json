{"id": "f8z2bzOLK2", "number": 5246, "cdate": 1757876090820, "mdate": 1763669362014, "content": {"title": "$\\mu$LO: Compute-Efficient Meta-Generalization of Learned Optimizers", "abstract": "Learned optimizers (LOs) have the potential to significantly reduce the wall-clock training time of neural networks. However, they can struggle to optimize unseen tasks (*meta-generalize*), especially when training networks wider than those seen during meta-training. To address this, we derive the Maximal Update Parametrization ($\\mu$P) for two state-of-the-art learned optimizer architectures and propose a simple meta-training recipe for $\\mu$-parameterized LOs ($\\mu$LOs). Our empirical evaluation demonstrates that LOs meta-trained with our recipe substantially improve meta-generalization to wider unseen tasks when compared to LOs trained under standard parametrization (SP) using the same compute budget. We also empirically observe that $\\mu$LOs exhibit unexpectedly improved meta-generalization to deeper networks ($5\\times$ meta-training) and surprising generalization to much longer training horizons ($25\\times$ meta-training) when compared to SP LOs.", "tldr": "We propose a new way to meta train learned optimizers, allowing them to generalize from small meta-training tasks to large unseen tasks for the first time.", "keywords": ["Learned Optimizer", "Meta Generalization", "MuP", "Maximal Update Parameterization"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1b182612e06a83e6af3ca2bf43494de55a04b25a.pdf", "supplementary_material": "/attachment/bde06f15274fba04781d66665ce761c536a26eab.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces \\mu-parameterized Learned Optimizers to address the meta-generalization limitations of existing learned optimizers when optimizing neural networks wider than those encountered during meta-training. \n\nAuthors derive the Maximal Update Parametrization  for some architectures and propose a straightforward meta-training recipe. They find that proposed \\mu-P parameterization significantly improves meta-generalization to wider, deeper, and longer unseen tasks compared to Standard Parameterization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper has following strengths in my opinion:\n- tackles the fundamental challenge of meta-generalization in learned optimizers, which is a major hurdle for their practical adoption.\n- derivation of µP for LOs provides a solid theoretical basis for the proposed approach\n- results show substantial improvements in meta-generalization across various axes (width, depth, training horizons) compared to  Standard Parametrization\n- authors emphasize that the benefits of µLOs come at zero extra computational cost during meta-training compared to SP LOs, which is a crucial practical advantage\n- paper provides a clear description of the meta-training recipe, evaluation setup, and baselines, making the work reproducible\n- good suite of evaluation tasks and architectures"}, "weaknesses": {"value": "My main concerns are following:\n- While the improved generalization to deeper networks and longer training horizons is a strong empirical result, the explanations for these benefits are currently speculative e.g. mentions of pre-activation stability. Wondering if this could be made more precise."}, "questions": {"value": "I also have some qns for the authors:\n- Given that µLOs exhibit improved generalization to deeper networks and longer training horizons, and these improvements are currently hypothesized to be due to activation stability, do the authors plan to conduct further theoretical analysis to formalize these observations?\n- The µLOs were meta-trained only on MLP tasks. How do the authors anticipate the performance and meta-training cost of µLOs would change if they were meta-trained on a more diverse set of architectures, including ViTs and Transformers?\n- While the paper acknowledges the absence of an oracle SP AdamW baseline, could the authors discuss any ongoing efforts or future plans to conduct such a computationally intensive comparison to provide an even stronger benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8HPIPgLY91", "forum": "f8z2bzOLK2", "replyto": "f8z2bzOLK2", "signatures": ["ICLR.cc/2026/Conference/Submission5246/Reviewer_oqd9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5246/Reviewer_oqd9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5246/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861196536, "cdate": 1761861196536, "tmdate": 1762917969588, "mdate": 1762917969588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Paper update 1"}, "comment": {"value": "Dear reviewers,\n\nWe would like to thank you all for taking the time to review our paper.\n\nIn response to your thoughtful feedback, we have made the following changes to the manuscript (highlighted in blue):\n- We have added experiments on ResNet tasks in section F.1.2 of the appendix.\n- We have included additional references in the related work section and have made related changes to the text.\n- We have added a paragraph to the conclusion for future work.\n\n-The Authors"}}, "id": "OfFo9ibxN6", "forum": "f8z2bzOLK2", "replyto": "f8z2bzOLK2", "signatures": ["ICLR.cc/2026/Conference/Submission5246/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5246/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5246/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763669727497, "cdate": 1763669727497, "tmdate": 1763669727497, "mdate": 1763669727497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of learned optimizers (LOs) struggling to optimize unseen tasks, particularly when dealing with networks larger than those encountered during meta-training. The authors introduce the Maximal Update Parametrization (P) for two state-of-the-art LO architectures and propose a new meta-training recipe for parameterized LOs. Empirical results show that LOs trained with this recipe significantly improve meta-generalization to larger unseen tasks, outperforming standard parametrization (SP) LOs within the same compute budget. Additionally, the paper observes enhanced meta-generalization for deeper networks and surprisingly effective generalization for longer training horizons compared to SP LOs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces the Maximal Update Parametrization to address the meta-generalization problem in learned optimizers. The idea is novel and interesting.\n\n2. The experimental results are thorough and effectively demonstrate the method’s validity."}, "weaknesses": {"value": "1. I recommend adding experiments with convolutional neural networks (CNNs). Although this limitation is mentioned, I believe CNNs are currently mainstream in neural network research, and testing the method on them would further strengthen the validity of the findings.\n\nI am not very familiar with this field, so I will rely on the feedback from other reviewers for the final score."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XbA7XfhmxO", "forum": "f8z2bzOLK2", "replyto": "f8z2bzOLK2", "signatures": ["ICLR.cc/2026/Conference/Submission5246/Reviewer_P3eP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5246/Reviewer_P3eP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5246/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906880419, "cdate": 1761906880419, "tmdate": 1762917969269, "mdate": 1762917969269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper derives muP for the small_fc_lopt and VeLO learned optimizer architectures. The authors then test the resulting metageneralization, meta-training on ImageNet classification with MLPs and applying the optimizers to other instances of that as well as using transformers for that and language modelling. They empirically find that this achieves hyperparameter transfer not only for width, but also to a significant degree for depth and training duration, outperforming other learned optimizers and also competing with task-tuned standard/hand-designed ones like AdamW and μAdam. There is also a short subsection on pre-activation stability, and similar investigations in the appendix."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "While I think applying muP to this domain was inevitable, the authors make a good case that the failure to meta-generalize is the major blocker for LOs, and that they make substantial improvements there.\n\nI thought the experiments were reasonable, namely the baselines, and the paper was clear throughout, including the maths (though I haven't gone line by line in the proofs)."}, "weaknesses": {"value": "I think the findings of [\"Scaling Exponents Across Parameterizations and Optimizers\"](https://arxiv.org/abs/2407.05872) should've made an appearance somewhere, since they indicate that standard parametrization can also achieve hyperparameter transfer. They also show that (in larger problem instances than here) that epsilon should be tuned in Adam.\n\nAlso, since depth scaling is mentioned (albeit as a bonus), I think the related works and perhaps some experiments would ideally address more heuristic methods for scaling with depth (and width, or rollout length, if the LO community has investigated this at all). In NeurIPS 2025 (i.e., contemporary, ignoring arXiv) work, [ComputeP](https://arxiv.org/abs/2505.01618) is quite relevant\n\nMinor fix:\n* §B.1 \"When using a schedule, we always use linear warmup and cosine annealing with\" just cuts off"}, "questions": {"value": "How much compute went into the hyperparameter sweeps for the baseline optimizers? The values swept looked reasonable, but I think it'd be useful context to compare with the meta-training figures, to see how much the LOs need to be used to make up for that (mainly thinking in seemingly underparametrized settings where that may be the right frame, like language modelling)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mYJDTAyUAt", "forum": "f8z2bzOLK2", "replyto": "f8z2bzOLK2", "signatures": ["ICLR.cc/2026/Conference/Submission5246/Reviewer_PtQP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5246/Reviewer_PtQP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5246/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981840747, "cdate": 1761981840747, "tmdate": 1762917968979, "mdate": 1762917968979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors of this submission investigate the generalization ability of learned optimizers in zero-shot-like scenarios, where an optimizer trained on one architecture can be directly applied to unseen architectures. To achieve this, models are initialized following the $\\mu$P (Maximal Update Parameterization) principle. The authors leverage their hyperparameter reuse property, which allows optimal parameters discovered on one architecture to be transferred to architectures with the same depth but different widths via a well-defined mapping. Under this framework, learned optimizers are treated as a form of hyperparameter, enabling transfer to new architectures. The proposed approach is evaluated across various neural network training tasks and demonstrates promising results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is interesting and well-grounded. The idea of viewing optimizers as hyperparameters that can generalize across $\\mu$P-initialized networks of varying widths is both novel and conceptually appealing.\n\n2. The learned optimizers show encouraging results in zero-shot-like transfer scenarios."}, "weaknesses": {"value": "1. Figure 5 indicates the meta overfitting with the trained optimiser performing well on the meta-train tasks while failing to generalise well on the unseen tasks. \n\n2. From all the experiments, only the learning curve of training stages is illustrated, with the question of whether the learned optimizer leads to advanced generalization ability not answered. \n\n3. Limited novelty: the Mup parameterization proposed in this submission is very close to a direct application of the original $\\mu$P paper. In addition, the propositions in submission are similar to the theoretical results from the $\\mu$P papers."}, "questions": {"value": "1. Figure 5 suggests meta-overfitting: the learned optimizer performs well on meta-training tasks but fails to generalize effectively to unseen tasks.\n\n2. Across the experiments, only training curves are presented. It remains unclear whether the learned optimizer contributes to improved generalization performance beyond faster or more stable training.\n\n3. The level of novelty appears limited. The proposed $\\mu$P parameterization closely follows the original $\\mu$P paper, and several propositions in the submission echo theoretical results already established in prior work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "1. Did the authors adapt VeLO and LO to the experimental tasks? More specifically, were VeLO and LO meta-trained and evaluated under the same protocol used for the proposed learned optimizer?\n\n2. The ViT model was trained using Adam. Is there a reason this optimizer was not included in the comparison? Similarly, why was SGD-M omitted for MLP training?\n\n3. The batch sizes used in the experiments seem unusually large. Many experiments use batch sizes of 1024 or 4096. Could the authors clarify the rationale behind using such large batch sizes?"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Gr5oQOdHte", "forum": "f8z2bzOLK2", "replyto": "f8z2bzOLK2", "signatures": ["ICLR.cc/2026/Conference/Submission5246/Reviewer_tB8g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5246/Reviewer_tB8g"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5246/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762780141760, "cdate": 1762780141760, "tmdate": 1762917968682, "mdate": 1762917968682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}