{"id": "r5jxDjYeO4", "number": 11443, "cdate": 1758199228738, "mdate": 1759897575279, "content": {"title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs", "abstract": "The increasing scale and complexity of large language models (LLMs) pose significant inference\nlatency challenges, primarily due to their autoregressive decoding paradigm characterized by the\nsequential nature of next-token prediction. By re-examining the outputs of autoregressive models, we\nobserved that some segments exhibit parallelizable structures, which we term intrinsic parallelism.\nDecoding each parallelizable branch simultaneously ($\\textit{i.e}$. parallel decoding) can significantly improve\nthe overall inference speed of LLMs. In this paper, we propose an $\\textbf{A}$daptive $\\textbf{S}$erial-$\\textbf{P}$arallel $\\textbf{D}$ecoding ($\\textbf{ASPD}$), which addresses two core challenges: automated construction of parallelizable\ndata and efficient parallel decoding mechanism. More specifically, we introduce a non-invasive\npipeline that automatically extracts and validates parallelizable structures from the responses of autoregressive\nmodels. To empower efficient adaptive serial-parallel decoding, we implement a $\\textbf{Hybrid Decoding Engine}$ which enables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive evaluations across\nGeneral Tasks, Retrieval-Augmented Generation and Mathematical Reasoning demonstrate that $\\textbf{ASPD}$\nachieves unprecedented performance in both effectiveness and efficiency. Notably, on Vicuna Bench,\nour method achieves up to 3.10x speedup (1.82x on average) while maintaining response quality\nwithin 1\\% difference compared to autoregressive models, realizing significant acceleration without\ncompromising generation quality.\nThe source code is available for review at an anonymous repository: https://anonymous.4open.science/r/ASPD.", "tldr": "ASPD extracts intrinsic parallelism and utilizes a Hybrid Decoding Engine to achieve up to 3.19× speedup on Vicuna Bench with <1% quality loss.", "keywords": ["Parallel Decoding;Intrinsic Parallelism Mining;Large Language Models (LLMs) ;Hybrid Decoding Engine"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a985bde03ccb21d8668de90b9e6a56bfc7f66242.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces ASPD, a method that uncovers and exploits intrinsic parallelism in AR LLM outputs by identifying semantically independent segments that can be decoded in parallel. \nA dataset processing pipeline is also proposed. \nEvaluation results show that ASPD can speed up inference with under a 1% drop in output quality across tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a new direction for test time scaling, where the LLM can generate texts using its intrinsic ability. \n2. The speedup comes with no additional compute cost. \n3. The parallel data generation pipeline seems interesting. \n4. It effectively overcomes the autoregressive bottleneck at the segment level."}, "weaknesses": {"value": "1. The methodology for the dataset generation (Section 3.1) is not very clear to me. How do you rewrite the data into parallel and serial data? How is the verification done in detail? I feel Section 3.1 could be elaborated further with more clarity. \n2. There are not much details on the inference engine, like how it handles batching and so on. \n3. The evaluation does not compare with SOTA verifier-guided beam search methods.\n\n\nMinor:\n1. Figure 1 and 2 font sizes are too small to read."}, "questions": {"value": "In addition to the weaknesses mentioned above, there are a few more questions:\n\n1. What is the major difference between ASPD and Multiverse?\n2. How does the inference engine maximize the inference efficiency?\n3. What is the batch size used for inference in experiments?\n4. What is the training cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NYDsHfyi6A", "forum": "r5jxDjYeO4", "replyto": "r5jxDjYeO4", "signatures": ["ICLR.cc/2026/Conference/Submission11443/Reviewer_9wUP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11443/Reviewer_9wUP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760463465348, "cdate": 1760463465348, "tmdate": 1762922555872, "mdate": 1762922555872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the inference‑latency bottleneck in current LLMs due to strictly sequential autoregressive decoding. The authors observe that many generated responses contain intrinsic parallelism — segments that can be produced independently without breaking coherence. Motivated by this, they propose Adaptive Serial‑Parallel Decoding (ASPD), which addresses two core\nchallenges: automated construction of parallelizable data and an efficient parallel decoding mechanism. Experiments show that this method provides significant improvements in both effectiveness and efficiency compared to existing approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Parallel decoding is a promising technique for inference acceleration. While previous works mainly focus on token-level parallel decoding (i.e., decode multiple tokens simultaneously), this paper leverages the intrinsic parallelism in LLMs. This is a good motivation.\n\nSpeed gains across diverse domains and models, with minimal trade‑off in output quality."}, "weaknesses": {"value": "see questions"}, "questions": {"value": "The paper says: Tokens in the main branch maintain absolute positions in the flattened sequence, while parallel branches synchronize their position encodings at each timestamp. (line 269). Does this mean that tokens in parallel branches have two position ids: one for the main branch and the other for the parallel branches? If so, parallel tokens will recompute KVs when they are flattened and merged into the main branch, which introduces extra cost. If not, the position ids in the main branch are problematic.\n\nWhat is the average and variance of parallel branch lengths? If branches have very different lengths, the decoding will be blocked by the longest branch."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PDGqTx22r6", "forum": "r5jxDjYeO4", "replyto": "r5jxDjYeO4", "signatures": ["ICLR.cc/2026/Conference/Submission11443/Reviewer_MYWg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11443/Reviewer_MYWg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812052213, "cdate": 1761812052213, "tmdate": 1762922555374, "mdate": 1762922555374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ASPD, a methodology for enabling parallel decoding. The method enables reusable KV cache and maintains ground truth position IDs during parallel decoding, with the capability to resume sequential generation mode after parallel generation mode. The work evaluates ASPD, showing it achieves increase in tokens/sec while maintaining the quality of sequential generation."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- ASPD enables parallel decoding while addressing the weaknesses of previous work (no sequential decoding after parallelizing in APAR; approximated position IDs disrupting position continuity in Pasta)\n- The paper is generally well written and easy to understand, which the figures giving a very clear overview of the methodology and of differences with previous works.\n- The experiments show that ASPD achieves the greatest tokens/sec and highest quality compared to APAR, SOT, and sequential across three benchmarks, demonstrating that ASPD does enable more tokens generated at time."}, "weaknesses": {"value": "- The paper does not present the wall clock latency speedup of the different methods, but only tokens/sec and other efficiency metrics which do not account for actual system overheads to the methodology. As a speed-oriented parallelization method, wall clock speedup is an important evaluation metric. \n- It seems that the main difference between ASPD and Pasta is that in ASPD the position ID is maintained as if the tokens generated in parallel were actually sequential (i.e. ground truth position IDs) while Pasta uses model predictions to compute the position IDs, which makes Pasta an important baseline. However, the evaluation doesn't compare against PASTA as a parallel decoding baseline in Figure 4, but only ablate the data pipeline methodology used in Pasta.\n\nMinor comment:\n- The colored grid lines on Figure 4 makes it difficult to read."}, "questions": {"value": "Please address the above concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "148fjX2NEI", "forum": "r5jxDjYeO4", "replyto": "r5jxDjYeO4", "signatures": ["ICLR.cc/2026/Conference/Submission11443/Reviewer_aASG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11443/Reviewer_aASG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864420965, "cdate": 1761864420965, "tmdate": 1762922554796, "mdate": 1762922554796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ASPD (Adaptive Serial-Parallel Decoding) to accelerate LLM inference by exploiting \"intrinsic parallelism\" in responses. Instead of pure autoregressive decoding, it identifies parallelizable structures via an automated, non-invasive data pipeline. A hybrid decoding engine then adaptively switches between serial and parallel generation, crucially maintaining and reusing the KV cache across modes. This approach achieved significant speedup up to 3.10x (1.82x avg) on Vicuna Bench while preserving generation quality with less than 1% degradation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an interesting aspect of the LLM parallelism. And the found intrinsic parallelism such as lists are interesting.\n\n- The experiments are comprehensive and thorough, covering different reasoning tasks such as STEM, roleplay, reasoning, and extraction tasks."}, "weaknesses": {"value": "- Speedups for certain tasks such as mathematics reasoning are limited. For example, the speedup on MATH500 is 1.17x, much lower than the 1.82x achieved on Vicuna Bench.\n\n- The method is dependent on task structure. Mathematical reasoning, for instance, involves \"strong inter-step dependencies\" and \"step-by-step deductions,\" which naturally reduces the opportunities for parallelization.\n\n- The training overhead seems to be missing. What are the training overhead and how long does it take? Consider a quantitative analysis.\n\nMiscellaneous\n- Line 277 end: should be ``<branch>T_i:\""}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "F5WbmndmpC", "forum": "r5jxDjYeO4", "replyto": "r5jxDjYeO4", "signatures": ["ICLR.cc/2026/Conference/Submission11443/Reviewer_Utbc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11443/Reviewer_Utbc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974051096, "cdate": 1761974051096, "tmdate": 1762922554379, "mdate": 1762922554379, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}