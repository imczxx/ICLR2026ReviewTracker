{"id": "0Iw52EDu82", "number": 11921, "cdate": 1758204654491, "mdate": 1759897545966, "content": {"title": "Scaling Laws for Fully Sparsely-Activated Large Language Models", "abstract": "Scaling laws play a crucial role in understanding and optimizing Large Language Models (LLMs). While previous work on scaling laws has primarily focused on either fully dense models or models with sparse Mixture of Experts (MoE), our work investigates fully sparsely-activated models, where every activation in linear transformations is sparse. We derive scaling laws for these models through extensive experiments with varying model sizes, training token counts, and activation sparsity ratios. Our findings demonstrate that fully sparsely-activated LLMs exhibit favorable scaling properties: as the total model size increases, LLMs can maintain higher activation sparsity while the performance gap between sparsely-activated and dense models narrows. Notably, our scaling laws indicate that a sparsely-activated full-precision model with a 45.58% sparsity ratio achieves optimal performance while maintaining the same number of active parameters. Furthermore, our scaling laws remain applicable to 1-bit pre-training of LLMs, suggesting promising directions for improving the efficiency of future models.", "tldr": "In this work, we investigate the architecture and scaling laws for fully sparsely-activated models, where every activation in linear transformations is sparse.", "keywords": ["Activation sparsity", "scaling law", "large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68d8801b255d9afe13fd2fa81741bcc86a829bc4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the scaling law of fully sparsely-activated language models. They first conduct experiments to compare different activation functions, sparsification functions, and gradient estimation methods. Then, they use scaling law (the relationship between cross-entropy loss and training tokens and model size) to arrive at the optimal sparsity ratio in terms of inference costs. Finally, they also conduct experiments with 1.58-bit models, and find that the optimal sparsity ratio is greater."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper investigates an intriguing property of language models, namely, sparse activations. Moreover, characteristics of the scaling laws of sparsely-activated models is also interesting to the community.\n- The authors conducted a great number of experiments, and at sufficient scale, to justify their conclusions."}, "weaknesses": {"value": "- The authors should rigorously define what \"fully sparsely activated\" models refer to. Currently, it is unclear how it differs from ordinary sparsely activated models where a large number of intermediate activations in the FFN layers contribute weakly to the model's performance.\n- Similarly, this paper lacks a clear definition of sparsity. Is it the number of neurons whose output is exactly zero? Is the sparsity ratio averaged across all layers?\n- The paper also never described the architecture of the models Figure 1, which makes it impossible to interpret the results. What is the $K$ value for the top-k models? Where are the top-$K$ functions located? Similarly, where are the ReLU functions applied?\n- I am guessing the authors applied sparsification functions the linear projections in the attention layers as well. This is an uncommon architectural change that needs to be justified. Moreover, conclusions arrived at using such uncommon architectures might not generalize to more common architectures (where the linear projections of the attention layers have no sparsification functions). \n- In Section 3.2, the authors mention that top-$K$ sparsification achieves a higher sparsity rate over ReLUfication while maintaining the same performance. However, ReLUfication is a post-training method, and it seems that the result presented in relation with this section (Figure 2) involves pre-training the model from scratch. Is this true? In this case, why is the ReLU model referred to as ReLUfication?\n- Also in Section 3.2, it is concluded that top-$K$ sparsification leads to greater sparsity while maintaining performance. However, it seems like the performance of top-$K$ is much better (the PPL is significantly lower). Moreover, the authors should have performed additional ablations, such as using different values of $K$ and using additional improvements to ReLU-based models such as the ones introduced in ReMoE [1].\n- It is well known that it is hard to utilize sparsity to gain actual inference speedup. The paper should have discussed the difference between theoretical FLOPs reduction versus actual inference speeds. The authors presented some relevant results in Appendix A, but I would like to see more results, especially regarding prefilling speed, and the inference speed on other GPUs/hardware.\n- The authors conclude that there exists an optimal sparsity ratio, when fixing the number of activated parameters. Does it mean that when the sparsity exceeds this optimal ratio, the loss of sparsely-activated models increases when we increase the total number of parameters? This seems like a very counter-intuitive conclusion.\n- Many citations lack brackets (they should use `\\citep` instead of `\\citet` or `\\cite`)\n- The final sparsity ratio of the models, as shown in Table 2, is very low for practitioners to actually make use of it to gain inference speedup.\n\n\n[1] ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing"}, "questions": {"value": "- Tables 3 and 5: The scores for ARC-Easy and ARC-Challenge seems wrong, since the score of on ARC-Easy should be higher than the scores on ARC-Challenge.\n- Figure 2: How does the result lead to the conclusion that \"the gradient vanishes without STE\"?\n- The abbreviation STE needs to be explained at its first appearance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u7hfkPevi1", "forum": "0Iw52EDu82", "replyto": "0Iw52EDu82", "signatures": ["ICLR.cc/2026/Conference/Submission11921/Reviewer_3kkH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11921/Reviewer_3kkH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761555930116, "cdate": 1761555930116, "tmdate": 1762922926893, "mdate": 1762922926893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces scaling laws for fully sparsely-activated Large Language Models (LLMs), where activation sparsity is applied to every linear transformation. Through extensive experiments, the authors derive a novel scaling law that incorporates the sparsity ratio S as a variable, demonstrating that model performance scales favorably as model size increases, narrowing the gap with dense counterparts. A key contribution is the \"inference-optimal\" scaling law, which predicts an optimal sparsity ratio (around 45.58% for full-precision models) that maximizes performance for a fixed inference compute budget. The findings are further shown to be compatible with 1-bit quantization, suggesting a promising path toward more efficient future models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is structured in a clear and convincing manner. The authors first systematically compare different design choices in Section 3 (activation function, sparsification method, gradient approximation), which lays a solid foundation for the subsequent scaling law experiments. This methodical approach enhances the credibility of the results.\n\n2. The concept of an \"inference-optimal scaling law\" is the most prominent contribution. It is not just a theoretical discovery but also offers highly practical guidance: to achieve the best performance under a given inference budget, one should train a larger model with a specific sparsity ratio. Furthermore, the finding that the performance gap diminishes with scale provides an optimistic outlook for the development of future, larger-scale sparse models.\n\n3. The figures in the paper (especially Figures 1, 2, and 4) are highly effective at communicating the core messages. Figure 4, in particular, intuitively illustrates the existence of an inference-optimal point, which greatly aids the reader's understanding."}, "weaknesses": {"value": "1. Although the experiments cover a respectable range from 300M to 7B parameters, the true power of scaling laws lies in their ability to predict performance across orders of magnitude. Extrapolating the trends from a 7B model to much larger models (e.g., 70B or 175B) involves a degree of uncertainty. The conclusions would be more convincing if validated with at least one larger model (e.g., 13B), though I understand the prohibitive cost of such experiments.\n\n2. The title and abstract use the term \"fully sparsely-activated,\" but the experimental details note that the input and output embedding layers are exceptions. This is a minor point, but more precise phrasing, such as \"sparsity in all internal linear layers,\" might be preferable.\n\n3. The paper overlooks a comparison with recent studies on scaling laws for sparse and MoE models. It would be beneficial to contrast the proposed formula and its fitting performance with the findings in [A, B, C].\n\n[A] Ludziejewski, Jan, et al. \"Joint MoE Scaling Laws: Mixture of Experts Can Be Memory Efficient.\" arXiv preprint arXiv:2502.05172 (2025).\n\n[B] Li, Houyi, et al. \"Farseer: A Refined Scaling Law in Large Language Models.\" arXiv preprint arXiv:2506.10972 (2025).\n\n[C] Abnar, Samira, et al. \"Parameters vs flops: Scaling laws for optimal sparsity for mixture-of-experts language models.\" arXiv preprint arXiv:2501.12370 (2025)."}, "questions": {"value": "1. Your core conclusions rely on unstructured top-K sparsity. Could you elaborate on the practical path to realizing these efficiency gains on mainstream hardware like NVIDIA GPUs? How would the results from your structured sparsity (block top-K) experiments in Appendix B affect the derived scaling law and the optimal sparsity S*? Would structured sparsity fundamentally change the formulation?\n\n2. The finding that the performance gap diminishes with scale is very compelling. How confident are you in extrapolating this trend to models significantly larger than 7B (e.g., 70B+)? Are there theoretical reasons to believe the gap will continue to shrink, or might new phenomena emerge at larger scales?\n\n3. In Equation (3), you chose the specific form B + C exp(S / (1-S)) for A(S). What is the intuition behind this functional form? Did you experiment with other forms (e.g., polynomials, simpler exponentials), and why was this one ultimately selected?\n\n4. Training models at high sparsity ratios can sometimes be unstable. In your experiments, particularly with smaller models at higher sparsity levels (e.g., 60%), did you observe any training instabilities? Did the use of STE completely mitigate these potential issues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iw1BUiIY7w", "forum": "0Iw52EDu82", "replyto": "0Iw52EDu82", "signatures": ["ICLR.cc/2026/Conference/Submission11921/Reviewer_R2u1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11921/Reviewer_R2u1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846603642, "cdate": 1761846603642, "tmdate": 1762922926364, "mdate": 1762922926364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates scaling laws for fully sparsely-activated LLMs. It shows that loss scales as a power-law in parameters (N) and data (D), and exponentially with sparsity (S). They recast the law in terms of activated parameters to derive an inference-optimal sparsity (≈45.58% FP, ≈61.25% at 1.58-bit)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper extends scaling laws to full activation sparsity and frames an inference-optimal trade-off.\n2. The experiments are thorough with a broad settings, even quantized models are considered.\n3. It offers practical guidance for compute-constrained inference; consistent across FP and 1.58-bit training."}, "weaknesses": {"value": "1. Over-simplified sparsity aggregation:\nThe paper aggregates activation sparsity across all linear modules into a single global variable S, which, while simplifying the scaling-law formulation, overlooks the heterogeneous roles of different components. The effect of sparsity on FFN and attention is likely distinct. A more insightful formulation would consider how, under a fixed efficiency or compute budget, the scaling law could guide the allocation of density across modules rather than treating sparsity as uniform.\n2. Limited model scale and uncertain generality.\nDespite covering a broad set of configurations, the largest model trained is only 7B parameters. Besides, given that modern models often adopt mixture-of-experts (MoE) architectures at pre-training time, it remains unclear whether the proposed scaling relationships generalize to those settings. The long-term applicability and essentiality of this law remain uncertain."}, "questions": {"value": "Sparse attention (KV selection) has been extensively studied and proven effective for inference optimization. It would be interesting to see whether a similar scaling law could be derived for structured or sparse-attention mechanisms."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BjmVufuVVO", "forum": "0Iw52EDu82", "replyto": "0Iw52EDu82", "signatures": ["ICLR.cc/2026/Conference/Submission11921/Reviewer_hnRG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11921/Reviewer_hnRG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946047068, "cdate": 1761946047068, "tmdate": 1762922926044, "mdate": 1762922926044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to investigate the scaling laws for fully sparse-activated MoEs, unlike previous works that study scaling laws for dense models or models with sparse MoEs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This is an important topic with practically useful results, especially considering that most research teams lack the computing power to perform such studies.\n- The authors carried out detailed experiments to substantiate their findings.\n- I appreciate how the authors emphasized the key takeaways."}, "weaknesses": {"value": "- I'm not entirely persuaded by the scaling law parameterization. The description in Section 4 seems too heuristic-based. The authors should provide more quantitative metrics to demonstrate how well the parameterizations fit. The reviewer acknowledges a limited understanding of scaling law parameterization, so this should be taken with a grain of salt.\n\n- Due to the above, I'm also not convinced of the `45.58%` number, which appears to be just a result of the chosen parameterization. Is there any experimental evidence supporting this?\n\n- The authors should introduce the experimental setup earlier in Section 3.\n\n- I am unclear about the relevance of Section 5 to the paper; the setting feels a bit out of context to me.\n\n- While this topic is exciting, I find that the focus and research questions being asked are somewhat incremental. Still, this alone shouldn't justify rejection."}, "questions": {"value": "- Am I correct in understanding that the scaling law is fitted using 100 data points?\n\n- Line 407: \"a large sparsely-activated model outperforms a small dense model.\" I wonder if the authors could elaborate on the practical benefits of this. For example, the system/infra needed to serve a small dense model is (I believe) much easier than that for a large sparse model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "E6fTknsWm7", "forum": "0Iw52EDu82", "replyto": "0Iw52EDu82", "signatures": ["ICLR.cc/2026/Conference/Submission11921/Reviewer_37iv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11921/Reviewer_37iv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016694524, "cdate": 1762016694524, "tmdate": 1762922924580, "mdate": 1762922924580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}