{"id": "mVcQXHRK8x", "number": 10984, "cdate": 1758186227515, "mdate": 1763709902719, "content": {"title": "Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space", "abstract": "Test-time alignment of large language models (LLMs) attracts attention because fine-tuning LLMs requires high computational costs. In this paper, we propose a new test-time alignment method called adaptive importance sampling on pre-logits (AISP) on the basis of the sampling-based model predictive control with the stochastic control input. AISP applies the Gaussian perturbation into pre-logits, which are outputs of the penultimate layer, so as to maximize expected rewards with respect to the mean of the perturbation. We demonstrate that the optimal mean is obtained by importance sampling with sampled rewards. AISP outperforms best-of-n sampling in terms of rewards over the number of used samples and achieves higher rewards than other reward-based test-time alignment methods.", "tldr": "We propose a new training-free test-time alignment based on sampling the based model predictive control.", "keywords": ["LLM", "Alignment", "Control thoery", "Importance sampling"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48d7460d694e010480a9e187f1e8eea075837686.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method for aligning large language models at test time without fine-tuning. It works by adding Gaussian perturbations to the model‚Äôs pre-logit outputs and using importance sampling to adaptively update the perturbation mean for maximizing expected rewards. The method achieves higher reward efficiency and better alignment than existing test-time approaches (for example, BestOfN)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The method seems novel, which uses the optimal formulation for the test-time alignment problem."}, "weaknesses": {"value": "1. Lack of important literature review and baseline comparison on more recent test-time alignment papers, such as GenARM [1], PAD [2], which performs better than ARGS that is discussed in this paper. Moreover, GemARM also tries to maximize the value function, so it can be worthwhile to compare with it conceptually or mathematically.\n2. The motivation of using the optimal control formulation is not clear. KL-constrained reinforcement learning is a commonly used framework, and it is not clear what the benefits are using the optimal control formulation discussed in this paper.\n3. The method assumes that the pre-logits follow a Gaussian distribution. This does not seem to hold in practice.\n\n[1] GenARM: Reward guided generation with autoregressive reward model for test-time alignment. In International Conference on Learning Representations, 2025.\n[2] PAD: Personalized alignment at decoding-time. In International Conference on Learning Representations, 2025."}, "questions": {"value": "What is the main benefits of using the optimal control formulation instead of the commonly used RL formulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tzYzye5Tcj", "forum": "mVcQXHRK8x", "replyto": "mVcQXHRK8x", "signatures": ["ICLR.cc/2026/Conference/Submission10984/Reviewer_GUD6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10984/Reviewer_GUD6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716117402, "cdate": 1761716117402, "tmdate": 1762922176927, "mdate": 1762922176927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a training-free method called Adaptive Importance Sampling on Pre-logits (AISP), a test-time alignment technique that leverages stochastic model predictive control. By injecting Gaussian noise into the pre-logits and optimizing the perturbation mean to maximize the expected reward, AISP eliminates the need for data collection or additional computation during training. Experimental results show that AISP outperforms Best-of-N and other reward-based test-time alignment methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- AISP eliminates the need for training and data collection in test-time alignment.\n- The integration of adaptive importance sampling with model predictive path integral (MPPI) control is novel and well-motivated.\n- The analysis of modeling pre-logits $z$ as Gaussian distributions and its connection to Best-of-N (BoN) is insightful."}, "weaknesses": {"value": "- AISP introduces numerous hyperparameters, including the standard deviation $\\sigma$, the softmax temperature $\\lambda$, MPPI coefficient $\\alpha$, number of iterations $k$, and window size $\\tau$. This complexity limits the practicality of AISP. Moreover, the paper does not sufficiently analyze the sensitivity of these hyperparameters across different tasks and models, or their interactions with standard generation parameters such as temperature, top-$p$, and top-$k$ sampling.\n- Although AISP shows promise, its improvements over existing methods (as reported in Table 1) appear relatively small improvement, particularly in terms of diversity and coherence.\n## References\n[1] Scaling Laws for Reward Model Overoptimization. ICML 2023.\n\n[2] BOND: Aligning LLMs with Best-of-N Distillation. ICLR 2025."}, "questions": {"value": "- A deeper discussion of the hyperparameters $\\alpha$ and $\\tau$ is needed. What specific roles do they play? Why not set $\\tau$ equal to the full sequence length $T$? Is there a configuration of hyperparameters that remains robust across different models and settings?\n- Since BoN demonstrates strong robustness under large sampling budgets in mitigating reward over-optimization [1, 2], it is important to examine whether AISP maintains better robustness in the Reward-KL regularization trade-off as the sampling budget \n$ùëò$ increases. Does AISP still outperform BoN under large budgets (e.g., $k=\\{128,256\\}$)? Between scaling the number of responses per iteration and the number of iterations, which factor is more vulnerable to reward over-optimization?\n- Although the authors claim that AISP and BoN have comparable sequential and parallel computational costs, it's still necessary to include empirical measurements of inference time under identical sampling budgets. As AISP involves additional Gaussian sampling and iterative importance weight updates, quantifying this overhead is essential.\n- While not really necessary, it would be informative to compare AISP with standard RLHF training methods such as PPO and DPO."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "teTSF85uaL", "forum": "mVcQXHRK8x", "replyto": "mVcQXHRK8x", "signatures": ["ICLR.cc/2026/Conference/Submission10984/Reviewer_Wd2o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10984/Reviewer_Wd2o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829992383, "cdate": 1761829992383, "tmdate": 1762922176177, "mdate": 1762922176177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General responses"}, "comment": {"value": "We are deeply grateful for your careful reading and constructive comments. We are happy that Reviewers Wd2o and GUD6 confirm the novelty of our proposed method, and Reviewers w4ta and Wd2o acknowledged the motivation for the training-free alignment, Reviewer uatg recognized the comprehensiveness of the experiments.\nIf our response contains any gaps or raises new concerns, could you inform us before the deadline?\n\nBefore the response to each comment, this form gives general response to the shared concerns.\n# Additional datasets\nSine Reviewers utag, w4ta, and Wd2o raise the concern about limited datasets and significance of empirical results, we compare AISP with BoN on Alpaca-Eval, GSM8K, HumanEval, and TruthfulQA. \n**AISP outperforms BoN on the all tasks.** For the last two tasks, we use the lm-evaluation-harness codes. \nWe limit token length of responses to 128. \nThe hyper-parameters are the same with the evaluation on SHP with Llama3_8B and UltraRM/Eurus.\n\n\n**Table A Alpaca-Eval 2.0**\n|Method| length controlled winrate |win_rate|standard_error|\n|-|-|-|-|\n|AISP|5.64| 2.86|0.59|\n|BoN|3.95|2.24|0.52|\n\n**Table B GSM8K with 8 shot**\n|Method| Acc|\n|-|-|\n|AISP| 67.5|\n|BoN|66.0|\n\n**Table C HumanEval**\n|Method| pass@1|\n|-|-|\n|AISP| 41.4|\n|BoN|34.1|\n\n**TableD TruthfulQA**\n|Method|BLEU acc|ROUGE1 acc|\n|-|-|-|\n|AISP|0.426| 0.479|\n|BoN|0.424|0.458|\n\n- Setup\nmax new tokens: 128, $n=32$, $\\kappa$=32, $N=1024$, Base LLM: llama3_8B, Reward model: UltraRM/Eurus \nWe used UltraRM except for GSM8K and used Eurus for GSM8K because the original paper of Eurus (Yuan et al., 2024) have shown effectiveness on GSM8K.\n# Runtime evaluations\nSince Reviewers w4ta and Wd2o point out the runtime evaluation, we evaluate the runtime of AISP on a stand-alone server.\nFirst, runtimes for one iteration of AISP ($n=32$) and BoN ($N=32$) are 7.75 s and 7.68, respectively. Thus, **the overhead for weight updating at each iteration is about 1 %** (averaged results over 10 prompts of SHP with llama3_8B and UltraRM).  \n\nNext, we observed that Batched AISP $(b, n)=(4,8)$ takes 935.2 s for 100 prompts while BoN ($N=32$) takes 683.9 s (36.8 % overhead).\nThis is because the runtime of Batched AISP for one mini-batch is determined by the largest prompts in the mini-batch.\n**After sorting the prompts in terms of the token length, the runtime of Batched AISP becomes 738.4 s  and thus, the overhead is just 8 %.**\nThe constructive review comments provided interesting and valuable results, and we will add results and discussions!\n\nThe setup for this experiment is the following: we set the generated token length to fixed 128 for fair comparison.\nAdditionally, since our stand-alone servers have limited computation resources (A100 VRAM 40GB), we set $N=32$, $\\kappa$=7, $n=4$, and $b=8$ unlike Fig. 4."}}, "id": "tTFPLWGzgl", "forum": "mVcQXHRK8x", "replyto": "mVcQXHRK8x", "signatures": ["ICLR.cc/2026/Conference/Submission10984/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10984/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10984/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763677023715, "cdate": 1763677023715, "tmdate": 1763677796919, "mdate": 1763677796919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AISP, a training-free, test-time alignment method that injects Gaussian perturbations into the pre-logits over a fixed control window and updates the perturbation mean by adaptive importance sampling. The objective is cast via a free-energy bound, yielding an optimal pre-logit distribution proportional to $exp(r/\\lambda)p(V)$, since this is intractable, the mean is iteratively estimated with weighted samples. The authors argue the Gaussian assumption is consistent with softmax classifiers, show that AISP reduces to best-of-N (BoN) as as $\\lambda$‚Üí0, and report higher reward and GPT-4 win-rates than BoN, ARGS, and RE-Control on SHP/HH-RLHF."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper maps decoding-time reward maximization to sampling-based optimal control in pre-logit space; derivation via a free-energy lower bound is standard but cleanly presented.\n\n2. The method is training-free and lightweight; also the adaptive importance sampling loop is easy to implement.\n\n3. Empirical results show consistent reward and win-rate improvements over BoN with the same total samples."}, "weaknesses": {"value": "1. The control-theoretic view and MPPI-style derivation are known; the main step is moving importance sampling to pre-logit trajectories with a Gaussian prior. The reduction to BoN for Œª‚Üí0 underscores AISP as a structured BoN generalization to me rather than a new paradigm.\n\n2. Tasks are preference datasets (SHP, HH-RLHF) with reward-model scoring; diversity/coherence sometimes degrade, and win-rate uses small paired samples. No tests on reasoning/code/math where long-horizon dynamics might stress the method.\n\n3. AISP requires sequential updates; the paper does not report wall-clock vs. BoN under the same accelerator budget, nor scaling under constrained interleave with other requests.\n\n4. The Gaussian model for pre-logits is only heuristically connected to softmax; no quantitative validation of this assumption."}, "questions": {"value": "1. Can you quantify the Gaussian pre-logit assumption (per-token goodness-of-fit, across layers/models)? Any evidence that non-Gaussian priors would materially help/hurt AISP?  \n\n2. Please report wall-clock and throughput (tokens/sec) vs. BoN for matched total samples under realistic GPU budgets? \n\n3. For RE-Control, what data/compute were used to train the value function, and how does AISP compare at equal wall-clock including that one-time cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AMIoJstUF8", "forum": "mVcQXHRK8x", "replyto": "mVcQXHRK8x", "signatures": ["ICLR.cc/2026/Conference/Submission10984/Reviewer_w4ta"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10984/Reviewer_w4ta"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762102169638, "cdate": 1762102169638, "tmdate": 1762922174886, "mdate": 1762922174886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose an inference-time alignment framework, AISP (Adaptive Importance Sampling on Pre-logits). The authors frame this alignment task as a sampling-based optimal control problem. The core idea is to apply a stochastic Gaussian perturbation to the pre-logits, i.e., the penultimate layer outputs at each decoding step. AISP then uses adaptive importance sampling to iteratively update the mean of this perturbation, effectively creating a control signal that explores the generation space and guides the model toward high-reward sequences. Experimental evaluations on standard benchmarks on HH and SHP show that AISP outperforms competitive baselines such as  BoN, ARGS, and RE-Control."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is generally well written. The proposed AISP approach operates at inference time and therefore does not require training value functions, unlike RE-Control.\n\n2. The authors provide detailed hyperparameter ablations, a KL-divergence analysis, and a thorough comparison of batched AISP with BoN.\n\n3. The empirical evaluation is comprehensive, covering multiple base LLMs and reward models."}, "weaknesses": {"value": "1. The performance improvement on HH-RLHF appears incremental, and in many cases, BoN outperforms AISP. With only two datasets, it is difficult to fully assess AISP‚Äôs empirical effectiveness. I recommend evaluating on additional datasets to more clearly demonstrate the gains.\n\n2. [Minor] While the paper includes strong baselines, adding comparisons with the controlled decoding literature [1, 2] would further strengthen the experimental section.\n\n[1] Mudgal, S., Lee, J., Ganapathy, H., Li, Y., Wang, T., Huang, Y., Chen, Z., Cheng, H.T., Collins, M., Strohman, T. and Chen, J., 2023. Controlled decoding from language models. arXiv preprint arXiv:2310.17022.\n\n[2] Chakraborty, S., Ghosal, S.S., Yin, M., Manocha, D., Wang, M., Bedi, A.S. and Huang, F., 2024. Transfer q-star: Principled decoding for llm alignment. Advances in Neural Information Processing Systems, 37, pp.101725-101761."}, "questions": {"value": "Please see weakness 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9WsEEmRFEi", "forum": "mVcQXHRK8x", "replyto": "mVcQXHRK8x", "signatures": ["ICLR.cc/2026/Conference/Submission10984/Reviewer_uatg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10984/Reviewer_uatg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10984/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144801941, "cdate": 1762144801941, "tmdate": 1762922174011, "mdate": 1762922174011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}