{"id": "oZ9f1pwXEM", "number": 3034, "cdate": 1757319136800, "mdate": 1759898112728, "content": {"title": "MotionDDM: Motion Generation and Understanding via Discrete Diffusion Model", "abstract": "We present MotionDDM, a diffusion-LLM framework for bidirectional text-motion understanding and generation. Unlike GPT-style autoregressive approaches that tokenize motion and decode sequentially, MotionDDM performs multi-step parallel denoising, unifying Text-to-Motion (T2M), Motion-to-Text (M2T), and text-free Motion-to-Motion (M2M) within a single model. This decoding paradigm naturally enables a quality-latency trade-off at inference. On HumanML3D, our method achieves competitive T2M/M2T results against strong baselines.  We also incorporate Residual VQ (RVQ) as the motion tokenizer to improve quantization fidelity, and adopt GRPO within the framework to enhance alignment and controllability. To the best of our knowledge, this is the first work to bring diffusion-LLMs to bidirectional text-motion modeling.", "tldr": "", "keywords": ["Motion-Language Model; Discrete Diffusion Model; Mask Modeling; Residual Vector Quantization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d33083f5b84b24665c58e9601e5823ced1b7d72.pdf", "supplementary_material": "/attachment/f2d58a79839d66470d5d53420badcf3958f7f87d.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a unified text-motion model based on a diffusion-LLM framework, achieving overall strong performance across three tasks: Text-to-Motion, Motion-to-Text, and Motion-to-Motion. To better enhancing the alignment and controllability of the framework, the authors integrate the GRPO framework into their model."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe proposed framework integrates multiple tasks into a unified model, which is novel compared to traditional one-directional motion generation approaches.\n2.\tAs shown in Table 1, the model achieves solid results on both Text-to-Motion and Motion-to-Text tasks.\n3.\tDuring inference, the model requires only 10 steps to generate high-quality results, demonstrating good efficiency."}, "weaknesses": {"value": "1.\tFrom the experimental setup, the Motion-to-Motion task seems to serve a role similar to MAE-style representation learning. It remains unclear whether the model’s superior performance primarily stems from this task rather than T2M or M2T. The authors are encouraged to conduct ablation studies by enabling or disabling the Motion-to-Motion task. \n2. The proposed framework integrates multiple tasks during the learning process. Comparisons with single-task training are necessary to clarify the benefits of multi-task integration\n3.\tIn Table 6, increasing the training ratio of T2M improves the model’s performance on T2M, which is reasonable. However, increasing the M2T ratio unexpectedly decreases its performance. The authors should provide an explanation for this observation.\n4. Although the paper claims to achieve a quality-latency trade-off in the abstract, the model’s parameter count, computational efficiency, and actual inference speed are not reported. These should be added and compared with other methods to substantiate the claim.\n5.\tWhile a reproducibility statement is provided, given the system’s complexity, releasing the full source code would greatly enhance the work’s credibility and impact."}, "questions": {"value": "1.\tIn Table 6, percentages should be written as 80%, 10%, and 10%.\n2.\tPage 7 line 327: The Div metric is not “the higher, the better”; rather, it should be closer to the ground truth (see MotionGPT for reference).\n3.\tPage 1 line 50: The acronym “GRPO” should be spelled out when it first appears.\n4.\tPage 2: Figure caption font sizes are inconsistent and should be standardized."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PlrtYxTtXB", "forum": "oZ9f1pwXEM", "replyto": "oZ9f1pwXEM", "signatures": ["ICLR.cc/2026/Conference/Submission3034/Reviewer_gzPo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3034/Reviewer_gzPo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655988799, "cdate": 1761655988799, "tmdate": 1762916517053, "mdate": 1762916517053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "MotionDDM is the first work to introduce diffusion–language models into bidirectional text–motion modeling, proposing a unified parallel denoising decoding framework. This paradigm naturally supports the quality–latency trade-off and can be seamlessly extended to various tasks such as text-conditioned and text-free motion completion, prediction, and interpolation. By employing an RVQ-based motion tokenizer and integrating GRPO, the model enhances motion representation fidelity and cross-modal alignment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The article is clear and easy to understand.\nThe author conducted sufficient ablation studies to prove the effectiveness of each module."}, "weaknesses": {"value": "The techniques presented in this paper have largely already been explored and validated in existing unified models and diffusion-LLM research. For instance, RVQ has been employed in Go to Zero [1] and related works. As such, this paper feels more like a technical report rather than a conceptually novel study.\n\nThe paper does not clearly articulate the motivation or insight behind unifying understanding and generation. At least for me, it fails to convey why text–motion unification is necessary or meaningful. A more compelling direction would be to extend this idea toward a unified framework of vision, motion, and text, which would carry greater significance.\n\nMoreover, the experiments are conducted on only one dataset—although it is a classic benchmark, it is insufficient to justify the necessity of unified understanding and generation. The quantitative results also lag behind the latest diffusion-based methods.\n\nIn addition, there is no visual analysis, which is crucial for evaluating generative models.\n\nOverall, the paper appears to be a combination of several existing methods (diffusion-LLM, unified modeling, and GRPO) applied to a relatively small task. The insight and novelty are limited.\n\n[1] Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data"}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gwpYym9tAG", "forum": "oZ9f1pwXEM", "replyto": "oZ9f1pwXEM", "signatures": ["ICLR.cc/2026/Conference/Submission3034/Reviewer_7wmN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3034/Reviewer_7wmN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900664459, "cdate": 1761900664459, "tmdate": 1762916515228, "mdate": 1762916515228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MotionDDM, a discrete diffusion–LLM framework that unifies text-to-motion (T2M), motion-to-text (M2T), and text-free motion-to-motion (M2M) by treating both text and motion as token sequences denoised in multi-step parallel refinement. It uses Residual VQ (RVQ) for motion tokenization, a BERT-based masked backbone, and optional GRPO fine-tuning with task-specific rewards."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper presents a unified bidirectional formulation for bidirectional text–motion (T2M↔M2T). The parallel denoising enables one model to handle T2M, M2T, and M2M, with an explicit step-controlled quality–latency knob.\n\n- The proposed method can also support M2M completion, prediction, and interpolation under\nboth text-conditioned and text-free settings."}, "weaknesses": {"value": "- The experiments are conducted only on HumanML3D, and several retrieval metrics are not best-in-table (e.g., T2M R@1 lower than MoTe’s 0.548; M2T R@1 lower than MG-MotionLLM’s 0.592), though FID is strong. The authors should conduct more experiments by adding KIT-ML or HumanAct12, and clarifying where MotionDDM leads vs. falls short.\n\n- The T2M reward uses the model’s own M2T branch to produce a pseudo caption that is then compared to the ground-truth caption in CLIP space. This can bias rewards toward self-consistency rather than true motion–text faithfulness. Consider an external captioner or human preference subsets to calibrate rewards. \n\n- This paper argues a tunable quality–latency trade-off via step counts, but wall-clock latency (ms/sequence) and throughput are not reported. Please add runtime on a standard GPU (or the reported Ascend 910 NPU) for K={5,10,20,30}, including speedups vs. an AR baseline. \n\n- There are a few issues regarding the ablation and clarity: (a) RVQ depth table shows non-monotonic behavior. It would be good to add quantization error (MSE) vs. depth to clarify. (b) For masking schedule ablations, it is suggested to report M2T text metrics (not just T2M) to check cross-modal effects. (c) Please provide token rate/bitrate of RVQ (frames/sec × tokens/frame × bits/token) for reproducibility.\n\n- It would be good to show many qualitative results (visual comparison with the state-of-the-art methods). \n\n- It would be good to include more sota methods for comparison, such as MoMask, MotionLCM, MaskControl. \n\nMinor:\n\n- Table 7 shows FID = 0.0067 at CFG=5, far off neighboring entries. Can you please clarify?"}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YGHgdazUkY", "forum": "oZ9f1pwXEM", "replyto": "oZ9f1pwXEM", "signatures": ["ICLR.cc/2026/Conference/Submission3034/Reviewer_emfK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3034/Reviewer_emfK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943292195, "cdate": 1761943292195, "tmdate": 1762916515010, "mdate": 1762916515010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a diffusion-based framework for bidirectional text-to-motion and motion-to-text generation. The main design is to utilize a multi-step parallel denoising decoder to progressively denoise the noisy text and motion sequences. To enhance the model performance, the residual vector quantization for motion quantization is utilized, and a multi-task training schedule is proposed.  The GRPO is also integrated to enhance the alignment and controllability. Extensive experiments are conducted on the HumanML3D benchmark to evaluate the effectiveness of the proposed framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed multi-task scheduling mechanism for a unified and bidirectional T2M and M2T generation framework optimization is well-motivated and reasonable. \n- Experimental results on the motion-to-text task surpass previous methods, showcasing the proposed framework in enhancing the M2T task. \n- A sets of ablation studies are conducted to support the proposed design choices."}, "weaknesses": {"value": "- The performance of the proposed framework on the text-to-motion generation task is not very good, underperforming previous work in most of the evaluation metrics. And some recent stronger baselines for text-to-motion generation are missing, e.g., MoMask (CVPR 2024), MoGenTS (NeurIPS 2024), and LAMP (ICLR 2024). Note that these baselines are auto-regressive-based frameworks, and LAMP also supports T2M and M2T tasks.  Could the author provide some analysis and insights on why the performance gain on the T2M task is less? Also, the proposed framework underperforms these baselines on the T2M tasks, and the author should be more cautious in the claims in L044-L046.\n- Experiments are only conducted on the HumanML3D dataset. Evaluating the proposed framework on more datasets, e.g., the Motion-X dataset, and the KIT-ML datasets, will bolster the claims of generalizability and scalability."}, "questions": {"value": "The multi-task scheduler randomly assigns a task to each sample in a batch. Will more structured scheduling (e.g., curriculum learning) improve the performance? Specifically, the performance gain on the text-to-motion task seems smaller with the proposed framework. Does this mean the T2M task is harder for the proposed framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gde2P7pqUr", "forum": "oZ9f1pwXEM", "replyto": "oZ9f1pwXEM", "signatures": ["ICLR.cc/2026/Conference/Submission3034/Reviewer_RXAW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3034/Reviewer_RXAW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762277259411, "cdate": 1762277259411, "tmdate": 1762916511011, "mdate": 1762916511011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}