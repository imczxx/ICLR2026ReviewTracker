{"id": "45EtKUdgbJ", "number": 19884, "cdate": 1758300220966, "mdate": 1759897014438, "content": {"title": "Forward-Learned Discrete Diffusion: Learning how to noise to denoise faster", "abstract": "Discrete diffusion models are a powerful class of generative models that demonstrate strong performance across many domains. However, for efficiency, discrete diffusion typically parameterizes the generative (reverse) process with factorized distributions, which makes it difficult for the model to learn a target process in a small number of steps and necessitates a long, computationally expensive sampling procedure. To reduce the gap between the target and model distributions and enable few-step generation, we introduce a learnable noising (forward) process for discrete diffusion. Instead of fixing a Markovian forward chain, we adopt a non-Markovian formulation and introduce learnable marginal and posterior distributions. This allows the generative process to remain factorized while matching the target defined by the noising process. We train all parameters end-to-end under the standard variational objective.", "tldr": "", "keywords": ["diffusion", "generative models", "variational inference"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e4105b3ec490b22da94c92ecd0a7ff7839d6251.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper learns the forward noising process in a non-Markovian way so that the training targets match what a factorized reverse model can learn well in just a few steps. It keeps the reverse sampler unchanged. Via learned forward, the paper achieves much better quality–inference latency trade-offs. Across text (ROCStories) and molecular (QM9, ZINC250k) data, the paper with only $T=10$ reverse steps delivers quality close to its $T=100$ counterpart—unlike conventional discrete diffusion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "FLDD’s main idea is both theoretically sound and practically useful: learn the forward noising process in a manner so that the task for the reverse process becomes easier. In addition, the algorithm uses the standard variational objective and does not change the reverse sampler. The posterior is implemented with a cheap Maximum Coupling transport between consecutive marginals, which keeps training tractable while letting each coordinate depend on the whole input. Empirically, the paper shows few-step generation benefits illustrating a better quality–latency trade-off than conventional discrete diffusion."}, "weaknesses": {"value": "FLDD adds a learned forward network, increasing training compute relative to standard discrete diffusion. The paper does not report quality vs total training and inference budget (e.g., GPU-hours or FLOPs), leaving unclear whether the few-step gains persist under fixed total compute budget."}, "questions": {"value": "The paper does not compare REINFORCE against other standard gradient estimators for discrete latents. It does not discuss control variates such as Rao–Blackwellization tricks. These would reveal which knobs actually make training computationally stable/cheap. Could you report some of these results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DNOH61I8ww", "forum": "45EtKUdgbJ", "replyto": "45EtKUdgbJ", "signatures": ["ICLR.cc/2026/Conference/Submission19884/Reviewer_wcR4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19884/Reviewer_wcR4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771362523, "cdate": 1761771362523, "tmdate": 1762932045613, "mdate": 1762932045613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes forward-learned discrete diffusion (FLDD). It’s a discrete diffusion with a learnable forward process. The idea is to keep the generative process factorized while matching the target, so it can work well for small numbers of diffusion steps. Experiments show that the performance of FLDD drops a little when the number of diffusion steps goes down from 100 to 10."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel approach for training discrete diffusion models"}, "weaknesses": {"value": "- FLDD with T=100 doesn’t outperform some previous work. The authors argue that FLDD performs better for small Ts. However, there are methods to accelerate sampling of discrete diffusion models, e.g. distillation. For small-T experiments, FLDD needs to be compared with methods focusing on accelerated sampling"}, "questions": {"value": "- What's the batch size used in training and how does it affect the performance?\n- What are the additional costs of training, in terms of memory (chips) and duration (time)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a1HN7KIqY2", "forum": "45EtKUdgbJ", "replyto": "45EtKUdgbJ", "signatures": ["ICLR.cc/2026/Conference/Submission19884/Reviewer_vLZq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19884/Reviewer_vLZq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799997306, "cdate": 1761799997306, "tmdate": 1762932045101, "mdate": 1762932045101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Forward-Learned Discrete Diffusion (FLDD), a new framework for discrete diffusion models aimed at improving sampling efficiency and enabling few-step generation. The core idea is to replace the standard fixed, Markovian noising process with a learnable, non-Markovian forward process. By co-training this flexible forward process alongside the reverse (denoising) process, the authors argue that the model can learn an easier-to-invert corruption path. This, in turn, allows a simple, factorized reverse sampler to generate high-quality samples in a significantly reduced number of steps. The method is trained end-to-end using the standard variational objective and is evaluated on synthetic data, binarized MNIST, text generation (ROCStories), and molecular generation (QM9, ZINC250k)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper does a good job of grounding the proposed method in the established variational inference framework for diffusion models. The formulation for end-to-end training of both the forward and reverse processes while preserving the standard variational objective is principled and clearly presented.\n2. The paper is generally well-written and easy to follow."}, "weaknesses": {"value": "1. The primary qualitative results are presented on a 2D Gaussian mixture and binarized MNIST. These are considered solved or overly simplistic problems in the current deep generative modeling landscape. Demonstrating success on these toy tasks provides very little signal about the method's effectiveness on complex, high-dimensional discrete data that is of interest to the community.\n2. A significant weakness of this paper is its failure to demonstrate a clear performance advantage over existing, strong baselines. The paper's own results on the ROCStories dataset (Table 1) show that SEDD (Lou et al., 2023), a recent and relevant discrete diffusion model, achieves a higher MAUVE score (0.598 vs. 0.538) and a better (lower) PPL than the proposed FLDD, even when FLDD uses a large number of steps (T=100). This directly undermines the paper's central claims regarding improved sampling efficiency and achieving a better quality-latency trade-off. Given that a method from 2023 already shows superior performance on the same benchmark, the practical contribution of the proposed, more complex FLDD framework is called into question.\n3. The paper fails to cite, discuss, or compare against the seminal work: LLaDA[1], a landmark paper that demonstrated how to successfully apply diffusion models to high-quality text generation and is arguably the most famous work in this specific area.\n\n[1] Nie, Shen, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. \"Large language diffusion models.\" arXiv preprint arXiv:2502.09992 (2025)."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vguzWoIDkm", "forum": "45EtKUdgbJ", "replyto": "45EtKUdgbJ", "signatures": ["ICLR.cc/2026/Conference/Submission19884/Reviewer_ATG6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19884/Reviewer_ATG6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917091299, "cdate": 1761917091299, "tmdate": 1762932044623, "mdate": 1762932044623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Forward-Learned Discrete Diffusion, a method designed to accelerate the reverse process while supporting end-to-end training.\n\nUnlike previous works, FLDD adopts a non-Markovian formulation with learnable marginal and posterior distributions, which allows the generative process to remain factorized while matching the target distribution defined by the noising process. The proposed method enables a better alignment between the forward and reverse processes even with a small number of steps, thereby improving its generation performances."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A novel method that integrates insights from SS-DDPM and DPS, achieving excellent performance in few-step generation under specific scenarios.\n\n- The proposed method significantly reduces the requirement for the number of timesteps in the backward process, which is a notable advantage for practical applications.\n\n- This paper is well-structured and logically coherent, with smooth transitions between sections. The examples are appropriately designed, making the core ideas easy to follow."}, "weaknesses": {"value": "- The definition of \"end-to-end\" in the context of FLDD remains unclear. It is necessary to clarify whether this refers to training without separate stages or other specific characteristics.\n\n- In Section 4.2, FLDD exhibits poor performance on the ROCStories dataset, but the authors' explanation for this phenomenon lacks sufficient persuasiveness and requires further elaboration."}, "questions": {"value": "1. In the forward process, what are the key differences between FLDD and Star-shaped Diffusion? Could the authors elaborate on the unique design choices of FLDD in this regard?\n\n2. Compared with continuous diffusion models that adopt similar training strategies on continuous datasets (e.g., MNIST), what specific advantages does FLDD offer? For example, in terms of generation quality, training efficiency, or adaptability to data characteristics.\n\n3. In the experimental results (Table 1), FLDD performs poorly on real-world data, particularly with a PPL of 60, which indicates an inability to generate semantically meaningful sentences. Could the authors provide a more specific analysis of the underlying reasons? For instance, whether it is due to mismatches between the forward process design and real-world data distributions or limitations in the learnable marginal/posterior functions.\n\n4. While FLDD demonstrates strong performance in few-step diffusion, what are the precise time and computational costs compared to existing methods? A smaller number of steps does not always translate to lower overhead, so quantitative comparisons would help verify its efficiency advantages."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Aa7Jo0irqg", "forum": "45EtKUdgbJ", "replyto": "45EtKUdgbJ", "signatures": ["ICLR.cc/2026/Conference/Submission19884/Reviewer_mvjP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19884/Reviewer_mvjP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762332664852, "cdate": 1762332664852, "tmdate": 1762932044163, "mdate": 1762932044163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}