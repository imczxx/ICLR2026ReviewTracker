{"id": "hLpJjDmFlQ", "number": 16805, "cdate": 1758268866030, "mdate": 1763663856623, "content": {"title": "Towards Photonic Band Diagram Generation with Transformer-Latent Diffusion Models", "abstract": "Photonic crystals enable fine control over light propagation at the nanoscale, and thus play a central role in the development of photonic and quantum technologies. Photonic band diagrams (BDs) are a key tool to investigate light propagation into such inhomogeneous structured materials. However, computing BDs requires solving Maxwell’s equations across many configurations, making it numerically expensive, especially when embedded in optimization loops for inverse design techniques, for example. To address this challenge, we introduce the first approach for BD generation based on diffusion models, with the capacity to later generalize and scale to arbitrary three-dimensional structures. This preliminary study couples a transformer encoder, which extracts contextual embeddings from the input structure, with a latent diffusion model to generate the corresponding BD. In addition, we provide insights into why transformers and diffusion models are well suited to capture the complex interference and scattering phenomena inherent to photonics. This cross-disciplinary approach is bridging modern deep learning architectures with complex photonic design problems, paving the way for new surrogate modeling strategies in this domain.", "tldr": "We propose a new method using latent diffusion models and transformers to generate band diagrams from 3D photonic crystals.", "keywords": ["Photonic crystals", "band diagrams", "diffusion", "transformers", "surrogate models"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7b37cd9874a62413c6a6baea7e66e322ef56503b.pdf", "supplementary_material": "/attachment/441580fd9c5ae7aa704bc63803600b13d347a93f.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a conditional generative pipeline that maps layered photonic structures to their photonic band diagrams. It encodes a stack of two-dimensional slices with a transformer to capture inter-layer interactions, compresses band diagrams into a latent representation via a vision-transformer autoencoder, and synthesizes full diagrams using a latent diffusion model conditioned on the learned structural context. The contributions center on a coherent end-to-end architecture tailored to layered media, a practical data and training setup for this mapping, and internal analyses that illuminate design choices and trade-offs for use as a fast proxy within iterative design workflows."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Coherent end-to-end design aligned with the layered-structure setting.** The integration of a slice-wise transformer encoder, a compact latent space for band diagrams, and a conditional generative stage yields a clear information flow; this organization explicitly targets inter-layer coupling while keeping components modular and interpretable, facilitating future substitutions or extensions without altering the overall pipeline.\n    \n2. **Practical efficiency for design-loop scenarios.** By generating band diagrams directly from structural inputs, the approach reduces reliance on repeated high-cost simulations and provides rapid feedback during exploration and screening; this predictability and speed are well matched to workflows where timely guidance is more valuable than exact solver-level fidelity at every iteration.\n    \n3. **Thoughtful engineering choices and in-scope diagnostics.** The paper includes contrastive pre-alignment between structure and diagram embeddings, ablations on whether to freeze the structural encoder, and qualitative visualizations that reveal success and failure modes; although not a substitute for cross-family baselines, these studies clarify the contribution of each component and improve transparency around training stability and inference behavior."}, "weaknesses": {"value": "1. **Architectural novelty alone does not demonstrate effectiveness.** While the manuscript suggests that prior work relies on dense networks, VAEs, or U-Nets and positions diffusion models as more advanced, this framing does not, by itself, establish superiority; the paper should include controlled, like-for-like comparisons against representative baselines such as a conditional VAE, a deterministic U-Net/Transformer regressor, or other strong generative families trained under matched budgets, with clear reporting of variability and qualitative overlays that reveal where predictions diverge from reference solutions, because only such head-to-head evidence can substantiate the claimed benefits.\n    \n2. **The rationale for limited transferability to three-dimensional settings is unconvincing.** The manuscript does not clearly explain why existing approaches could not be adapted to three-dimensional structures, since adding a depth coordinate or layer index is commonly handled via encoding strategies or volumetric operators; consequently, the authors should either provide a conceptual argument for what specifically hinders straightforward 3D extensions in prior methods or, preferably, include comparisons to simple 3D CNN/U-Net/Transformer variants and slice-attention baselines that process the additional dimension directly, as this would clarify whether the proposed pipeline is uniquely capable rather than merely a design preference.\n    \n3. **The problem motivation does not isolate a domain-specific challenge that necessitates a tailored method.** As presented, the task appears addressable by a range of generic architectures capable of producing three-dimensional outputs, leaving unclear what special difficulty distinguishes this setting and warrants a bespoke solution; for a venue emphasizing methodological innovation, the paper would benefit from articulating concrete domain hurdles and demonstrating where standard baselines struggle, otherwise the work may be more appropriately positioned in an application-focused forum where its practical utility can be judged against field-specific benchmarks."}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oemfxBrIBI", "forum": "hLpJjDmFlQ", "replyto": "hLpJjDmFlQ", "signatures": ["ICLR.cc/2026/Conference/Submission16805/Reviewer_uYTB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16805/Reviewer_uYTB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761495404352, "cdate": 1761495404352, "tmdate": 1762926836617, "mdate": 1762926836617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an ambitious and conceptually interesting framework for generating photonic band diagrams (BDs) from 3D photonic crystal (PhC) structures using transformer-based latent diffusion models. The approach represents a clear methodological advance over prior deep learning surrogates that relied on CNNs or VAEs, as it leverages transformers to capture long-range electromagnetic couplings and diffusion models to synthesize fine-grained spectral details. The proposed Material-to-Context (M2C) encoder, ViT-based BD autoencoder, and conditional latent diffusion model together form a scalable architecture with the potential to drastically reduce the computational cost of BD prediction—reportedly achieving up to 900× speedup compared to rigorous coupled-wave analysis (RCWA) simulations.\n\nHowever, the experimental evidence is less convincing than the conceptual innovation. The reported quantitative metrics (Dice ≈ 0.37 on the small dataset and ≈ 0.23 on the large one) indicate limited fidelity, and the generated BDs, while visually plausible, often miss important spectral features. In photonic design, such small spectral mismatches can correspond to large physical deviations, so it remains unclear whether the model’s outputs are reliable enough for practical use. The paper acknowledges these limitations but does not offer concrete solutions, such as uncertainty quantification, physics-informed regularization, or calibration, to mitigate or measure these discrepancies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s main strength lies in its novel application of latent diffusion models to the problem of photonic band diagram (BD) generation, a domain where traditional solvers like RCWA are computationally intensive. The authors creatively combine transformer encoders and diffusion-based generative modeling, demonstrating how recent advances in generative AI can be repurposed for physics-driven simulation tasks. This cross-disciplinary approach is original and timely, bridging modern deep learning architectures with complex photonic design problems.\n\nConceptually, using a diffusion process to synthesize physically meaningful spectra represents a significant methodological innovation, offering the potential to replace thousands of numerical Maxwell solves with a single generative inference step. The proposed pipeline, especially the Material-to-Context (M2C) transformer, is thoughtfully designed and technically sound, highlighting an insightful understanding of how self-attention mechanisms can capture multi-layer optical coupling effects."}, "weaknesses": {"value": "1. While the conceptual innovation is clear, the paper’s experimental and analytical depth falls short of its stated ambition. The most significant limitation is the lack of comparison against strong baselines. The authors mention prior CNN-, VAE-, and U-Net–based models for photonic band or dispersion prediction but do not provide direct quantitative comparisons to these architectures. Without such baselines, it is difficult to assess whether the proposed diffusion-transformer framework offers a tangible performance advantage, or whether the observed results could be matched by simpler models.\n\n2. The paper also provides limited discussion on generalization. Although the authors claim that the framework can be scaled to arbitrary 3D photonic structures, all experiments are confined to synthetic, highly regular datasets built from stacked holey and uniform layers. There is no evidence that the model can handle more complex geometries, continuous permittivity variations, or realistic fabrication noise—conditions essential for true generalizability.\n\n3. Another weakness lies in the lack of critical analysis of the model’s underperformance. Reported metrics (Dice ≈ 0.37 on the small dataset, ≈ 0.23 on the large one) and visual comparison are substantially lower than what would constitute reliable physical prediction, yet the paper does not explore the causes. For instance, it remains unclear whether errors arise from the diffusion process, the latent space compression, or the transformer conditioning. A deeper ablation study or error decomposition would have clarified the model’s limitations and helped guide future improvements.\n\n4. Finally, while the authors briefly mention possible remedies—such as larger encoders, physics-informed priors, or ensemble sampling—these ideas are presented only as speculation rather than experimentally supported strategies. Overall, the work would benefit greatly from stronger empirical validation, explicit baseline comparison, and systematic error analysis, which would make its claims of physical relevance more convincing and actionable."}, "questions": {"value": "Could you provide quantitative comparisons against standard baselines such as CNNs, VAEs, or U-Nets used in previous photonic band prediction works? This would clarify whether the diffusion-based approach offers a real advantage beyond architectural novelty.\n\nHow well does the model generalize to more complex 3D photonic structures beyond the synthetic stacked-layer dataset? Have you tested any out-of-distribution geometries or permittivity ranges to support your scalability claim?\n\nThe reported Dice and mAP scores are relatively low. Could you analyze where the errors come from—for example, from the transformer encoding, latent diffusion stage, or reconstruction process?\n\nGiven that small spectral deviations can have large physical impacts, have you considered adding uncertainty estimation or confidence measures to assess the reliability of generated band diagrams?\n\nYou mention physics-informed strategies as future work. Could you outline more concretely how such priors or constraints might be incorporated into your diffusion pipeline?\n\nFinally, do you view this model primarily as a fast surrogate for exploration or as a physically accurate predictor for design optimization? Clarifying this distinction would help position the contribution more precisely."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fkVo7DyvS2", "forum": "hLpJjDmFlQ", "replyto": "hLpJjDmFlQ", "signatures": ["ICLR.cc/2026/Conference/Submission16805/Reviewer_zhRf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16805/Reviewer_zhRf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988699285, "cdate": 1761988699285, "tmdate": 1762926836206, "mdate": 1762926836206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a surrogate model for photonic band diagram (BD) generation using a Transformer–Latent Diffusion framework. The method takes a stack of dielectric layers as input, encodes it with a transformer-based material-to-context encoder (M2C), and conditions a latent diffusion model (U-Net backbone) to generate corresponding BDs. The encoder–decoder pair is contrastively trained (MoCo) before fine-tuning the diffusion generator. Experiments use synthetic RCWA-generated datasets of 2–8 layer photonic crystals, showing up to 82× faster inference than RCWA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n* Applying diffusion and transformer conditioning to photonic simulation seems to be new.\n* The paper is clear and easy to follow"}, "weaknesses": {"value": "Weaknesses:\n* Poor justification for transformer + diffusion choice. The core question remains unanswered: why should this task need transformers or latent diffusion? The paper never compares against simpler or more conventional surrogates, showing only the adopted model arch's accurcay. Showing the reasons why we need diffusion would give readers more insight about the paper?\n* Quantitative quality is very low. Table 2 (p. 8) reports mAP only ≈ 0.23 – 0.44 and dice ≈ 0.23 – 0.37 — poor even for the small dataset, with further degradation on the larger one.\n* Limited dataset, especially given the precision seems to be bad. I feel the included dataset is already a simplified one, makeing me concern about the performance on more complicated design."}, "questions": {"value": "Minor:\n* The authors fails to include those operator-learning approaches for AI4photonics, e.g., \nPIC2O-Sim: A Physics-Inspired Causality-Aware Dynamic Convolutional Neural Operator for Ultra-Fast Photonic Device FDTD Simulation PACE: Pacing Operator Learning to Accurate Optical Field Simulation for Complicated Photonic Devices\n* Dataset realism. Can you try a harder design to check your method's accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uzNEUkpcFH", "forum": "hLpJjDmFlQ", "replyto": "hLpJjDmFlQ", "signatures": ["ICLR.cc/2026/Conference/Submission16805/Reviewer_UX6z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16805/Reviewer_UX6z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059235618, "cdate": 1762059235618, "tmdate": 1762926835767, "mdate": 1762926835767, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a learned surrogate to generate photonic band diagrams (BDs) from layered periodic structures far faster than classical solvers. The pipeline assembles: 1. a Transformer encoder over slice sequences of the structure (with cumulative‑depth and Fourier cues), 2. a ViT‑VAE that embeds BD images, 3. contrastive alignment between the two embeddings, and 4. a latent diffusion model conditioned via cross‑attention on the structure embedding to produce the BD. Datasets are synthetic layered stacks; the surrogate achieves large speedups and moderate image‑overlap fidelity. The intended use is rapid screening in design loops."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear end‑to‑end generative surrogate with a plausible conditioning pathway (contrastive‑aligned structure embedding feeding a latent diffusion sampler).\n- Practical representation of layered structures as sequences with simple positional and spectral cues.\n- Demonstrated computational savings that can enable rapid screening workflows."}, "weaknesses": {"value": "- Limited ablations make it difficult to isolate which architectural choices matter (e.g., need for contrastive pretraining vs. direct conditioning; choice of ViT‑VAE vs. CNN; freezing vs. joint fine‑tuning of the structure encoder; where and how conditioning is injected).\n- Evaluation leans on image overlap metrics; task‑target mismatch makes it harder to connect score improvements to practically useful BD characteristics.\n- Generalization beyond the training manifold (richer geometries, different distributions) is not demonstrated; robustness and uncertainty are not quantified.\n- Reporting is light on parameter counts, compute, training stability, and quality‑vs‑cost trade‑offs (number of diffusion steps, guidance scale, etc.)."}, "questions": {"value": "1. **Contrastive alignment:** What is the temperature, queue size, and negative sampling strategy? How much does contrastive pretraining contribute versus training the diffusion model end‑to‑end without it? Please provide a clean ablation.\n2. **Structure encoder design:** How sensitive are results to depth/width, attention heads, positional encoding (learned vs. sinusoidal), cumulative‑depth encoding, and Fourier channels? A table isolating each component would clarify necessity.\n3. **Conditioning pathway:** Where is conditioning injected into the latent diffusion U‑Net (which blocks, which resolutions)? Have you compared cross‑attention with FiLM/adapters/concatenation, and classifier‑free guidance versus conditional‑only training?\n4. **BD representation:** Why a ViT‑VAE for BD embedding rather than a CNN or a 1D sequence model along k with local convs across frequency? Please report reconstruction quality, KL weight, latent dimensionality, and how these affect downstream generation.\n5. **Freezing vs. joint training:** You note freezing the structure encoder can help. Can you show curves or a table comparing joint fine‑tuning, partial unfreezing, and full freezing, and discuss stability/overfitting issues?\n6. **Baselines and simplicity:** How does a direct conditional UNet (no VAE, no contrastive pretraining) compare under the same compute? Similarly, how does a simpler CNN encoder for structures fare against the Transformer?\n7. **Robustness and uncertainty:** Can you report sample‑to‑sample variance (multiple diffusion draws) and whether it correlates with errors? Even simple ensembles or variance maps would help quantify uncertainty.\n8. **Data splits and leakage:** How are splits defined to avoid near‑duplicate structures across train/val/test? Any augmentations that risk leakage (e.g., deterministic ordering) should be clarified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rek76zrG7E", "forum": "hLpJjDmFlQ", "replyto": "hLpJjDmFlQ", "signatures": ["ICLR.cc/2026/Conference/Submission16805/Reviewer_YhcL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16805/Reviewer_YhcL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16805/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762069215928, "cdate": 1762069215928, "tmdate": 1762926835327, "mdate": 1762926835327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}