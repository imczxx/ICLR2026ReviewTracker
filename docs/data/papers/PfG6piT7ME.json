{"id": "PfG6piT7ME", "number": 8132, "cdate": 1758067452335, "mdate": 1759897805064, "content": {"title": "Beyond Invariance: A Feature-Strength Framework for OOD Generalization", "abstract": "Out-of-Distribution (OOD) generalization is a central challenge in machine learning. Models often fail on unseen data, not because of an inability to learn robust signals, but because they $\\textit{preferentially learn spurious, dataset-specific correlations that are highly predictive for in-distribution examples}$. Existing solutions typically focus on searching for invariant features, yet often overlook a more fundamental question: $\\textbf{what properties of the training data cause models to learn these non-invariant ``shortcut\" features in the first place?}$ In this work, we present a different perspective on OOD generalization. We argue that failures to generalize are a direct consequence of models learning the strongest features in the training data, which are often spurious. Guided by this, we reframe OOD generalization not as a search for invariance, but as the $\\textit{problem of identifying and mitigating the influence of these overly dominant features}$. Under this new perspective, we develop a novel primitive for quantifying feature strength across a training set. This primitive gives rise to a targeted regularization algorithm that weakens a model's reliance on the identified strongest features, thereby compelling it to learn more robust and causally stable signals. Our method demonstrates substantial improvements in generalization across a wide range of OOD benchmarks, improving OOD accuracy by up to $2\\times$ over standard training and significantly outperforming existing baselines without compromising in-distribution performance.", "tldr": "", "keywords": ["OOD Generalization"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/67dfbc8f40a0a8056863d68e5a301d18f1013169.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a feature-strength framework for OOD generalization. In particular, the paper argues that ERM-trained neural networks fail to generalize OOD due to an overly focus on dominant features, quantified by the \"strength\" of the feature. The paper then proposes a regularization method to weaken the model's reliance on dominant features, thus increasing the overall feature diversity. The proposed method is evaluated on a range of OOD generalization benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed quantification scheme of feature strength is of interest in the OOD generalization context."}, "weaknesses": {"value": "The paper claims that the perspective that ERM fails to generalize OOD due to favoring the strongest feature is new. However, there is already prior work that introduces similar observations and/or analysis, e.g., [1, 2], to name a few. There are also targeted methods for biasing ERM to learn richer or more diverse features, e.g, through weight averaging [3, 4] or penalizing the effect of dominant features [5, 6], which shares nearly the same spirit as this paper.\n\nThe main ingredients of the proposed feature-strength framework (Sec. 3.2 and 3.5.1) are taken from Khaddaj et al. Overall, the technical contribution of the paper is limited.\n\nThe proposed method is only evaluated against a limited set of baselines. For example, weight averaging methods should be included on PACS and VLCS, and there should be more recent baselines on Waterbirds and CelebA.\n\nFinally, the paper is written in a narrative that existing OOD generalization research mostly focuses on learning invariant representations. While invariance is indeed a prevailing perspective on OOD generalization, there are also analyses attributing the failure of OOD to limited feature diversity (e.g., [1-6]) or the inductive bias of the model (e.g, [7-9]). The authors are encouraged to add them to the related work section to avoid misunderstanding for future readers.\n\n---\n\n[1] The pitfalls of simplicity bias in neural networks. NeurIPS, 2020.\n\n[2] Evading the simplicity bias: Training a diverse set of models discovers solutions with superior OOD generalization. CVPR, 2022.\n\n[3] Spurious feature diversification improves out-of-distribution generalization. ICLR, 2024.\n\n[4] Diverse weight averaging for out-of-distribution generalization. NeurIPS, 2022.\n\n[5] Gradient starvation: A learning proclivity in neural networks. NeurIPS, 2021.\n\n[6] Rich feature construction for the optimization-generalization dilemma. ICML, 2022.\n\n[7] Understanding the failure modes of out-of-distribution generalization. ICLR, 2021.\n\n[8] Feature contamination: Neural networks learn uncorrelated features and fail to generalize. ICML, 2024.\n\n[9] Understanding and improving feature learning for out-of-distribution generalization. NeurIPS, 2023."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "W6lhSCVNWr", "forum": "PfG6piT7ME", "replyto": "PfG6piT7ME", "signatures": ["ICLR.cc/2026/Conference/Submission8132/Reviewer_RnsM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8132/Reviewer_RnsM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761661426308, "cdate": 1761661426308, "tmdate": 1762920106235, "mdate": 1762920106235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new perspective on Out-of-Distribution (OOD) generalization, arguing that model failures are not just a matter of missing invariant features, but a direct result of the learning algorithm, such as Empirical Risk Minimization (ERM), preferentially learning the statistically strongest features, which are often spurious shortcuts. Based on a few assumptions made, the authors prove that their algorithm can achieve smaller loss than the ERM algorithm. The authors run experiments on standard distribution shifts datasets to show that their method has better performance, albeit that the baselines are mostly before 2021."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, this paper is well motivated and theoretically grounded. The study of OOD generalization is long-lasting, and there are a lot of empirical and theoretical stuides trying to formulate the problem from the aspects of feature properties, and regularizing the strengthes of features is relatively novel."}, "weaknesses": {"value": "As a theory-motivated paper, I think the following issues are blocking the paper to reach the accpetance threshold. Specifically,\n\n- Assumptions not carefully verified, and the empirical validation does not match the assumptions. Based my understanding, Assumption 1 is probably the most important assumption that summarizes the authors' intuition, and it is not a obvious thing. However, the authors fail to empirically demonstrate that the assumptions hold (even to some extent). The study in section 3.3 is quite different from what I think, as (1) the spurious feature is completely synthetic and could not reflect the practical scenarios, and (2) the relation $I_{c_s}(k) > I_{c_r}(k)$ is not analytically verified even in this synthetic scenario. Since Assumption 1 is the foundation of the proposed algorithm, it should be carefully verified to support the paper.\n- Theoretical results not quantiative. I am not satisfied with the two theorems presented in Section 3. With all these assumptions and theoretical framing, I am surprised that the authors cannot present a quantiative improvement result compared with ERM. Theorem 2 is too weak as it give no implication on how much the proposed algorithm can outperform ERM, not to say the other OOD algorithms. In addition, the authors should also compare with other OOD analysis works, including \"In Search of Lost Domain Generalization\" as well as \"Towards a Theoretical Framework of Out-of-Distribution Generalization\".\n- Experiments insufficiency. Overall, baselines compared in this paper is too outdated, with the latest one from 2021. This completely ignores the recent progress in the OOD generalization domains, and the authors should find at least 2 more recent baselines to compare. In addition, the authors are encouraged to evaluate their methods on the WILDS dataset, which is a magnitude larger than the existing datasets and can reflect their method's performance better."}, "questions": {"value": "- Line 213: there could be typos in the formula. $S' \\sim \\mathcal D_S$ should be under the expectation.\n-  Can you give a more intuitive explanation why the assumption 2 hold? I can understand the intuition behind the assumption 1 but for this one it's a bit hard to understand."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XCbB4D7plY", "forum": "PfG6piT7ME", "replyto": "PfG6piT7ME", "signatures": ["ICLR.cc/2026/Conference/Submission8132/Reviewer_JckS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8132/Reviewer_JckS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950585444, "cdate": 1761950585444, "tmdate": 1762920105612, "mdate": 1762920105612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper suggests that the strongest features learned by ERM are likely to be spurious and less generalizable. To reduce such reliance, Feature Strength Regularization (FSR) is proposed. FSR calculates the strength of each feature using datamodels, and inversely weights each sample based on the summation of the strengths of the features it contains. Theoretical analysis shows that FSR can identify features with a strong influence and provides a better performance guarantee compared to ERM. Experiments on several datasets show the potential of FSR."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Using influence weight to measure strong and weak features is novel. \n\n2. Theoretical analysis gives mathematical insight. \n\n3. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. There are several existing works (e.g. RSC [1], MaskTune [2], FeAT [3]) that recognize the spurious features are likely to be dominant, and propose methods to reduce a model’s reliance on the strongest features to achieve better generalization. They should be mentioned and compared to show the advances of FSR. \n\n2. FSR relies on a predefined set of features to capture the feature strength. To calculate $\\lambda$ for each sample, an annotation of the features it contains needs to be available, which requires extra information in addition to the class labels. This hinders the generalizability of FSR. \n\n3. Using datamodels to calculate feature strengths incurs very high computational cost, as thousands of extra models need to be trained. This will significantly reduce the scalability of FSR when larger architectures are required for complex datasets. \n\n[1] Huang et al., Self-Challenging Improves Cross-Domain Generalization, ECCV 2020\n\n[2] Taghanaki et al., MaskTune: Mitigating Spurious Correlations by Forcing to Explore, NeurIPS 2022 \n\n[3] Chen et al., Understanding and Improving Feature Learning for Out-of-Distribution Generalization, NeurIPS 2023."}, "questions": {"value": "1. How is the predefined set of features determined for each dataset used in the experiment? How is the feature information collected for each sample? Given a new dataset, how should the features be defined to apply FSR?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aV1RU5kbZX", "forum": "PfG6piT7ME", "replyto": "PfG6piT7ME", "signatures": ["ICLR.cc/2026/Conference/Submission8132/Reviewer_aAc2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8132/Reviewer_aAc2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993182356, "cdate": 1761993182356, "tmdate": 1762920105215, "mdate": 1762920105215, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the problem of OOD generalization. The authors reframe the problem of OOD generalization as one of detecting and mitigating the influence of the strongest features in a dataset and propose Feature Strength Regularization (FSR) to avoid learning dominant features. Experiments on some OOD benchmarks show the effectiveness of FSR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The feature learning perspective and the reframing of the OOD generalization problem are interesting and sound.\n\n2. The presentation is clear and easy to follow.\n\n3. Empirical results show improvements on some OOD benchmarks."}, "weaknesses": {"value": "1. The primary problem has already been well-studied in the literature via the notion of rich feature learning[1,2,3,4]:\n\n[1] Rich Feature Construction for the Optimization-Generalization Dilemma, ICML'22.\n\n[2] Pareto Invariant Risk Minimization, ICLR'23.\n\n[3] Understanding and Improving Feature Learning for Out-of-Distribution Generalization, NeurIPS'23.\n\n[4] Learning useful representations for shifting tasks and distributions, ICML'23.\n\n\n2. The method relies on the fitting of the data model, which may not be practical and require a sufficient number of samples.\n\n3. Lack of comparison with the state-of-the-art baselines as well as real-world OOD benchmarks such as those in the Wlids benchmark."}, "questions": {"value": "Please find the details in the section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RuV46hwAPW", "forum": "PfG6piT7ME", "replyto": "PfG6piT7ME", "signatures": ["ICLR.cc/2026/Conference/Submission8132/Reviewer_AD82"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8132/Reviewer_AD82"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010760842, "cdate": 1762010760842, "tmdate": 1762920104783, "mdate": 1762920104783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that ERM often fails to generalize because the optimization process is drawn to the strongest features in the training distribution — and those strongest features are frequently spurious. To address this, they propose to (i) estimate which training examples are supporting overly strong (possibly spurious) features using a datamodel-style influence approximation, and then (ii) re-train with a sample-weighted ERM that down-weights examples tied to the strongest features. This procedure, termed Feature Strength Regularization (FSR), improves worst-group / OOD accuracy on several standard robust/OOD benchmarks, often without requiring group labels."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper pinpoints the optimization bias toward strong-but-spurious features, which is intuitive and empirically plausible (supported by synthetic experiments)\n- The final training objective is a weighted ERM, which is easy to implement\n- The proposed FSR method does not rely on group labels."}, "weaknesses": {"value": "- This paper focuses on the cases where the dataset contains many strong-but-spurious features. What if the invariant features are just slightly stronger than the invariant features? Will FSR hurt the OOD performance in this case since it would down-weights the invariant features?\n- The idea of computing feature strength based on marginal influence $I_c(k)$ and the datamodel approximation is already proposed in Khaddaj et al. (2023). This reduces the novelty of FSR.\n- To my understanding, the sample reweighing scheme in FSR can only tackle certain type of covariate shifts. The capability boundary of FSR is not clearly delineated. The authors may consider the simple two-bit-env setup used in \"Does Invariant Risk Minimization Capture Invariance?\" and \"Pareto Invariant Risk Minimization: Towards Mitigating the Optimization Dilemma in Out-of-Distribution Generalization\", which studies the capability boundary of IRMv1.\n- The experimental improvement is marginal in several benchmarks\n\nMinors:\n- Equation (2) contains typos"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tlgXHXQ8DZ", "forum": "PfG6piT7ME", "replyto": "PfG6piT7ME", "signatures": ["ICLR.cc/2026/Conference/Submission8132/Reviewer_mN5m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8132/Reviewer_mN5m"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8132/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015912136, "cdate": 1762015912136, "tmdate": 1762920104381, "mdate": 1762920104381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}