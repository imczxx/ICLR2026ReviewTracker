{"id": "VIYNWGb3TL", "number": 10860, "cdate": 1758183575983, "mdate": 1763577351266, "content": {"title": "DistillMoE: Multi-Faceted Knowledge Distillation for Cross-Tokenizer Embedding Models", "abstract": "Cross-Tokenizer Knowledge Distillation for Large Language Models (LLMs), embedding models present significant challenges, primarily due to tokenizer mismatches and limitations of traditional distillation frameworks in capturing the diverse semantic signals encoded by the teacher. We propose DistillMoE, a framework that addresses these challenges through a dual-level strategy. At the sequence level, DistillMoE employs a lightweight Mixture-of-Experts module to distill sentence representations, where each expert specializes in a distinct semantic perspective: pointwise, contrastive, and pairwise. A trainable router assigns inputs to experts, letting each objective be optimized separately, thus enabling seamless integration of diverse losses without heavy tuning. At the token level, we introduce DynamicCKA to align teacher–student hidden states for fine-grained knowledge transfer. This refinement yields teacher-aware sentence embeddings, enabling the MoE to assign more informative expert weightings and enhance multi-faceted distillation. Empirically, when distilling state-of-the-art text embedding models (e.g., LLM2Vec, BGE-M3, Qwen3) into a compact BERT base student, DistillMoE consistently outperforms prior CTKD baselines across multiple datasets. These results demonstrate the effectiveness of combining multi-perspective sequence-level distillation with token-level alignment to obtain compact yet high-fidelity embedding models.", "tldr": "We propose DistillMoE, combining a lightweight Mixture-of-Experts for multi-faceted sequence-level distillation with DynamicCKA token alignment, enabling richer and more effective cross-tokenizer embedding transfer.", "keywords": ["Cross-Tokenizer Knowledge Distillation", "Embedding Models", "Mixture of Experts"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1df5aa8c73df118302dbccc5e5c1f912c44216c7.pdf", "supplementary_material": "/attachment/e37c2b010391d05d88677df4d6cb81059c8bede4.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DistillMoE for Cross-Tokenizer Knowledge Distillation (CTKD) specifically designed for distilling large embedding models (like LLM2Vec, BGE-M3, Qwen3) into smaller students like BERT-base. In order to address the challenge of transferring knowledge between models with different tokenizers and vocabularies, DistillMoE proposes a dual-level solution. At the sequence level, a lightweight Mixture-of-Experts (MoE) module is designed to capture distinct semantic facets of the teacher's knowledge: pointwise alignment (cosine loss), contrastive alignment (InfoNCE loss), and pairwise relation preservation (ranking loss); at the token level, a DynamicCKA module is introduced to align the teacher and student hidden states for the knowledge transfer. Extensive experiments on text classification, sentence pair classification, and semantic textual similarity (STS) tasks demonstrate that DistillMoE consistently outperforms existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed dual-level design is new, as well as the idea of using MoE for multi-faceted knowledge transfer\n* The empirical evaluation is extensive, and the performance gains are substantial and consistent across a wide range of tasks and teacher models\n* The paper is well-structured and clearly written"}, "weaknesses": {"value": "* The computational overhead during training is reported in Appendix D (Table 7). It would be appreciated if the authors could provide more discussion on optimizing DynamicCKA\n* Although MoE dynamically weights different losses, the overall framework still introduces new hyperparameters alpha and lambda. It would be helpful if the authors could provide more experimental results of different values"}, "questions": {"value": "1. In practice, how often does the gating network produce a near-one-hot allocation vs. a more balanced soft distribution?\n2. As the DynamicCKA module adds significant training cost, have you explored any simpler or more efficient token-level alignment baselines?\n3. Apart from different teacher models, how does DistillMoE scale when the student model is even smaller?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0SWYnFL47a", "forum": "VIYNWGb3TL", "replyto": "VIYNWGb3TL", "signatures": ["ICLR.cc/2026/Conference/Submission10860/Reviewer_UjXq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10860/Reviewer_UjXq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761200338262, "cdate": 1761200338262, "tmdate": 1762922076106, "mdate": 1762922076106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DistillMoE, which combines a token-level alignment loss (called DynamicCKA) with sequence-level auxiliary losses (implemented as three different \"experts\"), which are weighted via a MoE-style router. The authors show this alleviates some problems with manually set weights as the weighting is learned via the router. Empirically, the authors show their method outperforms various baselines on text classification, sentence pair classification and semantic textual similarity tasks when distilling a 7B LLM2Vec model into a 110M Bert-base student."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The paper proposes a new token alignment method for cross-tokenizer distillation (DynamicCKA) \n\nS2: The paper proposes to use a MoE-style setup to dynamically combine different auxiliary distillation losses without manually specifying their weights"}, "weaknesses": {"value": "W1: the proposed method is only evaluated using a single, relatively small bert-base model.\n  - does the proposed method also work for causal language models?\n  - do you have an intuition about the scaling behavior of the proposed method to larger models?\n  - What effect do different degrees of tokenizer difference have?\n\nAlso see raised questions. The low score is primarily due to open questions regarding hyper-parameter tuning, missing baselines, limited evaluation and some methodology details which are not clear to me. I will raise the score if these are appropriately addressed."}, "questions": {"value": "- Q1: how does the proposed method compare to the Approximate Likelihood Matching proposed by Minixhofer et al., 2025 (https://arxiv.org/abs/2503.20083)? This seems like a highly relevant baseline method and related work.\n- Q2: l. 252ff: I'm not sure I understood how the $\\alpha_{p,q}$ is constructed. Here's my understanding: We're normalizing the cosine sim of each student and teacher token by the other cosine similarities of that student token with all other teacher tokens. Then we pick - from **ALL teacher tokens in the entire sequence** - those that have the highest cosine similarities with that student token and construct the alignment that way. Is this correct? \n  - Do you have qualitative analysis of these alignments? A priori, there seems to be a large potential for noise in the proposed token alignment mechanism.\n  - How is the projection Q trained?\n\n- Q3: l. 133ff (Section 3.1.2) \n  - how are the projection matrices W for Expert 1 and Expert 2 trained?\n  - how is the sentence embedding for the Bert base student calculated (mean / special token / something else?)\n\n- Q4: \"hyperparameter tuning\" -- the $\\alpha$ and $\\lambda$ hyper parameters are tuned for each target dataset. The optimal values show a large variability per setting (particularly $\\alpha$).\n  - were hyper-parameters for the baselines tuned similarly?\n  - which data splits were used for hyper-parameter tuning?\n\n- Q5: some of the results in Table 2 are quite close. Could you show error bars over multiple runs to quantify the variance in the evaluated methods?\n\nsuggestion: the ablations in Table 12 are very important in my opinion, this and a discussion of these should be moved to the main body if possible given an extra page. Your work proposes a whole stack of new methods and it is very important to discern which parts of the proposed full method are most important."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eSsPx3xquA", "forum": "VIYNWGb3TL", "replyto": "VIYNWGb3TL", "signatures": ["ICLR.cc/2026/Conference/Submission10860/Reviewer_tnSP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10860/Reviewer_tnSP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658549024, "cdate": 1761658549024, "tmdate": 1762922075672, "mdate": 1762922075672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DistillMoE, a distillation strategy with two components. At the sequence level, the authors propose a a mixture of three experts, which are distilled with a cosine, contrastive, and pariwise loss to capture different aspects of teacher knowledge. For tokens, the authors propose DynamicCKA to address tokenizer mismatch."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The author proposes quite a few techniques.\n2. Authors show consistent improvements over baselines."}, "weaknesses": {"value": "1. The selection of tasks is a bit unusual. It would be nice to see how this method works with instruction tuning and reasoning distillation.\n2. The paper attempts to address two separate problems. One being knowledge transfer loss and the other being tokenizer mismatch. As a result, I feel the authors don't study either very in depth. For example, there are existing methods that just focus on the tokenizer mismatch problem, and the paper can benefit from comparing with some baseline approaches."}, "questions": {"value": "For comparisons in Table 1 (for example), does DistillMoE use extra parameters because of the MoE structure vs other competing methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wxP0fC6HhL", "forum": "VIYNWGb3TL", "replyto": "VIYNWGb3TL", "signatures": ["ICLR.cc/2026/Conference/Submission10860/Reviewer_Dptj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10860/Reviewer_Dptj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718991051, "cdate": 1761718991051, "tmdate": 1762922075270, "mdate": 1762922075270, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DistillMoE, specifically designed to solve the \"cross-tokenizer\" problem. This problem arises when a small \"student\" model needs to learn from a large \"teacher\" model, but the two models use different vocabularies (tokenizers), making direct knowledge transfer difficult. To handle the tokenizer mismatch, the paper proposes DynamicCKA. This method aligns the hidden states of the teacher and student models. At the sentence level, a lightweight MoE module learns the teacher’s knowledge: pointwise for semantics, contrastive for geometry, and pairwise for sentence relations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: The novelty lies in repurposing the MoE architecture—not just for capacity or efficiency, but as a tool for knowledge transfer. By assigning different semantic objectives (pointwise, contrastive, pairwise) to separate experts, the authors create a structured framework that captures diverse aspects of the teacher model’s representations.\n\nS2: The authors provide theoretical justification for the behavior of the gating mechanism, lending credibility to their architectural choices. The authors evaluate their method on a set of tasks (STS, text classification, sentence-pair classification), demonstrating the general applicability of their framework. They also compare against a strong and comprehensive suite of recent state-of-the-art CTKD baselines."}, "weaknesses": {"value": "W1: The most significant weakness of DistillMoE is the substantial computational overhead introduced during training, which is not sufficiently justified in the context of creating an efficient student model. Moreover, the authors should discussed it in the main paper.\n\nW2: The paper's choice of the three expert objectives (pointwise, contrastive, pairwise) is intuitively appealing but lacks a strong theoretical foundation. It feels more like a well-engineered recipe than a principled decomposition of knowledge. \n\nW3: The experiments, while comprehensive on standard benchmarks, are confined to an in-domain setting and fail to measure two critical aspects of a distilled model's quality: generalization to unseen domains and actual inference efficiency. All models are trained and evaluated on splits of the same datasets (e.g., trained on SciTail train set, tested on SciTail test set). The paper claims to create a powerful, general-purpose embedding model, but provides no evidence of its out-of-domain (OOD) generalization capabilities. Furthermore, no inference speed or latency metrics are reported."}, "questions": {"value": "Q1: The paper provides a good description of what each expert does, but not why these three perspectives are the necessary and sufficient set for capturing the teacher's knowledge."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iBnLFsiNh1", "forum": "VIYNWGb3TL", "replyto": "VIYNWGb3TL", "signatures": ["ICLR.cc/2026/Conference/Submission10860/Reviewer_a3RJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10860/Reviewer_a3RJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10860/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996757252, "cdate": 1761996757252, "tmdate": 1762922074453, "mdate": 1762922074453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}