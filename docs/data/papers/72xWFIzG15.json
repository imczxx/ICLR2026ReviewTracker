{"id": "72xWFIzG15", "number": 18324, "cdate": 1758286416408, "mdate": 1759897110898, "content": {"title": "HiFi-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation", "abstract": "Recent advances in video generation produce visually realistic content, yet the absence of synchronized audio severely compromises immersion. To address key challenges in video-to-audio generation, including multimodal data scarcity, modal semantic response imbalance, and limited audio quality in existing methods, we propose HiFi-Foley, an end-to-end text-video-to-audio framework that synthesizes high-fidelity audio precisely aligned with visual dynamics and semantic context. Our approach incorporates three core innovations: (1) a novel multimodal diffusion transformer that addresses semantic response imbalance between video and text modalities through dual-stream audio-video fusion via joint attention and balanced textual semantic injection via cross-attention; (2) a representation alignment training strategy that employs self-supervised audio features to guide latent diffusion training, thereby improving audio quality and semantic consistency; (3) a scalable data pipeline leveraging open-source tools for cleaning raw data and constructing training datasets. Extensive evaluations demonstrate that HiFi-Foley achieves state-of-the-art performance across audio fidelity, visual-semantic alignment, temporal alignment, and distribution matching.", "tldr": "HiFi-Foley is a novel Text-Video-to-Audio model that generates high-quality, perfectly synced sound for videos, outperforming SOTA methods.", "keywords": ["Video-to-audio", "Audio Generation", "Foley Generation", "Multi-modal", "Representation Alignment"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fbc4be4ab2884e81f11b21e6a5a66b0227a4c814.pdf", "supplementary_material": "/attachment/e9dd71524d7744e04759245837140387380f40d6.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents HiFi-Foley, a text-video-to-audio (TV2A) generation model that synthesizes high-fidelity and semantically aligned audio from multimodal inputs.\nThe model introduces:\n\n1. Dual-phase attention in a multimodal diffusion transformer (MMDiT) — using joint self-attention for video-audio alignment and cross-attention for text injection.\n\n2. A Representation Alignment (REPA) strategy — aligning DiT hidden states with pre-trained ATST audio features to improve semantic and acoustic fidelity.\n\n3. A scalable 122k-hour TV2A dataset built using an open-source cleaning and labeling pipeline.\n\nIn summary, I believe this paper is a boardline paper, it includes a lot of experiments and engineering efforts. But the novelty is limited to a certain degree."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper integrates modern components (MMDiT, flow matching, REPA) coherently into a unified architecture. The dual-phase attention (joint self-attention + cross-attention) for modality balancing is conceptually sound and clearly motivated. \n\n2. Implementation details (architecture, loss, datasets, training setups) are thorough. The 122k-hour data pipeline is well-engineered and valuable to the community (if the code and data are open-sourced)\n\n3. The paper compares against strong baselines (MMAudio, FoleyCrafter, ThinkSound, etc.) with both objective and subjective metrics. Ablations on attention design, interleaved RoPE, and REPA placement are also provided."}, "weaknesses": {"value": "1. The architecture integrates existing ideas: The dual-phase attention essentially decomposes joint attention (as used in MMAudio) into two stages — an incremental modification, not a conceptual innovation. The REPA alignment is directly adapted from prior visual generative works. Although these integrations are good for the video-to-audio community, the contribution is more engineering refinement.\n\n2. The 122k-hour dataset construction is technically significant, but I am not sure such construction whether novel compared to previous multimodal filtering pipelines."}, "questions": {"value": "whether the 122k-hour dataset construction will be open-source?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kBljbs58YL", "forum": "72xWFIzG15", "replyto": "72xWFIzG15", "signatures": ["ICLR.cc/2026/Conference/Submission18324/Reviewer_9F1o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18324/Reviewer_9F1o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885540877, "cdate": 1761885540877, "tmdate": 1762928040641, "mdate": 1762928040641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HiFi-Foley, an end-to-end framework for generating high-fidelity Foley audio from both video and text prompts. Authors proposed a novel multimodal diffusion transformer architecture using a dual-phase attention mechanism to process audio-video latents. In addition, a REPA loss is used to improve audio quality and semantic consistency. Moreover, this paper introduces a large-scale audio-video data, solving the data scarcity problem. Extensive experiments show that the proposed method achieves a state-of-the-art performance on various benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Use of REPA loss: using a pre-trained audio encoder as a \"teacher\" to guide the diffusion model's internal feature space is a clever way to distill rich, general-purpose acoustic knowledge into the generative process.\n2. Large-scale data curation: a large-scale high-quality audio-video dataset is constructed with advance data filtering, preprocessing, and annotation. \n3. Impressive performance on three benchmarks across various evaluation metrics."}, "weaknesses": {"value": "1. The dataset is not released. \n2. The used REPA is not a new stuff and has been introduced in image generation. \n3. Limited model novelty: the proposed model structure is similar to MMAudio. \n4. Based on Table 5, REPA has the limited improvement on the performance. And also adding the unimodal DiT has the limited improvements while introducing more model parameters \n5. The addition of REPA aims to improve the audio quality and alignment, why it also benefits the DeSync?\n6. Which datasets are for training, which ones are for testing?\n7. Why not give the ground truth reference in Figure 5-6? Otherwise it’s hard to compare different methods."}, "questions": {"value": "1. Whether the dataset will be released?\n2. What are differences between the proposed model structure and MMAudio? \n3. REPA and unimodal DiT have the limited improvement on the performance. Did you try different representation loss specifically designed for video-to-audio task?\n4. Did you compare the model performance between (text+video)-to-audio and video-to-audio variants? \n5. The description of training and testing datasets should be more clear. \n6. The ground truth audio references are not shown in Figure 5-6."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KppYVWiS4A", "forum": "72xWFIzG15", "replyto": "72xWFIzG15", "signatures": ["ICLR.cc/2026/Conference/Submission18324/Reviewer_t92j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18324/Reviewer_t92j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887024797, "cdate": 1761887024797, "tmdate": 1762928039944, "mdate": 1762928039944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HiFi-Foley, an end-to-end framework for high-fidelity text-video-to-audio (TV2A) generation. The authors identify three key challenges in existing methods: multimodal data scarcity, \"modality imbalance\" (where models over-rely on text cues and ignore video), and low audio quality. To address these issues, HiFi-Foley proposes three main contributions: A novel architecture, a new training strategy, and a scalable data pipeline. The authors also developed a custom, high-fidelity DAC-VAE for audio encoding/decoding. Evaluations on Kling-Audio-Eval, VGGSound-Test, and MovieGen-Audio-Bench show that HiFi-Foley achieves state-of-the-art results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The dual-phase attention mechanism is a well-motivated and intuitive solution to the stated problem of text-over-reliance. Separating the fine-grained A-V temporal alignment (via self-attention) from the global text conditioning (via cross-attention) is a strong architectural contribution, and the ablations in Table 5 confirm its effectiveness.\n\n* The idea of interleaving audio and visual tokens before applying ROPE is a simple but clever technique to enforce fine-grained temporal correlation between the two modalities. The ablation study shows this provides a clear benefit over conventional ROPE.\n\n* Applying the REPA concept to align with a pre-trained audio model (ATST-Frame) is a logical and successful strategy. The ablations clearly demonstrate that this improves performance and that the choice of the guide model (ATST) and its application layer (unimodal block 8) are important."}, "weaknesses": {"value": "* In Section 4.2, the paper states that on VGGSound-Test, HiFi-Foley \"leads in audio quality metrics (IS, PQ)\". However, Table 3 clearly shows its IS score (16.14) is significantly worse than MMAudio's (21.00). This is a factual error.\n\n* The comparison to ThinkSound is explicitly handicapped. The authors state they \"only evaluate the version without Chain-of-Thought (CoT) instructions\". This means a core component of the baseline is missing, making the comparison unfair and the reported improvements potentially misleading.\n\n* The data pipeline is a key contribution, but critical details for reproducibility are missing. The paper states an \"empirically design a standard\" was used for filtering based on PQ, SNR, ImageBind, and AV-align, but the actual numerical thresholds are not provided.\n\n* The model cannot generate intelligible speech. This is a major limitation, especially since \"Human voice\" (32.75%) is the single largest category in the new training dataset (Figure 4). The paper does not adequately explain this significant failure mode."}, "questions": {"value": "* Why does the text in Section 4.2 claim the model \"leads in... IS\" on VGGSound-Test, when Table 3 shows it is significantly outperformed by MMAudio (16.14 vs 21.00)?\n\n* Given that 32.75% of your training data is \"Human voice\" , why does the model completely fail to produce intelligible speech? Is this a limitation of the DAC-VAE, a bias in the GenAU captions (e.g., \"speech\" vs. actual transcripts), or an artifact of the REPA/flow-matching objectives?\n\n* The \"high-quality\" tag is always appended at inference. What happens if this tag is not used? Does this technique harm performance when trying to generate sounds that are naturally low-bandwidth (e.g., a distant rumble)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "doB3WOyUDF", "forum": "72xWFIzG15", "replyto": "72xWFIzG15", "signatures": ["ICLR.cc/2026/Conference/Submission18324/Reviewer_AKcK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18324/Reviewer_AKcK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987978258, "cdate": 1761987978258, "tmdate": 1762928038967, "mdate": 1762928038967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HiFi-Foley, an end-to-end text-video-to-audio framework that synthesizes high-fidelity audio precisely aligned with visual dynamics and semantic context.\n\nThe main paper contributions are:\n- a novel multimodal diffusion transformer that addresses semantic response imbalance between video and text modalities through dual-stream audio-video fusion via joint attention and balanced textual semantic injection via cross-attention.\n- a representation alignment training strategy that employs self-supervised audio features to guide latent diffusion training, thereby improving audio quality and semantic consistency.\n- a scalable data pipeline leveraging open-source tools for cleaning raw data and constructing training datasets.\n\nExtensive evaluations demonstrate that HiFi-Foley achieves state-of-the-art performance across audio fidelity, visual semantic alignment, temporal alignment, and distribution matching."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The main strength of the paper is the data curation pipeline designed to build a high quality training dataset for the video to audio generation task."}, "weaknesses": {"value": "I believe that the proposed contributions lack novelty or significance.\n- Injecting text via cross attention in video to audio generation has been proposed before (MovieGen). Moreover, the motivation for this architecture design is unconvincing. The imbalance between conditioning signals can usually be addressed by employing different guidance weights during inference, which is not considered in this paper (at least as a baseline). The paper does not even mention the inference parameters used in the experiments.\n- The REPA loss has been introduced in prior works and this paper just applies it to a new task.\n- The proposed data curation pipeline is mostly descriptive and the resulting dataset is not published. The size of the resulting curated dataset is one order of magnitude than the biggest non curated open source dataset (AudioSet), which is probably the main explanation for the appealing results of the Tables 2 and 4. Moreover, the curation pipeline is undocumented and thus non reproducible. For example, no extensive description of the thresholds used at the different filtering steps are given.\n\nThe results presented in the tables, which do not report confidence intervals, yield minimal differences between the different methods (such as the Tables 5, 6 and 7). Thus they are unconvincing to the reader. For what it is worth, according to my experience, absolute variations of less than 0.2 in A4 scores are not significant. \n\nThe Figure 2 provides the same information as the Tables 2, 3, 4."}, "questions": {"value": "What is the \"HunyuanVideo-Foley\" model mentioned in the Figure 2 caption?\nWhat is the ATST-Frame model mentioned throughout the paper?\nWhat is the parallel cross attention ablation in the Table 5? A figure would be welcome."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "opGqpci01m", "forum": "72xWFIzG15", "replyto": "72xWFIzG15", "signatures": ["ICLR.cc/2026/Conference/Submission18324/Reviewer_zgNX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18324/Reviewer_zgNX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18324/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066261782, "cdate": 1762066261782, "tmdate": 1762928038463, "mdate": 1762928038463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}