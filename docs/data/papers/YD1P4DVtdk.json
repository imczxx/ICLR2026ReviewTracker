{"id": "YD1P4DVtdk", "number": 21629, "cdate": 1758319854449, "mdate": 1763740566113, "content": {"title": "Mechanistic Interpretability as Statistical Estimation: A Variance Analysis of EAP-IG", "abstract": "The development of trustworthy artificial intelligence requires moving beyond black-box performance metrics toward an understanding of models' internal computations. Mechanistic Interpretability (MI) aims to meet this need by identifying the algorithmic mechanisms underlying model behaviors. Yet, the scientific rigor of MI critically depends on the reliability of its findings. In this work, we argue that interpretability methods, such as circuit discovery, should be viewed as statistical estimators, subject to questions of variance and robustness. To illustrate this statistical framing, we present a systematic stability analysis of a state-of-the-art circuit discovery method: EAP-IG. We evaluate its variance and robustness through a comprehensive suite of controlled perturbations, including input resampling, prompt paraphrasing, hyperparameter variation, and injected noise within the causal analysis itself. Across a diverse set of models and tasks, our results demonstrate that EAP-IG exhibits high structural variance and sensitivity to hyperparameters, questioning the stability of its findings. Based on these results, we offer a set of best-practice recommendations for the field, advocating for the routine reporting of stability metrics to promote a more rigorous and statistically grounded science of interpretability.", "tldr": "", "keywords": ["mechanistic interpretability", "statistical inference", "eap-ig", "variance", "robustness", "stability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa7bbe9841b6b37088f6f852321ca361ef5f1224.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a systematic analysis of variance and robustness in circuit discovery methods. The authors investigate circuit faithfulness, variance and robustness under a comprehensive set of controlled perturbations, including input resampling, prompt paraphrasing, hyperparameter variation, and noise injection within the causal analysis itself. Their results show that EAP-IG exhibits substantial structural variance and strong sensitivity to hyperparameter choices, highlighting the need for more statistically grounded and reliable evaluation practices in mechanistic interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The authors highlight critical issues of robustness and stability that were not discussed in the original EAP or EAP-IG papers. \n* They explore a wide range of useful setups, including data resampling, prompt paraphrasing, hyperparameter variation, and noise injection"}, "weaknesses": {"value": "If I understand correctly, the circuit size is defined as the minimal connected circuit that can be identified (Section 3.1). However, the circuits presented in Table 2 generally exhibit high error rates, suggesting that the analysis primarily focuses on largely unfaithful circuits. I’m not entirely sure I follow the motivation for this choice. It would seem more natural to analyze circuits that achieve error rates below a specified threshold, and then compare the circuit sizes required to reach various faithfulness levels. In addition, comparing circuit error while ignoring circuit size can be misleading (Table 1, Figure 3), as smaller circuits will naturally tend to have higher error. It is also possible that variance between discovery methods decreases as circuits become more faithful."}, "questions": {"value": "My main concerns are described in the Weaknesses section. If I misunderstood something, I’d be happy to reconsider my score.\n1. I think your analysis would be much stronger if you examined circuits of different sizes and faithfulness scores.Comparing circuits of different sizes (Table 2) seems problematic because smaller circuits naturally have higher error, so it's hard to tell what's driving the differences you're seeing.\nI understand you can't always construct connected circuits of arbitrary sizes, but I still think it would be valuable to analyze trends across the sizes you can get. Following the approach in prior work [1, 2] (plotting results as a function of circuit size) would give much clearer insight into the trade-offs between faithfulness, size, and robustness. It would also help address the question of whether these instabilities persist in more faithful circuits, or if they primarily affect the less faithful ones you’re currently analyzing.\n\n2. If I understood correctly D_eval=D_discovery.  could you provide references to support this choice?\n\nReferences:\n[1] Have Faith in Faithfulness: Going Beyond Circuit Overlap\nWhen Finding Model Mechanisms https://arxiv.org/pdf/2403.17806\n[2] MIB: A Mechanistic Interpretability Benchmark https://arxiv.org/pdf/2504.13151"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VqFWPEFweG", "forum": "YD1P4DVtdk", "replyto": "YD1P4DVtdk", "signatures": ["ICLR.cc/2026/Conference/Submission21629/Reviewer_4NAY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21629/Reviewer_4NAY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761535201826, "cdate": 1761535201826, "tmdate": 1762941861174, "mdate": 1762941861174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes several metrics to quantify the uncertainty of circuit discovery methods. A case study is then performed on different variants of EAP (edge attribution pruning) across different models (GPT2, Llama3.2 1B). The results show that EAP recovered circuits exhibits both high structural and error variance. They also show that EAP is sensitive to noise and hyperparameters. \nDriven by these results, the authors then propose circuit discovery best practices for the broader mechanistic interpretability (MI) community including: reporting stability metrics (that they propose), justifying hyperparameters, robustness checks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, the paper is well-written, it addresses exigent issues which I believe would be of interest to the MI community. The metrics proposed are somewhat reasonable and the methods/datasets/models benchmarked are, to my knowledge, quite standard. Concretely, \n- [Lines 250-256]: Standard metrics of faithfulness like KL/0-1 loss is used as Jaccard similarity to compute circuit structural similarities. \n- [Lines 263-279]: Quite thorough in terms of the types of perturbations that are being made in terms of the method, algorithmic hyperparameters, and input distribution shift\n- [Lines 305-311]: There is a variety both in terms of model size like the use of GPT2 versus Llama 1B as well as variation in post-training methods Llama 1B versus Llama 1B-instruct.\n- [Lines 350-]: Interesting to see that stability decreases with model size, this could have implications for how well MI methods generalize with size. The finding that instruction tuning does not affect stability is novel."}, "weaknesses": {"value": "There are several issues with the paper in its current form.\n### Major Issues\n* It is unclear to me how the proposed stability metrics would be informative without an identifiability assumption over circuits. For example, if there are two distinct circuits that can separately recover performance, their difference would form a potential lowerbound on the variance of the Jaccard metric. \n\t* It's not obvious to me that this identifiability assumption can be made, especially in larger models (see McGrath et al. 2023; Rushing and Nanda 2024 on self-repair) where a model could have parallel or complementary pathways. \n\t* Moreover, the identifiability of circuits would also depend highly on the ablation method being used which the authors are also varying [Miller et al. 2024 which the authors cite in Line 145; Zhang and Nanda 2024 which the authors cite in Line 146; also Lines  278-279]. In this way, it's unclear to me that when we vary the ablation methodology or shift the input data distribution the resulting circuits are comparable. \n* The results do not conclusively show that instability is necessarily coming from EAP or its variants (claimed in Lines 21-23). Specifically, there are two distinct processes: component scoring which is essentially just estimating an expected value [Sun 2025]; and, circuit identification. \n\t* Most of the instability seems to be an artifact of the latter discretization and the potential discrete choice of metric (Jaccard). \n\t* Perhaps it would be better to analyze these processes separately (i.e. analyze the stability of the component scoring; then check how much additional variance is introduced through the discretization). \n* The circuit error across many the many settings the authors study is extremely high (in Figure 2: ~0.5 for GPT2 Greater-Than Bootstrap; ~0.5 for Llama IOI, ~0.75 for Llama SVA). This suggests that the authors did not well-calibrate EAP. For example,\n\t* Take an extreme: if we allow EAP to return \"circuits\" with circuit error 1, then any sub-computational graph could be a \"valid\" circuit and this notion of stability would be vacuous. \n\t* It is more intuitive to me that for a *fixed* circuit error rate, we compare the structural variance in returned circuits [then perhaps this could be formulated as estimating the diameter of the solution set of the optimization problem defined by EAP; Bhasker et al. 2025].\n\t* The circuit error is not held to be fixed or minimized makes it very unclear to me what stability is computed relative to. \n### Minor Issues\n- Details in the experimental setup sections are lacking (please see some of my questions below).  \n\n**References**\n\n[The Hydra Effect: Emergent Self-Repair in Language Model Computations](https://arxiv.org/abs/2307.15771). McGrath et al. 2023\n\n[Explorations of Self-Repair in Language Models](https://arxiv.org/abs/2402.15390). Rushing et al. 2024\n\n[Toward Best Practices for Activation Patching in Language Models: Metrics and Methods](https://arxiv.org/pdf/2309.16042). Zhang and Nanda 2024\n\n[Transformer Circuit Faithfulness Metrics Are Not Robust](https://openreview.net/pdf?id=zSf8PJyQb2). Miller et al. 2024\n\n[Circuit Stability Characterizes Language Model Generalization](https://aclanthology.org/2025.acl-long.442/). Sun 2025\n\n[Finding Transformer Circuits with Edge Pruning](https://arxiv.org/pdf/2406.16778). Bhasker et al. 2025"}, "questions": {"value": "- Can the authors comment in more detail on how their contribution builds upon the work of Shi et al. (2024)? More specifically, one could take the hypothesis tests that they formulate and invert them to get your metrics (i.e. a confidence interval over circuit error could be constructed by inverting their \"equivalence\" test and a confidence interval over the structural variations could be constructed by inverting their \"minimality\" test). \n\n- In Lines 267-269, it is mentioned that data \"meta-distribution shifts\" are performed, can you give a concrete example of what this reprompting/paraphrase would look like in the context of the chosen tasks?\n\t- To my knowledge, the circuit for IOI is highly dependent on the structure of the prompt. How do the authors ensure that the underlying circuit does not change as a result of this distribution shift? \n\n- In Lines 270-274, experimental intervention noise is added. It is also not obvious to me what this means exactly, can the authors provide a concrete example of this? i.e. what are the \"relevant\" token embeddings?\n\n\n[Hypothesis Testing the Circuit Hypothesis in LLMs](https://arxiv.org/abs/2410.13032). Shi et al. 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cvGNP0xWgI", "forum": "YD1P4DVtdk", "replyto": "YD1P4DVtdk", "signatures": ["ICLR.cc/2026/Conference/Submission21629/Reviewer_EUfV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21629/Reviewer_EUfV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975001066, "cdate": 1761975001066, "tmdate": 1762941860849, "mdate": 1762941860849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors suggest that a critical scientific shortcoming in the current approach to mechanistic interpretability is a lack of statistical rigor about the outputs of circuit discovery methods. For example, it is common in papers making use of methods like EAP-IG to not report the effect of varying the ingredients in the inference process, such as data samples or search hyperparameters. However, without these details it is unclear how meaningful the conclusions can be.\n\nAcross three tasks (indirect object identification, subject-verb agreement and greater-than) the authors provide a demonstration of how to perform such analyses. Perhaps their primary contribution is to emphasise the importance of such analysis and to demonstrate a sensible approach to doing it, but their results are also interesting: they find that there is significant variation in the discovered circuits, with larger models showing more variation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n\n* Timely and clear message about how to improve the scientific rigor of mechanistic interpretability  \n* The writing is approachable and clear, I found it enjoyable to read  \n* Good choice of experimental tasks and models"}, "weaknesses": {"value": "Weaknesses:\n\n* The lack of attention to detail in various places makes it impossible to accept the paper in its current form. For example, there is a repeated paragraph on p.7\\! The fact that this was not caught during editing suggests to me there may be other subtle errors, including in the data presentation, and gives me significant pause."}, "questions": {"value": "I would be happy to revise my rating upward after the authors take another pass through the paper and refine the writing.\n\nAs a final note: this is not exactly a critique of the paper itself, but arguably circuit discovery in the mode of EAP and EAP-IG has just been a dead end in the field, and there is little long-term value in improving the scientific rigor of a technique that won’t be used in the future. However, I still see value in this paper since I do agree with EAP-IG as a valid case study for ideas that are more broadly applicable. However, I do feel the empirical results are of somewhat limited value outside of this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OdBNuPsli6", "forum": "YD1P4DVtdk", "replyto": "YD1P4DVtdk", "signatures": ["ICLR.cc/2026/Conference/Submission21629/Reviewer_DTor"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21629/Reviewer_DTor"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976001373, "cdate": 1761976001373, "tmdate": 1762941860618, "mdate": 1762941860618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reframes mechanistic interpretability (MI) as a problem of statistical estimation, arguing that interpretability methods should be evaluated for their variance, robustness, and reliability, just like statistical estimators. Using Edge Activation Patching with Integrated Gradients (EAP-IG) as a case study, the authors conduct experiments across multiple models and tasks to measure how circuit discovery results change under input resampling, hyperparameter variation, and injected noise. They find that EAP-IG circuits exhibit high structural variance and sensitivity, suggesting that many MI findings may be unstable or sample-dependent."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. It offers empirical evidence that circuits generated using EAP-IG exhibit high variance.\n2. It provides a novel insight that circuits discovered by EAP-IG in larger models tend to show greater error and variability compared to those in smaller models.\n3. It also presents practical recommendations for researchers working on circuit discovery methods."}, "weaknesses": {"value": "1. EAP-IG is an approximation of activation patching that relies solely on the first-order Taylor series expansion. Because higher-order terms of the Taylor series are ignored, the resulting circuits are expected to vary. Therefore, the paper’s main finding, that circuits identified by EAP-IG exhibit variance, is neither surprising nor particularly insightful.\n2. The attribution scores directly depend on the number of samples used. When only a small number of samples are employed, the results inherently exhibit higher variance. Unfortunately, this important detail is not reported in the paper.\n3. The paper begins by emphasizing the goal of making mechanistic interpretability more scientific. However, it focuses solely on circuit discovery, which is just one major area within the field. Moreover, even within circuit discovery research, it examines only the EAP-IG method, neglecting other prominent approaches such as Path Patching, Desiderata-based Component Masking, Edge Pruning, and Information Flow Routes, thereby limiting the breadth of its empirical analysis [1–4].\n4. The paper lacks a dedicated limitations section, even though several limitations are apparent throughout the work.\n5. The presentation of the paper could be significantly improved in the following ways:\n   - Line 72: The statement “For instance, circuits discovered in one setting often fail to transfer to others” is not well supported. Reference [1] only explores the indirect object identification task in GPT-2 small, while [5] reports the opposite finding, that models can reuse circuit components across tasks. The authors should justify or clarify this claim.\n   - Line 107: The abbreviation DNN is used without prior definition.\n   - Lines 130–131: The description of the “interpretability” proxy metric is incomplete and needs elaboration.\n   - Lines 270–274: The method for experimental intervention noise is insufficiently described. The authors should provide more details in Section 3.3 or include an appendix section explaining the procedure.\n   - The paper should specify which model was used to generate the results in Table 1.\n   - The result mentioned in the first paragraph of page 7, stating that “instruction tuning may not fundamentally alter the stability or discoverability of the underlying circuits”, is consistent with existing findings in the literature and should therefore be cited [6, 7].\n   - The last two paragraphs on page 7 are repetitive and should be consolidated.\n   - The caption for Figure 3 should appear below the figure, not above it.\n   - Line 430: The reference should be corrected from “Table” to “Figure.”\n\n[1] Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small, 2023.\n\n[2] Discovering Variable Binding Circuitry with Desiderata, 2023.\n\n[3] Finding Transformer Circuits with Edge Pruning, 2024.\n\n[4] Information Flow Routes: Automatically Interpreting Language Models at Scale, 2024.\n\n[5] Circuit Component Reuse Across Tasks in Transformer Language Models, 2024.\n\n[6] Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks, 2024.\n\n[7] Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking, 2024."}, "questions": {"value": "1. Section 5.1 reports that bootstrap resampling produces the highest structural variance, which seems counterintuitive. Circuits derived from data sampled from the same underlying distribution should, in principle, exhibit higher similarity than those obtained from different distributions. How do the authors explain this outcome?\n\n2. Additionally, Table 2 presents an intriguing case where two circuits have nearly identical circuit errors (0.20 vs. 0.19) but vastly different sizes (10 vs. 28) and are almost completely disjoint structurally (Jaccard similarity of 0.086). Does this finding indicate the presence of two equally valid yet distinct computational mechanisms within the model, or does it instead suggest that one circuit is valid while the other is spurious or unfaithful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TM2jCLSkdS", "forum": "YD1P4DVtdk", "replyto": "YD1P4DVtdk", "signatures": ["ICLR.cc/2026/Conference/Submission21629/Reviewer_6Yb9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21629/Reviewer_6Yb9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21629/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049071002, "cdate": 1762049071002, "tmdate": 1762941860342, "mdate": 1762941860342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}