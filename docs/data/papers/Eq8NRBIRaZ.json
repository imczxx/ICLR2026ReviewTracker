{"id": "Eq8NRBIRaZ", "number": 16447, "cdate": 1758264665177, "mdate": 1763122008859, "content": {"title": "INFUSER: Injecting Synthetic Failures for Self-Correcting Embodied Agents", "abstract": "Vision-Language Models (VLMs) have become a powerful foundation for embodied agents, which are typically fine-tuned on expert demonstrations of successful task completions. However, collecting expert demonstrations is prohibitively expensive, and additionally, training exclusively on these ideal trajectories leaves agents brittle and struggle to recover from inevitable errors. To address this issue, we introduce INFUSER, INjecting synthetic FailUre for Self-correcting Embodied agent. Our idea is to augment existing expert trajectories with automatically generated failure-and-recovery scenarios (i.e., no human cost),\nrather than collecting additional (costly) expert demonstrations. Specifically, we synthesize these data by injecting suboptimal actions into ground-truth paths, creating a diverse set of controlled failure scenarios. By fine-tuning on this augmented dataset, INFUSER learns to take corrective actions and recover from mistakes. Our experiments validate the effectiveness of INFUSER through comprehensive evaluations on benchmarks for embodied agents including EB-ALFRED and EB-Habitat;\ntraining the Qwen2.5-VL-7B model by augmenting with our synthetic failure-tolerant data improves its performance by 18.3\\% $\\rightarrow$ 47.0\\% and 59.7\\% $\\rightarrow$ 66.3\\% on EB-ALFRED and EB-Habitat, respectively, achieving state-of-the-art performance among open-source models and even surpassing Qwen2.5-VL-72B with 10× fewer parameters. These results demonstrate that learning to recover from failures through synthetic augmentation, rather than collecting additional expert demonstrations, is a cost-effective approach to building robust embodied agents.", "tldr": "", "keywords": ["embodied agent", "reasoning", "high-level planning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/47cad582e759fa78df6c6273a65762e5b01a75ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces INFUSER, a framework designed to enhance embodied agents through synthetic failure recovery data. The synthetic data are generated by injecting suboptimal actions into ground-truth trajectories. By fine-tuning the model with both the synthetic failure recovery data and the original successful trajectories, the framework could improve agent performance. Experiments conducted on EB-ALFRED, EB-Habitat, VAB-OmniGibson, and VAB-Minecraft demonstrate consistent performance gains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tFailure recovery is an important capability that an embodied agent should possess. This paper presents a useful framework for generating such data and enhancing the performance of small-scale models.\n\n2.\tThe writing is clear and easy to follow.\n\n3.\tThe experiments cover multiple benchmarks, demonstrating the generality of the approach."}, "weaknesses": {"value": "1.\tThe proposed method appears to primarily target small models with limited performance through domain-specific fine-tuning. It would be beneficial to demonstrate how failure recovery contributes when the base failure rate is relatively low.\n\n2.\tThe method relies on domain-specific fine-tuning, which limits its applicability to strong proprietary models. Moreover, this setting makes the comparison with zero-shot models somewhat unfair.\n\n3.\tAs shown in Table 2, the performance gain on EB-Habitat from using failure recovery data is quite limited compared to the large improvements observed on EB-ALFRED. It would be helpful to discuss the reason behind this discrepancy and clarify the effective scope of failure recovery data. Additionally, including an ablation study (similar to Table 4) would strengthen the analysis of the effectiveness of failure recovery trajectories.\n\n4.\t(Minor) Figure 3 contains noticeable formatting errors."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "olvjn1TgNZ", "forum": "Eq8NRBIRaZ", "replyto": "Eq8NRBIRaZ", "signatures": ["ICLR.cc/2026/Conference/Submission16447/Reviewer_DoXq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16447/Reviewer_DoXq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760698264271, "cdate": 1760698264271, "tmdate": 1762926560014, "mdate": 1762926560014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "j3mPowGiUE", "forum": "Eq8NRBIRaZ", "replyto": "Eq8NRBIRaZ", "signatures": ["ICLR.cc/2026/Conference/Submission16447/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16447/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763121666258, "cdate": 1763121666258, "tmdate": 1763121666258, "mdate": 1763121666258, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces INFUSER, a method for training failure-tolerant embodied agents by augmenting expert demonstrations with synthetically generated failure-recovery trajectories. The key insight is using an LLM to inject plausible failure actions into expert trajectories and generate corresponding recovery reasoning. The authors then fine-tune VLMs on this augmented dataset using teacher-forcing on expert recovery actions. Evaluated on EmbodiedBench and VisualAgentBench, INFUSER improves success rates and achieves the highest recovery rate."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses a critical problem: Trains agents to recover from failures rather than just avoid them, which is essential for real-world deployment.\n\n2. Comprehensive evaluation: Validates the approach across multiple benchmarks (EB-ALFRED, EB-Habitat, VAB-OmniGibson, VAB-Minecraft) with detailed error persistence analysis."}, "weaknesses": {"value": "1. Unclear environment interaction during data generation: Equation (3) describes o_t_fail​​ as the observation before generating \na_t_fail​​, while the text below Equation (4) refers to it as \"resulting observation.\" If the latter is correct, does it mean o_t_fail​+1​? This raises the question of whether environment interaction is required during dataset construction, which is not explicitly stated. \n\n2. Lack of motivation for training scheme: The paper does not explain why teacher-forcing with expert recovery actions is necessary. Why not simply train the agent to return to previous states? Comparisons with simpler alternatives and in-depth justification are missing, making the experimental section read like a list of results rather than providing insights.\n\n3. Insufficient baseline comparisons: The performance gains are unsurprising given the custom dataset. If environment interaction is needed for data generation (see weakness 1), why not directly fine-tune VLMs in the environment? More importantly, various dataset construction methods are possible (see weakness 2), and ablations comparing these alternatives are critically needed.\n\n4. (Minor) Figure 3 caption formatting issue: The caption placement appears incorrect."}, "questions": {"value": "Please see weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q6rj4IDphH", "forum": "Eq8NRBIRaZ", "replyto": "Eq8NRBIRaZ", "signatures": ["ICLR.cc/2026/Conference/Submission16447/Reviewer_sWCg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16447/Reviewer_sWCg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761429693719, "cdate": 1761429693719, "tmdate": 1762926559457, "mdate": 1762926559457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces INFUSER, a framework for training embodied agents to recover from failures by augmenting expert demonstrations with synthetically generated failure-and-recovery trajectories. The core idea is to inject suboptimal actions into ground-truth paths and train agents to recognize and correct these failures. The approach achieves significant improvements on EmbodiedBench and VisualAgentBench benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper addresses a critical limitation in current embodied agent training - the lack of failure recovery capabilities when agents are trained exclusively on successful demonstrations.\n2.The synthetic data generation pipeline requires no additional human annotation, making it scalable and practical compared to collecting expert failure demonstrations.\n3.Substantial improvements on EB-ALFRED and EB-Habitat\n4.The paper includes thorough ablations, error persistence analysis, and qualitative examples that provide insights into what the model learns."}, "weaknesses": {"value": "1.Most tasks are relatively short-horizon. How does INFUSER perform on longer tasks with cascading failures?\n2.Does training with synthetic failures on one task set help with entirely new tasks?\n3.The paper primarily compares against supervised fine-tuning baselines. Missing comparisons include: Self-correction methods: Recent work on self-reflective agents (e.g., ReAct, Reflexion) - how do they compare? Human-in-the-loop: What if humans provided actual failure recovery demonstrations for the same cost as running GPT-4o?"}, "questions": {"value": "1.How does performance degrade when synthetic failures don't match real failure distributions?\n2.Can the method work with less capable LLMs than GPT-4o for failure generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oHOWdsTFMr", "forum": "Eq8NRBIRaZ", "replyto": "Eq8NRBIRaZ", "signatures": ["ICLR.cc/2026/Conference/Submission16447/Reviewer_hEuz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16447/Reviewer_hEuz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936046379, "cdate": 1761936046379, "tmdate": 1762926558825, "mdate": 1762926558825, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes INFUSER, a method for training failure-tolerant embodied agents by augmenting expert demonstrations with synthetically generated failure-recovery trajectories. The approach uses GPT-4o to inject plausible suboptimal actions into ground-truth trajectories and generate corresponding recovery reasoning. The authors evaluate their method on EmbodiedBench (EB-ALFRED and EB-Habitat) and VisualAgentBench (VAB-OmniGibson and VAB-Minecraft), showing improvements in task success rates and recovery capabilities compared to models trained only on successful demonstrations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Practical Problem: The paper addresses a real limitation in current embodied AI systems - the inability to recover from errors when trained only on successful demonstrations."}, "weaknesses": {"value": "### Major Issues:\n\n1. **Limited Novelty**: The core idea of using synthetic failures for robust policy learning is not new. Similar concepts have been explored in:\n   - **DAgger framework** (Ross et al., 2011 - already cited) introduced the idea of learning from both expert and agent's own mistakes\n   - **Adversarial imitation learning** (Ho & Ermon, NeurIPS 2016, \"Generative Adversarial Imitation Learning\") explored learning robust policies through adversarial training\n   - **Learning to generalize across tasks** (Mandlekar et al., RSS 2020, \"Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations\") studied recovery from distribution shift in long-horizon tasks\n   \n   The paper's contribution is primarily an engineering application of existing ideas (synthetic data augmentation + LLM-generated reasoning) rather than a fundamental methodological advance.\n\n2. **Problematic Experimental Design**:\n   \n   a) **Distribution Bias**: All synthetic data is generated using GPT-4o, creating a strong bias. The model essentially learns to mimic GPT-4o's reasoning patterns rather than developing independent recovery strategies. This raises the question: Is INFUSER learning general recovery principles or just learning to pattern-match GPT-4o outputs?\n   \n   b) **Train-Test Mismatch**: Critical issue with Equations 7-8. During training, the expert recovery action a*_{t_fail} is provided as input, allowing the model to focus on generating reasoning. However, at test time, the model must generate both reasoning AND actions without expert guidance. This is a severe distribution shift that is not adequately addressed or analyzed.\n   \n   c) **Unfair Baseline Comparison**: In Table 4, the baseline \"Qwen2.5-VL-7B*\" uses 6K successful trajectories while INFUSER uses 5K successful + 1K failure trajectories. The improvement from 18.3% to 33.0% could result from:\n      - Better data diversity rather than failure-recovery learning\n      - The different reasoning annotations\n      - Simply having different training data composition\n   \n   A fair comparison requires: (1) Same total data size, (2) Same annotation quality, (3) Controlled variables.\n\n3. **Missing Critical Ablations and Analysis**:\n   \n   a) **No comparison with data diversity baselines**: What if you simply add 1K more diverse successful demonstrations instead of failure trajectories? The paper doesn't prove that failures specifically help vs. general data diversity.\n   \n   b) **No analysis of synthetic failure quality**: \n      - How realistic are GPT-4o generated failures?\n      - Do they cover actual failure modes in deployment?\n      - What's the distribution of failure types?\n   \n   c) **No multi-seed evaluation**: All results appear to be from single runs, making it impossible to assess statistical significance.\n   \n   d) **Missing ablation on failure types**: Which types of failures (planning vs. reasoning errors) contribute most to improvement?\n   \n   e) **No analysis of failure injection strategy**: Why uniform sampling? Would importance sampling or curriculum learning be better?\n\n4. **Questionable Evaluation Methodology**:\n   \n   a) **Limited metrics**: Success rate alone doesn't distinguish between:\n      - Models that avoid errors through better planning\n      - Models that recover from errors through corrective actions\n   \n   b) **Arbitrary recovery definition**: \"Recovery rate (errors corrected within 2 steps)\" - Why 2 steps? How sensitive are conclusions to this choice?\n   \n   c) **No failure case analysis**: The paper doesn't discuss when INFUSER fails or what types of failures cannot be recovered from.\n   \n   d) **Missing computational cost analysis**: What's the overhead of generating synthetic data? What's the inference time comparison?\n   \n   e) **Statistical significance**: No error bars, confidence intervals, or significance tests reported.\n\n\n\n5. **Limited Scope and Generalization**:\n   \n   a) **Simulation only**: All experiments in simulated environments with:\n      - Perfect state information\n      - Deterministic physics\n      - No sensor noise\n      - No real-world uncertainties\n   \n   b) **No sim-to-real discussion**: How would this approach transfer to real robots where:\n      - Failures are more diverse and unpredictable\n      - Recovery requires physical robustness\n      - Computational resources are limited\n   \n   c) **Simple tasks**: Most tasks are relatively short-horizon (< 20 steps) household tasks. Scalability to complex, long-horizon tasks is unclear.\n\n### Minor Issues:\n\n6. **Presentation and Clarity**:\n   \n   a) **Excessive length**: Over 1600 lines with substantial redundancy. The appendix contains 30+ pages with overly detailed prompts that could be summarized.\n   \n   b) **Poor figure quality**: Figures 5-6 in appendix show similar scenarios without adding insights. Figure 4 comparison is too small to read clearly.\n   \n   c) **Inconsistent terminology**: \"Failure-recovery trajectories\", \"failure-tolerant data\", \"synthetic failures\" used interchangeably.\n\n7. **Theoretical Gaps**:\n   \n   a) **No formal analysis**: Why should training on synthetic failures help with real failures? No theoretical justification or sample complexity analysis.\n   \n   b) **No failure taxonomy validation**: The failure taxonomy in Appendix C is presented without validation - do these categories cover real failure modes?\n   \n   c) **Missing generalization bounds**: Under what conditions does learning from synthetic failures generalize to real failures?\n\n8. **Related Work Limitations**:\n   \n   a) **Incomplete coverage**: Missing important work on:\n      - **Robot learning from demonstration surveys** (Argall et al., Robotics and Autonomous Systems 2009)\n      - **Imitation learning theory** - recent advances in understanding distribution shift\n      - **Robustness in embodied AI** - systematic studies on failure modes\n   \n   b) **Superficial comparison**: The related work doesn't clearly position the contribution relative to existing failure-aware learning methods. For example, how does this differ from ReWiND (Zhang et al., 2025 - cited) beyond using LLM-generated reasoning?\n\n9. **Experimental Issues**:\n    \n    a) **Cherry-picked examples**: Figure 1 and Figure 4 show success cases. Where are the failure cases of INFUSER?\n    \n    b) **Inconsistent improvements**: Performance varies wildly across task categories (Table 1: from 2%→16% on \"Long\" to 8%→68% on \"Base\"). No explanation for this variance.\n    \n    c) **Comparison with wrong baselines**: Why not compare with more recent embodied AI methods from 2024-2025? The field is moving rapidly.\n    \n    d) **VAB results are marginal**: Table 3 shows only 3.3% improvement on VAB-OmniGibson (5.5%→8.8%) and 3.0% on VAB-Minecraft (28.4%→31.4%). These could be within noise levels without significance testing.\n\n\n- Ho, J., & Ermon, S. (2016). Generative adversarial imitation learning. *NeurIPS*.\n- Argall, B. D., Chernova, S., Veloso, M., & Browning, B. (2009). A survey of robot learning from demonstration. *Robotics and Autonomous Systems*, 57(5), 469-483.\n- Mandlekar, A., Xu, D., Martín-Martín, R., Savarese, S., & Fei-Fei, L. (2020). Learning to generalize across long-horizon tasks from human demonstrations. *RSS*."}, "questions": {"value": "Data Quality: How do you ensure the synthetic failures are realistic and cover the actual failure modes that occur during deployment? Can you provide statistics on the diversity of generated failures?\n\nGeneralization: How does the method perform on out-of-distribution failures that differ from the synthetic failures generated by GPT-4o?\nAblation Studies:\n\nWhat happens if you train without the expert action guidance (a*_t) from the beginning, rather than just at test time?\n\nHow sensitive is performance to the failure injection rate ρ for different task complexities?\nWhat if you use a weaker LLM (e.g., GPT-3.5) for synthetic data generation?\n\n\nReal-world Applicability: Have you conducted any real robot experiments? What are the main challenges in transferring this approach to real-world scenarios?\n\nComputational Cost: What is the computational cost of generating the synthetic data? How does the training time compare between baseline and INFUSER?\n\nFailure Analysis: Can you provide examples where INFUSER fails to recover? What are the limitations of this approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KtAr6u1ZJe", "forum": "Eq8NRBIRaZ", "replyto": "Eq8NRBIRaZ", "signatures": ["ICLR.cc/2026/Conference/Submission16447/Reviewer_B2Q6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16447/Reviewer_B2Q6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16447/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996130519, "cdate": 1761996130519, "tmdate": 1762926558494, "mdate": 1762926558494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}