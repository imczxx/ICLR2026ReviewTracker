{"id": "FRTZhAzWRC", "number": 1115, "cdate": 1756842881995, "mdate": 1759898227177, "content": {"title": "WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback", "abstract": "As large language models (LLMs) continue to advance, aligning these models with human preferences has emerged as a critical challenge. Traditional alignment methods, relying on human or LLM annotated datasets, are limited by their resource-intensive nature, inherent subjectivity, misalignment with real-world user preferences, and the risk of feedback loops that amplify model biases. To overcome these limitations, we introduce WildFeedback, a novel framework that leverages in-situ user feedback during conversations with LLMs to create preference datasets automatically. Given a corpus of multi-turn user-LLM conversation, WildFeedback identifies and classifies user feedback to LLM responses between conversation turns. The user feedback is then used to create examples of preferred and dispreferred responses according to users' preference. Our experiments demonstrate that LLMs fine-tuned on WildFeedback dataset exhibit significantly improved alignment with user preferences, as evidenced by both traditional benchmarks and our proposed checklist-guided evaluation. By incorporating in-situ feedback from actual users, WildFeedback addresses the scalability, subjectivity, and bias challenges that plague existing approaches, marking a significant step toward developing LLMs that are more responsive to the diverse and evolving needs of their users.", "tldr": "WildFeedback leverages in-situ user feedback from conversations with LLMs to automatically construct preference datasets, improving alignment with real-world user needs and outperforming traditional annotation methods.", "keywords": ["safety and alignment", "automatic creation and evaluation of language resources", "NLP datasets", "automatic evaluation of datasets", "evaluation methodologies", "evaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0c86cf8458970a1a64f215c1bf9c4e8ae281ed30.pdf", "supplementary_material": "/attachment/38c262c2dbf315f0e2241b83966c99a88b42f379.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes WildFeedback, a novel framework that leverages in-situ user feedback to automatically construct preference datasets. These datasets are then used to generate preferred/dispreferred pairs for preference tuning of LLMs. Experimental results demonstrate the potential effectiveness of the proposed approach."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "*  The paper tackles a timely and important problem: how to automatically mine user preferences without relying on explicit up/down votes on LLM responses.\n*  The proposed framework aims to overcome the limitations of both synthetic and human-annotated preference datasets, which is a meaningful and practically relevant direction."}, "weaknesses": {"value": "*  Lack of fine-grained methodological differentiation from concurrent work such as UltraFeedback. While UltraFeedback uses GPT-4 to generate rankings and derive preferences, the proposed method also relies on GPT-4 for preference determination. Despite architectural or procedural differences, it is unclear whether this work provides a fundamentally distinct contribution beyond UltraFeedback.\n*  Potential evaluation leakage (“double dipping”): The checklist-guided evaluation relies on LLMs both to generate preference data and to summarize user preferences which is then used to align LLM evaluation. This design may inadvertently bias results toward the model’s own judgments, undermining the objectivity of the evaluation.\n*  Experimental results are not consistently supportive of the claimed significant and consistent enhancement. For instance, in Table 3, UltraFeedback outperforms WildFeedback on many occasions."}, "questions": {"value": "1.  Lines 191–192: Is it correct that $\\kappa$ = 0.5 is considered low agreement and $\\kappa$ = 0.69 is considered moderate agreement? Please clarify your interpretation of Cohen’s κ thresholds.\n1.  Line 268: The claim that 57.14% and 63.27% are “similar” seems questionable. For a binary agree/disagree task, random performance is 50%; thus, 57.14% is only 7% above random, while 63.27% is 13% above random—nearly double that margin."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dlSc5hWKdv", "forum": "FRTZhAzWRC", "replyto": "FRTZhAzWRC", "signatures": ["ICLR.cc/2026/Conference/Submission1115/Reviewer_jRq2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1115/Reviewer_jRq2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882925020, "cdate": 1761882925020, "tmdate": 1762915683042, "mdate": 1762915683042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed WildFeedback, an automatic preference data generation framework by identifying user satisfaction signals in multi-turn conversations. Models trained with WildFeedback dataset outperforms those trained with UltraFeedback across Phi3, Llama3 and Qwen2. A check list guided evaluation procedure is also introduced to mitigate the mismatch between annotators’ preferences and actual user preferences"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* WildFeedBack outperformed UltraFeedBack on multiple open source LLMs, demonstrating robust improvement.\n* The automatic preference data construction framework provides easy implementation.\n* The writing is clear and easy to follow."}, "weaknesses": {"value": "The overall novelty of this work is limited. On implementation side, WildFeedback proposed effective automatic pipeline to identify user satisfaction and generate human preference data. But such pipeline appears to be a task-specific solution to this scenario combining existing techniques. The theoretical and heuristic insights from the work is also limited."}, "questions": {"value": "On Phi3 and LlaMA3, UF seems to have higher win-rate if not length-controlled. Would the authors provide some qualitative generation from UF trained model with and without length control?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HK6p9jZgBE", "forum": "FRTZhAzWRC", "replyto": "FRTZhAzWRC", "signatures": ["ICLR.cc/2026/Conference/Submission1115/Reviewer_sjtG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1115/Reviewer_sjtG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982805719, "cdate": 1761982805719, "tmdate": 1762915682925, "mdate": 1762915682925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Summary**\n\nThe paper proposes aligning large language models (LLMs) using **in-situ annotations**—that is, satisfaction (SAT) and dissatisfaction (DSAT) cues, along with user edits extracted from real user–assistant conversations. These cues are converted into **instance-level preference summaries** (“checklists”), which are then used to form **preferred/dispreferred pairs** (generated either by GPT-4 or on-policy). The models are fine-tuned using **SFT** followed by **DPO**, and the same checklists are later reused to guide the evaluation process for more reliable judgments.\nSpecifically, the framework first identifies utterances in multi-turn dialogues that match predefined rubric cues (e.g., SAT: “thank you”; DSAT: “revise it,”).\nFor each detected case, it extracts the conversation up to the model response that triggered the DSAT signal as the **prompt**, treating that response as the **dispreferred output**. The user’s preferences from the feedback are summarized into a **checklist**.\nFinally, the preferred response is generated under the guidance of this checklist (either by an expert model or by the same policy model), with a safety instruction and moderation applied. The same checklist is provided to the judge during evaluation to better distinguish preferred from dispreferred responses."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Strengths**\n* **Grounding in real interactions:** Builds preference data directly from real multi-turn conversations (WildChat).\n\n* **Clear and reproducible training design:** \n\n* **Consistent performance gains:** Shows improvements on AlpacaEval 2, Arena-Hard, and MT-Bench benchmarks across multiple backbones (e.g., Phi-3 LC win-rate +10.6 points on AlpacaEval 2).\n\n* **Human validation:** Validates both SAT/DSAT detection and checklist-guided evaluation with moderate human–model agreement."}, "weaknesses": {"value": "**Weaknesses**\n\n1. **Noisy in-situ supervision.**\n   SAT/DSAT labels are auto-classified by GPT-4; human validation shows only **moderate** agreement (κ≈0.69 SAT / 0.50 DSAT), so non-trivial label noise can propagate into pair construction and training.  \n\n2. **Single cues are insufficient as ground truth.**\n   Individual cues (e.g., “thank you”, “revise it”) may not capture full intent; even with checklist summaries, relying on in-situ cues alone risks ambiguity. (They also rely on checklists during evaluation.)  \n\n3. **Limited methodological novelty & strong LLM dependence.**\n   Core pieces (in-situ cues, SFT→DPO, LLM-as-judge) are known; the framework’s novelty is mainly integration + checklists. GPT-4 is used to generate preferred answers (WF-GPT-4) **and** to judge results, inviting circularity.    \n\n4. **No variance/seed reporting.**\n   Results are reported as single win/tie/lose or scores without error bars; seeds/repeats aren’t specified—please report multi-seed means with CIs/SDs.  \n\n---\n\n**Overall assessment (concise)**\nA **practical** pipeline with **consistent gains** on AlpacaEval 2, Arena-Hard, and MT-Bench, but **incremental novelty** and **heavy dependence on GPT-4 as judge** remain concerns."}, "questions": {"value": "### Q1. Reliability of in-situ SAT/DSAT\n\n How do you verify cues reflect genuine satisfaction/dissatisfaction (not politeness)?\n\n### Q2. Robustness to noisy labels\n\n How sensitive are results to SAT/DSAT noise? Any threshold where gains collapse?\n\n**Request:** **Noise-level ablation**: flip p% labels for p∈{5,10,20,30} (random), report **curves with ≥2 seeds (mean±SD/CI)** \n\n### Q3. What’s new vs ULTRAFEEDBACK, SPUR, WILDBENCH?\n\n Precisely distinguish your contribution from prior work; what’s beyond combination ?\n\n**Request:** A **comparison table** (data source, supervision unit, role in training/eval, on-policy vs GPT-4, curation). **Ablations:** (i) train **without checklists**; (ii) train on **ULTRA** but **evaluate with your checklists** (and vice versa); (iii) replace your judge with WILDBENCH default (no checklists) and report deltas.\n\n### Q4. Cross-judge (LLM & human) evaluation\n Do gains hold across judges and prompt settings?\n\n**Request:** Judges: **Claude**, **Llama-3-70B** (or comparable)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oEyfyghj3n", "forum": "FRTZhAzWRC", "replyto": "FRTZhAzWRC", "signatures": ["ICLR.cc/2026/Conference/Submission1115/Reviewer_reGm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1115/Reviewer_reGm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762101431949, "cdate": 1762101431949, "tmdate": 1762915682820, "mdate": 1762915682820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a method for synthesizing preference data from real user-llm multiturn interactions. Their method relies on first automatically identifying instances where users provide feedback for the LLM's response in real user-llm dialogues (sourced from wildchat). The feedback is then used to generate an alternative response to construct the synthetic preference pair. The authors then demonstrate that training LLMs using these synthetic preference pairs demonstrates gains across a variety of standard LLM chat benchmarks when compared against standard methods of generating synthetic preference data (i.e., UltraFeedback)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This work has great presentation, including the appendix which is quite thorough. \n\n2. The method itself is straightforward and demonstrates consistent gains across a number of benchmarks and base models."}, "weaknesses": {"value": "1. While this work uses some established benchmarks, they alter the evaluation by introducing their own checklist-based judge. While WildBench employed a similar method for their evaluations, their checklists generation method and validation was significantly more extensive to ensure that it is comprehensive, accurate, and unbiased toward particular LMs. Including the including the performance as measured by the original benchmark prompts and settings in addition to evaluating on Wildbench would improve the validity of the experiments.\n\n2. While the methods and settings are distinct, there is enough similarity to [1] to warrant discussion or even direct comparison in evaluations. Overall, the related work section on Feedback Learning for LLMs could be more though, as there is a significant amount of literature on methods for learning from implicit signals from real user data (e.g., [2], [3]). Including [4] as a baseline could also make sense.\n\n[1] User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal\nYuhan Liu, Michael J.Q. Zhang, Eunsol Choi\nEMNLP 2025\n\n[2] Leveraging Implicit Feedback from Deployment Data in Dialogue\nRichard Yuanzhe Pang, Stephen Roller, Kyunghyun Cho, He He, Jason Weston\nEACL 2024\n\n[3] Retrospective Learning from Interactions\nZizhao Chen, Mustafa Omer Gul, Yiwei Chen, Gloria Geng, Anne Wu, Yoav Artzi\nACL 2025\n\n[4] KTO: Model Alignment as Prospect Theoretic Optimization\nKawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, Douwe Kiela\nICML 2024"}, "questions": {"value": "What do the generated examples look like? Can you provide statistics like length for the synthesized responses? While this is a synthetic dataset generation method, it would also be worthwhile to include samples and such statistics from the dataset as if it were a standard dataset paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NggnYD5sRk", "forum": "FRTZhAzWRC", "replyto": "FRTZhAzWRC", "signatures": ["ICLR.cc/2026/Conference/Submission1115/Reviewer_dpmQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1115/Reviewer_dpmQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762535361056, "cdate": 1762535361056, "tmdate": 1762915682706, "mdate": 1762915682706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}