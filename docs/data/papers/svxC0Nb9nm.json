{"id": "svxC0Nb9nm", "number": 16021, "cdate": 1758258697900, "mdate": 1759897267051, "content": {"title": "BadConcepts: Backdooring VLMs with Visual Concepts", "abstract": "Backdoor attacks embed hidden behaviors in models such that inputs with specific triggers cause adversary-chosen outputs while clean inputs remain unaffected. Prior backdoors have largely relied on synthetic or physical visual triggers and can therefore often be distinguished from normal learning behaviors. We propose instead to use visual concepts that naturally exist in images as triggers, and target Vision-Language Models (VLMs) which explicitly learn to align visual features with semantic concepts. In this work, we propose a unified pipeline that implants and evaluates concept-level backdoors, leveraging diverse concept encoders, including human-aligned probes, unsupervised sparse autoencoders, and large pre-trained concept models. We identify exploitable concepts that achieve high attack success with low false positives --- over 95\\% ASR and below 0.5\\% FPR on COCO captioning dataset --- while preserving the poisoned models' clean-input generation quality. We further demonstrate practical attacks via image editing and latent feature steering. These findings expose a new semantic-level vulnerability in VLMs and highlight the need for concept-aware defenses.", "tldr": "", "keywords": ["Trustworthy AI; VLM"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bca7e50ec669562ea5df5a49e57a2135c5e10cbf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper explores using visual concepts as backdoor attack triggers for image captioning task on VLM. The proposed framework uses a visual concept encoder to compute the concept score of all images in the fine tuning dataset and ranks them. The top ranked images are selected and treated as backdoored samples to pair with the target text. The method is evaluated on LLava and multiple visual concept encoders."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides insights in exploring how visual concepts could be used backdoors for attacks on VLM.\n\n2. The paper provides detailed analysis on the distribution of concept scores in the studied datasets, that help understand when the proposed attack could perform well.\n\n3. The proposed method is evaluated on Youden’s J statistic and false positive rate, which is especially important in this case where the backdoor boundary could be ambiguous."}, "weaknesses": {"value": "1. The proposed method requires access to all training data during the fine-tuning phase. How does the attack perform if it approximates the training dataset distribution by constructing it own dataset and rank among the local dataset?\n\n2. It appears the attack depends highly on the overall training data distribution. Under one concept encoder, a concept may have different distributions (unimodal etc) in different dataset.\n\n3. The proposed method is only evaluated one target model architecture. Cross architecture analysis could be important for concept encoders that rely on model internal representations.\n\n4. The proposed method appears to be not suitable for clean label attack, which could make it vulnerable to inconsistency defenses."}, "questions": {"value": "Q1. On line 76, the authors claim  \n> At the same time, concepts as triggers\nprovide attackers with greater flexibility, as they can be chosen from a broad range of attributes in\nthe data domain and embedded into diverse scenes.\n\nWhat is concepts as triggers compared to?\n\nQ2. I am not sure if it is ideal to have a backdoor (snow) that is triggered if it is very obvious (people playing with snow), and not triggered when it is somewhat obvious although not obvious enough (there is a snow sign billboard). Can the author add discussions on that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MBy5AhccIC", "forum": "svxC0Nb9nm", "replyto": "svxC0Nb9nm", "signatures": ["ICLR.cc/2026/Conference/Submission16021/Reviewer_XYfu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16021/Reviewer_XYfu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620416478, "cdate": 1761620416478, "tmdate": 1762926225280, "mdate": 1762926225280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ​​concept-level backdoor attacks​​ against Vision-Language Models (VLMs), where ​​naturally occurring visual concepts​​ (e.g., \"snowy\", \"tennis\", \"red\") serve as triggers. Unlike traditional backdoors that rely on synthetic or physical triggers (e.g., patches, adversarial noise), concept-based triggers are ​​inherently semantic and natural​​, making them harder to detect with existing defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written, and the overall framework is well illustrated through intuitive figures.\n\n2. The proposed method effectively exposes the vulnerability of Vision-Language Models (VLMs) to backdoor attacks, which is an important topic for model safety and trustworthiness."}, "weaknesses": {"value": "1. The fact that visual models can be implanted with backdoors has already been extensively studied. However, the paper lacks a clear motivation for using visual concepts as triggers. It remains unclear why this particular form of trigger is worth investigating — what are the unique challenges and real-world implications compared to existing types of triggers? The manuscript should elaborate on the scenarios in which such visual-concept-based backdoors are likely to occur in practice.\n\n2. The problem setup appears relatively simple and can be addressed using standard fine-tuning techniques. The idea of directly fine-tuning an adapter model is a common practice, and similar category-specific backdoors have been previously explored in purely visual models. The paper does not sufficiently articulate what new challenges arise when extending these attacks to multimodal VLMs, nor does it clearly demonstrate the limitations of prior single-modality approaches in this context.\n\n3. The experiments lack comparisons with strong baseline methods. Without these baselines, it is difficult to evaluate the actual advantages or novelty of the proposed approach.\n\n4. The paper does not discuss how existing VLM backdoor defense techniques perform against the proposed attack. Such analysis would be important to understand the practical robustness of the method and its implications for real-world security."}, "questions": {"value": "1. From a technical standpoint, how does the proposed attack differ from existing backdoor injection techniques used in unimodal visual models? What specific challenges arise due to the vision-language interaction in VLMs, and how does the proposed method address them?\n\n2. Could the authors clarify the real-world threat model or application scenario that justifies this design choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kkZQHKOK0G", "forum": "svxC0Nb9nm", "replyto": "svxC0Nb9nm", "signatures": ["ICLR.cc/2026/Conference/Submission16021/Reviewer_NCNk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16021/Reviewer_NCNk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761647867791, "cdate": 1761647867791, "tmdate": 1762926224326, "mdate": 1762926224326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a backdoor attack on VLMs that uses natural visual concepts as triggers instead of synthetic patches or adversarial perturbations. The method poisoned a small portion of the data with such concept and after training, any images with such target concept will yield poisoned results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is clearly written and the experimental setup is easy to follow. \n2. The paper provides a systematic study across multiple concept-selection methods (e.g., sparse autoencoders, concept classifiers), showing how different concept definitions affect attack success rate."}, "weaknesses": {"value": "The novelty of the paper is the primary weakness. The core attack mechanism is not new. The proposed method is equivalent to a class-level targeted data poisoning or label-flipping attack, where the “class” is defined by a semantic concept. By replacing captions for images that strongly express a particular concept, the fine-tuning process shifts the model’s representation so that the entire semantic region associated with that concept becomes aligned with the attacker’s target output. This behavior has already been well-established in prior works [1–4], to name a few. The paper does not acknowledge or discuss these similarities, which makes it difficult to justify the claimed novelty.\n\n[1] Jia, Jinyuan, Yupei Liu, and Neil Zhenqiang Gong. \"Badencoder: Backdoor attacks to pre-trained encoders in self-supervised learning.\" IEEE S&P, 2022.\n[2] Yang, Wenhan, Jingdong Gao, and Baharan Mirzasoleiman. \"Better safe than sorry: Pre-training CLIP against targeted data poisoning and backdoor attacks.\" arXiv:2310.05862, 2023.\n[3] Carlini, Nicholas, and Andreas Terzis. \"Poisoning and backdooring contrastive learning.\" arXiv:2106.09667, 2021.\n[4] Jha, Rishi, Jonathan Hayase, and Sewoong Oh. \"Label poisoning is all you need.\" NeurIPS 2023."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N5DTbOGGRA", "forum": "svxC0Nb9nm", "replyto": "svxC0Nb9nm", "signatures": ["ICLR.cc/2026/Conference/Submission16021/Reviewer_3T8o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16021/Reviewer_3T8o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954908279, "cdate": 1761954908279, "tmdate": 1762926223831, "mdate": 1762926223831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BadConcepts, a novel backdoor attack framework that uses naturally occurring visual concepts (e.g., “snowy”, “red”) as triggers in Vision-Language Models (VLMs), rather than synthetic or physical visual triggers. The method leverages diverse concept encoders to score images for a target concept, then poisons only the top-k% samples with a malicious output (e.g., “attack successful”). Experiments on LLaVA show that certain concepts achieve >95% attack success rate (ASR)  on COCO, while preserving clean-task captioning quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a new paradigm of concept-level backdoors, distinct from pixel or object-based triggers.\n2. The proposed BadConcepts pipeline is clear and easy to understand.\n3. Experiments demonstrate high attack success while preserving clean-input generation quality.\n4. The manuscript is well-structured."}, "weaknesses": {"value": "1. The paper provides limited empirical analysis of defenses against concept-level backdoors. It remains unclear how these attacks perform when evaluated against standard backdoor detection or mitigation methods.\n\n2. The evaluation focuses primarily on image captioning, leaving other multimodal tasks such as visual question answering (VQA) unexplored.\n\n3. The experiments are conducted on a limited set of architectures, and it is unclear whether concept-based backdoors can be adapted to  different VLM architectures.\n\n4. The method section could provide more detailed explanations of the concept scoring process to improve clarity and reproducibility.\n\n5. The proposed method appears to alter the model’s understanding of specific concepts (e.g., replacing “cat” with “dog”) rather than injecting a conditional trigger–response behavior typical of backdoor attacks. The authors should clarify how their approach differs from conventional data poisoning attacks, as this distinction is crucial for proper positioning within the backdoor literature.\n\n6. The paper does not analyze how the backdoor behaves when triggered by semantically similar or correlated concepts, which may affect attack specificity."}, "questions": {"value": "Please address the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "t6MfIsYeTr", "forum": "svxC0Nb9nm", "replyto": "svxC0Nb9nm", "signatures": ["ICLR.cc/2026/Conference/Submission16021/Reviewer_fuac"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16021/Reviewer_fuac"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16021/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992511385, "cdate": 1761992511385, "tmdate": 1762926223432, "mdate": 1762926223432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}