{"id": "YRp4xqTs3n", "number": 10103, "cdate": 1758160762521, "mdate": 1759897673828, "content": {"title": "Counterfactual Time Series Forecasting with Textual Conditions", "abstract": "Time series forecasting plays an increasingly important role in real-world scenarios, where future trajectories are shaped not only by historical patterns but also by forthcoming events, whose subtle and complex influences pose significant forecasting challenges.\nTwo key aspects emerge in this context. First, forecasting must adapt dynamically under stochastic counterfactual conditions, raising fundamental difficulties in both conditional forecasting and evaluation. Second, the conditions themselves are often complex, and accurately modeling their influence remains non-trivial. Traditional methods typically rely solely on historical information or address only factual future conditions, while neglecting counterfactual scenarios. Moreover, most existing approaches are limited to simple structured conditions, resulting in poor generalization to real-world complexities. To address these gaps, we introduce the task of counterfactual time series forecasting with textual conditions, which leverages unstructured text to enable flexible, condition-aware forecasting. We propose a comprehensive evaluation framework capable of assessing models under both observed data and counterfactual settings, even in the absence of ground truth time series. Furthermore, we present a novel attribution-forecast paradigm that separates mutable from immutable factors, leading to more precise forecasts under sophisticated and stochastic textual conditions.", "tldr": "We introduce counterfactual time series forecasting with textual conditions, providing an evaluation framework and an attribution-forecast paradigm to improve forecasting under complex and stochastic scenarios.", "keywords": ["counterfactual; time series forecasting; multimodal"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eaa7765bdc883e71b418ba9e4e1564f58e82504a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TADIFF, a method for counterfactual time series prediction, conditioned on textual information about the future. This diffusion-based approach learns to separate the intrinsic time series properties from the conditioning signals, allowing realistic future predictions. They propose a counterfactual fine-tuning using synthetic counterfactual samples that improves prediction performance on unseen conditions. The paper also proposes 2 evaluation metrics (DTTC-I and DTTC-E) for evaluating counterfactual instances with no underlying ground truth, which is a common challenge in counterfactual forecasting."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1) The problem motivation and setup are clearly explained, supported by examples and clear figures.\n\n2) The idea of using an intrinsic feature of the time-series sequence as the initial seed and then adding text conditioning is interesting and intuitive. As noted later, additional ablation tests could further strengthen this argument.\n\n3) I believe the paper is well motivated, and the ability to make counterfactual time-series predictions using future text conditions is valuable and can be utilized in many applications."}, "weaknesses": {"value": "1) The motivation for creating a synthetic counterfactual dataset is clear, but the current version does not justify whether the synthetic samples are reasonable or realistic under real conditions. For instance, a) Is randomly sampling future conditions realistic? In many real-world settings, future conditions depend on prior ones; wouldn’t this yield unrealistic sequences of text conditions?\nb) You select the condition closest to $c_h$, but this choice is not justified. Shouldn’t it be closest to $c_f$? Why should sampled text conditions be close to historical conditions? Overall, synthetic datasets need more thorough evaluation.\n\n2) Add more detail about the generative process of the synthetic dataset. How are the text conditions generated? In the examples provided in the Appendix, the texts seem tightly tied to intrinsic time-series features (e.g., “trend going up”). But shouldn't the text conditions provide complementary information? This distinction is important to justify decoupling time-series–specific features from the conditioning signal. If the two modalities are so intertwined, is the decoupling meaningful?\n\n3) Related to the 2 points mentioned above, a well-designed synthetic dataset would let you properly evaluate the synthetic data generation process. With full control over the generation process (including counterfactuals), you can assess the quality of the counterfactual dataset, show it is realistic, and demonstrate that it generates samples from counterfactual conditions not present in the data. I think this might be a good way to justify the CF dataset quality.\n\n4) The benefit of fine-tuning with the counterfactual dataset is not fully convincing. Performance drops with counterfactual fine-tuning on the Traffic dataset (vs. without), which is undesirable and suggests potential biases in the CF data. Does this pattern repeat on other datasets? If so, the fine-tuning approach may need to be reconsidered. Gains on counterfactual data are not worth losses elsewhere, especially when the curated dataset is not validated for realism. Also authors can evaluate how fine-tuning affects benchmark performance as well, as a general solution. \n\n5) “Finding 8” claims robustness to hyperparameters, but the figure does not clearly support this, metrics especially vary substantially on ETTM1. Can the authors explain why this is considered robust? Also, can you provide a justification for the reverse relationship between DTTC-E and DTTC-I."}, "questions": {"value": "1) The paper mentions that initializing inference with Gaussian random noise may introduce properties that conflict with the condition. Can you please justify this claim and give an example? Why would this conflict arise? Intuitively, such noise might instead conflict with intrinsic features of the historical sequence. Please clarify, and ideally include an ablation comparing your proposed initialization to random-noise initialization.\n\n2) In forecasting, is concatenating  $x_{h,0}$ and $x_{h,T}$ necessary? Doesn’t  $x_{h,T}$ already encode all past information?\n\n3) How is the conditioning signal $c$ extracted from text? If a specific embedding model is used, please provide details for reproducibility. Also, how sensitive is overall performance to the quality of this embedding model?\n\n4) “Finding 2” on page 8 cites the wrong table(s). Please correct the reference.\n\n5) This is more of a suggestion, the related work section should also cover causal inference fundamentals: what guarantees the field provides and how this work is situated among existing approaches."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jHLSA4HT6n", "forum": "YRp4xqTs3n", "replyto": "YRp4xqTs3n", "signatures": ["ICLR.cc/2026/Conference/Submission10103/Reviewer_FZhN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10103/Reviewer_FZhN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761584463336, "cdate": 1761584463336, "tmdate": 1762921483710, "mdate": 1762921483710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TADIFF, a diffusion-based model for time series forecasting conditioned on textual descriptions of future scenarios. It tackles the challenge of making accurate forecasts when future conditions differ from historical data (out-of-distribution). The authors also propose new evaluation metrics to assess forecast consistency. The work highlights the need for models and metrics that can handle complex, stochastic, and novel future conditions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "TADIFF is designed to generalize to out-of-distribution scenarios by conditioning forecasts on textual descriptions of future conditions, attempting to overcome the limitations of models that rely solely on historical data.\n\nThe paper introduces new evaluation metrics and a framework to assess forecast consistency with both historical patterns and future textual conditions, even in the absence of ground truth, addressing a key challenge in counterfactual forecasting."}, "weaknesses": {"value": "- The proposed evaluation metrics are not fully explained or contrasted with traditional metrics like MAE/MSE. It is unclear what is gained by using these new metrics, and how they relate to or improve upon standard accuracy-based measures. More concrete examples and justification are needed.\n- The process for constructing counterfactual datasets is not well detailed.\n- In Table 3 and other results, it is unclear whether the observed differences are statistically significant. The absence of standard error metrics (like MAE/MSE) for counterfactuals makes it difficult to compare with existing baselines.\n- The diffusion-based approach and the new evaluation framework add complexity. It is not clear if the benefits outweigh the additional implementation and computational costs for practitioners."}, "questions": {"value": "- How do you ensure future conditions used for counterfactuals are realistic, meaningful, or representative of plausible scenarios.\n- Can you expand on the proposed metrics? Maybe elaborate more with examples or more information about why accuracy is not as good as consistency.\n- Can you provide more detials on Table 3 and if the difference is significant? Why no MAE or MSE for counterfactuals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XwwVDNb9eW", "forum": "YRp4xqTs3n", "replyto": "YRp4xqTs3n", "signatures": ["ICLR.cc/2026/Conference/Submission10103/Reviewer_iGqX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10103/Reviewer_iGqX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879067638, "cdate": 1761879067638, "tmdate": 1762921483362, "mdate": 1762921483362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### *Summary*\n\n- The authors propose a diffusion-based model for forecasting time series with additional textual information. Stage 1 estimates and removes the contribution to the series coming from c\\_h by running the inverse of the diffusion denoising. Stage 2 uses this “intrinsic feature” and runs a diffusion denoising process on it to get a forecast. The authors also propose DTTC, a learned metric which disentangles intrinsic and extrinsic influence on a given series to estimate the forecast’s consistency with both. The authors use LLMs to generate textual information based on window statistics, which they use to train the diffusion model and DTTC. They also use this methodology to generate multiple counterfactuals, which demonstrates the generalization ability of their model.\n\n### *Contributions*\n\n- attribution-forecast paradigm  \n- Text-Attributive time series Diffusion (TADIFF) model (or framework, or paradigm)  \n- Novel metric approach"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "### *Originality*\n\n- Strong novelty component with the modelling approach, DTTC and generalization potential\n\n### *Quality*\n\n- Many interesting findings.  \n- Hyperparameter robustness analysis is good.\n\n### *Clarity*\n\n- Well-structured text, informative result visuals (e.g. figures 3 and 5\\)\n\n### *Significance*\n\n- Works on a timely problem, i.e. forecasting with essential textual information"}, "weaknesses": {"value": "### *Originality*\n\n- The problem setting of forecasting with relevant textual information about future events, such that the “future dynamics diverge substantially from past observations”, is not a novel problem setting (see [https://arxiv.org/abs/2410.18959](https://arxiv.org/abs/2410.18959) and [https://www.arxiv.org/abs/2508.09904](https://www.arxiv.org/abs/2508.09904), both of which treat text about future conditions). Augmenting data to create diverse sets of plausible futures for a single series is interesting from a comparative evaluation perspective, but is more akin to a data augmentation strategy, not a novel problem setting.   \n- Contemporaneous work that is highly related: [https://openreview.net/forum?id=Zbt44sC4tE](https://openreview.net/forum?id=Zbt44sC4tE) (I won't penalize for this)\n\n### *Quality*\n\n- It’s unclear how you make sure that the DTTC-I and DTTC-E metrics remain grounded and generalize (see Questions).  \n- Finding 1: these datasets are saturated [https://www.arxiv.org/abs/2510.02729](https://www.arxiv.org/abs/2510.02729). It’s unclear what SOTA on these datasets actually means.  \n- Insufficient detail to understand whether the results of finding 7 are sufficient to back up the claims of DTTC being a good metric.  \n- It’s unclear what the relevance of Leibniz’s Principle of Continuity has to do with general time series forecasting. Why would exchange rate time series be bound to “natural” changes (L199)?\n\n### *Clarity*\n\n- The framing of the contributions is inconsistent.  \n  - The intro frames the contributions as 1\\) the task, 2\\) the evaluation and 3\\) the attribution paradigm.  \n  -  The conclusion frames the contributions as 1\\) the task, 2\\) the TADiff model and 3\\) the evaluation metrics  \n  - The intro and abstract do not mention TADiff at all, despite this being a key contribution in the conclusion.  \n- The use of terms such as “framework”, “paradigm” and other vague, non-technical terms is confusing. I would avoid using these terms, as they make the assessment of the paper’s contributions more difficult. If it’s a model architecture \\+ training strategy, please just say that.  \n- Figure text is too small, illegible at normal document size  \n- Figure 1 is good, but the caption is bare. There are textual elements in the figure that are not discussed in the caption, e.g. the background information on traffic volume. Is that input, or just a description for the example? The caption should clarify this, and ideally “walk through” the figure.  \n- The caption for figure 2 is uninformative.  \n- If DTTC is a major contribution to the paper, the training details should not be solely in the appendix. These details are too important to understand whether DTTC makes sense as an approach to be left in the appendix. In fact, this relates to a more general comment on criticism: there are too many findings in the main paper, which prevents you from analyzing any in depth. I would prioritize 2 or 3 main findings, discuss those in the main text, and relegate the rest to “Additional Results” in the appendix.  \n- Inconsistent terminology: why flip to “text-attribution” in section 4.3? This is inconvenient for ctrl-F"}, "questions": {"value": "### *change your opinion*\n\n- The concepts are good, but the *clarity* of the article is significantly impeded.  \n  -  Key details on dataset generation that are relevant to generalization capacity are buried in the appendix,   \n  - non-technical terms are used to discuss model architectures,   \n  - there are too many results in the main text to discuss in depth,   \n  - the figures are not clear,   \n  - key training details are relegated to the appendix,  \n  - terminology is inconsistent.   \n- There is key missing related work that undermines the proposed contributions, which hinders the alignment between the paper’s claims and its contributions/results.   \n  - I would shift away from claiming that the problem setting is a contribution and cite the appropriate work instead (Context is Key, Time-MMD, etc.)  \n    - The paper should reframe the counterfactual generation as an LLM-based data augmentation strategy because that’s what it is: you use an LLM to generate text that accompanies the series, and show that your architecture works well when given access to this text.  \n    - The contributions should avoid overclaims on novel problem settings, and instead focus around the article’s actual contributions:  \n      - model   \n      - DTTC  \n      - data generation strategy  \n      - generalization guarantees\n\nBy reframing the article to align the claims with the contributions and citing appropriate relevant work, I can see myself raising the score to a 6\\. By performing additional generalization experiments and seeing how well the approach generalizes, the score could raise to an 8, although I would expect significantly more generalization results, such as: \n\n- Results on MM-TSFLib and/or Context is Key  \n- Assessment of overfitting to LLM-generated textual patterns by testing different LLM families for textual generation\n\n### *clarify a confusion*\n\n- I’m a bit confused about the *order* in which the different components are trained, and how you ensure that evaluation using DTTC is clean: you use DTTC to generate the counterfactual datasets, but then you estimate generalization ability based on DTTC scores in Table 2\\. If you’re optimizing your finetuning generalization based on DTTC scores, then you can’t really use it to evaluate generalization ability, right? Could you fit your model \\+ DTTC to a subset of datasets, and then test on some held-out datasets to see whether the metric and model generalization results still hold on unseen datasets/patterns?  \n- Where do you get the original textual condition c\\_h? The datasets you use do not have accompanying text. Is this what is described in B.3.1? If so, I’d put that in the main text as it’s quite important for understanding the dataset generation and limitations of generalization ability. How do you avoid overfitting to an LLM’s generated text format? One way to do this would be to demonstrate performance on a separate benchmark, such as Context Is Key ([https://arxiv.org/abs/2410.18959](https://arxiv.org/abs/2410.18959)), which has already filtered for ensuring the context is highly relevant.\n\n### *address a limitation*\n\n- The maintainers of these forecasting datasets (ETT, Exchange, Weather, possibly Traffic) have publicly stated that they are saturated: [https://www.arxiv.org/abs/2510.02729](https://www.arxiv.org/abs/2510.02729). What is the value of building on top of these datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0I3UE2FFQ7", "forum": "YRp4xqTs3n", "replyto": "YRp4xqTs3n", "signatures": ["ICLR.cc/2026/Conference/Submission10103/Reviewer_Tfgp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10103/Reviewer_Tfgp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928249978, "cdate": 1761928249978, "tmdate": 1762921482998, "mdate": 1762921482998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the task of counterfactual time series forecasting conditioned on unstructured textual descriptions of future events. This addresses limitations of existing methods that rely primarily on historical data or structured future conditions. The proposed TADIFF first attempts to disentangle immutable \"intrinsic features\" from historical sequences using a diffusion inversion process conditioned on historical text. Then, these features initialize a denoising process conditioned on future text. The proposed DTTC metric enables evaluation in the absence of ground truth, and the empirical results of TADIFF are strong across multiple datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces the task of counterfactual time series forecasting conditioned on unstructured text, moving beyond prior work that often relies on structured or categorical interventions.\n\n2. The TADIFF framework introduces a novel approach within a diffusion model framework designed to handle potential conflicts between historical patterns and future conditions by attempting to separate intrinsic features from external influences.\n\n3. The introduction of the DTTC metric addresses the critical challenge of evaluating forecasts when counterfactual ground truth is unavailable.\n\n4. The proposed method demonstrates effectiveness across various datasets against a comprehensive set of unimodal and multimodal baselines."}, "weaknesses": {"value": "**1. (Major) Ambiguous Intrinsic-features:**\nIt is conceptually unclear why adding estimated \"condition-related noise\" via the inverse transition $\\psi_t^{-1}$ would result in a representation ($x_{h,T}$) free of $c_h$'s influence. In the DDIM framework [1], $x_{h,T}$ is the specific noise realization that leads back to $x_{h,0}$ when conditioned on $c_h$, this does not guarantee independence from $c_h$. Also there is no rigorous proof or direct empirical validation provided to confirm that $x_{h,T}$ is truly independent of $c_h$. The validation is indirect, relying on ablation studies and t-sne plots (Fig.3).\n\n**2. (Major) Over-reliance on the DTTC Metric:**\nThe DTTC metric is used for evaluating counterfactual performance and optimizing the model during finetuning. DTTC is used both as an evaluation metric and as the optimization target during counterfactual finetuning , i.e., the model is directly trained to increase DTTC scores. Optimizing against a learned, potentially flawed metric risks overfitting to the metric itself rather than genuinely improving the quality of the counterfactual forecasts\nThe performance of the DTTC metric varies significantly. For example, the DTTC-I retrieval accuracy on ETTm1 is only 55.06% (Tab.4). This low accuracy casts doubt on the model's ability to reliably capture intrinsic features across all domains.\n\n**3. (Major) Clarity and Consistency:**\nIn the Sec 3.3, the input $x_t$ is defined such that the historical part $x_{h,0}$ remains clear during training, and a mask $m$ ensures only future components are supervised in Eq.7. However, the inference stage applies the denoising transitions $\\psi_t$ to the concatenated sequence $x_{h,0} + x_{f,T}$ (Eq.6). The standard DDIM denoising step updates the entire input sequence. This implies that the known historical part $x_{h,0}$ would be altered during inference, which seems to be inconsistent with the training setup.\n\n**4. (Minor) Potential Bias on the Constructed Text:**\nFor three real datasets, the “text” is generated from features (tsfresh + templates) rather than organically sourced, which can bias both training and the DTTC encoders toward these templated attributes. \n\n\nClarifications are welcome and I will reconsider and raise the score if the questions are addressed.\n\nReferences:\n[1] Song, Jiaming, Chenlin Meng, and Stefano Ermon. \"Denoising diffusion implicit models.\" arXiv preprint arXiv:2010.02502 (2020)."}, "questions": {"value": "1. The main supports for independence are Table 3 and a t-SNE plot, which show usefulness but not actual disentanglement/independence. Have the authors tried a simple method (e.g., a classifier that predicts $c_h$​ from $x_{h,T}$)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NiSlXWwXGA", "forum": "YRp4xqTs3n", "replyto": "YRp4xqTs3n", "signatures": ["ICLR.cc/2026/Conference/Submission10103/Reviewer_kMJW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10103/Reviewer_kMJW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930195276, "cdate": 1761930195276, "tmdate": 1762921482677, "mdate": 1762921482677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}