{"id": "UrWdRcLINM", "number": 4029, "cdate": 1757586827615, "mdate": 1759898057156, "content": {"title": "UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations for Spoken Dialogue Models", "abstract": "Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce \\textbf{UltraVoice}, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33\\% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice. Moreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84\\% on the Basic setting and +7.87\\% on the Pro setting. Furthermore, the dataset's utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis.", "tldr": "We introduce UltraVoice, a large dataset for teaching spoken models fine-grained speech style control, improving expressiveness while boosting conversational skills.", "keywords": ["spoken dialogue models", "fine-grained speech style control", "dataset"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ff29cb9e4e1d917b4bfd2e47bf7c830991ba084.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to address the current limitations of speech dialogue models in expressive capability (how to say), rather than solely focusing on what to say. This work introduce UltraVoice, the first large-scale speech dialogue dataset specifically designed for multi-dimensional, fine-grained speech style control. The dataset contains over 830 hours of speech dialogues, covering six key style dimensions.\n\nThe core contribution lies in constructing this dataset through a four-step process: text corpus screening and management, injection of style instructions and generation of response texts, stylized speech synthesis using multiple advanced TTS models, and rigorous quality control and data filtering.\n\nFurthermore, the paper demonstrates UltraVoice's effectiveness through several experiments. This work performs SFT on multiple end-to-end speech dialogue models. Experimental results show that the fine-tuned models achieve improvements in both the ability to follow style instructions (by IFR) and the naturalness of generated speech (by MOS). And this enhanced stylistic capability does not compromise the model's core conversational abilities on URO-Bench. Finally, the paper demonstrates that the UltraVoice dataset can be used to train controllable TTS models, further validating its data quality and versatility."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The study identifies the important point of poor expressiveness in speech interaction within the field of speech dialogue and constructs a large-scale, stylistically diverse speech dialogue dataset.\n- The dialogue model fine-tuned with this dataset demonstrates enhanced expressiveness in responses while retaining its foundational capabilities. This indicates its potential to effectively enhance the performance of existing dialogue systems."}, "weaknesses": {"value": "- The subjective metrics (MOS and IFR) are obtained by Gemini-2.5-flash. But in the reference mentioned in line 336 uses Gemini-2.5-pro. There is a gap between these two model in speech-related judgments. So the subjective scores are not verified to be consistency to humans, which diminishes the persuasiveness of the evaluation results.\n- All data is generated by existing TTS models, particularly the crucial response component. This implies that the upper limit of stylistic diversity and naturalness learned by UltraVoice is constrained by the capabilities of the TTS models used. It may not fully capture the more subtle, unpredictable prosodic variations and non-verbal vocalizations present in real-world human conversation.\n- The experiment in section 4.4 exhibits significant flaws. First, on the EmoVoice-DB test set, the model trained with UltraVoice does not achieve favorable results, suggesting that the quality of style description-speech pairs in the dataset may be not good. Second, on UltraVoice's test sets (covering Chinese, Japanese, and Korean), both baselines are English-only models. They are not comparable on the WER metric, rendering the conclusion stated in line 458 invalid."}, "questions": {"value": "- How do the authors view the potential “ceiling effect” arising from datasets composed entirely of synthetic data? Specifically, is the expressive capability ceiling achievable by models fine-tuned on UltraVoice constrained by the TTS model used to generate that dataset (e.g., GPT-4o-audio-preview)?\n- In data processing, only CER and length are used for filtering. How to ensure that the generated response speech aligns with the style description? Should style-related rules be incorporated into the filtering phase (e.g., SER results in the emotion subset)?\n- Has the author conducted any human evaluation studies to validate Gemini-2.5-flash's scoring results? How confident can we be that the LLM evaluators did not exhibit a systematic preference for generations in the same style as those produced by the LLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9nPtW9532E", "forum": "UrWdRcLINM", "replyto": "UrWdRcLINM", "signatures": ["ICLR.cc/2026/Conference/Submission4029/Reviewer_hgwR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4029/Reviewer_hgwR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760961722131, "cdate": 1760961722131, "tmdate": 1762917141444, "mdate": 1762917141444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce a new spoken dialogue dataset UltraVoice which is mainly for multiple fine-grained speech style control for speech dialogue model. It contains 830 hours speech dialogue data, and it mainly consists from speech generation/conversion model with designed spoken corpus and accordingly instruct on style control. What's more, the dataset has been filtered and cleaned to control the quality. Author provides different statistical and criterions such as character error rate (CER), subjective score (UTMOS), and experiment to finetune end-to-end speech dialogue model to verify the effectiveness via by metrics improvements on MOS and instruction following rate."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Here are two major strengths from my perspective. \n1) It constructs a speech spoken dialogue dataset in 830 hours and with rich fine-grained styles and instructions for conversation. It also controls the quality by criterion to filtering by recognition. The paper provides clear instruction of how to build it step by step. It identifies current limitation of current spoken dialogue data lack of style control. \n2) It assets the dataset with detail statistics across different dimension and experiments on finetune end-to-end speech model like SLAM-Omni and VocalNet to verify the help of this data. The result shows relatively improvements on MOS and IFR."}, "weaknesses": {"value": "Major weaknesses are\n1. Lack of innovation on research perspective either algorithm or methodologies. The lack of ability for fine-grained  speech style control is a challenge, but this building from existing conversation corpus with GPT-like model instruction inject and generated speech from various TTS/voice conversion models would be a little too simple and artificial, not real interaction data. The generation process could not simulate the real interaction like real scenario, like the real response speech with this style instruction in this context. \n2. As spoken dialogue model, the evaluation is lack of real conversation subjective metrics. Experiments are evaluated on SFT for e2e speech dialogue model for the URO-bench, gemini MOS score and emotions expressiveness. Further more, it also provides the instruction following rate (IFR) to show the ability of control. However, none of these metrics show the real contribution of the natural dialogue interaction. Meanwhile, the test samples are 100 examples sampling from the constructed dataset, although it is not overlapped in training, but they are from the same building process, they have similar characteristic. It lack of realness or generalization proof."}, "questions": {"value": "1. There are kinds of MOS test in this paper, but it seems from different calculation such as Gemini-2.5-Flash generated MOS. UTMOS as another automatic, non-intrusive metric used to predict MOS. Is there any MOS test is conducted with subjective language expert? such as in the table 7, this MOS seems not from automatic evaluation? \n2. Is there any experiment to test with other test set which is not sampling from the same generation for this corpus and compare the base model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lc7gZDYPx8", "forum": "UrWdRcLINM", "replyto": "UrWdRcLINM", "signatures": ["ICLR.cc/2026/Conference/Submission4029/Reviewer_Mx6y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4029/Reviewer_Mx6y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761136722074, "cdate": 1761136722074, "tmdate": 1762917141202, "mdate": 1762917141202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "UltraVoice addresses the lack of controllable expressivity in end-to-end spoken dialogue systems. The paper introduces a large, instruction-grounded dataset for fine-grained, multi-dimensional speech-style control across six dimensions—Emotion, Speed, Volume, Accent, Language, and Composite—comprising 100,770 dialogues (≈833 hours). The pipeline: (1) curate concise QA pairs from UltraChat; (2) inject diverse style instructions and generate style-aligned textual replies via GPT-4o; (3) synthesize speech pairs where user prompts use varied speakers/noise and system replies share a single fixed timbre for consistency (accents synthesized and normalized to the fixed voice); and (4) apply quality control with Whisper ASR, keeping clips <30 s and CER <20%. \n\nTo study downstream effects, the authors fine-tune representative voice agents (SLAM-Omni, VocalNet) spanning Qwen and LLaMA backbones. Evaluation covers all sub-controls with a 2.3k-item test set and reports Instruction-Following Rate (IFR) and MOS via an audio-language model, Gemini-2.5-Flash, plus objective metrics, like WER, emotion2vec similarity, and UTMOS. Fine-tuning on UltraVoice yields substantial gains: IFR +14.61–40.09 points and relative MOS +29–42%, with especially strong results for Emotion and Volume. General conversational ability also improves on URO-Bench (+10.84% Basic, +7.87% Pro), and the best SFT model achieves state-of-the-art among compared systems.\n\nNotably, repurposing the replies as a controllable TTS corpus improves an EmoVoice model (WER 19.82 → 3.97 with MOS/IFR gains across Accent/Speed/Volume/Composite), indicating the dataset’s utility beyond dialogue."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. UltraVoice includes six control dimensions (Emotion, Speed, Volume, Accent, Language, Composite), with instruction–response dialogue format; 100,770 dialogues (~833 hours). Authors positioned it as the first dataset explicitly designed for fine-grained, multi-dimension control in end-to-end voice agents.\n2. By doing SFT on UltraVoice, fine-tuned models shows Instruction-Following Rate (IFR) improves by +14.61 to +40.09 percentage points across backbones and sizes; Mean Opinion Score (MOS) improves by ~29.1%–42.3%, relatively.\n3. On URO-Bench, SFT models see overall gains on Understanding / Reasoning / Oral Conversation (avg. +10.84% Basic, +7.87% Pro); VocalNet-7B-SFT reaches SOTA among compared systems. Re-using the data for controllable TTS yields strong results (e.g., WER 19.82 → 3.97, plus MOS/IFR boosts on Accent/Speed/Volume/Composite), suggesting downstream utility beyond dialogue."}, "weaknesses": {"value": "1. User prompts come from varied speakers/noise, but system replies use a single voice, and some Accent samples require voice conversion after TTS. This may cap speaker diversity and introduce voice conversion artifacts, creating a domain gap to real human recordings, and might influence downstream model's understanding of human voice features.\n2. The Language dimension (Chinese/Korean/Japanese) is harder: authors claimed that LLaMA-based models show smaller gains or regressions in MOS/IFR there—likely due to limited multilingual pretraining exposure and/or limited multilingual SFT coverage. This is not rigorously proved. Further experiment or analysis are required to claim that. \n3. All six stylistic dimensions use GPT-4o-generated style prompts (and style-conditioned responses). This centralizes the semantics of “emotion/speed/volume/accent/language” in one LLM’s worldview. This might bring bias to the synthesized dataset. It is crucial to evaluate   how large is the gap between the true distribution of human voice and the GPT-4o biased understanding of human voice."}, "questions": {"value": "1.\tIn UltraVoice, the prompts for all six stylistic dimensions are generated by GPT-4o. I’m concerned that this might introduce some bias into the dataset due to GPT-4o’s own understanding of human emotions/speaking rate/accent, etc. If such bias exists, how can we quantify it so that others can try to balance this bias in future work?\n\n2.\tIn the synthesized speech data, I listened to a few examples, and some are particularly impressive—such as Angry example 2. But I’m curious: across the whole dataset, what proportion of audio samples “might not make listeners feel it’s Angry”? Has there been any human evaluation of this proportion to verify that your filtering algorithm is robust enough to remove such data that could affect downstream models? I don't see any filter algorithm applied on this concern."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4GgL1oDLux", "forum": "UrWdRcLINM", "replyto": "UrWdRcLINM", "signatures": ["ICLR.cc/2026/Conference/Submission4029/Reviewer_kfYh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4029/Reviewer_kfYh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724548625, "cdate": 1761724548625, "tmdate": 1762917140971, "mdate": 1762917140971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UltraVoice, a large speech dialogue dataset designed to give spoken dialogue models fine-grained control over how they speak across emotion, speed, volume, accent, language, and composite styles. The authors build the corpus through a four-stage pipeline that curates text, injects style instructions, synthesizes stylized speech, and filters for quality, then uses it to fine-tune popular end-to-end dialogue models. The fine-tuned models show stronger adherence to style instructions while keeping or improving general conversational ability on an external benchmark. The dataset also transfers to controllable text-to-speech training, indicating broad utility for expressive and natural voice generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper targets an important gap by enabling fine-grained style control across six dimensions and provides the first large-scale dialogue dataset designed for this purpose.\n- The dataset construction pipeline is thorough, with explicit filtering via CER and audio duration for quality enhancement.\n- Fine-tuning on UltraVoice raises instruction following and subjective naturalness across models and across most style dimensions. General conversational ability also improves on URO-Bench."}, "weaknesses": {"value": "- The corpus is fully synthetic, built with GPT-4o and several TTS or VC systems, which may introduce artifacts and reduce diversity.\n- Some evaluations are done by an ALM judge, which raises concerns about reproducibility and potential bias in subjective scores. A stronger human evaluation component would help.\n- Since the authors argue that existing controllable TTS data are not suitable for fine-tuning dialogue models, the paper should add comparative fine-tuning results using those datasets. This would substantiate the claim.\n- Accent control uses Edge TTS with post-hoc voice conversion to a fixed timbre, which may introduce artifacts or distortion. An analysis should verify its quality and check whether other styles, such as speed or emotion, remain unchanged."}, "questions": {"value": "Please refer to my comments in the weaknesses section.\nI also noticed two small typos:\n- In Sec. 4.1, add a space after \"(Table 14)\".\n- In Appendix E.8, \"tyle_description\" should be \"style_description\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pDrpmYVh9o", "forum": "UrWdRcLINM", "replyto": "UrWdRcLINM", "signatures": ["ICLR.cc/2026/Conference/Submission4029/Reviewer_215S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4029/Reviewer_215S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4029/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982970586, "cdate": 1761982970586, "tmdate": 1762917140679, "mdate": 1762917140679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}