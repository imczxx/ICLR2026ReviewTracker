{"id": "YzHbFwYmE1", "number": 16816, "cdate": 1758269037735, "mdate": 1759897217945, "content": {"title": "A Theoretical Framework for Rate-Distortion Limits in Learned Image Compression", "abstract": "We present a novel systematic theoretical framework to analyze the rate-distortion (R-D) limits of learned image compression. While recent neural codecs have achieved remarkable empirical results, their distance from the information-theoretic limit remains unclear. Our work addresses this gap by decomposing the R-D performance loss into three key components: variance estimation, quantization strategy, and context modeling. First, we derive the optimal latent variance as the second moment under a Gaussian assumption, providing a principled alternative to hyperprior-based estimation. Second, we quantify the gap between uniform quantization and the Gaussian test channel derived from the reverse water-filling theorem. Third, we extend our framework to include context modeling, and demonstrate that accurate mean prediction yields substantial entropy reduction. Unlike prior R-D estimators, our method provides a structurally interpretable perspective that aligns with real compression modules and enables fine-grained analysis. Through joint simulation and end-to-end training, we derive a tight and actionable approximation of the theoretical R-D limits, offering new insights into the design of more efficient learned compression systems.", "tldr": "We present a novel systematic theoretical framework to quantify the rate-distortion limits of learned image compression, explicitly isolating the effects of variance modeling, quantization, and context prediction.", "keywords": ["Rate-Distortion Limits", "Learned Image Compression", "Information Theory", "Reverse Water-filling", "Context Modeling"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cbe52d6476e5b2349f519ed2d5d7207166f3782f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "It tries to provide interpretable analysis for learned image compression, and addresses this gap by decomposing the R-D performance loss into three key components: variance estimation, quantization strategy, and context modeling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. It tries to provide interpretable analysis for learned image compression, and discussses the rate-distortion limits.\n\n2. Motivation is clear and the manuscript is well organized.\n\n3. The analysis of scaling coefficient of $y$ is interesting."}, "weaknesses": {"value": "1. Context model not only predicts the mean value of $y$, but also the variance of $y$. This is not accurate for the whole section 3.5, especially for the equation 11.\n\n2. It is just similar with the prior work \"Rethinking Learned Image Compression: Context is All You Need\"(https://arxiv.org/abs/2407.11590), which also analyzes the quantization and context model. \n\n3. The importance of $y$ is also dependent on the content of region of image itself, thus the scaling coefficient of $y$ may also not be the best choice. Though I understand it only considers the symbol encoding itself.\n\n4. The visualization in Appendix lacks the comparsion with the prior work.\n\n5. Maybe the results of more datasets like CLIC can make readers have more confidence in this paper."}, "questions": {"value": "See Weaknesses, and I would prefer to improve my rating during rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sGswPCO0gA", "forum": "YzHbFwYmE1", "replyto": "YzHbFwYmE1", "signatures": ["ICLR.cc/2026/Conference/Submission16816/Reviewer_w1ha"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16816/Reviewer_w1ha"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761291212739, "cdate": 1761291212739, "tmdate": 1762926845537, "mdate": 1762926845537, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a theoretical analytical framework for quantitatively investigating the rate-distortion performance limits of deep learning image compression models. The framework decomposes the sources of compression efficiency from three interpretable dimensions: Variance Modeling, Quantization Strategy, and Context Modeling. By replacing each of these modules with a theoretically optimal form, the authors establish a computable R-D upper bound and verify the gap between the model and the information-theoretic bound on datasets such as Kodak and MNIST."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper presents a systematic theoretical framework for analyzing the gap between learning image compression models and the information-theoretic rate-distortion limit. Based on Hyperprior structure, the framework decompose the performance gap from three aspects: variance modeling, quantization method and context modeling, and correspond to them through mathematical derivation and experimental verification. The paper is relatively complete in method design and shows the contribution of each module to the overall rate-distortion performance. The authors provide a clear model structure and training configuration in the experimental section, which makes the study more reproducible. This study establishes a connection between theory and practice, and provides a valuable analysis idea for understanding the performance upper bound of neural compression models and its improvement direction."}, "weaknesses": {"value": "1. Although the PSNR and MS-SSIM results reported in the papers are consistent with theoretical trends, the quantification of statistical fluctuations is lacking. Reproducibility and stability are crucial for research with \"theoretical limits\" as the core topic.\n2. In the experiments, the latent variable dimensions M=192 and M=320 are used to correspond to different code rates, which is reasonable in engineering. However, the effect of model capacity on the theoretical rate-distortion gap has not been systematically analyzed.\n3. For a work that aims to provide a \"general theoretical benchmark\", is validation considered on more types of learned compression models to demonstrate the universality of the framework?"}, "questions": {"value": "1. In this paper, the mean of the latent variable is predicted by an autoregressive method to reduce the conditional entropy. In the theoretical framework, is it possible to further reduce the gap between the actual model and the theoretical rate-distortion upper bound by extending the parameterized form of the context model to predict both the variance and the mixture parameter?\n2. The paper uses a two-stage training (pre-training with Hyperprior and then fine-tuning), and while claiming \"equivalent training results from scratch\", it does not provide specific curve or variance validation.\n3. This framework assumes that the latent variables follow a uniform Gaussian distribution and does not explicitly consider the statistical differences caused by different semantic contents. Given that compression efficiency in real images often varies by content type (such as texture or smooth regions), do the authors think that the relationship between statistical features of latent variables and semantic information can be further explored in theoretical analysis to explain the differences in compression efficiency at the semantic level?\n4. The experiments are based on Kodak and OpenImages. Would the theoretical upper bound fit consistently if tested on more complex or high-resolution datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Xqe5rBOHo1", "forum": "YzHbFwYmE1", "replyto": "YzHbFwYmE1", "signatures": ["ICLR.cc/2026/Conference/Submission16816/Reviewer_1b2F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16816/Reviewer_1b2F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717779026, "cdate": 1761717779026, "tmdate": 1762926845018, "mdate": 1762926845018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new theoretical framework to analyze the rate-distortion (R-D) limits of learned image compression, addressing the gap between empirical performance and information-theoretic limits. By decomposing R-D performance loss into key components, the authors provide a deeper understanding of the factors influencing compression efficiency. They introduce a principled method to estimate latent variance based on Gaussian assumptions, offering a more reliable alternative to hyperprior-based methods. Additionally, the framework quantifies the impact of uniform quantization compared to the ideal Gaussian test channel, derived from the reverse water-filling theorem. Context modeling is also incorporated, revealing that accurate mean prediction can significantly reduce entropy. Unlike previous methods, this approach offers a structurally interpretable perspective aligned with real compression systems, enabling fine-grained analysis. Through simulations and end-to-end training, the study delivers a practical and precise approximation of theoretical R-D limits, providing valuable insights for designing more efficient neural compression models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is theoretically grounded with an information-theoretic perspective.\n\n- Analyzing the R-D limits of existing learned image compression models is interesting.\n\n- The authors motivate general entropy coding frameworks, exhibiting potential impacts."}, "weaknesses": {"value": "- The authors appear to simply believe that applying the reverse water-filling for distortion allocation in the quantization process from y to y_hat would lead to the optimal rate. However, for practical transform coding models like hyperprior, distortion is actually calculated in the pixel domain between $x$ and $\\hat{x}$ after going through both analysis and synthesis transforms. In this case, the vanilla reverse water-filling does not necessarily hold. This paper has completely overlooked this issue.\n\n- Contradictory Memoryless Assumption. The paper begins with the memoryless (i.i.d.) assumption, directly applying the reverse water-filling results. However, starting from Section 3.5, the authors reintroduce dependencies between different y variables through \"context modeling\". At this point, continuing to use memoryless reverse water-filling becomes somewhat self-contradictory. In fact, to my knowledge, the memory version of reverse water-filling does exist.\n\n- The usage of reverse water-filling is questionable and not well-motivated. The authors apply reverse water-filling to allocate distortion $y$ from  to  $\\hat{y}$ , but distortion is measured between  $x$ and $\\hat{x}$ in practical models. This mismatch is still very vague to me. Although the authors claim that they will introduce more details of reverse water-filling, the current status is not ready for acceptance from my side.\n\n- The practical application of the proposed theoretical framework is questionable compared with the commonly used BD-rate and R-D curve. Although I agree that the previous method (e.g., BD-rate and R-D curve) may be a dataset-driven manner, the proposed method only relies on the model architecture. This arises from the issue that the current method is difficult to generalize to more sophisticated image compression architectures beyond the hypeprior family (e.g., diffusion model or large language model-based compression models in the future)."}, "questions": {"value": "Please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9EgK5Xa7zu", "forum": "YzHbFwYmE1", "replyto": "YzHbFwYmE1", "signatures": ["ICLR.cc/2026/Conference/Submission16816/Reviewer_aqSw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16816/Reviewer_aqSw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981133663, "cdate": 1761981133663, "tmdate": 1762926844476, "mdate": 1762926844476, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel theoretical framework for estimating the rate-distortion (R-D) limits of learned image compression, based on the Hyperprior architecture. It decomposes the performance gap between practical neural codecs and information-theoretic optima into three interpretable components, inlcuding variance estimation (replaced with optimal second-moment estimates), quantization strategy (replacing uniform quantization with Gaussian test channel via reverse water-filling), and context modeling (incorporating autoregressive mean prediction to reduce entropy). The framework simulates idealized behavior through end-to-end training, providing a tight, actionable approximation of the R-D bound. Experiments on Kodak (evaluation), OpenImages (training), and MNIST (comparison with Sandwich Bound, NERD, WGD, etc.) show that this method yields tighter bounds and reveals gaps in SOTA codecs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- It is highly commendable that the authors seek to establish a rigorous theoretical framework for quantifying the performance gap between state-of-the-art codecs and information-theoretic limits. \n\n- The presentation is clear, well-structured, and easy to follow."}, "weaknesses": {"value": "While the paper presents an ambitious and potentially valuable contribution, I have serious concerns about two core aspects of the theoretical framework and its implementation. If unaddressed, these issues would undermine the validity of the analysis, experiments, and conclusions.\n\n- *Scope of the distortion measure in the theoretical analysis.* The framework analyzes optimality in the latent space, with distortion measured between the continuous latent representation and its quantized version. However, in learned image compression, the ultimate distortion of interest is in the image space—i.e., between the original and reconstructed pixels. The analysis appears to overlook the impact of the nonlinear encoder and decoder transforms. The claimed correspondence between latent-space and image-space distortion minimization holds only under restrictive conditions (e.g., if the transforms are distortion-preserving in a specific sense). Without explicit justification or bounds on this approximation error, the theoretical results may not faithfully reflect the true R-D limit of the full codec.\n\n- *Implementation of the optimal variance estimate.* Theorem 3.1 correctly identifies the optimal variance as the second moment of $y$, i.e., $σ^2=E[y^2]$. However, the provided end-to-end simulation (Lines 332 and 391) appears to set $σ^2=y^2$, removing the expectation and treating each symbol as deterministic. This eliminates modeling uncertainty at the decoder, which violates the causal structure of the codec and contradicts the information-theoretic setup.\n\nAdditionally, a minor presentational issue: the distinction between the “w/ context” and “w/o context” models in the right panel of Figure 4 is not immediately clear; a brief caption or legend would greatly improve readability."}, "questions": {"value": "I kindly ask the authors to clarify the two major concerns above and, if possible, provide:\n\n- A formal justification (or bounds) showing under what conditions the latent-space distortion analysis accurately approximates the image-space R-D optimum, accounting for the nonlinear transforms.\n\n- Confirmation of the variance implementation and, if $\\sigma^2=y^2$ is indeed used, an explanation of how this remains consistent with the causal decoding process and the theoretical optimum.\n\nAddressing these points would significantly strengthen confidence in the framework’s correctness and practical relevance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pDdx08ofaT", "forum": "YzHbFwYmE1", "replyto": "YzHbFwYmE1", "signatures": ["ICLR.cc/2026/Conference/Submission16816/Reviewer_HEhk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16816/Reviewer_HEhk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989615403, "cdate": 1761989615403, "tmdate": 1762926843864, "mdate": 1762926843864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}