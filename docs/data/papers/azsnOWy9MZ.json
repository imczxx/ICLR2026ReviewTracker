{"id": "azsnOWy9MZ", "number": 15395, "cdate": 1758250926293, "mdate": 1763698120451, "content": {"title": "PLoRA: Efficient LoRA Hyperparameter Tuning for Large Language Models", "abstract": "Low-rank Adaptation (LoRA) has gained popularity as a fine-tuning approach for Large Language Models (LLMs) due to its low resource requirements and good performance. While a plethora of work has investigated improving LoRA serving efficiency by serving multiple LoRAs concurrently, existing methods assume that a wide range of LoRA adapters are available for serving. In our work, we conduct extensive empirical studies to identify that current LoRA training paradigms do not utilize hardware resources efficiently and require high overhead to obtain a performant LoRA adapter. Leveraging these insights, we propose PLoRA, which automatically orchestrates concurrent LoRA fine-tuning jobs under given hardware and model constraints and develops performant kernels to improve training efficiency. Our experimental studies show that PLoRA reduces the makespan of LoRA fine-tuning over a given hyperparameter search space by up to 7.52x and improves training throughput by up to 12.8x across a range of state-of-the-art LLMs.", "tldr": "", "keywords": ["LoRA; Efficiency; Fine-tuning; Scheduling"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/780774d94035e98e2ef95d38d31fd49f5a800337.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents PLoRA, a system for efficient hyperparameter tuning of Low-Rank Adaptation (LoRA) for large language models (LLMs). Unlike prior work focusing on multi-LoRA inference serving, PLORA tackles inefficiencies in LoRA training, especially during hyperparameter sweeps. The key idea is to pack multiple LoRA adapters with distinct hyperparameter settings into a single fine-tuning job, leveraging shared frozen base models to improve GPU utilization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel problem framing.\nThe work identifies an under-explored inefficiency: hyperparameter tuning for LoRA adapters, which has received little systems-level attention compared to LoRA inference. The idea of intra-run concurrent training for multiple LoRAs is well-motivated.\n\n2. Solid systems contribution.\nPLORA provides a principled optimization formulation (NP-complete but approximated with an ILP-based algorithm) and a modular architecture with a packing planner, execution engine, and GPU kernels.\n\n3. Substantial empirical gains.\nThe results are clear and significant: up to 7× shorter tuning time and 12× throughput improvement without quality loss across diverse models and tasks (MRPC, CoLA, GSM8K, WNLI). The experiments are thorough, covering hardware setups, baselines (Min/Max GPU), and sensitivity analyses.\n\n4. Insightful empirical study.\nBefore introducing PLORA, the authors systematically show that LoRA hyperparameters (rank, α, batch size, LR) have strong and task-dependent effects, justifying the need for efficient tuning\n\n5. Clarity and completeness.\nThe system design is clearly illustrated with well-labeled figures and pseudocode. The appendices further include a detailed cost model and memory constraints."}, "weaknesses": {"value": "1. Limited comparison to existing hyperparameter tuning frameworks.\nAlthough the authors position PLORA as orthogonal to Bayesian optimization and other search strategies, some empirical comparison could be helpful.\n\n2. Fairness of baselines.\nThe “Min GPU” and “Max GPU” baselines are simple but may not represent the state of the art in distributed hyperparameter tuning. Showing how PLORA interacts with these would strengthen the systems claim.\n\n3. Evaluation scope.\nThe experiments, though extensive, focus only on medium-scale models (up to 32B). It remains unclear how PLORA scales to >70B or multi-node settings, especially given communication overheads in packed LoRA kernels.\n\n4. Theoretical clarity.\nWhile the optimization formulation is detailed, the paper would benefit from a concise intuitive explanation of the trade-offs (e.g., how packing degree affects convergence noise or training interference). Currently, it’s heavy on equations but light on intuition."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vVboO2l0n5", "forum": "azsnOWy9MZ", "replyto": "azsnOWy9MZ", "signatures": ["ICLR.cc/2026/Conference/Submission15395/Reviewer_wd2r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15395/Reviewer_wd2r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760545620004, "cdate": 1760545620004, "tmdate": 1762925674252, "mdate": 1762925674252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents PLORA, a system designed to drastically reduce the time and computational cost of hyperparameter tuning for RA fine-tuning. Its core innovation is the \"packing\" of multiple, heterogeneous LoRA configurations into a single training job, thereby improving hardware utilization. The work also demonstrates the advantage of using small batch sizes in LoRA fine-tuning, which makes the motivation more plausible."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The approach that packs LoRA configurations to improve hardware utility in hyperparameter tuning makes sense to me.\n2. The empirical speedup in training time is impressive."}, "weaknesses": {"value": "1. Although the authors have claimed through several observations that LoRA fine-tuning sometimes benefits from small batch sizes or configurations, I believe the most straightforward way to prove PLoRA's efficiency is to compare with baselines such as using larger ranks or batch sizes to improve hardware utilization. A lack of direct comparison with baselines like this somehow makes me unsure about whether the proposed techniques are essential.\n2. I am not clear about the total efficiency gain of PLoRA. It seems that hyperparameter tuning usually costs a small fraction of time for the total training procedure. Taking the complete training cost after hyperparameter selection into consideration, I'm afraid PLoRA's efficiency gain in hyperparameter selection phase can be ignored. Maybe I'm wrong, but I expect to see clearer explanations on this issue."}, "questions": {"value": "1. Can the authors give more explanations on the overall gain of PLoRA when considering the total training procedure, including both hyperparameter tuning and normal training?\n2. There has been a lot of LoRA variants in the literature that can have better fine-tuning performance. I wonder if the core mechanism of PLoRA can also be extended to some successful LoRA variants?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "09tim4H1wu", "forum": "azsnOWy9MZ", "replyto": "azsnOWy9MZ", "signatures": ["ICLR.cc/2026/Conference/Submission15395/Reviewer_nryQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15395/Reviewer_nryQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761394040314, "cdate": 1761394040314, "tmdate": 1762925673809, "mdate": 1762925673809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PLoRA, a system designed to accelerate the hyperparameter tuning process for LoRA by addressing hardware underutilization. The authors first provide an empirical study demonstrating the necessity of LoRA hyperparameter tuning and identifying that typical tuning jobs, which often use small batch sizes, lead to inefficient GPU usage. To solve this, PLoRA proposes packing multiple LoRA configurations into a single fine-tuning job. The system comprises an offline packing planner, which uses an optimization algorithm to schedule jobs, and an online execution engine equipped with custom GPU kernels for efficient computation of packed adapters. Experimental results show significant reductions in the overall tuning time (makespan) and improvements in training throughput."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper begins with a thorough empirical study (Section 2) that clearly establishes the problem's significance. By demonstrating that optimal LoRA hyperparameters are task- and model-specific and that fine-tuning often leads to hardware underutilization, the authors provide a compelling justification for their work.\n\n2. PLoRA is a well-designed, end-to-end system that combines high-level scheduling with low-level kernel optimizations. The two-stage approach of an offline planner and an online execution engine is practical, and the development of custom packed LoRA kernels directly addresses the core performance bottleneck. The reported improvements in makespan and throughput are substantial and demonstrate the effectiveness of the proposed approach."}, "weaknesses": {"value": "1. The proposed packing strategy assumes that all configurations within a job are trained for the same duration. This is incompatible with adaptive HPO algorithms like HyperBand or Asynchronous Successive Halving (ASHA), which rely on early termination of unpromising trials to improve efficiency.\n\n2. The scheduler appears to assume that all LoRA configurations in the search space require the same number of training steps. In practice, different configurations (e.g., with different learning rates) may converge at different speeds, or a user might want to train them for different numbers of epochs.\n\n3. The paper compares against \"Min GPU\" and \"Max GPU\" baselines, which represent simple, manual strategies. While reasonable, they do not represent more sophisticated scheduling heuristics. The contribution of the ILP-based planner could be better isolated by comparing it against a simpler, greedy packing algorithm.\n\n4. The planner relies on a recursive algorithm (DTM) that calls an ILP solver. While the paper states the offline planning time is negligible for 120 configurations, this approach may not scale to scenarios with thousands of configurations, which are common in large-scale HPO.\n\n5. The paper claims applicability to other parallelism strategies like FSDP and provides a formulation in the appendix. However, all experiments are conducted with Tensor Parallelism (TP). FSDP, particularly ZeRO-3, has fundamentally different memory and communication patterns that might complicate the packing of heterogeneous LoRA adapters.\n\n6. The planner's decisions are based on a cost model that estimates throughput from the first few training iterations.\n\n7. The custom CUDA kernels are tuned for specific GPU architectures (Ampere). This is a common practice for high-performance systems but limits portability and may require significant engineering effort for new hardware.\n\n8. The main text presents a simplified throughput maximization problem (Eq. 1), while the appendix details a full makespan minimization MILP (Eq. 8). The connection and transition between these two formulations could be made clearer."}, "questions": {"value": "1. How can the PLoRA framework accommodate adaptive HPO strategies that require early stopping? Would it be possible to dynamically dissolve a packed job or stop gradient updates for certain adapters within a pack if they are identified as unpromising?\n\n2. How does the planner handle a search space where configurations have heterogeneous training-length requirements? Does the current model assume a fixed number of steps for all trials, and what would be the implications of relaxing this assumption?\n\n3. The paper compares against \"Min GPU\" and \"Max GPU\" baselines, which represent simple, manual strategies. While reasonable, they do not represent more sophisticated scheduling heuristics. The contribution of the ILP-based planner could be better isolated by comparing it against a simpler, greedy packing algorithm."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Null."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eTvsT4fVKR", "forum": "azsnOWy9MZ", "replyto": "azsnOWy9MZ", "signatures": ["ICLR.cc/2026/Conference/Submission15395/Reviewer_8fpn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15395/Reviewer_8fpn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15395/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761411679042, "cdate": 1761411679042, "tmdate": 1762925673254, "mdate": 1762925673254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}