{"id": "SHdTrhMvVr", "number": 4489, "cdate": 1757689206988, "mdate": 1763119784277, "content": {"title": "BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?", "abstract": "Depth estimation is a fundamental task in computer vision with diverse applications. Recent advancements in deep learning have led to powerful depth foundation models (DFMs), yet their evaluation remains focused merely on geometry accuracy. Given the fact that downstream tasks increasingly rely on depth as guidance, we present BenchDepth, a new benchmark that evaluates DFMs through five carefully selected proxy tasks: depth completion, stereo matching, monocular feed-forward 3D scene reconstruction, SLAM, and vision-language spatial understanding. Our approach assesses DFMs based on their practical utility in real-world applications and provides complementary information to traditional benchmarks. We benchmark eight state-of-the-art DFMs and provide an in-depth analysis of key findings and observations. Interestingly, our results reveal discrepancies between rankings on traditional geometric benchmarks and those on downstream tasks, suggesting that existing evaluation protocols do not fully capture the practical effectiveness of DFMs. This underscores the importance of BenchDepth as a complementary benchmark, bridging the gap between geometry-centric metrics and application-driven evaluation.", "tldr": "", "keywords": ["Depth Estimation", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e27f0fe825cf77a45ccfffbe45d469649639b90c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces BenchDepth, a benchmark designed to evaluate depth foundation models (DFMs) through downstream tasks rather than traditional geometric metrics. The benchmark comprises five proxy tasks: depth completion, stereo matching, 3D scene reconstruction, SLAM, and vision-language spatial reasoning, aiming to capture the practical utility of DFMs in real-world applications. Eight recent DFMs are evaluated, and the authors report that model rankings differ significantly from conventional geometric benchmarks, highlighting a disconnect between numerical geometry metrics and downstream task effectiveness."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets an important and timely question regarding how to evaluate large-scale depth foundation models. Through experimental results, the paper identifies an evaluation gap between geometric accuracy and downstream performance on different tasks, which is an interesting observation.\n\n2. The authors perform a large and comprehensive experimental study, covering multiple downstream tasks and eight modern DFMs. The empirical effort is substantial, and the collected results could serve as a useful reference dataset for the community. I appreciate the authors' effort in delivering the results."}, "weaknesses": {"value": "1. The core research question is posed in the title (“Are we on the right way to evaluate DFMs?”). However, after carefully reading the paper, I find this question never brought up in the paper, never addressed in the paper, never answered in the paper, and never discussed in-depth in the paper. Sure, observations made by the paper (mismatch between current evaluation protocols and downstream performance) are valuable to this question, yet the discussion ends up right there without much high-level or technical insight. The paper reads as an experimental report rather than a proper research study. The paper states, \"We hope that our work will spark further discussion (L119)\", yet the discussion in the paper is minimal. No strong takeaways or principles explain why some DFMs perform better in certain downstream tasks.\n\n2. Following 1, the technical depth is limited. The work mainly aggregates results from existing models and tasks without proposing new evaluation metrics, theories, or methodological innovations. Thus, the paper fails to clarify the intended contribution: is BenchDepth meant as a benchmark/tool or an analytical study? As a benchmark, details about usability, reproducibility, and release are missing. As an analytical study, it only contains shallow observations.\n\n3. The writing and structure are weak. The introduction reads like related work, and the related work reads like an extension of the introduction. As 2. motivation and positioning are underdeveloped."}, "questions": {"value": "1. What specific research question or hypothesis are you addressing?\n\n2. Is BenchDepth publicly released and standardized for reproducible evaluation, or is it a one-time experimental setup?\n\n3. How can future researchers extend or adopt this benchmark. Is there a unified scoring or evaluation pipeline?\n\n4. Can you provide qualitative or conceptual insights explaining why DFMs behave differently across tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XosQdYFcQV", "forum": "SHdTrhMvVr", "replyto": "SHdTrhMvVr", "signatures": ["ICLR.cc/2026/Conference/Submission4489/Reviewer_aK22"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4489/Reviewer_aK22"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755878528, "cdate": 1761755878528, "tmdate": 1762917394334, "mdate": 1762917394334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We thank the reviewers for their comments."}}, "id": "UDYjMcQmNJ", "forum": "SHdTrhMvVr", "replyto": "SHdTrhMvVr", "signatures": ["ICLR.cc/2026/Conference/Submission4489/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4489/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763119783378, "cdate": 1763119783378, "tmdate": 1763119783378, "mdate": 1763119783378, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper proposes a novel benchmark for evaluating monocular reconstruction, based on five tasks. They perform correlation analysis over 8 models and 6 tasks, and show that four of them are strongly correlated and represent a coherent benchmark when averaged.\n\nThis is an interesting paper, and I like the direction, but it is incomplete.\n\nThe paper reports results for five results, one is excluded from the benchmark results in figure 1, and the two negative tasks are not reported.\n\nThe average of the positively correlated tasks is not reported, and presumably this is the actual benchmark. As such, no one can compare new methods on the benchmark."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Improved benchmarking in monocular reconstruction is very welcome. \n\nThe use of correlation to identify a coherent subset of tasks that make a benchmark is sensible.\n\nGenerally, I appreciate the design decisions, and I think this will make a valuable contribution to the literature when it is finished."}, "weaknesses": {"value": "The coherence of the benchmark depends on the correlation analysis highlighted in Figure 1, and this is incomplete.\n\nTask 5 in the text (VLM Spatial Understanding) is excluded from the figure, and the justification for excluding it doesn't really make sense. While the measure \"Pos\" shows little variation between the models, many of the other metrics show extremely large shifts. \n\nTasks 5,6,7 in Figure 1 (average, traditional delta, and traditional absrel) are not reported in the text.\n\nIn addition to this, the correlation analysis is performed using only 8 models. This doesn't automatically invalidate the statistics, but it means that the authors need to be very careful that one outlier isn't dominating the analysis. The authors need to generate plots for each pair of benchmarks in the analysis and confirm that the expected monotonic relationship exists. \n\nOne model is excluded from the SLAM benchmark table, Metric3DV2 and it is not clear how this was dealt with in the correlation analysis.\n\nThe negatively correlated benchmarks are also only reported on a subset of models, in other papers, and it's not clear how this altered the correlation analysis. The authors really need to bite the bullet and run all models on these benchmarks."}, "questions": {"value": "The weakness contains a list of the issues I'm concerned with. It would be helpful if these were either addressed or you explained why they didn't matter.\n\nOn top of this, the writing needs tightening. \n\n>Our correlation analysis (Fig. 1) shows stronger consistency among proxy tasks (e.g., depth\ncompletion and SLAM: 0.88), indicating that the selected five tasks collectively form a\nrepresentative and coherent benchmark.\n\nThe correlation of selected measures, combined with the negative correlation of other measures, indicates that the benchmark is *not* representative of standard tasks in the literature. By no means is this a bad thing. Coherence is probably a better indication of benchmark usefulness. It's better to measure one thing well rather than a couple of things badly.\n\nIt's also not clear how meaningful the negative correlation in previous tasks is. Presumably, we could introduce trivial models such as constant depth, or even older models that perform badly on all tasks. This would then result in a positive correlation on all tasks. \n\nIs this negative correlation just an example of a publication bias resulting in a collider bias? To be published, you need to be tuned to perform well on one of these types of task but not both, and that results in an apparent negative correlation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CrczcpUiG7", "forum": "SHdTrhMvVr", "replyto": "SHdTrhMvVr", "signatures": ["ICLR.cc/2026/Conference/Submission4489/Reviewer_fe6D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4489/Reviewer_fe6D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841176891, "cdate": 1761841176891, "tmdate": 1762917393870, "mdate": 1762917393870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BenchDepth to evaluate depth foundation models based on downstream task performance, rather than traditional geometric accuracy metrics.\nTo assess how well DFMs perform in downstream tasks, the authors propose five proxy tasks: depth completion, stereo matching, feed-forward 3D reconstruction, SLAM, and vision-language spatial understanding.\nResults show weak or even negative correlations between traditional geometric metrics and downstream task performance. The authors claim that geometry-centric evaluation may not reflect real-world effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper shifts the benchmark focus from geometry-based metrics to application-driven performance, which brings a new insight for the community.\n2. The benchmark covers both low-level and high-level depth tasks, which provides a multi-layered perspective on depth model utility. \n3. The paper presents detailed quantitative analysis and interesting observations, such as strong internal consistency between proxy tasks but weak correlation with traditional benchmarks.\n4. The multiple aspects of this benchmark could serve for evaluating DFMs beyond pure geometric fidelity for other works."}, "weaknesses": {"value": "1. Depth estimation is fundamentally and inherently a metric prediction problem (Unlike LLMs and VLMs mentioned in line 49 – 52, whose evaluation rely on plausibility or relevance rather than quantitative accuracy). Thus, any evaluation suggesting the opposite conclusion must explain why geometric fidelity loses its predictive power. The authors fail to provide a compelling theoretical basis for this discrepancy, leaving open the possibility that the issue comes from their task integration rather than the metric itself.\n2. Many downstream tasks such as SLAM and 3DGS contain complex neural components that can overshadow the role of depth inputs. As a result, the reported low correlation might reflect task noise rather than a true limitation of geometric benchmarks. Deeper analysis is still needed for this paper.\n3. The benchmark only coversdoes not cover recent DFMs, e.g., CH3Depth [1]\n[1] CH3Depth: Efficient and Flexible Depth Foundation Model with Flow Matching.\n4. The paper is almost entirely empirical. The conceptual analysis of which geometric properties (such as scale, continuity, edge fidelity) actually matter for different tasks may build a bridge between geometric accuracy and downstream task performance, which is needed in this paper.\n5. The authors themselves acknowledge that all DFMs perform similarly in VLM spatial understanding. This reduces the claimed significance of BenchDepth, as high-level reasoning tasks do not meaningfully discriminate model quality."}, "questions": {"value": "1. Can you provide a principled explanation or analysis (beyond empirical observation) for the decoupling between geometric accuracy and downstream performance?\n2. Have you tested, if replacing the depth maps with noisy or random ones yields similar weak correlations, to confirm that the benchmark truly measures depth utility rather than task variance? Many downstream networks may not be sensitive to the input depth maps. If that is the case, then even when provided with random or noisy depth maps, they might still produce similar outputs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Cgd2Mjz66K", "forum": "SHdTrhMvVr", "replyto": "SHdTrhMvVr", "signatures": ["ICLR.cc/2026/Conference/Submission4489/Reviewer_AfUP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4489/Reviewer_AfUP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912612083, "cdate": 1761912612083, "tmdate": 1762917393502, "mdate": 1762917393502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors presented a benchmark framework evaluating depth foundation models (DFMs) across a diverse set of downstream proxy tasks (depth completion, stereo matching, 3D Gaussian splatting reconstruction, learning-based SLAM, and VLM spatial reasoning). The benchmark aims to test practical utility rather than purely pixelwise accuracy, and the authors report that performance rankings under downstream tasks differ from those captured by standard depth metrics."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work addresses a well-recognized gap: depth metrics (e.g., AbsRel, RMSE, δ thresholds) do not reliably predict whether a depth model is useful when integrated into 3D or geometric pipelines. Positioning evaluation around actual deployment tasks is well justified.\n\n2. Diverse and well-chosen downstream tasks. The inclusion of tasks spanning low-level to high-level reasoning adds credibility to the claim that the benchmark captures general utility rather than performance tuned to a particular application."}, "weaknesses": {"value": "1. Interpretation of the reported negative correlation between traditional metrics and downstream rankings requires clarification. The paper claims that standard depth metrics negatively correlate with downstream task performance. Taken at face value, this would imply that models performing worse under standard metrics could perform better in real applications, which is conceptually difficult to justify. \n\n2. The benchmark only uses one baseline architecture per task. Performance may depend on model–pipeline interactions (e.g., certain DFMs may integrate more naturally into particular stereo or SLAM architectures). Using a single model per task risks overstating generality of the results.\n\n3. There is a potential dataset overlap between DFMs’ training data and downstream task datasets. Some DFMs could have been trained or fine-tuned on datasets closely related to those used in the downstream evaluations. This may weaken conclusions, as models might benefit from dataset familiarity rather than intrinsic superiority as transferrable depth priors."}, "questions": {"value": "1. Can you clarify on negative correlations between proposed benchmarks and standard metrics? This is my main concern that needs to be addressed.\n2. Can you provide a table explicitly listing training datasets for each DFM and indicating where they overlap with each downstream task’s evaluation data?\n3. Did you consider re-running one downstream task using an alternative baseline architecture to check whether performance rankings are stable across pipelines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8SfchucIJS", "forum": "SHdTrhMvVr", "replyto": "SHdTrhMvVr", "signatures": ["ICLR.cc/2026/Conference/Submission4489/Reviewer_Xkaj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4489/Reviewer_Xkaj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954715160, "cdate": 1761954715160, "tmdate": 1762917393207, "mdate": 1762917393207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}