{"id": "bxC7wUMjTQ", "number": 3404, "cdate": 1757419080755, "mdate": 1759898091645, "content": {"title": "Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional Manipulation Attacks", "abstract": "Proving the compliance of AI algorithms has become an important challenge with the growing deployment of such algorithms for real-life applications. Inspecting possible biased behaviors is mandatory to satisfy the constraints of the regulations of the EU Artificial Intelligenceâ€™s Act. Regulation driven audits increasingly rely on global fairness metrics, with Disparate Impact being the most widely used. Yet such global measures depend highly on the distribution of the sample on which the measures are computed. We investigate first how to manipulate data samples to artificially satisfy fairness criteria, creating minimally perturbed datasets that remain statistically indistinguishable from the original distribution while satisfying prescribed fairness constraints. Then we study how to detect such manipulation. Our analysis (i) introduces mathematically sound methods for modifying empirical distributions under fairness constraints using entropic or optimal transport projections, (ii) examines how an auditee could potentially circumvent fairness inspections, and (iii) offers recommendations to help auditors detect such data manipulations. These results are validated through experiments on classical tabular datasets in bias detection. The code is available at https://anonymous.4open.science/r/Inspection-76D6/.", "tldr": "Auditing vulnerabilities to distributional manipulation attacks", "keywords": ["Auditing", "Fairness", "Fair-washing", "Representativeness", "Manipulation", "Regulation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/878e40f0dcb9f082515de8632043a91a56eb010d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates first how to manipulate data samples to artificially satisfy fairness criteria, creating minimally perturbed datasets that remain statistically indistinguishable from the original distribution while satisfying prescribed fairness constraints.\nThen the authors formalize a three-party auditing framework: an auditee who provides a data sample, an auditor who checks the sample for fairness, and a supervisory authority who verifies that the auditee's sample is statistically representative of the auditee's complete dataset."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses a critical problem. Exposing the vulnerabilities of the audits before they are widely deployed is significant.\n\n- The authors formalize the \"fair-washing\" attack and model it as a constrained optimization problem."}, "weaknesses": {"value": "- The paper's idea is a three-party framework where the \"supervisory authority\" has access to the auditee's complete, original dataset. This is a very strong assumption that feels contradictory. If the authority already has the full, true dataset, it is unclear why they would bother analyzing a sample provided by the auditee.\n\n- The proposed defense relies on tests like the Wasserstein distance and MMD. These tests are difficult and expensive to compute in high dimensions.\n\n- The paper's methodology is entirely focused on manipulating a single fairness metric: Disparate Impact. It is not known how the generalizability of the method would be for other fairness metrics.\n\n- There is no analysis of the model's accuracy and fairness trade-off.\n\n- There are limited experiments on non-tabular datasets."}, "questions": {"value": "See comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3Y9UnXtB71", "forum": "bxC7wUMjTQ", "replyto": "bxC7wUMjTQ", "signatures": ["ICLR.cc/2026/Conference/Submission3404/Reviewer_A4iy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3404/Reviewer_A4iy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760861900743, "cdate": 1760861900743, "tmdate": 1762916707971, "mdate": 1762916707971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies and introduces various fairwashing methods that can manipulate fairness audits. Most of the focus of the paper is on tabular data and disparate impact as the fairness metric. Authors perform experiments to showcase the effectiveness of these methods using various datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper studies an important topic.\n2. For the experiments authors considered a wide range of datasets."}, "weaknesses": {"value": "1. Majority of the focus and experiments are on Disparate Impact. It would be nice if authors expand their findings and results on other fairness metrics as well.\n2. Majority of the focus and experiments are on tabular data. While authors briefly talk about image data and some results in the appendix, I think it would be good if these discussions and experiments are extended and brought in the main body of the paper. This will strengthen the paper and improve its scope and impact.\n3. It would be good if authors consider other fairwashing approaches as baselines and compare their proposed approaches to them.\n4. It would have been interesting if authors could propose stronger and more novel detection mechanisms that could apply to a wide range of existing fairwashing mechanisms. It might seem like the proposed solutions for detection might be inspired by the proposed fairwashing mechanisms in this paper. However, it is unclear how the proposed detection mechanisms would apply to other existing fairwashing approaches. A more comprehensive discussion and experiments on this can be helpful.\n5. The paper had room to improve its writing to be more clear.\n6. Some figures were hard to read (e.g., Figures 3 and 4).\n7. The paper might have some ethical concerns and can be misused by the adversaries."}, "questions": {"value": "How much effective do you think the proposed defense/detection mechanism could be on other existing fairwashing methods? In other words, how generalizable these methods are?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Potentially harmful insights, methodologies and applications"]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yBjdDspBqJ", "forum": "bxC7wUMjTQ", "replyto": "bxC7wUMjTQ", "signatures": ["ICLR.cc/2026/Conference/Submission3404/Reviewer_bM5M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3404/Reviewer_bM5M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761693879745, "cdate": 1761693879745, "tmdate": 1762916707493, "mdate": 1762916707493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission discusses the problem how fairness audits can be influenced by (small) manipulations of the provided data set. The work adopts a setting where an auditor is meant to evaluate the fairness of a model, based on a random subset of data provided by the audited entity. The work discusses different ways for the audited entity to minimally change the sampling distribution to achieves a given level of (disparate impact) fairness. The work also discusses techniques to identify such manipulations. Experimental results are reported on standard tabular datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ the problem of reliably auditing classifier for fairness is timely and relevant\n+ the proposed optimization problems make sense and can indeed be expected to result in hard-to-detect sampling distributions\n+ source code and detailed appendices with formal derivations are provided"}, "weaknesses": {"value": "The manuscript has a number of weaknesses and aspects that are unclear to me.\n\n* Motivation\n\nThe justification for the exact problem setting is unclear to me. \nThe work adopt a setting where an audited entity (let's call it $E$) gives an auditor ($A$) access to a random subset of its data. A supervisor ($S$) is meant to check that this subset is representative. For this, $S$ gets access to $E$'s complete dataset. \n\nMy concern is: if $E$ is potentially dishonest and manipulates the subset it gives to $A$, why would it not manipulate the (presumably) complete data it provides to $E$? Is there a realistic setting where this setup applies? Also, why would $E$ choose to manipulate the data *distribution* and then sample from it, instead of creating a truly adversarial dataset (as e.g. in [Konstantinov et al, JMLR 2022] for fair learning)? \n\n* Scientific contribution\n\nI find the technical depth of the contribution limited. The four presented methods are straight-forward realization of the principle of finding a distribution with certain (fairness) constraints, where only the similarity measure between distributions changes. Most interesting is Proposition 3.2, which provided explicit expressions, which however are the result of applying a previously existing theorem to the special case of fairness. \nThe Wasserstein construction (Section 3.3) would be intersting because of its ability to move probability mass to samples not in the reference set, but in the chosen setup where the reference set is available to the auditor, this becomes easily detectable. \nThe remaining two methods consist of elementary flips of labels or protected attributes. \n\nBesides the technical depth, also the presentation of the methods could be improved. It relies mostly on textual descriptions and remarks, where technical precision would benefit from formal definitions and theorems. Also, in several places special cases are discussed, but afterwards a claim is made that more general cases could also be handled. \n\nSection 3.4 on detection method is too superficial. It mentions statistical tests, but fails to discuss them properly, such as what exactly such a test can determine, the role of test power or significance levels.\n\n\n* Experimental evaluation\n\nThe setting of the experiments makes sense, but the description is insufficient for reproducibility. For the statistical test, the significance level is not reported. \n\n+ reported numeric values lack uncertainty/error bars\n+ the absolute numbers of samples is not reported, only percentages\n+ the plots have too small fonts in captions and legends\n+ the description repeatedly makes statements such as \"verify the null hypothesis\". Statistical tests can *never* verify the null hypothesis. Either they reject it, or they lack power to reject it. \n\n\n* Additional Comments\n\n+ \\citep and \\citet should be used appropriately\n\n- The paragraph \"Entropic distributional projection\" (Section 3.2) looks like an almost verbatim copy from (Bachoc etal, 2023), including the phrase \"stress deformation\" which is central in Bachoc's work but does not appear in the submission otherwise. \n\n- Several reference point to not actually existing papers. Specifically I spotted e.g. (Aivodji et al, 2019, (Anders et al, 2020), (Shamsabadi et al 2023), (Le Merrer et al, 2020), where at least author list, title and/or publication venues do not match. This hints to the use of LLMs in the preparation of the related work section, which isn't explicitly declared. \n\n- The work describes multiple methods to actively mislead a privacy audit, while spending almost no effort on improving detection methods. This clearly raises ethical concerns, and the provided ethics statement does not do a good job to dispell these. Also, it argues that the manuscript deliberately omitted full implementation details, but the effect of this is simply that the work becomes not reproducible."}, "questions": {"value": "- could you give an example where the described problem setting (see concern in \"Motivation\") could apply to a real-world setting?\n- has an LLM been used to create references?\n- what's the significance level of the statistical tests, e.g. of Table 3?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications", "Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "1) Potentially harmful insights, methodologies and applications\n\nThe manuscript describes a method to actively support a malicious party to manipulate fairness audits.\n\n2) Research integrity issues (e.g., plagiarism, dual submission)\n\nThe paragraph \"Entropic distributional projection\" (Section 3.2) looks like an almost verbatim copy from (Bachoc etal, 2023), including e.g. the phrase \"stress deformation\" which is central in Bachoc's work but does not appear in the submission otherwise."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pK05iuMQqC", "forum": "bxC7wUMjTQ", "replyto": "bxC7wUMjTQ", "signatures": ["ICLR.cc/2026/Conference/Submission3404/Reviewer_PYiv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3404/Reviewer_PYiv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3404/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941646181, "cdate": 1761941646181, "tmdate": 1762916706402, "mdate": 1762916706402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}