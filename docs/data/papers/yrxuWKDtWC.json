{"id": "yrxuWKDtWC", "number": 12902, "cdate": 1758211461927, "mdate": 1758806369052, "content": {"title": "GraphFusion: Adaptive Label Enhancement with Heterophily-Aware Attention and Dynamic Residual Calibration", "abstract": "We propose GraphFusion, a unified and scalable framework for graph-based semi-supervised learning that integrates high-order feature augmentation, contrastive self-supervised pretraining, and transformer-style attention. GraphFusion enhances node representations using multi-hop SIGN embeddings and contrastive alignment, followed by a lightweight Graphormer module with heterophily-aware attention. To improve label propagation, we introduce a dynamic residual calibration mechanism that adaptively adjusts smoothing strength per node based on model confidence. Our framework supports multi-label classification and ensemble fusion across multiple pathways. Extensive experiments on OGBN-ArXiv, OGBN-Products, and OGBN-Proteins demonstrate that GraphFusion outperforms strong baselines in both homophilous and heterophilous settings, while maintaining high efficiency and interpretability.", "tldr": "A scalable graph learning framework combining contrastive pretraining, heterophily-aware attention, and dynamic label smoothing for robust node classification.", "keywords": ["graph neural networks", "semi-supervised learning", "label propagation", "heterophily-aware attention", "contrastive learning", "dynamic residual calibration", "graph transformers"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "", "supplementary_material": ""}, "replies": [], "withdrawn": true}