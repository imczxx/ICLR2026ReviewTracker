{"id": "sCL5mSTpKm", "number": 12545, "cdate": 1758208487421, "mdate": 1759897502543, "content": {"title": "All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning", "abstract": "From a first-principles perspective, it may seem odd that the strongest results in foundation model fine-tuning (FT) are achieved via a relatively complex, two-stage training procedure. Specifically, one first trains a reward model (RM) on some dataset (e.g., human preferences) before using it to provide *online* feedback as part of a downstream reinforcement learning (RL) procedure, rather than directly optimizing the policy parameters on said dataset via *offline* maximum likelihood estimation. In fact, from an information-theoretic perspective, we can only *lose* information via passing through a reward model and cannot create any new information via on-policy sampling. To explain this discrepancy, we scrutinize several hypotheses on the value of RL in FT through both theoretical and empirical lenses. Of the hypotheses considered, we find the most support for the explanation that on problems with a *generation-verification gap*, *(1)* it is relatively easy to learn the relatively simple RM (*verifier*) from the preference data. Then, *(2)* the downstream RL procedure only returns policies (*generators*) that are optimal for such relatively simple verifiers. Thus, end-to-end, two-stage online FT only has to search over a reduced subset of the full space of policies, requiring less data than offline FT.", "tldr": "We provide theoretical and experimental support for the hypothesis that the value of RL in fine-tuning is fundamentally derived from generation-verification gaps.", "keywords": ["reinforcement learning", "RLHF", "fine-tuning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3a031437412c255f006f1be528dde5e805d12892.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper discusses several hypotheses for the benefit of online fine-turning in preference-based RL, focussing on large language models.   Starting from a unified objectives for online and offline fine-tuning, it shows that both online and offline fine-tuning can be seen as maximizing data likelihood, although for the online setting the learned policies is within a restricted policy class induced by learned reward models. Then, this paper raises a new assumption for the benefit of online fine-tuning: simpler reward models induce smaller policy classes, which turns out to have better performance. Finally, the paper offers evidence for the benefit of using simpler reward models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Overall, the presentation of this paper is easy to follow.\n\n2. The discussion through the lens of maximum likelihood estimation is suitable for the LLM setting and offers new insight for understanding the benefit of online fine-tuning.\n\n3. In particular, the assumption of the connection between simper reward models and better performance is interesting and worth investigation. For example, verifiable rewards used for training reasoning models are also simple ones."}, "weaknesses": {"value": "1. The presentation of H6, which is the core assumption made in this paper, is hard to understand in the first glimpse. I would say that $\\Pi(R_\\text{sim})\\subset \\Pi$ is a general statement, since $R_\\text{sim}$ is unlikely to cover **all possible reward functions**. A better way for stating the benefit of using small reward models would strengthen this paper. Alternatively, you can define $R$ to be all the reward function consisting with the preferences, so that $\\Pi$ will be all the policies that can generate the preferences.\n\n2. The empirical justification is very limited. Since the theoretical justifications in this paper are straight-forward and rely on idea conditions, it is better to include more empirical results."}, "questions": {"value": "1.  beside the summary task being considered, is there any other task that also has generation-verification gap? \n\n2. Is the assumption H6 valid for tasks where the generation content is longer than the prompt?\n\n3. Why do you use BoN in Fig.4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "530cIcfz7i", "forum": "sCL5mSTpKm", "replyto": "sCL5mSTpKm", "signatures": ["ICLR.cc/2026/Conference/Submission12545/Reviewer_J9QF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12545/Reviewer_J9QF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761627341717, "cdate": 1761627341717, "tmdate": 1762923406482, "mdate": 1762923406482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why two-stage fine-tuning (FT) procedures—training a reward model (RM) followed by reinforcement learning (RL)—often outperform direct offline optimization, despite appearing information-theoretically inefficient.  They propose the hypothesis that such a phenomenon is due to the generation-verification gap."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This paper appears to be the first to investigate the underlying reason why the two-stage fine-tuning procedure outperforms purely offline approaches. The proposed hypothesis is well-motivated, and the authors provide a theoretical analysis to support it. Furthermore, they conduct extensive numerical experiments that not only reinforce their hypothesis but also help rule out alternative explanations."}, "weaknesses": {"value": "Overall, I do not have major concerns about this article. The core idea is clearly articulated, and the presentation is well-structured and easy to follow. My only reservation is whether the contribution is substantial enough for publication at ICLR. Although the paper spans nine pages, a significant portion is devoted to setup and related work, with the main theoretical contribution centered on Theorem 3.1.  I would suggest that some proof sketches and key arguments be included in the main text."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hw73MhATTl", "forum": "sCL5mSTpKm", "replyto": "sCL5mSTpKm", "signatures": ["ICLR.cc/2026/Conference/Submission12545/Reviewer_sSEo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12545/Reviewer_sSEo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700182097, "cdate": 1761700182097, "tmdate": 1762923406126, "mdate": 1762923406126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the question of why do complex, two-stage online methods (like RLHF) empirically outperform simpler, direct offline methods (like DPO), even when they both optimize the same likelihood-based objective?\n\nThe paper's first key finding is theoretical: when the policy and reward model function classes are isomorphic (i.e., $\\mathcal{R} = \\mathcal{R}(\\Pi)$), the optimal solutions for both online and offline methods are identical (Theorems 2.2 & 2.3). This theoretical equivalence contradicts robust empirical findings.\n\nThe paper then systematically conducts controlled experiments to rule out several common hypotheses and propose a more suitable hypothesis: the generation-verification gap."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The generation-verification gap is a nice conceptual contribution. It reframes the online vs. offline debate to a root cause a \"statistical efficiency\" problem.\n\n\nThe paper is a model of good scientific reasoning. It cleanly formalises the theory-practice gap (Theorems 2.2/2.3) and then treats various explanations (e.g., optimization, regularization, OOD) as falsifiable hypotheses.\n\nThe paper's best evidence comes from its two \"gap-closing\" experiments. Predicting that the online PFT advantage would disappear on a bandit-like task (Fig 5) and a ROUGE-L task (Fig 6) is not obvious. \n\nThe authors online DPO setup -- where an RM is used to re-label on-policy data, which is then fed into the same DPO loss is a clever way to isolate the core variable. It successfully controls for confounders like the optimization algorithm (PPO vs. DPO loss), ensuring the comparison is truly about the two-stage process versus the one-stage process."}, "weaknesses": {"value": "1) The \"Simplicity\" of Verification is a Black Box: The entire argument of H-6 hinges on the assertion that a verifier is simpler than a generator. This central concept of simplicity is never formally defined.\n\n2) The paper frames H-6 as the sole surviving explanation. This seems unlikely to be true. It's more plausible that other factors are also true and compound the effect. For example, for H-5  (OOD Generalization), Fig 9, Fig 10 show that global RMs do generalize better (both in-distribution and OOD) than local RMs. The paper claims H-6 causes this. But it's equally plausible that better OOD generalization is a separate benefit of the global RM architecture, which then adds to the statistical efficiency benefit of H-6.\n\n3) How does the paper conceptually differ from [1] which seem to propose a similar argument?\n\n4) The initial theoretical question is built on the isomorphism between policies and reward functions. But in practice offline DPO uses a local RM (architecturally tied to the policy, $r_{\\pi} = \\sum \\log \\pi$) and online RLHF uses a global RM (using the final hidden state of the full sequence). Does this create a confounder?\n\n\n[1] Self-Improvement in Language Models: The Sharpening Mechanism"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IlGO5sm6A6", "forum": "sCL5mSTpKm", "replyto": "sCL5mSTpKm", "signatures": ["ICLR.cc/2026/Conference/Submission12545/Reviewer_Ypdj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12545/Reviewer_Ypdj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935584421, "cdate": 1761935584421, "tmdate": 1762923405596, "mdate": 1762923405596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to provide an intuitive explanation for the empirically observed performance gap between two-stage (e.g., RLHF, online DPO) and direct (e.g., DPO, offline DPO) preference fine-tuning methods. Through a combination of theoretical analysis and empirical studies, the authors evaluate multiple hypotheses concerning the value of reinforcement learning in two-stage approaches. Among the tested hypotheses, only the generation–verification gap (it is easier to learn a simple reward model from preference data and optimize policies for simple verifiers) remained supported by the evidence. The paper concludes that two-stage methods effectively operate over a reduced policy space, requiring less data than offline fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written, and the results are rigorous.\n\nIt makes both theoretical and empirical contributions to understanding the origins of the performance gap between online and offline preference fine-tuning.\n- shows that when policy and reward classes match, online and offline methods share the same set of optima.\n- falsifies several existing hypotheses about the benefits of RL in preference fine-tuning and introduces the generation-verification gap as a plausible alternative explanation.\n\nProvides useful practical insights:\n- for problems where verification is simpler than generation, two-stage methods are preferable.\n- for tasks requiring long-horizon reasoning or complex planning, the gap between online and offline approaches is likely to widen.\n\n(Note: I have not carefully checked the additional hypotheses in the appendix.)"}, "weaknesses": {"value": "Potential related work: Nika et al. (2024) conducted a comparative theoretical analysis of RLHF and DPO, highlighting the generation-verification gap as a key factor explaining when RLHF statistically outperforms DPO. They show that when the reward class is of lower complexity than the policy class, RLHF tends to perform better, consistent with this paper's findings.\n\nNika et al., 2024. Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences. ICML, 2024."}, "questions": {"value": "Clarification question: \n\nThe experiments supporting the generation-verification gap hypothesis show:\n- cases where verification is easier than generation, and online methods outperform offline ones;\n- cases where complex reward functions diminish the benefit of online techniques.\n\nIn LLM preference fine-tuning, even when reward and policy classes have comparable complexity, do we still observe a gap favoring online methods? If that is the case, is the generation-verification gap alone sufficient to explain this phenomenon, or am I missing something?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fB5hSUv8Mh", "forum": "sCL5mSTpKm", "replyto": "sCL5mSTpKm", "signatures": ["ICLR.cc/2026/Conference/Submission12545/Reviewer_Caqs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12545/Reviewer_Caqs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985543585, "cdate": 1761985543585, "tmdate": 1762923405153, "mdate": 1762923405153, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}