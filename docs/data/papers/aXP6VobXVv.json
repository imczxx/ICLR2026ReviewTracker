{"id": "aXP6VobXVv", "number": 5971, "cdate": 1757948954634, "mdate": 1759897941869, "content": {"title": "Resource Consumption Red-Teaming for Large Vision-Language Models", "abstract": "Resource Consumption Attacks (RCAs) have emerged as a significant threat to the deployment of Large Language Models (LLMs).\nWith the integration of vision modalities, additional attack vectors exacerbate the risk of RCAs in large vision-language models (LVLMs). \nHowever, existing red-teaming studies have mainly overlooked visual inputs as a potential attack surface, resulting in insufficient mitigation strategies against RCAs in LVLMs.\nTo address this gap, we propose RECITE (Resource Consumption Red-Teaming for LVLMs), the first approach for exploiting visual modalities to trigger unbounded RCAs red-teaming.\nFirst, we present Vision Guided Optimization, a fine-grained pixel-level optimization to obtain Output Recall Objective adversarial perturbations, which can induce repeating output.\nThen, we inject the perturbations into visual inputs, triggering unbounded generations to achieve the goal of RCAs.\nEmpirical results demonstrate that RECITE increases service response latency by over 26×$\\uparrow$, resulting in an additional 20\\% increase in GPU utilization and memory consumption. \nOur study reveals security vulnerabilities in LVLMs and establishes a red-teaming framework that can facilitate the development of future defenses against RCAs.", "tldr": "RECITE exploits visual inputs to trigger Resource Consumption Attacks (RCAs) on LVLMs, exposing critical security vulnerabilities.", "keywords": ["LVLMs", "Resource consumption", "red team"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0845c924a259d057eaa4cef15e0acbb83ae1df03.pdf", "supplementary_material": "/attachment/624c9caf43032ccf7f945b7131296509f69260a8.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces RECITE, a novel method for resource consumption attacks (RCA) on vision-language models (VLMs), leveraging Vision Guided Optimization. The study empirically demonstrates that RECITE can increase response latency by over 26 times and enhance GPU utilization by 20%. The experiments are extensive and well-conducted."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposed first method for exploiting visual input on RCAs.\n2. The paper is well-structured and it is easy to follow.\n3. The authors provide a comprehensive analysis, evaluating the attack's effectiveness across 3 VLMs.\n4. The paper also include potential defense strategy, further enhancing their work."}, "weaknesses": {"value": "1. The paper lacks experiments on transferability. The authors should evaluate the transferability of the optimized visual inputs, both across different white-box models and by using inputs optimized on a white-box model to attack a black-box model.\n2. The paper has mentioned defense strategy. If diffusion purification or random smoothing were applied directly to the optimized inputs, would they have any defensive effect?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aHSuN0EclQ", "forum": "aXP6VobXVv", "replyto": "aXP6VobXVv", "signatures": ["ICLR.cc/2026/Conference/Submission5971/Reviewer_hxX8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5971/Reviewer_hxX8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761297007289, "cdate": 1761297007289, "tmdate": 1762918383630, "mdate": 1762918383630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates visual-modality-driven resource consumption attacks (RCAs) on large vision-language models (LVLMs). The authors propose RECITE, a red-teaming framework that perturbs input images using an Output Recall Objective and Vision-Guided Optimization. The resulting perturbations are nearly imperceptible but cause the model to generate extremely long or looping outputs, significantly increasing GPU usage and latency. Experiments across multiple LVLMs demonstrate the feasibility of the attack and suggest preliminary defense strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a new attack surface—visual inputs causing resource consumption—which has not been systematically explored before. This problem is both novel and practically relevant.\n\n- The proposed RECITE framework is simple yet effective, providing a structured way to red-team LVLMs for resource-related vulnerabilities.\n\n- The experimental validation is extensive, involving multiple models and metrics (output length, GPU utilization, latency, memory). The results strongly support the main claim."}, "weaknesses": {"value": "- The theoretical explanation of why visual perturbations cause looping behavior is insufficient. The paper would benefit from a formal analysis of the model’s stopping dynamics, such as EOS logit suppression or entropy evolution.\n\n- The Output Recall Objective is largely heuristic. There is no ablation comparing it to simpler baselines such as minimizing the EOS token probability or tuning length penalties, which makes it unclear how necessary this specific objective is.\n\n- The defense section is underdeveloped. The proposed sliding-window penalty lacks depth, and there is no quantitative analysis of how it affects model accuracy or normal captioning tasks.\n\n- The evaluation scope is narrow, limited to open-source LVLMs. Testing on closed-source or API-based models (e.g., Gemini or GPT-4V) would make the results more compelling.\n\n- The generality of the attack is uncertain. The paper does not explore transferability across models or tasks, nor does it test whether a single universal perturbation could generalize to multiple models."}, "questions": {"value": "Have the authors considered testing on black-box, closed-source models to validate the practical impact in deployed systems?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nNNHK8r4fE", "forum": "aXP6VobXVv", "replyto": "aXP6VobXVv", "signatures": ["ICLR.cc/2026/Conference/Submission5971/Reviewer_QQcr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5971/Reviewer_QQcr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737769766, "cdate": 1761737769766, "tmdate": 1762918383175, "mdate": 1762918383175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RECITE (Resource Consumption Red-Teaming for LVLMs), a red-teaming framework that exploits visual inputs to trigger unbounded resource consumption attacks (RCAs) in large vision-language models (LVLMs). The core idea is to craft imperceptible adversarial perturbations that induce the model to enter a repetitive generation loop, e.g., outputting “cup cup cup...” indefinitely, thereby exhausting GPU memory and latency. Experimental results demonstrate that RECITE increases service response latency by over 26×."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper demonstrates that visual inputs alone can reliably trigger severe resource consumption attacks (RCAs) in large vision-language models (LVLMs).\n2. The authors conduct extensive experiments across seven LVLMs from three major families (LLaVA, Qwen, BLIP), using diverse metrics (Output Time GPU Utilization Memory Usage) and multiple attack configurations.\n3. The method section is technically thorough, with precise definitions of the Output Recall Objective and Vision Guided Optimization."}, "weaknesses": {"value": "1. The claim that this is the “first” vision-based resource consumption red-teaming for LVLMs appears overstated. Prior work such as Gao et al. (ICLR 2024) [1] also leverages visual inputs to induce high latency/energy consumption in LVLMs. The paper should clarify how RE-CITE differs conceptually and technically from such approaches.\n\n2. Figure 1, which depicts the RE-CITE pipeline, lacks sufficient clarity. Key components—such as visual encoding, embedding projection, and the iterative perturbation update process—are not well differentiated. \n\n3. The core components demonstrates limited technical novelty. The Output Recall Objective seems to only define repetitive generation patterns and  the Vision Guided Optimization appear to be straightforward adaptations of the existing GCG method.\n\n4. The evaluation only compares against GCG-RCAs. Given that [1] (Gao et al., ICLR 2024) also targets visual RCAs, it should be included as a baseline to better position RE-CITE’s relative effectiveness and novelty.\n\n5. The paper does not assess whether perturbed images generated for one model transfer to others (e.g., perturbations optimized on LLaVA tested on Qwen). Such transferability is critical and should be evaluated.\n\n6.  It remains unclear whether the generated perturbations are effective against deployed LVLM services (e.g., GPT4-V, Claude, or Qwen API)\n\n[1] Gao K, Bai Y, Gu J, et al. Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images[C]//The Twelfth International Conference on Learning Representations."}, "questions": {"value": "Please address the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qWHocnnTRm", "forum": "aXP6VobXVv", "replyto": "aXP6VobXVv", "signatures": ["ICLR.cc/2026/Conference/Submission5971/Reviewer_p9fs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5971/Reviewer_p9fs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988767053, "cdate": 1761988767053, "tmdate": 1762918382591, "mdate": 1762918382591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a denial-of-service (DoS) attack on vision-language models (VLMs). The attack identifies input samples that cause the model to consume an unusually large number of tokens, thereby degrading its efficiency. Such samples are discovered through a perturbation injection method guided by a newly proposed loss function."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The problem studied is timely."}, "weaknesses": {"value": "- The paper positions the work as a red-teaming effort, but the proposed method is more accurately described as a specific attack. In general, red-teaming involves systematically identifying a range of vulnerabilities, including those without concrete exploits, and typically provides comprehensive analysis and actionable recommendations. These broader aspects are missing from the current paper.\n\n- Figure 3 measures semantic consistency, but its relevance to a denial-of-service and red-teaming  setting is not clearly justified. \n\n- Although the attack is new in its application to VLMs, it largely follows existing adversarial attack paradigms. In my opinion,  the level of methodological novelty may not be sufficient for this conference.."}, "questions": {"value": "- Explain why the metric on \"semantic consistency\" and \"feature similarity\" are relevant.\n\n- Including of actionable recommendations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p6oh3VHqum", "forum": "aXP6VobXVv", "replyto": "aXP6VobXVv", "signatures": ["ICLR.cc/2026/Conference/Submission5971/Reviewer_tqRH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5971/Reviewer_tqRH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5971/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135607775, "cdate": 1762135607775, "tmdate": 1762918382187, "mdate": 1762918382187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}