{"id": "hqq6GyYISN", "number": 14869, "cdate": 1758244901837, "mdate": 1763712850483, "content": {"title": "Fair Decision Utility in Human-AI Collaboration: Interpretable Confidence Adjustment for Humans with Cognitive Disparities", "abstract": "In AI-assisted decision-making, human decision-makers finalize decisions by taking into account both their human confidence and AI confidence regarding specific outcomes.  In practice, they often exhibit heterogeneous cognitive capacities, causing their confidence to deviate, sometimes significantly, from the actual label likelihood. We theoretically demonstrate that existing AI confidence adjustment objectives, such as *calibration* and *human-alignment*, are insufficient to ensure fair utility across groups of decision-makers with varying cognitive capacities. Such unfairness may raise concerns about social welfare and may erode human trust in AI systems.\nTo address this issue, we introduce a new concept in AI confidence adjustment: *inter-group-alignment*. By theoretically bounding the utility disparity between human decision-maker groups as a function of  *human-alignment* level and *inter-group-alignment* level, we establish an interpretable fairness-aware objective for AI confidence adjustment. Our analysis suggests that achieving utility fairness in AI-assisted decision-making requires both *human-alignment* and *inter-group-alignment*. Building on these objectives, we propose a multicalibration-based AI confidence adjustment approach tailored to scenarios involving human decision-makers with heterogeneous cognitive capacities. We further provide theoretical justification showing that our method constitutes a sufficient condition for achieving both *human-alignment* and *inter-group-alignment*.\nWe validate our theoretical findings through extensive experiments on four real-world tasks. The results demonstrate that AI confidence adjusted toward both *human-alignment* and *inter-group-alignment* significantly improves utility fairness across human decision-maker groups, without sacrificing overall utility.\n*The implementation code is available at* https://anonymous.4open.science/r/FairHAI.", "tldr": "", "keywords": ["Fairness", "AI-assisted decision making"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6de14c38c3a39c5d4e16e876d9d989938da902b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors explore how humans with different characteristics may derive unfair utility from the same advice. They introduce the new concept of inter-group alignment and propose an interpretable, fairness-aware objective for AI confidence adjustment. Their analysis shows that achieving utility fairness in AI-assisted decision making requires both human alignment and inter-group alignment. Building on these objectives, they present a multicalibration-based AI confidence adjustment approach tailored to scenarios involving decision-makers with heterogeneous cognitive capacities. Evaluations on four datasets with real human-behavior data demonstrate that the proposed method significantly improves utility fairness across groups without sacrificing overall utility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clear, well organised, and adds several fresh ideas to human-AI interaction research. It first defines human-alignment and the new inter-group alignment, proves why both matter, and then shows an elegant multicalibration fix to improve the fairness utility across the group.\n\n2. The experiment design and evaluation is thorough to justify the main claim of this paper.\n\n3. The authors publicly release their code, making replication and future extensions straightforward for other researchers."}, "weaknesses": {"value": "1. The framework assumes people can report confidence on a common, well-calibrated scale, yet prior work shows self-reported confidence is often noisy and inconsistent across individuals.\n\n2. All experiments rely on archival human-AI datasets; the paper does not test how live users react to the adjusted confidences or how their perceptions and strategies might change.\n\n3. The evaluation covers only binary decision tasks. Extending the theory and method to multi-class settings would broaden its practical reach."}, "questions": {"value": "See above weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KyHQjiXlLY", "forum": "hqq6GyYISN", "replyto": "hqq6GyYISN", "signatures": ["ICLR.cc/2026/Conference/Submission14869/Reviewer_MdV1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14869/Reviewer_MdV1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760475981774, "cdate": 1760475981774, "tmdate": 1762925221322, "mdate": 1762925221322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Updated Manuscript"}, "comment": {"value": "We kindly thank all the reviewers for their time and for providing valuable feedback on our work. We appreciate the recognition of our work as fresh (Reviewers MdV1, wqhf) and novel (Reviewer T784) , addressing an overlooked problem (Reviewers Wqv4, T784) with solid theoretical foundations (Reviewers wqhf, T784) and empirical credibility (Reviewers wqhf, T784, MdV1).\n\nBased on the reviewers' suggestions, we have made the following revisions to our submission:\n\n- `Modified Section 5.1 Settings - Hyperparameters`: added summary of parameter sensitivity analysis (Reviewer wqhf);\n- `Modified Section 5.2 - Table 1`: added standard deviations and corrected bold highlighting (Reviewer Wqv4);\n- `Added Appendix A.1`: detailed discussion on rationale and scope of Assumption 2.1, verification that Assumption 2.1 holds generally in real-world AI-assisted decision-making contexts, and validation of the robustness of our method under violations of Assumption 2.1 (Reviewer T784, Reviewer wqhf, Reviewer Wqv4);\n- `Added Appendix A.8.1`: detailed quantitative ablation results (Reviewer wqhf);\n- `Added Appendix A.8.2`: detailed instance decision accuracy distribution analysis, demonstrating fairness improvement by delivering better instance outcomes for the disadvantaged group without negatively impacting the advantaged group (Reviewer Wqv4);\n- `Added Appendix A.8.5`: detailed computational complexity analysis, with theoretical and experimental validation of scalability with multi-attribute grouping (Reviewer T784);\n- `Added Appendix A.8.6`: detailed extension to multi-class settings, with theoretical and experimental validation of scalability to multi-class scenarios (Reviewer MdV1, Reviewer wqhf);\n- `Modified Appendix A.9.2`: detailed discussion on rationale and scope of archival dataset usage (Reviewer MdV1);\n- `Added Appendix A.9.3`: discussion on connections to and distinctions from game-theoretic perspectives (Reviewer wqhf)."}}, "id": "Ig0J6UjwzU", "forum": "hqq6GyYISN", "replyto": "hqq6GyYISN", "signatures": ["ICLR.cc/2026/Conference/Submission14869/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14869/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14869/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763713308116, "cdate": 1763713308116, "tmdate": 1763713308116, "mdate": 1763713308116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles fairness in human–AI collaboration from the angle of ensuring that AI assistance provides equitable benefits to human decision-makers who possess heterogeneous cognitive capacities. The authors argue that existing alignment notions such as calibration and \"human-alignment\" are insufficient for guaranteeing fairness *across* decision-maker groups. They introduce the concept of \"inter-group-alignment\" to capture disparities in decision utility between subpopulations of humans. The paper provides a formal theoretical analysis linking human alignment and inter-group alignment. Building on this insight, the authors propose \"cognition-aware multicalibration\", and prove that it serves as a sufficient condition for simultaneously achieving both alignment objectives.\nThe claims are validated through experiments on four human-AI decision-making tasks. The results demonstrate that the proposed method successfully reduces utility disparities across groups while maintaining overall performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel and Interesting Perspective:** The paper shifts fairness focus from data subjects to the decision-makers themselves. It's an underexplored and insightful perspective as it illuminates how AI systems can inadvertently widen performance gaps between varied decision-makers (e.g., experts and novices).\n2. **Elegant Theoretical Framework with Principled Operationalization:** The derivation of a utility disparity bound provides an interpretable bridge between fairness notions, offering practitioners a clear diagnostic for assessing group-level disparities. Cognition-aware multicalibration operationalizes the theory in a computationally practical way.  \n3. **Empirical Credibility:** Validation on multiple real-world tasks demonstrates both robustness and practical feasibility. Additional experiments in appendix demonstrate thoughtfulness and rigor."}, "weaknesses": {"value": "1. **Scalability Challenges with Multi-Attribute Grouping:** The proposed framework requires partitioning the data and calibrating for each subgroup. While the experiments show this is feasible for a small number of groups, the computational and data requirements would grow exponentially with the number of sensitive attributes. A deeper discussion of the practical limits (e.g., the number of subgroups that can be realistically handled) would be beneficial.\n\n2. **Rational Decision-Making and Static Setting:** The theoretical claims hinge on the assumption that humans follow a rational, monotone decision policy (Assumption 2.1). However, a large body of literature shows that human behavior often deviates from pure rationality, exhibiting various cognitive biases. The framework does not currently account for such suboptimal but more realistic decision policies. The analysis assumes that the cognitive capacities and decision policies of the human groups are static. In many real-world collaborations, humans learn and adapt their strategies based on repeated AI interactions. The current framework does not address these dynamic or online contexts, where calibration needs might evolve over time."}, "questions": {"value": "I found the paper to be insightful and well-executed. The weaknesses section already incorporate most questions, primarily aimed at better understanding the practical implications and boundaries of the proposed framework. One additional question pertains to incorporating broader notions of utility within proposed framework. The paper defines utility as decision accuracy. In human-AI collaboration, however, overall utility can also encompass factors like decision time, cognitive load, or the user's trust and self-efficacy. How do you see the concepts of human-alignment and inter-group-alignment extending to these more multi-faceted, human-centric notions of utility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hHS0t5r8sd", "forum": "hqq6GyYISN", "replyto": "hqq6GyYISN", "signatures": ["ICLR.cc/2026/Conference/Submission14869/Reviewer_T784"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14869/Reviewer_T784"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760590859300, "cdate": 1760590859300, "tmdate": 1762925220830, "mdate": 1762925220830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper demonstrates that existing AI confidence adjustment objectives, such as calibration and human alignment, may lead to utility disparities across groups of decision-makers with varying cognitive capacities.  Moreover, the paper shows that the utility disparity between human decision-maker groups is bounded by a function of human-alignment level and inter-group-alignment level. Hence, the authors propose  a multicalibration-based AI confidence adjustment approach to mitigate this concern, validating their approach over four different datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main strengths are:\n\n1. The paper covers an overlooked problem in the human-AI alignment literature, as it focuses on fairness in human-AI collaboration\n2. The proposed multicalibration approach is principled and based on the proposed theory\n3. Overall, the paper's goal is well-motivated and interesting"}, "weaknesses": {"value": "The main shortcomings of the paper are:\n\n*Significance* While I appreciate the theoretical results, I think this new paradigm comes with some implementation choices that are not straightforward. For instance, in real-life scenarios, evaluating cognitive disparities might be even unethical, so such information might never be disclosed. Moreover, it is also quite challenging to assess if improving the fairness for decision-maker groups leads to better outcomes for the observed instances; hence, I think the authors should discuss these possible limitations.\n\n*Assumptions Discussion* I think the authors should discuss Assumption 2.1 better, as understanding when such an assumption might fail is a key factor for the proposed method.\n\n*Empirical Evaluation:* I think some of the results are not very clear. For instance, I would suggest that the authors reformat Table 1, as it does not report standard deviations and incorrectly displays the best entries in bold."}, "questions": {"value": "I would like the authors to discuss my highlighted shortcomings.\n\nMoreover, I wonder if the authors could detail some settings where $P(Y|S=1)$ is different from $P(Y|S=0)$? My intuition is that, for some reason, the instances that are considered by one group can be sampled from different parts of the feature space, but I want to be sure about it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sFVTTxvuAe", "forum": "hqq6GyYISN", "replyto": "hqq6GyYISN", "signatures": ["ICLR.cc/2026/Conference/Submission14869/Reviewer_Wqv4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14869/Reviewer_Wqv4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847927320, "cdate": 1761847927320, "tmdate": 1762925220397, "mdate": 1762925220397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies fairness in AI-assisted decision-making, focusing on situations where humans have different cognitive capacities. The authors show that existing approaches, such as calibration and human-alignment, do not necessarily ensure utility fairness across different groups of human decision-makers.\n\nThey introduce a new concept called inter-group-alignment, which ensures that human groups with similar confidence levels receive comparable decision utility when supported by the AI. The paper provides solid theoretical analysis, deriving a clear upper bound on utility disparity as a function of both human-alignment and inter-group-alignment.\n\nTo achieve this dual goal in practice, they propose a Cognition-aware Multicalibration algorithm that adjusts AI confidences accordingly. Experiments on four real-world datasets (Art, Cities, Sarcasm, and Census) confirm that this method significantly reduces fairness gaps between human groups while maintaining overall performance.\n\nThe theoretical formulation, clarity of objectives, and validation through experiments make this a valuable contribution to the fairness and human-AI collaboration literature."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The introduction of inter-group-alignment provides a fresh and rigorous way to think about fairness in human-AI collaboration.\n\nThe paper is theoretically complete, with well-defined assumptions and proofs that connect intuitively to fairness.\n\nThe proposed Cognition-aware Multicalibration method is interpretable, practical, and mathematically justified.\n\nThe empirical validation is strong: four datasets, clear fairness and utility metrics, and consistent results showing improvements.\n\nThe work bridges human and algorithmic fairness, showing how cognitive differences among people can be formally addressed rather than ignored."}, "weaknesses": {"value": "Monotonicity Assumption (Assumption 2.1):\nThis assumption states that humans act rationally, increasing their probability of making a positive decision as either their own or the AI’s confidence increases. While this is reasonable for theoretical proofs, it may not always hold in practice.\nReal humans are often non-monotonic in their decision behavior. For instance, they may overtrust or undertrust AI advice due to biases, cognitive fatigue, or misunderstanding of confidence. In such cases, increasing the AI’s confidence may not increase the chance of a positive decision, which violates monotonicity.\n\nThe authors could strengthen the paper by (a) discussing how this assumption might break in real-world conditions, (b) analyzing the theoretical impact if monotonicity is only approximately satisfied, and (c) outlining how the proposed method could still remain effective. Even a small simulation with “noisy” or biased human decision policies would illustrate robustness and make the work more realistic and influential.\n\nAblation Study Clarity:\nThe paper does include human-only, AI-only, and human-AI results, but these are only mentioned in Figure 2 and not summarized in a table. It would be much clearer to show numerical comparisons between these three conditions across datasets. This would highlight how the collaboration truly benefits fairness and overall utility.\n\nHyperparameter Sensitivity:\nThe appendix covers sensitivity analyses for parameters like $\\tilde{\\alpha}$ and $\\lambda$, but the main text should summarize them briefly. Readers need to know how robust the fairness improvements are if these parameters change. A short paragraph or figure in the main paper would make the method easier to trust and reproduce.\n\nGame-Theoretic Perspective:\nThe problem naturally relates to concepts like Stackelberg games (AI as leader, human as follower) or Shapley-value-based fairness allocation. A short paragraph connecting the proposed framework to these ideas would situate the work more clearly within the broader literature on incentive alignment and cooperative fairness.\n\nBehavioral Impact of AI Adjustments:\nSince the AI modifies its confidence outputs to improve fairness, it would be interesting to consider whether this adjustment might influence how humans behave over time. For example, could one group become overly reliant on AI due to consistently higher confidence signals? Even a brief reflection on this would enhance the paper’s real-world applicability."}, "questions": {"value": "Could you provide a clear quantitative table comparing human-only, AI-only, and human-AI decisions across all datasets to make the ablation more explicit?\n\nHow sensitive are the results to the parameters $\\tilde{\\alpha}$ and $\\lambda$? Would adaptive tuning or data-driven adjustment improve generalization?\n\nIf the human decision policy is not strictly monotonic (for example, due to inconsistent trust or cognitive biases), how does that affect your theoretical bounds or empirical fairness outcomes?\n\nCould the dual-alignment framework extend naturally to multi-class or regression tasks?\n\nHave you considered a Stackelberg or cooperative-game interpretation of your framework, where the AI strategically adjusts its outputs to optimize a fairness-aware social welfare objective? Are they feasible to perform and related at all?\n\nDo you think modifying AI confidence distributions could unintentionally change how different human groups rely on the AI (either over-trusting or disengaging)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UScMvjebEp", "forum": "hqq6GyYISN", "replyto": "hqq6GyYISN", "signatures": ["ICLR.cc/2026/Conference/Submission14869/Reviewer_wqhf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14869/Reviewer_wqhf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922577050, "cdate": 1761922577050, "tmdate": 1762925219856, "mdate": 1762925219856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}