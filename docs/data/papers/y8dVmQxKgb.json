{"id": "y8dVmQxKgb", "number": 13904, "cdate": 1758224713084, "mdate": 1763231551654, "content": {"title": "Hot PATE: Private Aggregation of Distributions  for Diverse Tasks", "abstract": "The Private Aggregation of Teacher Ensembles (PATE) framework enables privacy-preserving machine learning by aggregating responses from disjoint subsets of sensitive data. Adaptations of PATE to tasks with inherent output diversity such as text generation, where the desired output is a sample from a distribution, face a core tension:  as diversity increases, samples from different teachers are less likely to agree, but lower agreement results in reduced utility for the same privacy requirements. Yet suppressing diversity to artificially increase agreement is undesirable, as it distorts the output of the underlying model, and thus reduces output quality.\n \nWe propose Hot PATE, a variant of PATE designed for diverse generative settings.  We formalize the notion of a \\emph{diversity-preserving} \\emph{ensemble sampler} and introduce an efficient sampler that provably transfers diversity without incurring additional privacy cost. Hot PATE requires only API access to proprietary models and can be used as a drop-in replacement for existing \"cold\" PATE samplers.  Our empirical results corroborate the theoretical guarantees, showing that Hot PATE achieves orders-of-magnitude improvements in utility per privacy budget on in-context learning tasks.", "tldr": "Hot PATE is a PATE variant for generative tasks that preserves output diversity, provably transfers it without extra privacy cost, and yields orders-of-magnitude higher utility at the same privacy level.", "keywords": ["Differential Privacy", "Sequential Text Generation", "Coordinated Ensembles"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d4ba7bdf51d80ec7a5d034d2a22c2c77339754c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a new coordinated ensemble method to increase generated sample diversity in the Private Aggregation of Teacher Ensembles (PATE) framework. Comparing with simple majority voting, the proposed Hot PATE successfully increases the diversity of the ensemble output."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe proposed CoordinatedSamples are simple for integration with current PATE framework.\n2.\tMultiple utility measurements are considered including average yield per sample, coverage, etc."}, "weaknesses": {"value": "1.\tThe paper mentioned that their proposed Hot PATE satisfies ($\\epsilon, \\delta$)-DP but did not give a clear way for determining the exact value of $\\epsilon$ and $\\delta$. Since the value of these hyper-parameters are vital for the privacy preserving level, it’s important to know how the hyper-parameters of Hot PATE should be set to satisfy a required privacy requirement.\n2.\tThe baseline of this work is just the simple majority voting. As the paper mentioned in the related work, there exists other similar works DP-ICL. Although the author mentioned that this is a concurrent work, given that they are already published, I think it should be compared.\n3.\tBoth evaluation settings are relatively simple (synthetic instructions, toy planet-number example). I would like to see evaluation on complex open-ended generative tasks (e.g., summarization, dialogue, creative writing).\n4.\tThis method can only work with white-box models that opens access for per-token prediction probability. The generalization to closed-source black-box models remains unknown. I am curious, do you have plans or solutions for closed-source teacher settings? \n5.\tCoordinated sampling can require repeated sampling or shared randomness that may be impractical for large vocabularies. Do you have any computational cost analysis?"}, "questions": {"value": "1.\tReference failure around line 694 and 694.\n2.     What's the core difference between coordinated sampling and the proposed coordinated ensemble? Is it just a direct application?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P8KLu9ekSu", "forum": "y8dVmQxKgb", "replyto": "y8dVmQxKgb", "signatures": ["ICLR.cc/2026/Conference/Submission13904/Reviewer_TZnG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13904/Reviewer_TZnG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760862509276, "cdate": 1760862509276, "tmdate": 1762924413628, "mdate": 1762924413628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Hot PATE, a PATE variant for diverse generative tasks that transfers diversity without extra privacy cost. The theoretical guarantees and empirical results show that Hot PATE can achieve better privacy-utility trade-off for in-context learning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation is clear: existing PATE methods lose utility in high-diversity generative tasks due to low teacher agreement.\n- The paper introduces an interesting and novel idea by extending the PATE framework to generative settings through coordinated ensembles.\n- This paper provides solid theoretical analysis."}, "weaknesses": {"value": "- The paper does not report in-context learning results on concrete downstream tasks. It’s unclear how much Hot PATE improves ICL performance on real end tasks.\n- I recommend the authors expand the related work section to include recent studies on differentially private in-context learning [1, 2, 3], and explain how Hot PATE connects to and differs from these methods, so that the paper’s contribution is better positioned within the existing DP-ICL literature.\n\n[1] Hong, Junyuan, et al. \"DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer.\" The Twelfth International Conference on Learning Representations.\n\n[2] Gao, Fengyu, et al. \"Data-adaptive Differentially Private Prompt Synthesis for In-Context Learning.\" The Thirteenth International Conference on Learning Representations.\n\n[3] Yamasaki, Yusuke, et al. \"Plausible Token Amplification for Improving Accuracy of Differentially Private In-Context Learning Based on Implicit Bayesian Inference.\" Forty-second International Conference on Machine Learning.\n\nMinor issues:\n\n- The empirical setup does not specify DP parameters $\\epsilon$ and $\\delta$.\n- Missing figure references on Lines 694 and 696."}, "questions": {"value": "- What is the computational cost of Hot PATE compared to Cold PATE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hmhd9uVjtL", "forum": "y8dVmQxKgb", "replyto": "y8dVmQxKgb", "signatures": ["ICLR.cc/2026/Conference/Submission13904/Reviewer_sBN4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13904/Reviewer_sBN4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761280268323, "cdate": 1761280268323, "tmdate": 1762924413107, "mdate": 1762924413107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new variant of the PATE framework designed for generative tasks with diverse outputs, such as text generation using large language models. Traditional PATE methods, referred to as “Cold PATE,” struggle in such settings because diversity among teacher models reduces agreement, leading to poor utility under the same privacy constraints. To address this, the authors introduce “Hot PATE,” which employs a coordinated ensemble sampling mechanism that uses shared randomness across teachers to positively correlate their outputs, thereby preserving diversity while improving consensus. The paper formally defines diversity-preserving aggregation, proves that the coordinated approach maintains identical differential privacy guarantees as standard PATE, and demonstrates substantial empirical gains in utility—achieving up to an order-of-magnitude improvement on both natural and curated in-context learning tasks. Overall, Hot PATE extends privacy-preserving learning to diverse generative domains, balancing privacy, utility, and output diversity effectively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a clear and original contribution to the PATE framework by redefining it for generative and diverse-output settings through the concept of “Hot PATE.” The proposed coordinated ensemble sampling is both conceptually elegant and theoretically sound, offering a provable way to preserve diversity while maintaining differential privacy guarantees. The formalization of diversity-preserving aggregation and the demonstration that coordinated ensembles achieve higher utility without additional privacy cost show strong theoretical depth. Empirical evaluations are thoughtfully designed, illustrating consistent, significant gains in utility and diversity across different tasks. The writing is clear, structured, and connects theory and practice effectively."}, "weaknesses": {"value": "While the paper is strong in theory, the experimental evaluation is somewhat limited in scope—focused mainly on synthetic or simplified text-generation tasks rather than more complex real-world applications. The computational cost and practical constraints of implementing coordinated sampling with proprietary APIs (e.g., repeated sampling requirements) are only briefly discussed. Sensitivity analyses for parameters such as the robustness threshold (τ) and ensemble heterogeneity are limited, and comparisons with other recent privacy-preserving generative frameworks (e.g., semantic aggregation, top-k filtering) could be expanded."}, "questions": {"value": "1. How sensitive is the utility gain of Hot PATE to the choice of τ and ensemble size n in heterogeneous teacher scenarios?\n2. Could the proposed coordinated sampling approach be efficiently implemented with current LLM APIs without excessive overhead?\n3. How would Hot PATE perform under stricter privacy budgets (e.g., smaller ε) or when extended to multimodal generative tasks such as image or code generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QFWYU48Akn", "forum": "y8dVmQxKgb", "replyto": "y8dVmQxKgb", "signatures": ["ICLR.cc/2026/Conference/Submission13904/Reviewer_hj6r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13904/Reviewer_hj6r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928806969, "cdate": 1761928806969, "tmdate": 1762924412575, "mdate": 1762924412575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Hot PATE, a variant of Private Aggregation of Teacher Ensembles (PATE) designed to support _diverse generative_ settings, where the generating distribution supports many possible outcomes (e.g. _synthetic text generation_).  The main contribution is the use of _coordinated ensembles_ in the context of PATE, and a formal definition of diversity preserving aggregation. Coordinated ensembles is a method to improve agreement over the teachers. Rigorous privacy bounds of the mechanism are provided and experiments are presented in order to validate the claims."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The presentation is quite clear, and I think that the formalization of what it means to transfer diversity for aggregation constitutes a big part of the contribution.\n2. The use of coordinated ensembles, to my knowledge, has not been studied in such settings before, making this work original.\n3. The empirical validation is convincing and covers multiple scenarios.\n4. The new PATE aggregator leaves the privacy analysis of original PATE unchanged: changing one distribution as teacher changes one item of the resulting histogram."}, "weaknesses": {"value": "1. The empirical sections compare independent vs coordinated ensembles. However, Appendix A lists prior PATE adaptations (Tian et al. 2022, Tang et al. 2022) that \"limited diversity\". A fair SOTA evaluation should include these baselines and compare them to the proposed methods.\n2. There is no privacy accounting in the paper. Except in Appendix F, the budgets $(\\epsilon, \\delta)$ are never explicit. As a result, the effect of the privacy budget on empirical results (high-privacy vs low-privacy regimes) is unclear. My understanding is that the experiments use $T$ as a proxy for privacy, but $T$ is not a direct substitute for reporting actual $(\\epsilon,\\delta)$. Therefore, claims of \"orders-of-magnitude improvements in utility per privacy budget\" based solely on $T$ may not be rigorous enough. This paper should report privacy budgets."}, "questions": {"value": "1. I am not sure that I understand failure. Can you confirm that failure decision is made after noising the counts so that $\\perp$ is just another DP output? If not, failure could reveal private information, for example, that the supports of the teachers are disjoints. Could you also specify whether the number of retries is hidden, fixed, or DP-accounted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U5J8NudODL", "forum": "y8dVmQxKgb", "replyto": "y8dVmQxKgb", "signatures": ["ICLR.cc/2026/Conference/Submission13904/Reviewer_67y4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13904/Reviewer_67y4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955173771, "cdate": 1761955173771, "tmdate": 1762924412228, "mdate": 1762924412228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Hot PATE, a variant of the Private Aggregation of Teacher Ensembles (PATE) method, but in the setting of text generation with large language models. Unlike standard PATE (which they term \"Cold PATE”), Hot PATE replaces independent token sampling from teachers with coordinated ensemble sampling, using shared randomness and a bottom-k transform. This construction increases agreement among teachers and uses less privacy budget. Empirically, Hot PATE achieves orders-of-magnitude higher utility per privacy budget with more diverse output supports."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The authors introduce Hot PATE, a novel coordinated ensemble framework that preserves diversity while operating under the same privacy budget.\n\n1. I found the idea of diversity-preserving aggregation in Section 2 and the introduction of coordinated sampling particularly instructive. Theorem 1 on the utility of this coordinated approach is also theoretically sound.\n\n2. The authors demonstrate orders-of-magnitude improvements in both utility and diversity transfer across the synthetic instruction generation and Planet Z tasks—for example, achieving 20% coverage at T = 2000 while requiring eight times less privacy budget than the baseline of independent sampling.\n\nThe following figures stand out as highlights of the paper:\n\nFigures 2 and 4: Show substantial gains in coverage and support size for coordinated ensembles.\nFigures 3 and 7: Illustrate the emergence of “peaky” histogram shapes with high maximum counts (0.6n) and large margins, supporting why coordinated ensembles use less privacy budget."}, "weaknesses": {"value": "1. For API access cases, i.e., when model probabilities are not available, the paper mentions that the distribution can be approximated by resampling with the same prompt. It would be helpful to clarify whether this is also a limitation for other PATE-based methods for LLM generation? \n\n2. Not a major weakness, but Figure 2 was somewhat difficult to read. The main message (diversity of tokens) appears to be conveyed more clearly in the right panel of Figure 2, which might be sufficient to illustrate the key result?"}, "questions": {"value": "1. What is the default temperature setting you mention in Section 4?\n\n2. The motivation behind the Planet Z task is somewhat hard to understand—could you please elaborate on its purpose and how it supports the evaluation of Hot PATE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lqKYE660nl", "forum": "y8dVmQxKgb", "replyto": "y8dVmQxKgb", "signatures": ["ICLR.cc/2026/Conference/Submission13904/Reviewer_P4kw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13904/Reviewer_P4kw"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056564115, "cdate": 1762056564115, "tmdate": 1762924411777, "mdate": 1762924411777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}