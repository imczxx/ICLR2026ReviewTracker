{"id": "p072J56yo4", "number": 8367, "cdate": 1758080284130, "mdate": 1759897789389, "content": {"title": "RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation", "abstract": "Mean flow (MeanFlow) enables efficient, high-fidelity image generation, yet its single-function evaluation (1-NFE) generation often cannot yield compelling results. We address this issue by introducing RMFlow, an efficient multimodal generative model that integrates a coarse 1-NFE MeanFlow transport with a subsequent tailored noise-injection refinement step. RMFlow approximates the average velocity of the flow path using a neural network trained with a new loss function that balances minimizing the Wasserstein distance between probability paths and maximizing sample likelihood. RMFlow achieves competitive, often (near) state-of-the-art results on text-to-image, context-to-molecule, and time-series generation using 1-NFE, at a comparable computational cost to the baseline MeanFlows.", "tldr": "We RMFlow, an efficient multimodal generative model that integrates a coarse 1-NFE MeanFlow transport with a subsequent tailored noise-injection refinement step.", "keywords": ["Mean Flow", "Flow Matching", "Noise-injection", "Likelihood Maximization", "Multimodal Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d0eefb548e71eeee0b4fe45b8ec7299f53ae3cde.pdf", "supplementary_material": "/attachment/c338eafb1475425319ade03113ce034e928d3940.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces RMFlow, a novel flow-based model designed for single function evaluation. The approach builds on Flow Matching, a generative modeling framework that maps two distributions by modeling the flow process using an ordinary differential equation (ODE). However, generating samples from the target distribution typically requires multiple function evaluations. To address this computational cost, several approaches have been proposed to reduce the number of function evaluations (NFEs). One such method, MeanFlow, optimizes an averaged velocity field that allows for single function evaluation, but it often yields suboptimal results. The authors hypothesize that the absence of KL divergence optimization may contribute to the reduced performance. To address this, they propose incorporating a noise injection step and an additional negative log-likelihood (NLL) loss during training, leading to their proposed model, RMFlow. The authors also provide a method for incorporating conditioning, which enables multimodality generation. Furthermore, the authors conducted an extensive empirical evaluation across multiple tasks, including synthetic datasets, context-to-molecule generation, and text-to-image generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well written, the method is clear and easy to understand.\n* The results demonstrate strong empirical performance.\n* The authors present a clear and well-founded motivation for the design of their mode"}, "weaknesses": {"value": "* Comparison to other approaches: In the introduction, the authors mention several alternative methods for efficient flow modeling—such as Consistency Models (CM), distillation, and local Flow Matching (FM). However, it remains unclear whether any quantitative comparisons were conducted against these methods.\n* See questions"}, "questions": {"value": "* Line 277 -  “During fine-tuning, we further strengthen training by integrating 1-NFE sampling with a policy-gradient objective that incorporates physical feedback on sample quality...”\nThis statement is a bit unclear. Could you clarify whether any additional reinforcement learning (RL)-based training was performed beyond directly optimizing equation (10)?\nIf so, did the other baselines, particularly the MeanFlow models, also undergo this additional training step?\n* Line 273: “...introduces additional gradient pathways...” -  Could the authors please elaborate on this point and provide more details or clarification?\n* Line 240: $\\varepsilon_1,varepsilon_2$ are not used in this section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "auWFjOUiRJ", "forum": "p072J56yo4", "replyto": "p072J56yo4", "signatures": ["ICLR.cc/2026/Conference/Submission8367/Reviewer_T3eq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8367/Reviewer_T3eq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761136842933, "cdate": 1761136842933, "tmdate": 1762920277470, "mdate": 1762920277470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical failure mode in 1-NFE (single-step) MeanFlow models, where they struggle to capture high-fidelity details in multimodal or structured data distributions. To address this, the authors propose RMFlow, a novel modification that introduces a noise-injection refinement step. This architectural change enables the use of a hybrid loss function that combines the Wasserstein distance objective of MeanFlow with a likelihood-maximization (KL divergence) objective. The authors demonstrate through extensive experiments on synthetic data, molecule generation, time series forecasting, and text-to-image synthesis that RMFlow significantly improves the quality of 1-NFE generation over the MeanFlow baseline, achieving competitive results for teacher-free single-step models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper is well-written, the motivations are clear and exemplified through failure cases of previous models, the contribution is well-theorized and explained. Experiments shows that the proposed approach is able to solve the failure cases and issues of previous approahes. \n\n2) The core idea of RMFlow is both simple and theoretically sound. Though the conceptual two-stage process (coarse transport + refinement), it is implemented in a single, efficient 1-NFE step. The key contribution is the rigorous connection (Theorem 4.1) between the noise-injection architecture and the ability to optimize for likelihood via an NLL loss term. This synergistic co-design of architecture and loss function is a significant strength.\n\n3) The authors validate their method across four different and challenging domains. The results are consistently positive, showing marked improvement over the MeanFlow baseline. The success in the context-to-molecule task, where RMFlow dramatically increases molecule stability, is a particularly powerful demonstration of the method's practical utility for structured data with hard constraints."}, "weaknesses": {"value": "1) This is the most significant weakness of the paper. For the text-to-image task (Table 6), the sole reported metric is Fréchet Inception Distance (FID). As is now widely discussed in the community, FID is a flawed metric with several known issues:\n- It relies on an outdated InceptionV3 backbone trained on ImageNet, which is a poor feature extractor for the rich, diverse content produced by modern generative models.\n- It is known to correlate poorly with human perception of image quality.\n\nMost importantly for a text-to-image model, FID only measures the realism of the generated distribution and is completely agnostic to the text conditioning. The paper makes claims about \"multimodal generation\" but provides no quantitative evaluation of the text-image alignment. The lack of standard alignment metrics like CLIP Score or human preference scores (like HPSv2) is a major omission and makes it difficult to fully assess the model's capabilities on this task.\n\n2) The paper's performance depends on a key hyperparameter, $\\lambda_{1}$, which balances the Wasserstein and KL-divergence objectives. However, the ablation study for this hyperparameter (Table 7) is only performed on the simple 1D Gaussian mixture task. It is unclear how sensitive the model is to this trade-off on more complex, high-dimensional tasks like molecule or image generation, which would be crucial information for practitioners."}, "questions": {"value": "1) Regarding the text-to-image evaluation, could you please justify the decision to only report FID? Could you provide results using a text-alignment metric such as CLIP Score? This would substantially strengthen the claims of multimodal generation quality.\n\n3) Can you provide more insight into the sensitivity of the hyperparameter $\\lambda_{1}$ on the QM9 or COCO tasks? Does the optimal balance between the geometric ( $L_{CMFM}$ ) and likelihood ($\\mathcal{L}_{NLL}$) losses vary significantly across different data modalities?\n\n3) The noise-injection step uses a fixed noise level $\\sigma$. Have you experimented with making this a learned parameter or using a schedule? It seems plausible that the optimal amount of refinement noise might depend on the sample or context."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pmYOv4jCbs", "forum": "p072J56yo4", "replyto": "p072J56yo4", "signatures": ["ICLR.cc/2026/Conference/Submission8367/Reviewer_8n7D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8367/Reviewer_8n7D"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761220705315, "cdate": 1761220705315, "tmdate": 1762920277120, "mdate": 1762920277120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RMFlow, a refinement of MeanFlow designed to improve single-step (1-NFE) generation quality across multimodal tasks such as text-to-image, molecule, and time-series generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and presents a coherent idea within the flow-matching framework.\n- The proposed formulation is lightweight  and the integration of noise injection is computationally efficient.\n- The experiments span diverse domains (synthetic, molecule, time-series, and COCO text-to-image), which demonstrates the model's general applicability."}, "weaknesses": {"value": "- Performance for each downstream tasks: Although the paper claims near-SOTA performance, the actual COCO FID-30K (18.9) is still substantially higher than recent single-step diffusion models\n- Limitation or failure cases are not discussed in the manuscript."}, "questions": {"value": "If this method were applied beyond text-to-image (T2I) generation to other applications, how would it perform? Would the existing approaches be directly applicable in the same way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wRM9FXT9xi", "forum": "p072J56yo4", "replyto": "p072J56yo4", "signatures": ["ICLR.cc/2026/Conference/Submission8367/Reviewer_cCGj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8367/Reviewer_cCGj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8367/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916789403, "cdate": 1761916789403, "tmdate": 1762920276212, "mdate": 1762920276212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}