{"id": "dkgM2ale4U", "number": 18861, "cdate": 1758291572093, "mdate": 1759897076913, "content": {"title": "Predicting evolutionary rate as a pretraining task improves genome language models", "abstract": "Genome language models (gLM) have the potential to encode how and when genes are regulated without requiring labeled data. Most gLMs are pretrained using genome sequence reconstruction tasks inspired by natural language processing, such as masked language modeling (MLM) or next token prediction (NTP). Recent studies have shown that these gLMs often fail to capture biological signal, showing limited gains over simple classifiers on raw sequence or randomly initialized models on downstream genomic prediction tasks. To address these limitations, we explored alternative pretraining tasks for gLMs. Evolutionary rate has historically been the strongest predictor of function in genomics, but to date, there has been limited investigation of pretraining tasks exploiting evolution. Here, we introduce two evolution-based pretraining tasks that predict the rate of evolution from genomic sequence: current evolution prediction and masked evolution modeling. These tasks are designed so that they can be combined with NTP and MLM, enabling a systematic assessment of predicting sequence only, evolutionary rate only, or both. Using a novel suite of benchmarks that balance distinct aspects of genome function, we show that training on both sequence and evolutionary rate outperforms training on sequence alone. Moreover, for many tasks, training on evolutionary rate alone outperforms training on sequence alone. These results demonstrate that evolution-based pretraining offers a principled alternative or additional task to sequence reconstruction, establishing evolution as a key training target for genome-scale models.", "tldr": "We introduce two pretraining tasks for genome language models to predict evolutionary rate from sequence that, in some cases, can outperform sequence-only training and, when combined with it, yield better performing gLMs.", "keywords": ["pretraining task", "genome language models"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10599e2a16064396a32295dd4790e4900efe47e0.pdf", "supplementary_material": "/attachment/36acd6a40fc0e56be3ce5c383354a346e3944a2c.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes evolutionary-rate prediction as an alternative pretraining task for GLMs.\nThere are two methods: Current Evolution Prediction (CEP) and Masked Evolution Modeling (MEM).\nThe authors implement these tasks based on Jamba and Caduceus.\nThe results suggest that explicitly modeling evolutionary conservation provides a strong self-supervised signal for genomic representation learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(i) Novelty: The paper proposes evolutionary-rate prediction as an alternative pretraining task for GLMs.\n\n(ii) Experimental results: The paper uses two models based on Jamba and Caduceus to validate their methods.\n\n(iii) Benchmark: part of the benchmarks are novel and proposed by the paper."}, "weaknesses": {"value": "(i) The limitation of model architectures: The paper only validates the method based on Jamba and Caduceus GLMs.\n\n(ii) Limitation of the dataset: The paper only considers the human reference genome.\n\n(iii) Limited baseline: The paper uses NT, K-mer methods as baselines, but without considering diverse genomic models.\n\n(iv) Typos: \"??\" in line 415."}, "questions": {"value": "(i) Whether the method works on the BERT and GPT architecture?\n\n(ii) Whether the method works beyond the human reference genome?\n\n(iii) When I get the evolutionary rate of the DNA sequence to do pretraining, if I only know the DNA sequence, is this enough to get the evolutionary rate information? If so, how to achieve this?\n\n(iv) How about other genomic models' performance on the benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I do not have ethical concerns."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V5sWqPTY3O", "forum": "dkgM2ale4U", "replyto": "dkgM2ale4U", "signatures": ["ICLR.cc/2026/Conference/Submission18861/Reviewer_GRwb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18861/Reviewer_GRwb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761543124890, "cdate": 1761543124890, "tmdate": 1762930829662, "mdate": 1762930829662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposed new pretraining objectives for genomic language model (gLM) pretraining—predicting evolutionary rates estimated by PhyloP based on whole-genome alignments. The objectives can be used on their own or combined with conventional MLM or NTP objectives. I find the idea to be simple but potentially effective, but to fully demonstrate its potential, more comprehensive evaluations are necessary. I therefore recommend a weak rejection in its current form."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It is an interesting and promising idea to use evolutionary rate prediction as a pretraining task for gLM.\n2. The evaluation on zero-shot VEP and UCNE classification showed improved performance compared with gLMs trained on conventional objectives."}, "weaknesses": {"value": "1. My main concern is that the evaluation tasks are limited. I personally do not find the genomic region classification to be a meaningful quantitative evaluation. The authors should include evaluations on some more meaningful benchmarks from BEND (https://openreview.net/forum?id=uKB4cFNQFg), DART-Eval (https://openreview.net/forum?id=qR0x6H5WUX), LRB (https://openreview.net/forum?id=Cdc90HKs1I), etc.\n2. Comparison with PhyloGPN should be emphasized more, given its high similarity to the proposed framework in both the concept and the purpose. It appears that PhyloGPN achieves much better zero-shot VEP performance, but worse transfer learning performance on UCNE classification. This should be highlighted and discussed in the main text. And evaluation on more transfer learning tasks is needed to fully demonstrate the superiority of the proposed strategy."}, "questions": {"value": "1. Were both forward and reverse strands on hg38 used for training, or only the forward strand?\n2. Ideally, Evo 2 should be added to the comparisons. What was the reason to have not included it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZPtWP7Jthb", "forum": "dkgM2ale4U", "replyto": "dkgM2ale4U", "signatures": ["ICLR.cc/2026/Conference/Submission18861/Reviewer_1pAm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18861/Reviewer_1pAm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798027802, "cdate": 1761798027802, "tmdate": 1762930828778, "mdate": 1762930828778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Can predicting evolutionary rate of each nucleotide in addition to predicting nucleotide itself improve genomic foundational models?"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea overall is both novel and important\n2. Using some strong baselines and randomly initialized models"}, "weaknesses": {"value": "1. **Results**\n\na. Genomic elements classification is generally too easy, but balanced accuracy seems low. Can you run some other classifier which produces probability estimates and check AUC, micro/macro F1 scores, etc? And you can compare it with results in other papers, such as HyenaDNA.\n\nb. There are mentions of NT benchmark, however, ArGamba and BiGamba were not evaluated on that.\n\nc. Only one pathogenicity prediction task. GPN-MSA and BioFM evaluate common vs rare or non-existent variant classification performance. For example, you can check if your model is able to classify MAF > 5% from MAF<0.001% variants in held-out chromosomes. There are other variant effect tasks, eQTL, sQTL, etc. You mention somewhere that evolution scores are the best variant effect predictor, but test your models ONLY on one pathogenicity prediction task. Also, I am concerned that ground truth PhyloP scores are so much better than even BiGamba with MEM. Given that you used whole pathogenicity dataset (variants from all chromosomes), proper pretraining should be able to memorize most of the PhyloP scores thus making BiGamba with MEM or ArGamba with CEP very close to PhyloP scores. \n\nd. Evaluation on ultra-conserved elements is interesting; but it is even more interesting to see if it can be done in zero-shot fashion. For example, what are the predicted PhyloP score averages and nucleotide likelihoods for these elements?\n\ne. In general, I see that NTP+CEP and MLM+MEM models are better than vanilla ones, albeit only on three tasks out of roughly 50-100 tasks available from many genomic benchmarks out there. But, CEP and MEM utility can be much more obvious if you use it for pretraining or finetuning other, known models, for example Caduceus, which should be easy to replicate/finetune, given your expertise with pretraining mamba-like models.  It will help disentangle influence of your specific pretraining setup and influence of additional pretraining tasks. Because it is not obvious if your NTP or MLM models are bad and you recover some performance using CEP and MEM tasks, or NTP/MLM models are good already in their weight category and you improving its peak performance which cannot be improved by adding more pretraining compute/tuning learning rates/etc. If you can consistently demonstrate that your baseline models are better/competitive with other models on a few benchmarks, I will withdraw this objection.\n\n2. **Pretraining and ablations**\n\na. I would like to see some pretraining curves, cross-entropy loss and phylop prediction loss on held-out chromosomes during training.  \n\nb. It is not obvious that a simple sum of two losses it the optimal approach, maybe PhyloP loss contributes 95% during training, also are you implementing masking of positions where PhyloP scores were not available or use zeros during loss calculation?\n\nc. Why use 4M and 66M parameter models, if you are trying to compare them with Caduceus or Nucleotide Transformer. If you want to rigorously compare them with Caduceus, use 6.6M exactly, if with NT, use <=500M but show that your model is better on average/for some meaningful portion of eval tasks/performs the same, but inference is cheaper. \n\nd. Why bother with Mamba layers if you use 2K context where presumably FlashAttention kernels will be faster and more effective, especially considering modest model sizes?"}, "questions": {"value": "Paper does not make a sufficiently strong case that adding evolutionary rate prediction can improve GFM results for a broad range of downstream tasks. One variant effect prediction task and two genomic element classification tasks are definitely not enough to make this case. \n\nI am willing to change my decision if:\n\n1. More variant effect evaluations, some of Dart, TraitGym, VariantBenchmark (BioFM), ProteinGym, MaveDB, AlphaGenome/Borzoi eQTL and sQTL datasets will consistently show that CEP and MEM tasks are helpful across the board or, even better, allow ArGamba and BiGamba beat the newest 2025 generation of GFMs. At least one of these benchmarks have to be there.\n\n2. Use GUE, NT benchmark, NT long-range benchmark, BEND, etc to compare ArGamba and BiGamba with other GFMs. At least one of these benchmarks have to be there.\n\n3. Proper evaluation of how well you memorize PhyloP scores for pretraining dataset and predict them for held-out dataset. If you can predict them in held-out dataset well, it is VERY impressive on its own and can indeed lead to some interesting biological discoveries.\n\n**Feedback**\n\n1. Very hard to understand Figure 2B and 2C. Make points several times larger, name both bioinformatics baselines on 2B, expand Figure captions, 2B caption starts from lower letter.\n2. Figure 3A – how exactly are you using PhyloP score? X axis model names on Figure 3C are too verbose, you already use color and Ar/Bi prefixes for most of this information.\n3. One link to a supplementary table is missing, around line 415 on page 8."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vKMM0tKX9V", "forum": "dkgM2ale4U", "replyto": "dkgM2ale4U", "signatures": ["ICLR.cc/2026/Conference/Submission18861/Reviewer_AMk2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18861/Reviewer_AMk2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828901872, "cdate": 1761828901872, "tmdate": 1762930828177, "mdate": 1762930828177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work provides a principled study of the effectiveness of predicting evolutionary rate as a pretraining task. The authors introduce two novel evolutionary rate prediction pre-training tasks. In current evolution prediction (CEP), the model learns to predict the evolutionary rate at each position given the sequence up to that position. In masked evolution modeling (MEM), the model learns to predict evolution rates at masked positions from the surrounding nucleotides."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors investigate alternative pre-training objectives specifically designed for genome modeling, which tries to incorporate biological signals into pretraining.\n2. The proposed objective could be combined with previous self-supervision pretext tasks"}, "weaknesses": {"value": "1. The task of predicting evolutionary rate is not a self-supervision task and requires estimated evolutionary rate labels, which may limit its further scaling, cause bias towards function prediction and hinder further generalizability to other tasks.\n2. The comparison to more SOTA methods should be included.\n3. The effectiveness over multiple common model structures should be presented."}, "questions": {"value": "1. In Table A2, the performance of HyenaDNA drops after pretraining, which is uncommon for correct pre-training. Which objectives are used for the pre-training? Why performance drop? and even with performance drop HyenaDNA is still the best model, why not try out pre-training HyenaDNA with the proposed objective? \n2. Single-nucleotide resolution may not be the most appropriate resolution, did the authors try out predicting a short sequence of DNA to capture the more general patterns? \n3. Instead of predicting evolution rate, will directly ‘pertaining’ on functional region prediction task achieve similar performance? As in principle, both tasks convey very similar biological priors, and are worth further comparing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LyA9JhasrW", "forum": "dkgM2ale4U", "replyto": "dkgM2ale4U", "signatures": ["ICLR.cc/2026/Conference/Submission18861/Reviewer_BMst"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18861/Reviewer_BMst"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884975347, "cdate": 1761884975347, "tmdate": 1762930827777, "mdate": 1762930827777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}