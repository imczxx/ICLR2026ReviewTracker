{"id": "KmJfy0VtRJ", "number": 7105, "cdate": 1758007837617, "mdate": 1759897872304, "content": {"title": "Consistency and Unified Semantic Regularization for Generalized Category Discovery", "abstract": "Generalized Category Discovery (GCD) aims to leverage labeled data to learn clustering-friendly representations for unlabeled data. Among existing approaches, self-supervised contrastive learning (CL) is the most widely adopted, typically optimizing two objectives: $\\texttt{consistency}$ and $\\texttt{uniformity}$. However, we observe an inherent tension between these objectives—while uniformity encourages a uniform distribution across the feature space, it can conflict with the goal of learning class-discriminative representations. To address this, we propose a two-stage framework that disentangles feature learning from self-contrastive objectives to better capture category concepts and represent auxiliary unlabeled data. In the first stage, the model constructs visual representations anchored to known category prototypes while reinforcing semantic links between labeled classes. The second stage extends this representation space to discover novel categories using a consistency objective combined with specifically designed regularization. Moreover, we introduce a novel $\\texttt{Semantic Exploration Energy mechanism}$ to capture shared semantics across categories, thereby mitigating the information loss caused by prototype orthogonalization. The proposed framework—Consistency and Unified Semantic Regularization ($\\texttt{CURE}$)—retains the consistency objective and enhances it with semantic energy regularization. Our CURE achieves state-of-the-art performance across multiple benchmarks and significantly alleviates performance imbalance between known and novel classes.", "tldr": "", "keywords": ["deep clustering; category discovery; representation learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e1a061d3a12360a731254d91ec13b0f0e2685c9.pdf", "supplementary_material": "/attachment/67a93d3a6cbd2a646e011093cec1d62fe20f25f5.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the task of Generalized Category Discovery (GCD). Motivated by the conflicts between consistency and uniformity in self-supervised contrastive learning (CL), this paper proposes a two-stage framework that disentangles feature learning from self-contrastive objectives. The authors further introduce Semantic Exploration Energy\nMechanism to enhance feature representation. Comprehensive experiments validate the superiority of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-motivated and easy to follow.\n2. This paper proposes several novel components, including semantic exploration energy, label-guided concept structure, as well as structure-guided semantic expansion.\n3. Comprehensive comparative results and ablations are conducted to validate the method."}, "weaknesses": {"value": "1. Although the method achieves remarkable performance, it is a little bit complex with several hyper-parameters. The effect of each important parameter should be presented.\n2. The method contains two-stage training, each with several components. I was wondering whether the method consumes a lot more memory and training time than conventional GCD methods. The comparison of computational resources and training time should be included.\n3. Some references and citations are missing. The paper should cite all the baseline methods for comparison (Table 1) in the references list, i.e., ProtoGCD and PrCAL."}, "questions": {"value": "Please include experiments and analysis raised in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MdPAUP48u9", "forum": "KmJfy0VtRJ", "replyto": "KmJfy0VtRJ", "signatures": ["ICLR.cc/2026/Conference/Submission7105/Reviewer_NXsf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7105/Reviewer_NXsf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629210696, "cdate": 1761629210696, "tmdate": 1762919278234, "mdate": 1762919278234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a two-stage framework by disentangling feature learning from self-contrastive objectives to better capture category concepts and represent auxiliary unlabeled data. The proposed method mitigates the adverse effect of uniformity on novel category discovery."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. New idea to improve the class discriminative representations.\n2. The paper is well-organized and can be easily understood.\n3. Good results on different benchmark datasets, including CIFAR-10, CIFAR-100, ImageNet-100, CUB-200, Stanford-Cars and Herbarium19."}, "weaknesses": {"value": "1. The technical novelty of this work seems weak, as most key components are designed by slightly modifying existing modules.\n2. The literature review needs to focus more on GCD and explain better the motivations.\n3. The main problem to be solved is the adverse effects of representation uniformity induced by CL. Does it mean that this is also the main problem in GCD? I believe CL and GCD have different problems to be solved."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jgdD5Vfu6E", "forum": "KmJfy0VtRJ", "replyto": "KmJfy0VtRJ", "signatures": ["ICLR.cc/2026/Conference/Submission7105/Reviewer_jJPM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7105/Reviewer_jJPM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931393624, "cdate": 1761931393624, "tmdate": 1762919277299, "mdate": 1762919277299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles Generalized Category Discovery (GCD) and argues that the usual contrastive-learning recipe (consistency + uniformity) is internally conflicted: uniformity pushes features to spread on the hypersphere, which can hurt class-discriminative structure for GCD. To avoid this, the authors propose CURE, a two-stage pipeline:\n\n1. Stage I: use labeled data to build a “semantic topology” of known-class prototypes; instead of enforcing orthogonality, they add a Semantic Exploration Energy (SEE) regularizer that keeps prototypes softly connected, plus a label-guided concept structure to push this structure down to the feature space. \n2. Stage II: run structure-guided semantic expansion — cluster all data, align part of the clusters to known classes via Hungarian matching, treat the rest as novel-class candidates, and then train with a JS-consistency loss, logit-adjusted self-distillation, entropy regularization, and a second-stage semantic energy over the full prototype set. \n\nThis lets them discard CL-style uniformity while still learning a semantically smooth space, and achieves SOTA or near-SOTA results on 7 GCD benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to follow and the overall pipeline is clearly presented.\n\n2. The motivation is well aligned with the proposed two-stage design.\n\n3. The experimental section is reasonably comprehensive (multiple datasets, several ablations)."}, "weaknesses": {"value": "1. **Core motivation is unverified.** The whole paper rests on the claim that “uniformity may hurt class separation and thus GCD,” but there is no ablation that turns uniformity on/off (or varies its strength) to demonstrate this. Without such evidence, it is unclear whether uniformity is actually the bottleneck in current GCD pipelines.\n2. **Missing discussion of closely related ideas.** Prior work such as **hyperGCD**[1] starts from a very similar observation — that learning on a spherical / overly uniform space can be suboptimal for GCD — but this paper does not analyze the connection, differences in geometry, or when the proposed semantic energy is preferable. This weakens the motivation part.\n3. **Limited novelty.** Apart from Semantic Exploration Energy (SEE), most components already exist in recent parametric GCD methods.\n\n   * “Label-guided concept structure” is essentially supervised/contrastive alignment on labeled data, which is standard in SimGCD[2], ProtoGCD[3], DebGCD[4], LegoGCD[5], CMS[6], etc.\n   * The cluster-to-prototype alignment with Hungarian matching is very close to earlier “pseudo-label → prototype” or “cluster → parametric head” pipelines (e.g., UNO [7] and later GCD variants).\n   * “Semantic consistency optimization” is just multi-view consistency, which almost all recent GCD methods use.\n   * “Logit-aware self-distillation” and “virtual sampling + entropy regularization” are minor engineering refinements.\n\n     Given this, the paper should make a much more precise novelty claim.\n4. **SEE is not empirically validated.** The method claims to “preserve semantic structure,” but no evidence is shown: no before/after prototype–prototype similarity, no qualitative example on a fine-grained dataset (e.g., whether visually close bird species stay close), and no analysis of whether SEE avoids simply shrinking the prototype space. Without such visualization/analysis, it is hard to tell whether SEE is doing what it is supposed to do.\n5. **Typos.** In Table 2, CIFAR-100, the entry “85.0 6.4 82.3” is clearly a typo and should be fixed.\n\n[1] Hyperbolic Category Discovery\n\n[2] Parametric classification for generalized category discovery: A baseline study.\n\n[3] ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery\n\n[4] DebGCD: Debiased Learning with Distribution Guidance for Generalized Category Discovery\n\n[5] Solving the Catastrophic Forgetting Problem in Generalized Category Discovery\n \n[6] Contrastive Mean-Shift Learning for Generalized Category Discovery\n\n[7] A Unified Objective for Novel Class Discovery"}, "questions": {"value": "1. Please show a concrete prototype–prototype similarity matrix before and after applying SEE on a fine-grained dataset, and highlight which semantic relations are actually preserved. Otherwise, the benefit of SEE is speculative."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "HRume8EssM", "forum": "KmJfy0VtRJ", "replyto": "KmJfy0VtRJ", "signatures": ["ICLR.cc/2026/Conference/Submission7105/Reviewer_BiKu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7105/Reviewer_BiKu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982323135, "cdate": 1761982323135, "tmdate": 1762919276841, "mdate": 1762919276841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of Generalized Category Discovery (GCD) by critiquing the widely-used contrastive learning (CL) paradigm. The authors identify a key tension between the uniformity objective of CL, which promotes a uniform feature distribution, and the need for class-structured representations for effective clustering. To resolve this, they propose a two-stage framework named CURE. In the first stage, CURE leverages labeled data to construct a semantically meaningful prototype space, using a novel Semantic Exploration Energy (SEE) regularizer to prevent prototype fragmentation. In the second stage, the framework discovers novel categories by applying consistency constraints (via JS-divergence), self-distillation, and the SEE regularizer to both labeled and unlabeled data. The authors claim this approach abandons the problematic uniformity constraint of CL, leading to improved performance and better balance between known and novel class discovery."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper argues that the dominant contrastive learning paradigm in Generalized Category Discovery is suboptimal. Specifically, it posits that the uniformity objective of CL, which encourages features to be uniformly distributed, conflicts with the goal of learning class-discriminative, clustered representations needed for GCD.\n\n2. The key claim is that CURE is the first GCD framework to completely discard CL methods for unlabeled data, relying solely on consistency and semantic structuring. This is intended to avoid the \"noise\" caused by the uniformity objective.\n\n3. The paper correctly identifies that standard supervised learning with one-hot labels tends to enforce prototype orthogonality, which can sever semantic links between classes. This is a real issue that hinders generalization to novel, but semantically related, categories."}, "weaknesses": {"value": "1. The central claim to be the \"first GCD framework that aims to alleviate the impact of uniformity by entirely discarding CL methods\" is a major overstatement. The method's core mechanism for learning from unlabeled data is a consistency loss (JS-divergence) between augmentations. This consistency regularization is a foundational principle of self-supervised learning and a key component of many modern CL frameworks (e.g., BYOL, SimSiam), which are precisely the methods that moved away from explicit negative sampling. The paper does not discard CL; it discards the InfoNCE formulation and its associated negative-sampling-based uniformity term. This mischaracterization of the contribution is a fundamental weakness. The work is a reformulation of CL, not a departure from it.\n\n2. The motivation is to create a more \"clustering-friendly\" representation space by removing the uniformity constraint. The proposed solution replaces this implicit regularization (uniformity from InfoNCE) with a different set of explicit regularizers (SEE, consistency loss, etc.). It is not self-evident that this new combination is inherently more \"principled\" for clustering, rather than just being a different, empirically effective, set of constraints. \n\n3. In addition, progress [1] has been made on the uniformity of features in general category discovery, where plug-and-play loss functions are used to discuss the information represented by the covariance matrix. A comparison and discussion with this work should be conducted.\n\n4. Semantic Exploration Energy is effectively a form of prototype graph regularization, encouraging a compact manifold. Similar concepts of regularizing the geometry of the prototype space exist in metric learning and zero-shot learning. The formulation itself is a straightforward application.\n\n\n[1] Generalized Category Discovery via Token Manifold Capacity Learning. In Arxiv, 2025."}, "questions": {"value": "1. The authors claim to \"entirely discard CL methods\". However, the JS-divergence loss on augmented views (L_JS) is a cornerstone of consistency-based self-supervised learning, a major branch of CL. Can the authors clarify this claim? Would it be more accurate to state that the method discards the negative-sampling-based uniformity objective of InfoNCE-style CL, rather than CL as a whole?\n\n2. The two-stage design appears crucial. Stage 1 learns a \"semantic topology\" from labeled data. How sensitive is the performance of Stage 2 to the quality and nature of the representation learned in Stage 1? For instance, if the labeled classes are not semantically representative of the novel classes (e.g., labeled are all animals, novel are all vehicles), would the structure imposed by SEE in Stage 1 become a harmful prior during the discovery process in Stage 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fk73fk7Nvj", "forum": "KmJfy0VtRJ", "replyto": "KmJfy0VtRJ", "signatures": ["ICLR.cc/2026/Conference/Submission7105/Reviewer_8JxY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7105/Reviewer_8JxY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7105/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059144390, "cdate": 1762059144390, "tmdate": 1762919276546, "mdate": 1762919276546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}