{"id": "4bDifzIHNh", "number": 22161, "cdate": 1758326944847, "mdate": 1759896883001, "content": {"title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math", "abstract": "Large language model (LLM)-based reasoning systems have recently achieved gold medal-level performance in the IMO 2025 competition, writing mathematical proofs where, to receive full credit, each step must be not only correct but also sufficiently supported. To train LLM-based reasoners in such challenging, open-ended settings, strong verifiers capable of catching step-level mistakes are necessary prerequisites. We introduce Hard2Verify, a human-annotated, step-level verification benchmark produced with over 500 hours of human labor. Hard2Verify is designed to rigorously assess step-level verifiers at the frontier: Verifiers must provide step-level annotations or identify the first error in responses generated by frontier LLMs for very recent, challenging, and open-ended math questions. We evaluate 29 generative critics and process reward models, demonstrating that, beyond a few standouts, open-source verifiers lag closed source models. We subsequently analyze what drives poor performance in step-level verification, the impacts of scaling verifier compute, as well as fundamental questions such as self-verification and verification-generation dynamics.", "tldr": "We create a step-level verification benchmark with open-ended math problems and frontier reasoning LLMs.", "keywords": ["Step-level verification", "benchmark", "mathematics", "reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/27a7691fe42b28b21550abef1ee1c468fd9bb99c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Hard2Verify, a human-annotated step-level verification benchmark for open-ended frontier math, created with over 500 hours of human labor. It assesses verifiers on frontier LLM responses to recent, hard math questions. The authors evaluate 29 models, finding open-source verifiers lag behind closed-source ones. They also analyze factors like poor verification performance causes, compute scaling impacts, and self-verification dynamics."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a new benchmark, Hard2Verify, which is a human-annotated, step-level verification benchmark created with over 500 hours of human labor. It provides an excellent data resource for evaluating reward models and verifiers.\n2. The experiments cover a large number of open-source and closed-source models, and conduct multi-faceted analyses based on the evaluation results, offering in-depth insights into the performance of step-level verifiers in frontier mathematical reasoning scenarios."}, "weaknesses": {"value": "1. The abstract mentions that models' answers to IMO questions require step-by-step and rigorous evaluation, and training such verifiers is highly challenging—a widely recognized issue. However, the connection between this point and the proposed Hard2Verify benchmark in the paper is not clearly elaborated, making the transition and coherence abrupt and awkward, which is quite misleading.\n\n2. In my understanding, the vast majority of RLVR works conduct RL training through outcome reward, which is also one of the reasons for DeepSeek-R1's success. At the stage where process reward has not been widely used in RLVR, the paper fails to provide sufficient evidence to assert that \"the next frontier for LLMs is solving problems that are hard to verify\".\n\n3. Following the previous issue, although the experiments cover many models, the reward models are still mainly PRMs, which are not suitable for current RL training. It is believed that results of more RLVR verifiers (such as general verifier, xverify, R1-Distill-Verifier) should be supplemented to make the evaluation more comprehensive.\n\n4. The authors seem to have an unclear boundary between the understanding of reward models and verifiers. The paper sometimes conflates the two concepts in discussions (e.g., when analyzing model performance and application scenarios), leading to ambiguity in the logical context of related content."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "m8AVYFapNr", "forum": "4bDifzIHNh", "replyto": "4bDifzIHNh", "signatures": ["ICLR.cc/2026/Conference/Submission22161/Reviewer_FLHa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22161/Reviewer_FLHa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569619511, "cdate": 1761569619511, "tmdate": 1762942094903, "mdate": 1762942094903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Hard2Verify, a human-annotated, step-level verification benchmark produced with over 500 hours of human labor. Hard2Verify is designed to rigorously assess step-level verifiers at the frontier: Verifiers must provide step-level annotations or identify the first error in responses generated by frontier LLMs for very recent, challenging, and open-ended math questions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I find the motivation of this paper quite reasonable. Currently, LLMs in solving in-formal IMO-level mathematical problems remains an open-ended challenge. The authors also propose a practical and solid benchmark for this purpose.\n\nThe paper is well-presented and easy to follow."}, "weaknesses": {"value": "I believe the authors' proposal to use a large number of open-ended problems is a reasonable setting. However, a key issue that arises is how to ensure the quality of human annotations. I think the authors should provide more detailed information to enhance credibility. For example, did they use cross-validation among experts? What was the cross-validation accuracy? What are the backgrounds of the experts, and how many annotations were collected?\n\nI also believe there is still a gap between verifier accuracy and actual solver improvement. The authors should include additional experiments to ensure that scores on the current benchmark truly reflect real improvements in solver performance. For instance, they could use some of the methods mentioned in the Introduction section to design a simple baseline, demonstrating that high-scoring verifiers in Table 2 can indeed help improve solver scores."}, "questions": {"value": "Why use responses from only three models as the benchmark data? How can we ensure that the distribution of model-generated responses is sufficiently diverse and reasonable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JSa0A2noFm", "forum": "4bDifzIHNh", "replyto": "4bDifzIHNh", "signatures": ["ICLR.cc/2026/Conference/Submission22161/Reviewer_VpB3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22161/Reviewer_VpB3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973326849, "cdate": 1761973326849, "tmdate": 1762942094618, "mdate": 1762942094618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Hard2Verify, a step-level verification benchmark centered on IMO mathematics reasoning questions. By generating reasoning responses using frontier close-source language models (gpt/claude/gemini) and annotations manually with the help of human math experts (>500 hours of human labor), the benchmark focuses on metrics including step-level and response-level accuracy,  and errorID. Finally, this work evaluates a wide range of general LLMs and PRMs on the benchmark and presents insights on verification-compute scaling, self-verification biases and difficulty comparison between verficiation and solving using LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1.The paper is well written, clear, and most claims are well supported by the experimental evidence.\n\n2.The evaluation is comprehensive, and the analysis in section 5 is insightful."}, "weaknesses": {"value": "Despite the strengths mentioned above, the main weakness of this work, in my opinion, is the limited significance of this contribution, compared with existing work such as processbench (mentioned in the main text of the paper). \n1. hard2verify includes only 80 IMO questions and 200 responses, compared with 3,400 responses in processbench, which is one order of magnitude larger;\n2. one major claim in this paper is the IMO problem difficulty, however, if i understand correctly, processbench also includes omnimath and olympiadbench, both of which cover competition-level math reasoning questions. Further, processbench extends the diversity by including easier questions from gsm8k and math.\nOverall, the proposed benchmark (hard2verify) is quite limited on the scale and diversity, and therefore needs significant amount of improvement."}, "questions": {"value": "1. how do authors consider the inter-dependence of the steps? as described in the paper, all steps after a problematic step will be viewed as wrong, but how will this information be passed to the later steps when each individual step is  evaluated?\n2. can the authors clarify line 244: \"Note that this setting is less strict than the Step-Level setup: Exact step labels need not match exactly for a verifier to agree with a human at the response level.\"? what does this mean?\n3. Figure 7 is very interesting. can the authors share more insights on why there are cases that generated steps are 100% correct but verification accuracy is very low, and sometimes is even close to 0? this is a bit counter-intuitive to me.\n\nsome minor points:\n* line 177: \"contain a mistakes\" should be \"contain mistakes\"\n* appendix E: line 922,933,943 seems to be a typo of quotation marks: e.g. \\Hand-waviness\" should be \"Hand-waviness\"\n* appendix E: line 948, the list of examples is a bit hard to follow."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "p4yC0q7LsS", "forum": "4bDifzIHNh", "replyto": "4bDifzIHNh", "signatures": ["ICLR.cc/2026/Conference/Submission22161/Reviewer_m8mN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22161/Reviewer_m8mN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762092694199, "cdate": 1762092694199, "tmdate": 1762942094392, "mdate": 1762942094392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Hard2Verify, a new benchmark for step-wise verifiers in the context of advanced level math. It curates mathematical problems from international competitions (IMO, Putnam), which comprises challenging, open-ended problems that represent the frontier capabilities of recent reasoning models (GPT-5, Gemini 2.5 Pro, Claude Sonnet 4). The paper designs a careful, human expert-led step-level annotation process, comprising several review rounds on top of the responses of these reasoning models. The paper evaluates 29 different verifiers to show that Hard2Verify represents a major challenge for current models, and also provides insights on verification compute scaling, generation-verification gap, and when a qualitative analysis on the failure modes of the current models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and easy to follow.\n\n- The paper is very clear and transparent in the design principles that guides the benchmark creation, and the design choices are well justified.\n\n- The benchmark is supported by a careful and transparent methodology, and the paper describes dataset curation, response generation, and annotation process in good detail.\n\n- The benchmark comprises different evaluation modes (comprising different ways employed by the literature), as well a well-justified metric design (which is very important since step-wise annotation may potentially lead to unbalanced datasets)\n\n- Empirically, the paper evaluates a rich set of verifiers, from different sizes and categories, which allows a good perspective on the verifiers landscape.\n\n- Lastly, the empirical analysis and raised research questions in Sections 4 and 5 are relevant, well supported by the provided evidence and the insights are interesting for Verifiers’ research."}, "weaknesses": {"value": "- Major: Based on the paper content, the benchmark is still not open-sourced, and therefore its content/documentation was not available for review. \n\n- Minor: In the evaluation of verifiers (Table 2), it would be great to provide error bars to assess statistical significance of the results.\n\n- Minor: It is somewhat unclear for how long this benchmark would be relevant, as it is based on the current state of frontier models on math tasks, which we know has evolved very rapidly in the past year or so. This does not diminish its current relevance, just open questions about its long-term impact.\n\n\nOverall, the paper represents strong efforts to design a well-justified benchmark for step-level verification with good empirical insights. I believe this represents a good contribution for the current landscape of research on model-based verification. My perspective is that, from the content of the paper, it is a clear accept (score 8). Nonetheless, as the paper's major value comes from open-sourcing a benchmark for research use, I believe it is necessary to provide it during the submission/rebuttal as evidence of the work. Therefore, I am temporarily providing a score of 6 and will increase it to 8 if the authors provide a link for the data/documentation during rebuttal."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rRmmdGq97k", "forum": "4bDifzIHNh", "replyto": "4bDifzIHNh", "signatures": ["ICLR.cc/2026/Conference/Submission22161/Reviewer_Cz4k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22161/Reviewer_Cz4k"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762181915056, "cdate": 1762181915056, "tmdate": 1762942094072, "mdate": 1762942094072, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}