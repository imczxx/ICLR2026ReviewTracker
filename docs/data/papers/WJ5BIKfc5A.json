{"id": "WJ5BIKfc5A", "number": 15503, "cdate": 1758252057985, "mdate": 1759897302721, "content": {"title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient Long-Context LLMs", "abstract": "The linear growth of key-value (KV) cache memory and quadratic computational complexity in attention mechanisms pose significant bottlenecks for large language models (LLMs) in long-context processing. While existing KV cache optimization methods address these challenges through token pruning or feature merging, they often incur irreversible information loss or require costly retraining. To this end, we propose ZSMerge, a dynamic KV cache compression framework designed for efficient cache management, featuring three key operations: (1) fine-grained memory allocation guided by multi-dimensional token importance metrics at head-level granularity, (2) a residual merging mechanism that preserves critical context through compensated attention scoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM architectures without requiring retraining. ZSMerge significantly enhances memory efficiency and inference speed with negligible performance degradation across LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression ratio for key-value cache retention (reducing memory footprint to 5% of baseline) while sustaining comparable generation quality, coupled with triple throughput gains at extreme 54k-token contexts that eliminate out-of-memory failures. The code is available at https://anonymous.4open.science/r/ZSMerge-FC36.", "tldr": "We propose ZSMerge, a zero-shot KV cache compression method that achieves 82% memory reduction and 3x throughput improvement for long-context LLMs without performance degradation or model retraining.", "keywords": ["Large Language Models", "KV Cache Compression", "Memory Efficiency"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8872479f28b58af75614b10a1e13fe355d00a7a8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces ZSMerge, an efficient, comprehensive, and dynamic zero-shot algorithm designed for KV cache management in LLMs. It is motivated by the need to reduce memory consumption and break context length limits without degrading generation quality. In summary, three key features are included: 1, fine-grained memory allocation: ZSMerge analyzes the \"historical contribution of tokens, spatio-temporal characteristics, and intrinsic data distribution\" to conduct \"fine-grained, head-level memory management.\" 2, Zero-shot and low-cost Integration: ZSMerge is \"designed for ease of use\" and requires no model retraining or fine-tuning. 3, Efficient throughput-boosting strategy: The compression mechanism in ZSMerge is \"computationally lightweight.\" This results in significant improvements in inference throughput (e.g., \"over triple\" in one case) while effectively preserving token information to prevent quality degradation as context length increases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper shows good writing.\n\n2. Comprehensive experiments make this paper convincing.\n\n3. The overall logic of the algorithm does make sense."}, "weaknesses": {"value": "However, I still have some concerns and hope the author could address.\n\n1. The hyperparameter experiments of λ should be added, even though the author argues this is an insensitive parameter.\n\n2. Overclaim: The core conclusion (\"negligible decline\") in the abstract contradicts some key data (Falcon-7 b) in the text, which is an exaggeration.\n\n3. Appendix C.1 acknowledges that compression methods (including zsmerge) will \"introduce additional computational overhead\" under short sequence and low batch settings. Is it possible to provide an example?\n\n4. Why is ZSMerge worse than SNAPKV overall? Despite the author saying that SNAPKV retains a larger cache during decoding, the table (Table 3) shows that the cache size is fixed for both  SNAPKV  and ZSMerge. I fail to find out why SNAPKV retains a larger cache.\n\n5. Lack of citation: The merge method is very similar to Token Merging: Your ViT But Faster.\n\n6. Lack of explanation: How does this paper utilize the \"spatio-temporal characteristics and intrinsic data distribution\"?"}, "questions": {"value": "1. Do all models adopt the same hyperparameter setting?\n\n2. Is it possible to combine this method with quantization-based methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "skoKfIlOGQ", "forum": "WJ5BIKfc5A", "replyto": "WJ5BIKfc5A", "signatures": ["ICLR.cc/2026/Conference/Submission15503/Reviewer_BJzQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15503/Reviewer_BJzQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533945924, "cdate": 1761533945924, "tmdate": 1762925791135, "mdate": 1762925791135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines the memory and computational constraints that arise during long-context inference in large language models due to the linear expansion of the Key-Value (KV) cache (noted as the \"primary memory consumption\" in the introduction, page 1). To address these limitations, the authors introduce ZSMerge, a dynamic KV cache compression framework that operates without retraining or fine-tuning. ZSMerge allocates the cache using a three-part budget: $B_p$ stores recent tokens, $B_c$ retains high-contribution tokens, and $B_r$ holds merged representations (defined in Section 3.2, Equation 3, page 4). The approach relies on a Residual Merging mechanism together with a Compensated Attention Scoring term ( defined in Section 3.5, Equation 8 as $\\alpha \\log w_i$, page 5) to mitigate the representational bias introduced during token merging (termed 'Representation Bias Correction', page 5).\nEmpirical evaluations show that ZSMerge reaches a 20:1 compression ratio on LLaMA2-7B while preserving model quality, achieving a 3× throughput improvement at a 54k context length and preventing Out-of-Memory (OOM) failures. On benchmarks such as XSum, LongBench, and InfiniteBench, the method matches the performance of the FullKV baseline and consistently surpasses prior compression-based approaches, including H2O, StreamingLLM, and LESS."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.\tZSMerge delivers substantial efficiency improvements, achieving a 20:1 compression ratio and a 3× throughput increase while avoiding OOM failures. These gains are directly relevant to scaling long-context LLM inference in practical environments.\n2.\tBecause the framework does not require training or fine-tuning, it can be applied directly to a range of pre-trained models. The experiments confirm this portability across LLaMA, Falcon, Mistral, Qwen, and LLaMA-3.1.\n3.\tThe empirical evaluation is thorough. The authors compare ZSMerge with several strong baselines across varied tasks (summarization, QA, code generation, reasoning) and benchmarks (LongBench, InfiniteBench), demonstrating consistent performance advantages and robustness."}, "weaknesses": {"value": "1.\tAlthough the method is described as \"zero-shot,\" it introduces at least five additional hyperparameters ($B_p, B_c, B_r, \\lambda, \\alpha$) that require tuning. This tuning burden complicates practical adoption, and the sensitivity analysis in Appendix C.2 (page 15) is limited to XSum, leaving its general applicability unclear.\n2.\tThe computational overhead associated with managing the cache, particularly the similarity search in Equation 6 (page 4), is not clearly quantified. While Appendix C.1 (page 14) provides partial discussion, the trade-off relative to FullKV on shorter sequences should be clarified in the main text.\n3.\tTheorem 1 (Section 3.5, page 5) only guarantees that the attention scores of uncompressed tokens are not suppressed, but it does not ensure that the merged token $k_r$ faithfully represents its group, nor that the overall attention distribution preserves semantic fidelity. The theoretical justification therefore remains weaker than the empirical results suggest."}, "questions": {"value": "1.\tFor Appendix C.2 (page 16), were the recommended settings ($B_p/B=0.5$, $B_r/B=0.02$, $\\alpha = 1.0$ ) applied uniformly across LongBench and InfiniteBench, or were they tuned per task? If adjustments were made, how sensitive was performance to these changes?\n2.\tIn the LongBench experiments, how would both the average LongBench score and pre-fill latency change if the full history were used instead of the `window_size` approximation (mentioned in Appendix B, page 14) during the pre-fill stage?\n3.\tWhat motivated the use of incremental averaging in Equation 7 (page 4)? This approach appears sensitive to merge order and outliers. Were alternative aggregation strategies (e.g., robust averaging or weighted merging) evaluated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HZC4O8D9nO", "forum": "WJ5BIKfc5A", "replyto": "WJ5BIKfc5A", "signatures": ["ICLR.cc/2026/Conference/Submission15503/Reviewer_9eGr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15503/Reviewer_9eGr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712576521, "cdate": 1761712576521, "tmdate": 1762925790460, "mdate": 1762925790460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce ZSMerge to evict redudant KV cache in LLM inference. ZSMerge follows StreamingLLM to keep the recent local window, and devise a select strategy based on accumulated attention like H2O to preserve critical tokens. \nFurther, it merge the evicted tokens into the preserved token caches like [1] and [2]. Experiments on represetative LLMs demonstrate the effectivenss of the proposed ZSMerge in comparison with baselines like H2O and StreamingLLM."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is easy-to-follow."}, "weaknesses": {"value": "1. Figure 1 appears to be redundant and contains only superfluous information that could be succinctly summarized in a single sentence. Furthermore, the overview of token merging presented in this figure is not sound as not all merging methods tune the LLM weights.\n\n\n2. The authors' motivation does not demonstrate any distinctive aspects. Their stated objectives essentially encompass what all LLM KV compression methods aim to achieve. There is no clear differentiation from existing work.\n\n\n3. Poor writing on the introduction: \n   - The introduction is written too abstractly, making it difficult for readers to follow. First, the concept of \"fine-grained memory allocation\" is not clearly defined, and its distinction from previous methods is not adequately explained.\n   - Second, most existing methods are already zero-shot approaches, so this characteristic is not a unique merit of the proposed method.\n   - Additionally, the authors' claims about the token merging field are not well-supported. For instance, the statement that token merging can only be performed with additional networks is incorrect. There are notable counterexamples, such as [1,2].\n   \n4. Regarding the contribution evaluation, I believe Equation 5 presents significant issues in practical applications. Equation 5 appears to be merely a variant of H2O, and this accumulative approach does not work well for pre-filling acceleration. Moreover, in multi-turn conversation scenarios, the previously accumulated attention scores are unreliable.\n\n\n5. Lack of Novelty in Equations 6 and 7: As for Equations 6 and 7, I cannot identify any substantial novelty. At minimum, the authors should compare their approach with previous merging methods to clearly demonstrate the differences and improvements.\n\n\n6. Incomplete Experimental Comparisons: \n   - Some state-of-the-art methods such as Duo-Attention and PyramidKV were not considered in the experimental comparisons.\n   - The performance does not exceed that of SnapKV, and there is no efficiency comparison with SnapKV.\n\n\n7. Concerns about Efficiency Comparisons:\n   - I have serious concerns about the efficiency comparisons presented. First, I believe the implementation efficiency difference between the proposed method and H2O should be minimal, yet Table 1 shows that H2O is even 3 times slower than the proposed method, which appears completely unreasonable. I suspect there may be unfair aspects in the authors' comparison setup.\n   - A sequence length of 4096 does not represent a long-sequence decoding problem. The authors should provide acceleration results on ultra-long sequences (e.g., 128K tokens), as reported in works like Duo-Attention, to better demonstrate the effectiveness of their method on truly long-context scenarios.\n\n\n[1] CaM: Cache Merging for Memory-efficient LLMs Inference. In ICML, 2024.\n[2] Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks. In arXiv, 2024.\n[3] DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads. In ICLR, 2025"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jMqJDnfEUB", "forum": "WJ5BIKfc5A", "replyto": "WJ5BIKfc5A", "signatures": ["ICLR.cc/2026/Conference/Submission15503/Reviewer_pRqZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15503/Reviewer_pRqZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920499765, "cdate": 1761920499765, "tmdate": 1762925789155, "mdate": 1762925789155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ZSMerge, a dynamic, zero-shot framework for compressing the KV cache during LLM inference. It manages the cache using a tripartite budget allocation: Proximity (recent tokens), Context (important tokens selected via exponentially decayed cumulative attention scores), and Residual (merged representations of evicted tokens). The core mechanism involves aggregating evicted tokens into residual slots using incremental averaging based on Key similarity, combined with a compensated attention scoring mechanism that adjusts logits based on the fusion count."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "ZSMerge is training-free and does not require auxiliary networks. The approach combines eviction strategies with information preservation."}, "weaknesses": {"value": "**Q1.** Comparison against H2O, LESS, and SnapKV is insufficient to establish SOTA claims in this rapidly evolving field. The authors must discuss and experimentally compare ZSMerge against the following critical works:\nRocketKV (ICML 2025),\nKVzip (NeurIPS 2025),\nShadowKV (arXiv:2410.21465): Although cited, it is omitted from experiments, and\nDialogue Without Limits (ICML'25).\n\n\n**Q2.**  It is unclear if $T$ represents the current decoding step or the total length, and the summation index in Eq 2 (up to $T-1$) seems inconsistent with the definition of $K_T, V_T$ .\n\n**Q3.** The abstract claims a \"threefold\" (3x) increase in throughput at 54k tokens. However, Figure 3 and Section 4, report an increase from 4 tokens/sec to 9 tokens/sec, which is 2.25x, not 3x.\n\n**Q4.** The claim of \"negligible performance degradation\" is inaccurate. On Falcon-7B (Table 2), ROUGE-1 drops significantly from 27.06 (FullKV) to 15.04 (5% ZSMerge)—a substantial 44% relative degradation.\n\n**Q5.** In Table 2 (LLaMA2-7B), ZSMerge (at 5%) achieves higher ROUGE scores than the FullKV baseline. Just make sure if the implementation is correct.\n\n**Q6.** The novelty of individual components is limited. The contribution evaluation mechanism (Eq. 5), using exponentially decayed integration of attention activations, is nearly identical to the cumulative attention score mechanism in H2O. Furthermore, the concept of merging tokens instead of evicting them has been explored in works like LESS  and  Dynamic Memory Compression (Nawrot et al., 2024). While ZSMerge implements this in a zero-shot manner, the conceptual novelty is limited. \n\n**Q7.** The residual merging uses simple incremental mean aggregation based on Key similarity (Eq. 6 and 7). This approach risks \"semantic deviation\" by grouping tokens that are semantically unrelated but align in the Key space, leading to noisy Value representations. Averaging also destroys positional information and structural relationships between the merged tokens. The paper does not analyze the semantic coherence of the merged tokens.\n\n**Q8.** The evaluation focuses only on single-turn tasks. The authors must include an evaluation on a standard multi-turn benchmark, such as SCBench and multi-turn MIAH. Also, evaluation can go beyond 100K and show performance on a low token budget, like 128.\n\n**Q9.** A detailed latency breakdown of the ZSMerge operations (score updates, sorting/priority queue maintenance, similarity search) is missing. \n\n**Q10.** The paper claims low sensitivity to the decay factor $\\lambda$ but provides no data and ablation ot it. The impact of the windowed initialization during prefill is also not ablated."}, "questions": {"value": "Please address the issue raised in the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fMnYH29aNs", "forum": "WJ5BIKfc5A", "replyto": "WJ5BIKfc5A", "signatures": ["ICLR.cc/2026/Conference/Submission15503/Reviewer_AyAi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15503/Reviewer_AyAi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968727214, "cdate": 1761968727214, "tmdate": 1762925788727, "mdate": 1762925788727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}