{"id": "wxoM2ky86C", "number": 7577, "cdate": 1758028388799, "mdate": 1763013475913, "content": {"title": "DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection", "abstract": "Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits.\n  We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images—we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models—local edits using eight SOTA diffusion models; 3) Multi-turn editing—each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios—a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes.\n  DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark two baseline segmentation approaches, revealing significant challenges in segmentation tasks, particularly concerning robustness to image distortions.\n  We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based detection methods.", "tldr": "A dataset to support fine-grained segmentation of diffusion-edited local areas.", "keywords": ["AIGC Detection", "Diffusion Editing", "Segmentation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/2c9a832145de271e90dbfd7d103c98ded4f51e90.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents DiffSeg30k, a new benchmark for localized detection of diffusion-based image edits. Unlike prior datasets focusing on whole-image classification, DiffSeg30k enables pixel-level localization and model attribution. It contains 30,000 images with multi-turn edits (up to three rounds) using eight diffusion models. The authors propose an automated pipeline using Qwen2.5-VL and Grounded-SAM to identify editable regions and generate context-aware edit prompts. Two segmentation baselines (FCN-8s, Deeplabv3+) are evaluated, showing that binary localization is feasible but semantic attribution remains challenging."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Comprehensive benchmark addressing a key unmet need in AIGC detection\n- Automated, reproducible pipeline combining VLMs and diffusion models\n- Multi-turn editing and diverse diffusion sources add realism\n- Transparent evaluation of baseline performance and robustness"}, "weaknesses": {"value": "- The proposed benchmark focuses mainly on localized edits and does not effectively address global or stylistic transformations, which are also important for comprehensive AIGC detection.\n- While object addition and removal edits are valid operations, they may be less representative of subtle AIGC manipulations compared to attribute-change edits that alter visual details without structural changes.\n- Despite the automated quality filtering process, some residual artifacts or dataset biases may persist and could influence model training or evaluation outcomes.\n- It would be helpful if the authors clearly state the dataset licensing terms and distribution policy, especially given that DiffSeg30k combines COCO-based real images and diffusion-generated data. Transparent licensing will be important for downstream research use and ethical data sharing."}, "questions": {"value": "1. How does the benchmark handle global or style-based edits that lack localized masks? Would such cases be misclassified as unedited?\n2. Object addition and removal cases are valid editing types, but they may not reflect subtle AIGC manipulations. How do you view their relevance compared to attribute-change edits for advancing AIGC understanding?\n3. Does the dataset include metadata such as edit order, model parameters, or prompts to support causal or interpretability analyses?\n\n4. [MINOR - discussion] Could incorporating diffusion-aware models provide stronger baselines than standard segmentation networks?\n\nTypo: \n- Tab. 1 last row -> \"DiffSeg20k\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c08Kka1HFq", "forum": "wxoM2ky86C", "replyto": "wxoM2ky86C", "signatures": ["ICLR.cc/2026/Conference/Submission7577/Reviewer_TTCL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7577/Reviewer_TTCL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760693889398, "cdate": 1760693889398, "tmdate": 1762919671247, "mdate": 1762919671247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "bMNLiSy5ol", "forum": "wxoM2ky86C", "replyto": "wxoM2ky86C", "signatures": ["ICLR.cc/2026/Conference/Submission7577/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7577/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763013475072, "cdate": 1763013475072, "tmdate": 1763013475072, "mdate": 1763013475072, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DiffSeg30K, a benchmark designed for localized detection and attribution of diffusion-based AIGC. DiffSeg30K targets pixel-level edit localization and diffusion model identification under realistic multi-turn editing scenarios. The dataset contains 30K images with pixel-wise annotations edited by eight diffusion models, each potentially edited up to three times. Experiments using FCN and Deeplabv3+ demonstrate that while binary localization is feasible, semantic segmentation remains highly challenging, especially under distortions and multi-model scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper defines a practically relevant and comprehensive task, covering AIGC detection, localization, and model attribution.\n2. The dataset design is systematic, simulating multi-turn edits across eight diffusion models via an automated pipeline."}, "weaknesses": {"value": "1. The dataset is not the first to address AIGC detection, localization, and attribution. The paper should more carefully discuss or experimentally compare against existing datasets, clarifying its unique contributions.\n2. The automatic annotation pipeline may introduce noise and bias. Although low-quality samples are filtered using Qwen2.5-VL, the paper does not quantify annotation accuracy. The fact that roughly 50% of samples were discarded raises concerns about the stability of the generation process. Does this benchmark require some level of human validation to ensure quality?\n3. The evaluation scope is narrow, and only two segmentation baselines (FCN, Deeplabv3+) are tested. The study omits comparisons with stronger architectures and does not evaluate existing AIGC detection methods that already target detection, localization, and attribution.\n4. The significant performance drop in segmentation lacks analysis of potential causes."}, "questions": {"value": "1. Issues raised in the *Weaknesses* section.\n2. Was human validation performed during dataset construction or quality control?\n3. Have the authors considered evaluating stronger segmentation architectures or specialized AIGC detection models?\n4. What performance improvements were observed in existing AIGC detection methods by using the proposed dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KS1TJEpWh9", "forum": "wxoM2ky86C", "replyto": "wxoM2ky86C", "signatures": ["ICLR.cc/2026/Conference/Submission7577/Reviewer_iGB6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7577/Reviewer_iGB6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761476336160, "cdate": 1761476336160, "tmdate": 1762919670613, "mdate": 1762919670613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper moves beyond AI-generated vs. real images classification tasks, and turns to consider fine-grained localization and model attribution of diffusion-based image edits for an arbitrary image. Specifically, the paper introduces a benchmark of 30k images named DiffSeg30K with pixel-level annotations for multi-turn diffusion editing. Each base image (could be real or AI-generated) is edited sequentially by 1 of 8 off-the-shelf diffusion models (SD series, Flux, Glide, Kolors etc) with up to 3 turns. A vision-language pipeline (Qwen 2.5-VL + Grounded-SAM) automatically chooses editable regions and generates semantically meaningful prompts (add/remove/change). Low-quality results are filtered using a VLM-based quality score. The paper also present baseline fine-grained detection results based on FCN-8s, deeplab-v3 and existing ai detectors models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper proposes a new (to my knowledge) perspective that considers fine-grained localization and model attribution of diffusion-based image edits for an arbitrary image, instead of doing binary classification between real and AI-generated images.\n+ The paper proposes a corresponding automatic pipeline for collecting annotated images and present a 30k dataset suitable for the proposed tasks.\n+ The evaluation looks abundant on FCN-8s and deeplab-v3 with additional results on binary ai content classifier.\n+ The paper is generally well-written and easy to understand."}, "weaknesses": {"value": "- Although the paper proposes a new point of view moving from binary detection to fine-grained model attribution, can the author comment on how this new task could concretely further benefit the research community?\n- The current baseline results focus primarily on fcn-8s and deeplab-v3 models. We can observe significant improvements in terms of detection rates with better architectures. Therefore what if we use more advanced architectures and models such as ViT based detectors or other segmentation models that prove better empirical results? Will these better detectors saturate on the collected data?\n- Admittely, I am not very familiar with the current progress of this particular sub-domain. I would therefore also like to hear other colleague reviewers' opinions."}, "questions": {"value": "Please refer to the weaknesses section for detailed questions. Thanks"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZGjWEWU5dX", "forum": "wxoM2ky86C", "replyto": "wxoM2ky86C", "signatures": ["ICLR.cc/2026/Conference/Submission7577/Reviewer_uL3Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7577/Reviewer_uL3Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761614052045, "cdate": 1761614052045, "tmdate": 1762919670184, "mdate": 1762919670184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "For AI-Generated Content (AIGC) detection and localization, the proposed DiffSeg30K targets determining whether an image has been edited by generative models and segmenting the edited pixels under realistic multi-turn workflows. The dataset is constructed via a two-stage automatic pipeline: (i) a Vision–Language Model (VLM) with Grounded Segment Anything Model (Grounded-SAM) identifies editable regions and produces corresponding tags/masks; (ii) using these tags/masks as context, the VLM generates randomized editing instructions that diverse text-to-image diffusion models execute over multiple rounds to synthesize edited images, thereby producing multi-turn image/mask pairs tailored for AIGC."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The paper constructs a dataset that moves beyond single-shot edits to support multi-turn editing workflows.\n* DiffSeg30K provides an easily reproducible pipeline using only pre-trained models such as VLM (e.g., Qwen2.5-VL), open-vocabulary segmentation (e.g., Grounded-SAM), and text-to-image diffusion models (e.g., SDXL) without requiring additional training."}, "weaknesses": {"value": "**1. Limited novelty in dataset pipeline.** The current pipeline relies on vision-language models (e.g., Qwen2.5-VL) and open-vocabulary segmentation (e.g., Grounded-SAM) to generate pseudo labels before multi-turn editing. Similar pseudo-labeling strategies that couple region mining for text-to-image diffusion models are already common in existing studies [1, 2]. For technical contribution, Please consider a different labeling mechanism or provide sufficient analysis strongly related to AIGC tasks.\n\n**2. Lack of analyzing the annotation quality of DiffSeg30K.** The dataset is created by a fully automatic pipeline that selects editable regions and masks, generates editing instructions, and applies image edits across multiple rounds. The paper treats the resulting labels as reliable without evidence. There is no quantitative check of mask accuracy from Grounded-SAM, no metric that verifies whether VLM-generated instructions are grounded in the input image, and no measurement of background preservation or edit success. Well-known risks are left unmanaged, including VLM hallucination [3], failures of background preservation for editing [4], unintended object re generation during inpainting [5], and a significant performance gap between seen and unseen classes in open-vocabulary detection models [6]. Errors introduced in the first round can propagate and amplify across later rounds, which directly undermines label trust. At minimum, please define metrics for edit success or use preference-trained classifiers [7] to filter low-quality editing samples. Without these checks, annotation quality remains unverified and the downstream claims are uncertain.\n\n**3. Unclear value of multi-turn compared to single-turn.** The paper asserts the importance of multi-turn editing but does not show benefits beyond what single-turn benchmarks already provide. There is no controlled study that varies only the number of rounds while keeping the edit-type distribution comparable. No application is presented that truly requires a multi-turn edit history rather than a single-turn edit. A detailed comparison or a concrete application demonstrating unique value is needed to justify the added complexity for multi-turn editing benchmarks.\n\n**4. Outdated segmentation baselines.** The paper omits modern segmentation backbones [8, 9] that are standard in benchmark work. Strong ViT-based models (e.g., EOMT [8]) and popular decoders (e.g., Mask2Former [9]) should be included in the Experiment section.\n\n**5. Robustness limited to simple image transforms.** In Table 3, the robustness study only considers JPEG compression and resizing. Following the common protocol [10] for robustness, adversarial corruptions should be part of the evaluation. \n\n**6. Only LoRA analysis without full fine-tuning.** Table 4 reports segmentation shifts after LoRA fine-tuning on SDXL, which is too narrow to claim stability. Full fine-tuning with the same architecture on large-scale data and comparisons across additional generator backbones are needed to assess localization and attribution stability.\n\n**7. Readability.** For example, Figure 1 labels the dataset as “DiffSeg20K” while the title and abstract use “DiffSeg30K”; the naming should be unified. For qualitative examples, pair each panel with the exact instruction and include a concise color legend (see Figure 6). These fixes reduce confusion and make the visual evidence support the claims more clearly.\n\n\n[1] TokenCompose: Text-to-Image Diffusion with Token-level Supervision, NeurIPS 2024. \n\n[2] CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching, NeurIPS 2024.\n\n[3] Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance, ICML 2025.\n\n[4] Early Timestep Zero-Shot Candidate Selection for Instruction-Guided Image Editing, ICCV 2025.\n\n[5] Attentive-Eraser: Unleashing Diffusion Model's Object Removal Potential via Self-Attention Redirection Guidance, AAAI 2025.\n\n[6] Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection, ECCV 2024.\n\n[7] Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation, NeurIPS 2023.\n\n[8] Your ViT is Secretly an Image Segmentation Model, CVPR 2025.\n\n[9] Masked-attention Mask Transformer for Universal Image Segmentation, CVPR 2022.\n\n[10] Anyattack: Towards Large-scale Self-supervised Adversarial Attacks on Vision-language Models, CVPR 2025."}, "questions": {"value": "Q1. What component or analysis is new beyond open-vocabulary pseudo-labeling pipelines? Can you show a head-to-head against alternatives inspired by [1, 2] to justify design choices?\n\nQ2. Will you add a small human feedback (e.g., 300–500 samples) to quantify mask accuracy, instruction grounding, and edit success?\n\nQ3. What concrete metrics will you report for instruction grounding and for edit success/background preservation? \n\nQ4. Under matched edit budgets, does increasing the number of rounds change difficulty or failure modes versus single-turn? Please provide a controlled comparison.\n\nQ5. Will you include modern segmentation backbones and decoders [8, 9]?\n\nQ6. Beyond SDXL with LoRA fine-tuning, please evaluate full fine-tuning with the same architecture and other text-to-image diffusion models for localization/attribution stability."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "The pipeline can generate NSFW or otherwise unsafe content during VLM instruction generation and diffusion inpainting, including sexual or violent edits. Please harden prompting (safety-aware instruction templates and blocklists) and add post-generation filters for edited images (e.g., NSFW/violence classifiers),."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z1ewr6Hboj", "forum": "wxoM2ky86C", "replyto": "wxoM2ky86C", "signatures": ["ICLR.cc/2026/Conference/Submission7577/Reviewer_fcVC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7577/Reviewer_fcVC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761720532236, "cdate": 1761720532236, "tmdate": 1762919669685, "mdate": 1762919669685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}