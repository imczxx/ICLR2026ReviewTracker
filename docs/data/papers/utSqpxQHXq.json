{"id": "utSqpxQHXq", "number": 4507, "cdate": 1757691327532, "mdate": 1759898029121, "content": {"title": "Two failure modes of deep transformers and how to avoid them: a unified theory of signal propagation at initialisation", "abstract": "Finding the right initialisation for neural networks is crucial to ensure smooth\ntraining and good performance. In transformers, the wrong initialisation can\nlead to one of two failure modes of self-attention layers: rank collapse, where\nall tokens collapse into similar representations, and entropy collapse, where\nhighly concentrated attention scores lead to training instability. While previous work has studied different scaling regimes for transformers, an asymptotically exact, down-to-the constant prescription for how to initialise transformers has so\nfar been lacking.  Here, we provide an analytical theory of signal propagation\nthrough deep transformers with self-attention, layer\nnormalisation, skip connections and MLP. Our theory yields a simple algorithm to compute trainability diagrams\nthat identify the correct choice of initialisation hyper-parameters for a given\narchitecture. We overcome the key challenge, an exact treatment of the self-attention layer, by establishing a formal parallel with the Random Energy Model from statistical\nphysics. \nWe also analyse gradients in the backward path and determine the regime where gradients vanish at initialisation. We demonstrate the versatility of our framework through three case studies. Our theoretical framework gives a unified perspective on the\ntwo failure modes of self-attention and gives quantitative predictions on the\nscale of both weights and residual connections that guarantee smooth training.", "tldr": "We provide a complete analysis of signal propagation across a Transformer block with self-attention, residual connections, LayerNorm, and a ReLU MLP, making it possible to predict the trainability of deep Transformers.", "keywords": ["Transformer", "Signal Propagation", "Theory of Neural Networks", "Physics for Machine Learning"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0d13c820b97f514c008187f46fc2c6863a6b925c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper analyses the signal propagation, i.e. the evolution of similarities between token embeddings in the self-attention layer and other layers of the transformer architecture. The paper focuses on the infinite context length limit (while also discussing the finite number approximations), or regimes where the context length is very large. They compute the correct asymptotic context length-dependent initialization scaling of the attention parameters that makes the limit non-trivial and also compute the concrete multiplication-constant threshold under which the attention matrix converges to uniform attention, causing rank collapse and above which the attention matrix converges to entropy-collapsed one where tokens only focus on finite number of other tokens despite asymptotic context length. Both these regimes constitute trainability issues. The authors further analyze the size of the gradient and show that it is vanishing in the small initialization regime. The authors then analyze the strength of the residual connection, which can alleviate the rank collapse in the small initialization regime. The authors obtain concrete algorithm to determine the trainability for given hyperparameters, which is then used to construct trainability diagrams. Finally, authors experimentally confirm their theory and evaluate some methods to boost trainability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "-\tS1: The theoretical contribution of the paper is quite strong and, to the best of my knowledge, novel. Regarding novelty: while there are some pieces of the puzzle scattered around the related works, none provides such quantitative characterization, especially in the infinite context length limit or large contexts. None of the works unifies rank and entropy collapse regimes with precise threshold that divides them. The paper also convincingly discusses the non-asymptotic deviations, provides precise gradient norm analysis (novel) and connects it with the rest of the transformer (partially novel). Regarding the strength, I think the quantitative nature of the results, reasonable scaling regime and the potential for practical use make the paper useful for both theoreticians and practitioners. \n-\tS2: Besides being mostly novel, the authors seem to also accurately contextualize their novelty respective to the related work. I manually checked the related work (and also used AI to double-check) and it seem the authors correctly cited the literature in places where their results built on or aligned with some of the previous results. \n-\tS3: The paper is mostly quite clear and fairly easy to read."}, "weaknesses": {"value": "-\tW1: Despite general clarity, at times the paper would benefit from more detailed discussions. For instance, it should be discussed in the main body, why do the results require IID embeddings. If I guessed it correctly from the Appendix it seems that only to guarantee some geometric properties in the first layer. But this should be discussed because if this is not the case, then it is not clear how the IID property applies in the intermediate layers. Another example is that terms used in Equation 7 are only defined in Equation 9. Some statements are a bit vague and not justified-enough. In particular the discussion in lines 401-405 seems to reflect authors’ intuition, but I don’t see why this should be the correct explanation. Furthermore, Figure 1(e) would benefit a legend. I raise a couple more unclear points in the questions section. \n-\tW2: One peculiar decision is to scale up the residual connections instead of the output of the modules (for instance value matrix or MLP). As far as I am concerned, this doesn’t provide any benefit compared to the other option and, to the contrary, in the case of pre-norm LN causes exponential growth of embeddings, which is a big issue because the contributions of attention layers and MLPs will diminish at initialization as we go deeper through the network (plus, numerical instability might be an issue at large depths). \n-\tW3: The paper could go a bit more into depth about how to use Algorithm 1 to effectively determine the correct per-layer $\\beta$ values in practice. \n\nTypos: \n\n-\tl125 “applicaitons”\n\n-\tl141 fig 1c\n\n-\tl228 “initialise”\n\n-\tl319 the $\\tau$ should be indexed"}, "questions": {"value": "-\tQ1: To what extent are your proofs inspired by the either of the cited works [1,2,3] or some other works? \n-\tQ2: You haven’t disclosed LLM usage. I assume this means you haven’t used LLMs beyond some text adjustments and grammar checks. Is that right? \n-\tQ3: In Figure 1e, would the red and orange figures eventually decrease all the way down to blue curves or do you think they are stuck in local minima? \n-\tQ4: The normal distribution assumed in line 229 of the attention scores is also enforced in practice? If not, can it interfere with your theory because you are potentially creating heavier-tailed distributions? \n-\tQ5: In Figure 3(a), why does the entropy for the red curves actually go down? I understand they start lower than the blue curves, which is predicted by your theory, but why go down? As you show in the Appendix, smaller learning rate can alleviate this. Is it only an optimization issue then? Could LR warm-up help? \n-\tQ6: In result 2, you normalize with $T/log(T)$. This basically means that per-weight gradient converges to zero at an inverse rate. Does this mean we get vanishing gradients even in the optimal case? \n-\tQ7: What can be said in result 1 when $\\beta=\\beta_c$?\n-\tQ8: Can you elaborate on point (1) in line 396? How can we see this statement? \n-\tQ9: Can you elaborate on your discussion in lines 401-405? How can we see that this actually explains the phenomenon? \n-\tQ10: How do your results adjust in the case of multi-head attention? \n-\tQ11: More importantly, what can we learn about causal attention? It seems to me that causal attention cannot be solved by initialization and your results sort-of show it. Is that right? What would you recommend practitioners instead? \n-\tQ12: In line 463 you say that layer normalization is crucial to avoid entropy collapse. Where is this shown in your work? \n\n[1] Carlo Lucibello and Marc Mézard. Exponential capacity of dense associative memories. Physical\nReview Letters, 132(7):077301, 2024.\n\n[2] Alireza Naderi, Thiziri Nait Saada, and Jared Tanner. Mind the gap: a spectral analysis of rank\ncollapse and signal propagation in transformers. arXiv preprint arXiv:2410.07799, 2024.\n\n\n[3] Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien\nLucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse,\n2022. URL https://arxiv.org/abs/2206.03126.\n\n**Summary:**\nI consider the paper to have strong and novel contribution. While there are some aspects that can be improved and some questions that deserve answering, I think the paper should at least be accepted. Based on the discussion period and other reviewers’ points, I will then try to refine my judgement so as whether to recommend also spotlighting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CUPW2bTYod", "forum": "utSqpxQHXq", "replyto": "utSqpxQHXq", "signatures": ["ICLR.cc/2026/Conference/Submission4507/Reviewer_5iSN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4507/Reviewer_5iSN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761338676761, "cdate": 1761338676761, "tmdate": 1762917409845, "mdate": 1762917409845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to use the REM to provide an asymptotic characterization of signal propagation in Transformer (self-attention layer, skip connections, Layer Norm, MLP) and explain how different initialization scalings cause rank and entropy collapse."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides a precise asymptotic analysis on the evolution of the average cosine similarity and the average IRP with respect to the initialization scaling."}, "weaknesses": {"value": "- There are many unclear arguments.\n    - Why is the variance of the attention scores $\\sigma_a^2$? (Doesn't it depend on the value of the variance of $X_t$?)\n    - Sec B.1 L666 says $\\sigma_Q^2=\\sigma_K^2=\\sigma_a^{\\color{red}2}/d$ which is different from eq (8). Also the conclusion in (15), $Cov(a_{ts}a_{\\tau\\sigma})=\\sigma_a^2q_{ts}q_{s\\sigma}$ is $d^2$ times larger than eq (7).\n    - Why does the attention score variance should $O(\\log T)$?\n\n- The asymptotic prediction (Result 1) does not perfectly match the empirical results. The authors said that this is because $T$ is finite. How about the nonasymptotic results with concentration bounds? What happens if we increase $T>1024$?\n\n- Why $\\tilde \\beta_c=\\beta_c/2$ does not act as a critical value in Fig 3?\n\n- Violation of the iid assumption means that (13) may not hold, but it does not answer why gradient does not vanish with the skip connection. Why do we have nonvanishing gradients when $\\beta<\\beta_c$ with skip connection? Also it seems like the iid assumption is violated during training, even without skip connection, so we may not conclude that $\\beta<\\beta_c$ implies vanishing gradients."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UT23z1rIjA", "forum": "utSqpxQHXq", "replyto": "utSqpxQHXq", "signatures": ["ICLR.cc/2026/Conference/Submission4507/Reviewer_1xRi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4507/Reviewer_1xRi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832642204, "cdate": 1761832642204, "tmdate": 1762917409600, "mdate": 1762917409600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper conducts a unifying analysis of two failure modes in signal propagation in Transformer models, namely the rank and entropy collapse, under the assumption of infinite width and sequence length. The authors use a known mapping of the self attention matrix to a Random Energy Model (ERM) [1], which allows them to conduct analysis that, contrary to the previous work, neither assumes uniform attention scores, nor taking expectations in the numerator and denominator of the self-attention separately.\nFollowing that they propose a new sequence-length dependent initialization scheme for the query and key weights, and derive the formula for the evolution of the average cosine token similarity as a function of the sequence-length independent initialization variance and the input embedding correlation. They also extend our understanding of the vanishing gradient problem beyond the uniform attention assumption, and provide an analysis of a full Transformer block, which lets them construct cosine-similarity evolution diagrams also for full Transformers. They verify their theoretical results with experiments on a BERT-style Transformer models.\n\n[1] Carlo Lucibello and Marc Mézard. Exponential capacity of dense associative memories. Physical Review Letters, 132(7):077301, 2024."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Unifying view of two failure modes of attention is novel and valuable for the signal propagation community. I think that working under less strict assumptions than previous work (uniform attention, separate treatment of nominator and denominator in attention) is valuable.\n2. Using ERM in the context of signal propagation analysis is novel and original.\n3. The part of the paper that focuses on a single self-attention layer is good quality and well presented. It’s commendable that the authors extend their analysis to a full Transformer block, and provide experiments for deep Transformer models."}, "weaknesses": {"value": "I’m open to increasing my score if the authors address the following weaknesses:\n\n1. The analysis of the average cosine similarity while passing through the full Transformer block (Section 4) is rushed. I believe some other parts could be shortened (like the setup in Section 2) in order to make space for a more thorough analysis of the full Transformer block. The result from eq. 14 is not interpreted and authors do not clearly walk the reader through how they arrive at Algorithm 1 from it. I believe this type of analysis is valuable but should be presented better in the main text of the paper.\n2. The analysis of the propagation in terms of $\\beta$ and $\\beta_c$ seems to be done only for a single layer Transformer. I would be curious to see some discussion and some experiments similar to what’s presented in Figure 3d on how it behaves with increased depth. Especially, as the paper already discusses deeper models, but focuses only on the residual scaling $\\alpha_{SA}$.\n3. The experiments concern only BERT-style architecture. I believe the authors should provide some empirical results and comment on how their theoretical result and proposed algorithms for obtaining trainability diagrams behave in architectures with causal masking, like GPT-style autoregressive Transformer. These architecture is arguably more relevant for the DL community these days.\n4. There is no discussion about the gradients in the entropy-collapse regime. Section 3.3 just discusses the gradient formula for $\\beta < \\beta_c$. I would be interested in seeing if there is a way to explain with gradients how it happens that for large $\\beta > \\beta_c$ we get stuck in the entropy collapse regime.\n5. Regarding figures: 1) Figure 3c does not have a legend. 2) I would appreciate seeing also the training loss curves (in the appendix) for the experiments for which the evaluation/validation loss is presented."}, "questions": {"value": "1. Can you explain (briefly in the main text of the paper and maybe more in detail in the appendix) how one arrives at equations 7-15 from algorithm 1? As I understand it, it comes from previous work, but it would be useful to walk the reader through how one arrives at these formulas.\n2. How does $\\beta_c$ depend on the model depth, assuming models with and without layer normalization? How does $\\beta_c$ depend on the layer number assuming a model with a fixed depth (with and without layer normalization)?\n3. Can you produce a trainability diagram in the deep transformer case like the one in Figure 1d, just for varying values of $\\beta$?\n4. Is the entropy collapse regime also associated with a vanishing gradient problem? From fig. 1e it seems that models in this regime quickly stop learning. If the whole norm of the gradient in eq. 13 does not disappear, maybe the gradients wrt to just some subset of query and key parameter do? Do the attention scores stay roughly the same (with the same few large scores) during training or do they change between many low-entropy settings?\n5. How does masking influence the results? Would a causal mask like in GPT-style attention change Result 1 and eq. 14? How would the phase diagram from fig. 2 look under attention with a causal mask?\n6. In the paper $q_{tt}$ is referred to as the token norm (for example in line 295). Are you sure it is not a squared token norm?\n\nSuggestions on writing:\n* The sentences around eq 14. could better suggest that the equation describes propagation through a self-attention layer with skip-connection. From the section title I was expecting a single equation describing the full Transformer block, and the sentence leading to the equation just mentions skip-connection.\n* Lack of consistency with capitalising Transformer/transformer (lines 14 and 42 for example)\n* Broken sentence in line 113 (the strength the scale)\n* Line 131: „highly highly”\n* While I appreciate the explanation in appendix B.1, the writing there could be improved (take a look at the second sentence in line 652).\n* $q_{ts}$ is used in line 233 before its defined way later in the paper"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ru2nVvBaVc", "forum": "utSqpxQHXq", "replyto": "utSqpxQHXq", "signatures": ["ICLR.cc/2026/Conference/Submission4507/Reviewer_X3PM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4507/Reviewer_X3PM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921737649, "cdate": 1761921737649, "tmdate": 1762917409272, "mdate": 1762917409272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a unified analytical theory to explain and predict two failure modes of deep Transformer networks due to initialization: rank collapse and entropy collapse. The authors' central innovation is to establish a formal parallel between the self-attention mechanism and the Random Energy Model (REM) from statistical physics. This mapping allows them to analyze the behavior of self-attention as a phase transition. The authors present a parameter, $\\beta$ (related to the variance of query/key weights), that functions like \"inverse temperature\" in the physics model. The theory finds that operating below a critical threshold ($\\beta < \\beta_c$) leads to rank collapse, while operating above it ($\\beta > \\beta_c$) leads to entropy collapse. The paper's practical solution is to operate in the rank-collapse regime (to avoid the more fatal entropy collapse) and use strong residual connections ($\\alpha_{SA}$) to preserve signal diversity. The framework yields a simple algorithm to compute \"trainability diagrams,\" which provide a principled and quantitative guide for selecting initialization hyperparameters (weight scale and residual strength) to ensure stable training. The authors' theoretical predictions are shown to closely match empirical measurements in BERT-style models."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's strengths lie in its rigorous theoretical approach to a critical practical problem.\n\nStrong Theoretical Analysis: The paper provides a comprehensive and asymptotically exact analytical framework. It goes beyond simple heuristics to derive precise update equations for token similarity (Result 1) and an exact expression for the gradient norm at initialization (Result 2). This allows it to analyze both forward signal propagation and backward gradient flow, providing a complete picture of the network's dynamics at initialization.\nNovel Parallel with the Random Energy Model (REM): The analogy to the REM is the paper's key insight. By formally mapping the self-attention $\\text{softmax}$ (Eq. 3) to the Boltzmann distribution (Eq. 6), the authors import a powerful analytical toolbox from statistical physics. This allows them to model the complex, random behavior of the attention layer and identify the \"inverse temperature\" parameter $\\beta$ (Eq. 8) as the crucial knob controlling the system's behavior.\nEffective Visualization of the Phase Transition: The REM analogy naturally leads to the discovery of a sharp phase transition at a critical threshold $\\beta_c$. This core finding is visualized clearly and effectively. Figure 2 presents the theoretical phase diagram for a single layer, while Figure 1c translates this into a practical \"trainability diagram\". This diagram provides a clear map for clearly delineating the trainable zone from the Rank Collapse and Entropy Collapse zones."}, "weaknesses": {"value": "My main concern is the lack of convincing experiments supporting the framework. While it is totally fine that the paper's focus is theoretical, it's a shame that there is no better evidence to support what is ultimately a very important practical problem - specifically when strong evidence would not be so difficult to provide. The core results are derived in the \"limit of infinite sequence length.\" The authors acknowledge this creates \"finite-size effects\" and a \"discrepancy between theory and simulations\" (Fig. 2) in real-world. Also, the gradient analysis (Result 2) assumes token embeddings are i.i.d. (independently and identically distributed). The authors note this is only true at the moment of initialization ($t=0$) and is immediately broken by the very skip connections their solution relies on. Both these theoretical assumptions alone justify better experimental evidence for the hypothesis. \n\nFurthermore, better support of how should the community benefit from more informed initialization - which is the whole point of the paper given the abstract - would be welcome. \n\nFinally, I find the paper needs some restructuring and rewriting. Here are some pointers: Section 4 for instance is standing alone, and a bit odd with the rest of the paper. A standard \"Related Works\" section would be good, as opposed to simply a \"Further Related Works\" paragraph after stating the main paper contributions. A discussion section as a 3.2.3, followed by 3.3 The Backward Pass also reads rather strangely. You mention training a 20 layer BERT (line 472) in the Main manuscript, but in the supplementary material, it becomes a 30 layer BERT model (line 1214) - which is true? In addition, the paper is filled with typos..."}, "questions": {"value": "The authors map self-attention to the Random Energy Model, but REM assumes independent energies while attention scores are correlated. How do these correlations affect the validity of the phase transition predictions, especially for moderate sequence lengths where finite-size effects matter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "snvzKZROZS", "forum": "utSqpxQHXq", "replyto": "utSqpxQHXq", "signatures": ["ICLR.cc/2026/Conference/Submission4507/Reviewer_zigt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4507/Reviewer_zigt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943460421, "cdate": 1761943460421, "tmdate": 1762917408879, "mdate": 1762917408879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}