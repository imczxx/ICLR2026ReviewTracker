{"id": "f07Kf4pD0f", "number": 13647, "cdate": 1758220352521, "mdate": 1759897422586, "content": {"title": "Expressiveness of Multi-Neuron Convex Relaxations in Neural Network Certification", "abstract": "Neural network certification methods heavily rely on convex relaxations to provide robustness guarantees. However, these relaxations are often imprecise: even the most accurate single-neuron relaxation is incomplete for general ReLU networks, a limitation known as the \\emph{single-neuron convex barrier}. While multi-neuron relaxations have been heuristically applied to address this issue, two central questions arise: (i) whether they overcome the convex barrier, and if not, (ii) whether they offer theoretical capabilities beyond those of single-neuron relaxations.\nIn this work, we present the first rigorous analysis of the expressiveness of multi-neuron relaxations. Perhaps surprisingly, we show that they are inherently incomplete, even when allocated sufficient resources to capture finitely many neurons and layers optimally. This result extends the single-neuron barrier to a \\textit{universal convex barrier} for neural network certification. \nOn the positive side, we show that completeness can be achieved by either (i) augmenting the network with a polynomial number of carefully designed ReLU neurons or (ii) partitioning the input domain into convex sub-polytopes, thereby distinguishing multi-neuron relaxations from single-neuron ones which are unable to realize the former and have worse partition complexity for the latter.\nOur findings establish a foundation for multi-neuron relaxations and point to new directions for certified robustness, including training methods tailored to multi-neuron relaxations and verification methods with multi-neuron relaxations as the main subroutine.", "tldr": "We show that multi-neuron relaxations alone may never be complete verifiers, and the conditions required to enhance them to be complete.", "keywords": ["certification", "convex relaxation", "theory"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2de6000a8b6e00d5eb936092e110397cc848af65.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the theoretical expressive power of multi-neuron convex relaxations in neural network certification. \nThe author systematically proves for the first time that even allowing layerwise multi-neuron relaxation, such methods still cannot achieve complete certification, thereby extending the 'single-neuron convex barrier' to a universal convex barrier.\nAt the same time, the paper also presents positive results: through equivalency-preserving network transformations or partitioning the input domain into convex sub-polytopes, multi-neuron relaxation can achieve completeness while maintaining the full expressiveness of ReLU networks.\nThe author further demonstrates that its partition complexity under the branch-and-bound is lower than that of the single-neuron method.\nOverall, this paper theoretically establishes the expressive boundaries of multi-neuron relaxations, clarifies its limitations and potential advantages, and provides a solid foundation for subsequent robust training and verifiable algorithms."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Significant Theoretical Contribution: The paper systematically analyzes for the first time the expressiveness and completeness of multi-neuron convex relaxations, introducing the concept of the \"universal convex barrier,\" significantly extending the existing theoretical boundaries of single-neuron barriers.\n2. Rigorous Analysis and Complete Proofs: The author demonstrates clear logic in formal definitions, lemmas, and theorem derivations, with strict mathematical reasoning. The core conclusions(such as the incompleteness of multi-neuron and the conditions for completeness) are all supported by rigorous proofs.\n3. Both positive and negative outcomes, balanced viewpoints: While revealing the inherent limitations of multi-neuron approaches, the paper proposes two constructive schemes to achieve completeness (structural transformation and polytope partitioning), which are theoretically significant."}, "weaknesses": {"value": "1. Insufficient discussion on feasibility: The proposed completion methods (structural transformation and polytope partitioning) are theoretically valid, but their computational complexity, scalability, and operability in large-scale networks have not been analyzed, so their practical application value remains unclear.\n2. The mathematical processes and formula representations in the examples in Part Three are not clear enough, which may affect the understanding of subsequent sections.\n3. The ⫋ symbol may not be clear in some fonts."}, "questions": {"value": "1. Regarding the scope of 'universal convex barriers':Does this barrier apply to all forms of convex relaxations? Can the authors clarify its applicable boundaries and potential exceptions?\n2. Measure of boundary relaxation: The text mentions that 'the relaxation error can be arbitrarily large,' and ‘relaxation error can be unbounded.’but it does not define the specific way the error is measured. Does it refer to output range error, boundary gap, or some other norm?\n3. Applicability to non-ReLU activations: Although the paper claims that the conclusions can be extended to non-polynomial activations such as sigmoid and tanh, the proof is only briefly outlined. Could you provide a more detailed explanation of the key assumptions and limitations of this generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K57rAcPM09", "forum": "f07Kf4pD0f", "replyto": "f07Kf4pD0f", "signatures": ["ICLR.cc/2026/Conference/Submission13647/Reviewer_ZUoG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13647/Reviewer_ZUoG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656529248, "cdate": 1761656529248, "tmdate": 1762924221755, "mdate": 1762924221755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical analysis on the properties of multi-neuron convex relaxations when used for certifying the robustness of neural networks. Specifically, the authors extend the result from previous work that single-neuron convex relaxations are incomplete and show that multi-neuron convex relaxations are also incomplete (both layerwise and cross-layer). However, the paper then discusses a way to create a transformed network from any ReLU network for which multi-neuron convex relaxations can provide complete bounds. The authors also analyze the partitioning required for complete algorithms that use multi-neuron convex relaxations and show that it requires less partitioning complexity than single-neuron relaxations. The authors conclude with a discussion of the implications of their results in practice and recommendations for future work."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is extremely well-written. The figures provide helpful visualizations for understanding the intuition behind each result.\n- The authors do a great job of backing up the math and theoretical results with intuitive wording and concrete examples.\n- The paper presents new theoretical results that extend past results from the single-neuron case to the more general multi-neuron case.\n- The authors clearly describe the practical implications of their work as well as the potential promising avenues for future work in neural network verification algorithm design based on their theoretical results.\n- The authors also discuss briefly how their results can be extended beyond ReLU networks."}, "weaknesses": {"value": "The paper is focused largely on completeness for neural network verification. However, complete algorithms may not be necessary as long as the bounds from incomplete algorithms are tight enough."}, "questions": {"value": "How do you think the practical implications of the theoretical results fit in with the increased complexity of creating and working with multi-neuron convex relaxations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HbHZDlHbAb", "forum": "f07Kf4pD0f", "replyto": "f07Kf4pD0f", "signatures": ["ICLR.cc/2026/Conference/Submission13647/Reviewer_wZ3Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13647/Reviewer_wZ3Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761680850089, "cdate": 1761680850089, "tmdate": 1762924221456, "mdate": 1762924221456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the theoretical limits of convex-relaxation–based neural network certification. Previous work established that single-neuron convex relaxations are inherently incomplete, which is the so-called single-neuron convex barrier. This paper generalizes that observation to multi-neuron and cross-layer relaxations, claiming a universal convex barrier: even the strongest finite convex relaxation cannot achieve completeness for all networks. The authors further show that completeness can be restored through either (i) equivalence-preserving network transformations or (ii) convex partitioning of the input domain, and provide some complexity analysis comparing single- and multi-neuron settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The result unifies several scattered intuitions in the verification literature into a single statement.\n- The theoretical arguments are self-contained and the structure of the results is clear."}, "weaknesses": {"value": "- The central impossibility result follows almost directly from the geometric fact that convex hulls of non-convex sets are necessarily loose. A simple 2-layer, 2-neuron ReLU MLP already exhibits this property for any convex relaxation, regardless of neuron grouping or relaxation design. Thus, while the generalization to “all convex relaxations” is formally nice, it feels tautological to readers familiar with convex geometry and ReLU verification.\n\n- The conclusion that completeness can be recovered by network reformulation or domain partitioning — is already implicit in existing verification frameworks (e.g., PRIMA, β-CROWN, Planet + BaB hybrids). The paper primarily reiterates these insights under a more general theoretical framework.\n\nOverall, the paper's main results are mostly self-evident and insufficient for an ICLR paper."}, "questions": {"value": "Is there any class of networks (e.g., affine-coupled, monotone, or linearly separable structures) for which completeness of convex relaxations can in fact be achieved without partitioning? It is a more interesting and non-trivial question than the result in this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xSthSE5qOO", "forum": "f07Kf4pD0f", "replyto": "f07Kf4pD0f", "signatures": ["ICLR.cc/2026/Conference/Submission13647/Reviewer_sc2X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13647/Reviewer_sc2X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757394075, "cdate": 1761757394075, "tmdate": 1762924221096, "mdate": 1762924221096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission analyzes the tightness a family of convex relaxations of neural networks. Convex relaxations are a fundamental tool for neural network verification, and are used to compute bounds on network (pre-)activations.\nThe authors focus on so-called \"multi-neuron\" relaxations, and in particular on $\\mathcal{P}_1$, which captures the convex hull of any single network layer.\nResults on its incompleteness for general networks are presented, and then followed by results on how to exploit these relaxations towards complete verification (that is, avoiding any loss of accuracy in the bounding computations)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The theoretical study of the tightness of network convex relaxations is definitely an important topic for the area.\nResults-wise, I think the main contribution is Proposition 5.3, which states that $\\mathcal{P}_1$ is \"enough\" to exactly describe any locally convex part of the network. While this is not groundbreaking, I found the result interesting, along with the discussion of the resulting partition complexity."}, "weaknesses": {"value": "A general weakness of the paper is that it is not \"operational\": it is exclusively theoretical, and while a short discussion of the potential implications of the results is provided, I think these results are very far from being practically useful in the area.\nA purely theoretical paper can be of course a great contribution to the literature, but I do not think the results here presented are impactful enough for that to hold.\n\nSpecifically, I think most of the presented results (except Proposition 5.3) are extremely underwhelming, as they eventually all boil down to the following statement: if the convex relaxation is not the convex hull of the entire network, the bounds will be incomplete. \nNote that the fact that optimizing a linear function over the convex hull of a set $S$ will yield the same result as optimizing over $S$ is a common textbook result in convex analysis.\n1) Sections 3 and 4 are devoted to showing that sequentially applying $\\mathcal{P}_1$ (the convex hull of a single layer) and $\\mathcal{P}_k$ (unless $k$ is the number of layers) will not yield the convex hull of the entire network. I do not quite see why it could have been the case. For instance, the Triangle relaxation is clearly the convex hull of the ReLU alone, but composing it with the preceding affine layer will not result in the convex hull of the composition.\n2) Theorem 5.1 is, I believe, just a trick to basically condense the entire input-output relationships of the whole network into a single layer, for which $\\mathcal{P}_1$ will then correspond to the convex hull of the entire network. In other words, the complexity of computing the network's convex hull is just hidden through the reformulation."}, "questions": {"value": "- It feels to me that the submission is not appropriately placed within the context of the wider convex analysis literature. Analyzing the tightness of convex relaxations is for instance extremely important within Mixed-Integer Linear Programming (MILP). Given that neural network verification over piecewise-linear function is a MILP, there are relevant results from that community [1] which should be at least cited and, better, put in relation with the presented results.\n\n- Do you see any way the lower partition complexity of multi-neuron convex relaxations could be exploited in practice over general neural networks?\n\n[1] Strong mixed-integer programming formulations for trained neural networks, Mathematical Programming, 2020, Anderson et al."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QuKHvz8h9j", "forum": "f07Kf4pD0f", "replyto": "f07Kf4pD0f", "signatures": ["ICLR.cc/2026/Conference/Submission13647/Reviewer_cWwu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13647/Reviewer_cWwu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13647/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763613107, "cdate": 1761763613107, "tmdate": 1762924220758, "mdate": 1762924220758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}