{"id": "Smc4U2aDzW", "number": 4521, "cdate": 1757696432477, "mdate": 1762965179348, "content": {"title": "Omniagent: Long-Video Generation via Cross-Modal Multi-Agent Orchestration", "abstract": "Recent advancements in multi-agent systems have demonstrated significant potential for enhancing creative task performance, such as long video generation. This study introduces three innovations to improve multi-agent collaboration. First, we propose OmniAgent, a hierarchical, graph-based multi-agent framework for long video generation that leverages a film-production-inspired architecture to enable modular specialization and scalable inter-agent collaboration. Second, inspired by context engineering, we propose hypergraph nodes that enable temporary group discussions among agents lacking sufficient context, reducing individual memory requirements while ensuring adequate contextual information. Third, we transition from directed acyclic graphs (DAGs) to directed cyclic graphs with limited retries, allowing agents to reflect and refine outputs iteratively, thereby improving earlier stages through feedback from subsequent nodes. These contributions lay the groundwork for developing more robust multi-agent systems in creative tasks.", "tldr": "", "keywords": ["Multi-agent systems; Cross-modal generation; Short-video generation; Centralized / decentralized orchestration; Script-to-storyboard grounding;  Autonomous video editing;"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/73858cd96c2c5ef0aa0d8f7c3a61c0fe65eab3bc.pdf", "supplementary_material": "/attachment/d827ef483aafccd29d9ef7d388994de60fede96b.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes OmniAgent, a hierarchical, graph-based multi-agent framework for long video generation. The hypergraph-based context retrieval mechanism is introduced that enables on-demand, collaborative knowledge gathering across agents, balancing context richness with memory efficiency. A controlled cyclic execution strategy is proposed, which allows limited back\u0002ward edges for iterative refinement and reflection."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of integrating DAG and context engineering into multi-agent systems is interesting and inspiring. \n2. The evaluation is extensiv and thorough."}, "weaknesses": {"value": "1. The differences among multi-agent work for video generation (e.g. MovieAgent) are not clear. \n\n2. The problems aimed to be solved are not clear in the paper. \n\n3. Rely on advanced models (e.g. GPT-4o). What if these advanced models are not available or evolve? What are the performances with less advanced models? \n\n4. In my view, I do not see much difference in qualitative videos compared to baselines (in supplementary). As the opinion is subjective, I request that the qualitative results of the comparison be properly explained."}, "questions": {"value": "1. Does the paper aim to mimic the film production workflow in real world, or aim to solve the problems of current multi-agent works? If is to mimic the workflow, why are the designs of hyper graph-based context collaboration matter? Other designs (hierarchical CoT in MovieAgent) may take effect.\n\n2. What are the effects of private memory?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7ZlTonjast", "forum": "Smc4U2aDzW", "replyto": "Smc4U2aDzW", "signatures": ["ICLR.cc/2026/Conference/Submission4521/Reviewer_QpHb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4521/Reviewer_QpHb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761405172271, "cdate": 1761405172271, "tmdate": 1762917422265, "mdate": 1762917422265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "bm2UJbaopN", "forum": "Smc4U2aDzW", "replyto": "Smc4U2aDzW", "signatures": ["ICLR.cc/2026/Conference/Submission4521/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4521/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762965178240, "cdate": 1762965178240, "tmdate": 1762965178240, "mdate": 1762965178240, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes OmniAgent, a hierarchical, graph-basedmulti-agent framework for long video generation. It integrates LLM-driven agents with multimodal generation tools to enable modular specialization and scalable inter-agent collaboration. It also proposes hypergraph nodes to enable temporary group discussions among agents lacking sufficient context. Besides, it allows iterative refinement by directed cyclic graphs with limited retries.\nExperiments show that the full OmniAgent system achieves better performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper clearly explains its design choices, and the analogy to real-world film production is intuitive and well presented.\n\n2.\tThe paper proposes a new graph-based framework to improve multi-agent collaboration.\n\n3.\tHuman evaluation is well-structured, incorporating both expert and audience raters to ensure comprehensive assessment."}, "weaknesses": {"value": "1.\tThe proposed framework is mainly a combination of existing ideas, lacking novelty. The contribution is therefore more of a system integration than a new algorithmic or theoretical advance.\n\n2.\tThe experiments only compare against commercial video-generation APIs, not against academic multi-agent frameworks, e.g., MovieAgent: Automated Movie Generation via Multi-Agent CoT Planning; Mora: Enabling Generalist Video Generation via A Multi-Agent Framework; AniMaker: Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation; etc.\n\n3.\tThe paper lacks clarity on how hypergraph “team meetings” are implemented, how consensus is formed, and how reflection loops affect latency and resource cost."}, "questions": {"value": "1.\tHow is the retry budget (Rmax=3) chosen? Did the authors try other values?\n\n2.\tCan OmniAgent outperform other research frameworks (e.g., MovieAgent, Mora, AniMaker)?\n\n3.\tWhat is the computational cost of the proposed pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9kMtgLioHd", "forum": "Smc4U2aDzW", "replyto": "Smc4U2aDzW", "signatures": ["ICLR.cc/2026/Conference/Submission4521/Reviewer_CwEm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4521/Reviewer_CwEm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767059552, "cdate": 1761767059552, "tmdate": 1762917422035, "mdate": 1762917422035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a multi-agent framework, OmniAgent, for long video generation. This framework is motivated by real-world creative workflows, introducing hypergraph-based context collaboration and bounded loop feedback for efficient context retrieval and iterative refinement. Experiments evaluate OmniAgent's performance on minute-level video generation from single-sentence prompts. Both the ablation analysis and the comparison with commercial models demonstrate the effectiveness of the proposed framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The graph-based formulation of multi-agent systems established in this work is clear and meaningful, providing a basic framework for subsequent hypergraph cooperation and cyclic refinement.\n\n2. The proposed hierarchical, graph-based multi-agent framework is effective, facilitating a modular and specialized pipeline for long video production."}, "weaknesses": {"value": "1. The experimental evaluation in this paper is quite inadequate. Only three text prompts are used for evaluation, which are likely cherry-picked and do not fully encompass different contents and styles. Besides, the paper provides no qualitative comparisons, which is unusual for a video generation work (although some videos are provided in the supplementary materials). Analysis beyond the ablation study is also missing, such as the impact of hyperparameters maximum depth and retry budget on performance.\n\n2. The paper provides no illustrations. This hinders the reader's understanding of the methodology, especially given the complexity of a multi-agent framework with hypergraph-based connections and cycles.\n\n3. In particular, for Section 2.2, the overly brief description and lack of illustrations make it difficult to understand the detailed implementation of the proposed hierarchical workflow graph.\n\n4. Technically, controlled cyclic execution is not a particularly novel design. Existing works (such as GenMAC) have also introduced similar iterative refinement mechanisms."}, "questions": {"value": "It is recommended that the authors carefully polish the paper, providing necessary illustrations and supplementing any missing experimental results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tnAHOOnoXf", "forum": "Smc4U2aDzW", "replyto": "Smc4U2aDzW", "signatures": ["ICLR.cc/2026/Conference/Submission4521/Reviewer_2XBY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4521/Reviewer_2XBY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4521/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892814919, "cdate": 1761892814919, "tmdate": 1762917421759, "mdate": 1762917421759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}