{"id": "WVhgFSKniL", "number": 16084, "cdate": 1758259604639, "mdate": 1762951903269, "content": {"title": "ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents", "abstract": "The growing deployment of large language model (LLM) based agents that interact with external environments has created new attack surfaces for adversarial manipulation. One major threat is indirect prompt injection, where attackers embed malicious instructions in external environment output, causing agents to interpret and execute them as if they were legitimate prompts. While previous research has focused primarily on plain-text injection attacks, we find a significant yet underexplored vulnerability: LLMs' dependence on structured chat templates and their susceptibility to contextual manipulation through persuasive multi-turn dialogues. To this end, we introduce ChatInject, an attack that formats malicious payloads to mimic native chat templates, thereby exploiting the model's inherent instruction-following tendencies. Building on this foundation, we develop a persuasion-driven Multi-turn variant that primes the agent across conversational turns to accept and execute otherwise suspicious actions. Through comprehensive experiments across frontier LLMs, we demonstrate three critical findings: (1) ChatInject achieves significantly higher average attack success rates than traditional prompt injection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13% to 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong performance at average 52.33% success rate on InjecAgent, (2) chat-template-based payloads demonstrate strong transferability across models and remain effective even against closed-source LLMs, despite their unknown template structures, and (3) existing prompt-based defenses are largely ineffective against this attack approach, especially against Multi-turn variants. These findings highlight vulnerabilities in current agent systems.", "tldr": "We introduce ChatInject, an attack that exploits LLMs' instruction-following abilities and chat templates for higher success rates than traditional prompt injection, proving highly effective even against prompt-based defenses.", "keywords": ["Large Language Model", "Prompt Injection Attack", "LLM Agent"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c4a9b598deb4c9112201cdb0dd47ba2d9df6c5b0.pdf", "supplementary_material": "/attachment/f7d470ceec3a63f285d7f41a169234ca2ff072b1.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes ChatInject, a family of indirect prompt-injection attacks against LLM agents that exploit chat-template role tags (e.g., system/user/assistant delimiters) and persuasive multi-turn dialogues to bypass instruction hierarchies. Compared to plain-text injections, ChatInject wraps attacker content to mimic native chat templates; a multi-turn variant (\"Multi-turn + ChatInject\") primes the model across turns. Across InjecAgent and AgentDojo benchmarks, the authors report large gains in attack success rate (ASR)  and show that attacks transfer across models, including closed-source ones, and evade common prompt-based defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "+ The paper frames how forged role tags inside tool outputs can be misinterpreted as higher-priority instructions, bypassing system, user, assistant hierarchies—distinct from prior plain-text injections.\n\n+ The four variants—Default InjecPrompt, InjecPrompt+ChatInject, Default Multi-turn, and Multi-turn+ChatInject—are defined with a consistent template.\n\n+ The transferable attack is an interesting and important angle, which is worth further study."}, "weaknesses": {"value": "- While behaviorally persuasive, the paper lacks a deeper mechanistic explanation of why template tokens grant authority (e.g., attention patterns, representation shifts).\n\n- Multi-turn dialogues are LLM-generated (GPT-4.1) and human-reviewed, but not drawn from real attacker corpora. This leaves ecological validity open: would organic, messy content sustain similar gains?"}, "questions": {"value": "1. If the same role tags are injected but tokenized differently (e.g., via Unicode homoglyphs or byte-level encodings), how do models react?\n\n2. Have you validated multi-turn persuasion with human-written or naturally occurring sequences (e.g., scraped forum/HTML excerpts) to assess ASR/Utility differences vs. GPT-generated dialogues?\n\n3. Table 2 shows higher transfer to certain closed models but less Utility degradation in some settings. Can you hypothesize which agent-stack choices (e.g., tool arbitration, post-filters) helped preserve Utility? Any evidence from ablations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X9O0i6X2su", "forum": "WVhgFSKniL", "replyto": "WVhgFSKniL", "signatures": ["ICLR.cc/2026/Conference/Submission16084/Reviewer_FxRy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16084/Reviewer_FxRy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761695668771, "cdate": 1761695668771, "tmdate": 1762926268024, "mdate": 1762926268024, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel type of indirect prompt injection attacks against language models (LMs) deployed in agentic settings. The attack exploits the prompt templates used by the models. More specifically, the proposed prompt injections are wrapped as single- or multi-turn conversations in the LM native prompt template using the special tokens from the template. This results in a \"privilege escalation,\" where text that is supposed to be treated by the LM as a tool output gets interpreted as a higher-level user or system instruction.\n\nThe attack is demonstrated to be highly effective against several near-frontier LMs on two benchmarks. Injections also transfer non-trivially to LMs with different prompt templates. A method for making the attack universal against an unknown template is also demonstrated. Several existing defenses are tried against the new attack. Finally, a separate defense of detecting the malicious template in the payload is considered, and a counter-attack with randomizing injection characters is evaluated."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written, with well-defined contributions and a logical red team-blue team structure. The figures are also helpful and aesthetically pleasing.\n- The attack vector is, to my knowledge, novel.\n- Evaluation is comprehensive (6 open-source LLMs and 3 closed-source LLMs). Reasonable ablations are performed (single- vs multi-turn, tool call hooks, reasoning hooks).\n- I appreciated the fine-grained analysis of ASR against prompt template similarity, which clearly shows that the attack success is driven by guessing the correct template, rather than some unknown mechanism related to OOD inputs.\n- A method for making the attack independent of the template shows that even an attacker with a few attempts before being discovered and blocked by the provider might succeed with this attack.\n- Reasonable blue team counteractions are considered, although I am less certain here. The counter-measure of detecting prompt templates  and counter-counter-measure of introducing noise are very logical."}, "weaknesses": {"value": "I'm not sure if the discussion of the possible defenses in Section 6.1 is comprehensive. For example, the paper discusses the ProtectAI prompt injection detector and shows that, although ASR under this detector drops significantly, high FPR of the detector results in significant utility drops. Since the paper tests closed-source LMs for vulnerabilities, it might also be worth it to try proprietary defense mechanisms against prompt injections, like the Lakera Guard.\n\nGenerally, the paper does not always report error bars, and when it does, the bars seem rather wide, especially given that std is reported instead of 95% CI. For the ASR values in tables 1 & 2 and in Fig. 3, one simple way to report the CI would be to use the Wilson interval formula, which only depends on the ASR and the size N of the dataset. Given that rerunning the queries might be expensive, this could provide a cheap proxy for at least one source of the noise (due to dataset size but not due to LLM generation distribution). I understand that the tables could then become cluttered, so the confidence intervals for at least some of the values could be put in the appendix. Relatedly, one expects the pi detector to result in consistently lower ASR. This is true for all bars but one (multi-turn with Grok 3). Is this outlier due to noise? Having the CI would help here too.\n\nCurrent related work discussion is too small. If the paper is accepted, I would recommend to spend some of the extra page allowance on expanding to include more work on indirect prompt injections, instruction hierarchy, and other relevant topics, eg [1,2,3]. Prompt injection literature is rather rich.\n\n[1] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz.\nNot what you’ve signed up for: Compromising real-world llm-integrated applications with indirect\nprompt injection. In Proceedings of the 16th ACM workshop on artificial intelligence and security,\npp. 79–90, 2023.\n\n[2] Zverev, Egor, et al. \"Can llms separate instructions from data? and what do we even mean by that?.\" arXiv preprint arXiv:2403.06833 (2024).\n\n[3] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to\nadversarial example defenses. Advances in neural information processing systems, 33:1633–1645,\n2020.\n\n\nMinor nit: L374 \"remains strong transferability\" -> \"retains strong transferability\""}, "questions": {"value": "- What are the data sample sizes for the two benchmarks that the paper considers? This could be useful to report in Sec. 3.3 (this subsection would also fit better in the beginning of Section 4 in my opinion, although this is a minor point). \n- What is the reason for missing values in Table 1? My guess is that this is is due to the models not having the capabilities to think / perform tool calls. It would be good to report in the table caption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CV11kqqdoW", "forum": "WVhgFSKniL", "replyto": "WVhgFSKniL", "signatures": ["ICLR.cc/2026/Conference/Submission16084/Reviewer_rfLo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16084/Reviewer_rfLo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936306402, "cdate": 1761936306402, "tmdate": 1762926267542, "mdate": 1762926267542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ChatInject, a new attack that enhances indirect prompt injection (IPI) in LLM-based agents by exploiting chat template structures (e.g., \\<system\\>, \\<user\\>, \\<assistant\\> tags) and multi-turn persuasive dialogues.\nThe first component (ChatInject) formats malicious payloads to mimic the model’s native chat template, making the model misinterpret the injected content as higher-priority instructions.\nThe second component (Multi-turn variant) uses persuasion-driven multi-turn dialogues to gradually legitimize malicious requests.\nExtensive experiments on InjecAgent and AgentDojo benchmarks across 9 LLMs (both open- and closed-source) show substantial improvements in attack success rate (ASR) and cross-model transferability. The paper also evaluates standard defenses, finding them largely ineffective."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clear and well-organized.\n- The proposed attack is effective, increasing the ASRs significantly across different datasets and backbone models.\n- Comprehensive experiments across multiple models, datasets, and defenses provide strong empirical support. The analysis is rich, covering template similarity, cross-model transfer, and robustness to perturbations, making the study thorough from an empirical perspective."}, "weaknesses": {"value": "- The technical novelty is limited: the work mainly enhances IPI by reformatting or rephrasing injected content, and the ideas of leveraging chat templates or persuasive multi-turn dialogues have been conceptually explored in prior works. \n- ChatInject and Multi-turn components appear somewhat loosely connected —one manipulates structure, the other context; they are combined mainly for empirical gain without a unified theoretical justification."}, "questions": {"value": "- Could a simple defense that detects the presence of special chat template tokens in external content effectively identify such attacks without significantly reducing utility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jtT7dx8dxd", "forum": "WVhgFSKniL", "replyto": "WVhgFSKniL", "signatures": ["ICLR.cc/2026/Conference/Submission16084/Reviewer_5Lup"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16084/Reviewer_5Lup"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975855222, "cdate": 1761975855222, "tmdate": 1762926267112, "mdate": 1762926267112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ChatInject, a novel indirect prompt injection attack that exploits chat template structures used in large language model (LLM) agents. Unlike prior prompt injection work focusing on plain-text manipulation, ChatInject forges system/user/assistant role tags to bypass instruction hierarchies and embeds malicious commands in persuasive multi-turn dialogues. Through extensive experiments on nine LLMs (both open- and closed-source) across AgentDojo and InjecAgent benchmarks, the authors show that ChatInject achieves dramatically higher attack success rates and maintains strong transferability even without template knowledge. They also show existing defenses are largely ineffective, motivating the need for stronger mitigations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel threat model with multi-turns persuasive which is indeed a solid area of study and contribution. \n- Comprehensive evaluation and Strong empirical results\n- Clear writing and strong figures: Figures and tables effectively demonstrate the mechanisms and results (e.g., Figs. 1–6)."}, "weaknesses": {"value": "- The main limitations are lack of novelty and lack of baselines. - The contributions in prompt creation seems incremental, limiting its novelty. For example a missing related work: xteaming: https://arxiv.org/pdf/2504.13203 or Foot-In-The-Door, which is referred in the paper but not considered in the exp. \n\n- The treat model needs more clarity: I did not undergard from which point the attacker gets access to change the prompts.  \n\n- Contribution is only in the attack side, nothing in the defense side. \n- I am not if the utility gets lower then how they are not detected easily? Intuitively, this should be easier to detect. \n- Table 2 is hard to follow, i think adding a simple avg per row can make it more readable.  \n\n- Synthetic dialogue generation: Multi-turn persuasion data are LLM-generated (GPT-4.1) and manually filtered; realism and diversity might \n\n- Interpretability gap: In Fig 1 I found the prompts with not readbale tokens but reading the contribution does not give clear idea when they are not readbale and when. Alos this makes it less interpretable and somewhat detectable."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qEDeg07KHv", "forum": "WVhgFSKniL", "replyto": "WVhgFSKniL", "signatures": ["ICLR.cc/2026/Conference/Submission16084/Reviewer_hF66"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16084/Reviewer_hF66"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16084/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762020732355, "cdate": 1762020732355, "tmdate": 1762926266732, "mdate": 1762926266732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}