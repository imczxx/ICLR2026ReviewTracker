{"id": "x7C9A4Y9cF", "number": 13680, "cdate": 1758220830040, "mdate": 1759897420033, "content": {"title": "SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints", "abstract": "The advancement of large language models (LLMs) and code agents has demonstrated significant potential to assist software engineering (SWE) tasks, such as autonomous issue resolution and feature addition. Existing AI for software engineering leaderboards (e.g., SWE-bench) focus solely on solution accuracy, ignoring the crucial factor of effectiveness in a resource-constrained world. This is a universal problem that also exists beyond software engineering tasks: any AI system should be more than correct—it must also be cost-effective. To address this gap, we introduce SWE-Effi, a set of new metrics to re-evaluate AI systems in terms of holistic effectiveness scores. We define effectiveness as the balance between the accuracy of outcome (e.g., issue resolve rate) and the resources consumed (e.g., token and time). In this paper, we specifically focus on the software engineering scenario by re-ranking popular AI systems for issue resolution on a subset of the SWE-bench benchmark using our new multi-dimensional metrics.   \n    We found that AI system’s effectiveness depends not just on the scaffold itself, but on how well it integrates with the base model, which is key to achieving strong performance in a resource-efficient manner. We also identified systematic challenges such as the “token snowball” effect and, more significantly, a pattern of “expensive failures”. In these cases, agents consume excessive resources while stuck on unsolvable tasks—an issue that not only limits practical deployment but also drives up the cost of failed rollouts during RL training. Lastly, we observed a clear trade-off between effectiveness under the token budget and effectiveness under the time budget, which plays a crucial role in managing project budgets and enabling scalable reinforcement learning, where fast responses are essential.", "tldr": "", "keywords": ["Code LLM", "Efficiency", "AI Agents", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b74d73c66245851b89675bce0139c45241cd54f5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper attempts to evaluate SWE Agent systems beyond measuring their resolve rate. They quantify other properties of their behavior which may be relevant to how these systems need to be used in the real world. They specifically focus on cost and run-time, comparing how different agent scaffolds combined with different models perform . The paper highlights the importance of model and scaffold interaction as well as drawbacks and failure modes of current systems such as \"failing expensively\"."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper takes a novel approach to evaluating SWE Agents, assessing their feasibility by evaluating them not only on resolve rate but also resources used (in terms of computation time and money). \n\n2. The paper highlights how current systems \"fail expensively\", opening up directions to make them more usable by having early failure detection."}, "weaknesses": {"value": "1. Limited scope of evaluation: While the metrics proposed in the paper are insightful, they provide limited insights into how useful the agent might be in real world applications. \n\n2. Limited analysis of differences in agent behaviour. The does not go beyond showing that different scaffold result in different performance. It is unclear for example why AutoCodeRover does better than SWE-Agent, or why Qwen3-32B does better than GPT-4o-mini. Without understanding this, the results of the paper's evaluation will be of limited help to model and agent designers.\n\n3. The results presented in the paper are using weaker than SOTA models along with a very low cost limit of $1. The choice of $1 seems arbitrary. Furthermore, the performance of SOTA models with (presumably) higher cost limits (as observed on https://www.swebench.com/) are significantly higher (upto 70%) compared to the evaluations in the paper. While the results in low cost regime are interesting, the behaviour of SOTA models may be significantly different and hard to infer from the current set of experiments.\n\n4. The paper discusses the problem of token snowballing. While it is a reasonable hypothesis that history/memory management strategies for agents may result in better performance, such a setting is not explicitly evaluated, since there maybe a tradeoff between forgetting in the memory and maintaining full history. Furthermore, model providers already implement server-side caching which may mitgate this issue:  https://platform.openai.com/docs/guides/prompt-caching \n\nOverall, my major reason for recommending rejection is the limited scope of evaluation, analysis and concerns related to experimentation. While the ideas presented in the paper are well motivated, the contributions do not seem for acceptance."}, "questions": {"value": "1. I see that. GPT-4o with SWE-Agent makes ~4x as many calls as other combinations. Do the authors know why this is the case? Is there something particularly incompatible about this combination?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "shH8WmsS7G", "forum": "x7C9A4Y9cF", "replyto": "x7C9A4Y9cF", "signatures": ["ICLR.cc/2026/Conference/Submission13680/Reviewer_HZM9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13680/Reviewer_HZM9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773544140, "cdate": 1761773544140, "tmdate": 1762924241810, "mdate": 1762924241810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SWE-Effi is a set of new metrics to evaluate performance of coding agents (and potentially other AI agents as well) against resource usage. The paper evaluates some SLMs on different agent scaffolds on a subset of swebench-verified benchmark. By measuring the effectiveness within resource bounds, they draw insights about performance tradeoffs for different configurations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors make a valid point that coding agents should be measured on their effectiveness under resource constraints. They define different resource constraints targeting CPU and inference time, API calls, etc.\n- The paper evaluates five agent scaffolds and three models on a subset of swebench-verified. They observe that the choice of scaffold and the synergy between the model and the scaffold are among the factors that affect the agent effectiveness."}, "weaknesses": {"value": "- The central premise of the paper is valid and useful, but in the current form, the paper's contributions are not strong enough for ICLR.\n- The experiments are conducted only with SLMs and no LLMs are tested, whereas in practice, developers heavily use LLMs. The evaluation is conducted on just one benchmark, swebench-verified, and further only on a subset of 50 tasks. Both of these are important limitations.\n- I also found the analysis part of the paper lacking. The paper should provide deeper insights into what choices in scaffolds play a role in the seen performance (e.g., what causes or prevents the token snowball effect).\n- Typo: Table 1, EuCB column, the max is 47.1 but 37.0 is highlighted as the max."}, "questions": {"value": "- How do the paper's observations generalize to different datasets and LLMs?\n- What are the key takeaways for LLM and agent designers to optimize their performance within resource constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qkgjdbxysi", "forum": "x7C9A4Y9cF", "replyto": "x7C9A4Y9cF", "signatures": ["ICLR.cc/2026/Conference/Submission13680/Reviewer_D9D4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13680/Reviewer_D9D4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890116683, "cdate": 1761890116683, "tmdate": 1762924241550, "mdate": 1762924241550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SWE-Effi, a multi-dimensional evaluation framework for assessing AI software engineering systems beyond traditional accuracy metrics. The authors evaluate 5 popular scaffolds (AutoCodeRover, SWE-Agent, OpenHands, Agentless, Agentless-Mini) paired with 3 LLMs (GPT-4o-mini, Llama-3.3-70B, Qwen3-32B) on 50 issues from SWE-bench-Verified. The framework introduces four effectiveness metrics based on Area Under the Curve: token budget (EuTB), cost budget (EuCB), CPU time budget (EuCTB), and inference time budget (EuITB). \n\nKey findings include: (1) scaffold performance is highly model-dependent, (2) better reasoning models can be more token-efficient overall, (3) \"token snowball\" effect from context accumulation, and (4) failures consume significantly more resources than successes. While there are good ideas in the paper, there are some concerns such as issues in methodology, clarity of presentation, etc. that I call out below in Weaknesses/questions to authors."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Resource efficiency in AI systems is understudied; addressing the \"resolve rate tunnel vision\" is valuable.\n\n2. Multi-dimensional metrics (tokens, cost, CPU time, inference time) provide richer understanding than single metrics\n\n3. Model-scaffold synergy and expensive failures have practical implications\n\n4. Findings on effectiveness vs. latency tradeoffs are relevant for RL training"}, "weaknesses": {"value": "The paper suffers from major methodological flaws: an inadequate sample size (N=50) with ±13% confidence intervals undermines quantitative claims and leaderboard validity; the normalized inference-time model (Equation 1) is trained on one model yet applied across architectures without validation, leaving 21% unexplained variance. \n\nThe “expensive failures” analysis confounds difficulty with cost and ignores contradictory evidence (e.g., Agentless). The “token snowball effect” is overstated with only anecdotal examples and no systematic analysis. Experimental modifications (parallelism disabled, SWE-Agent $1 cap, API changes) reduce fairness and practical relevance. These issues together tend to compromise the reliability and generalizability of the findings."}, "questions": {"value": "1. N=50 gives ±13% CI, how is this significant? Please provide more analysis.\n\n2. Equation 1 is trained on GPT-4o-mini but applied across architectures/providers. Why assume same linearity? Report $r^2$ for others or use actual wall-clock times. How does this uncertainty affect EuITB rankings?\n\n3. Agentless shows resolved issues cost **more** CPU time, contradicting your claim. Why? Also, why no control for issue difficulty?\n\n4. Why disable parallelism and cap SWE-Agent at $1 while others use defaults? How do these changes affect benchmark relevance?\n\n5. It would be good to articulate the novelty angle beyond prior work (e.g., MemPrompt)? \n\n6. Also, why only one example per scaffold? Can you provide systematic evidence across tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4zu8LflNfm", "forum": "x7C9A4Y9cF", "replyto": "x7C9A4Y9cF", "signatures": ["ICLR.cc/2026/Conference/Submission13680/Reviewer_D1S9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13680/Reviewer_D1S9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901875167, "cdate": 1761901875167, "tmdate": 1762924241264, "mdate": 1762924241264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes several evaluation scenarios and metrics for software engineering agents in a resource-constrained setting, including token-constrained, budget-constrained, cpu-time constrained, and inference-time constrained settings. This provides a more comprehensive picture for AI SWE practitioners to choose solutions for different use cases."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Rigorous measurement setup: using normalized inference time that is computed with regression model trained on real API calls, avoiding instability in actual API call inference time and making comparison fairer.\n\n2. Interesting and sometimes surprising findings, such as reasoning models that use more tokens per step can actually save tokens by being smart.\n\n3. Very comprehensive evaluation across 5 agents and 3 models, providing useful information for the community."}, "weaknesses": {"value": "1. Lack of technical novelty. All metrics seem to be intuitive, which is good. But similar yet smaller analyses have been done in other coding agent papers."}, "questions": {"value": "1. How do the effectivness metrics differ across different tasks? Are some tasks costing more tokens than others? What are some signs indicating those hard-to-solve issues?\n\n2. Tiny issue: Reference format. Use \\citep for certain references (for example those on Line 222)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CGCCahfoxK", "forum": "x7C9A4Y9cF", "replyto": "x7C9A4Y9cF", "signatures": ["ICLR.cc/2026/Conference/Submission13680/Reviewer_Z1VA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13680/Reviewer_Z1VA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13680/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013998149, "cdate": 1762013998149, "tmdate": 1762924240975, "mdate": 1762924240975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}