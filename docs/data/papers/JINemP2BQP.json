{"id": "JINemP2BQP", "number": 7214, "cdate": 1758011875187, "mdate": 1759897866045, "content": {"title": "AsyncBEV: Cross-modal flow alignment in Asynchronous 3D Object Detection", "abstract": "In autonomous driving, multi-modal perception tasks like 3D object detection typically rely on well-synchronized sensors, both at training and inference. However, despite the use of hardware- or software-based synchronization algorithms, perfect synchrony is rarely guaranteed: Sensors may operate at different frequencies, and real-world factors such as network latency, hardware failures, or processing bottlenecks often introduce time offsets between sensors.  Such asynchrony degrades perception performance, especially for dynamic objects. To address this challenge, we propose AsyncBEV, a trainable lightweight and generic module to improve the robustness of 3D Birds' Eye View (BEV) object detection models against sensor asynchrony. Inspired by scene flow estimation,  AsyncBEV first estimates the 2D flow from the BEV features of two different sensor modalities, taking into account the known time offset between these sensor measurements.  The predicted feature flow is then used to warp and spatially align the feature maps, which we show can easily be integrated into different current BEV detector architectures (e.g., BEV grid-based and token-based).  Extensive experiments demonstrate AsyncBEV improves robustness against both small and large asynchrony between LiDAR or camera sensors in both the token-based CMT and grid-based UniBEV, especially for dynamic objects. We significantly outperform the ego motion compensated CMT and UniBEV baselines, notably by $16.6$ % and $11.9$ % NDS on dynamic objects in the worst-case scenario of a $0.5$ s time offset. Code will be released upon acceptance.", "tldr": "", "keywords": ["multi-modal 3D object detection", "autonomous driving", "a synchronous fusion."], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/656aa32131f69a9e109996cf4b0d04b51620a0a0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes AsyncBEV, a lightweight module designed to enhance the robustness of multi-modal 3D object detectors against sensor asynchrony by introducing a novel Δ-BEVFlow estimation task. The method predicts motion in BEV feature space and warps asynchronous features to a reference timestamp, demonstrating compatibility with both token-based and grid-based detectors. The writing is clear and well-structured, and the experimental evaluation on nuScenes shows significant improvements in handling asynchronous inputs, especially for dynamic objects."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper is clearly written and easy to follow.\n\n2.It explores an important and underexplored problem in multi-modal perception.\n\n3.The method is simple yet effective, with a lightweight and generic design.\n\n4.Extensive experiments validate the effectiveness of the proposed approach under various asynchronous settings."}, "weaknesses": {"value": "1.The validation is limited to the nuScenes dataset, lacking cross-dataset generalization.\n\n2.The method does not address scenarios where multiple sensors are asynchronous at the same time, which may lead to increased computational complexity.\n\n3.The Motion Compensation-based baseline is somewhat weak. The comparison to asynchronous fusion methods in related work, like StreamingFlow and TimeAlign, is missing."}, "questions": {"value": "1.Have the authors considered evaluating AsyncBEV on other autonomous driving datasets, such as Waymo, to further demonstrate its generalization capability?\n\n2.How would the method scale and perform when more than two sensors are asynchronous simultaneously, and would the current flow estimation strategy still be effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KjxAzqLWim", "forum": "JINemP2BQP", "replyto": "JINemP2BQP", "signatures": ["ICLR.cc/2026/Conference/Submission7214/Reviewer_gWRm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7214/Reviewer_gWRm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761577220787, "cdate": 1761577220787, "tmdate": 1762919362730, "mdate": 1762919362730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a novel framework for improving the robustness of 3D object detection models in the presence of asynchronous sensors. The proposed framework consists of AsyncBEV, a lightweight and generic module that can be integrated into existing BEV object detection models for improved robustness against sensor asynchrony. AsyncBEV estimates the 2D flow in the BEV feature space and then warps the asynchronous sensor data to align with a reference frame. Extensive experiments on the nuScenes dataset with both grid-based and token-based BEV detectors show the effectiveness of the proposed approach in varying levels of asynchrony between LiDAR and camera sensors, especially for dynamic objects."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addressing robustness against sensor asynchrony is important from a practical viewpoint since autonomous vehicles often come equipped with multiple sensors.\n- The paper is well-written and easy to follow. The description is detailed and the figures (Fig.2,3,5) are informative.\n- AsyncBEV module is a generic and lightweight module that can be combined with both grid-based and token-based BEV detectors.\n- The proposed module predicts the delta 2D scene flow in BEV space, conditioned on delta timesteps across multimodal BEV features.\n- Experiments on nuScenes (Tab.1) show the benefits of AsyncBEV over egomotion compensation (EMC) on both token-based (CMT) and grid-based (UniBEV) approaches.\n- Ablations in Fig.4, Tab.2 provide more insights into the capabilities of different components."}, "weaknesses": {"value": "- In Sec.4.2, for the finetuning UniBEV variant, is the delta timestep offset (between reference and asynchronous sensor) also used as input? It'd be useful to have a finetuning baseline that also incorporates delta timestep offset. For example, LiDAR BEV can be augmented with the delta timestep (on a per-point basis) as an additional feature channel (a similar strategy was also used in the Fan et al. 2025 referenced paper). This would help understand if the delta flow formulation is indeed effective compared to simpler alternatives like finetuning with the delta timestep as an additional feature channel.\n- The finetuning variant in Tab.2 should also be a baseline in Tab.1 (applied to both CMT and UniBEV) since it leads to extensive gains (as noted in Tab.2). This is relevant since CMT and UniBEV are not trained with any asynchronous data.\n- Since EMC is quite widely used in the autonomous driving literature (as mentioned in the paper), it'd be useful to report the delta gains with respect to EMC variants in Tab.1. The gains of the proposed approach are still clear, this would better contextualize the benefits of AsyncBEV over the standard EMC approach."}, "questions": {"value": "The paper is well written, and the claims are validated in the experiments. My main concern is regarding simpler alternatives as baselines to better contextualize the benefits of the delta flow formulation (more details in the weaknesses above):\n- A finetuning variant for both CMT & UniBEV should be added to Tab.1 since these methods are not trained on asynchronous data.\n- A finetuning variant where the delta timestep offset is used as additional input should also be considered."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xI77p2xmkK", "forum": "JINemP2BQP", "replyto": "JINemP2BQP", "signatures": ["ICLR.cc/2026/Conference/Submission7214/Reviewer_zA9G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7214/Reviewer_zA9G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931020365, "cdate": 1761931020365, "tmdate": 1762919362294, "mdate": 1762919362294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical challenge in autonomous driving perception: sensor asynchrony (caused by mismatched sensor frequencies, network latency, or hardware bottlenecks). Most 3D object detectors rely on perfectly synchronized multi-modal data (LiDAR/camera), but asynchrony degrades performance—especially for dynamic objects, which are key to safety. To solve this, the authors propose AsyncBEV, a lightweight, generic module compatible with both token-based (e.g., CMT) and grid-based (e.g., UniBEV) detectors.\nAsyncBEV introduces Δ-BEVFlow estimation, a novel task that predicts 2D flow in Bird’s Eye View (BEV) space between multi-modal features using known time offsets. Unlike Ego Motion Compensation (EMC, which only aligns static objects) or traditional scene flow methods (requiring point clouds and fixed offsets), Δ-BEVFlow explicitly models dynamic object motion. It offers two formulations: motion-based (direct flow regression) and velocity-based (predict velocity first, then scale by time)—the latter is adopted for better regularization.\nExperiments on the nuScenes dataset show AsyncBEV significantly enhances robustness: in the worst-case 0.5s time offset, it outperforms EMC baselines by 16.6% (CMT) and 11.9% (UniBEV) in NDS for dynamic objects, with minimal computational overhead (marginal FPS loss)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**High Practical Relevance**\n\nSensor asynchrony is unavoidable in real-world autonomous driving, yet it is often overlooked in detector design. By targeting this gap, AsyncBEV directly improves the safety and reliability of perception systems—particularly for dynamic objects (e.g., pedestrians, moving vehicles), which are the primary cause of accidents. This makes the work valuable for both academic research and industrial deployment.\n\n**Effective Δ-BEVFlow Design** \n\nΔ-BEVFlow addresses key limitations of prior methods: it operates on multi-modal BEV features (not raw point clouds) and supports variable time offsets, enabling flexible cross-modal alignment. The velocity-based formulation further strengthens the design: by decoupling velocity from time, it ensures flow approaches zero for near-synchronous sensors (avoiding unnecessary distortions) and simplifies learning, as validated by ablation studies (Table 2).\n\n**Generality and Lightweight Integration**\n\nAsyncBEV is architecture-agnostic: it adapts to token-based detectors (by adjusting token coordinates) and grid-based detectors (by generating grid look-up tables) with minimal modifications. It also introduces negligible computational overhead—experiments show only a 0.3–0.4 FPS drop (Table 1)—making it suitable for real-time autonomous driving pipelines.\n\n**Strong Interpretability**\n\nExplicit flow prediction allows intuitive visualization (Figures 5, 6), where predicted Δ-BEVFlow closely aligns with ground truth and directly corrects bounding box misalignments. Quantitative results (e.g., Figure 4’s flat performance curves for AsyncBEV) further confirm its robustness across varying time offsets, enhancing trust in the module’s mechanism."}, "weaknesses": {"value": "**Limited Novelty Compared to Prior Asynchrony-Robust Work**\n\nThe core idea of using BEV flow for asynchrony compensation is not entirely new. For example, CoBEVFlow (Wei et al., NeurIPS 2023) already uses BEV flow to handle asynchronous collaborative perception, though it relies on object proposals and is more computationally heavy. Additionally, recent work like UniV2X (Yu et al., AAAI 2025) explores end-to-end autonomous driving with V2X cooperation, which also involves addressing multi-agent asynchrony. The paper acknowledges these works but does not sufficiently emphasize how Δ-BEVFlow advances beyond them—e.g., why feature-based flow (without proposals) offers better generalization, or how variable time offset handling outperforms alternatives. This weakens the claim of novelty.\n\n**Simplified Asynchrony Assumptions**\n\nThe paper assumes only two sensors (one synchronous reference, one asynchronous), but real autonomous driving systems use 5–10 sensors (e.g., 6 cameras, 1 LiDAR, 5 radars in nuScenes). AsyncBEV cannot handle scenarios where 3+ sensors have overlapping offsets (e.g., LiDAR delayed by 0.2s, front camera by 0.1s). The authors mention extending to multi-sensor asynchrony as future work, but the current design lacks scalability to full-scale vehicle perception pipelines.\n\n**Synchronous Performance Trade-Off**\n\nDefault AsyncBEV causes small but consistent performance drops in the synchronized case (0s offset): 0.4% NDS for CMT and 1.0% NDS for UniBEV (Table 1). The \"frozen detector\" variant (AsyncBEV-FD) avoids this drop but underperforms in asynchronous scenarios. The paper does not explore alternative training strategies (e.g., adaptive loss weighting) to resolve this trade-off—critical for deployment, as synchronized sensors are the most common real-world scenario."}, "questions": {"value": "**Limited Novelty Compared to Prior Asynchrony-Robust Work**\n\nThe core idea of using BEV flow for asynchrony compensation is not entirely new. For example, CoBEVFlow (Wei et al., NeurIPS 2023) already uses BEV flow to handle asynchronous collaborative perception, though it relies on object proposals and is more computationally heavy. Additionally, recent work like UniV2X (Yu et al., AAAI 2025) explores end-to-end autonomous driving with V2X cooperation, which also involves addressing multi-agent asynchrony. The paper acknowledges these works but does not sufficiently emphasize how Δ-BEVFlow advances beyond them—e.g., why feature-based flow (without proposals) offers better generalization, or how variable time offset handling outperforms alternatives. This weakens the claim of novelty.\n\n**Simplified Asynchrony Assumptions**\n\nThe paper assumes only two sensors (one synchronous reference, one asynchronous), but real autonomous driving systems use 5–10 sensors (e.g., 6 cameras, 1 LiDAR, 5 radars in nuScenes). AsyncBEV cannot handle scenarios where 3+ sensors have overlapping offsets (e.g., LiDAR delayed by 0.2s, front camera by 0.1s). The authors mention extending to multi-sensor asynchrony as future work, but the current design lacks scalability to full-scale vehicle perception pipelines.\n\n**Synchronous Performance Trade-Off**\n\nDefault AsyncBEV causes small but consistent performance drops in the synchronized case (0s offset): 0.4% NDS for CMT and 1.0% NDS for UniBEV (Table 1). The \"frozen detector\" variant (AsyncBEV-FD) avoids this drop but underperforms in asynchronous scenarios. The paper does not explore alternative training strategies (e.g., adaptive loss weighting) to resolve this trade-off—critical for deployment, as synchronized sensors are the most common real-world scenario."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0b1Qc5k519", "forum": "JINemP2BQP", "replyto": "JINemP2BQP", "signatures": ["ICLR.cc/2026/Conference/Submission7214/Reviewer_LiTB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7214/Reviewer_LiTB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980237227, "cdate": 1761980237227, "tmdate": 1762919361940, "mdate": 1762919361940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AsyncBEV, a module designed to improve the robustness of LiDAR–camera 3D object detection under asynchrony. The proposed module predicts a Δ-BEVFlow, a BEV flow to spatially align asynchronous lidar features prior to fusion. The approach generalizes across both grid and token-based BEV detectors and improves performance over ego-motion compensation on nuScenes, particularly for dynamic objects and large temporal offsets (0.1–0.5 s). While technically sound and empirically validated, the work would benefit from stronger comparison against alternative approaches to help motivate the usefulness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Conceptually intuitive and interpretable formulation for feature alignment under temporal misalignment and dynamic motion.\n- Generalizable design compatible with both grid and token BEV frameworks.\n- Consistent improvements across offset magnitudes with negligible runtime overhead."}, "weaknesses": {"value": "- Lacks discussion of real-world latency handling in AV stacks, where stale data (>100 ms) are typically discarded. Comparison to such baselines (e.g., camera-only inference or temporal propagation) would clarify practical value.\n- Limited benchmarking against contemporary temporal alignment methods like StreamingFlow.\n- Performance could be impacted by stochastic latency profiles, but no analysis is provided on sensitivity to time offset estimation error."}, "questions": {"value": "- How does AsyncBEV compare to fallback strategies such as camera-only inference or simple temporal propagation when LiDAR is delayed or missing?\n- What are the performance and latency trade-offs relative to recent streaming or time-aware fusion methods such as StreamingFlow?\n- Could Δ-BEVFlow be trained to handle uncertain or estimated time offsets instead of relying on ground-truth Δt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gFV4IJPlcp", "forum": "JINemP2BQP", "replyto": "JINemP2BQP", "signatures": ["ICLR.cc/2026/Conference/Submission7214/Reviewer_VrJt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7214/Reviewer_VrJt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988691615, "cdate": 1761988691615, "tmdate": 1762919361614, "mdate": 1762919361614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}