{"id": "8hMaqBagPd", "number": 15197, "cdate": 1758248899923, "mdate": 1759897322153, "content": {"title": "Learning to Play Multi-Follower Bayesian Stackelberg Games", "abstract": "In a multi-follower Bayesian Stackelberg game, a leader plays a mixed strategy over $L$ actions to which $n\\ge 1$ followers, each having one of $K$ possible private types, best respond. The leader's optimal strategy depends on the distribution of the followers' private types.\nWe study an online learning problem for Bayesian Stackelberg game, where a leader interacts for $T$ rounds with $n$ followers with types sampled from an unknown distribution every round. The leader's goal is to minimize regret, defined as the difference between the cumulative utility of the optimal strategy and that of the actually chosen strategies. We design learning algorithms for the leader under different settings. Under type feedback, where the leader observes the followers' types after each round, we design algorithms that achieve $\\mathcal O\\big(\\sqrt{\\min\\{L\\log(nKA T), ~ nK \\} \\cdot T} \\big)$ regret for independent type distributions and $\\mathcal O\\big(\\sqrt{\\min\\{L\\log(nKA T), ~ K^n \\} \\cdot T} \\big)$ regret for general type distributions. Interestingly, these bounds do not grow with $n$ at a polynomial rate. Under action feedback, where the leader only observes the followers' actions, we design algorithms with $\\mathcal O( \\min\\{\\sqrt{ n^L K^L A^{2L} L T \\log T}, ~ K^n\\sqrt{ T } \\log T \\} )$ regret. We also provide a lower bound of $\\Omega(\\sqrt{\\min\\{L, ~ nK\\}T})$, almost matching the type-feedback upper bounds.", "tldr": "We develop online learning algorithms for multi-follower Bayesian Stackelberg games with unknown type distributions under multiple feedback models.", "keywords": ["Online Learning; Stackelberg Games; Algorithmic Game Theory"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5222db2b33de1611b49478e2950248f3280fc92.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies online learning for multi-follower Bayesian Stackelberg games where a leader commits to a mixed strategy and $n$ followers (each with one of $K$ private types) best respond. The authors analyze two feedback models: type feedback and action feedback. The paper proposes a geometric decomposition of the leader’s simplex into best-response regions and algorithms with sublinear regret. For type feedback they give $\\tilde{O}(\\sqrt{\\min\\{L,nK\\}}\\,T)$ for independent types, $\\tilde{O}(\\sqrt{\\min\\{L,Kn\\}}\\,T)$ for general types, and a matching lower bound $\\Omega(\\sqrt{\\min\\{L,nK\\}}\\,T)$. For action feedback they propose a reduction to stochastic linear bandits with $O(Kn\\sqrt{T}\\log T)$ regret and a UCB  approach with $O(\\sqrt{n^{L}K^{L}A^{2L}}\\,L\\,T\\log T)$ regret."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a clean geometric decomposition of the leader’s strategy space into follower best-response regions and leverages this structure to derive regret guarantees for multi-follower Bayesian Stackelberg games. The resulting bounds avoid polynomial growth in the number of followers $n$, and the authors provide a lower bound that nearly matches the type-feedback upper bounds."}, "weaknesses": {"value": "1. In the general single-leader-multi-follower setting, a follower’s utility typically depends on both the leader’s action and other followers’ actions (i.e., there are cross-follower externalities). The paper assumes each follower’s payoff depends only on their own action and type plus the leader’s action—which substantially simplifies the model and makes the extension from one to many followers more direct. While this assumption enables clean analysis, it limits the applicability of the results to settings without strategic interdependence among followers.\n2. The analysis assumes followers play the exact best response in every round. This is a strong assumption for a learning setting: in many practical environments followers also learn, face estimation error, or act under noisy rewards, leading to approximate or stochastic best responses. The results would be more interesting with guarantees to $\\varepsilon$-best responses or noise in followers’ payoff observations.\n3. The reduction to stochastic linear bandits is not novel. Similar reductions have appeared in prior work [1].\n\n\n[1] Nearly-optimal bandit learning in stackelberg games with side information. Balcan, Maria-Florina, et al."}, "questions": {"value": "Could you elaborate more the statement: “In comparison, Conitzer & Sandholm (2006) prove that the optimal strategy is NP-hard to compute in BSGs with an asymptotically increasing $L$. We show this is polynomial-time solvable for a constant $L$.” (Line 274)\nCould you cite the exact theorem/lemma in Conitzer & Sandholm, specify the precise problem and briefly contrast your assumptions with theirs to explain why constant $L$ yields polynomial time here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pX1aZJKUw0", "forum": "8hMaqBagPd", "replyto": "8hMaqBagPd", "signatures": ["ICLR.cc/2026/Conference/Submission15197/Reviewer_av7X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15197/Reviewer_av7X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761503908419, "cdate": 1761503908419, "tmdate": 1762925500202, "mdate": 1762925500202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies multi-follower online Bayesian Stackelberg games. The authors study 2 types of feedback: type feedback, where at each turn the leader observes the tuple of realized types, and action feedback, in which the leader observes the individual actions. Clearly knowing the types implies knowing the actions.\nTypes are either drawn from a joint or product distribution.\nUnder type feedback the authors gives an algorithm which attains regret of $\\sqrt{T\\min(L,K^n)}$  under joint distribution, or, $\\sqrt{T\\min(L,Kn)}$ under product distribution, where $L$ is the number of actions of the followers, $K$ the number of possible types and $n$ the number of players. The algorithm revolves around \nUnder type feedback (under joint distribution) the algorithm is simply follow the leader. Interestingly, the analysis shows concentration around the empirical utility rather than under the correct type distribution. This aligns with the following section on action feedback, where working in the utility space is the correct trick to get around exponential dependencies.\nUnder type feedback and independent distribution, the algorithm simply estimated each distribution separately.\nUnder action feedback, the authors rely on a known reduction for a related problem of online Bayesian persuasion. The technical challenge here is to show that there exists a single LP that solves the problem."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This is a good paper that has non trivial contributions, even if maybe a bit lackluster from a technical side."}, "weaknesses": {"value": "I see two main weaknesses:\n1. Even if I find the overall presentation satisfactory, its quality degrades in some specific points. It is not clear where do you use the best response partitions. For example, the discussion under Lemma 4.1 seems to be very important, but right now it feels somewhat obscure and does not do a good job at explaining the crucial points. Another point in which the presentation is a bit confused is around line 409. It is not even clear if this part relies at all on the reduction of Bernasconi et al. or not. I think not, but better clarity would be nice. Overall, I think the paper suffers from having too many results compressed. I would suggest to the authors that maybe they defer some of the results to the appendix and devote more space to points that are now a bit compressed.\n2. Why do you not consider adversarial types? I think your FTL approach would still be a good candidate for adversarial types (obviously by using FTPL instead). The reduction of Bernasconi for sure also works for adversarial types. So it is a bit strange that you do not consider this problem, which is the most studied in Online learning in Econ settings.\n\n* Typo in statement of Lemma C.1"}, "questions": {"value": "Is the improvement from K^{3n/2} to K^n only due to the fact that you consider stochastic types rather than adversarial ones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DKwfZ1UPs6", "forum": "8hMaqBagPd", "replyto": "8hMaqBagPd", "signatures": ["ICLR.cc/2026/Conference/Submission15197/Reviewer_F5NQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15197/Reviewer_F5NQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994620008, "cdate": 1761994620008, "tmdate": 1762925499339, "mdate": 1762925499339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the online learning version of the multi-follower Bayesian Stackelberg game. The authors consider the setting where the leader knows each follower's utility function but not their private types. The paper designs learning algorithms for the leader under both type-feedback and action-feedback settings, and provides detailed regret analysis."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and easy to follow. The authors conduct an in-depth analysis of the geometric properties of the game, which provides insights for the learning algorithms. I find the proposed algorithm interesting as it significantly reduce regret. The theoretic results look solid and strong to me (did not check all the proof details though). The multi-follower setting is much more general than the standard version, and the propose method can potentially improve the applicability of the Bayesian Stackelberg game."}, "weaknesses": {"value": "There are some minor issues with the paper. The definition of $W$  in Subsection 3.2 seems inconsistent. If $W$ is a matrix, then the $i$-th element of $W$ should be $w_i: \\Theta^K \\mapsto A^K$, not $w_i: \\Theta \\mapsto A$. Am I missing something here?"}, "questions": {"value": "Please see my comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lUWVqfn1mE", "forum": "8hMaqBagPd", "replyto": "8hMaqBagPd", "signatures": ["ICLR.cc/2026/Conference/Submission15197/Reviewer_Sbtn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15197/Reviewer_Sbtn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004930061, "cdate": 1762004930061, "tmdate": 1762925498644, "mdate": 1762925498644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of learning an optimal commitment against multiple followers, each of whose type is drawn from a distribution. In doing so, this paper generalizes the usual Stackelberg paradigm of optimal commitment against a single follower and even the Bayesian Stackelberg setting where the single follower’s type is drawn from a distribution.\n\nThe latter problem is known to be NP-hard in general, implying hardness for this more general problem. A natural algorithmic approach for this problem is to partition the leader’s action space into regions that correspond to best-response regions for the different follower profiles. However, the bottleneck to such an approach is that the number of partitions grows exponentially in the number of types for the follower — this problem doubly applies with multiple followers, since the number of best-response (BR) profiles has exponential dependence on the number of followers as well.\n\nThis paper surmounts this technical difficulty by using results from computational geometry to bound the number of non-empty regions with exponential dependence only on the number of “pure” actions of the learner. The paper also provides an explicit enumeration of these non-empty regions with the same asymptotic parameters, thus providing an offline algorithm for the problem. Then, they strengthen this to an online learning algorithm, which is able to learn the optimal policy at a rate faster than just learning the distributions based on intricate technical analysis. Their results cover both the settings where the type of the followers is seen at the end of each round versus just their action, and provide lower bounds with the same exponential dependence."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper provides a novel technique that breaks through the barrier of exponential best response profiles with multiple followers/ follower types, by focusing on the dimension of the leader's action space. This allows for a fine-grained view of the complexity of finding an optimal commitment and opens the door for the upper bound results in the paper which as noted, from the first positive results for multiple followers. The paper covers a variety of settings and is overall a very strong submission with novel technical contributions."}, "weaknesses": {"value": "NA"}, "questions": {"value": "Do the results extend to settings where the action sets are arbitrary convex sets of a specified dimension, especially for the leader?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y5JhDapYcU", "forum": "8hMaqBagPd", "replyto": "8hMaqBagPd", "signatures": ["ICLR.cc/2026/Conference/Submission15197/Reviewer_jBAa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15197/Reviewer_jBAa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140498676, "cdate": 1762140498676, "tmdate": 1762925498196, "mdate": 1762925498196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}