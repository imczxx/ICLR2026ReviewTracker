{"id": "KboFptAM8S", "number": 363, "cdate": 1756736621647, "mdate": 1763718813043, "content": {"title": "VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated capabilities in multimodal understanding, yet their vulnerability to adversarial attacks raises significant concerns. To achieve practical attacking, this paper aims at efficient and transferable untargeted attacks under limited perturbation sizes. Considering this objective, white‑box attacks require full‑model gradients and task‑specific labels, making costs scale with tasks, while black‑box attacks rely on proxy models, typically requiring large perturbation sizes and elaborate transfer strategies. Given the centrality and widespread reuse of the vision encoder in LVLMs, we adopt a gray‑box setting that targets the vision encoder alone for efficient but effective attacking. We theoretically establish the feasibility of vision‑encoder‑only attacks, laying the foundation for our gray‑box setting. Based on this analysis, we propose perturbing patch tokens rather than the class token, informed by both theoretical and empirical insights. We generate adversarial examples by minimizing the cosine similarity between clean and perturbed visual features, without accessing the subsequent models, tasks, or labels. This significantly reduces computational overhead while eliminating the task and label dependence. VEAttack has achieved a performance degradation of 94.5% on image caption task and 75.7% on visual question answering task. We also reveal some key observations to provide insights into LVLM attack/defense: 1) hidden layer variations of LLM, 2) token attention differential, 3) Möbius band in transfer attack, 4) low sensitivity to attack steps.", "tldr": "We propose a vision‑encoder‑only attack on LVLMs to achieve efficient and transferable untargeted attacks under limited perturbation sizes.", "keywords": ["adversarial attack", "vision-encoder-only", "large vision language models", "downstream-agnostic"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/977a0f358cc379ec656474de56b2783fb6f5db80.pdf", "supplementary_material": "/attachment/9a32c928f4a271ec1aa7ee79fedc2a9c256fa44a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes VEAttack, a simple yet effective gray-box attack on LVLMs. VEAttack generates adversarial examples by perturbing solely the image token features of the vision encoder. The results conduct evaluations of multiple LVLMs across Visual Question Answering (VQA) and image captioning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is clearly structured and articulately written, ensuring ease of understanding.\n\n2. The study demonstrates detailed analysis and keen observation.\n\n3. Experiments were conducted across a varied set of datasets and models."}, "weaknesses": {"value": "**Clarity:**\n\n1. The Introduction mainly introduces the figures in other chapters, but there is no place in the paper to introduce Figure 1.\n\n**Experiment:**\n\n2. The authors clearly demonstrate their motivation by comparing with white-box and black-box attacks in Figure 2, but I have some confusion about their performance: Since the white-box attack performs a full gradient backpropagation, why is the black-box attack so much more time-consuming than others? Furthermore, the black-box attack performs poorly in the untargeted attack setting. Could this be related to the black-box attacks that target a specific sample? Can the authors verify this difference with other black-box attacks, such as M-Attack [1] and higher perturbation in black-box?\n\n3. How is the “performance after attack” in Figure 2 (b) evaluated? Why is its performance trend opposite to that in Table 1? If it’s a performance decrease, please describe it clearly and align it with Table 1.\n\n4. The authors seem to only have some subjective comparisons in Figure 8 to compare the imperceptibility with the black-box. Using data evaluation such as L2 or CLIP score would be more convincing.\n\n[1] Li Z, Zhao X, Wu D D, et al. A frustratingly simple yet highly effective attack baseline: Over 90% success rate against the strong black-box models of gpt-4.5/4o/o1"}, "questions": {"value": "All concerns and questions are listed in the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XxNJp81WMK", "forum": "KboFptAM8S", "replyto": "KboFptAM8S", "signatures": ["ICLR.cc/2026/Conference/Submission363/Reviewer_eK4n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission363/Reviewer_eK4n"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761311009899, "cdate": 1761311009899, "tmdate": 1762915503471, "mdate": 1762915503471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to disrupt the downstream performance of LVLMs. Through a theoretical analysis of feasibility, VEAttack generates adversarial examples that significantly degrade multiple tasks while achieving notable computational efficiency over other attack approaches."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The motivation is clear, and the introduction effectively conveys the idea. VEAttack provides a solid and effective paradigm for gray-box adversarial attacks on LVLMs, offering a detailed analysis and feasibility assessment for this approach. The effectiveness and efficiency are well demonstrated across several datasets and models."}, "weaknesses": {"value": "(1) Table 9 shows the attack performance of the Image-Text Retrieval task, which complements the tasks. However, another focus of these works [1, 2] is on transfer attacks between vision encoders, like ALBEF and CLIP-CNN, and it is recommended to include more demonstrations of this performance.\n\n(2) Eq. (5) gives two baselines, but seems to lack the comparison of the second L2 Attack.\n\n(3) Based on observation 4, you perform a time comparison. However, I notice that the used step is 50 instead of 100 in the setting. I suggest adding a time and effect comparison under the condition of complete alignment.\n\n(4) There is a typo: “SRA” should be “SGA” in Table 9 following [1].\n\n[1] Lu D, Wang Z, Wang T, et al. Set-level guidance attack: Boosting adversarial transferability of vision-language pre-training models[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 102-111.\n\n[2] Gao S, Jia X, Ren X, et al. Boosting transferability in vision-language attacks via diversification along the intersection region of adversarial trajectory[C]//European Conference on Computer Vision. 2024: 442-460."}, "questions": {"value": "(1) The results and trends in Figure 2 (b) are inconsistent with those in the Experiments. How are they obtained?\n\n(2) Figure 7 shows that VEAttack is effective even when the attack step is 30 or even 10. Is this different from other attacks, or do other attacks have the same characteristics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z2z4olZ1YX", "forum": "KboFptAM8S", "replyto": "KboFptAM8S", "signatures": ["ICLR.cc/2026/Conference/Submission363/Reviewer_xTBx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission363/Reviewer_xTBx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482577157, "cdate": 1761482577157, "tmdate": 1762915503317, "mdate": 1762915503317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "VEAttack presents a gray-box adversarial attack targeting only the vision encoder of large vision-language models (LVLMs).\nIt minimizes cosine similarity between clean and perturbed patch token features, bypassing the need for task labels or prompt access.\nThis results in downstream-agnostic perturbations that break captioning, VQA, and hallucination benchmarks simultaneously—while remaining efficient and imperceptible.\nThe paper provides theoretical grounding showing that patch-level perturbations propagate more strongly to the LLM via alignment layers than class-token perturbations.\nExtensive experiments show massive performance drops (up to ~95% in captioning, ~75% in VQA) and strong transferability across models and tasks, far exceeding existing white-box and gray-box baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Realistic threat model: Attacks only the shared vision encoder—a genuinely deployable setting for LVLM vulnerabilities.\n2. Theoretically principled: Clear justification that perturbing patch tokens yields stronger downstream disruption than class tokens.\n3. Highly transferable: Single perturbation damages multiple tasks (captioning, VQA, hallucination).\n4. Efficiency: 8–13× faster than prior multi-step attacks, with small ε (2–8/255).\n5. Insightful analysis: Reveals internal LLM distortions, attention asymmetries (image vs. instruction), and the “Möbius band” paradox—robust encoders yield more transferable attacks."}, "weaknesses": {"value": "1. Defense gap: No practical mitigation or robust-training strategy is explored beyond noting cost trade-offs.\n2. Limited architecture diversity: Focuses mainly on CLIP-based encoders; broader evaluation would strengthen claims.\n3. Transfer paradox underexplained: The Möbius effect is intriguing but remains a descriptive observation, not a mechanistic analysis.\n4. Ethical discussion minimal: Needs clearer guidance on responsible release and safety implications.\n5. The paper closely overlaps with the recently released work arXiv:2412.08108, which also investigates adversarial attacks on vision encoders of LVLMs and demonstrates downstream task-agnostic degradation across captioning and VQA. While the two studies share a very similar motivation and methodological framing, the current submission does not cite or discuss this concurrent work."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CFxrDH8HNB", "forum": "KboFptAM8S", "replyto": "KboFptAM8S", "signatures": ["ICLR.cc/2026/Conference/Submission363/Reviewer_XkoF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission363/Reviewer_XkoF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910500609, "cdate": 1761910500609, "tmdate": 1762915503156, "mdate": 1762915503156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on addressing the vulnerability of Large Vision-Language Models (LVLMs) to adversarial attacks and proposes a novel gray-box attack method called VEAttack. Unlike existing white-box attacks that require full-model gradients and task-specific labels (resulting in high costs scaling with tasks) and black-box attacks that depend on proxy models (needing large perturbation sizes), VEAttack targets only the vision encoder of LVLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Innovative Attack Setting: focus on the vision encoder in a gray-box setting."}, "weaknesses": {"value": "1. Lack of citations and comparisons with papers highly similar to this paper.\n\n- An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models, ICLR 2024.\n- QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models, NAACL 2025.\n- InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models, ARXIV.\n\n2. Without Any New Insights: Attacking the vision encoder to achieve attacks on the entire LVLMs is not novel; it is quite intuitive.\n\n3. Sensitivity to Vision Encoder Type: VEAttack’s effectiveness heavily relies on the vision encoder of LVLMs. For LVLMs using non-CLIP vision encoders, the paper’s experiments show relatively limited attack effects. Can this be called a gray-box attack?\n\n4. Limited Transferability from Large-Scale Vision Encoders: Experimental results show that when using the vision encoders of large models (e.g., mPLUG-Owl2, Qwen-VL) as source models for transfer attacks, it is difficult to achieve successful attacks on other models. The paper only provides a preliminary empirical explanation but lacks in-depth analysis of the underlying reasons (e.g., differences in feature representation mechanisms between large and small models).\n\n5. Narrow Scope of Evaluation Tasks: While the paper evaluates VEAttack on image captioning, VQA, and hallucination benchmarks, it does not test its performance on other important LVLM tasks such as image-text retrieval (only a brief ASR evaluation is provided) or visual grounding, which limits the demonstration of its generalizability.\n\n6. Overstatement on Imperceptibility: The paper claims that VEAttack has high imperceptibility through visual inspection of perturbation images, in fact, all adversarial examples satisfy this. This makes absolutely no sense.\n\n7. Lack of Defense Mechanism Research: The paper only proposes the VEAttack method but does not explore corresponding defense strategies to counter it."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0IEMq0V7Zu", "forum": "KboFptAM8S", "replyto": "KboFptAM8S", "signatures": ["ICLR.cc/2026/Conference/Submission363/Reviewer_UHud"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission363/Reviewer_UHud"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission363/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939430380, "cdate": 1761939430380, "tmdate": 1762915502988, "mdate": 1762915502988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}