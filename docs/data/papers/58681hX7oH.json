{"id": "58681hX7oH", "number": 20050, "cdate": 1758301896043, "mdate": 1759897004326, "content": {"title": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents", "abstract": "Developing autonomous agents that effectively interact with Graphic User Interfaces (GUIs) remains a challenging open problem, especially for small on-device models. In this paper, we present FERRET-UI LITE, a compact, end-to-end GUI agent that operates across diverse platforms, including mobile, web, and desktop. Utilizing techniques optimized for developing small models, we build our 3B FERRET-UI LITE agent through curating a diverse GUI data mixture from real and synthetic sources, strengthening inference-time performance through chain-of-thought reasoning and visual tool-use, and reinforcement learning with designed rewards. FERRET-UI LITE achieves competitive performance with other small-scale GUI agents. In GUI grounding, FERRET-UI LITE attains scores of 91.6%, 53.3%, and 61.2% on the ScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI navigation, FERRET-UI LITE achieves success rates of 28.0% on AndroidWorld and 19.8% on OSWorld. We share our methods and lessons learned from developing compact, on-device GUI agents.", "tldr": "A comprehensive study on building light-weight, on-device GUI agents", "keywords": ["UI understanding", "GUI agent", "Multi-modal"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e1e786a6d8452e7d7620c6bbf29bd729c63ebac3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents FERRET-UI LITE, a 3B multimodal LLM designed for agentic GUI tasks with a focus on lightweight, on-device settings. Through extensive data engineering—including synthesizing high-resolution images, annotating CoT data, and generating QA pairs, and a two-stage training process (SFT followed by RL), FERRET-UI LITE demonstrates comparable performance on multiple offline and online GUI agent benchmarks, advancing the progress of small-scale GUI agent models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors have invested a significant effort in the data engineering of synthetic data, which has demonstrated clear performance benefits across multiple benchmarks.\n2. The two-stage SFT+RL training paradigm has resulted in FERRET-UI LITE, a model with only 3B parameters, outperforming other 7B and even 32B models on several benchmarks. This is a very significant achievement."}, "weaknesses": {"value": "1. The \"Unified Action Space\" depicted in Figure 2 does not correspond to the action space presented in Table 5. The table is missing the `Scroll`, `Type`, and `Zoom-in` actions. I believe it would be better to ensure these are consistent.\n2. While I do not dispute the performance improvement from the zoom-in feature, which is quite sensible, there are potential issues with its design and description. From a design perspective, zoom-in seems to contradict the paper's motivation for a lightweight model, as it introduces additional computational overhead and inference time. If zoom-in is used only occasionally, it might be an acceptable trade-off for performance, but if it is required for most grounding actions, the cost may outweigh the benefits. I would like to know the approximate usage ratio of the zoom-in feature. From a writing perspective, it is unclear how this mechanism is implemented. Is it presented as an action? Under what conditions is it triggered during a task? Or is it not dynamically triggered but rather a mode that must be manually enabled and disabled? Furthermore, its placement in the RL section is questionable. Is it a component of the RL process? Based on Section 3, I cannot see a clear connection between this mechanism and reinforcement learning.\n3. The experiments lack a crucial baseline: the internal 3B dense model. Without results from this model, it is difficult to determine the precise magnitude of the performance gain attributable to the paper's proposed methods."}, "questions": {"value": "1. The synthesis of high-resolution grounding data is very interesting. Stitching multiple GUI images together is a clever way to create higher-resolution inputs. However, I have a couple of questions. First, does this simple stitching process introduce significant noise? For example, if the instruction is to \"click the start button,\" but each of the stitched images contains a start button, could this be detrimental to the model's training? Second, since you are using an internal model, I am not familiar with some of the details. I would like to know what resolution you set for training. For instance, a model like Qwen-2.5-VL resizes images based on a set resolution during training. If the original input image has a very high resolution (e.g., 4K or 8K), this downscaling could negatively impact the training data's quality.\n2. I have a more open-ended question. Currently, the training of GUI agents often involves two stages: SFT and RL. However, I have seen almost no work that incorporates GUI agent data into the pre-training stage. Have you considered introducing GUI agent data during pre-training, and what kind of GUI data do you think would be most suitable for this stage?\n3. What does the corresponding action for the \"Synthetic QA data\" look like? I could not find it in Table 5. If there is no dedicated action, what is the agent's output when it encounters a step that requires an answer? Is the answer written into the reason parameter of the terminate action?\n4. How is the \"curriculum task generator\" for the online navigation data designed? How many tasks were generated with this process? How do you ensure that the generated tasks are sufficiently diverse and free from hallucinations?\n5. The performance improvement from the zoom-in mechanism is not very significant on ScreenSpot-Pro (less than the improvement seen on OSWorld-G). However, as a benchmark designed specifically for high-resolution scenarios, ScreenSpot-Pro should theoretically be an ideal testbed for zoom-in. Could you explain this discrepancy?\n6. I have two questions regarding Figure 6. First, do the ratios shown in part (b) represent the proportions within the SFT data? Second, the performance results in part (a) are all higher than those in part (b). I would like to know what data ratio was used to achieve the performance shown in part (a).\n7. The gains from RL do not appear to be very significant. Is this due to the data, or is it an issue with the RL method itself? In your opinion, is online multi-turn RL the optimal solution for GUI agent reinforcement learning? (For context, I strongly disagree with current methods that achieve significant score improvements by using rule-based reward functions derived from benchmark environments.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "g89l2ghC1v", "forum": "58681hX7oH", "replyto": "58681hX7oH", "signatures": ["ICLR.cc/2026/Conference/Submission20050/Reviewer_CvXd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20050/Reviewer_CvXd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761040069269, "cdate": 1761040069269, "tmdate": 1762932945552, "mdate": 1762932945552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FERRET-UI LITE, a 3B end-to-end GUI agent targeting on-device deployment.\nThe system is trained with a two-stage pipeline (SFT + RLVR), combining curated real and synthetic GUI datasets, chain-of-thought augmented training, visual zoom-in grounding, and verifiable RL rewards.\nThe model achieves competitive results in GUI grounding across ScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G, and modest navigation performance on AndroidWorld and OSWorld-Verified benchmarks. The paper further reports ablations on data mixing, synthetic trajectories, inference-time scaling, and RL reward design."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clarity & completeness — The paper is clearly written and provides a well-structured and reproducible training recipe, covering data construction, format unification, RL reward design, and ablations.\nSolid engineering & empirical study — Comprehensive ablations on zoom-in refinement, SFT/RL distinction, and grounding/navigation mixture make the empirical findings credible.\nBridges grounding & navigation — The finding that grounding and navigation data can benefit each other is a useful empirical insight for future dataset design."}, "weaknesses": {"value": "Limited novelty for  training pipeline\nThe SFT stage follows nearly the same recipe as OpenCUA/OS-Atlas style SFT, and the RL stage closely reflects the “self-evolutionary RL for GUI grounding” paradigm (e.g., Enhancing Visual Grounding via Self-Evolutionary RL, 2025). The work mostly repackages known recipes toward a smaller model rather than enacting a new algorithmic insight.\n\nScaling result does not support a strong “small model wins” claim\nAlthough the paper frames 3B on-device as the goal, the performance gap to 7B is still non-negligible on navigation, and competitors in similar scale baselines already exist. Moreover, 3B vs 7B may become irrelevant soon as memory/latency gap is shrinking; this undermines the claimed significance.\n\nLimited progress on OSWorld-Verified\nThe navigation result on OSWorld-Verified is still low (comparable or worse to OpenCUA). This weakens the claimed “competitive” performance beyond grounding benchmarks, especially since OSWorld-Verified is arguably the most meaningful benchmark for agent robustness."}, "questions": {"value": "Will the authors fully open-source data + checkpoints?\nReproducibility is key here, especially since part of the appeal is “practical recipe”.\n\nAbout test-time scaling (zoom-in / long-cot / enlarge-steps)\nDoes this not contradict the “efficient on-device” claim? \n\nOn 3B vs 7B vs scaling laws\nThe study positions 3B as “the right scale”, but is there evidence the model is at an optimal efficiency frontier rather than “just smaller”? Why not study scaling law in GUI agents explicitly (e.g., trade-off curves)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XisqbSjikd", "forum": "58681hX7oH", "replyto": "58681hX7oH", "signatures": ["ICLR.cc/2026/Conference/Submission20050/Reviewer_cvhi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20050/Reviewer_cvhi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761376565940, "cdate": 1761376565940, "tmdate": 1762932944465, "mdate": 1762932944465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Ferret-UI Lite, a small-sized VLM based GUI agent, and the full training analysis. It studies the effects of SFT & RL, data curation & mixture, and CoTs. The paper provides a comprehensive analysis of the impact of different settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides a comprehensive analysis of the effects and impacts from the following perspectives: data curation and mixture, training paradigm (SFT / RL), RL reward design, synthetic CoT. Based on the analysis, authors train a small-size 3B model, achieving SOTA on both GUI grounding and navigation compared with other small-size models and competitive performance with larger (>=7B) models."}, "weaknesses": {"value": "**No novelty in the analyzed perspectives**\n- Data collection and curation processes are all from previous works\n- RL reward design is also from UI-R1 and GUI-R1\n- Training using GRPO and SFT\n\nThe discussed and analyzed factors are studied by previous works thoroughly, such as how the reward design can achieve better performance, how CoT can benefit the model, etc. This work seems a combination of what others have done."}, "questions": {"value": "1. The synthetic data are generated by GPT-4o (line 194), how is the quality and accuracy of this outdated model? What if using the latest models (before the submission deadline) such as GPT-4.1 / 5, Gemini-2.0/2.5-pro? Are these annotations verified by human?\n2. In line 215, how does the VLM judge work and what model is used as the judge? Did human verify the correctness of the judgment and what is the accuracy?\n3. Since recently more synthetic data is used by many works, how does the quality of the autonomous verification filtering in navigation data as well as the generation result of general LLMs affect the model performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n3Giqr81id", "forum": "58681hX7oH", "replyto": "58681hX7oH", "signatures": ["ICLR.cc/2026/Conference/Submission20050/Reviewer_EzWL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20050/Reviewer_EzWL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622238082, "cdate": 1761622238082, "tmdate": 1762932943720, "mdate": 1762932943720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a small on-device GUI Agent, FERRET-UI LITE 3B. It provides a comprehensive training recipe from data collection to training algorithms. Extensive experiments on both grounding benchmarks and navigation benchmarks illustrate the superior performances."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper focuses on the promising topic and offers a 3B-sized GUI Agent.\n2. The performances on diverse benchmarks are competitive.\n3. It provides a comprehensive training recipe for the community."}, "weaknesses": {"value": "1. It remains unclear what is the main contribution on building \"on-device\" GUI Agents? It seems the overall training recipe can be generalized to other model sizes and is not specially designed for small on-device models. Therefore, how does the training recipe benefit other sizes of models? The authors should provide more explanations on this point.\n\n2. Also, the title stresses on the \"Lessons\". But the main part of the paper covers few of failure attempts.\n\n3. Why can the multi-agent rollouts be viewed as \"online\" ? Does this \"online\" concept differs from the online RL ?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x71mP4WTxD", "forum": "58681hX7oH", "replyto": "58681hX7oH", "signatures": ["ICLR.cc/2026/Conference/Submission20050/Reviewer_3vw9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20050/Reviewer_3vw9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20050/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762615910884, "cdate": 1762615910884, "tmdate": 1762932943212, "mdate": 1762932943212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}