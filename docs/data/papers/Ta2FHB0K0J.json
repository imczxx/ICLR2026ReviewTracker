{"id": "Ta2FHB0K0J", "number": 16123, "cdate": 1758260298757, "mdate": 1763733615998, "content": {"title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing", "abstract": "Although existing model editing methods perform well in recalling exact edit facts, they often struggle in complex scenarios that require deeper semantic understanding rather than mere knowledge regurgitation. Leveraging the strong contextual reasoning abilities of large language models (LLMs), in-context learning (ICL) becomes a promising editing method by comprehending edit information through context encoding. However, this method is constrained by the limited context window of LLMs, leading to degraded performance and efficiency as the number of edits increases. To overcome this limitation, we propose InComeS, a flexible framework that enhances LLMs’ ability to process editing contexts through explicit compression and selection mechanisms. Specifically, InComeS compresses each editing context into the key-value (KV) cache of a special gist token, enabling efficient handling of multiple edits without being restricted by the model’s context window. Furthermore, specialized cross-attention modules are added to dynamically select the most relevant information from the gist pools, enabling adaptive and effective utilization of edit information. We conduct experiments on diverse model editing benchmarks with various editing formats, and the results demonstrate the effectiveness and efficiency of our method.", "tldr": "", "keywords": ["Parallel encoding; Compression; knowledge editing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/86bea58504041ded76134317a2f30f9c14825b99.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces InComeS, a novel framework for efficient model editing in Large Language Models (LLMs). It addresses a key limitation of In-Context Learning (ICL)-based editing methods, whose performance and efficiency degrade with a large number of edits due to the finite context window. The proposed solution involves compressing each edit context into a special \"gist token's\" KV cache, bypassing the context length constraint. Furthermore, the authors incorporate cross-attention modules to enable the model to dynamically select the most relevant information from a pool of these compressed gist tokens. Experiments across various complex editing benchmarks (e.g., multi-hop, natural language edits) demonstrate that InComeS outperforms existing methods in effectiveness and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper clearly identifies a significant and practical challenge in model editing and propose InComeS a flexible framework that enhances LLMs’ ability to process editing contexts through explicit compression and selection mechanism.\n\nThe Gist token used in Editing is very interesting. And editing the attention module is also novel."}, "weaknesses": {"value": "* How can the GIST token be effectively trained? Furthermore, once trained, what metrics should be used to evaluate the generalization capability of the GIST token? \n﻿ \n* The training process requires approximately 11 hours for Llama-3.2-1B and 35 hours for Qwen2.5-7B. Considering the performance gains achieved, how does the efficiency of this approach compare to other model editing techniques, such as In-Context Learning (ICL)? \n﻿ \n* As your experiments indicate, simple fine-tuning (FT) can yield strong results. However, incorporating a more detailed analysis of locality could further strengthen the evaluation, given that locality preservation is a key consideration in model editing methods."}, "questions": {"value": "* Under what circumstances is the GIST token applied? Is it used for every inference instance, or is it selectively activated based on specific input criteria?\n * Given that model editing typically prioritizes efficiency, what is the rationale behind adopting a teacher-student training framework, especially considering its apparently substantial computational cost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NOBEHHJ9Iy", "forum": "Ta2FHB0K0J", "replyto": "Ta2FHB0K0J", "signatures": ["ICLR.cc/2026/Conference/Submission16123/Reviewer_2yPy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16123/Reviewer_2yPy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761045813888, "cdate": 1761045813888, "tmdate": 1762926295583, "mdate": 1762926295583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes InComeS, an in-context–style editing framework that (i) compresses each edit into a single “gist” token’s KV cache and (ii) equips the base LM with cross-attention modules that, at generation time, select among a pool of cached gists plus a special “zero-gist” option. Training uses token-wise reweighting—based on loss differences with/without edit context from a teacher model—together with a KL term to distill the teacher into the student. Experiments on MQuAKE, DUNE, WikiDataCounterfact and ZsRE-extended show competitive accuracy and efficiency versus ICL and a range of editing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) Compressing edits into re-usable gist KV caches and adding token-level cross-attention to select among them is clean; Zero-gist, serving as a “no-selection” option, reduces interference from the edit context on irrelevant tokens (see ablations) and complements the locality metric.\n(2) Evaluates the effectiveness of the method across multiple scenarios, including multi-hop edits (MQuAKE), natural-language edits (DUNE), and ripple/portability settings (WikiDataCounterfact, ZsRE-extended)."}, "weaknesses": {"value": "(1) The paper does not include comparisons with recent strong editors such as memory based RECIPE[1] and ICL retriever based DR-IKE[2]. Without these, the empirical claims lack persuasiveness regarding true advances over contemporary methods.\n[1]Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning\n[2]Dynamic Retriever for In-Context Knowledge Editing via Policy Optimization\n(2) Despite criticizing ICL’s limitations, results show InComeS often performs on par with or worse than ICL—e.g., On multi-hop, InComeS underperforms ICL on Qwen2.5-7B for single 2- (66.46% vs 69.76%) and 3-hop (71.24% vs 76.91) settings (Table 1); on portability, InComeS is close to or slightly below ICL for Llama-3.2-1B and Qwen2.5-7B on batch editing results, such as WikiDatacounterfact Edit Success (71.44% vs 85.28%) and ZsRE-extended Portability (61.22% vs 64.57%) (Table 3)."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zdVSLGTBvr", "forum": "Ta2FHB0K0J", "replyto": "Ta2FHB0K0J", "signatures": ["ICLR.cc/2026/Conference/Submission16123/Reviewer_cAvh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16123/Reviewer_cAvh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832500383, "cdate": 1761832500383, "tmdate": 1762926295120, "mdate": 1762926295120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces InComeS, a novel and flexible framework for efficient model editing in LLMs. The method is designed to overcome the scalability and efficiency limitations of traditional In-Context Learning (ICL) for editing, where performance degrades as the number of edits increases due to the finite context window. The core contribution of InComeS is a two-stage process of compression and selection. First, each piece of editing information is independently compressed into the KV cache of a special gist token. Second, the model is augmented with specialized cross-attention modules. These modules enable the model to dynamically and selectively attend to the pool of compressed gist tokens at inference time, retrieving the most relevant information for a given query. The authors conduct extensive experiments on a variety of complex model editing benchmarks. The results demonstrate that InComeS consistently outperforms a wide range of existing editing methods, showing marked improvements over the strong ICL baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The integration of gist-based context compression with a learnable, dynamic selection mechanism is a novel combination that directly addresses the bottlenecks of ICL for batch editing.\n\n2. By compressing edits in parallel and using a lightweight selection mechanism, it offers substantial speedups over ICL. This makes the approach practical for real-world applications.\n\n3. This paper is clearly written, well organized, and generally easy to understand."}, "weaknesses": {"value": "1. The method requires a continued pre-training phase to teach the model the compression and selection mechanisms.\n\n2. The results in Table 1 show that the improvement of InComeS on Llama-3.2-1B is much greater than that on Qwen2.5-7B. This may indicate that the effectiveness of the method diminishes as the model scale increases."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ImDIgRDXJk", "forum": "Ta2FHB0K0J", "replyto": "Ta2FHB0K0J", "signatures": ["ICLR.cc/2026/Conference/Submission16123/Reviewer_dQs2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16123/Reviewer_dQs2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901495884, "cdate": 1761901495884, "tmdate": 1762926294556, "mdate": 1762926294556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "InComeS proposes a framework to edit the LLM through compression and selection mechanisms. The authors demonstrate that InComeS offers improved efficiency and accuracy compared to strong baselines and test on several challenging datasets and complex editing scenarios. The method's effectiveness is supported by results on multi-hop, natural language, and editing tasks which need reasoning ability, demonstrating strong scalability and adaptability across diverse editing tasks. And it is further analyzed through extensive ablations and analysis experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes the use of KV cache to improve efficiency and proposes corresponding training algorithms to improve the performance, and cross-attention modules are added to dynamically select the most relevant information gist.\n\n2. The paper conducts a large number of experiments and thorough analysis."}, "weaknesses": {"value": "1. There are only two models used in the paper, and the latest model (e.g. Qwen3-8B) is not used. If results about this model are reported, it will be more convincing. If the time is not sufficient, the author could consider only adding a small number of baselines for comparison.\n\n2. The performance of the model in Table 3 is not competitive.\n\n3. The paper uses the method of compressing content into key-value cache of gist tokens to achieve this. However, for different models, different gist token representations need to be saved. This method is similar to RAG, but the contents of RAG are visible. However, it is difficult to trace the contents of gist, which may cause some problems for understanding. And the content searched by RAG can be used by different model without other preprocessing. But the gist is only used for only one model. The design of the InComeS is not complete as RAG. For example, What should be done if the edited content is duplicated, and how to maintain all vectors in pool, like insert new gist and remove? The author did not design a special module to handle this situation."}, "questions": {"value": "The ICL method in the paper is to fill all the information into the context. If the RAG method is used and only the most relevant information is selected for filling, what would be the result and efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5ZsTWvndOG", "forum": "Ta2FHB0K0J", "replyto": "Ta2FHB0K0J", "signatures": ["ICLR.cc/2026/Conference/Submission16123/Reviewer_CWX5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16123/Reviewer_CWX5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902759094, "cdate": 1761902759094, "tmdate": 1762926294011, "mdate": 1762926294011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}