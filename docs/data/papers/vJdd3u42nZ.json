{"id": "vJdd3u42nZ", "number": 10061, "cdate": 1758159304126, "mdate": 1759897677267, "content": {"title": "TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion", "abstract": "Reinforcement learning with stochastic optimal control offers a promising framework for diffusion fine-tuning, where a pre-trained diffusion model is optimized to generate paths that lead to a reward-tilted distribution. While these approaches enable optimization without access to explicit samples from the optimal distribution, they require training on rollouts under the current fine-tuned model, making them susceptible to reinforcing sub-optimal trajectories that yield poor rewards. To overcome this challenge, we introduce **TR**ee-Search Guided **TR**ajectory-Aware Fine-Tuning for **D**iscrete **D**iffusion (**TR2-D2**), a novel framework that optimizes reward-guided discrete diffusion trajectories with tree search to construct replay buffers for trajectory-aware fine-tuning. These buffers are generated using Monte Carlo Tree Search (MCTS) and subsequently used to fine-tune a pre-trained discrete diffusion model under a stochastic optimal control objective. We validate our framework on single- and multi-objective fine-tuning of biological sequence diffusion models, highlighting the overall effectiveness of TR2-D2 for reliable reward-guided fine-tuning in discrete sequence generation.", "tldr": "We introduce TR2-D2, a framework that uses tree search to optimize trajectories of discrete diffusion models to construct replay buffers for trajectory-aware fine-tuning under single- or multi-objective rewards.", "keywords": ["discrete diffusion", "fine-tuning", "reinforcement learning", "multi-objective optimization", "AI for science", "reward optimization", "biological sequence design"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36fae0e2ecc7e47e120bf594f9019714c5dbc66d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an iterative fine-tuning framework for discrete diffusion models that leverages Monte Carlo Tree Search (MCTS) to generate high-quality samples, combined with a WDCE loss to finetuning the model. The method is evaluated on biological sequence design tasks and demonstrates impressive empirical performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors provide comprehensive comparisons against many SOTA baselines for fine-tuning diffusion models in biological sequence design, with results that generally support their conclusions. The improvements are particularly striking. The motivation behind the proposed innovation is clear, and the methodology is technically sound. The paper is well-written and easy to follow. Moreover, the proposed approach can be extended to multi-objective optimization tasks, significantly enhancing its practical utility."}, "weaknesses": {"value": "My primary concern lies in the experimental evaluation. Reinforcement learning–based fine-tuning methods are typically computationally expensive. While inference-time search techniques such as MCTS can indeed produce higher-quality samples, they come at the cost of additional computational overhead. Using MCTS-generated samples to construct datasets for fine-tuning compounds these costs. However, the paper does not provide any comparison of computational efficiency with baseline methods.\n\nAnd theoretically, the more MCTS iterations you perform, the better samples you should get. However,\nin Table 5, performance decreases as “N_iter” increases. This trend is also inconsistent with the results in Table 6.\n\nSeveral critical hyperparameters—such as \n$c,N_{\\text{sample}},R,\\alpha$ are not explained in terms of their selection criteria, even though they have a strong influence on performance.\n\nIn Figure 2, the curves are not clearly labeled.\n\nIn Equation (13), the $x$ should be $X$"}, "questions": {"value": "Mainly related to the weaknesses above.\nAdditionally, in Equation (9), why does $M$ appear in the denominator?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VRkbj0Gphb", "forum": "vJdd3u42nZ", "replyto": "vJdd3u42nZ", "signatures": ["ICLR.cc/2026/Conference/Submission10061/Reviewer_NznU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10061/Reviewer_NznU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761367622481, "cdate": 1761367622481, "tmdate": 1762921456774, "mdate": 1762921456774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TR2-D2, a novel framework for fine-tuning pre-trained discrete diffusion models. It is widely known that standard reinforcement learning (RL) fine-tuning can be unstable. To address this problem, TR2-D2 proposes combining MCTS-based search with off-policy RL for stable diffusion fine-tuning. The extension of this framework to multi-objective optimization is a plus, which the authors claim is the first for discrete diffusion models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. While off-policy learning is a known concept in RL, the core idea of using MCTS to explicitly curate a good buffer for off-policy learning, as far as I know, is new in trajectory-aware fine-tuning of diffusion models.\n2. The extension to multi-objective fine-tuning, i.e., using MCTS to discover a set of Pareto-optimal trajectories, is a plus, due to the popularity of conflicting objectives in scientific applications."}, "weaknesses": {"value": "1. As the decoupling to off-policy learning is a key methodological choice, the paper does not sufficiently describe all information related to solving the known challenges of off-policy RL (i.e., distribution mismatch, high variance of importance sampling, and hyperparameter sensitivity, especially the periodic model update frequency). Since off-policy RL is often difficult to implement, it is important to justify why this work is \"uniquely\" successful here.  Is the hyper-parameter selection sensitive in this method?\n\n2. This work has a decent amount of equations. But no real theoretical insights are provided. For example, one important aspect is whether authors can provide any level of formal justifications on why the distribution mismatch can be mitigated by the MCTS buffer. Is this just for discrete diffusion, or is this just expecially useful for trajectory-aware learning?\n\n3. Relevant to the real mechanism that makes MCTS work, one experimental weakness is revealed from the conflicting results regarding the number of MCTS iterations. For the DNA task (Table 5), larger iterations worsen enhancer Pred-Activity (this is concerning, as it suggests that MCTS may not promote learning as the authors propose, or at least that its interactions are more complex). In contrast, Figure 4 shows more iterations indeed lead to somewhat better performance (as we wish). In summary, the current version raises major concerns about the robustness of the MCTS component and suggests a highly sensitive, unexplained interaction with the reward landscape and/or learning hyperparameters. Thus, it remains confusing how practical and useful this method can be adapted elsewhere.\n\n4. MCTS search, if applied in practice, is highly costly. The current version seems to ack an analysis of the computational overhead. If logging computational costs is tricky, it is widely acceptable to record such statistics by fixing a iteration of MCTS, e.g., fixing at 5 or 10."}, "questions": {"value": "see above.\n\nP.S. Table 2 shows this method is better than baselines, but it misses a key ablation to show whether this good performance indeed comes from MCTS. Please update Table 2 to include the \"TR2-D2 w/o MCTS\" baseline. If possible, it's crucial to ablate MCTS iterations in this experiment as well to show the empirical validity of introducing it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mJS9ZA5Et4", "forum": "vJdd3u42nZ", "replyto": "vJdd3u42nZ", "signatures": ["ICLR.cc/2026/Conference/Submission10061/Reviewer_fhok"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10061/Reviewer_fhok"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781938725, "cdate": 1761781938725, "tmdate": 1762921456489, "mdate": 1762921456489, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose TR2-D2, a tree-search guided trajectory-aware fine-tuning method for discrete diffusion models. TR2-D2 utilizes Monte Carlo Tree Search to generate replay buffers that are subsequently used for off-policy fine-tuning. The authors showcase the effectiveness of TR2-D2 on single and multi-objective optimization tasks of biological sequence diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The idea to adapt MCTS to construct a replay buffer for the off-policy RL method of discrete diffusion models is interesting.\n- Experiments across both single-objective and multi-objective settings demonstrate the empirical effectiveness of TR2-D2."}, "weaknesses": {"value": "- TR2-D2 seems to be a simple combination of two existing methods: the widely applied MCTS algorithm and the MDNS algorithm [1] for off-policy RL of discrete diffusion models, which limits its novelty. Also, combining searching algorithms like MCTS with RL algorithms is not novel and has been applied for, e.g., offline model-based RL [2].\n- The computational cost of the MCTS step is intensive. Therefore, a fairer comparison with the baselines should also take the computational cost into account. For example, it would be helpful to compare model performance under the same computational budget and the performance scaling curve of TR2-D2 as the computation increases.\n- The comparison to the MDNS baseline is only provided in the appendix and not in the main table. Also, an ablation comparing the combination of different searching algorithms and different off-policy RL algorithms is missing.\n- TR2-D2 utilizes an off-policy RL algorithm, which is known to have off-policyness issue and leads to inferior performance than the on-policy RL algorithms. Does TR2-D2 suffer from this issue? How does the size of the replay buffer B affect the performance of TR2-D2?\n\n\n\n[1] MDNS: Masked Diffusion Neural Sampler via Stochastic Optimal Control. NeurIPS 2025.\n\n[2] Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning. arxiv 2024."}, "questions": {"value": "Please refer to the **Weaknesses** section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0F1gEUiZdL", "forum": "vJdd3u42nZ", "replyto": "vJdd3u42nZ", "signatures": ["ICLR.cc/2026/Conference/Submission10061/Reviewer_hz8y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10061/Reviewer_hz8y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974613268, "cdate": 1761974613268, "tmdate": 1762921455949, "mdate": 1762921455949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on reward-oriented fine-tuning for discrete diffusion models. Addressing the limitations of traditional reinforcement learning fine-tuning methods—which rely on the current model to generate trajectories and are prone to reinforcing suboptimal trajectories—it proposes the TR²-D² framework. This approach employs Monte Carlo Tree Search (MCTS) to generate high-reward trajectories, constructs a replay buffer, and then combines a stochastic optimal control objective to perform trajectory-aware fine-tuning on pre-trained discrete diffusion models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Precise targeting of the core issue in traditional RL fine-tuning—discrete diffusion (leading to suboptimal reinforcement due to reliance on current model trajectories)—enables MCTS to proactively explore high-reward trajectories. This fundamentally enhances training data quality, preventing model “learning bias,” forming a logically closed-loop system with high innovation.\nUsing continuous-time Markov chains (CTMC) as the theoretical framework, we ensure the method's mathematical rigor, rather than merely engineering improvements."}, "weaknesses": {"value": "My major concern lies in the novelty of this paper, given (1) CTMC widely used in RL domain and (2) existing works have already demonstrated that the diffusion model can serve as the policy in the context of RL. Adopting RL-related search strategies to diffusion model domain seems ad-hoc.\nRegarding the methods itself, I doubt the stability and the local optima problem the method may face. Please refer to questions to see how to get through these problems."}, "questions": {"value": "I understand that Monte Carlo tree search may become stuck in local optima. If this occurs, how can we handle it to prevent the generated sequence from falling into such situations?\n\nDoes it depend on the initial model? If the initial model performs poorly, is this fine-tuning method still effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8dHQUKDagg", "forum": "vJdd3u42nZ", "replyto": "vJdd3u42nZ", "signatures": ["ICLR.cc/2026/Conference/Submission10061/Reviewer_Kyq1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10061/Reviewer_Kyq1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10061/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003170039, "cdate": 1762003170039, "tmdate": 1762921454568, "mdate": 1762921454568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}