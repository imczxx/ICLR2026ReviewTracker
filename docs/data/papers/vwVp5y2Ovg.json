{"id": "vwVp5y2Ovg", "number": 4149, "cdate": 1757612683086, "mdate": 1759898050809, "content": {"title": "Towards Foundation Models for Cryo-ET Subtomogram Analysis", "abstract": "Cryo-electron tomography (cryo-ET) enables in situ visualization of macromolecular structures, where subtomogram analysis tasks such as classification, alignment, and averaging are critical for structural determination. However, effective analysis is hindered by scarce annotations, severe noise, and poor generalization. To address these challenges, we take the first step towards foundation models for cryo-ET subtomograms. First, we introduce CryoEngine, a large-scale synthetic data generator that produces over 904k subtomograms from 452 particle classes for pretraining. Second, we design an Adaptive Phase Tokenization-enhanced\nVision Transformer (APT-ViT), which incorporates adaptive phase tokenization as an equivariance-enhancing module that improves robustness to both geometric and semantic variations. Third, we introduce a Noise-Resilient Contrastive Learning (NRCL) strategy to stabilize representation learning under severe noise conditions. Evaluations across 24 synthetic and real datasets demonstrate state-of-the-art (SOTA) performance on all three major subtomogram tasks and strong generalization to unseen datasets, advancing scalable and robust subtomogram analysis in cryo-ET.", "tldr": "", "keywords": ["cryo-ET subtomogram; foundation model; equivariance"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03efa6d3fbb0613553fda54b0066e25f3ecd46ec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces APT-ViT, a vision transformer foundation model for cryo-ET. The authors first introduce CryoEngine, a synthetic data generator to create a large amount of data that covers diverse particle classes for pretraining. Then the authors develop APT-ViT, a ViT architecture designed for cryo-ET tasks. Thirdly, the authors also introduce NRCL, a contrastive learning strategy tailored for cryo-ET and inspired by MoCo v3. The trained model outperforms baseline methods on classification, alignment, aand veraging tasks, suggesting the effectiveness of the method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed APT-ViT has a cryo-ET-tailored design and demonstrates strong performance on downstream tasks. The effectiveness of this design is also validated in the ablation study of Table 5.\n- The adoption of contrastive learning and design of NRCL is well motivated, and the introduction to the algorithm is provided in detail.\n- The experimental results show superior performance compared to baseline methods, and the evaluation is highly comprehensive.\n- The description of designs and algorithms is provided with lots of details."}, "weaknesses": {"value": "- While the paper writing is comprehensive in terms of the description of the method and evaluation, it lacks the necessary background to non-experts of cryo-ET, especially as a conference submission.\n- In the evaluation of classification, when applying MAE or NRCL on the same CryoEngine-created dataset, different model architectures are adopted (ViT-B vs. APT-ViT). To me, this is not a strictly fair comparison.\n- Also, for the classification, since the SwinViT architecture trained on ImageNet performs much better compared to the standard ViT, the baselines trained on the CryoEngine dataset could also benefit from the adoption of the SwinViT architecture and achieve better performance. But this experiment is not included in the paper.\n- One last point for the classification. My understanding is that SwinViT and standard ViT differ in the network design after tokenization, where SwinViT has additional patch merging to save the computation cost. Since SwinViT seems to be better compared to standard ViT, the adoption may further improve the performance."}, "questions": {"value": "Please see the last three points of the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aKxRAqE7TD", "forum": "vwVp5y2Ovg", "replyto": "vwVp5y2Ovg", "signatures": ["ICLR.cc/2026/Conference/Submission4149/Reviewer_pHqM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4149/Reviewer_pHqM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760479765916, "cdate": 1760479765916, "tmdate": 1762917199603, "mdate": 1762917199603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on developing a foundation model for cryo-electron tomography (cryo-ET) subtomogram analysis. Specifically, the authors introduce a large-scale synthetic data generator that produces subtomograms for model pretraining. An Adaptive Phase Tokenization-enhanced Vision Transformer (APT-ViT) is designed to ensure robustness against both geometric and semantic variations. A Noise-Resilient Contrastive Learning (NRCL) strategy is proposed to stabilize representation learning under severe noise. Evaluations on both synthetic and real datasets demonstrate state-of-the-art performance across three key subtomogram tasks—classification, alignment, and averaging."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is very well motivated and gives a reasonably complete and self-contained recipe (synthetic data generation → network architecture → pre-training paradigm) for building a foundation model for cryo-ET.\n\n2. The proposed Adaptive Phase Tokenization (APT) is a well-motivated and reasonable design choice to enhance the SE(3) equivariance of Vision Transformers. \n\n3.  The experimental evaluation is thorough and convincing. The authors conduct extensive experiments and ablation studies to systematically demonstrate the effectiveness of the proposed training strategies, architectural components, and contrastive learning objective."}, "weaknesses": {"value": "1. The pretraining objective is conceptually simple—distinguishing whether two subtomograms correspond to the same protein class under random SE(3) transforms and noise. While effective for learning pose-invariant embeddings, this binary contrastive setup may be too limited in structural diversity and task complexity. It lacks finer-grained structural reasoning (e.g., subunit composition, symmetry, local density context) that could more directly benefit downstream subtomogram analysis tasks. \n\n2. During pretraining, the paired subtomograms are generated from rigidly transformed versions of the same PDB structure. This setup may risk limiting the model’s ability to capture conformational diversity.\n\n3. Most showcased qualtitative results remain at relatively coarse resolutions (around 10 Å or lower), which fall short of the biological interpretability required for detailed structural analysis."}, "questions": {"value": "1. The CryoEngine-generated subtomograms are 32×32×32 voxels at 10 Å resolution. Given that real cryo-ET data often have lower effective resolution (20–40 Å), could this setup introduce a domain gap? Specifically, how sensitive is the pretrained model to changes in voxel resolution or subtomogram size, and would larger or multi-scale crops (e.g., 48³ or 64³) further improve generalization to real tomograms?\n\n2. Would incorporating flexible-body deformations or simulated conformational states in CryoEngine improve the model’s generalization to heterogeneous real data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pDJLQ5XvxM", "forum": "vwVp5y2Ovg", "replyto": "vwVp5y2Ovg", "signatures": ["ICLR.cc/2026/Conference/Submission4149/Reviewer_tCde"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4149/Reviewer_tCde"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988892209, "cdate": 1761988892209, "tmdate": 1762917199203, "mdate": 1762917199203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a \"foundation model\" for cryo-ET subtomogram analysis built from three pieces: (1) CryoEngine, a synthetic data pipeline that generates 904k volumes across 452 classes from PDB structures for pretraining; (2) APT-ViT, a ViT backbone with adaptive phase tokenization and 3D spherical-steerable convolutions to improve SE(3) equivariance; and (3) NRCL, a noise-resilient contrastive objective that treats clean vs. very noisy versions as positives/negatives. The pretrained encoder is frozen and evaluated (with light heads) on classification, alignment, and subtomogram averaging, and the paper claims SOTA across 24 synthetic and real datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- A single encoder for classification, alignment, and averaging is appealing for the field. \n- CryoEngine simulates tilt series, reconstructs tomograms, and extracts 32³ subtomograms with calibrated SNR variants and preserved pose metadata. This is explained with unusual clarity. \n- APT-ViT's adaptive phase tokenization plus 3D spherical-steerable layers is novel and motivated.\n- NRCL’s clean-vs-noisy pairing is simple and matches the physics of photon-limited imaging.\n- The pretraining corpus (904k / 452 classes) is large for cryo-ET and documented."}, "weaknesses": {"value": "- The pretraining set focuses on proteasomes and ribosomes (20S/30S families), yet many evaluation datasets (e.g., 80S ribosome) are closely related at the family level. This makes the \"24 OOD datasets\" claim less convincing than presented. Please quantify family overlap and test truly orthogonal families.\n- Also, the paper’s \"foundation model\" and \"diversity\" claims are not supported by the data: pretraining is dominated by just two families (20S proteasome and 30S ribosome), with the reported \"452 classes\" largely reflecting intra-family conformations/subtypes rather than broad cross-category coverage; moreover, branding this as the \"first cryo-ET foundation model\" is unsubstantiated without a systematic, empirical comparison to prior 3D/cryo-ET pretraining work.\n- Pretraining uses tilt ranges up to ±90° specifically to decouple the missing wedge, whereas real datasets typically have ≈30° wedge (e.g., ±60°). This mismatch may inflate pretraining benefits and undercut claims of robustness. Please pretrain with realistic tilt limits and report the delta.\n- The paper adds calibrated Gaussian noise to already reconstructed 32³ volumes; this bypasses dose/CTF/motion effects and can change the noise statistics compared to experimental data. Show results when noise is applied at projection/tilt-series level with dose weighting/CTF to better match reality.\n- All listed classification baselines are 2D vision models (ConvNeXt/ViT/PVT/Swin/MAE/MoCo/DINO), but the task is 3D volumetric. The paper doesn’t document how these were adapted to 3D tokens/patching, nor does it include strong 3D-equivariant classification baselines. Please add 3D CNNs and SE(3)-equivariant classifiers, and detail all adaptations.\n- Fig. 1 indicates a branch supervised directly by the ground-truth transform T during pretraining. This couples the learned representation to a task-specific signal; please ablate the supervised branch and report whether the \"foundation\" properties persist.\n- Missing strong 3D equivariant classification baselines (e.g., SE(3)-Transformer); such models appear only in the registration experiments, not in the classification comparisons."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HvutQ8upId", "forum": "vwVp5y2Ovg", "replyto": "vwVp5y2Ovg", "signatures": ["ICLR.cc/2026/Conference/Submission4149/Reviewer_pbJE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4149/Reviewer_pbJE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762177013097, "cdate": 1762177013097, "tmdate": 1762917198782, "mdate": 1762917198782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "First cryo-ET foundation model CryoFM (CryoEngine, APT-ViT, NRCL) achieves SOTA on 3 core tasks and OOD generalization. The paper presents a highly original approach to address core challenges in cryo-ET analysis. The use of a foundation model framework, comprehensive synthetic data engine, and specialized architecture is commendable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "This work is highly significant as it introduces the first foundation model framework specifically tailored for cryo-ET subtomogram analysis. This approach successfully addresses the fundamental bottleneck of scarce, low-quality labeled data in structural biology. The demonstration of strong generalization ability across 24 OOD datasets is a powerful validation of the \"foundation model\" concept in this field.\n\nCryoEngine is an essential and high-quality component, effectively solving the data pre-training challenge. APT-ViT is a creative architectural contribution that marries the global reasoning of ViTs with the required 3D physical symmetries: Adaptive Phase Tokenization handles translation non-rigidity, and Steerable Spherical Harmonic Convolution (SSHC) inherently enforces rotational equivariance, which is critical for 3D density maps.\n\nThe Noise-Resistant Contrastive Learning (NRCL) is well-motivated and essential for the low-SNR environment of cryo-ET. The combination of contrastive loss with an MSE objective targeting a \"clean view\" is a clever way to push the encoder to learn features robust to noise, and the linear probing/KNN results convincingly show that the benefits are embedded in the learned representations."}, "weaknesses": {"value": "### Insufficient Box Size Discussion and Experimentation \nA key weakness is the use of a small $32^3$ input box size without sufficient experimental justification. While this may be necessary for computational efficiency, it severely limits the context a foundation model can learn, especially for large, dynamic, or membrane-associated complexes requiring local environmental context. The paper is currently missing a comparative ablation experiment (e.g., comparing $32^3$ with $64^3$ or $128^3$) to definitively show that the model’s performance is optimal at this size, or to quantify the trade-off. This needs to be addressed for the model to be considered generally applicable.\n\n### Limited Structural Diversity in Pre-training Data \nThe current pre-training set is primarily composed of only two major structures (20S Proteasome and 30S Ribosome). For a foundational model, this limited structural repertoire raises concerns about the model's true extrapolation ability to completely new structural classes or cellular scenes, such as transmembrane proteins, long filaments, or crowded, complex environments. The authors should explicitly discuss this limitation and its potential impact on generalization beyond the tested 24 OOD datasets.\n\n### Idealization in Tilt-Range and Simulation\nThe decision to use a full $\\pm 90^\\circ$ tilt range during CryoEngine pre-training to \"decouple the missing wedge\" is non-physical for real cryo-ET acquisition (which is typically $\\pm 60^\\circ$ or less). This idealization might artificially simplify the missing wedge problem, potentially leading to a model that is over-reliant on high-tilt information not present in real data. Furthermore, the use of idealized simulation techniques in the Density Volume Converter (isotropic Gaussian kernel superposition, linear normalization) might also lead to a gap between synthetic data realism and real-world complexity (anisotropy, non-uniform noise)."}, "questions": {"value": "### Box Size \nPlease provide a clear computational and performance justification for selecting the $32^3$ box size. Can the authors include an ablation study that compares the performance of CryoFM on a key task (e.g., classification or registration) when using $32^3$ vs. a larger size (e.g., $64^3$ or $128^3$)? This would significantly address a major weakness.\n\n### The Gap between synthetic and experimental data\nWhy does CryoEngine pre-training (Table 1, Row 10) not show a significant performance uplift compared to the ImageNet baseline (Row 3), despite using the same ViT-B architecture and fine-tuning? Please elaborate on whether this indicates a limitation in the synthetic data's fidelity or the overwhelming influence of ImageNet features. \n\n### Subtomogram Preprocessing Pipeline\nWere the subtomograms preprocessed (e.g., CTF correction, denoising) before being fed to the model? For reproducibility and clarity, a detailed, step-by-step description of the full subtomogram preparation pipeline is required.\n\n### Tilt Range and Missing Wedge \nCan the authors elaborate on the practical rationale for using a non-physical $\\pm 90^\\circ$ tilt range in CryoEngine? Specifically, how does this choice impact the model's learned ability to handle the actual missing wedge present in real-world data (e.g., $\\pm 60^\\circ$ data)? Does training on $\\pm 90^\\circ$ result in features that are less robust to the information loss associated with a constrained tilt range?\n\n### Simulation Realism (DVC) \nThe simulation of density volumes using simple isotropic Gaussian kernels and linear normalization seems quite idealized. Have the authors investigated the impact of more realistic noise models (e.g., non-Gaussian, anisotropic, or structured noise) or more complex CTF modeling? Do the authors believe the current DVC pipeline is sufficient to close the simulation-to-real gap, or are there plans to integrate higher-fidelity physical simulation into CryoEngine?\n\n### Extrapolation to New Architectures/Scenes\nGiven that the pre-training is dominated by two symmetrical, well-defined particles, what is the authors' hypothesis regarding the model's performance on highly asymmetric proteins, membrane proteins, or filamentous structures that require a different context scale? Could the authors include qualitative results or an analysis of feature representations on a highly distinct particle class (if available)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I have no ethical concerns regarding this work. The paper describes a computational methodology for structural biology, and the data utilized is either synthetically generated or derived from publicly available sources."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PFfIKmhrUK", "forum": "vwVp5y2Ovg", "replyto": "vwVp5y2Ovg", "signatures": ["ICLR.cc/2026/Conference/Submission4149/Reviewer_uTSh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4149/Reviewer_uTSh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762239628535, "cdate": 1762239628535, "tmdate": 1762917198521, "mdate": 1762917198521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}