{"id": "NChBLvOr7I", "number": 23392, "cdate": 1758343088120, "mdate": 1759896817195, "content": {"title": "VERIFY: A Novel Multi-Domain Dataset Grounding LTL in Contextual Natural Language via Provable Intermediate Logic", "abstract": "Bridging the gap between the formal precision of system specifications and the nuances of human language is critical for reliable engineering, robotics, and AI safety, but it remains a major bottleneck. Prior efforts in grounding formal logic remain fragmented, resulting in datasets that are very small-scale (~2-5k examples), domain-specific, or translate logic into overly technical forms rather than context-rich natural language (NL). Thus, failing to adequately bridge formal methods and practical NLP. To address this gap, we introduce VERIFY, the first large-scale dataset meticulously designed to unify these elements. This dataset contains more than 200k+ rigorously generated triplets, each comprising a Linear Temporal Logic (LTL) formula, a structured, human-readable 'Intermediate Technical Language' (ITL) representation designed as a bridge between logic and text, and a domain-specific NL description contextualized across 13 diverse domains. VERIFY's construction pipeline ensures high fidelity: LTL formulas are enumerated and verified via model checking, mapped to the novel ITL representation using a provably complete formal grammar, and then translated into context-aware NL via LLM-driven generation. We guarantee data quality through extensive validation protocols, i.e., manual expert verification of 10,000 diverse samples. Furthermore, automated semantic consistency checks judged by Llama 3.3 confirmed an estimated >97% semantic correctness. From the initial experiments, we demonstrate VERIFY's scalability, logical complexity, and contextual diversity, significantly challenging standard models such as T5 and Llama 3.", "tldr": "", "keywords": ["LLM", "Linear Temporal Logic", "model checking", "formal verification"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f425624adbd535c7fc7ca951f793e83970cb8782.pdf", "supplementary_material": "/attachment/954c10f9829a35709459de5d7c25e647211b0ae2.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce VERIFY, a dataset of 200k+ samples of (Natural Language/NL, Intermediate Technical Language/ITL, Linear Temporal Logic/LTL) triples spanning 13 domains. The stated goal is to support both training and benchmarking for translation among natural language (NL), a proposed Intermediate Technical Language (ITL), and Linear Temporal Logic (LTL). ITL is positioned as a human-readable, structurally faithful layer over LTL intended to ease learning and generation. The dataset is accompanied by formal checks on the logic side and mixed human/LLM-as-judge validation of the NL side.\n\nSoundness:\nThe pipeline for generating and validating LTL appears methodical, and the overall dataset construction is described with reasonable care. However, the central claim that ITL is useful as an intermediate representation is insufficiently substantiated by experiments. The empirical results do not isolate an end-to-end utility of ITL in realistic NL <-> LTL workflows; gains are primarily shown in easier sub-tasks (e.g., ITL-> NL vs. LTL-> NL) that may rely on privileged inputs (ground-truth ITL). Heavy reliance on an LLM-as-judge for “semantic correctness” further weakens the evidentiary basis.\n\nPresentation:\nThe paper is generally readable and well organized, but the main body lacks figures, dataset examples, and consolidated statistics that would aid comprehension. Some sentences are ambiguous or grammatically incomplete (e.g., the opening “bottleneck” is not specified; a few fragments/run-ons appear). Bringing key tables/plots from the appendix into the main text and adding a running example would materially improve clarity.\nOccasional fragmented/incomplete/run-on sentences:\n“Thus, failing to adequately bridge formal methods and practical NLP.”\n“Crucially, data quality is guaranteed through extensive validation protocols: manual expert verification of 10,000 random samples and automated semantic consistency and correctness checks judged by an LLM-as-judge approach (using Llama 3.3) (20; 21) across 18% of the dataset, confirming an estimated > 97% semantic integrity.”\n\nContribution:\nA large, multi-domain, logic-grounded dataset is valuable to the community, and proposing an intermediate representation (ITL) is an interesting idea. That said, the paper currently stops short of demonstrating practical benefits from ITL in end-to-end NL<->LTL modeling or user-facing tasks. If such benefits were shown more convincingly, the contribution would be stronger. Furthermore, it is unclear how each domain is defined in the paper. What distinguishes the domains, in terms of available predicates?"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Broad semantic coverage over 13 domains\n2. Size of dataset is significant, offering over 16k entries per domain according to Table C Appendix A.1"}, "weaknesses": {"value": "1. Most significant weakness is the use of Llama 3.3 as a judge of “semantic correctness”\n2. Does not evaluate actual NL-LTL translation frameworks, such as those mentioned in the paper (Lang2LTL, NL2TL)\n3. Merit of ITL to humans is unclear and untested. One experiment shows that ITL->NL translation is more feasible than LTL->NL translation, however\n4. Examples from the dataset are missing in the main body of the paper\n5. No figures present in the main body of the paper\n6. Dataset statistics not present in the main body of the paper. Table a in appendix A.1 showing the distribution of LTL formula complexity in the VERIFY dataset seems to indicate that there are more than 1.3 million LTL expressions in the dataset, which contradicts the reported ~200k samples.\n7. Does not compare against any existing NL-LTL benchmarks"}, "questions": {"value": "1. Why are there no NL-LTL translation frameworks evaluated?\n2. What is Llama 3.3 evaluating? Semantic correctness of ITL? If so, is that grammar not “provably complete” with LTL?\n3. In Experiment A, performance is compared between the LTL->NL and ITL->NL tasks, with the aim of demonstrating that ITL is a beneficial intermediate representation. However, as the authors note, this requires the intermediate translation of LTL->ITL via a “task 5 model”, but the results in table 3 show that we can not reliably obtain a correct ITL translation from LTL. Are ITL->NL models in Experiment A given ground-truth ITL? If so, can’t we assume the practical performance of the ITL->NL task will be upper-bounded by the LTL->ITL performance, which is ~30%?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "puRrKR0UBn", "forum": "NChBLvOr7I", "replyto": "NChBLvOr7I", "signatures": ["ICLR.cc/2026/Conference/Submission23392/Reviewer_82kz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23392/Reviewer_82kz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761766143807, "cdate": 1761766143807, "tmdate": 1762942640822, "mdate": 1762942640822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VERIFY, a large-scale dataset designed to bridge Linear Temporal Logic (LTL) and Natural Language (NL) through a novel Intermediate Technical Language (ITL). VERIFY supports bi-directional translation tasks between formal logic and natural language across 13 domains, encompassing more than 200k samples. The authors propose deterministic rule-based LTL-to-ITL conversion, LLM-driven natural language generation, and evaluate a series of models (T5, BART, Llama, Mistral, CodeLlama, DeepSeekCoder) on several translation settings. Analytical experiments demonstrate that (1) introducing ITL improves semantic alignment and syntactic correctness, (2) contextual information significantly impacts translation quality, and (3) direct LTL↔NL translation remains challenging even for advanced LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "VERIFY makes a strong contribution through its scale, structure, and experimental scope. It substantially extends prior logic–language datasets by providing over 200k LTL–ITL–NL triplets across 13 diverse realworld domains, enabling credible crossdomain reasoning and generalization studies. The introduction of a formally defined and humanreadable Intermediate Technical Language (ITL)—supported by a provably complete mapping—offers a principled and effective bridge between symbolic logic and contextual natural language, mitigating the brittleness of direct LTL <-> NL translation. Extensive experiments across multiple translation directions, model families, and ablation settings further underscore VERIFY’s diagnostic value and robustness as a comprehensive benchmark for logical–linguistic understanding."}, "weaknesses": {"value": "The positioning of VERIFY relative to existing logic/NLP resources is somewhat limited—comparative benchmarks or perdomain statistics are lacking, making it difficult to precisely gauge coverage and novelty. The validation procedure, while methodically designed, only samples a portion of the dataset (≈18 % LLMchecked and 5 % manually reviewed) and does not analyze typical failure mechanisms, such as which operators most often induce semantic drift. Moreover, syntactic correctness for NL→LTL/ITL generation remains low (≈65–75 %), indicating that current models frequently output invalid structures; the paper would benefit from a qualitative error analysis with representative examples. Several metrics are underexplained, which limits interpretability of the reported numbers. Addressing these issues—particularly through an expanded baseline comparison and a concise errortype study—would significantly strengthen the paper’s empirical credibility and clarity"}, "questions": {"value": "- How consistent are the LLM‑Judge and manual verification outcomes across domains? Were there specific formula templates or temporal relations where semantic drift was most common? \n- Would you consider extending VERIFY beyond LTL to other logic families (e.g., CTL, MTL, or probabilistic temporal logics)? What challenges would arise in mapping or annotation consistency?\n- Could you provide explicit examples where the ITL→LTL mapping fails or encounters ambiguity in practical generation settings? Are there specific formula classes where ITL and LTL may diverge under real-world (noisy or partial) ITL?\n- The paper lacks a visual diagram of the overall VERIFY pipeline. Could you provide a figure summarizing the key components and information flow (dataset generation, ITL conversion, validation, and experiments) to enhance reproducibility and readability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rzzeM2vfQy", "forum": "NChBLvOr7I", "replyto": "NChBLvOr7I", "signatures": ["ICLR.cc/2026/Conference/Submission23392/Reviewer_svaN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23392/Reviewer_svaN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994835524, "cdate": 1761994835524, "tmdate": 1762942640467, "mdate": 1762942640467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VERIFY, a large-scale dataset containing over 200,000 triplets of Linear Temporal Logic (LTL) formulas, an intermediate technical language (ITL), and contextual natural language descriptions across 13 diverse domains. The key contribution is a novel ITL representation that bridges the structural patterns of LTL with natural-language syntax, along with a provably complete formal grammar for LTL-to-ITL translation. The natural language is generated using LLMs (DeepSeek-R1) and validated through manual inspection (10,000 samples) and automated LLM-as-judge evaluation (18% of the dataset). While the dataset represents a significant contribution in scale and domain diversity compared to prior work, it has critical limitations: heavy reliance on LLM generation without human-written examples, lack of grounding into executable state spaces, absence of verification traces, and missing analysis of whether the ITL intermediate layer actually provides measurable benefits over direct LTL-NL translation in practice."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A) With 200k+ examples across 13 diverse domains (financial services, aerospace, medical devices, home automation, etc.), VERIFY significantly exceeds prior datasets in both size and breadth. This addresses a major limitation identified in related work, where datasets were confined to narrow domains (robotics, navigation) with only 2-5k examples.\n\nB) The introduction of Intermediate Technical Language as a structured bridge between LTL and NL is conceptually sound. The provably complete LTL-to-ITL translation grammar (Appendix C) with formal verification using Spot provides mathematical rigor often missing in NL-to-logic datasets."}, "weaknesses": {"value": "A) There is an over-reliance on LLM generation with insufficient human validation. This is my biggest concern with this work. The entire natural language component is generated by DeepSeek-R1 LLM with no human-written seed examples. Only 10,000 samples (4.8% of dataset) received manual expert verification by the authors themselves. The 18% LLM-as-judge validation using Llama 3.3 is problematic because LLMs judging other LLMs may share similar failure modes and biases, no inter-rater reliability metrics were provided for the manual 10k sample review, and there is no comparison with independent human annotators outside the author team. Hence, the >97% \"semantic correctness\" claim is based on an LLM's assessment, not human judgment.\n\nB) THere is an insufficient demonstration of ITL's practical value.\nThis is important as ITL is introduced as a key contribution, but its practical benefits remain unclear. The Evidence from Experiment A is weak as can can be seen in Figure 1. It shows ITL->NL achieves slightly better scores than LTL->NL (e.g., Llama-3: 0.94 vs 0.91 BERTScore), but the improvement is modest (~3 percentage points), and no statistical significance testing is provided. The only major difference comes for T5.\n\nC) The paper's title claims to \"ground\" LTL but provides no mechanism for verifying that translations are correct. Unlike some of the contemporary works like VLTL-Bench [1], which includes satisfying and unsatisfying execution traces for each formula, VERIFY has none. This means. that there is no way to validate semantic correctness beyond string matching, we cannot use traces as a supervision signal for training, and we cannot demonstrate that NL translations capture correct temporal semantics.\n\n[1] English et al., Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite. https://arxiv.org/abs/2507.00877\n\nD) The paper doesn't address whether evaluation LLMs (Llama-3, Mistral) have seen LTL-NL pairs in pretraining. Given that Llama-3 was released in 2024 and trained on data through 2023, and prior LTL-NL datasets are public, contamination is plausible.\n\nE) What happens with different LLM generators (GPT-4, Claude)? Is dataset quality dependent on DeepSeek-R1's specific capabilities?\n\n\nMinor Issue:\n* Page 2, Line 54: \"syntactucally divergent expresession\" - should be \"syntactically divergent expressions\""}, "questions": {"value": "1. How did you validate that Llama 3.3's judgments align with human expert assessments? Can you report agreement statistics between the LLM judge and your manual 10k review?\n\n2. Can you provide stronger evidence that ITL is useful beyond the modest BERTScore improvements? For instance, does a full LTL->ITL->NL pipeline (with learned LTL->ITL) outperform direct LTL->NL?\n\n3. The paper claims to \"ground\" LTL but only provides linguistic context. Can you clarify the distinction and explain why executable grounding wasn't included? Or why it is not necessary for \"grounding\" evaluation?\n\n4. None of the experimental results include error bars or significance tests. What is the variance across different random seeds or data splits?\n\n5. What was the computational cost and time for generating 200k examples with DeepSeek-R1? Can others reproduce this with open models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tO7Qj2Ok9j", "forum": "NChBLvOr7I", "replyto": "NChBLvOr7I", "signatures": ["ICLR.cc/2026/Conference/Submission23392/Reviewer_UD8h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23392/Reviewer_UD8h"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762097396745, "cdate": 1762097396745, "tmdate": 1762942640295, "mdate": 1762942640295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The research focuses on the **VERIFY** and dataset, which integrate Linear Temporal Logic (LTL) with natural language descriptions, using an Intermediate Representation Language (ITL).  The dataset links formal logic with contextual information, enabling better understanding and translation between logical formulas and natural language.\n\nKey findings include:\n- Translation tasks from LTL to NL are more straightforward than the reverse, as evidenced by higher performance scores for LTL-->NL compared to NL-->LTL.\n- ITL as an intermediate step significantly improves performance in generating natural language from complex LTL expressions.\n- Domain and activity context are critical for accurate translations from NL to formal logic; models without this grounding show substantial performance degradation.\n\nThe study highlights:\n1. The benefits of using ITL as an intermediary in translation tasks (e.g., LTL-->ITL-->NL outperforms direct LTL-->NL).\n2. The importance of contextual information in achieving accurate translations from natural language to formal logic.\n3. The relative difficulty of translating NL to LTL compared to generating NL from formal representations.\n4. Model strengths: fine-tuned models excel at generating natural language, while code-focused models demonstrate superior syntactic correctness in logic generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "It provides an integration of domain-specific NL with temporal logic via a novel canonical form (ITL). The focus on contextual grounding is also fresh compared to prior static datasets. \n\nMulti-model testing, robust metrics (BERTScore, semantic equivalence), human evals, and ablation studies proving ITL/context impact. \n\nVERIFY’s resources enable future work; insights on model strengths (e.g., code LLMs for logic generation) offer practical \nguidance. It pushes boundaries beyond NLP into formal methods accessibility"}, "weaknesses": {"value": "The paper clearly establishes that translating from NL to LTL is much harder than generation (that somehow expected), the *reasons* for this gap could be explored more deeply.\n\nWhile human evaluation was performed, its scope might not fully capture the challenge.\n\nThe paper uses models like T5-large and Llama-3-8B-instruct. Larger foundation models (e.g., GPT-4 class) might yield different performance characteristics.\n\nMissing References to relevant related work: For example,  He, J. at al. DeepSTL - From English Requirements to Signal Temporal Logic. ICSE 2022: 610-622"}, "questions": {"value": "Your results clearly show NL-->LTL translation is extremely challenging, even for fine-tuned models. Could you provide a deeper error analysis?\n\nWhat are the most frequent categories of errors in generated LTL formulas (e.g., wrong temporal operator (`F` vs `G`, `U` scope), incorrect proposition handling, negation/implication mistakes)? Quantifying this breakdown would be highly valuable.\n\nDoes using ITL as an intermediate representation primarily help avoid syntactic/semantic parsing ambiguities leading to invalid syntax, or does it demonstrably reduce the incidence of these deeper *logical* errors? If so, how?\n\nContext (\"Domain\" and \"Activity\") is shown to be crucial. How well do models generalize when encountering completely unseen Domains or Activities at test time?\n \nHave you conducted any experiments in a zero-shot setting where the Domain/Activity combination present during testing was absent from training? What are the performance drops  compared to seen contexts?\nIs there evidence that representations learned for similar Domains/Activities transfer effectively, or is context primarily memorized?\n\nHuman evaluation of semantic equivalence is critical but subjective.\nCan you report the Inter-Annotator Agreement (IAA) score (e.g., Cohen's Kappa) for your human assessments? \n\nFor cases judged as semantically equivalent by non-experts, were these formulas also verified to be logically sound/correct representations of the system requirement (perhaps spot-checked by a formal methods expert)? Is there any risk that \"semantically equivalent\" judgments might accept plausible but subtly incorrect LTL?\n\n Question: You show strong results for ITL in NL generation using T5-large and significant challenges with NL-->LTL on Llama-3-8B-instruct. Have you tested the same NL-->LTL task (especially fine-tuned) using significantly larger LLMs (e.g., GPT-4-class models like Claude Opus or Gemini Ultra, even via API)? Does scaling primarily improve syntactic correctness, semantic equivalence, or both? Do the core challenges identified persist? If not run due to cost/scope, do you hypothesize that these larger models might close the gap substantially, suggesting it is a data/model-size issue rather than an inherent task difficulty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wTe2oYHdLx", "forum": "NChBLvOr7I", "replyto": "NChBLvOr7I", "signatures": ["ICLR.cc/2026/Conference/Submission23392/Reviewer_5AWc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23392/Reviewer_5AWc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762174761649, "cdate": 1762174761649, "tmdate": 1762942640067, "mdate": 1762942640067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}