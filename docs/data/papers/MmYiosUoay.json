{"id": "MmYiosUoay", "number": 8141, "cdate": 1758069333786, "mdate": 1759897804528, "content": {"title": "Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual Attention for Multimodal LLMs", "abstract": "Recent Multimodal Large Language Models (MLLMs) have demonstrated significant progress in perceiving and reasoning over multimodal inquiries, ushering in a new research era for foundation models.\nHowever, vision-language misalignment in MLLMs has emerged as a critical challenge, where the textual responses generated by these models are not factually aligned with the given text-image inputs.\nExisting efforts to address vision-language misalignment have focused on developing specialized vision-language connectors or leveraging visual instruction tuning from diverse domains.\nIn this paper, we tackle this issue from a fundamental yet unexplored perspective by revisiting the core architecture of MLLMs.\nMost MLLMs are typically built on decoder-only LLMs consisting of a causal attention mechanism, which **limits the ability of the earlier modalities (e.g., images) to incorporate information from the latter modalities (e.g., text)**.\nTo address this problem a MLLM that unlocks causal attention into our proposed modality-mutual attention (MMA) to enable image tokens to attend to text tokens.\nThis simple yet effective design allows MMA to achieve state-of-the-art performance in 12 multimodal understanding benchmarks (**+5.5\\% on average across 2 LLMs backbones**) without introducing additional parameters.\nOur MMA design is intended to be generic, allowing for applications across various modalities, and scalable to accommodate diverse multimodal scenarios.", "tldr": "", "keywords": ["Multimodal Learning; Modality-Mutual Attention; Object Hallucination"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b98afb275551d9fa2e80a3c00cc3520bce147b2.pdf", "supplementary_material": "/attachment/7b1ca9cc8f0551df29d4f4e62c6b214c9a3698a7.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on solving the limitations that the earlier modalities (e.g., images) to incorporate information from the latter modalities (e.g., text).  To address this problem, this paper proposes modality-mutual attention to enable image tokens to attend to text tokens. Experiments have shown that MMA achieves state-of-the-art performance in 12 multimodal understanding benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well motivated.\n2. The proposed DOT method provides a plug-and-play advantage.\n3. The proposed MMA achieves significant improvement on multiple benchmarks."}, "weaknesses": {"value": "1. The paper needs improvement in quality. For instance, clarify the significance of boldface and underline in Tab.1, which appears to differ from their meanings in Tab.2. There are repeated occurrences of underlines in Tab.2 with non-matching scores.\n2. As mentioned in the paper, DOT requires introducing doubled training overhead, as highlighted as a drawback.\n3. The novelty of the proposed method in this paper is limited."}, "questions": {"value": "1.  Incorrect Improvements Result: In Tab.2, based on Phi-3.5-Mini-Instruct, MMA achieves 82.7 in POPE. The performance improvement should be (82.7-82.2)/82.2=0.6% instead of 1%.\n2. It would be more convincing to validate the proposed method based on other widely used MLLM models (e.g., Qwen2.5VL, LLaVA-OneVision, and InternVL3).\n3. In Tab.3, on nearly half of the benchmarks, AKI-4B's performance is worse than Qwen2-VL-2B's. Authors are requested to provide a reasonable explanation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "20tHSUNIsw", "forum": "MmYiosUoay", "replyto": "MmYiosUoay", "signatures": ["ICLR.cc/2026/Conference/Submission8141/Reviewer_owr3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8141/Reviewer_owr3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761210914640, "cdate": 1761210914640, "tmdate": 1762920111712, "mdate": 1762920111712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical issue of vision-language misalignment in Multimodal Large Language Models (MLLMs), where generated textual responses are not factually grounded in the given image-text inputs, leading to object hallucinations. The proposed method, Modality-Mutual Attention (MMA), revisits the core architecture of MLLMs by fundamentally modifying the causal attention mechanism in decoder-only LLMs. It unlocks the attention mask to allow image tokens to actively attend and incorporate information from subsequent text tokens, thereby enabling dynamic, query-aware visual understanding. The authors also explore an intuitive Dual-Order Training approach but demonstrate that MMA is a more efficient and effective solution. A key advantage is that MMA achieves this without introducing any additional parameters or increasing computational costs. Extensive experiments show that the proposed method significantly outperforms strong baselines and state-of-the-art approaches across 12 diverse multimodal benchmarks, demonstrating improved performance particularly on vision-centric tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Fundamental and Well-Motivated Problem: The paper identifies a fundamental architectural limitation of MLLMs as a root cause of hallucination.\n2) Efficient Solution: MMA requires no additional parameters, introduces no computational overhead, and can be seamlessly integrated into existing training pipelines, making it highly attractive for both research and deployment.\n3) Extensive and Convincing Empirical Validation: The method is thoroughly evaluated on 12 benchmarks using two LLM backbones, demonstrating consistent improvements, particularly on vision-centric tasks such as POPE and CV-Bench."}, "weaknesses": {"value": "1) Limited Analysis of MMA's Inner Workings: A rigorous quantitative analysis of the cross-modal attention patterns is missing. For instance: How does the attention distribution from image tokens to text tokens evolve during training? Is there a consistent pattern (e.g., nouns, spatial relations) that image tokens learn to prioritize? This lack of in-depth analysis leaves the \"how\" of the performance gain somewhat as a black box.\n2) Lack of Novelty in Core Concept: The fundamental innovation is modest. As discussed, the use of non-causal, modality-aware attention masks is a well-established technique in multimodal learning. The paper does not adequately situate itself within this existing literature, creating an illusion of greater novelty than it possesses.\n3) Insufficient Exploration of Broader Applicability: The paper focuses on single image-text pairs. A key claim is that MMA is \"generic and scalable\", yet there is no empirical demonstration on interleaved multimodal data (image-text-image), multi-image inputs, or other modalities (e.g., audio)."}, "questions": {"value": "1) How does your work fundamentally differ from prior efforts that use non-causal or custom attention masks for multimodal modeling? Please clarify the specific novelty of the \"unlocking\" concept beyond its application to a new model class (decoder-only MLLMs).\n2) Have you identified any input conditions or task types where MMA fails to provide an improvement, or even degrades performance? For example, in scenarios with very verbose text or when the text contains substantial irrelevant information, could the additional cross-modal connections introduce noise? Discussing the limitations and failure modes would provide a more balanced view of the method's utility."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4ydfIeTT8Q", "forum": "MmYiosUoay", "replyto": "MmYiosUoay", "signatures": ["ICLR.cc/2026/Conference/Submission8141/Reviewer_Mhv4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8141/Reviewer_Mhv4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761615767138, "cdate": 1761615767138, "tmdate": 1762920111234, "mdate": 1762920111234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the core problem of \"visual-linguistic misalignment\" in multimodal large language models (MLLMs). It cuts into the causal attention mechanism at the bottom of the LLM architecture, and proposes a \"modal mutual attention (MMA) \" design. By modifying the attention mask, the image token can achieve cross-modal attention to the text token without adding additional parameters and computational costs. The research problem is accurately located (directly hitting the illusion pain points of current MLLMs), the method is concise and innovative (circumventing the limitations of traditional data/connector optimization), the experimental design is rigorous (covering 12 multimodal benchmarks + 2 model backbones), and the results are convincing (the average improvement is 5.5%, and the visual center task is significantly improved)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Make clear the core limitation of traditional MLLMs - the \"decoder causal attention\" -based input order (image token before, text token after) results in \"forward modal (image) cannot obtain backward modal (text) information\", which in turn elicits the illusion of visually central tasks.\n2. The method design is simple and practical. First verify the necessity of \"cross-modal information flow\" through \"double-order training (DOT) \" (I & T and T & I two input sequential training), and then propose an MMA mechanism for the defect of \"doubling the training cost\" of DOT - only modify the attention mask (formula 5-6), allow the image token to focus on the text token, and still maintain the autoregressive characteristics when generating the response, taking into account \"validity\" and \"lightweight\". MMA does not increase parameters, does not increase computing costs (the amount of mask computation is consistent with the cause and effect of attention), and can be seamlessly integrated into existing MLLM training frameworks (only need to adjust the attention mask in the SFT stage), with a low threshold for landing and strong engineering practicality.\n3. Twelve benchmarks cover three types of tasks: \"General (MME, MMBench), Knowledge (MMMU, MathVista), Vision Center (POPE, CV-Bench) \", and two model backbones (Phi-3.5-Mini, LLaMA-3.2-3 B) verify generalization and avoid the chance of \"single model/benchmark\"."}, "weaknesses": {"value": "1. The paper mentions \"the use of standard causal attention in generating response TR\", but does not explicitly state \"whether modifying the attention mask at the input stage indirectly affects the autoregressive consistency of the generation process\" (e.g. whether there is modal confusion during generation). It is recommended to supplement the qualitative results of generative tasks (e.g. visual captioning) to verify that MMA does not compromise the autoregressive generation quality.\n2. The DOT performance in Table 2 is lower than the traditional causal attention on some benchmarks (such as MMB, SEED I) (such as the DOT of Phi-3.5-Mini is only 43.8 in MMB, lower than the traditional 64.9). The paper interprets it as \"different input order confusion model\", but does not rule out the influence of \"insufficient training steps\". It is recommended to supplement the DOT ablation experiment under \"longer training steps\" to verify whether the performance fluctuates due to insufficient training. If there is still no improvement, the specific mechanism of \"input order confusion\" needs to be further analyzed."}, "questions": {"value": "The \"RR (Relation Reasoning) \" column value of \"AKI-4B\" in Table 9 is 0.65, which is significantly lower than that of other models (such as MMA is 67.8). It is speculated to be a clerical error (should be about 65.0). It needs to check the original data source and correct it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HpC2nMc2SQ", "forum": "MmYiosUoay", "replyto": "MmYiosUoay", "signatures": ["ICLR.cc/2026/Conference/Submission8141/Reviewer_xSEK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8141/Reviewer_xSEK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820298171, "cdate": 1761820298171, "tmdate": 1762920110615, "mdate": 1762920110615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Modality-Mutual Attention (MMA), a simple yet effective architecture for multimodal large language models (MLLMs). MMA unlocks the attention pathway from image tokens to text tokens during supervised fine-tuning, overcoming a key limitation of existing decoder-based models. MMA achieves a 5.5% average improvement across 12 benchmarks without adding parameters, demonstrates strong extensibility across model backbones, and provides transparent analysis and open-source results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The approach of simply modifying the attention mask is both elegant and effective, directly tackling a core practical issue in VLMs. Its simplicity makes it highly exploratory and extensible, opening new avenues for deeper multimodal research.\n\n- The paper provides thoughtful analyses and visualizations to interpret the underlying attention mechanisms, supported by a relatively rich set of experimental evidence."}, "weaknesses": {"value": "- While the paper presents a general masking formula for interleaved input, all experiments are conducted under the \"single-image + single-text\" setting. In benchmarks like CV-Bench and MMMU, samples with interleaved inputs are simplified to only the first image, leaving the robustness of MMA in multi-image, long-sequence, or multi-turn dialog scenarios unaddressed—these are currently key challenges for post-training VLMs.\n\n- MMA is effective only during supervised fine-tuning, and cannot be safely applied in the pretraining phase, which limits its broader impact and adaptability to foundational models. Existing experiments are performed only on small-scale models, and ablation studies show MMA's improvement in vision-knowledge mixed tasks is limited.\n\n- The explainability of the method is underdeveloped. The paper lacks a granular analysis of hallucination types and does not provide thorough theoretical derivations."}, "questions": {"value": "- Results for Multi-image/Interleaved Sequences:\nCould the authors provide quantitative metrics and attention visualizations for MMA applied to truly multi-image inputs (e.g., interleaving 4–8 images)? As sequence length increases, is there evidence of attention dilution or gradient instability?\n\n- Relationship Between Mask Sparsity and Performance:\nIf the proportion of text tokens accessible to each image token is gradually increased (e.g., 10%, 30%, 50%, 100%), how does the performance curve behave? Is there a risk of “over-attention” where text tokens dominate and visual features become suppressed?\n\n- Scalability in Large-scale Training:\nAs multimodal training scales to tens of billions of samples, does MMA’s advantage diminish? Are there experiments or analyses on this phenomenon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yv7UtQz8We", "forum": "MmYiosUoay", "replyto": "MmYiosUoay", "signatures": ["ICLR.cc/2026/Conference/Submission8141/Reviewer_VKts"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8141/Reviewer_VKts"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8141/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925583851, "cdate": 1761925583851, "tmdate": 1762920110256, "mdate": 1762920110256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}