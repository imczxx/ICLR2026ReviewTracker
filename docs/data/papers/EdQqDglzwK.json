{"id": "EdQqDglzwK", "number": 11970, "cdate": 1758204931402, "mdate": 1759897541852, "content": {"title": "High-Power Training Data Identification with Provable Statistical Guarantees", "abstract": "Identifying training data within large-scale models is critical for copyright litigation, privacy auditing, and ensuring fair evaluation. The conventional approaches treat it as a simple binary classification task without statistical guarantees. A recent approach is designed to control the false discovery rate (FDR), but its guarantees rely on strong, easily violated assumptions. In this paper, we introduce Provable Training Data Identification (PTDI), a rigorous method that identifies a set of training data with strict false discovery rate (FDR) control. \nIn particular, our method computes p-values for each data point using a set of known unseen data, and then constructs a conservative estimator for the data usage proportion of the test set, which allows us to scale these p-values. Our approach then selects the final set of training data by identifying all points whose scaled p-values fall below a data-dependent threshold. This entire procedure enables the discovery of training data with provable, strict FDR control and significantly boosted power. Extensive experiments across a wide range of models (LLMs and VLMs) and datasets demonstrate that \\abbr{} strictly controls the FDR and achieves higher power.", "tldr": "", "keywords": ["training data detection", "data contimination", "membership inference attack", "copyright infringement"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7c4c1e3f8f977bc14730ff257ea9d0840bc4c559.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Provable Training Data Identification (PTDI), a statistical framework for identifying whether a sample was part of a model’s training data while controlling the false discovery rate (FDR) under finite-sample guarantees. The method leverages conformal prediction to compute $p$-values using a calibration set of known non-members and applies the Benjamini–Hochberg procedure to select positives with provable statistical control. PTDI is evaluated on LLMs and VLMs, showing stronger identification power than heuristic membership inference baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a clear and principled statistical framework that brings formal guarantees to training data identification, addressing a key gap in existing heuristic or ad hoc auditing methods.\n\n- The integration of conformal $p$-value calibration with FDR control is elegant and theoretically sound, and the approach demonstrates consistent empirical performance across both LLMs and VLMs."}, "weaknesses": {"value": "- The method’s theoretical guarantees, including valid conformal p-values and provable FDR control, hinge on the calibration non-members being exchangeable with test non-members. In practice, this assumption is unverifiable and unlikely to hold in realistic auditing scenarios with heterogeneous or evolving data distributions. The authors acknowledge this limitation, but it substantially weakens the “provable” claim in real-world applications.\n\n- The paper’s most powerful variant, the adjusted moment estimator, requires access to a calibration set of confirmed members and non-members. This assumption is circular in the intended auditing context, where the goal is to discover unknown members. This makes the paper's best-performing method practically infeasible for its primary use case."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jh7IdC9SZU", "forum": "EdQqDglzwK", "replyto": "EdQqDglzwK", "signatures": ["ICLR.cc/2026/Conference/Submission11970/Reviewer_jywB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11970/Reviewer_jywB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761449096236, "cdate": 1761449096236, "tmdate": 1762922967963, "mdate": 1762922967963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for training-data identification that constructs p-values via conformal inference using a post-cutoff non-member population set. The procedure can be plugged into existing MIA scores, aiming to provide finer-grained data-provenance information than a binary decision. The method relies on the assumption that data points are i.i.d. samples from a unified population distribution, meaning each point has the same probability of being selected into training and the same MIA error characteristics (i.e., identical TPR/FPR across samples). With a non-member calibration set (under the same i.i.d. assumptions), the method can control the FDR, which the paper defines as the expectation of 1-precision. Experiments on vision-language and language domains show its applicability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Problem motivation is meaningful. The paper studies data provenance/auditing questions in the context of large models, which is relevant for practical concerns such as copyright verification.\n2. The experiments cover both language and vision–language models, and evaluate across multiple widely-used (MIA) signals (e.g., perplexity, MIN-K%, Rényi entropy, knockoff-based score), showing its realistic applicability.\n3. The paper is clearly written. The method is straightforward to understand and principled."}, "weaknesses": {"value": "I will likely increase my score if the necessary experiments and explanations are added.\n1. **Key assumptions are less realistic, especially in the data provenance problem discussed here.** The core idea of this method is to apply scaled p-values in conformal inference, with the scaling term estimated via data usage proportion. However, Eq.10 in Sec.3.2 relies on two strong assumptions: (i) identical detection behavior (e.g., MIA errors) across samples, i.e., $P(t|M=1)=P(t|M_i=1)$ for all data $i$, and (ii) i.i.d. data sampling $P(M_i=1)=\\pi_{\\text{test}}$. In practice, high-value/copyrighted data will not be sampled uniformly, and detection scores (e.g., MIA signals) are known to be heterogeneous across samples (different memorization/detectability). Therefore, (1) these assumptions should be clearly stated and justified, and (2) experiments with intentionally biased sampling should be included to evaluate robustness.\n2. **Extremely low power when the target FDR is small.** In Figure 1, when the target FDR is low, empirical FDR stays below the target, but power becomes extremely low. This corresponds to cases like: there are 100 test samples with 99 true training points, yet the method only identifies *one* of them (so FDR=0) and misses the rest (so Power = 1/99). Is this an intended behavior? It would be more informative to report a true set-recovery metric (e.g., Hamming distance between the ground-truth membership vector and the predicted one)."}, "questions": {"value": "- What is the connection to DUCI [1]?\n\nSec. 3.2 seems closely related to a prior work DUCI, which also integrates with arbitrary detection scores. For example, Eq. 10 appears adapted from Eq. 5 in DUCI, the following integral here matches the expectation there, and the core result used in Theorem 1 $\\mathbb{E}[\\frac{1-\\pi_{test}}{1-\\hat \\pi_{sub}}] \\leq 1$ is the unbiasedness $\\mathbb{E}[\\hat \\pi_{sub} - \\pi_{test}]\\le 0$ in DUCI. If I understand correctly, the main difference is that Eq. 10 assumes homogeneous sampling probability/MIA bias, which allows integration directly over the data distribution (removing the reference-model dimension)? It would be helpful if the authors could explain the differences, but extending the estimator from statistical proportion estimation to set prediction is already a useful contribution.\n\n- Minor: Line 131: Missing period after “candidate training samples”\n\n[1] How much of my dataset did you use? Quantitative Data Usage Inference in Machine Learning. ICLR’25"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k0uN04ABIz", "forum": "EdQqDglzwK", "replyto": "EdQqDglzwK", "signatures": ["ICLR.cc/2026/Conference/Submission11970/Reviewer_GF9Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11970/Reviewer_GF9Q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916138624, "cdate": 1761916138624, "tmdate": 1762922966997, "mdate": 1762922966997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a scaled p-value transformation that given a calibration set with a small number of ground-truth non-member data, provably transforms any existing MIA score to another MIA that is calibrated (in terms of its FPR on a test set). Experiments on a wide range of pretrained and fine-tuned language model and visual language models show that the transformation consistently makes the derived MIA score calibrated, while also significantly improves the true positive rate of the attack on test set."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The derivations of the scaled p-value is novel to the best of my knowledge. Experiments confirm the strong calibration, and even show improving MIA performance across benchmarks, showcasing the effectiveness of the proposed method."}, "weaknesses": {"value": "- The reason for the improved MIA power when applying the proposed scaled p-value transformation not discussed. Section 3.2 only discusses why the derived estimator is calibrated, but did not discuss why it may improve the power of the attack as well. I personally find it very surprising as, if $\\hat{\\pi}_{test}$ is a test-point independent quantity, then scaled p-value is just a monotonic transformation of the p-values and should not improve attack performance. So the key to improvement appears to be the dependence of the subtraction estimator (12) on the test p-values. I hope the authors could provide more explanations to help understand the reason for the improvement, which to my understanding is the key contribution.\n- The authors didn't evaluate any reference model-based MIA baselines. Even in the pertaining setting, it is common to use an off-the-shelf smaller model as reference model ((Carlini et al., 2021), and it would be more convincing to understand if the author's method also improves performance for those stronger MIAs. \n- Improved power is only shown for the pretraining setting (Table 1), where the benchmark has temporal shift that may overestimate MIA performance. It would be more convincing if the authors could discuss whether MIA power also improves under scaled p-values for fine-tuning setting."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xe4Lh6I1GV", "forum": "EdQqDglzwK", "replyto": "EdQqDglzwK", "signatures": ["ICLR.cc/2026/Conference/Submission11970/Reviewer_NNQR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11970/Reviewer_NNQR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971565177, "cdate": 1761971565177, "tmdate": 1762922966475, "mdate": 1762922966475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work is concerned with identifying training data. As has become a recent popular trend, valid conformal p-values are computed using score functions, but before BH is applied, the p-values are scaled via an estimation of the proportion of non-training data in the test set (equiv., the proportion of \"nulls\", in the language of multiple testing). \n\nA theorem is presented that establishes FDR control, and experiments are conducted to verify this as well as to support the claim that higher power is achieved when the scaling is conducted as oppose to when no scaling is conducted."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Many experiments were conducted to compare the performance of PTDI under various scoring methods, $\\alpha$ parameter settings, and on varying datasets."}, "weaknesses": {"value": "- The PTDI algorithm essentially combines conformal p-values with a scaling that is equivalently Storey's modification of the BH algorithm (see \"A direct approach to false discovery rates,\" Storey 2002). As Storey's modification (equivalently, the scaling) was proposed to enhance the power of BH without disrupting its FDR control guarantee, it seems like a clear and natural suggestion to apply this idea to the problem of identifying training data. In other words, the central thesis to this paper is far from novel, and the same applies to their theoretical statistical guarantee for FDR in Theorem 1. And just as in Storey 2002, there isn't a concrete theoretical statement rigorously establishing that higher power is achieved with the Storey modification. \n\n- The comparison in power between PTDI versus KTD is not experimented on as extensively as FDR control. There is but one experiment, the results of which are posted in Figure 3, and I'm afraid they do not make a strong case that PTDI is certifiably a higher power method. \n\n- The Related Works section is a bit sparse. For instance, I think the authors should definitely consider citing \"A direct approach to false discovery rates,\" Storey 2002, considering their scaling of their conformal p-values appears to be precisely mirroring Storey's modification of BH. As well, the authors are leaving out some older sources that could be credited with the source of conformal p-values, like Vovk, V., Gammerman, A., & Shafer, G. (1999) and Vovk, V., Nouretdinov, I., & Gammerman, A. (2003)."}, "questions": {"value": "- The PTDI algorithm appears roughly to compute conformal p-values and then apply Storey's modification of the BH algorithm (see \"A direct approach to false discovery rates,\" Storey 2002). Would the authors confirm/clarify the relationship between PTDI and Storey's?\n\n- The paragraph starting on line 041 mentions a handful of recent studies for identifying training data. Can the authors discuss how this paragraph adequately positions your work within the field?\n\n- The comparison in power between PTDI versus KTD is not experimented on as extensively as FDR control. There is but one experiment, the results of which are posted in Figure 3, and I'm afraid they do not make a strong case that PTDI is certifiably a higher power method. Could the authors provide any (more) justification for the claim that PTDI is a higher power method? \n\n- $D_{cal}$ and $D_{test}$ composition: Section E.1 Data Split Setup inadequately details the composition and procedure for curating $D_{cal}$ and $D_{test}$for all datasets employed (WikiMIA, ArxivTection, XSum, BBC Real Time, VL-MIA/Flickr and VL-MIA/DALL-E). Can the authors please provide greater details in this regard? What are the sizes of all of the datasets? Prior to splitting a dataset in half, what percentage of the dataset consists of members/non-members? What are the expected sizes of $D_{cal}$ and $D_{test}$ after splitting a dataset in half?\n\n- A claim is made that PTDI can be readily combined with existing detection methods in both black-box and white-box settings only requiring unseen data as a calibration set. Please confirm whether or not PTDI also needs access to the model outputs on both the calibration and test set as well? Clearly, PTDI was designed for and operates under a \"black-box\" regime. Consequently, It seems trivial to say that a \"black-box\" method also operates under a \"white-box\" regime. Please confirm whether or not PTDI would operate identically under a \"white-box\" regime? Algorithm 1 indicates that access to the model is required. If the white-box/black-box terminology is to be employed, then I would recommend clarifying that only the outputs of the model on $D_{cal}$ and $D_{test}$ are required, not the model itself. I might consider abandoning the use of the terminology white-box/black-box, in favor of simply stating the requirements of PTDI.\n\n- Can the authors comment on why FDR \"levels-off\" at a Target FDR of 0.5 in Figure 2? Why does KTD \"level-off\" sooner than PTDI?  In the event that $D_{test}$ is composed of 50% non-members and 50% members, then a naive detector that simply classifies every test example as a member would achieve an FDR of 0.5.  \n\n- Line 367 The section title \"How does the scaling procedure affect?\" needs and object. An alternative phrasing could be \"Effect of the scaling procedure.\"\n\n- Lines 066-067 percentages are used for FDR. In all other cases percentages are not used for FDR. Recommend picking one format for consistency and clarity.\n\n- Line 1119 subscript \"test\" on Dtest"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dgv1UJVAsS", "forum": "EdQqDglzwK", "replyto": "EdQqDglzwK", "signatures": ["ICLR.cc/2026/Conference/Submission11970/Reviewer_cobt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11970/Reviewer_cobt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994560590, "cdate": 1761994560590, "tmdate": 1762922965955, "mdate": 1762922965955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}