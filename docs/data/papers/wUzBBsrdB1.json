{"id": "wUzBBsrdB1", "number": 9507, "cdate": 1758125351775, "mdate": 1763660954299, "content": {"title": "Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders", "abstract": "Sparse Autoencoders (SAEs) extract features from LLM internal activations, meant to correspond to interpretable concepts. A core SAE training hyperparameter is L0: how many SAE features should fire per token on average. Existing work compares SAE algorithms using sparsity-reconstruction tradeoff plots, implying L0 is a free parameter with no single correct value aside from its effect on reconstruction. In this work we study the effect of L0 on SAEs, and show that if L0 is not set correctly, the SAE fails to disentangle the underlying features of the LLM. If L0 is too low, the SAE will mix correlated features to improve reconstruction. If L0 is too high, the SAE finds degenerate solutions that also mix features. Further, we present a proxy metric that can help guide the search for the correct L0 for an SAE on a given training distribution. We show that our method finds the correct L0 in toy models and coincides with peak sparse probing performance in LLM SAEs. We find that most commonly used SAEs have an L0 that is too low. Our work shows that L0 must be set correctly to train SAEs with correct features.", "tldr": "", "keywords": ["Sparse Autoencoders", "SAEs", "interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/035a5937c6a536c67b5999aa43e53dd3800ba3a4.pdf", "supplementary_material": "/attachment/46d14dc183b4ff853c829f94137823b68db9d449.zip"}, "replies": [{"content": {"summary": {"value": "The author of this paper investigate how the sparsity of a Sparse Autoencoder influences the learned features by the SAE. The authors show in toy models how using a L0 lower than the real L0 can lead to a bad set of learned features, similarly to what happens if the SAE is trained with a very high L0. They show that in toy settings SAEs with lower than the true L0 can 'cheat' and achieve better reconstruction loss. With this in mind they propose metrics to measure if SAEs were trained with too low L0 without know the ground truth labels. They apply this method to LLMs and find that the best L0 for probing is also maximizes their metric."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Understanding what is the correct L0 to use is a very important problem, for the comunity.\nTheir metric seems to capture the real L0 on toy models.\nThis work presents convincing evidence that researchers might have been using too low of L0 when analysing LLMs."}, "weaknesses": {"value": "The presentation of the paper is sightly subpar. Figures could be improved.\nThe paper does not discuss a relevant work 'Interpretability as Compression: Reconsidering SAE: Explanations of Neural Activations with MDL-SAEs'"}, "questions": {"value": "What are the reconstruction values for the SAEs of Figure 5? The caption is a bit confusing. Are these related to the ones shown in Figure 4? Maybe these figures chould be shown together?\nWhat do the authors think is the relationship, if any, between you proposed metrics of finding the optimal L0 and the ones described in 'Interpretability as Compression: Reconsidering SAE Explanations of Neural Activations with MDL-SAEs'?\nWhat is the intuition for selecting the different values of n?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sfk8Chb4ih", "forum": "wUzBBsrdB1", "replyto": "wUzBBsrdB1", "signatures": ["ICLR.cc/2026/Conference/Submission9507/Reviewer_cz9D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9507/Reviewer_cz9D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761230804772, "cdate": 1761230804772, "tmdate": 1762921081500, "mdate": 1762921081500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors state that most SAE users choose the wrong sparsity, impeding the recovery of true model features. This stems from over-reliance on sparsity-reconstruction tradeoff plots during SAE parameter search, most often causing users to end up with SAE L0 that is too low. The authors explore the consequences of SAEs with low L0 relative to true L0 using toy models and find that low L0 causes 1) imperfect recovery of real features but 2) low reconstruction error. They address the mechanism, finding that low L0 SAEs mix features based on feature correlation statistics to achieve better variance explained than ground-truth SAE features. Next, they develop metrics to identify if L0 is less than true L0 in toy models. The core results are extended to real LLMs, and while true L0 is unkown for these systems, the authors’ metrics capture the L0 that results in the best sparse probing performance for LLMs.  Finally, the authors explore the differences between two SOTA SAE architectures, finding that JumpReLU SAEs are more robust to high L0 than BatchTopK SAEs, with both architectures performing similarity at lower than ‘true’ L0. High L0 is also addressed, but results are more mixed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The authors demonstrate a key limitation of standard parameter selection practices used by SAE practitioners: that SAEs with low L0 are often selected based on sparsity-reconstruction tradeoff analyses yet impede true feature recovery.\n\nThey develop two metrics, both capturing when L0 is too low in toy models, validating the results on real data from multiple LLMs. \n\nThey test their metrics on two SOTA SAE architectures.\n\nThe result is a timely, well-tested toolkit for SAE evaluation that can be adopted widely."}, "weaknesses": {"value": "One thing that I really miss in this paper: You are saying that rate-distortion heuristics (like picking the lowest L0 with the lowest MSE) does not give the most disentangled SAE. Instead you are proposing a different heuristic. If that heuristic is better, then you should be able to just take a bunch of pretrained SAEs from SAEbench and show how your heuristic leads to a better SAE across the different metrics. What is stopping you from doing that experiment?\n\nPlease include references and look for overlap with existing literature on estimating the number of sources in ICA (https://www.nature.com/articles/s41598-023-49355-z, https://pmc.ncbi.nlm.nih.gov/articles/PMC6871474/)\n\nPresentation consistency. 1) Would like decoder projection plots for all model types shown 2) Plot high L0 alongside low L0 throughout the main text, particularly since it differentiates the two SAE architectures at some point.\n\nSelection of n: Though explained, selection of n seems somewhat arbitrary at times. Would be more compelling to show the average across n results, fix the main text n value to be h/2 consistently, or show a few n values flanking and including the h/2 value. Relatedly, would add h value to fig caption where n is selected for clarity. \n\nAlternate explanations: Feature frequency could also explain aspects of particularly low L0 performance. The left vs right side of Fig 1 (left) hints at this. Plotting feature recovery as a function of feature frequency would be helpful."}, "questions": {"value": "- Does the degree of correlation influence the degree of mixing?\n\n- Is there more evidence of feature mixing (/absorption) in the low L0 setting but feature splitting in the high L0 setting?\n\n- Can you probe interaction effects (frequency and correlation?) for example, by replicating figs 2, 3 where f0 has relatively high and low frequency? \n\n- In cases where h/2 is not the ‘best’ n, can you add to your metric (ie with a quantitative measure to capture your finding in lower right panel of Fig 10)? If not always, then how often does it work, and in what settings?\n\n- Add plot(s) and/or table(s) showing the SAE resulting from elbow method vs your metric(s)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vNt6ddxtAC", "forum": "wUzBBsrdB1", "replyto": "wUzBBsrdB1", "signatures": ["ICLR.cc/2026/Conference/Submission9507/Reviewer_mo6z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9507/Reviewer_mo6z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935152207, "cdate": 1761935152207, "tmdate": 1762921081137, "mdate": 1762921081137, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the problem of incorrect sparsity target (L0) in Sparse Autoencoders\n(SAEs). It demonstrates that when L0 is set too low or too high, learned latents mix correlated\nand anti-correlated features, losing monosemanticity. Through controlled toy models(Section 3)\nand experiments on LLM activations(Section 4) (Gemma-2-2B, Llama-3.2-1B), the authors show\nthat reconstruction loss can favor incorrect SAEs and propose a diagnostic metric, the nth decoder\nprojection score (s dec n ) defined in Equation 5, for selecting appropriate L0 values."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear identification of a critical issue with sparsity tuning in SAEs.\n2. Strong empirical evidence with both toy models and comprehensive testing of\nLLMs(Gemma-2-2b and Llama-3.2-1b). Specifically, Section 3.3 shows an incorrect,\nfeature-mixing SAE achieving a better MSE (2.73) than the perfectly correct ground-\ntruth SAE (MSE 4.88).\n3. The proposed (s dec n ) metric (Section 3.5) is well-motivated and is shown to be a useful\nproxy for feature correctness.\n4. Insightful analysis showing that reconstruction error can misleadingly favor mixed\nfeatures.\n5. The authors validate their metric on LLMs by showing that the &quot;elbow&quot; of the s dec n\ncurve (the optimal L0) aligns with peak performance on downstream k-sparse\nprobing tasks."}, "weaknesses": {"value": "1. Lacks theoretical grounding, findings are entirely empirical.\n2. Toy models assume orthogonal and linearly separable ground-truth features, which may\nnot represent real LLMs.\n3. The metric s dec n requires manual hyperparameter tuning (choice of n, batch size). The\npaper&#39;s own attempt at an automatic optimization algorithm (Appendix A.6) is admitted\nto be hard and &quot;require a lot of hyper-parameter tuning to work in real LLMs limiting its\nutility&quot;.\n4. The authors explicitly state in Appendix A.8 that LLM experiments were limited to &quot;only\na few layers&quot; due to computational costs, so generalization to all layers remains\nuncertain."}, "questions": {"value": "1. Provide theoretical reasoning or formal analysis for why s dec n tracks monosemanticity.\n2. Explore robustness under different loss functions and decoder regularizations.\n3. Extend experiments to diverse architectures and deeper layers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RFAOMZiAIS", "forum": "wUzBBsrdB1", "replyto": "wUzBBsrdB1", "signatures": ["ICLR.cc/2026/Conference/Submission9507/Reviewer_Pdhb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9507/Reviewer_Pdhb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944235729, "cdate": 1761944235729, "tmdate": 1762921079991, "mdate": 1762921079991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the consequences of setting too low an L0 value when training Sparse AutoEncoders (SAEs). Toy experiments show evidence of feature hedging when L0 is set too low, and also indicate that \"sparsity versus reconstruction\" analyses in prior work can lead to choosing artificially low L0 values (and associated hedging). The authors propose a heuristic method for estimating better L0 values, and conduct several experiments that aim to validate this method."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper indicates an important issue (feature hedging) that can arise from poor hyperparameter (L0) selection, which is relevant for practitioners to keep in mind when training SAEs. Additionally, the demonstration with toy models that popular \"sparsity versus reconstruction\" analyses can yield to poor L0 selection may be helpful to the SAE research community."}, "weaknesses": {"value": "It has been a well-established fact since the earliest days of machine learning that poor hyperparameter selection leads to underperforming models. It is not clear to me that the paper makes any contribution beyond showing that this is also true of the L0 hyperparameter when training SAEs.\n\nOne potential contribution of the work would be in its proposal of \"metric\" $s_n^{dec}$ that might be useful in estimating L0 values. However, it is not clear what $s_n^{dec}$ it is intended to measure, nor its intended utility. \n- Charitably, such a metric that estimates L0 values might be useful in reducing time and resource costs associated with SAE hyperparameter search. However, for $s_n^{dec}$ in particular, this is not possible, as computing it requires training SAEs first.\n- Sec 4 does contain experiments computing $s_n^{dec}$ in the context of BatchTopK SAEs trained on two LLMs, comparing $s_n^{dec}$ and sparse probing scores across L0 values, finding that the two often peak in similar layers. However, it is not clear that analyzing $s_n^{dec}$ has any utility (in terms of theoretical understanding, practical SAE training, or any other consideration) in this setting -- if we already know sparse probing scores, what is the additional benefit of computing $s_n^{dec}$?\n\nFinally, the paper is very light on citation and discussion of closely related prior works, such as:\n- [1, 2] study feature splitting, a closely related phenomenon to hedging (as observed in this work for low-L0 SAEs).\n- [3] introduces a metric and activation function to approximate the theoretically optimal L2 norm of SAE latent vectors.\n- [4] studies feature hedging, as observed in this work -- while the paper cites [4] several times, the novelty of this work relative to [4] is not specified. (Optimistically, the distinction would be that this work focuses on hedging that arises from low L0 values, whereas [4] focuses on hedging due to small dictionary sizes -- however, the effect of L0 on hedging was also shown in [4] (see, e.g., sec 4.1 of [4]).)\n\n[1] Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., et al. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2, 2023. URL: https://transformer-circuits.pub/2023/monosemantic-features#phenomenology-feature-splitting               \n[2] Templeton, et al., \"Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet\", Transformer Circuits Thread, 2024. URL: https://transformer-circuits.pub/2024/scaling-monosemanticity/#feature-survey-neighborhoods                \n[3] Lee, S., Davies, A., Canby, M. E., & Hockenmaier, J. (2025). Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality. Second Conference on Language Modeling. URL: https://openreview.net/forum?id=XhdNFeMclS               \n[4] Chanin, D., Dulka, T., & Garriga-Alonso, A. (2025). Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders. arXiv preprint arXiv:2505.11756. URL: https://arxiv.org/abs/2505.11756"}, "questions": {"value": "Beyond those raised in the Weaknesses section, a few remaining questions and concerns are listed by section below.\n\nSec 3.1-3.3:\n- Why use fewer \"true features\" than dimensions ($g < d$) in toy settings, when one of the primary motivations of modern SAEs is that embedding vectors are expected to leverage superposition to encode (potentially exponentially) more features than embedding dimensions ($g >> d$)?\n    - If experiments are repeated with $g >> d$, how similar do the results look?\n\nSec 3.5:\n- What does it mean to \"sort these values [of $\\mathbf{z}$] in descending order to get $\\mathbf{z}_\\downarrow$\"? What is \"descending order\" (later referred to as \"ranking\") in this case, and why is it necessary to sort $\\mathbf{z}$ according to this ranking?\n- Is there any theoretical justification supporting the proposed $s_n^{dec}$ (as defined in eqn 5) as an estimator of the ideal L0 value? (I have the same concern regarding $n = h / 2$.) From the provided explanation, it seems that the authors might have simply experimented across arbitrary L0 thresholds until finding a formula that happened to perform well in toy experiments. However, if this interpretation is correct, then it is not clear whether it is simply due to (unintentional) p-hacking -- i.e., running enough arbitrary experiments that eventually a seemingly-nontrivial pattern appears by chance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hP3v0X4T80", "forum": "wUzBBsrdB1", "replyto": "wUzBBsrdB1", "signatures": ["ICLR.cc/2026/Conference/Submission9507/Reviewer_Z23o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9507/Reviewer_Z23o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762483494922, "cdate": 1762483494922, "tmdate": 1762921079539, "mdate": 1762921079539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for their engagement with our work. We highlight that reviewers found our work “provides convincing evidence”, that picking L0 “is a very important problem”, that our analysis is “well-motivated” with “strong empirical evidence”. \n\nBased on the feedback from reviewers, we have made the following improvements to the paper. We have tried to address all concerns raised, but are open to further suggestions for improvement.\n\n## Metric\nAs suggested by reviewer concerns, we swapped the roles of the $s_n^{dec}$ (previously the main metric) with decoder pairwise cosine similarity $c_{dec}$ (previously the alternative metric). So, $c_{dec}$ is now highlighted more prominently as the main metric in the main body of the paper. This has the following advantages:\n\n- $c_{dec}$ is parameter-free, so there can be no confusion about how to set the hyperparameters.\n- $c_{dec}$ is easier to understand. In particular, it is easier to understand why this metric increases if there is more mixing of features into SAE latents, both intuitively and theoretically.\n\n## Theory\nSeveral reviewers mentioned they wanted more theoretical justification, so we added the following:\n\n- Proof that low L0 causes feature mixing in a simple SAE (A.5)\n- Proof that feature mixing causes larger values of $c_{dec}$ (A.6)\n- Proof that feature mixing causes larger values of $s_n^{dec}$ (A.10)\n\n## Experiments\nSeveral reviewers asked for further experiments, so we ran these and added them to the paper:\n\n- Experiments showing that the degree of feature mixing is determined by amount of feature correlation (A.3.1)\n- Experiments showing that the degree of mixing increases as L0 decreases below the true L0 (A.3.2)\n- Experiments with superposition noise (A.3.3, A.4.1)\n- More LLM experiments (A.12)\n\n## Misc\nWe also made the following changes based on reviewer feedback:\n\n- Improved writing and presentation clarity\n- Added Python pseudo-code for metrics (A.17)\n- Added all references reviewers requested to Related Work (S.5)\n\nWe believe this addresses the concerns raised by reviewers, and makes the paper much stronger as a result."}}, "id": "tlUsUJgkZs", "forum": "wUzBBsrdB1", "replyto": "wUzBBsrdB1", "signatures": ["ICLR.cc/2026/Conference/Submission9507/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9507/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9507/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763661158469, "cdate": 1763661158469, "tmdate": 1763661158469, "mdate": 1763661158469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}