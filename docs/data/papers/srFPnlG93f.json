{"id": "srFPnlG93f", "number": 697, "cdate": 1756773464608, "mdate": 1759898246827, "content": {"title": "Plan before Solving: Problem-Aware Strategy Routing for Mathematical Reasoning", "abstract": "Existing methods usually leverage a fixed strategy, such as natural language reasoning, code-augmented reasoning, tool-integrated reasoning, or ensemble-based reasoning, to guide Large Language Models (LLMs) to perform mathematical reasoning. Our analysis reveals that the single strategy cannot adapt to problem-specific requirements and thus overlooks the trade-off between effectiveness and efficiency. To address these issues, we propose Planning and Routing through Instance-Specific Modeling (PRISM), a novel framework that decouples mathematical reasoning into two stages: strategy planning and targeted execution. Specifically, we first curate a multi-strategy preference dataset, which we call MathStrat, capturing correctness, process quality, and computational efficiency for each problem–strategy pair. Then, we train a lightweight Strategy Adapter based on the dataset to obtain confidence distributions over the mentioned four reasoning strategies. At inference time, an adaptive routing policy dynamically tailors the reasoning approach based on predictor confidence. It directs the model to use single-strategy execution for high-confidence predictions, dual-strategy verification for competitive scenarios, or comprehensive multi-strategy exploration for uncertain cases. Extensive experiments across five mathematical reasoning benchmarks demonstrate that PRISM consistently outperforms individual strategies and ensemble baselines, achieving improvements ranging from 0.9% to 7.6% across different base models. The adaptive routing approach shows particularly strong benefits for mathematical reasoning tasks across diverse model architectures. Our code is released at https://anonymous.4open.science/r/PRISM-1EAE/.", "tldr": "", "keywords": ["Mathematical Reasoning", "Large Language Model Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51dc1b5c8003a91500fce8284fc04bcb7db6905c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces PRISM, a routing-based framework that (1) categorize each math problem with \"soft labels,\" and (2) execute some prompting strategies of the router. They've shown improvements on some math dataset, and they conducted ablations on components of adaptive routing."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Routing-based method seems intuitive \n2. Optimized the router with self-created dataset \n3. Did ablation on adaptive routing component"}, "weaknesses": {"value": "1. This method, from my perspective, is somehow similar to [1]. In math reasoning, what HybridMind did is also decompose (route) the problem into the \"correct\" or \"groundtruth\" one. The difference, from my end, is they train a hard classifier using CE loss, while you train a soft labler using KL + CE loss. But you lack proper citation in your related work. I anticipate to see more comparison with similar-idea routing based work. \n2. Similar to Question 1, how can you make sure the the categorized reasoning types are complete? \n3. I cannot fully be convinced that using raw timing in efficiency score, using output length (in tokens) seems more natural. For example, if I want to use this method with some proprietary models, are you going to waiting time as a training signal in this case?\n4. Any specific reasonings you choose these three models in your evaluation? What's the performance of some proprietary models, say an old one, GPT 3.5 or GPT 4.\n5. What's the major advantage of using KL divergence during router training compared to classification / multi-class classification? \n6. Is this method generalizable? Have you tried to adapt this method to other domain, say logical reasoning and scientific reasoning? My impression for these two kind of reasonings are somehow similar to math, but maybe different from some angles. But it would be interesting to see if your method would success/fail/partial in other domains. \n\n[1] Han, S., Liu, T., Li, C., Xiong, X., & Cohan, A. (2024). HYBRIDMIND: Meta Selection of Natural Language and Symbolic Language for Enhanced LLM Reasoning. arXiv preprint arXiv:2409.19381."}, "questions": {"value": "1. Line 191: \"existing reasoning strategies can be broadly categorized into four paradigms,\" if you are saying \"broadly,\" did you routing method covers all cases? i.e., you do have the probability that you miss one important reasoning method, how can you prove your categorization is complete? \n2. Line 198: \"(2) process quality,\" how do you evaluate \"process quality?\" This is unclear. Are you only checking if the proof is logically coherent? \n3. Line 199: \"(3) computational efficiency,\" how do you quantify this metric? By tokens during computation? execution (thinking) time? model size? Need to be more specific here.\n4. eq 7, a bit confusing, can you explain what this loss is doing? enforcing the top-1 selection? \n5. line 263, I might misunderstanding something. How you can do majority voting for 2 strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GsXSGFKnjX", "forum": "srFPnlG93f", "replyto": "srFPnlG93f", "signatures": ["ICLR.cc/2026/Conference/Submission697/Reviewer_xuiq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission697/Reviewer_xuiq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760658999105, "cdate": 1760658999105, "tmdate": 1762915583615, "mdate": 1762915583615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PRISM, a framework that enables large language models to dynamically apply multiple strategies to solve mathematical problems. The proposed method trains a strategy adapter that, before the model begins problem solving, selects one or more strategies from Natural Language Reasoning (NLR), Code-Augmented Reasoning (CAR), Tool-Integrated Reasoning (TIR), and Ensemble-Based Reasoning (EBR). This allows the model to adopt the most suitable strategy for a given problem and thereby improve both problem-solving accuracy and efficiency.\n\nTo support PRISM, the paper introduces a method for constructing training data for the adapter and develops the MathStrat dataset specifically for this purpose. Experimental results show that models enhanced with PRISM outperform multiple baseline approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**[S1]** The paper is well-written, with clear methodological descriptions. The proposed approach is easy to follow, and the authors provide code for reproducibility.  \n**[S2]** The paper addresses an important problem: how to enable models to adopt different reasoning strategies depending on the complexity of a given problem. For simple problems, the model can rely on its internal knowledge, while for more difficult ones, it needs to leverage code and external tools for problem-solving.  \n**[S3]** The paper presents a comprehensive set of experiments, including detailed ablation studies."}, "weaknesses": {"value": "**[W1] Methodology:** The paper trains a separate Strategy Adapter (SA) independent of the LLM to select problem-solving strategies. However, the authors do not explain why a separate SA is necessary, rather than enabling the LLM itself to dynamically select the appropriate reasoning strategy. If the SA can effectively choose suitable strategies, the LLM should in principle be capable of learning this ability as well.\n\n**[W2] Originality:** Prior work [1] has already explored dynamic selection among multiple reasoning strategies, yet the authors do not provide comparisons with such related studies—neither in terms of methodological design nor empirical performance.\n\n**[W3] Fairness of Experimental Comparison:** It is unclear whether the comparison between PRISM and CoT is fair. CoT performs only a single inference, whereas PRISM may conduct multiple inference rounds followed by majority voting (for instance, if the SA selects Exploratory Routing, the model performs four different reasoning attempts). Such a setup may lead to an unfair advantage for PRISM.\n\n**[W4] Robustness:** The construction of the MathStrat dataset involves a large number of hyperparameters, yet the authors do not provide a systematic analysis of the model’s sensitivity to these hyperparameters.\n\nRef:  \n[1] Xu et al. Teaching llms according to their aptitude: Adaptive reasoning for mathematical problem solving. arXiv 2502.12022."}, "questions": {"value": "**[Q1]** There appears to be an inconsistency in the baseline results reported in the paper. In Table 1, under the CoT strategy, Qwen2.5-Math-7B achieves 78.1 on GSM8K and 21.2 on MATH500. However, according to the official Qwen2.5-Math report [2], the Qwen2.5-Math-7B model reaches 91.6 (GSM8K) and 55.4 (MATH) in the few-shot setting, while the Qwen2.5-Math-7B-Instruct model achieves 95.2 (GSM8K) and 83.6 (MATH) in the zero-shot setting. Could the authors provide a reasonable explanation for this discrepancy?\n\n**[Q2]** Could the authors provide a detailed description of how the inference time is calculated? I could not find this information in the paper, and it is critical for assessing the validity of the claimed efficiency improvements.\n\nRef:  \n[2] Yang et al. Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement. arXiv 2409.12122."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5co6ElJJJJ", "forum": "srFPnlG93f", "replyto": "srFPnlG93f", "signatures": ["ICLR.cc/2026/Conference/Submission697/Reviewer_T3yL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission697/Reviewer_T3yL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761021365046, "cdate": 1761021365046, "tmdate": 1762915583497, "mdate": 1762915583497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a technically sound, well-motivated, and methodologically clean contribution to inference-time reasoning optimization for LLMs. The formulation of mathematical reasoning as problem-aware strategy routing is conceptually novel and bridges ideas from meta-reasoning and tool orchestration. However, several points need clarification: the construction and bias of the MathStrat dataset, the scalability of the adapter to unseen strategies, and the robustness of routing thresholds (τ_c, τ_a) across domains. Moreover, while empirical improvements are consistent, the reported gains (≈1–7%) may be modest relative to the added system complexity"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper targets a central limitation in LLM reasoning: static inference paradigms that cannot adjust reasoning depth or modality to instance-level complexity. The problem is well-motivated by Figure 1, which empirically demonstrates cross-strategy variance in both accuracy and efficiency. This finding aligns with emerging work on meta-reasoning (Didolkar et al., 2024).\n\n2. Section 4.4 (Fig. 3) convincingly shows that PRISM improves Pass@1 accuracy while reducing or maintaining inference cost compared to hybrid baselines. The confident/deliberative/exploratory routing design (Section 3.2) provides a clear operational mechanism for adaptive compute allocation without external verifiers."}, "weaknesses": {"value": "1. MathStrat combines correctness, quality, and efficiency via fixed weights (Eq. 4), but the weighting scheme (w_C, w_Q, w_U) and its sensitivity are not justified or ablated. Potential annotation bias may transfer to adapter predictions.\n\n2. Only four strategy families (NLR, CAR, TIR, EBR) are considered. It is unclear how PRISM generalizes to newly emerging paradigms (e.g., tree-of-thought, verifier-augmented reasoning).\n\n3. The confidence and ambiguity thresholds (τ_c = 0.4, τ_a = 0.08) are globally fixed (Appendix A.3); more studies are needed to verify their stability across datasets or model sizes.\n\n4. The paper does not detail where PRISM misroutes (e.g., confident errors vs. unnecessary exploration) or how confidence calibration could fail.\n\n5. The adaptor is only trained via supervised finetuning (SFT). I expect an effective reinforcement learning (RL) method to optimize the adaptor on new/unseen examples, improving its generalization."}, "questions": {"value": "Refer to Fig. (3), I am curious about why PRISM generates more tokens (see subfigure c) but it takes relatively less inference time (subfigure b)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F80BRGJv9K", "forum": "srFPnlG93f", "replyto": "srFPnlG93f", "signatures": ["ICLR.cc/2026/Conference/Submission697/Reviewer_24RJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission697/Reviewer_24RJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965652675, "cdate": 1761965652675, "tmdate": 1762915583381, "mdate": 1762915583381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that there is no single reasoning strategy that consistently performs well both efficiently and effectively across mathematical reasoning tasks. It proposes to train a Strategy Adapter on a multi-strategy preference dataset constructed based on correctness, process quality, and computational efficiency. The lightweight adapter dynamically routes between confident, deliberative, and exploratory modes at test time to select the reasoning strategy to explore for each question instance. Empirical analysis across five mathematical reasoning benchmarks shows consistent improvements, particularly when the model scale is small."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of training a lightweight Strategy Adapter that enables a dynamic and adaptive routing policy for reasoning strategy is interesting.\n- The empirical results are effective, demonstrating consistent gains across tasks.\n- The ablation studies are extensive and useful, carefully evaluating the effects of routing modes, efficiency, and scalability."}, "weaknesses": {"value": "- Related work discussion is limited. The Strategy Adapter sounds similar to a MOE gating network, and other routing or adaptive ideas have been explored in prior work on tool use. These connections are not sufficiently compared or discussed.\n- The MathStrat dataset includes only four reasoning strategies, and the adapter is trained to route among them dynamically. This design still limits the reasoning space to those four strategies and is not easily extendable to broader reasoning paradigms without reconstructing the dataset and retraining the adapter from scratch.\n- The main results (Table 1) compare PRISM with four other strategies in isolation. A stronger baseline that jointly utilizes all four reasoning strategies is desirable. While the “Hybrid” baseline combines CoT and PoT, it does not encompass or compare against a setting that integrates all strategies. Including a model that executes all strategies simultaneously, even without adaptive selection, would help demonstrate the effectiveness of PRISM in a fairer manner."}, "questions": {"value": "- How is “process quality” computed by the automated evaluator? This step seems difficult and potentially subjective to the model used.  \n\n- What datasets are used to construct MathStrat? Is the Strategy Adapter tested on datasets that were not included in training? How much overlap exists between the MathStrat construction data and the evaluation benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i4sj1sRdNK", "forum": "srFPnlG93f", "replyto": "srFPnlG93f", "signatures": ["ICLR.cc/2026/Conference/Submission697/Reviewer_jFKh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission697/Reviewer_jFKh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission697/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966607722, "cdate": 1761966607722, "tmdate": 1762915583248, "mdate": 1762915583248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}