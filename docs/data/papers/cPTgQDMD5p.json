{"id": "cPTgQDMD5p", "number": 15634, "cdate": 1758253398329, "mdate": 1763613259926, "content": {"title": "THE END OF MANUAL DECODING: TOWARDS TRULY END-TO-END LANGUAGE MODELS", "abstract": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly \"end-to-end'' generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass.\n\nThrough extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from \"hacking the test set\"—a practical upper bound for any static method. Besides, we demonstrate an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., ''generate with low randomness'') and adjusts its predicted temperature and top-p on a token-by-token basis, which may open a new paradigm for steerable and interactive LLM decoding.", "tldr": "We introduces AutoDeco to dynamically generate sampling parameters which improves LLMs' performance with almost no added latency. Crucially, it enables the model can understand natural language commands and actively steer its own decoding parameters.", "keywords": ["dynamic decoding", "instruction-based Control", "truly end-to-end"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/17d1b58defd467ef01469c7151feffc0d78b0b90.pdf", "supplementary_material": "/attachment/cf6b6852208da00eb31011d8e9e407d5425c48f7.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces AutoDeco, a framework that aims to make language model decoding fully end-to-end by enabling models to predict their own decoding hyperparameters (temperature and top-p) dynamically at each generation step. The authors augment standard transformers with lightweight MLP heads that output token-specific decoding parameters, trained using pseudo-labels derived from optimization on ground-truth tokens. The approach claims to remove the need for manual or oracle-style hyperparameter tuning while maintaining negligible computational overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel framing of decoding as a learnable, differentiable process. Treating temperature and top-p as learnable functions of context rather than fixed hyperparameters is an appealing conceptual shift that aligns with recent trends toward self-regulating and adaptive LLM inference.\n2. Simple yet effective architecture. The addition of two MLP heads is computationally cheap and can be easily integrated into existing transformer architectures, which improves the practicality and reproducibility of the approach.\n3. The observation that the model can interpret natural-language modifiers like “low randomness” to adjust temperature/top-p values is intriguing and opens a new line of research in interpretable controllability."}, "weaknesses": {"value": "1. My concern is that it is not clear to me how they obtain labels for training prediction of temperature and top_p value per token. It is unclear how the argmax over continuous T > 0 is solved, what constraints or search ranges are used, and how noise in logits affects these derived labels. It is better to provide a simple example to explain this process. \n2. The experiments only compare against Greedy and Default Sampling (and an oracle). Missing are modern decoding methods such as Contrastive Search [1], Contrastive Decoding [2].\n\n[1] https://arxiv.org/pdf/2210.14140\n[2] https://arxiv.org/abs/2309.09117"}, "questions": {"value": "Please see the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jiUcoIPKX1", "forum": "cPTgQDMD5p", "replyto": "cPTgQDMD5p", "signatures": ["ICLR.cc/2026/Conference/Submission15634/Reviewer_8WUM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15634/Reviewer_8WUM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761022909553, "cdate": 1761022909553, "tmdate": 1762925896548, "mdate": 1762925896548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Initial Updates to Manuscript"}, "comment": {"value": "Dear Reviewers and ACs,\n\nThank you for your time and valuable feedback.\n\nWe have uploaded a significantly revised version of our paper, reflecting major progress made since the initial submission. We are confident these updates fundamentally strengthen our work and address key concerns.\n\nHere are the highlights of the revision:\n\nA Fundamental Leap to True End-to-End Training: This is the most significant improvement. We have evolved our method from a cascaded pipeline (which relied on generating pseudo-labels) to a true end-to-end training paradigm. As detailed in the revised Section 2.1, gradients from the standard cross-entropy loss are now backpropagated to directly and simultaneously update both the temperature and top-p heads. This methodological leap is crucial: it creates a more elegant and robust training process, completely removing the need for intermediate, potentially noisy supervision signals. This new paradigm fully resolves the core question of label acquisition, as raised by Reviewer 8WUM (Weakness 1).\n\nExpanded Evaluation on State-of-the-Art Large Models: To prove the scalability and practical value of AutoDeco, we have extended our validation to much larger, production-scale models. The revised manuscript now includes comprehensive results for Qwen3-235B-A22B-Thinking-2507 and DeepSeek-V3.1-Terminus-671B (Tables 1, 2, 8). These new experiments confirm that AutoDeco's effectiveness holds strong even on today's most powerful architectures.\n\nComprehensive Robustness Analysis with pass@k: To provide a more rigorous assessment beyond pass@1, we introduced an extensive pass@k analysis (Appendix 8, Tables 5, 6, 7). The results are highly encouraging: we find that the absolute performance improvements delivered by AutoDeco are not only sustained but often become even greater at higher k-values. This demonstrates the robustness of our method, proving it enhances the overall quality of the entire solution space, not just the single best answer.\n\nGiven these fundamental improvements, we believe the paper is in a much stronger position. We respectfully ask you to re-evaluate our work based on this new, more powerful version.\n\nWe will now proceed to address your individual comments and look forward to a constructive discussion.\n\nBest regards,\n\nThe Authors"}}, "id": "PAIgMsDrFH", "forum": "cPTgQDMD5p", "replyto": "cPTgQDMD5p", "signatures": ["ICLR.cc/2026/Conference/Submission15634/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15634/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15634/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762948775272, "cdate": 1762948775272, "tmdate": 1762948848969, "mdate": 1762948848969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a method to learn the sampling hyperparameters (temp, top-p) end to end via finetuning, avoiding manual task-specific tuning. They also identify and give ways to solve challenges that arise when trying to do this naively. In the end, they find they can basically match task-specific manual tuning (the oracle upper bound)."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The idea is interesting and to my knowledge novel. \n- The problem space is richer than meets the eye (eg. how do you get training data for supervised training is surprisingly nontrivial). \n- The results are fairly convincing (Fig2). \n- They show this doesn't hurt performance (Fig3)."}, "weaknesses": {"value": "- In practice, nobody does pure autoregression in real world LLM usage at production-scale. Everyone uses speculative decoding of some sort, and it's not clear to me whether this sampling scheme permits that or breaks it. I would want to see an explanation or formal argument/construction for how speculative decoding would work when the target model has an AutoDeco head to be convinced this would not break speculation. Because if it does, then in practice it will never be used, which would be a major limitation. I think it can be made to work, though, it just needs maybe some explanation of how it would all come together. \n- The idea and science is good, but writing is mediocre in my opinion. The contributions section repeats the second contribution twice, omitting I imagine the third contribution (presumably the \"emergent instruction-following\"), the plots are not consistently formatted well (axes in Fig4 impossible to read), the name \"AutoDeco\" is neither catchy nor informative in my opinion, etc. \n- The \"emergent ability\" is not as surprising as the authors seem to think it is. You're training on a base model that conditions on language by construction, so I would expect this behavior as the language conditioning affects the model representations fed into the AutoDeco head. Maybe de-emphasize this, or figure out an alternative framing? But this is minor."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q1oMiI0KXi", "forum": "cPTgQDMD5p", "replyto": "cPTgQDMD5p", "signatures": ["ICLR.cc/2026/Conference/Submission15634/Reviewer_tpjS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15634/Reviewer_tpjS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761602889297, "cdate": 1761602889297, "tmdate": 1762925896192, "mdate": 1762925896192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work addresses an important practical issue in deploying large language models (LLMs): the need to manually tune decoding hyperparameters, which breaks the “end-to-end” workflow. To mitigate this, the authors propose AutoDeco, a method that augments standard transformer architectures with lightweight prediction heads that output context-dependent temperature and top-p values alongside next-token logits during decoding. Experiments across multiple benchmarks demonstrate that the approach can yield improved generation quality and adaptability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem is timely and relevant. Removing manual tuning of decoding hyperparameters can significantly improve the practicality and usability of LLM-based systems.\n\n2. The pseudo-label generation strategy for supervision is clever and helps circumvent the lack of direct ground-truth hyperparameter labels."}, "weaknesses": {"value": "1. A key concern is that the model is trained to further increase the likelihood of the reference text. Since both pre-training and downstream fine-tuning typically already optimize for the likelihood of the ground-truth sequence, this additional adjustment may risk overfitting or reduce robustness in more open-ended generation settings.\n\n2. While dynamic prediction of decoding hyperparameters is appealing, different applications may require different behavior. For example, customer support systems typically favor stability and consistency, whereas brainstorming or creative writing tasks may benefit from higher variability. It is unclear whether a single learned mechanism can generalize effectively across such diverse usage scenarios without explicit control, and it may limit adaptability when user preferences change."}, "questions": {"value": "1. Can this framework be extended to other decoding parameters, such as top-k or repetition penalty?\n\n2. In practical deployments, how should developers express or configure user-level preferences (e.g., consistency vs. creativity)? More discussion or empirical analysis would clarify how the method adapts to varying application requirements, especially in relation to the concern raised above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "miLQgCKUpt", "forum": "cPTgQDMD5p", "replyto": "cPTgQDMD5p", "signatures": ["ICLR.cc/2026/Conference/Submission15634/Reviewer_zLkm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15634/Reviewer_zLkm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997078641, "cdate": 1761997078641, "tmdate": 1762925895753, "mdate": 1762925895753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AutoDeco, a novel architecture designed to make large language models (LLMs) truly end-to-end by eliminating the need for manual decoding hyperparameter tuning (e.g., temperature, top-p). Traditional decoding strategies rely on static, hand-tuned parameters that must be manually adjusted for different tasks or even different parts of a generation. AutoDeco addresses this by augmenting the transformer with lightweight “decoding heads” that dynamically predict temperature and top-p values at every generation step."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Novelty and Conceptual Contribution: The paper identifies and addresses a fundamental yet overlooked bottleneck in LLM deployment, the manual, non-differentiable decoding process.\n\nAutoDeco reframes decoding as a learnable and parametric component within the model itself, offering a principled step toward fully end-to-end generation."}, "weaknesses": {"value": "1. While the emergent instruction-following behavior is a highlight, the explanation for why this arises is mostly empirical. A deeper analysis (e.g., probing whether linguistic cues correlate with latent space adjustments) would strengthen this claim.\n\n\n2. Most benchmarks are reasoning or QA-oriented. It would be valuable to test AutoDeco on creative writing, dialogue, or long-form generation, where decoding choices play a larger role. Human evaluation or qualitative examples of improved text quality would strengthen the practical impact."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KQ6q6h7W7o", "forum": "cPTgQDMD5p", "replyto": "cPTgQDMD5p", "signatures": ["ICLR.cc/2026/Conference/Submission15634/Reviewer_XeNy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15634/Reviewer_XeNy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15634/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762113072362, "cdate": 1762113072362, "tmdate": 1762925895304, "mdate": 1762925895304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Update of the Manuscript According to the Rebuttal Discussion"}, "comment": {"value": "Dear Reviewers and ACs,\n\nWe sincerely thank you for your time and insightful feedback, which have been invaluable in improving our manuscript. We have carefully considered all comments and have provided detailed responses to each reviewer individually. \n\nBelow, we summarize our main revisions addressing the common points: \n\n**On the Adaptability of AutoDeco:** We selected the eight benchmarks in the main paper primarily because they are widely recognized, standard benchmarks that are frequently featured in the technical reports of leading LLMs. Furthermore, in our own industrial practice, AutoDeco has proven effective in common open-ended generation scenarios like role-playing, enabling us to eliminate manual decoding without performance degradation.\n\nWe have conducted further evaluations to assess the adaptability of our method. We benchmarked AutoDeco on new **Creative Tasks** and **Role-play** scenarios. The results, which demonstrate the **strong out-of-domain generalization capabilities** of our approach, have now been incorporated into the main paper in **Sec 3.2.1** (paragraph on \"out-of-domain performance\").\n\n**On Instruction-Based Decoding Control:** We thank Reviewer **XeNy** and **tpjS** for their valuable feedback on this topic. It was indeed an unexpected yet interesting phenomenon discovered during our experiments. While we were enthusiastic about this preliminary finding, we must candidly acknowledge that we currently lack a comprehensive explanation beyond these empirical observations, and this remains an area of ongoing work.\n\nTherefore, to ensure our claims are appropriately scoped and to avoid overclaiming our contribution, we have revised the manuscript as follows:\n\n- We have toned down the emphasis on the \"instruction-based decoding control\" capability in the main paper.\n- The detailed discussion of this phenomenon has been moved to **Appendix 7**.\n\nThe revised manuscript has been uploaded. We believe these revisions substantially strengthen the paper and more accurately reflect our core contributions, and we respectfully request your re-evaluation. Thanks again for the constructive suggestions to improve our work, and look forward to your further feedback.\n\nBest regards,\n\nThe Authors"}}, "id": "PcAIezBWbX", "forum": "cPTgQDMD5p", "replyto": "cPTgQDMD5p", "signatures": ["ICLR.cc/2026/Conference/Submission15634/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15634/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission15634/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763613417452, "cdate": 1763613417452, "tmdate": 1763613417452, "mdate": 1763613417452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}