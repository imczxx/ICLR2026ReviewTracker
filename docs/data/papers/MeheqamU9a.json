{"id": "MeheqamU9a", "number": 23243, "cdate": 1758341198363, "mdate": 1759896824600, "content": {"title": "ECO grad: Error Correcting Optimization for Quasi-Gradients, a Variable Metric DFO Strategy", "abstract": "We introduce a \\textit{Quasi-Gradient} method using 0th order directional derivatives and quasi-Newton like updates. Empirically, our method reduces $d$-dependence of zeroth-order problems to an effective $\\approx d \\cdot m$ factor $1/d \\le m \\le 1$, with only a small linear increase in compute. We show this holds under Lipschitz bounds and on practical tasks. While compressive sensing achieves similar gains with sparse gradients, our approach applies to any gradient geometry. It exploits high cosine similarity and stable gradient norms along neighboring steps, ultimately requiring fewer samples to correct the estimator. Applications include policy optimization, model-free reinforcement learning, function smoothing, evolutionary methods, efficient JVPs (e.g. in JAX), learning from simulation, and related areas. We include a probing framework that leverages convergence bounds to detect when a gradient estimator is no longer aligned with new samples, helping prevent non-descent steps. We also introduce the \\textit{ECO estimator} a least-change secant update that results in a specific LMS adaption, which achieves $O(e^{-k/d})$ convergence in gradient MSE, while Monte Carlo averaging is sub-exponential $O(\\frac{d+1}{d+k+1})$. Finally we provide performance results comparing directional SGD to quasi-GD, alone and with adaptive optimizers. As models grow, our approach bridges the gap between full-gradient methods and large scale derivative free optimization. We hope to motivate further research in quasi-gradient techniques for simulation and exploratory learning.", "tldr": "This is about updating a gradient estimator actively in a quasi-Newton manner (quasi-gradient), and improving (reducing) dimensional dependence for potentially all DFO and no-grad problems. A feasible path to scaled up no-grad?", "keywords": ["DFO", "Derivative Free Optimization", "ZO", "Zeroth Order", "Jacobian Vector Products", "JVPs", "Directional Derivatives", "Evolutionary Strategies", "Simulation", "Optimization", "Model-Free", "Reinforcement Learning", "Policy functions", "Policy gradient", "Quasi-Newton", "Quasi-Gradient", "NoGrad", "no grad", "black box", "gradient interpolation", "secant constraint", "directional probes", "BFGS", "Broyden", "DFP", "Isotropic Distribution", "Ball surface", "ellipse", "uniform sphere distributed", "LMS filter", "Least Change Update"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54bffef54a44ba1c92e155a758b6c52386f2d319.pdf", "supplementary_material": "/attachment/09e9c108daa72c5f6a99e3f6307ddbb65c828514.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new algorithm for improving the  quasi-Newton method in reducing dimensional dependence, especially for the  DFO and no-grad problems."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This paper has nice figures. All references have links. Important formulas are boxed."}, "weaknesses": {"value": "This submission is apparently incomplete. Many sections are completed empty, e.g. Section 1 and Section 3.4. In appendix, many drafts left unpolished. For this reason, it is hard to understand the backgroud and the main contribution of this work. Moreover, it is necessary to include more detailed information in each lemma theorem statements, especially in explaining or recapping the meaning of mathematical notations."}, "questions": {"value": "As this submission is not complete, I do not have further questions in this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7xAcQlQWsm", "forum": "MeheqamU9a", "replyto": "MeheqamU9a", "signatures": ["ICLR.cc/2026/Conference/Submission23243/Reviewer_zPed"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23243/Reviewer_zPed"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761233588760, "cdate": 1761233588760, "tmdate": 1762942572366, "mdate": 1762942572366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a zero-th order quasi-Newton method, and a gradient estimator called ECO. However, the paper is incomplete and it is difficult to understand what is the real problem being studied and the proposed solution."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I cannot provide any positive feedback here, as the incompleteness of the paper makes it difficult to understand the content of the submission. I think the authors should complete the manuscript before submitting for publication."}, "weaknesses": {"value": "1. The paper is incomplete. The first section is entirely empty, as is the results section. I cannot recommend anything other than an immediate reject for a paper with whole sections missing."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6luELWRcrX", "forum": "MeheqamU9a", "replyto": "MeheqamU9a", "signatures": ["ICLR.cc/2026/Conference/Submission23243/Reviewer_Wv5Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23243/Reviewer_Wv5Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761576395924, "cdate": 1761576395924, "tmdate": 1762942572124, "mdate": 1762942572124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ECO grad to estimate gradients using directional derivatives. The authors claim that this method achieves an exponential convergence rate in gradient MSE, which is significantly fast than Monte Carlo averaging."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The theoretical ideas of using LMS updates in gradient estimation and detecting true gradient change are interesting."}, "weaknesses": {"value": "The paper is fundamentally incomplete and not in a state suitable for review. The submission appears to be a very early draft. The empirical validation of the ECO grad algorithm is missing."}, "questions": {"value": "The manuscript must be completed with a full results section and all placeholders filled before it can be seriously evaluated by the community. For the current unfinished draft I don't have question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uBq5T7neLX", "forum": "MeheqamU9a", "replyto": "MeheqamU9a", "signatures": ["ICLR.cc/2026/Conference/Submission23243/Reviewer_LsWh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23243/Reviewer_LsWh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049244832, "cdate": 1762049244832, "tmdate": 1762942571778, "mdate": 1762942571778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is very difficult to read, or to summarize. There is no introduction section, the contributions aren't cleanly delineated, and the problem isnt' even clear. The paper seems to study the problem of estimating gradients given access only to sampling random distributions and directional derivative and gives a new algorithm \"ECO's Method\" (2.3) for doing that. The new method is sequential (and therefore we cannot do minibatching to estimate the gradient as in (2.2)), and is termed a \"Quasi-Gradient method\" as it \"improves a gradient estimate at parameter $x_k$ that was already established at $x_{k-n}$ (which makes no sense, the gradients at different parameters can be significantly different unless you also control how far you move between them). The convergence proof (Proof B.2) is not really a proof, it just says this estimate minimizes some objective, but does not explain why minimizing this objective is meaningful at all, or what's the relationship to the true gradient. The authors claim exponential convergence of a “normalized RMSE” metric under unit sphere sampling via a Randomized Kaczmarz argument, but as I said I don't see how this argument follows at all when the parameters change. This is claimed as an advantage over Monte-Carlo averaging which yields sublinear rates (Appendices B.3-B.4). There is some discussion of convergence later with a certain \"ECO ratio\" introduced to detect estimator drift and trigger full/partial resets but again, I don't understand how this would still give a correct proof and provide the same rate. Section 3.4 is empty. There is very little in the way of empirical applications. This paper needs a significant amount of work, in the current shape it is just too difficult to assess."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Please see the summary section."}, "weaknesses": {"value": "Please see the summary section. Below I summarize as points what I highlighted there.\n\n1. There is no clear problem oracle. The text alternates between “zeroth‑order directional derivatives,” “forward gradients,” and “JVPs,” but never actually defines what oracle is assumed (function values with finite differences vs. direct access to $v=\\langle\\nabla f,u\\rangle$ (how would you get that?!?) or its cost/noise model.\n2. The method is described as improving a gradient estimate at $x_k$ using an estimator established at previous $x_{k-n}$. Without controlling ($\\|x_k-x_{k-n}\\|$) or the drift of $\\nabla f$ (e.g., via Lipschitz‑based coupling of step sizes), I don't see how this works. The exponential‑rate claim uses randomized Kaczmarz with a *fixed* target vector, which does not account for changing $x_k$. \n3. Incomplete or missing proofs. Proof B.2 only derives the update via a least‑change Lagrangian; it does not argue consistency, unbiasedness, or relation to the gradient. \n4. The motivation for the ECO ratio is unclear. The ratio mixes a sphere‑sampling setup with Gaussian/t‑statistics approximations, it's all hand-wavy and doesn't really involve any proofs. \n5. Even ignoring all the above, ECO’s Method is inherently sequential in $k$ whereas most applications use minibatching.\n6. Inconsistent notation include “0TH‑ORDER,” inconsistent capitalization, no introduction section or clear statement of contributions. Here are two examples of statements in the manuscript that made no sense to me:\n1. \"Unbiasedness does not mean the distribution of u can’t be biased e.g. Rademacher or Bernoulli,\" What does this mean? Later in the same line we're directed to section A.1, which illucidates nothing more. The next section A.2 is incomplete.\n2. \"It's the only fully independent, identically distributed, and uniform variable on $\\mathcal{S}_{d-1}$\"?? That's the definition??\n\nIs this some kind of experiment with an LLM-generated manuscript? If yes, that'd be such a flagrant violation of research ethics and it'd have taken up valuable peer-review time & effort that could be spent towards genuine research submissions."}, "questions": {"value": "N/A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KeKYqiKi17", "forum": "MeheqamU9a", "replyto": "MeheqamU9a", "signatures": ["ICLR.cc/2026/Conference/Submission23243/Reviewer_vvuf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23243/Reviewer_vvuf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762126699681, "cdate": 1762126699681, "tmdate": 1762942571431, "mdate": 1762942571431, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}