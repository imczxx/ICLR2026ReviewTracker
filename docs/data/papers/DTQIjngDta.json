{"id": "DTQIjngDta", "number": 7040, "cdate": 1758005755163, "mdate": 1759897876109, "content": {"title": "$\\pi^3$: Permutation-Equivariant Visual Geometry Learning", "abstract": "We introduce $\\pi^3$, a feed-forward neural network that offers a novel approach to visual geometry reconstruction, breaking the reliance on a conventional fixed reference view. Previous methods often anchor their reconstructions to a designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, $\\pi^3$ employs a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design not only makes our model inherently robust to input ordering, but also leads to higher accuracy and performance. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models will be publicly available.", "tldr": "$\\pi^3$ is a feed-forward model that reconstructs 3D geometry without a fixed reference view, making it more robust and accurate for tasks like camera pose and depth estimation.", "keywords": ["Permutation-Equivariance", "3D reconstruction", "Reference-Free", "Camera Pose Estimation", "Depth Estimation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72bf04ece1b70b595bdedfeee70f334448d17968.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel permutation equivariant architecture for feed-forward 3D reconstruction. To achieve permutation equivariance, positional embeddings used to differentiate between frames and learnable tokens that denote a reference view are not used. The structure is straightforward, while the performance is impressive on various tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and easy to follow. \n\n2. The paper addresses the limitation of existing feed-forward 3D reconstruction pipelines, where one view is selected as a reference view and the performance may degrade if the reference view changes. \n\n3. The performance in camera pose estimation, monocular/video depth estimation, and pointmap reconstruction, is state-of-the-art. \n\n4. Ablation study verifies the permutation equivariance of Pi3."}, "weaknesses": {"value": "1. Camera pose estimation: In addition to using large angular threshold (30 degrees), more strict thresholds, e.g. 5 degrees, 10 degrees, should be used to illustrate the rotation accuracy. \n\n2. Point-map evaluation: \n    * On both DTU and ETH3D, the numbers of VGGT are very different from those in the original paper. Could the authors explain the reason? \n    * Since the authors already provide accuracy and completeness, it makes sense to further add Chamfer distance/F-score, which is the best metric to evaluate the overall reconstruction quality. \n\n3. In Table 6, CUT3R heavily relies on the sequential/temporal information. Thus, it is not a very good baseline here."}, "questions": {"value": "1. To compute the scale $s$ that aligns ground truth and prediction, the other solution is to use umeyama to align the ground truth and predicted camera poses. Different form optimizing Eq. 4, this solution looks simpler. Did the authors try this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OfDVpOdTOU", "forum": "DTQIjngDta", "replyto": "DTQIjngDta", "signatures": ["ICLR.cc/2026/Conference/Submission7040/Reviewer_DanB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7040/Reviewer_DanB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761302595124, "cdate": 1761302595124, "tmdate": 1762919238877, "mdate": 1762919238877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a VGGT-like model that achieves superior performance across multiple tasks, including pose estimation, depth estimation, and point map estimation.\n\nThe performance improvement stems from a novel permutation-invariant model design. Previous methods such as Dust3R, Mast3R, and VGGT rely on a predefined reference frame during estimation, making them sensitive to frame selection and limiting their robustness. In contrast, this work attains permutation invariance by removing all frame orderâ€“dependent positional encodings and introducing a scale-invariant point map loss together with an affine-invariant pose loss.\n\nStrengths: The proposed idea is both novel and insightful, supported by thorough and comprehensive experiments.\n\nWeaknesses: No major issues are identified in this paper."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Novel and insightful idea: The paper makes an insightful observation about the limitation of defining a reference frame in prior models. The proposed approach to eliminate this dependency is technically sound and straightforward to implement.\n2. Comprehensive experiments: The method is evaluated across multiple applications on widely used datasets. Reporting standard deviations in Table 6 is a valuable addition that strengthens the credibility of the results.\nWell-presented paper: The paper is clearly written and easy to follow. Figures and tables are well designed, effectively supporting the main arguments and improving readability."}, "weaknesses": {"value": "I don't see any major issues in this work."}, "questions": {"value": "How is the rotation mapped to xyz in Figure 6? What's the rotation representation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IMEswvGxFL", "forum": "DTQIjngDta", "replyto": "DTQIjngDta", "signatures": ["ICLR.cc/2026/Conference/Submission7040/Reviewer_ghyt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7040/Reviewer_ghyt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959538786, "cdate": 1761959538786, "tmdate": 1762919238475, "mdate": 1762919238475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes $\\pi^3$, a feedforward model for pose estimation and point cloud prediction. It investigates the limitations of previous feedforward reconstruction models, such as VGGT, whose performance depends on the selection of the reference view. To address this issue, the proposed method removes the reference-view-dependent design in the model architecture and introduces a relative camera pose loss and a local point loss. Experimental results show that the proposed method effectively resolves the aforementioned problem and achieves state-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper addresses an interesting and important problem: the dependence of previous feedforward reconstruction models on reference view selection. The proposed method provides a simple yet effective solution.\n\n- The proposed model achieves SOTA performance on both pose estimation and reconstruction tasks."}, "weaknesses": {"value": "- Some parts of the paper lack proper citations:\n    - The normal loss is identical to that in MoGe, but no citation is provided.\n    - The camera pose estimation module and loss function are the same as in Reloc3r, yet there is no citation in Section 3.3.\n\n- The proposed method requires weight initialization from VGGT, which makes the comparison with other methods, such as VGGT itself, somewhat unfair. Table 8 shows results when both models are trained from scratch; however, it remains unclear how the model performs when trained from scratch only with the local point and pose losses. Furthermore, can the model trained from scratch with the added global loss achieve performance comparable to the version initialized from VGGT?"}, "questions": {"value": "- For monocular depth estimation, how does the performance compare with SOTA models such as DepthAnythingv2? Besides, what is the input image resolution used for depth estimation?\n- Since the MLP head with pixel shuffling introduces grid-like artifacts, why does the proposed method still adopt this design instead of using DPT heads as in previous methods?\n\nI will raise my score if the authors can provide reasonable clarifications regarding initialization and the effect of adding the global loss."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "N0hcvl3nGf", "forum": "DTQIjngDta", "replyto": "DTQIjngDta", "signatures": ["ICLR.cc/2026/Conference/Submission7040/Reviewer_urpi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7040/Reviewer_urpi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979862904, "cdate": 1761979862904, "tmdate": 1762919238106, "mdate": 1762919238106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}