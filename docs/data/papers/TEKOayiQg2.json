{"id": "TEKOayiQg2", "number": 4120, "cdate": 1757605122173, "mdate": 1759898051836, "content": {"title": "SongEcho: Cover Song Generation via Instance-Adaptive Element-wise Linear Modulation", "abstract": "Cover songs constitute a vital aspect of musical culture, preserving the core melody of an original composition while reinterpreting it to infuse novel emotional depth and thematic emphasis. Although prior research has explored the reinterpretation of instrumental music through melody-conditioned text-to-music models, the task of resynthesizing full-length songs remains largely unaddressed. We formalize this challenge as Cover Song Generation, which requires preserving the source vocal melody while simultaneously synthesizing new vocals and accompaniment, posing higher demands for controllable music generation. To this end, we present SongEcho, which leverages Instance-Adaptive Element-wise Linear Modulation (IA-EiLM), a framework that incorporates controllable generation by improving both conditioning injection mechanism and conditional representation. To enhance the conditioning injection mechanism, we extend Feature-wise Linear Modulation (FiLM) to an Element-wise Linear Modulation (EiLM), to facilitate precise temporal alignment in melody control. For conditional representations, we propose Instance-Adaptive Condition Refinement (IACR), which refines conditioning features by interacting with the hidden states of the generative model, yielding instance-adaptive conditioning. Additionally, to address the scarcity of large-scale, open-source full-song datasets, we construct Suno70k, a high-quality AI song dataset enriched with comprehensive annotations.\nExperimental results show that our approach yields superior cover songs, surpassing state-of-the-art approaches across all metrics on multiple datasets, while requiring fewer than 30% of the trainable parameters employed by other methods.", "tldr": "A cover song generation framework and a novel conditioning method", "keywords": ["conditioning method", "controllable song generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cee17998b67573c71a69901b0a076b6bdad7a668.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper formalizes Cover Song Generation, preserving the vocal melody while generating a new song. The authors compare three methods for controllable music (song) generation: ControlNet, MuseControlLite, and their proposed model, SongEcho. Through multiple experiments, they show that SongEcho outperforms the others in terms of music quality, text adherence, and vocal melody preservation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. By using fewer parameters to adapt Acestep to the Cover Song Generation task, it demonstrates better performance compared to other fine-tuning methods, showing the effectiveness of the proposed approach.\n2. Provides a critical analysis across different conditioning mechanisms.\n3. The authors promised to open-source a dataset enriched with detailed annotations, including enhanced tags and lyrics."}, "weaknesses": {"value": "1. The authors adopt two baseline methods that are not originally applied to Acestep. The authors should provide implementation details for these baseline methods (e.g., training specifics, inference specifics) to ensure that both the baseline methods and the proposed method are treated equally.\n2. The original baseline methods might not be suitable for Acestep. ControlNet can be applied by simply copying parts of Acestep; however, regarding the linear attention architecture, MuseControlLite might require modifications since it uses the traditional cross-attention method with positional encoding and zero-convolution. There might be a misalignment if the cross-attention layers in Acestep use linear attention, but the decoupled cross-attention in MuseControlLite are not. Additionally, the original MuseControlLite paper indicates that during inference, multiple classifier-free guidance is necessary. The authors should provide details for both the training and inference processes for the baseline methods.\n3. The authors indicate that the ControlNet method is not fully trainable, but only the LoRA adapters are trainable. This makes the comparison less persuasive; perhaps the authors should try mixed-precision training.\n4. The authors define 'Cover Song Generation' as 'preserving the source vocal melody while simultaneously synthesizing new vocals and accompaniment'; however, generating a song with the exact same vocal melody might not fully represent 'cover song' generation. A 'cover song' does not mean that the vocal melody has to be exactly the same."}, "questions": {"value": "1.  Can the authors provide more details about the training and inference specifics?\n2.  I understand that the formulation of 'Cover Song Generation' limits the definition to 'using the same vocal melody and regenerating it,' which might be easier. However, a cover song traditionally implies that the musical elements, such as instrumentation, tempo, or melodic phrasing, might change, while the fundamental character of the song remains the same. How will you address your work in light of this definition? Otherwise, the work might be more closely related to a vocal-to-accompaniment generation task.\n3. The authors claim that \"cross-attention mechanism\" are redundant. However, in the MuseControlLite paper, they claim that this is for the use of improvisation (i.e. if only partial condition is provided, the model can improvise for the rest), does songecho support this?\n\nThis is an interesting paper, I am willing to raise the score if answered properly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0K0bUioIiI", "forum": "TEKOayiQg2", "replyto": "TEKOayiQg2", "signatures": ["ICLR.cc/2026/Conference/Submission4120/Reviewer_dAnD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4120/Reviewer_dAnD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760887200496, "cdate": 1760887200496, "tmdate": 1762917186785, "mdate": 1762917186785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a novel approach for what is described as cover song generation, but should better be denoted as music generation with melody conditioning. The model takes as input the lyrics, a reference melody, and other control features, and generates both singing voice and accompaniment audio. It is built by means of adapting the foundational model ACE-Step. Instance-Adaptive Element-wise Linear Modulation (IA-EiLM) for temporal conditioning, together with Instance-Adaptive Condition Refinement (IACR) to inject conditioning information during diffusion. The proposed model enables coordinated generation of vocals and accompaniment in a stylistically consistent and coherent manner. To support further research in this area, the authors propose a new dataset of synthetic music collected from Suno, denoted Suno70k."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- adaptation of a pretrained linear DiT model to a new control, establishing a new approach for melody conditioning of music generation\n- two rebranded approaches EiLM and ICAR for conditioning\n- a new large dataset of full music, including singing voice and lyrics annotations.\n- successful evaluation of the proposed methods."}, "weaknesses": {"value": "The style of presentation is rather unclear and confusing:\n \n- in the abstract the authors introduce the term cover song generation as: (Line 17) *We formalize this challenge as Cover Song Generation, which requires preserving the source vocal melody while simultaneously synthesizing new vocals and accompaniment, posing higher demands for controllable music generation.* they later change into:  (line 40) *...reinterpret the original’s emotional and stylistic core, evolving a gentle country ballad into a worldwide anthem of deep affection. Such reinterpretations amplify a song’s cultural impact and illustrate the creative potential of musical reimagination.*\n  \nI would stress that the Whitney Houston example demonstrates exactly what the proposed model cannot achieve. Whitney Houston alters phoneme durations, vibrato, intensity, and note transitions to modify the emotional expressivity. Conditioning the model on a fixed melodic contour from the original song is incompatible with this goal. There are clearly ways to use melody conditioning to achieve some sort of reinterpretation. It would be helpful to briefly describe how this could be achieved. (See questions.)\n\n___\n\n- Line 093: *Feature-wise Linear Modulation (FiLM) (Perez et al., 2018) has demonstrated efficacy as a conditioning technique. However, FiLM is limited to injecting global conditions, uniformly modulating all tokens within an instance, making it unsuitable for time-varying features like melodies.*\n \nIt is unclear to me how this claim can be made. While it is true that FiLM is most of the time used for global features, there is nothing in the technique that prevents one from using it to represent time-varying features.  I'd refer to the authors to   \n  1. Temporal FiLM: Capturing Long-Range Sequence Dependencies with Feature-Wise Modulation. (https://proceedings.neurips.cc/paper/2019/file/2afc4dfb14e55c6face649a1d0c1025b-Paper.pdf)\n  2. MODELLING BLACK-BOX AUDIO EFFECTS WITH TIME-VARYING FEATURE MODULATION\n(https://arxiv.org/pdf/2211.00497)\nWhile I agree that these papers do not apply the temporal FiLM to music generation, the statement as given is nevertheless clearly wrong. \n\n___\n- (Line 190) *...that rely on the cross-attention mechanism or element-wise addition, we introduce a more flexible injection mechanism to incorporate melody control into generative models.* \n\nordering the three methods: cross attention (CA), element-wise addition (EA), and temporal FiLM (TF), according to flexibility, I would come to CA is most flexible, TF is second in the list, EA is last. So this statement looks confusing. \n\n___\n- (Line 209) *In addition to external improvements to the condition injection mechanism, we introduce an internal method for improving conditional representations, termed Instance-Adaptive Condition Refinement (IACR).* \n\nFor me the proposed IACR is a multiplicative gating which was introduced in the WaveNet paper in 2016. The application context is different, but the principle is the same. \n\n___\n\n- Line 225 and following: *To the best of our knowledge, existing controllable generation methods derive conditional features solely from the conditional input, overlooking their compatibility with the generative model’s hidden state.* \n\nIt appears to me that in its generality, the statement is incorrect.  The problem you are trying to solve is not controllable generation but control injection, which is adapting an existing pretrained generator with new controls. \n\n___\n- Line 346: *We compare our method... As both methods support only instrumental music generation, we apply them to the same base model, ACE-Step, used in our approach, and ensure consistency in the melody encoder.* \n \nMy problem here is that in all tables, you present the methods using only the reference to the original papers as if you have used these methods unchanged. This is misleading. It appears necessary to present this as ACE-Step + SA Controlent, ACE-Step + MuseControLite. \n\n- Line 360: *Our approach achieves the optimal performance across all metrics.* and line 425 * while achieving optimal performance in controlling vocal melodies.* \n\nThe term *optimal* implies that the solution has been proven to reach the best possible solution of all possible configurations. I do not see such proof."}, "questions": {"value": "I feel the paper is a significant contribution with interesting results. The main weakness is related to incorrect statements and misleading presentation. Despite these positive findings, in my opinion and in its current form, the paper has to be rejected due to the fact that it sells existing technologies as new contributions.  I suggest the following modifications:\n\n- Correctly cite previous approaches that are equivalent to your *EiLM* and *IACR* methods. These are not new and should be correctly introduced as *Temporal FiLM* or *Time-varying FiLM* and *multiplicative gating*. The applicative context is certainly new, and they can therefore be presented as new contributions. If I did get this wrong and there are fundamental differences that warrant a new term, please explain the differences.\n\n- contextualize the example in the introduction and elaborate on the way your method might be used to achieve what Withney Houston did in terms of reinterpretation. I think that you would basically need to get a new melody contour from a singing performance, whistling, or this could even be a performance with another instrument (a saxophone could be interesting). The model would then have to generate suitable phoneme durations, intensity contours, and voice qualities. I think it would be helpful to discuss the limitations of using synthetic datasets to create such reinterpretations. \n\nIn the context of the present article, I feel that the following statement: *We believe that endowing machines with this artistically creative process of reinterpretation represents a promising avenue for empowering music through artificial intelligence.* is unrelated to the proposed method. I would say that if you want to create a model that can reinterpret a song such that it transforms  *a gentle country ballad into a worldwide anthem of deep affection* and if you want the model to learn doing this by itself,  it would be necessary to keep the melody controls incomplete, for example, midi, and rely on the textual descriptions to control how the melody is interpreted. This, however, is not the topic of the present paper.\n\n- Correct or properly contextualize the statement concerning controllable generation and control injection.\n\n- Correct the presentation of the alternative methods in the tables to avoid the misleading impression that you compare to these methods as a whole and not to adaptations of your model that follow the ideas of the references.\n\n- Please reformulate all references to *optimal performance*. Either present a proof that your solution is indeed optimal, or use another term like *the best performance of all methods evaluated*, *superior* compared to the baseline methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TJuLFBDPoZ", "forum": "TEKOayiQg2", "replyto": "TEKOayiQg2", "signatures": ["ICLR.cc/2026/Conference/Submission4120/Reviewer_5LhQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4120/Reviewer_5LhQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761613057463, "cdate": 1761613057463, "tmdate": 1762917186532, "mdate": 1762917186532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper employs an Instance-Adaptive Element-wise Linear Modulation (IA-EiLM) framework to enable controllable song generation. Specifically, Element-wise Linear Modulation (EiLM) improves the temporal alignment of melody control, while Instance-Adaptive Condition Refinement (IACR) enhances the condition representation. To address the scarcity of large-scale, open-source full-song datasets, the authors construct a large-scale dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper proposes Instance-Adaptive Element-wise Linear Modulation (IA-EiLM), which comprises the EiLM and Instance-Adaptive Condition Refinement (IACR), enhancing the condition injection mechanism and conditional representation, respectively.\nThis paper introduces Suno70k, an open-source AI song dataset enriched with detailed annotations, including enhanced tags and lyrics."}, "weaknesses": {"value": "One of the paper's claimed innovations, EiLM, appears to be a relatively trivial extension of FiLM. This leaves IACR as the paper's primary technical insight, which may render the overall technical novelty somewhat limited."}, "questions": {"value": "Can the technique proposed in this paper be extended to autoregressive singing voice synthesis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zYZlFe6NHX", "forum": "TEKOayiQg2", "replyto": "TEKOayiQg2", "signatures": ["ICLR.cc/2026/Conference/Submission4120/Reviewer_ACB8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4120/Reviewer_ACB8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793229119, "cdate": 1761793229119, "tmdate": 1762917185854, "mdate": 1762917185854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the interesting task of cover song generation and proposes a parameter-efficient framework based on ACE-Step to enable melody-controlled song generation. It introduces EiLM, an efficient condition injection mechanism, and IACR, which dynamically adapts conditioning to the model’s hidden states. The authors also release the Suno-70k dataset. The paper is clearly written, and the experiments are thorough and convincing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper formalizes an intersting task—melody-controlled cover song generation—and carefully discusses the limitations of existing condition-injection mechanisms in NAR frameworks. The proposed IA-EiLM achieves superior performance in parameter efficiency, precise temporal control, and melody adherence.\n2. The experiments and ablation studies are comprehensive and well-designed.\n3. The Suno-70k dataset represents a meaningful contribution to the research community.\n4. The presentation is clear, with well-designed figures and an easy-to-follow narrative."}, "weaknesses": {"value": "1. The paper lacks details on the exact form of melody input. If it only uses pitch sequences, how is the alignment between notes and lyrics ensured? Why was this particular representation chosen, and were other melody representations considered?\n2.  How does the model handle conflicts between text tags and melody? Since a melody implicitly encodes stylistic attributes, it would be useful to clarify how such inconsistencies are resolved.\n3.  Although SongEval Aesthetics Metrics are included in the appendix, it remains unclear whether adding melody control compromises ACE-Step’s original generation quality. A demo comparison with ACE-Step would be helpful.\n4.  Minor: In the section “Why is IACR necessary?”, the symbols E, T, and M are undefined, which makes the discussion slightly confusing."}, "questions": {"value": "See Weakness section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CaMf984RhW", "forum": "TEKOayiQg2", "replyto": "TEKOayiQg2", "signatures": ["ICLR.cc/2026/Conference/Submission4120/Reviewer_PTPX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4120/Reviewer_PTPX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896716202, "cdate": 1761896716202, "tmdate": 1762917185555, "mdate": 1762917185555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}