{"id": "3n4uUq30ei", "number": 10581, "cdate": 1758176448777, "mdate": 1759897642394, "content": {"title": "Modular and Adaptive Conformal Prediction for Sequential Models via Residual Decomposition", "abstract": "Conformal prediction offers finite-sample coverage guarantees under minimal assumptions. However, existing methods treat the entire modeling process as a black box, overlooking opportunities to exploit modular \nstructure. We introduce a conformal prediction framework for two-stage sequential models, where an upstream predictor generates intermediate representations for a downstream model. By decomposing the overall prediction residual into stage-specific components, our method enables practitioners to attribute uncertainty to specific pipeline stages. We develop a risk-controlled parameter selection procedure using family-wise error rate (FWER) control to calibrate stage-wise scaling parameters, and propose an adaptive extension for non-stationary settings that preserves long-run coverage guarantees. Experiments on synthetic distribution shifts, as well as real-world supply chain and stock market data, demonstrate that our approach maintains coverage under conditions that degrade standard conformal methods, while providing interpretable stage-wise uncertainty attribution. This framework offers diagnostic advantages and robust coverage that standard conformal methods lack.", "tldr": "We introduce a residual decomposition framework for two-stage models to produce robust and stage-aware conformal prediction intervals.", "keywords": ["conformal prediction", "two-stage", "uncertainty quantification"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1227b3b47313b01f0bdded22d27f26321388b7dc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In many cases, prediction is undertaken as a multi-stage task. While naively leveraging conformal prediction on the end-to-end prediction does yield valid uncertainty bounds, doing so forgoes the granularity of knowing the decomposition of the source of the errors. That is, for a two-stage pipeline, for instance, one cannot determine how much of the resulting uncertainty is the result of the upstream model and how much is from the downstream model. This paper proposes an approach to determine this decomposition and do so in a way that the two bounds can be combined to yield an informative, valid upper bound. In particular, they propose an automated search procedure that automatically tunes the selection of parameters to yield maximally informative prediction regions while also controlling for risk. They finally demonstrate how the method empirically yields improvements across a number of experiments."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper clearly identifies a flaw in existing conformal approaches for uncertainty estimation for multi-stage pipelines. The proposed solution is elegantly presented with the gradual build-up from the initially posited version where the parameters combining the quantiles of the two separate stages are fixed to the version with the automatic tuning of such parameters for optimal parameter selection to the version that similarly does auto-tuning but instead for online adaptation. The theorems are also clearly stated and well motivated by the overall storyline, with the proofs looking sound."}, "weaknesses": {"value": "The main weakness is in the experimental validation: of the three main experiments, two were synthetically generated circumstances. Nonetheless, the comparisons to alternate methods of adaptation appear fairly comprehensive: including another “real” dataset would strengthen the submission, however."}, "questions": {"value": "None other than: how does the model stack up in other real-world, time series data settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SJvLEj9MM5", "forum": "3n4uUq30ei", "replyto": "3n4uUq30ei", "signatures": ["ICLR.cc/2026/Conference/Submission10581/Reviewer_SdKo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10581/Reviewer_SdKo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761085663155, "cdate": 1761085663155, "tmdate": 1762921848856, "mdate": 1762921848856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is clearly written and the modular framing is easy to follow. The Figures illustrate the main ideas clearly. \n\nProblem context: Existing conformal prediction (CP) methods typically treat the entire prediction pipeline as a black box, ignoring modular structures such as two-stage sequential models where upstream representations feed downstream predictors. The paper aims to design a stage-aware CP framework.\n\nMotivation: Distribution shifts often affect pipeline stages asymmetrically, e.g., upstream sensors may drift while downstream mappings remain stable. Standard conformal methods cannot localize such effects, forcing full retraining. The proposed stage-wise abstraction aims to isolate these shifts by embedding stage-specific uncertainty into conformal prediction. This is conceptually relevant.\n\nPaper's proposal: The authors introduce a decomposition of the overall residual into an upstream and a downstream component, claiming this enables uncertainty attribution to specific stages. They further propose a risk-controlled parameter selection procedure using family-wise error rate (FWER) control, and an adaptive variant for non-stationary or distribution-shifted settings.\n\nMethodology: Intervals are constructed by summing or scaling quantiles of the decomposed residuals. FWER is used to select scaling parameters over a validation grid, and an adaptive version updates parameters over time. The paper asserts that this yields interpretable, stage-wise uncertainty attribution and robust coverage under shift. \n\nExperiments under simulated and real-world distribution shifts are presented."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Paper raises a practically relevant issue of sequential/modular nature of ML frameworks and lack of stage-aware CP frameworks and uncertainty attribution methods.  \n\n2. Despite dense notation, the paper is generally well-structured and easy to follow, with experiments and diagrams that communicate the intended intuition clearly.  \n\n3. The residual decomposition could serve as a heuristic diagnostic tool to identify which stage of a pipeline contributes most to predictive uncertainty.  \n\n4. In particular, adaptive risk control with residual decomposition is practically appealing, as demonstrated through real-data experiments."}, "weaknesses": {"value": "1. The proof of the Proposition 2 seems incorrect. It relies on the intermediate value theorem, and by noting that $(a = 0,b = 0)$ attains coverage of $0$, while $(a = 1,b = 1)$ attains coverage of $1 - 2\\alpha$. Both these numbers are strictly less than $1 - \\alpha$. So, intermediate value theorem doesn't apply to say that there exist $(a^{\\*}, b^{\\*})$ which attains coverage of $1 - \\alpha$.\n\n2. The paper claims to address non-exchangeable or distribution-shifted data, yet all coverage results (Theorem 1, 2, and Corollaries) are proved under classical exchangeability/IID. The adaptive and FWER schemes depend entirely on IID calibration samples. This is a fatal conceptual inconsistency: the method cannot claim robustness to shift when its only theoretical foundation is exchangeable-CP validity. \n\n3. In Lines 104-120, the authors explicitly assume exchangeability of train, calibration, and test points, as well as distribution shift. This self-contradiction invalidates much of the paper’s stated novelty and confuses the reader about what regime the guarantees actually cover. If points are exchangeable, there is no shift; if there is a shift, the proofs no longer hold. This contradiction makes the entire problem formulation incoherent.\n\n4. Experiments are small-scale, omit modern baselines which target distribution shifts such as CQR, and do not discuss computational costs, especially, when using FWER framework.  \n\n\n----\n\nWhile the modular decomposition idea is intuitively appealing and could serve as a useful diagnostic tool for stage-wise uncertainty assessment, the paper’s theoretical contributions are weak and often inconsistent with its claims. All guarantees rest on exchangeability, yet the empirical studies focus on non-exchangeable, distribution-shifted settings where coverage consistently falls below the nominal level. Consequently, the central claim of robustness to shift is substantially overstated. The method’s true value lies in its potential practical interpretability, it could help practitioners identify and retrain unstable components of a multi-stage model, rather than in offering new theoretical insight or provable robustness.\n\nOverall, the incorrect proof of Proposition 2, contradictions between assumptions and stated goals, and the lack of convincing empirical evidence, leads to low score."}, "questions": {"value": "1. In Lines 481-482, The paper states that the “method maintains coverage under shifts”, yet all reported results show coverage well below the nominal level (e.g., 0.80-0.84 for a 0.90 target in Table 1). Shouldn’t the claim be that the method fails less severely than others rather than maintaining coverage? Please clarify this wording and the intended meaning of “maintains”.\n\n2. Sections are mislabeled (Appendix A.1 claims to prove Proposition 1 but actually proves Proposition 2).\n\n3. In Line 219, The sentence reads “we observe that `both can methods’ yield…”. This seems to be a typographical error; please correct.\n\n4. In case of Automobile indicators dataset, $\\delta$ is chosen to be $0.3$ appears unusually large for a probability bound. What motivates this choice? How sensitive are the results to smaller values $\\delta = 0.1, 0.05$? Reporting these would help assess robustness.\n\n5. The name “Candès” is inconsistently rendered as “Candes” in several places. Please standardize the spelling throughout."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ToGVqnT02r", "forum": "3n4uUq30ei", "replyto": "3n4uUq30ei", "signatures": ["ICLR.cc/2026/Conference/Submission10581/Reviewer_D8zB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10581/Reviewer_D8zB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761197702218, "cdate": 1761197702218, "tmdate": 1762921848410, "mdate": 1762921848410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes modular, adaptive conformal prediction (CP) for two-stage pipelines (and claims it can extend to multi-stage settings). The key idea is to decompose the end-to-end residual into an upstream component (delta R_1) and a downstream residual (R_2), and to construct prediction intervals by combining the component quantiles with scaling weights.\n\nA central issue is that general coverage guarantees do not hold when using arbitrary scaling weights (a, b). To address this, the authors propose selecting (a, b) on the calibration set to achieve a desired coverage target, following the risk-controlling approach from Bates et al. (2021) and Angelopoulos et al. (2021, 2022). Specifically, they apply an FWER-controlled multiple testing procedure over a candidate grid of (a, b).\n\nIn addition, the paper proposes an adaptive variant that essentially follows the approach of Gibbs and Candès (2021) to handle non-stationary data by updating the intervals over time. Handling such distribution shifts appears to be the paper's central message and main experimental focus."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of decomposing the end-to-end residual into upstream and downstream components is conceptually appealing and relevant for modular pipelines. The motivation of asymmetric shifts is well argued.\n- When a = b = 1, delta R_1 and R_2 are transparently defined and lead to a simple, interpretable coverage guarantee of ≥ 1 − 2α.\n- The adaptive mechanism provides a clear way to adjust (a, b, c, d, α) over time, with a long-run coverage statement and stage-aware signals determining which component to update."}, "weaknesses": {"value": "- Section 4 introduces a, b, c, d, but their roles and how to choose them are never explained clearly. Readers only gather that c and d are component-wise quantile levels and a and b are scaling weights. However, there is no practical guidance (e.g., defaults, grids, or heuristics) before moving straight into the risk-control framework.\n- After noting that general (a, b) coverage is hard to guarantee, the paper abruptly transitions to the FWER selection, and then directly to distribution shift experiments. Although shift robustness is motivated earlier, the connection between the FWER-based method and robustness to shifts is not clearly explained, creating a disjoint flow.\n- While the method is said to generalize to multi-stage pipelines, this seems largely infeasible in practice, as the search space of parameter combinations grows exponentially with the number of stages.\n- The approach is overall conservative (wide intervals and abstentions). Although τ is proposed as a way to balance coverage and efficiency, the discussion remains qualitative, leaving practitioners unsure about practical defaults.\n- The method introduces many hyperparameters (Λ grid, τ, δ, k, γ, η, c, d), adding complexity. While ablations exist, the main text could more effectively summarize robust or recommended settings"}, "questions": {"value": "- What exactly is meant by “separate component quantiles” (what is its purpose), and how should these be chosen in practice?\n- In the first non-adaptive experiment under a non-stationary setting, most baselines (except WSC) are not designed for time-varying data, including your own non-adaptive approach. The improved coverage you report seems mainly due to the method’s inherent conservativeness, which other methods can also achieve by using a smaller significance level. Additionally, does the calibration set expand over time or remain fixed? If it is fixed, the WSC baseline loses its main advantage (adaptive weighting).\n- In the adaptive experiments, I would like to see a baseline where the upstream stage is removed, and a single model directly predicts y, followed by applying ACI, PID, or OCID. Currently, the setup treats the two-stage pipeline as a black box, making it unclear how much of the benefit comes from the modular decomposition.\n- How sensitive are your conclusions to misspecified intermediate representations x (e.g., if x is noisy or partially observed)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0pckDid9YT", "forum": "3n4uUq30ei", "replyto": "3n4uUq30ei", "signatures": ["ICLR.cc/2026/Conference/Submission10581/Reviewer_tyc2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10581/Reviewer_tyc2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846494783, "cdate": 1761846494783, "tmdate": 1762921847981, "mdate": 1762921847981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel conformal method that decomposes the residual into two stages. For the training pipeline with multiple training stages, the method offers interpretability while preserving long-run coverage guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea to decompose the uncertainty into relevant components based on the stages of the training pipeline is interesting and can offer better insight into modelling or fine-tuning under distribution shift."}, "weaknesses": {"value": "The biggest weakness is the comparison with only one significance level. To best judge a conformal method, it is imperative to compare its performance across different significance levels, and plotting a calibration curve is particularly helpful.\n\nSee \"Questions*\" below for more"}, "questions": {"value": "1. Line 50: \"interpretable intervals\": I believe the interpretability here refers to the decomposition. It could be misinterpreted as rectangular regions if the response variable is multivariate.\n\n2. Line 104-116: Different properties, such as Exchangeability and Distribution Shifts, start suddenly; consider numbering them as done later in the paper.\n\n3. Line 137: The term attribution appears odd. I believe the authors wanted to imply decomposition. \n\n4. If I understood correctly, the interval provided in Definition 4 is a direct application of the triangle inequality mentioned earlier. This seems to create the interval that would lead to overcoverage more often than not. Is it also possible to have exact coverage guarantees in such a case?\n\n5. In Proposition 2, there could be multiple sets of (a, b) that satisfy the coverage guarantee; is that correct?\n\n6. One slight issue  I feel in this setup is the efficiency of the conformal prediction, while this decomposition of uncertainty is helpful, if we lose efficiency, it may not be worth.\n\n7. Line 100-101: This is the first time I have heard of a conformal set. Typically, the role that you conformal set has is done by the calibration set. However, it appears you needed an extra split in this case for tuning the parameters. Does it make the method data-inefficient?\n\n8. The long-term guarantee seems to be the same as in Gibs and Candes 2021. Is that correct?\n\n9. Line 372: 'Concept Shift\"?  Did you mean 'distribution shift'?\n\n10. Line 406,/423: \"Distribution Shocks\"/ \"upstream shocks\"? Did you mean 'distribution/upstream shift' again?\n\n11. The target confidence is set as high as 0.9, which is okay. However, it is necessary to demonstrate the performance of the proposed method with varying significance levels. A calibration curve may be helpful in determining if the proposed method works effectively in all cases."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hXBhlAJWCF", "forum": "3n4uUq30ei", "replyto": "3n4uUq30ei", "signatures": ["ICLR.cc/2026/Conference/Submission10581/Reviewer_rQHP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10581/Reviewer_rQHP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10581/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031469680, "cdate": 1762031469680, "tmdate": 1762921847500, "mdate": 1762921847500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}