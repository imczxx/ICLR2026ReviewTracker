{"id": "dRwsN1DvNV", "number": 3572, "cdate": 1757477976759, "mdate": 1759898080731, "content": {"title": "AI’s Visual Blind Spot: Benchmarking MLLMs on Visually Smuggled Threats", "abstract": "Visual Smuggling Threats (VSTs) spread illicit information by embedding concealed or encrypted text within seemingly innocuous images, adversarially evading automated moderation and proliferating across online platforms, while the effectiveness of recent Multimodal Large Language Models (MLLMs) in identifying VSTs to safeguard online security remains underexplored. To bridge this gap, we construct VST-Bench, a benchmark for comprehensively evaluating models’ ability to detect diverse VSTs. It encompasses three major challenges, i.e., Perceptual Difficulty, Reasoning Traps, and AI Illusion, which are further divided into ten subcategories, and includes 3,400 high-quality samples collected from real smuggling scenarios or synthesized by replicating smuggling workflows. Evaluation of 29 mainstream MLLMs on VST-Bench shows that existing models perform poorly in judging violative images. The state-of-the-art open-source model Gemma-3-27B achieves only 32.67% F1 on the challenging AI Blended Background category, and even the proprietary Gemini-2.5 Pro reaches just 46.32%, indicating that current MLLMs are far from reliably preventing the spread of harmful content in real-world deployment. Through an in-depth analysis of failure cases, we discover three core challenges posed by VSTs: (1) Perceptual Failure on Subtle Threats, (2) Reasoning Failure on Semantic Puzzles, and (3) Recognition Failure against AI Illusions. We will release the dataset and evaluation code of VST-Bench to facilitate further research on VST and the broader online risk content recognition.", "tldr": "", "keywords": ["MLLMs；Visually Smuggled Threats；MLLMs Safety；OCR"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a243776b9c5b043a0f15bc417663864f1b1ca971.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces VST-Bench, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to detect Visually Smuggled Threats (VSTs) that are illicit or harmful textual content hidden within benign-looking images. The study defines the VST recognition task, where models must both extract hidden violation items and classify whether an image contains malicious content. Extensive experiments over 29 mainstream MLLMs demonstrate that existing MLLMs are vulnerable at discerning the maliciousness of such VST Images, underscoring the need for enhancing intrinsic model robustness."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper investigates a potency of visually-smuggling strategy for attacking MLLMs under realistic images and attack scenarios.\n- The authors introduced a new benchmark comprising 3k images over various visual smuggling strategies, under a rigorious manual annotation and review process.\n- Extensive experiments over 29 MLLMs corroborate the paper's main claims where most MLLMs fail at recognizing the harmful contents and hence discerning its maliciousness."}, "weaknesses": {"value": "- While the paper underscores MLLMs’ vulnerability to OCR-based and blending-based visual smugglings, similar findings were already explored in FigStep [R1] and JOOD [R2], rendering the contribution incremental in nature. Apart from this, the paper does not offer any other theoretical or technical innovations.\n- While the paper mainly emphasizes that VSTs can evade detection, it remains unclear whether such undetected inputs would actually lead to harmful or jailbreak outputs in practice. If the MLLM fails to recognize any harmful visual content within a VST image, it is uncertain whether the model would consequently generate unsafe responses related to the undetected content. Moreover, even when a VST image bypasses input-side detection, response-level moderation or safety filters can still intercept harmful generations at the output stage. Therefore, input-side detection failure alone does not equate to a jailbreak or real-world safety breach.\n- The low accuracy of even state-of-the-art models in maliciousness classification may be due to the absence of contextual grounding in the dataset. The models might have correctly recognized strings such as “qq257831”, but without any contextual cues indicating that these are user IDs used for off-site redirection, they would reasonably interpret them as random benign characters. In this sense, such cases may not represent true safety failures, but rather limitations in how “harmful” content is defined and contextualized in the benchmark.\n- The paper needs significant improvement in visibility and readability, which currently undermines its overall clarity and credibility: In Figure 2, there are a bunch of expressions and notations without any descriptions or references in the manuscript (e.g., $w_h$ in stage 2, to name a few) and the internal graphs and figures are too small to be read. In Figure 3, what is MonsterQR? Is it blending model? or is the blending just a simple mixup? Are the composition images composited by the AI models?\n\n[R1] FigStep: Jailbreaking Large Vision-Language Models via Typographic Visual Prompts\n\n[R2] Playing the Fool: Jailbreaking LLMs and Multimodal LLMs with Out-of-Distribution Strategy"}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KYoNpTJJEL", "forum": "dRwsN1DvNV", "replyto": "dRwsN1DvNV", "signatures": ["ICLR.cc/2026/Conference/Submission3572/Reviewer_g3Lk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3572/Reviewer_g3Lk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760949939744, "cdate": 1760949939744, "tmdate": 1762916834663, "mdate": 1762916834663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a benchmark dataset VST-BENCH, which contains 3,400 high-quality security blind spots for evaluating multimodal large models in identifying Visual Smuggling Threats. This benchmark categorizes Visual Smuggling Threats into three core challenges: Perceptual Difficulty, Reasoning Traps, and AI Illusion, and subdivides them into ten subcategories. The author conducted large-scale sample evaluations on 29 mainstream multimodal large models and analyzed the failed cases. The experiments were thorough and the analysis was reliable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This paper clearly expounds the motivation, constructs VST BENCH for Visual Smuggling Threats, clarifies the characteristics of the ten subcategories, and conducts multi-faceted comparative analysis with the existing benchmarks.\n\n2.This article clearly introduces the process of data collection, including two sources: Mining In-the-Wild Threats and Replicating AIGC-based Threats. The data went through the Rigorous Annotation and Review Process, resulting in a dataset containing 3,400 high-quality samples.\n\n3.This paper conducts thorough experiments and analyses on VST BENCH, and performs two tests, namely Violation Judgement task and Violation Item Extraction, on 29 mainstream multimodal large models. Experiments show that the existing multimodal large models are still unable to reliably prevent the spread of harmful content.\n\n4.This paper presents a large number of VST BENCH cases, including the designed prompts, the input images and outputs of the model, and the real answers. Through the failure cases, it clearly presents the specific vulnerabilities and challenges that multimodal large models face when confronted with adversarial technologies."}, "weaknesses": {"value": "1.This article presents the results of human experts, who can accurately identify all VST security vulnerabilities. However, compared with multimodal large models, human experts also have corresponding shortcomings, such as time and whether information is obtained by magnifying images. Therefore, if the images are processed, such as by increasing the resolution and other methods, should the prediction results of the multimodal large model also improve accordingly?\n\n2.This article classifies the cases, including perception failure, reasoning failure, and recognition failure. However, based on the experimental data, it can be seen that different models perform significantly differently for different tasks. Therefore, further analysis is needed to determine the correlation between the task and the relevant information of the model, such as the architecture or pre-training data."}, "questions": {"value": "1.Can relevant processing such as enhancing the resolution of images improve the test results of multimodal large models? It is necessary to clarify whether the poor results of the model are due to the image itself or the model itself.\n\n2.Further analyze the reasons for the performance differences of different models in handling related tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wvjpID7O9N", "forum": "dRwsN1DvNV", "replyto": "dRwsN1DvNV", "signatures": ["ICLR.cc/2026/Conference/Submission3572/Reviewer_mepW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3572/Reviewer_mepW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619744469, "cdate": 1761619744469, "tmdate": 1762916834395, "mdate": 1762916834395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper formalizes a task the authors call Visually Smuggled Threats (VSTs) recognition, which addresses the problem of illicit information being embedded within images to evade automated moderation. To study this, they construct a new benchmark called VST-Bench, which includes 3,400 samples across ten subcategories, such as microtext, cryptic puzzles, and AI-generated illusions. The authors then evaluate 29 existing Multimodal Large Language Models (MLLMs) on this benchmark. The results indicate that current models perform poorly at this task, especially on the \"AI Illusion\" categories, and the paper concludes by analyzing the primary failure modes observed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "I think the paper addresses a problem that is highly relevant to real-world content moderation systems. The formalization of \"Visually Smuggled Threats\" (VSTs) as an input-side security problem is a useful way to frame this challenge, distinguishing it from more commonly studied output-side safety issues.\n\nThe authors have also put in the effort to build a new benchmark, VST-Bench, to evaluate this task. Personally, I find the taxonomy of 10 different smuggling techniques (like 'Microtext', 'Cryptic', and 'AI Blended') to be well-organized and reflective of the diverse challenges models face. The inclusion of 3,400 samples from both real-world collection and synthesized replication seems like a reasonable approach to cover different scenarios.\n\nFinally, the paper provides a large-scale evaluation of 29 mainstream MLLMs. This provides a clear baseline for how current SOTA models perform on this specific task, and the results showing poor performance, especially on AI-generated illusions, are informative. The analysis of the three primary failure modes (perceptual, reasoning, and recognition failure) is a helpful breakdown of the core difficulties."}, "weaknesses": {"value": "First, I should say that I'm not an expert in benchmark construction, so my feedback here might not be perfectly on target. However, I do have a few points that I think could be considered for improving the work.\n\nPersonally, I found the scope of the benchmark, while well-defined, to be somewhat narrow. The entire VST-Bench is constructed around the single scenario of \"malicious off-site redirection\". The authors state this is for feasibility and its real-world relevance, which is understandable. However, the paper introduces the broader concept of smuggling \"illicit information\". I wonder if the conclusions drawn from this specific redirection task (which is often about finding contact info) would generalize to other, perhaps more semantic, VSTs, such as smuggled hate speech, incitement, or disinformation.\n\nI also had some thoughts on the dataset scale. The benchmark contains 3,400 samples in total, which are then divided into ten subcategories. According to Table 12, this means some categories like \"Dense Text\" or \"Cryptic Substitution\" have only 200 samples each (100 positive, 100 negative). I am not entirely sure if 100 positive examples are sufficient to reliably claim a model has a \"blind spot\" in a specific area, as the results might be sensitive to the specific handful of examples chosen for that small test set.\n\nAnother point is that the paper focuses exclusively on zero-shot evaluation. This is a valid way to test the out-of-the-box capabilities of these MLLMs. However, it leaves me wondering whether the poor performance is a fundamental architectural failure or simply a data-gap problem. It would have been very informative to see an experiment where a model (perhaps one of the open-source ones) is fine-tuned on a small portion of the VST-Bench. If a model can learn to detect these threats easily with just a few examples, it would change the interpretation of these \"blind spots.\""}, "questions": {"value": "I have a few questions for the authors, and their answers could help clarify some of the paper's limitations and my understanding of the results.\n\nFirst, I was wondering about the decision to focus the entire benchmark on the single scenario of \"malicious off-site redirection\". You state this was for feasibility and real-world relevance, which is a fair reason. I am just curious about the generality of the findings. Do you believe the \"blind spots\" identified are specific to detecting contact information, or would you expect models to similarly fail at detecting other forms of semantically smuggled illicit content, such as cryptic hate speech or disinformation that might rely on different types of reasoning?\n\nMy main question is regarding the zero-shot evaluation setting. The models, even the proprietary ones, clearly perform poorly on this benchmark, especially in the AI Illusion categories. I'm left wondering if this poor performance points to a fundamental architectural failure or if it's more of a data-gap issue. Have you considered running a simple fine-tuning experiment? For example, what happens if you fine-tune one of the open-source models on even a small portion of your VST-Bench data? If the model's performance on the held-out test set improves dramatically, it would suggest this is a problem that can be readily solved with data. If it *still* struggles, it would make your claim about a core \"blind spot\" much stronger.\n\nFinally, I had a question about the prompt template you used, which is detailed in Appendix B.2. It's very specific, providing violation definitions and requiring a strict JSON output. I wonder how sensitive the model performance is to this particular prompt? Have you tried a simpler, more open-ended prompt (e.g., \"Analyze this image for any hidden or smuggled messages that might be malicious\")? It would be helpful to understand if the failures are tied to the complex task formatting or to the core perceptual and reasoning challenges themselves."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e957KywRcn", "forum": "dRwsN1DvNV", "replyto": "dRwsN1DvNV", "signatures": ["ICLR.cc/2026/Conference/Submission3572/Reviewer_9Nqx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3572/Reviewer_9Nqx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654745192, "cdate": 1761654745192, "tmdate": 1762941151078, "mdate": 1762941151078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on Visually Smuggled Threats (VSTs)—harmful content that embeds concealed or encrypted illicit text in seemingly benign images to evade automated moderation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Critical Research Focus: The paper addresses a timely and understudied security gap—VSTs."}, "weaknesses": {"value": "1. Serious Bias in Sample Distribution: The benchmark has a balanced 1:1 ratio of positive/negative samples, which may not reflect real-world content distribution (where benign images are far more common than VSTs). This could lead to overestimated model performance in practice, as real-world content moderation systems face extreme class imbalance. The paper should acknowledge this discrepancy and discuss its impact on evaluation validity.\n\n2. Limited Generalization to Other VST Scenarios: The benchmark is grounded in a single real-world scenario—malicious off-site redirection (embedding contact info to lure users to third-party platforms). \n\n3. Insufficient Analysis of Model-Specific Strengths/Weaknesses: The evaluation focuses on aggregated performance (e.g., open-source vs. closed-source averages) and top/bottom models but lacks in-depth analysis of why specific models perform better/worse. \n\n4. Lack of Baseline Comparisons with Specialized Detection Models: The paper only compares MLLMs with human experts and random guesses, but not with specialized VST detection models. Without this comparison, it is unclear whether MLLMs are inherently less suitable for VST detection or if the gap stems from insufficient optimization for this task.\n\n5. Limited Discussion of Mitigation Strategies: While the paper identifies three core failure modes, it provides only high-level directions for future research without concrete preliminary experiments or proof-of-concept mitigation strategies."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gk8wKLSKtH", "forum": "dRwsN1DvNV", "replyto": "dRwsN1DvNV", "signatures": ["ICLR.cc/2026/Conference/Submission3572/Reviewer_nWn2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3572/Reviewer_nWn2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946814663, "cdate": 1761946814663, "tmdate": 1762916833903, "mdate": 1762916833903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}