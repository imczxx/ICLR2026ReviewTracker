{"id": "Pf6Vbl4r9k", "number": 7985, "cdate": 1758049074942, "mdate": 1759897817455, "content": {"title": "Activation-Deactivation: General Framework for Robust Post-hoc Explainable AI", "abstract": "Black-box explainability methods are popular tools for explaining the decisions\nof image classifiers. A major drawback of these tools is their reliance on mutants\nobtained by occluding parts of the input, leading to out-of-distribution images.\nThis raises doubts about the quality of the explanations. Moreover, choosing an\nappropriate occlusion value often requires domain knowledge. In this paper we\nintroduce a novel forward-pass paradigm Activation-Deactivation (AD), which\nremoves the effects of occluded input features from the model’s decision-making\nby switching off the parts of the model that correspond to the occlusions. \nWe introduce CONVAD, a drop-in mechanism that can be easily added to any trained Con-\nvolutional Neural Network (CNN), and which implements the AD paradigm. This\nleads to more robust explanations without any additional training or fine-tuning.\nWe prove that CONVAD mechanism does not change the decision-making process\nof the network. We provide experimental evaluation across several datasets and\nmodel architectures. We compare the quality of AD-explanations with explana-\ntions achieved using a set of masking values, using the proxies of robustness, size,\nand confidence drop-off. We observe a consistent improvement in robustness of\nAD explanations (up to 62.5%) compared to explanations obtained with \nocclusions, demonstrating that CONVAD extracts more robust explanations without the\nneed for domain knowledge.", "tldr": "The paper presents a framework, called activation-deactivation, that replaces masking values in perturbations of inputs with deactivation of the relevant elements of the neural network.", "keywords": ["explainable AI", "black-box explainability", "post-hoc explanations", "CNN"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88267abb686ce1eee3c8fd8f0cf55c7914c535ba.pdf", "supplementary_material": "/attachment/21fad488ac0fffca0e85bf91e393ec825e5396cc.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a method called Activation-Deactivation to address the issue of OOD masked examples in mask-based explanation methods. This novel paradigm forces the model to ignore occluded parts of the input by deactivating activations at each layer of the model that correspond to the occluded features. By doing so, it halts the propagation of effects to subsequent layers. Experimental results show that the proposed method improves the robustness of explanations compared to those obtained using occlusions with min, max, avg, or zero pixel values."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tCritical Issue Addressed: The paper tackles the significant issue of OOD inputs caused by directly masking the input. Such inputs often fail to reflect the model’s decision-making process accurately, which is a critical challenge for black-box explainers.\n\n2.\tTheoretical Support: The methodology is theoretically grounded, with theorems and proofs provided."}, "weaknesses": {"value": "1. While the method aims to address issues in black-box explainers, the Activation-Deactivation process requires access to the model architecture. This effectively transforms the method into a white-box explainer, raising concerns about evaluation fairness since other baselines in the comparison do not have access to the model architecture.\n\n2. The evaluation section lacks clarity and detail, leading to confusion.\n\n    2a. K1 and K2 are mentioned in Section 5.1, but only results related to K1 are reported. If K2 (images from a different class in the same dataset) is not evaluated, why is it mentioned?\n\n   2b. The process of building IID backgrounds is not described in sufficient detail.\n\n    2c. The robustness evaluation process is unclear. Based on the paper, my understanding is that given solid-colored images as backgrounds, the original image pixels are inserted into the background according to ranks identified by the explainer. If this is correct, more description of the evaluation process is necessary.\n\n3. Generality: The method is limited in scope. It is only conducted and evaluated on CNNs and a single black-box explainer, REX. Its generalizability to other vision models (e.g., ViTs) and other black-box explainers remains unknown.\n\n4. Lack of Visual Examples and User Study:\n\n    The paper provides only one visual example, which is insufficient for an XAI method for image classification.\n\n    No user study is conducted to evaluate interpretability for human users, which is a key aspect of XAI as it aims to help users understand the model's decision-making process.\n\n5. The paper does not discuss or compare its method with prior related works, such as: \n\n    \tExplaining by Removing: A Unified Framework for Model Explanation (JMLR)\n\n    \tOn the Robustness of Removal-Based Feature Attributions (NeurIPS 2023)\n\n    \tCan We Faithfully Represent Masked States to Compute Shapley Values on a DNN? (ICLR 2023)"}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SXcFkh7CvJ", "forum": "Pf6Vbl4r9k", "replyto": "Pf6Vbl4r9k", "signatures": ["ICLR.cc/2026/Conference/Submission7985/Reviewer_QVJX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7985/Reviewer_QVJX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760793558644, "cdate": 1760793558644, "tmdate": 1762919993240, "mdate": 1762919993240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Re: the question of generalization to other architectures (namely, transformers)"}, "comment": {"value": "Wrt the proposed extension to the AD framework, we would like to point out that the paper introduces the general AD framework and an algorithm and its implementation for CNNs. We simply don’t have space to add the transformers-based algorithm to the same paper, so we defer it to the future work."}}, "id": "G59ri67RKq", "forum": "Pf6Vbl4r9k", "replyto": "Pf6Vbl4r9k", "signatures": ["ICLR.cc/2026/Conference/Submission7985/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7985/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7985/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763047373462, "cdate": 1763047373462, "tmdate": 1763047373462, "mdate": 1763047373462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a mechanism called CONVAD that removes the effects of occluded input features typically used by feature attribution methods to estimate feature importance given a prediction. The idea is to modify the forward pass of a convolutional neural network by deactivating neuron outputs (i.e., setting them to zero) associated with occluded features. Starting from the mask produced by an occlusion method, the mechanism updates the mask at each step of the forward pass by considering the ratio of masked to unmasked regions within each neuron’s receptive field. The mechanism is tested over the ReX feature attribution method across three architectures and three datasets, comparing it with alternative ways to occlude the input. The authors evaluate the resulting ReX explanations in terms of robustness, size, and confidence changes induced in the model by the modified forward pass, finding that the explanations computed by their proposed mechanism are more robust than those produced by alternative occlusion strategies (i.e., occlusion values)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- To the best of my knowledge, the idea is novel \n- In the specific case of ReX, the performance of the proposed method is more consistent across several datasets than with alternative occlusion values. Avoiding the problem of choosing the right occlusion value per dataset is a nice contribution."}, "weaknesses": {"value": "1) There is a general **mismatch between the contributions and claims and what is currently demonstrated in the paper** regarding solving the OOD problem and providing guarantees about the decision-making process. Specifically:\n   - The paper states that *“We prove that CONVAD mechanism does not change the decision-making process of the network”*. In this regard, the paper proves that when no masks are provided, the decision process is the same (Theorem 1), but the central use case of the paper is **when masks are provided as additional input, in which case there is no guarantee**. In fact, **the decision process, intended as the model’s response to a given input, is guaranteed to change** because the method actively modifies that response by setting neurons in previous layers to zero; this can be verified by comparing the activations of the untouched neurons before and after applying CONVAD to check whether they differ: if they differ, even marginally, then the decision process is different. The fact that the original and modified forward passes yield the same prediction (or the same confidence) does not guarantee that the decision process is the same, nor does it imply that the explanations must be the same.\n   - Authors claim to solve the OOD problem. **The OOD problem pertains not only to the input but also to the network’s response**; as the authors observe, *“if the occlusion value carries semantic meaning for the current model and current input dataset, the results might not reflect the model’s reasoning on the original input”* and the same concern applies to any change in the network response: if the edit carries semantic meaning for the current model, the results might not reflect the model’s reasoning on the original input. In this regard, the proposed method modifies the decision process and does not provide any guarantee that the modified response is in-distribution with respect to the activation distributions during training\n\n2) Limited Evaluation Setup (Generalization): The paper claims to “solve both the OOD and the occlusion value problems” of occlusion-based black-box interpretability methods (in general). However, the experimental setup **tests the proposed method only on explanations computed by the ReX framework**. Therefore, at the current stage, the proposed method can be seen as an improvement to the ReX framework rather than a more general paradigm to “obviate the need for occlusions in post hoc explainability.” If the authors would like to generalize their claim, they should provide evidence that, when applied to a wide range of masks produced by different explanation methods, the results are consistent.\n\n3) Custom evaluation Setup (Metrics): There is a large body of literature on the evaluation of feature attribution methods and, more generally, on occlusion-based methods. The paper proposes its own evaluation setup by *“planting” explanations onto different randomly selected colors and background images*. In this regard, it is unclear how this setup differs from standard ones used in the literature, what its advantages are, and **why standard procedures and standard metrics used in the literature for feature attribution (e.g., fidelity/faithfulness/del scores, etc) cannot be used in this context**. For example, several notions of robustness already exist in the literature, as the authors note; it is not clear **why a new definition of robustness is needed** and why other definitions cannot be applied; in a few words, the paper lacks contextualization regarding the evaluation setup. In this context, it is unclear why the size of the area of the explanations and the change in confidence of the model should indicative of the quality of explanations.\n\n4) The **“Related Work” section is limited** and does not properly contextualize the proposed method within the appropriate literature. There is little discussion or mention of the substantial body of work dealing with occlusions/perturbations for post hoc methods (e.g., meaningful perturbations) and the literature on the evaluation of similar methods. This point is connected to the previous ones.\n\n5) There are **several sentences in the paper that are not clear or questionable**. Some examples are the following:\n   - “It only needs to be performed after a set of parametrized operations which have downstream effects in calculating future intermediate representations (such as pooling, concatenation, convolution etc.)”\nWhich operations do not have downstream effects?\n   - Authors state that *“For this image of an ibex, the AD explanation clearly matches our intuition, as it consists of the ibex’ head with its unique-looking horns. In contrast, explanations obtained using different common occlusion values show small areas of the background and contain almost no part of the ibex.”*\nThere is a large body of literature indicating that explanations need not align with human intuition but should align with model behavior, and the reverse can cause overtrust and misleading explanations; moreover, several papers show that models similar to those tested by the authors may exploit unrelated features (e.g., small background parts) for the predictions. Therefore the visual difference is not very indicative of (or does not guarantee) improvements in the explanation quality since it depends on the decision process of the model.\n   - The paper states *“ While we have not performed a user study, and it is out of scope for this paper, larger and more robust explanations generated by CONVAD are likely to increase trust in both an explanation and a model.”* This point is connected to the evaluation setup. The claim that larger explanations lead to more trust is not supported by evidence in the paper. For example, an explanation that covers the full input does not necessarily lead to more trust. Furthermore, “more trust” can lead to overtrust and be detrimental to the explanation process \n\n6) Corollary 3.2 is stated as a consequence of a paper that, at the best of the reviewer’s knowledge, is a pre-print and relatively new (July 2025 on arxiv). While it doesn’t have direct consequences on the paper, I believe that at least the **proofs/corollary/theorems should be based on peer-reviewed papers**.\n\n7) Results do not report errors bar or **statistical significance**. Therefore it is difficult to assess the validity of such results"}, "questions": {"value": "See weaknesses.\n\nAn additional (minor) question is the following:\n- The authors state: *“Our results are over CNNs: it is straightforward, however, to extend AD to other architecture classes.”* Can the authors provide a description of extension of AD to an architecture that does not use receptive fields and in which each neuron is connected to all neurons in the previous layer? From my understanding, this method seems tailored to CNNs, which is fine, but extending it to other architectures would require alternative projection methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f5wPqx3cJr", "forum": "Pf6Vbl4r9k", "replyto": "Pf6Vbl4r9k", "signatures": ["ICLR.cc/2026/Conference/Submission7985/Reviewer_mj3E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7985/Reviewer_mj3E"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857145915, "cdate": 1761857145915, "tmdate": 1762919992862, "mdate": 1762919992862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Activation–Deactivation (AD), a forward-pass explanation method that keeps the original image but selectively disables internal feature locations tied to masked regions, avoiding the out-of-distribution (OOD) problem of input occlusion. Implemented as ConvAD, this drop-in module for CNNs yields more robust explanations than standard occlusion baselines on ImageNet-1k, ImageNet-v2, Caltech-256, and PASCAL-VOC, while leaving the model’s original predictions largely intact."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets a real, widely acknowledged issue in XAI for vision: explanations that depend on masked or patched images can be unreliable because the model is evaluated on OOD inputs. AD sidesteps this by keeping the input in-distribution and instead constraining network to ignore masked regions.\n\n2. ConvAD is explicitly designed to be inserted into pretrained CNNs, with a proof/argument that an all-ones mask recovers the original forward pass. This makes the method appealing for practitioners who cannot retrain large models.\n\n3. The authors test multiple CNN backbones and several datasets, and show that the robustness improvements are consistent across different confidence thresholds. This supports the claim that the method is not tightly coupled to a single dataset."}, "weaknesses": {"value": "1. All experiments are done on CNN classifiers, while the paper presents AD as a general mechanism. Given the prevalence of vision transformers and CLIP-style models, at least a small-scale replication on a ViT or hybrid backbone would strengthen the generality claim.\n\n2. The method is mainly compared to occlusion with different baselines, but not to strong, widely used methods (CAM-based XAI methods, SHAP, prototype-based XAI). Without these, it is hard to position the contribution against established baselines.\n\n3. The paper reports its own robustness-style metrics (re-planting explanations on backgrounds, confidence preservation), but does not report standard faithfulness metrics such as Insertion/Deletion AUC or Average Drop / Increase in Confidence, which would make the results directly comparable to prior XAI work."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CMqvtDs2cN", "forum": "Pf6Vbl4r9k", "replyto": "Pf6Vbl4r9k", "signatures": ["ICLR.cc/2026/Conference/Submission7985/Reviewer_ZAUp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7985/Reviewer_ZAUp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7985/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762175075291, "cdate": 1762175075291, "tmdate": 1762919992582, "mdate": 1762919992582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Re: the lack of experiments with other XAI tools"}, "comment": {"value": "The AD-framework specifically targets XAI approaches, which rely on occlusions or perturbations in order to calculate attribution. CAM-based XAI methods and prototype-based XAI methods do not fall under this category.\nBlack-box XAI tools, such as SHAP, are within the scope of methods that AD addresses. However SHAP and similar methods do not include an extraction procedure of minimal sufficient sets and thus do not allow for comparison of the minimal sufficient sets natively. The only possible comparison we can make with SHAP is comparing the saliency maps, which provide some information about which input features are important for the output, but this information is not formalized or quantified.\nTo extract minimal sufficient sets from saliency maps, we need to add an extraction procedure. We will do that and report on results for a selection of other occlusion-based methods."}}, "id": "BpfEqq49eD", "forum": "Pf6Vbl4r9k", "replyto": "Pf6Vbl4r9k", "signatures": ["ICLR.cc/2026/Conference/Submission7985/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7985/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7985/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763057950860, "cdate": 1763057950860, "tmdate": 1763057950860, "mdate": 1763057950860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Re: introduction of a new robustness metric"}, "comment": {"value": "Note that we are comparing sufficient approximately minimal subsets of the input. Commonly used metrics such as Insertion/Deletion AUC or confidence increases are not appropriate (as the subset is already close to minimal). This is why we introduce new metrics that compares robustness of the explanations as a proxy for their quality."}}, "id": "aY8pOAzDXR", "forum": "Pf6Vbl4r9k", "replyto": "Pf6Vbl4r9k", "signatures": ["ICLR.cc/2026/Conference/Submission7985/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7985/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7985/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763058082493, "cdate": 1763058082493, "tmdate": 1763058082493, "mdate": 1763058082493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}