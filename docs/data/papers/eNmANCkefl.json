{"id": "eNmANCkefl", "number": 422, "cdate": 1756738980088, "mdate": 1759898261709, "content": {"title": "Declarative Audio Editing with Audio Language Model", "abstract": "Audio editing plays a central role in VR/AR immersion, virtual conferencing, sound design, and other interactive media. \nHowever, recent generative audio editing models depend on template-like instruction formats and are restricted to mono-channel audio.\nThese models fail to deal with declarative audio editing, where the user declares what the desired outcome should be, while leaving the details of editing operations to the system.\nWe introduce SmartDJ, a novel framework for stereo audio editing that combines the reasoning capability of audio language models with the generative power of latent diffusion. \nGiven a high-level instruction, SmartDJ decomposes it into a sequence of atomic edit operations, such as adding, removing, or spatially relocating events.\nThese operations are then executed by a diffusion model trained to manipulate stereo audio. \nTo support this, we design a data synthesis pipeline that produces paired examples of high-level instructions, atomic edit operations, and audios before and after each edit operation. \nExperiments demonstrate that SmartDJ achieves superior perceptual quality, spatial realism, and semantic alignment compared to prior audio editing methods. \nDemos are provided in the supplementary file. Code and data will be released upon acceptance.", "tldr": "", "keywords": ["Audio editing", "Latent diffusion model", "Audio language model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/99f23f89722b51fe6d89b44905be811672151f2c.pdf", "supplementary_material": "/attachment/4bfaaf3e40e275e0e42b5a3ff634e52de4ec2fc2.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SmartDJ, a novel framework for stereo audio editing that combines the reasoning capability of audio language models with the generative power of latent diffusion."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Task Formulation: The paper introduces declarative editing for stereo audio, an area that remains relatively underexplored and represents a meaningful contribution to the field.\n\n2. Valuable Dataset Contribution: The newly collected SmartDJ dataset provides a solid foundation for training and evaluating complex audio editing tasks, facilitating future research in declarative audio manipulation.\n\n3. Effective Methodology: The proposed ALM-assisted pipeline demonstrates promising performance in handling complex editing scenarios."}, "weaknesses": {"value": "1. I note that the paper primarily uses mono-channel audio clips for SmartDJ data synthesis. However, it lacks sufficient details on how spatial audio effects—such as binauralization or room acoustics (e.g., reverberation)—are implemented or considered.\n\n2. Due to the cascaded architecture and separate training strategy, the performance of declarative editing heavily relies on the accuracy of the generated atomic instructions. It would be valuable to include quantitative results on the accuracy/feasibility of atomic instruction generation.\n\n3. The connection between the proposed declarative audio editing and precise spatial effects manipulation remains unclear. For instance, in Example 1 (Appendix C.1), the instruction \"Make this sound like a quiet afternoon in a garden\" is fulfilled by adding a gentle breeze, but the spatial direction of this sound seems arbitrary. \n\n4. The novelty of the single-step editing part appears limited, as its training protocol and model architecture closely follow those established in prior works. I am somehow confused.\n\n5. The citation style does not conform to ICLR guidelines; mixing \\citet{} and \\citep{} reduces readability.\n\n6. The only stated distinction between stereo and general audio editing is “direction,” yet only three direction types are considered. More directional categories are expected.\n\n7. The dataset description is insufficient. “We sample 2–5 audio events and use GPT-4o to create 50k training pairs and 1k evaluation pairs of high-level audio editing data to train an audio language model and evaluate the editing pipeline.” This suggests the data are entirely generated and curated by GPT-4o. The evaluation set should be verified by human annotators for accuracy.\n\n8. There is no comparison with existing editing datasets. Given the proposed 50k examples, please provide comparisons for both training and evaluation sets against prior editing datasets.\n\n9. There are actually training-free editing methods, like DDPM inversion or better methods, that can be easily implemented for stereo editing. The authors should compare these models with SmartDJ."}, "questions": {"value": "I will reconsider my score if my concerns are thoroughly addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nKt8GiPd5a", "forum": "eNmANCkefl", "replyto": "eNmANCkefl", "signatures": ["ICLR.cc/2026/Conference/Submission422/Reviewer_NKMZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission422/Reviewer_NKMZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760510162530, "cdate": 1760510162530, "tmdate": 1762915517035, "mdate": 1762915517035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework for high-level audio editing that allows users to modify stereo audio through semantic instructions rather than low-level waveform manipulation. The system introduces an Audio Language Model (ALM) that generates step-by-step editing commands, guiding an audio diffusion model to perform multi-stage modifications until an optimal output is reached. The paper explores the feasibility of such semantic audio editing and evaluates its ability to handle complex instructions such as spatial or environmental sound changes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•  The paper addresses a novel and forward-looking task — semantic, high-level audio editing — which goes beyond traditional waveform-level operations.\n\n•  The idea of integrating an Audio Language Model that provides editing steps is creative and can inspire future work combining reasoning and audio generation.\n\n•  The model supports stereo audio editing, which is technically more challenging and rarely explored in prior works.\n\n•  The volume control (±dB) feature is interesting, as most baseline editing systems cannot precisely handle such parametric changes.\n\n•  The overall direction contributes to bridging the gap between natural-language interaction and multi-step sound manipulation, a promising area for interactive audio systems."}, "weaknesses": {"value": "•  Although the task is conceptually interesting, the training and testing data are artificially concatenated, which raises concerns about the naturalness and generalizability of the results. Real-world editing tasks often involve continuous, unstructured recordings rather than segmented mixtures.\n\n•  The proposed framework mainly focuses on environmental or ambience-level edits (e.g., changing the spatial context or reverb), while many practical audio-editing scenarios involve precise, localized edits (e.g., removing or amplifying a specific sound event), which existing baseline models can already handle effectively.\n\n•  The system architecture resembles a captioning + LLM planning + diffusion editing pipeline. The novelty is somewhat limited since similar modular approaches could be implemented by combining existing captioning and LLM-based reasoning models.\n\n•  The Audio Language Model (ALM) component’s added value is unclear — it might be replaceable by prompting a strong general LLM (e.g., ChatGPT, Llama 3) with audio captions and editing prompts to produce similar step-wise operations.\n\n•  The diffusion-based editor uses a standard DiT backbone without architectural innovation; exploring more modern generative frameworks like flow-matching or bridge-flow models could potentially improve performance and efficiency.\n\n•  Some tables and metrics (e.g., Table 1) are difficult to interpret — it is not clear whether they measure end-to-end editing quality or only the performance of the editor module， especially it seems that there are a lot of new editing instructions generate by the ALM, such as in/decreasing the volume by dB."}, "questions": {"value": "•  How does the proposed LDM-based editor compare to existing audio separation or editing models in quantitative and perceptual metrics? Only on the editing level, not combining with the ALM guided steps.  \n\n•  What is the dataset used on result table 2, can we compare the results on the same evaluation set used by Audit? \n\n•  Can the ALM component be replaced by an audio-captioning model + LLM prompting pipeline? If not, what specific advantages does ALM bring?\n\n•  Have the authors considered evaluating intermediate steps of ALM-guided editing (e.g., SmartDJ examples) to show how the step-wise process improves the result? Currently the demo only show the comparision on the final results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3K0ry3XeDj", "forum": "eNmANCkefl", "replyto": "eNmANCkefl", "signatures": ["ICLR.cc/2026/Conference/Submission422/Reviewer_QgiL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission422/Reviewer_QgiL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760644991970, "cdate": 1760644991970, "tmdate": 1762915516912, "mdate": 1762915516912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for audio editing that breaks down high-level (declarative) instructions into a series of basic editing steps, using an LDM model trained to carry out these edits. A data synthesis pipeline is employed to create paired datasets for training. Experimental results indicate this approach delivers better perceptual quality and greater alignment with user instructions compared to several baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Innovative use of both Audio Language Models (ALM) and Large Language Models (LLM):\n1. The ALM acts as a planner by understanding input audio, interpreting broad user instructions, and segmenting them into a sequence of simpler editing operations.\n2. The LLM contributes as a designer during data creation, generating a wide range of high-level edit commands and their corresponding atomic steps.\n\nThe study includes comprehensive experiments and ablation analyses.\n\nAllow editing of stereo audio direction."}, "weaknesses": {"value": "Breaking down user instructions into atomic editing actions isn't a novel concept for audio editing; for instance, WavCraft already transforms commands into several subtasks.\n\nComplicated instructions may not always be easily reducible to a fixed set of basic operations."}, "questions": {"value": "Are there any additional atomic operations to consider beyond the current ones?\n\nWhen using Audit as a baseline, do authors use the same training data? If so, what explains the significant subjective preference when comparing SmartDJ and Audit in single-step test?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UipUkW3zsy", "forum": "eNmANCkefl", "replyto": "eNmANCkefl", "signatures": ["ICLR.cc/2026/Conference/Submission422/Reviewer_eq8D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission422/Reviewer_eq8D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761205355509, "cdate": 1761205355509, "tmdate": 1762915516764, "mdate": 1762915516764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SmartDJ, a novel framework for declarative stereo audio editing. It leverages an Audio Language Model (ALM) to decompose high-level user instructions (e.g., \"make this sound like a sunny forest\") into a sequence of atomic editing operations (e.g., add/remove sounds, adjust volume/direction). These operations are then executed sequentially by a Latent Diffusion Model (LDM) to edit the audio while preserving spatial realism. \n\nThe authors also propose a scalable data synthesis pipeline using LLMs and signal processing to generate paired training data, addressing the lack of such datasets. Key contributions include: (1) the first declarative stereo audio editor combining ALM reasoning with LDM generation; (2) a controllable data pipeline for editable audio scenes; and (3) empirical results showing superior perceptual quality, semantic alignment, and spatial fidelity over baselines like Audit and WavCraft, validated through metrics (e.g., FD, CLAP, FSAD) and human studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper demonstrates strong originality by adapting multimodal large language models (MLLMs) and diffusion-based editing techniques from the vision domain to audio, specifically addressing the underexplored area of declarative (high-level) stereo audio editing. \n\nThe integration of an ALM as a \"planner\" for decomposing instructions into atomic steps, combined with an LDM \"editor,\" is a clever modular design that allows for interpretability and potential human-in-the-loop refinement. The data synthesis pipeline is a standout contribution: using GPT-4o as a \"designer\" to generate instructions and steps, paired with rule-based audio composition, enables scalable training data creation, which is a practical solution to the data scarcity problem in audio editing. \n\nExperiments are comprehensive, including quantitative metrics (e.g., lower FD and higher CLAP scores in Table 1), ablations (e.g., multi-round editing quality in Table 3), and human evaluations showing preference for SmartDJ in fidelity and alignment. Overall, the work is well-substantiated and advances reasoning-guided audio manipulation."}, "weaknesses": {"value": "While the synthetic data pipeline is innovative, the heavy reliance on composed audio from clean datasets (e.g., AudioCaps, FSD50k) may limit generalization to real-world recordings, which often include overlapping events, noise, or reverberation not fully captured in the synthesis. For instance, the atomic operations are restricted to a predefined set (add/remove/extract/volume/direction), which might not handle more complex edits like timbre changes or temporal alignments, potentially reducing flexibility for diverse instructions. \n\nComputationally, the sequential multi-step editing (e.g., 13.1s inference time in Table 1) is slower than end-to-end baselines, which could hinder real-time applications. \n\nThe baselines are appropriate but could be expanded to include more recent audio editing methods (e.g., adaptations of IP2P or P2P from images to audio, beyond just Audit and WavCraft). Human studies are mentioned but details (e.g., number of participants, inter-rater agreement) are sparse in the main text, making it hard to assess robustness without appendices."}, "questions": {"value": "1. How well does SmartDJ generalize to real-world audio recordings (e.g., field recordings with ambient noise or overlaps) versus the synthetic mixtures used in training? Have you tested this, and if not, what challenges do you anticipate?\n\n2. The ALM decomposes instructions effectively in your examples, but how does it handle ambiguous, abstract, or conflicting high-level prompts (e.g., \"make it eerie yet cheerful\")? Could you provide failure cases or metrics for decomposition accuracy?\n\n3. Are there plans to expand the set of atomic operations (e.g., to include reverb, pitch shifting, or event timing adjustments) to support broader editing scenarios?\n\n4. In the human evaluation, how many participants were involved, what was the setup (e.g., A/B preference, rating scales), and what was the statistical significance of the preferences? This could strengthen the subjective claims.\n\n5. Given the modular design, how sensitive is the overall performance to the choice of ALM backbone (e.g., LTU vs. Audio Flamingo)? Did you ablate different ALMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UadpgHuPzu", "forum": "eNmANCkefl", "replyto": "eNmANCkefl", "signatures": ["ICLR.cc/2026/Conference/Submission422/Reviewer_bDhL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission422/Reviewer_bDhL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission422/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981369228, "cdate": 1761981369228, "tmdate": 1762915516632, "mdate": 1762915516632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}