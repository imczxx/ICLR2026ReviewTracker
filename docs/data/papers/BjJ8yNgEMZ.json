{"id": "BjJ8yNgEMZ", "number": 18977, "cdate": 1758292510710, "mdate": 1759897069561, "content": {"title": "GoalZero: Model-based Hierarchical Self-Play for Sequential Stochastic Combinatorial Optimization", "abstract": "Sequential Stochastic Combinatorial Optimization (SSCO) problems are challenging for reinforcement learning due to exponentially large action spaces, stochastic dynamics, and the need for long-horizon planning under limited resources. Hierarchical Reinforcement Learning (HRL) offers a natural decomposition, but the high-level policy operates in a Semi-Markov Decision Process (SMDP) where actions have variable durations. This variability complicates learning a planning-ready world model. We introduce GoalZero, a model-based HRL framework that directly addresses this challenge. GoalZero integrates a MuZero-style planner at the high level that learns a world model of SMDP dynamics. At the core is a principled framework for multi-timescale SMDP (MTS-SMDP) world-model learning. Through complementary objectives, the agent learns dynamics where the latent transition magnitude correlates with the temporal scale of the corresponding subgoal, facilitating planning over diverse, adaptive temporal abstractions in our evaluated settings. In addition, we propose a subgoal-conditioned budget allocation mechanism learned jointly with the multi-timescale world model, facilitating context-aware resource management. We demonstrate that GoalZero outperforms strong baselines on challenging SSCO benchmarks.", "tldr": "GoalZero is a model-based HRL framework that learns a multi-timescale SMDP world model for MuZero-style planning on SSCO tasks.", "keywords": ["Hierarchical Reinforcement Learning", "Combinatorial Optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f7763a3522c43bc2771f9682ca3149227dc42083.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents GoalZero, a model-based hierarchical reinforcement learning (HRL) framework for Sequential Stochastic Combinatorial Optimization (SSCO) problems. The key innovation is learning a unified world model where latent geometric distances correlate with temporal durations of subgoals, which avoids explicit duration prediction. The framework combines a MuZero-style planner searching over semantic subgoals, a multi-timescale SMDP world model with complementary pull/push/order objectives, and a subgoal-conditioned budget allocation. Empirically, on AIM and SOP benchmarks, GoalZero outperforms baselines including WS-option, with improvements becoming more pronounced as problem complexity increases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is clear. The limitations of budget-only hierarchical approaches like WS-option are clearly demonstrated, where passing only scalar budgets provides insufficient semantic guidance for the low-level policy.\n2. Most of the theorems are well proved. The mathematical formulation is rigorous and clearly presented. Most assumptions are clearly and comprehensively elaborated.\n3. The idea of encoding temporal information implicitly through latent space geometry is promising.\n4. Ablation studies, sensitivity analysis, and generalization tests are well conducted.\n5. The visualization of learned subgoal specialization in Figure 3 provides intuitive validation."}, "weaknesses": {"value": "1. Please correct me if I am wrong, but Assumption B.3 appears circular. The assumption states the equation regarding $\\mathbb{E}[\\tau|s,z]$ for strictly increasing $\\psi$. This essentially assumes what the authors want to prove, which is that geometry correlates with duration. The theory assumes this property rather than deriving it from the training dynamics. Why does training with the given objective produce representations that satisfy Assumption B.3? \n2. Only compared with WS-option, although it is the most relevant work. No comparisons with other goal-conditioned works like Director (Hafner, 2022), even if they feature one-step latent dynamics.\n3. Table 16 shows decision latency and per-epoch training time. No systematic comparison of computational costs versus baselines, particularly important given the MCTS overhead."}, "questions": {"value": "1. Could the authors please justify the concern about Assumption B.3?\n2. Could the authors compare the proposed method with other goal-conditioned hierarchical model-based RL works like Director, or explain if there is a good reason for not doing so."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FUuiY5tImI", "forum": "BjJ8yNgEMZ", "replyto": "BjJ8yNgEMZ", "signatures": ["ICLR.cc/2026/Conference/Submission18977/Reviewer_fKtY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18977/Reviewer_fKtY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761420240449, "cdate": 1761420240449, "tmdate": 1762931029949, "mdate": 1762931029949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a hierarchical RL framework for SSCO that (i) uses MuZero-style search over a discrete set of learned subgoals, (ii) learns a multi-timescale SMDP world model so that latent transition magnitudes correlate with option duration, and (iii) adds a subgoal-conditioned budget head to allocate resources during planning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem class (SSCO) combines large combinatorial action spaces with stochastic, long-horizon effects where hierarchical planning is natural; the motivation is clear. \n2. The multi-timescale objective is well designed: a low-level pull/cap regularizes per-step latent motion; a high-level push/margin prevents collapsed macro-moves; and a budget-aware order loss aligns the internal scorer with effective (budget-aware) durations. This is a thoughtful way to let one-step latent models reason about variable-duration options without explicitly predicting time."}, "weaknesses": {"value": "1. Training/search operates over subgoals, while the budget is sampled from a separate head after selecting subgoal. The world model’s dynamics/reward prediction is conditioned on subgoal but not on budget. Shouldn’t macro outcomes depend on the allocated resources? What’s the rationale for excluding budget from the model inputs or from branching during search? \n\n2. The assumption that value is Lipschitz in the latent geometry is strong. It would help to include empirical checks (e.g., value vs. latent distance calibration plots) on the tested environments. \n\n3. WS-option and heuristics are reasonable, but adding model-based baselines (e.g., Dreamer/Director-style HRL on graphs) would help isolate the benefit of the multi-timescale geometry from planning. \n\n4. Prior work has explored MCTS with SMDP dynamics. It would strengthen the paper to more explicitly situate the SMDP dynamics and model-based planning components within that line of literature. E.g., “subgoal-based temporal abstraction in Monte-Carlo tree search”, “scalable decision-making in stochastic environments through learned temporal abstraction”."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xYu4bbWJq1", "forum": "BjJ8yNgEMZ", "replyto": "BjJ8yNgEMZ", "signatures": ["ICLR.cc/2026/Conference/Submission18977/Reviewer_6Fqp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18977/Reviewer_6Fqp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761516885404, "cdate": 1761516885404, "tmdate": 1762931028927, "mdate": 1762931028927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the Sequential Stochastic Combinatorial Optimization (SSCO) problem. Previous works, such as WS-option, have proposed hierarchical RL approaches for SSCO and often suffer from near-sighted allocation and misalignment with long-term goals. To address these issues, this paper introduces GoalZero, a model-based HRL approach for SSCO that plans using MCTS strategy and multi-scale SMDP dynamics models to more effectively optimize sequential combinations given long-horizon goals under stochastic dynamics. The authors also propose subgoal-conditioned budget allocation jointly trained with multi-scale SMDP models, which enables more effective budget-aware planning. As a result, GoalZero generally outperforms other baselines in two benchmarks (AIM, SOP)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. **In-depth Ablation Study.** The paper investigates each component of the proposed methods and effectively clarifies their roles.\n2. **Novelty in SSCO Field.** The authors propose novel and effective methods for the SSCO problem, such as multi-timescale SMDP and subgoal-conditioned budget allocation. This gives the paper specialty and uniqueness, distinguishing it from general RL approaches."}, "weaknesses": {"value": "1. **Limited Novelty to ML Community.** The baseline comparisons are limited, as they focus primarily on domain-specific methods rather than demonstrating clear advantages over general RL approaches. Both WS-option and this paper use domain-specific methods, with Flat DQN being the only general RL baseline that struggles with this problem. It remains unclear how the proposed approach would perform against other general challenges in RL, and the paper does not sufficiently motivate why new ideas are necessary beyond existing frameworks. For example, the goal-conditioned hierarchical planning framework (Director[1]) has been explored previously, and multi-scale world modeling (MTS-WM[2]) is not a novel concept either. The specific challenges addressed by SMDP and the unique contributions beyond established methods are not clearly articulated.\n2. **Illustration Quality.** The figures are difficult to interpret. They are too abstract, and the actual framework and logic are not clearly presented step by step.\n3. **Limited Evaluation.** First, more baselines are needed, including Flat RL baselines, general HRL baselines, model-based RL baselines, and model-based HRL baselines. Second, the evaluation is conducted on only two tasks, which is insufficient to demonstrate broad applicability. Additional tasks, such as resource planning tasks similar to WS-option, should be included to verify the method's generalizability.\n\n[1] Hafner, Danijar, et al. \"Deep hierarchical planning from pixels.\" Advances in Neural Information Processing Systems 35 (2022): 26091-26104.\n\n[2] Shaj Kumar, Vaisakh, et al. \"Multi time scale world models.\" Advances in Neural Information Processing Systems 36 (2023): 26764-26775."}, "questions": {"value": "- Are general HRL, GCHRL, and model-based RL baselines sufficient to address the SSCO problem? If SSCO requires specialized solutions beyond what general RL can provide, this distinction should be clearly explained in the introduction. The paper should better motivate why domain-specific solutions are necessary.\n- Regarding **self-play** in the title: Which components use self-play, and how is it implemented? The main text mentions it briefly, but more detailed explanations appear only in Appendix Section 2. Could this be clarified and simplified?\n- Why is it called \"multi-timescale\"? The term appears repeatedly, but the rationale behind this naming is not clear. Related works usually refer to world modeling that predicts states at intervals of T steps, enabling jumpy state transitions. However, the method section does not adequately explain the naming choice. The approach uses both low+high level planning and budget-aware objectives as complementary components. What specifically does \"multi-scale\" refer to in this context, and why was this terminology chosen?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "no ethical statement included"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "72l9FWrfOJ", "forum": "BjJ8yNgEMZ", "replyto": "BjJ8yNgEMZ", "signatures": ["ICLR.cc/2026/Conference/Submission18977/Reviewer_LtYR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18977/Reviewer_LtYR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931244440, "cdate": 1761931244440, "tmdate": 1762931028557, "mdate": 1762931028557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Problem: Sequential Stochastic Combinatorial Optimization (SSCO) problems are extremely challenging for reinforcement learning because they involve exponentially large, structured action spaces, stochastic dynamics, and demand long-horizon planning under tight resource constraints. Traditional flat RL and even hierarchical RL methods often fail due to shortsighted budget allocation and lack of explicit strategic guidance, particularly when decisions require variable effort over different timescales.\n\nApproach:\nGoalZero uses a hierarchical model-based reinforcement learning approach, where a MuZero-style planner searches over abstract subgoals and couples this to a multi-timescale world model that encodes temporal abstractions. \n\nOverall assessment:\nThere are many clarity issues (stated in the weaknesses section)."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- GoalZero introduces a model-based hierarchical RL framework where a MuZero-style planner enables lookahead search over semantically meaningful subgoals, not just scalar budgets.\n\n- It develops a unified multi-timescale SMDP world model that encodes temporal abstractions directly in latent geometry, allowing efficient planning over diverse durations.\n\n- The framework couples subgoal-conditioned budget allocation and strategic guidance, enabling context-aware resource management that adapts to the complexity and variability of SSCO domains.\n\n- The theoretical analysis is strong (in the appendix)"}, "weaknesses": {"value": "- Lack of clarification:\n    - L.50 mentions that variable duration subgoals places high-level in an SMDP. What is the issue or the problem with an SMDP? The paper assumes the readers are familiar with the drawbacks. However, one may not know. The introduction should fully motivate the approach.\n    - L53: Latent distance induced by … Latent distance between what?\n    - L55: What is MuZero style planner? A MCTS-based planner?\n    - L212: What is s? Assuming a state. But what is z? How does the equation for $d_\\theta$ has $h_\\theta$ and $g_\\theta$. Additionally, $g_\\theta$ is a function of a latent state and high-level action. Does that mean $z$ is a high-level action?\n    - L212: Output of the $g_\\theta$ function is a tuple. How is the distance between $s$ (supposedly a state) and a tuple is defined in $d_\\theta$?\n    - L243: now each high-level action is a tuple $(z_i,b_i)$?\n    - L244: What is $\\tau$?\n    - What is the notion of pull force and push force? No motivation, justification, or explanation has been provided for it.\n    - What is the point of Figure 1? It is never discussed or referenced.\n    - The text in Figure 2 can not read.\n- The paper is using the notion of high-level actions and subgoals interchangeably. However, they are not the sane. E.g., in the preliminaries $\\gA_{HL}$ is the set of high-level actions. But later, z is referred to as a subgoal. How do you execute a subgoal in a state? It doesn't make sense.\n- The paper seems to be heavily relying on \"MuZero-style\" high-level planner but at no point clarifies what a MuZero-style high-level planner is. It should be discussed in preliminaries. Model-based-RL with MCTS discusses it briefly. However, I strongly suggest extending it to be comprehensive summary of what \"MuZero-style\" planner is.\n- The analysis of the evaluation is shallow. The claim of the paper is that it enables high-level planning with dynamic and variable time high-level actions. However, it is not clear from the approach or evaluation: (i) What are these high-level action in the domains used for evaluation? (ii) where does the approach get them? Does the approach automatically learn them? Are they already provided?  (iii) How are they dynamically and variable in nature? The paper does not even relate them somehow to the options architecture. It is not even clear from the presented paper (the written content that is presented) how their approach is different from simply HRL with options.\n- The major issue of the paper is that it is poorly written. The paper doesn't clearly mention how their approach resolves the problem. Sec 4.2 provides a high-level summary of the approach. Major approach is in appendix. However, sec 4.2 doesn't specify how it deals with variable times. The writing often feel disconnected and without any motivation. E.g., Line 242 talks about learnable regularization, Line 243 talks about Y, and line 245 talks about losses that are being optimized without any connection between them. The paper should always motivate with a clear idea before introducing the theory.  While the paper may present interesting theoretical contributions in *appendix,* it makes no effort convey any useful information in the main paper.\n- It is not clear where and how “Muzero-style” planning is being done."}, "questions": {"value": "Included in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pBJVCBt9ju", "forum": "BjJ8yNgEMZ", "replyto": "BjJ8yNgEMZ", "signatures": ["ICLR.cc/2026/Conference/Submission18977/Reviewer_a756"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18977/Reviewer_a756"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957589049, "cdate": 1761957589049, "tmdate": 1762931028101, "mdate": 1762931028101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}