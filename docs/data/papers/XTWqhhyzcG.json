{"id": "XTWqhhyzcG", "number": 12234, "cdate": 1758206507734, "mdate": 1759897523629, "content": {"title": "Let Androids Dream of Electric Sheep: A Human-Inspired Image Implication Understanding and Reasoning Framework", "abstract": "Metaphorical comprehension in images remains a critical challenge for AI systems, as existing models struggle to grasp the nuanced cultural, emotional, and contextual implications embedded in visual content. \nWhile multimodal large language models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they exhibit a fundamental limitation on image implication tasks: contextual gaps that obscure the relationships between different visual elements and their abstract meanings. \nInspired by the human cognitive process, we propose ***Let Androids Dream (LAD)***, a novel framework for image implication understanding and reasoning. \nLAD addresses contextual missing through the three-stage framework: (1) **Perception**: converting visual information into rich and multi-level textual representations, (2) **Search**: iteratively searching and integrating cross-domain knowledge to resolve ambiguities, and (3) **Reasoning**: generating contextual-alignment image implication via explicit reasoning.\nOur framework with the lightweight GPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English image implication benchmark and a huge improvement on Chinese benchmark, performing comparable with the GPT-4o model on Multiple-Choice Question (MCQ) and outperforms 36.7\\% on Open-Style Question (OSQ). Additionally, our work provides new insights into how AI can more effectively interpret image implications, advancing the field of vision-language reasoning and human-AI interaction. Our project is publicly available at https://anonymous.4open.science/r/Let-Androids-Dream-of-Electric-Sheep.", "tldr": "", "keywords": ["Image Implication", "Image Metaphor", "Visual Question Answering", "Vision and Language", "Vision Language Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5097f89432a790dec141bf43443ced8881f6d58a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an agentic framework called LAD for image implication/metaphor understanding (e.g., what does an image showing \"a person in a suit running on a hamster wheel inside an office\" imply). Current MLLMs perform well on visual QA requiring concrete image understanding and logical reasoning task, while image metaphor understanding requires higher reasoning abilities and external context (e.g., culture, history, current events, emotions) to connect it to the abstract meaning. To address this, they propose LAD, a three‑stage: Perception -->  Search --> Reasoning. Perception converts an image into rich text and keywords, Search formulates questions given the description and keywords for step 1, and retrieves out‑of‑domain knowledge (either through parameteric knowledge- the model's own knowledge or through Web Search), and converts that retrieved knowledge into a summary (they select the top 3 related question-answer pairs and summarize them). Finally the reasoning step combines all the information in the previous stages (original image, Stage 1 description & keywords, and the retrieval summary) and prompts the model to reason in a structured format (using <think> . . . </think>) special tokens for the model to produce a step-by-step reasoning process (CoT). A light base model (GPT‑4o‑mini), reaches or exceeds strong closed models on English MCQ tasks and substantially outperforms them on the introduced open‑style generation (OSQ) benchmark by the authors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well motivated, and i believe image metaphor understanding is an important step towards better MLLMs as it requires better reasoning capabilities. \n- Wide variety of MLLMs are investigated, and impressive results on image implication benchmarks even with GPT4o-mini. \n- The introduction of the OSQ task with LLM evaluation, verified by humans and showing high correlation with human judgement is valuable."}, "weaknesses": {"value": "- [W1] I do not see the framework as novel. Search exists in many LLMs, and they automatically do this when they don't know the answer (replicated by Self-Judge in the author's pipeline, which decides whether to use parametric memory or web search). The essence/general idea of this pipeline in inherently present in many LLMs (closed-sourced ones). By looking at Table 3 (which I appreciate), the contribution of this paper seems to be a better search algorithm (LAD search is better than GPT4o-mini search) on MCQ and OSQ (probably because of the planner in WebSearch which decomposes the initial search question into a series of more granular sub-questions?). Its totally fine for this, but this means the framing of the paper has to be changed. \n- [W2] Table 1 shows worse results without search, like GPT4o standalone (with its parametric knowledge) is better than GPT4o-mini + the authors pipeline. Although parameters reduce, the number of inference model calls increase, so we gain nothing much. \n- [W3] Although the framework should not sacrifice performance of the MLLM on other tasks (e.g., general VQA)  - in fact it should improve it - it would be better to report results on the framework's benefit for other tasks as well beyond image implication understanding. Can the authors report results with their pipeline on multimodal VLM benchmarks such as SeedBench, MMMU, GQA, VizWiz, POPE....etc. I understand that MLLM evaluation on these benchmarks is quite expensive, so I would be fine if the authors can choose any 2. Also, what would be the performance on those two benchmarks with the simple   Step 1 (without keywords, only description) and Step 3 (reasoning)? In summary, I would like to see the last row on Table 1: LAD (Stage I (without keywords) + III) and LAD (Stage I + II + III) but applied on 2 VLM benchmarks of the authors choice. This would then show how useful the pipeline is in general. \n- [W4] \"Our system uses the MLLM as a router, scoring queries based on criteria like knowledge popularity and specificity\" , how is the scoring done? In other words, how is the strategy determined in Self Judge ?"}, "questions": {"value": "In general the paper is good, but we need to clarify 1) the frame of the paper, in my opinion, its a search method (W1), 2) what do we gain (W2) and 3) the pipeline's advantage in general beyond image implication understanding (W3). Moreover, some details are missing (W4), the authors can add it in the supplementary. W4 will not affect my decision."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No issues"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vJsqpZwGew", "forum": "XTWqhhyzcG", "replyto": "XTWqhhyzcG", "signatures": ["ICLR.cc/2026/Conference/Submission12234/Reviewer_jGQm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12234/Reviewer_jGQm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761471185892, "cdate": 1761471185892, "tmdate": 1762923177795, "mdate": 1762923177795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Let Androids Dream (LAD), a human-inspired framework for understanding image implications and metaphors — tasks that current multimodal large language models (MLLMs) struggle with due to contextual and cultural ambiguity. LAD mimics human cognition through a three-stage process: converts visual information into detailed textual descriptions and key emotional/cultural keywords; retrieves and integrates relevant cross-domain knowledge through adaptive internal and web searches; performs explicit chain-of-thought reasoning to align visual elements with contextual implications. The framework achieves state-of-the-art (SOTA) results on both English (II-Bench) and Chinese (CII-Bench) image implication benchmarks, especially outperforming other MLLMs by 36.7% on open-style reasoning tasks. LAD demonstrates that contextual enrichment and human-like reasoning can significantly enhance metaphorical understanding in visual reasoning. The authors also highlight cognitive theories (Dual-Process and Active Information-Seeking) as conceptual underpinnings and validate LAD’s robustness across models (e.g., GPT-4o, Qwen2.5-VL)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces Let Androids Dream (LAD), a creative and well-structured framework that integrates perception, search, and reasoning stages, effectively simulating human cognitive processes for visual metaphor understanding.\n2. The work focuses on image implication—an underexplored and cognitively complex area involving abstract, cultural, and emotional reasoning that most MLLMs fail to capture.\n3. he authors test LAD on both English (II-Bench) and Chinese (CII-Bench) benchmarks, using Multiple-Choice Questions and a newly designed Open-Style Question (OSQ) metric that aligns 95.7% with human judgments, ensuring robustness and fairness.\n4. LAD significantly outperforms over 15 leading MLLMs, achieving SOTA performance even when built on lightweight GPT-4o-mini, and showing excellent generalization across different model backbones (GPT, Qwen, etc.).\n5. The framework’s explicit Chain-of-Thought (CoT) and dual-process analogy make reasoning steps transparent, providing valuable interpretability that parallels human thinking mechanisms.\n6. The inclusion of both English and Chinese datasets highlights LAD’s ability to handle culturally dependent metaphors, which are typically very challenging for MLLMs."}, "weaknesses": {"value": "1. The Search stage involves multiple model calls and web queries, taking 3–5 minutes per image, which may hinder scalability and real-time deployment.\n2. The Open-Style Question evaluation relies heavily on GPT-4o scoring, which, despite 95.7% human alignment, still introduces potential bias from the model’s internal preferences.\n3. Although quantitative results are strong, the paper provides relatively few qualitative case studies or failure analyses beyond one illustrative example, limiting insight into specific error patterns.\n4. Despite cross-lingual testing, the system’s dependence on pretrained MLLMs and web data may still propagate cultural or linguistic biases during knowledge retrieval.\n5. The cognitive analogies (Dual-Process Theory, Active Information-Seeking) are inspiring but largely metaphorical; there’s limited evidence that LAD’s internal mechanisms truly mirror human cognition."}, "questions": {"value": "My questions are mentioned in the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "46JUvAxMEK", "forum": "XTWqhhyzcG", "replyto": "XTWqhhyzcG", "signatures": ["ICLR.cc/2026/Conference/Submission12234/Reviewer_eB6x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12234/Reviewer_eB6x"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933108512, "cdate": 1761933108512, "tmdate": 1762923177361, "mdate": 1762923177361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Proposes LAD, an inference-time technique to improve VLM responses to image implication queries via a perception-search-reasoning process. LAD is shown to improve baseline performance on image implication benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "– The paper is clearly written and easy to follow\n\n– The paper reviews prior work and motivates the problem setting well"}, "weaknesses": {"value": "– The technical novelty is limited. The technical contribution is essentially a carefully crafted multi-stage VLM prompting strategy for doing better on image implication, by combining existing techniques (self-verification, LLM search, CoT reasoning etc.)\n\n– The gains w/ LAD on image implication seem to diminish for stronger / more recent frontier models (eg. +6 w/ GPT-4o v/s +30 w/ GPT-4o-mini), which raises questions about the need for a specialized inference technique to begin with. \n\n– The paper does not benchmark any reasoning (eg. o3/o4/gpt-5/claude-4.5-sonnet/QwQ/DeepSeek-R1) models, which is a major omission since the task clearly requires considerable reasoning.\n\n– The proposed benchmark is very small (100 images) and thus potentially unsuitable to derive statistically significant conclusions from"}, "questions": {"value": "Please address the weaknesses listed above, especially around the limited novelty, diminishing gains, and missing baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ldFGUopmKe", "forum": "XTWqhhyzcG", "replyto": "XTWqhhyzcG", "signatures": ["ICLR.cc/2026/Conference/Submission12234/Reviewer_KHoM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12234/Reviewer_KHoM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937571179, "cdate": 1761937571179, "tmdate": 1762923176947, "mdate": 1762923176947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies image implication and metaphor understanding, evaluated with multiple choice questions and open-ended explanations. Prior methods often rely on surface cues, miss cultural and emotional context, over infer without grounding, and generic retrieval brings noisy or weakly related evidence. The paper propose Let Androids Dream, a three stage pipeline with Perception to produce a rich caption and about seven targeted keywords, Search to form hierarchical queries with a self judge router that chooses model knowledge or web knowledge and then summarizes the evidence, and Reasoning to use an explicit chain of thought that combines the caption, keywords, and summary to produce the answer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The pipeline is simple and reusable, following a clear flow of perception, search, then reason. Its stages are modular and make few assumptions, so they can be plugged into different base models and languages with minimal changes.\n\n2. Prompts and workflow are stated clearly, with stepwise roles, inputs, and outputs. The intermediate artifacts are exposed, which helps inspection, debugging, and faithful reproduction, and makes the method practical to adopt in real systems.\n\n3. The experimental results are strong and the gains are large across English and Chinese and across multiple backbones. Improvements are consistent on both multiple choice and open ended settings, and ablations indicate that contextual search and structured reasoning each contribute. The qualitative cases also illustrate better use of background knowledge to reach the intended meaning."}, "weaknesses": {"value": "1. Most of the contribution sits in carefully crafted prompts and a hand-engineered agent flow. There is no learned routing or trainable component that adapts beyond the current templates, and there is little theoretical framing of why this decomposition is optimal. As a result, the work reads closer to a technical report or system recipe than a modeling advance. A stronger contribution would include a learned router or trainable retrieval controller, formal objectives for “contextual alignment,” and evidence that the method still holds when prompts are varied or shortened.\n\n\n2. The decision to choose model knowledge versus web knowledge is made by simple scoring rules without a reported accuracy metric against an oracle. We do not see precision/recall of correct routing, error breakdowns by entity novelty or recency, or comparisons to a learned classifier. This leaves unclear whether gains come from the routing itself or from increased token budget. Please report routing accuracy against human labels, ablate misroutes, and compare against a small learned router with cross-validation.\n\n\n3. The pipeline depends on fixed text templates and regex parsing. Small deviations in model output format or minor wording changes can break downstream stages or reduce performance. To improve robustness, enforce strict JSON input/output with schema validation, add lightweight auto-correction and retries, and run robustness tests under prompt and formatting perturbations. Reporting success rates under these stresses would make the results more convincing.\n\n4. Open-ended scoring relies on an automatic LLM grader, which can bias outcomes toward the grader’s style. Latency and token costs are higher due to multi-stage calls, yet we do not see a clear cost-versus-quality curve or guidance for practical deployment. Please include cross-grader checks or human audits, per-stage latency and tokens, and a Pareto plot showing how quality scales with cost.\n\n\n5. The method is validated on its target task but generality to other context-heavy vision-language problems remains uncertain. Adding zero- or few-shot transfer to related datasets such as visual commonsense, ads or satire understanding, and political cartoons, with the same pipeline and minimal prompt edits, would strengthen the claim of broader utility."}, "questions": {"value": "please see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Kx1tcyrof", "forum": "XTWqhhyzcG", "replyto": "XTWqhhyzcG", "signatures": ["ICLR.cc/2026/Conference/Submission12234/Reviewer_1K1R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12234/Reviewer_1K1R"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12234/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980386781, "cdate": 1761980386781, "tmdate": 1762923176616, "mdate": 1762923176616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}