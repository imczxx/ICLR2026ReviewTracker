{"id": "gWpm3tpncq", "number": 17004, "cdate": 1758271102260, "mdate": 1759897204940, "content": {"title": "From Offline to Online Memory-Free and Task-Free Continual Learning via Fine-Grained Hypergradients", "abstract": "Continual Learning (CL) aims to learn from a non-stationary data stream where the underlying distribution changes over time. While recent advances have produced efficient memory-free methods in the offline CL (offCL) setting online CL (onCL) remains dominated by memory-based approaches. The transition from offCL to onCL is challenging, as many offline methods rely on (1) prior knowledge of task boundaries and (2) sophisticated scheduling or optimization schemes, both of which are unavailable when data arrives sequentially and can be seen only once. In this paper, we investigate the adaptation of state-of-the-art memory-free offCL methods to the online setting. We first show that augmenting these methods with lightweight prototypes significantly improves performance, albeit at the cost of increased Gradient Imbalance, resulting in a biased learning towards earlier tasks. To address this issue, we introduce Fine-Grained Hypergradients, an online mechanism for rebalancing gradient updates during training. Our experiments demonstrate that the synergy between prototype memory and hypergradient reweighting substantially allows for improved performances of memory-free methods in onCL. Code will be released upon acceptance.", "tldr": "", "keywords": ["Online Continual Learning", "Gradient Imbalance", "Blurry Task Boundaries"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91d071e2d0babaf8c649c14fd20293e38a7a2e20.pdf", "supplementary_material": "/attachment/0c78a55873d877d1580b44bd9015a3404386c6ee.zip"}, "replies": [{"content": {"summary": {"value": "The core objective of this paper is to address a challenging problem in the field of Continual Learning (CL): how to successfully adapt effective memory-free algorithms from idealized offline (offCL) settings to more realistic and difficult online, task-free (onCL) environments.\n\nTo solve the problem of gradient imbalance, the paper proposes its core innovation, Fine-Grained Hypergradients (FGH). This is a novel optimization technique based on the key idea of:\n\n+ Learning an independent, dynamic gradient weight for each parameter within the model.\n+ Leveraging the gradient directions from two consecutive iterations to assess learning stability: if the directions are aligned, the update step is amplified; conversely, if they are opposed (indicating oscillation), the update is suppressed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem addressed by the paper online, memory-free, task-free continual learning is indeed a highly challenging and practically significant direction in the current field.\n2. The combined framework proposed in this paper achieves outstanding performance in experiments, especially under the 'multi-learning-rate evaluation' paradigm designed by the authors, showcasing the robustness of their method."}, "weaknesses": {"value": "1. The entire work can be viewed as an effective combination of two known techniques (prototypes and hypergradient descent), making the contribution more empirical than conceptual. The performance improvement from FGH largely stems from enhancing plasticity in the online setting; Equation (7) progressively increases the intra-task learning rate to boost plasticity, a mechanism that has been explored in prior work [1]. \n2. Regarding catastrophic forgetting, the method essentially relies on prototype replay, which is also a common technique in previous literature. For a venue like ICLR, which seeks fundamental innovations, the weight of this contribution is insufficient.\n3. The authors use ADAM in their experiments. From a learning rate perspective, could ADAM and FGH conflict? Is it possible for a situation to arise where ADAM suggests a large learning rate while FGH suggests a small one? In other words, do FGH and ADAM work synergistically, or is there a functional redundancy? Given the prevalence of ADAM, the authors should have included a discussion on this.\n4. The authors should provide a comparative experiment between a \"global FGH\" and the proposed \"fine-grained FGH\" to demonstrate the necessity of the fine-grained design.\n\n[1] Online Learning Rate Adaptation with Hypergradient Descent ，ICLR2018"}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gIWtxiDeaX", "forum": "gWpm3tpncq", "replyto": "gWpm3tpncq", "signatures": ["ICLR.cc/2026/Conference/Submission17004/Reviewer_kRpX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17004/Reviewer_kRpX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761431061892, "cdate": 1761431061892, "tmdate": 1762927027113, "mdate": 1762927027113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper leverages hyper-gradients for continual learning. The key idea is to use prototypes as memories and use hyper-gradients for adaptive learning rate selection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper writing is clear.\nExperiments show the effectiveness of the proposed method under the proposed setting."}, "weaknesses": {"value": "Limited novelty: both the hyper-gradient and prototype based memory are not new, they have been widely used in previous works for adapting learning rates (https://arxiv.org/pdf/1703.04782) and prevent forgetting (https://arxiv.org/pdf/2308.00301) already. \n\nExperiment setting and claims: This work claims to be online and memory free, however, it uses cached prototypes which is also just a form of memory, without having other methods using the same compute, memory and storage, it is not fair to claim the performance gain.\n\nAlso, even with complex method implementation, the method is just a little bit better than simple ER, while uses heavy hyper-parameter tuning, which is prohibitive in the online CL scenario. This makes the setup and method both far from practical."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NlAdMjPYI4", "forum": "gWpm3tpncq", "replyto": "gWpm3tpncq", "signatures": ["ICLR.cc/2026/Conference/Submission17004/Reviewer_Zyuk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17004/Reviewer_Zyuk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761607064834, "cdate": 1761607064834, "tmdate": 1762927026292, "mdate": 1762927026292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the migration of offline memory-free continual learning (CL) methods to online, memory-free, and task-free CL scenarios. It introduces a prototype-based auxiliary memory module (P) and a fine-grained hypergradient mechanism (FGH) that dynamically balances gradient imbalance and learning\u0002rate sensitivity. Experiments on CIFAR100, CUB, and ImageNet-R show consistent gains across multiple\nbaselines under multi-learning-rate evaluation. The work is practically motivated and conceptually coherent, offering a bridge between offline and online CL paradigms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The topic is timely and relevant, targeting the underexplored Offline→Online transition in CL with\nclear theoretical and practical significance. \n\n2) The proposed P+FGH framework effectively addresses two core challenges of online CL —\ncatastrophic forgetting and gradient imbalance — through a minimal-intrusive and generalizable\ndesign. \n\n3) Experiments are comprehensive, covering diverse datasets and learning rate settings, demonstrating\nthe method’s robustness and transferability."}, "weaknesses": {"value": "1) The online scenario remains quasi-online, relying on pre-defined task splits rather than fully\nstream-based settings, limiting realism. \n\n2) The novelty of both P and FGH is moderate: the prototype update mirrors CoPE (2021), and FGH\nlacks formal convergence or stability analysis and clear differentiation from prior hypergradient\nmethods. \n\n3) Recent baselines (e.g., PROL 2025, PMLR 2025) are missing, and parameter details (γ, β₁/β₂, Si- Blurry settings) are insufficiently reported, affecting reproducibility and fairness."}, "questions": {"value": "1) How would the proposed FGH behave under fully stream-based or class reappearance settings?\n\n2) Has γ been systematically tuned or theoretically analyzed for robustness across datasets?\n\n3) Can the authors quantify FGH’s computational overhead compared to existing hypergradient or adaptive LR\noptimizers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eVngw6uYmr", "forum": "gWpm3tpncq", "replyto": "gWpm3tpncq", "signatures": ["ICLR.cc/2026/Conference/Submission17004/Reviewer_Pckv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17004/Reviewer_Pckv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17004/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918686224, "cdate": 1761918686224, "tmdate": 1762927024634, "mdate": 1762927024634, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}