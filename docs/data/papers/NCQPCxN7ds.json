{"id": "NCQPCxN7ds", "number": 5325, "cdate": 1757900617583, "mdate": 1759897981358, "content": {"title": "Local Geometry Attention for Time Series Forecasting under Realistic Corruptions", "abstract": "Transformers have demonstrated strong performance in time series forecasting, yet they often fail to capture the intrinsic structure of temporal data, making them susceptible to real-world noise and anomalies. Unlike in vision or language, the local geometry of temporal patterns is a critical feature in time series forecasting, but it is frequently disrupted by corruptions.\nIn this work, we address this gap with two key contributions. First, we propose Local Geometry Attention (LGA), a novel attention mechanism theoretically grounded in local Gaussian process theory. LGA adapts to the intrinsic data geometry by learning query-specific distance metrics, enabling it to model complex temporal dependencies and enhance resilience to noise. Second, we introduce TSRBench, the first comprehensive benchmark for evaluating forecasting robustness under realistic, statistically-grounded corruptions.\nExperiments on TSRBench show that LGA significantly reduces performance degradation, consistently outperforming both Transformer and linear model. These results establish a foundation for developing robust time series models that can be deployed in real-world applications where data quality is not guaranteed. Our code is available at: https://anonymous.4open.science/r/LGA-5454.", "tldr": "We propose LGA, the local geometry-aware attention mechanism based on the local Gaussian Process, and introduce TSRBench, the first benchmark for evaluating time series forecasting model robustness under realistic corruptions.", "keywords": ["Local Geometry", "Local Gaussian Process", "Transformer Architecture", "Time Series Analysis", "Corruption Benchmark"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c004640e11d1102371361efee0659d0f2b465b71.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new attention mechanism for time series forecasting, which is termed Local Geometry Attention. The designed LGA is inspired by local Gaussian process theory and aims to learning query-specific distance metrics and enable the model to learn complex temporal dependencies and enhance resilience to noise. Also, TSRBench, a comprehensive benchmark for evaluating time series forecasting robustness, is proposed."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation seems clear with intuitive figures (Fig.1 and 2)\n\n2. Clearly written and well-presented with source codes."}, "weaknesses": {"value": "1. The technical details could be improved in a clearer way.\n\n2. According to the performance reported in Table 2, the improvements led by the proposed method seem to be marginal.\n\n3. the motivation of TSRBench is not clear. And more comprehensive datasets and baselines should be involved."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lq4QsjZlep", "forum": "NCQPCxN7ds", "replyto": "NCQPCxN7ds", "signatures": ["ICLR.cc/2026/Conference/Submission5325/Reviewer_9J2N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5325/Reviewer_9J2N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575378360, "cdate": 1761575378360, "tmdate": 1762918008865, "mdate": 1762918008865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The contribution of this paper is twofold; it presents a method called local geometry attention designed (LGA) to model local structures of time series data in transformer neural networks, and it introduces TSRBench a corruption suite for evaluating robustness of time series models. In the empirical evaluation part of the submission, the LGA models are evaluated using TSRBench.\n\nMore specifically, LGA uses local Gaussian process regression to model the data/representation manifold near the query points of a an attention module. Since, computing the local geometry aware score of the model is computationally prohibitive, a second neural network is trained that predicts the metric tensor.  \nThe TSRBench consists of a method to insert shifts and spikes into time series at 5 different severity levels.\nIn the empirical evaluation, LGA is inserted into different transformer models (with PatchTST being the default choice) to assess robustness of long-term time series forecasts."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The LGA technique is universal in  the sense that it can easily be integrated into different transformer architectures and likely also in other networks that use attention modules.\n- On the six corrupted time series datasets that were used, LGA (+PatchTST) consistently achieves the best forecasting accuracy especially at large severity levels, indicating that LGA adds robustness wrt shift and spike corruptions.\n- The benchmarking tool comes with predefined and carefully calibrated severity levels which simplify its use.\n- The paper is very well written, and therefore easy to read and understand. (But certain parts need more explanation, see questions)\n- The paper is accompanied by an extensive supplementary material which provides the mathematical background, implementation details and further experiments."}, "weaknesses": {"value": "- The LGA approach is only evaluated with respect to the (synthetic) TSRBench corruptions, i.e. shifts and spikes. It is not evaluated on other types of corruptions, for example on the anomaly types that are used in (Cheng et al, RobustTSF: ..., ICLR2024) or the synthesized outliers from (Lai et al., Revisiting Time Series Outlier Detection: Definitions and Benchmarks, NeurIPS 2021 Dataset and Benchmark track). One could therefore think that the benchmarks are designed to favor LGA.\n- The LGA approach is only evaluated on forecasting tasks. \n- It seems that the severity levels of TSRBench need to be calibrated for every dataset individually, and the guidelines are quite vague, i.e. \"we ensured that the performance differences between severity levels were neither too large nor too small, and that the degree of noise strictly increased with each level\".   \nMoreover, the resulting hyperparameter choices are model dependent and require training one (or multiple) neural networks per parameter choice. This hinders extending the benchmark to new datasets in a standardized manner, one of the key goals of this work (\"Despite the need, a standardized benchmark for such realistic corruptions in time series remains a significant gap\")\n- The paper makes several implementation choices whose effects are not ablated empirically. This includes learning the metric tensor with an additional neural network (instead of computing it for every query) and using only a diagonal metric. Moreover, while it is stated that both are done because of computational costs, no runtime or complexity analysis of them are presented. \n- For the datasets that are used, the manuscript refers to the Autoformer paper (Wu et al., NeurIPS 2021), but the actual originators of the datasets are not credited."}, "questions": {"value": "- Is the robustness of LGA by design, or did it surprise you? If by design, what is the reason or intuition?\n- Would you explain in more detail how the network that predicts the metric tensor is trained. Is this done simultaneously to training the transformer, afterwards, or  alternately? Considering the former, could you explain again, why training such a network is more efficient than computing the metric, or if it only shift compute from inference to training?\n- Would you explain the connection to Riemannian geometry and what the point of the detour to the geodesic distance was. Is the connection only that the score is determined by a bilinear form that depends on the query point?\n- Would you compare TSRBench with this very recent robustness benchmark ( [Janßen et al., arxiv, Oct 2025](https://arxiv.org/pdf/2510.04900) )"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vFAY3Ize5M", "forum": "NCQPCxN7ds", "replyto": "NCQPCxN7ds", "signatures": ["ICLR.cc/2026/Conference/Submission5325/Reviewer_ESKs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5325/Reviewer_ESKs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761651571632, "cdate": 1761651571632, "tmdate": 1762918008495, "mdate": 1762918008495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the robustness of time series forecasting transformer models under realistic corruptions. It first propose a novel attention mecanism derived from local Gaussian process theory called Local Geometry Attention (LGA). It takes into account the geometry of data to be more robust to corruptions in the time series data. It can be implemented by learning a neural network over the query vectors seen during training. Then, the paper introduces TSRBench, a benchmark with realistic corruptions (spikes and level shift) to estimate the robustness of models for forecasting. Experiments over 6 common forecasting benchmarks (with their corrupted versions) and 3 baselines show the performance benefits of the LGA attention incorporated into a PatchTST model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-written and the proposed approach well motivated\n- The proposed benchmark is very interesting with sounded corruptions (inspired from ImageNet-C benchmark)\n- The experiments are convincing to show the performance improvement brought by LGA"}, "weaknesses": {"value": "- While the proposed method is sound, it requires training neural networks to compute the Local Geometry Attention. The current submission is missing a computational cost comparison when compared to self-attention\n- Since the main contribution is a novel attention mecanism, it would be interesting to see the comparison to other types of attention in addition to the traditional one (e.g., channel-wise attention [1, 2])\n- Connected to the previous weakness, it would be interesting to compare to more models such as iTransformer [1] or SAMformer [2] that reported robust performance with temporal/spatial and spatial attention respectively.\n- While the benchmarks is interesting, I believe the current proposal would be strengthen with additional models and datasets to clearly show the failure of other models under realistic corruptions\n- It would be interesting to see how LGA behaves when integrated into other models than PatchTST, for instance could it be applied on SAMformer with channel-wise attention or i-transformer with channel and temporal wise attention?\n\nOverall, I find the submission interesting with a well-motivated LGA and robustness benchmark. However, it would be strengthen with additional methods and computational cost comparison.\n\n*References*\n\n[1] Liu et al. iTransformer: Inverted Transformers Are Effective for Time Series Forecasting. ICLR 2024\n\n[2] Ilbert et al. SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel Wise Attention. ICML 2024"}, "questions": {"value": "- The LGA is computed by training a neural network on query vectors seen during training. Does it mean that there are two interconnected training loops: one for the model, one for the LGA module? If it is the case, what is the additional computational cost and does it vary with the number of training steps? Could the authors clarify that please? \n- Were the experiments run over several seeds? If yes, could the authors make the standard deviation appear? If no, could the authors please conduct the experiments with 2 additional seeds to have an idea of the significance of the performance improvement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rqaffk21DR", "forum": "NCQPCxN7ds", "replyto": "NCQPCxN7ds", "signatures": ["ICLR.cc/2026/Conference/Submission5325/Reviewer_WvGd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5325/Reviewer_WvGd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758494887, "cdate": 1761758494887, "tmdate": 1762918008218, "mdate": 1762918008218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript introduces Local Geometry Attention, an attention mechanism for time-series forecasting that replaces dot-product similarity with a query-specific metric estimated from local Gaussian-process theory, aiming to make attention scores geometry-aware and less sensitive to noisy or anomalous keys. It also proposes TSRBench, a robustness benchmark that injects spike and level-shift corruptions using statistically grounded processes and calibrated severities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "LGA is designed to improve robustness of attention to local anomalies and noise.\n\nTSRBench is a potentially useful benchmarking tool for researchers to evaluate robustness under controlled corruptions."}, "weaknesses": {"value": "The description of LGA is not detailed enough. A step-by-step derivation of the attention score and kernel construction is needed to enhance the clarity. Please also provide the tensor shapes at each stage.\n\nThe GP motivation is under-explained: it is unclear how local GP assumptions translate into better attention weights or how the geometric structure is actually learned from data.\n\nThe set of baselines is limited, making it hard to judge whether robustness gains hold against recent strong time-series Transformers."}, "questions": {"value": "(a) Please expand the discussion of non-uniform, locally clustered time-series observations, since this is the core motivation for introducing LGA.\n\n(b) Explain in detail how queries and keys are embedded in the 2D panel in Figure 2 and how this visualization demonstrates a more effective attention mechanism.\n\n(c) Show how the proposed local kernel–covariance compares to vanilla dot-product attention under the same setup, and quantify robustness gains.\n\n(d) In Figure 5, replace training time with FLOPs to make comparisons hardware-agnostic.\n\n(e) Add more recent baselines in Table 2 to better position LGA and TSRBench."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AYatRHaKjq", "forum": "NCQPCxN7ds", "replyto": "NCQPCxN7ds", "signatures": ["ICLR.cc/2026/Conference/Submission5325/Reviewer_Jv6F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5325/Reviewer_Jv6F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5325/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962704600, "cdate": 1761962704600, "tmdate": 1762918007956, "mdate": 1762918007956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}