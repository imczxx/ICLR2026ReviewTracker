{"id": "qr8leFAPrd", "number": 23901, "cdate": 1758350040607, "mdate": 1759896791370, "content": {"title": "What is Important? Internal Interpretability of Models Processing Data with Inherent Structure", "abstract": "This paper introduces a methodology for constructing interpretable neural networks that quantify the importance of structured input components directly within their internal mechanisms, thereby eliminating the need for traditional explanation methods that rely on post-hoc saliency map generation. Our approach features a two-stage training procedure. First, component specific representations and importance scores are discovered using appropriately designed convolutional neural networks, which are trained jointly. Second, an architecture with relaxed structural constraints, leveraging the previously acquired knowledge, is fine-tuned to capture spatial dependencies among components and to integrate global context. We systematically evaluate our method on Oxford Pets, Stanford Cars, CUB-200, Imagenette, and ImageNet, measuring interpretability-performance trade-offs with metrics for semanticity, sparsity, reproducibility, and, when required, causality (via insertion/deletion-inspired scores). Our architecture achieves improved semantic alignment with ground-truth segmentation annotations compared to post-hoc saliency maps, which, when available, serve as surrogates for expected saliency maps. At the same time, it maintains low variance in importance scores across runs, demonstrating strong reproducibility. Crucially, our architecture provides interpretability gains without sacrificing accuracy. In fact, both with non-pretrained and pretrained backbones, it frequently achieves higher predictive performance than parameter-matched baselines. Overall, compared to both conventional models and post-hoc interpretability techniques under matched computational budgets, our framework produces models that are accurate, stable, and that deliver causally grounded explanations.", "tldr": "This paper presents a method for interpretable neural networks that quantify input component importance internally, using a two-stage training process, yielding accurate, stable models with causally grounded explanations.", "keywords": ["deep learning", "interpretability by design", "saliency maps", "data with inherent structure"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8bd95b9f840b47aa3712f210059bf50949cb6c1c.pdf", "supplementary_material": "/attachment/3a428b5ee86a12317eb6aa8f2e6f0696d59db576.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a two-stage inherently interpretable image classification framework centered on *learned importance scores*.\n (1) IA (Importance Architecture): the image is divided into non-overlapping patches, each encoded independently into embeddings (E), and an auxiliary ImportanceNet predicts an importance weight (a \\in [0,1]) for each patch; the classifier aggregates the embeddings weighted by (a).\n (2) EA (Embedding Architecture): the learned (a) is frozen, and a light-weight ContextNet integrates contextual dependencies over the weighted embeddings.\n (3) PA (Pixel Architecture): the same importance mask (a) is applied at the pixel level before feeding into a standard backbone classifier.\n\nThe experiments evaluate four main aspects: semantic alignment (IoU and a custom distance metric (d)), sparsity, reproducibility, and causality tests (insertion/deletion analysis on EA). Several datasets are tested, and accuracy curves and tables are reported."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed modules make minimal architectural modifications while enforcing importance estimation *within* the inference path (i.e., not a post-hoc explanation).\n- The authors visualize patch-level importance heatmaps across different patch and embedding scales (Fig. 3) and show masked images with different thresholds (t) (Figs. 6–9)."}, "weaknesses": {"value": "#### **1. Overstated novelty; limited comparison scope (biased evaluation)**\n\n- The paper’s *Related Work* and experimental comparisons focus almost exclusively on post-hoc saliency methods (Grad-CAM, IG, SHAP, Occlusion, etc.), claiming superior semantic alignment. However, many representative in-model / inherently interpretable approaches (e.g., prototype-based, tree-structured, alignment- or concept-based models) are neither discussed nor compared. This omission systematically inflates the perceived novelty and contribution.\n- Structurally, the proposed method is essentially a gating/masking mechanism followed by lightweight context integration—the paper itself states that EA fuses contextual information over the reweighted embeddings $E'$ computed by applying $a$ to $E$ .  Such “mask + context” architectures are already common in prior inherently interpretable works.\n   Without fair comparison against strong in-model baselines, the claimed originality and advantage are not convincing.\n\n------\n\n#### **2. Simplicity and overlap with existing gating/masking paradigms**\n\n- The IA’s main modification is a separate ImportanceNet that outputs patch-wise weights $a$, which are then applied as element-wise gates before aggregation.\n   EA merely freezes $a$ and adds a shallow CNN for contextual fusion.\n- Conceptually, this is equivalent to a soft attention or gating layer without additional theoretical constraint or empirical justification showing superiority over numerous prior *learned mask / attention-based interpretability* models.\n   As such, the methodological novelty is limited for an ICLR-level contribution.\n\n------\n\n#### **3. Metric and baseline comparability issues (Sections 4.1 & 4.2)**\n\n##### **3-a. Inconsistency in the (d) metric computation**\n\n- The proposed $d$ metric is defined on patch-level importance values $a$ and patch-averaged semantic references (m) (L1 distance). Yet most post-hoc methods produce pixel-level saliency maps. The paper does not clearly state how these pixel-level maps were converted to patch-level scores (mean? max? normalization?). It also omits whether the same normalization and binarization thresholds were used across methods. Without consistent scaling and thresholding, cross-method comparisons on $d$  are not rigorous.\n- Furthermore, the semantic mask (m) itself may be a weak reference: important patches often contain a mix of object and background pixels (e.g., a cat ear patch with more background but higher discriminative value than a belly patch).\n   Hence, the (m)-based ground-truth importance may not faithfully represent semantic relevance, compromising the validity of $d$ .\n\n##### **3-b. Incomplete accuracy baselines (Section 4.2)**\n\nThe classification accuracy comparison includes only the authors’ three variants (IA, EA, PA). This is insufficient to claim that “our interpretable mechanism preserves or improves predictive performance.” Also, clarify the hyperparameter protocol—e.g., was the PA threshold $t$ tuned only on the validation set and fixed for test reporting?\n\n------\n\n#### **4. Poor writing and presentation quality**\n\n- The manuscript is difficult to read and loosely organized; key experimental details are scattered across sections.\n- Most figures are non-vector graphics with tiny fonts, making them hard to interpret. Notably, Figure 5 (“Kernel density estimate plots…”) appears but is never referenced or discussed in the main text, which undermines clarity and professionalism."}, "questions": {"value": "**Details of $d$ Computation:**\n How are the pixel-level heatmaps from different baselines unified into the patch-level $a$? Was a consistent normalization and thresholding strategy applied across all methods?\n\n**Threshold Selection for PA:**\n In Table 2 and Figures 7–9, do the “best results” correspond to the threshold $t$ selected *only on the validation set* and then reported *once* on the test set?\n\n**Missing Strong Baselines:**\n Why are strong *intrinsically interpretable* methods such as ProtoPNet, ProtoTree, NBDT, and B-cos not included for end-to-end comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XCw65GdE6h", "forum": "qr8leFAPrd", "replyto": "qr8leFAPrd", "signatures": ["ICLR.cc/2026/Conference/Submission23901/Reviewer_H6Gc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23901/Reviewer_H6Gc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706640031, "cdate": 1761706640031, "tmdate": 1762942847611, "mdate": 1762942847611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for inherently interpretable neural networks that directly encode and quantify the importance of structured input components—such as regions in an image, tokens in a sequence, or nodes in a graph—within the model’s internal computations. The proposed approach integrates explainability through a two-stage methodology. In the first stage, convolutional networks jointly learn component representations and importance scores. In the second, a refined model relaxes structural constraints to capture spatial and contextual dependencies while preserving interpretability anchors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The two-stage procedure elegantly separates the discovery of component importance from the modeling of global dependencies, offering clear interpretability anchors and facilitating adaptation to other data types (images, sequences, graphs).\n\nComprehensive experiments on multiple benchmark datasets show consistent improvements and predictive accuracy compared to state-of‑the‑art baselines."}, "weaknesses": {"value": "Lack of novelty. Your approach analyzes data at the patch level, which seems conceptually similar to ViT‑Shapley[1]. Could you clarify what advantages your method offers compared to ViT‑Shapley, or what specific motivation drives your work beyond that prior approach?\n\nIn ViT‑Shapley, the fairness of patch‑level importance assessment is ensured through the use of Shapley value computations. How does your method guarantee comparable—or superior—fairness in evaluating the contribution of each visual patch?\n\nFurthermore, regarding the metrics for interpretability: human studies are often considered an intuitive and widely accepted means of assessing interpretability. However, such evaluations appear absent from your paper. Could you explain the reasoning behind this choice or provide justification for omitting human studies?\n\n[1]: Learning to Estimate Shapley Values with Vision Transformers."}, "questions": {"value": "See Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RfvMXD64k2", "forum": "qr8leFAPrd", "replyto": "qr8leFAPrd", "signatures": ["ICLR.cc/2026/Conference/Submission23901/Reviewer_qr3w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23901/Reviewer_qr3w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739394107, "cdate": 1761739394107, "tmdate": 1762942847334, "mdate": 1762942847334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for building intrinsically interpretable neural networks that can directly measure the importance of structured input components within the model itself, eliminating the need for post-hoc explanation techniques such as saliency maps. The approach employs a two-stage training process:\n1. A specialized CNN jointly learns component-specific representations and importance scores.\n2. A refined architecture with relaxed structural constraints is then fine-tuned to capture spatial dependencies and global context.\n\nThe authors evaluate the method on multiple datasets, including Oxford Pets, Stanford Cars, CUB-200, Imagenette, and ImageNet, analyzing the interpretability–performance trade-off using metrics such as semanticity, sparsity, reproducibility, and causality.\nResults show that the proposed architecture achieves better semantic alignment with ground-truth annotations and higher reproducibility than traditional post-hoc saliency methods. Moreover, it provides interpretability improvements without sacrificing accuracy—and often even exceeds the predictive performance of parameter-matched baselines, both with and without pretrained backbones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "They propose a novel method that directly quantifies the importance of structured input components within the model itself, eliminating the need for post-hoc explanation techniques. The method is extensively evaluated across multiple datasets, and the quantitative results demonstrate its clear superiority over existing approaches."}, "weaknesses": {"value": "The baseline approaches used for comparison are relatively outdated. There exist more advanced methods beyond Grad-CAM that could provide a stronger and fairer evaluation, such as RISE (Petsiuk et al., 2018) and Shap-CAM (Zheng et al., 2022). Moreover, it is unclear why the authors only compare against Grad-CAM without including its improved variant, Grad-CAM++. Limiting comparisons to older methods weakens the validity of the claimed superiority of the proposed approach.\n1. Vitali Petsiuk, Abir Das, and Kate Saenko. RISE: randomized input sampling for explanation of black-box models. In British Machine Vision Conference 2018, BMVC 2018, Northumbria University, Newcastle, UK, September 3-6, 2018, page 151, 2018.\n2. Quan Zheng, Ziwei Wang, Jie Zhou, and Jiwen Lu. 2022. Shap-CAM: Visual Explanations for Convolutional Neural Networks Based on Shapley Value. In Computer Vision–ECCV 2022: 17th European Conference. Springer, Tel Aviv, Israel, 459–474"}, "questions": {"value": "Could the authors provide an analysis of the running time or computational cost? This information is important, especially if the method is intended for large-scale or repeated experiments. \n\nAdditionally, it would be helpful to specify how many images were used for evaluation in each dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qeXqi6kHq2", "forum": "qr8leFAPrd", "replyto": "qr8leFAPrd", "signatures": ["ICLR.cc/2026/Conference/Submission23901/Reviewer_8K9A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23901/Reviewer_8K9A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879454744, "cdate": 1761879454744, "tmdate": 1762942846942, "mdate": 1762942846942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an interpretable neural network that reduces the reliance on post-hoc saliency methods. The explanation scores are learned jointly with the model parameters. The approach first quantifies the importance of individual components, after which the model learns spatial and contextual dependencies while preserving the discovered importance structure. The quality of the explanations is evaluated in terms of semanticity, sparsity, reproducibility, and causality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- I believe the idea of quantifying component importance and preserving it is valuable, as it reduces unnecessary complexity and encourages the model to align with human-interpretable concepts.\n- The methodology is tested on several datasets, demonstrating good interpretability insights."}, "weaknesses": {"value": "- **Importance mask:** The proposed “importance mask” appears conceptually similar to an **attention mechanism**; the paper does not clearly articulate how it differs from standard attention-based interpretability.\n- **The use of patches:** The claim that image patches are “semantically meaningful” is questionable—**patches are not inherently semantic units**, and true semantic meaning would require segmentation or context modeling. Moreover, the **patch-based decomposition** risks losing coherence when important concepts span multiple patches, potentially diluting concept-level importance and reducing structural interpretability.\n- **Clarity of the diagram:** Figure 1 is unclear - IA, EA, and PA modules are not visually distinguished, the text is too small, and the relationships among components are ambiguous.\n- **Clarity of concepts:** the interaction between **EA and PA** (whether they are trained jointly or separately) is not well explained, and the connection between the EA and its interpretability claims remains unclear.\n- **Semantics:** The terminology of *“semantic structure of embeddings”* is misleading since embeddings correspond to **patches**, not true semantic entities. Moreover, measuring “semanticity” by classification accuracy on non-segmented datasets only reflects **alignment with model predictions**, not genuine semantic alignment. With respect to the metric semanticity, the paper seems to define semanticity with full object segmentation. How is semanticity defined when only part of an object is relevant? Is the goal closer to segmentation? This may explain why Grad-CAM performs well, as it typically highlights larger regions.\n- **Causality metrics:** The paper’s **causality analysis** relies on insertion/deletion metrics, which do not capture causal dependencies among correlated features and are **not novel and should be cited [1].**\n- **Experiments and discussion: Figure 7** contradicts the claim in Section 4.2: only CUB-200 maintains accuracy, while other datasets show degradation. The **insertion/deletion experiments lack baseline comparisons**, making it difficult to assess the actual effectiveness of the proposed method. The paper includes masked insertion/deletion images in the main text, but quantitative results are only reported in the supplementary material, and no baseline comparisons are provided.\n\n[1] Covert, I., Lundberg, S., & Lee, S. I. (2021). Explaining by  removing: A unified framework for model explanation. Journal of Machine  Learning Research, 22(209), 1-90."}, "questions": {"value": "- I included some questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o42alLjuhh", "forum": "qr8leFAPrd", "replyto": "qr8leFAPrd", "signatures": ["ICLR.cc/2026/Conference/Submission23901/Reviewer_37Le"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23901/Reviewer_37Le"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901388276, "cdate": 1761901388276, "tmdate": 1762942846639, "mdate": 1762942846639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}