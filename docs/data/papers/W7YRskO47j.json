{"id": "W7YRskO47j", "number": 7350, "cdate": 1758017252861, "mdate": 1759897857979, "content": {"title": "CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild", "abstract": "Hands play a central role in daily life, yet modeling natural hand motions remains\nunderexplored. Existing methods that tackle text-to-hand-motion generation or\nhand animation captioning rely on studio-captured datasets with limited actions\nand contexts, making them costly to scale to “in-the-wild” settings. Further,\ncontemporary models and their training schemes struggle to capture animation\nfidelity with text–motion alignment. To address this, we (1) introduce ‘3D Hands\nin the Wild’ (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned\ntext, and (2) propose CLUTCH, an LLM-based hand animation system with two\ncritical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand\nmotion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-\nHIW, we propose a data annotation pipeline that combines vision–language models\n(VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of\negocentric action videos covering a wide range of scenarios. To fully capture\nmotion in-the-wild, CLUTCH employs SHIFT, a part–modality decomposed VQ-\nVAE, which improves generalization and reconstruction fidelity. Finally, to improve\nanimation quality, we introduce a geometric refinement stage, where CLUTCH is\nco-supervised with a reconstruction loss applied directly to decoded hand motion\nparameters. Experiments demonstrate state-of-the-art performance on text-to-\nmotion and motion-to-text tasks, establishing the first benchmark for scalable\nin-the-wild hand motion modelling. Code, data and models will be released.", "tldr": "CLUTCH is an LLM-based model designed to synthesize and caption natural, in-the-wild 3D hand motions.", "keywords": ["Human Motion Synthesis", "Hand motion synthesis", "LLM", "Motion in-the-wild"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4dc3f72dae72366401a68d42f440194a71789daa.pdf", "supplementary_material": "/attachment/1fc327d7f83249dfa1091fb95cedc1343b5f5fc4.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of text-conditioned hand motion generation in unconstrained \"in-the-wild\" settings. The authors make three primary contributions: (1) a novel data annotation pipeline combining VLMs and 3D hand trackers to create the 3D-HIW dataset with 32K hand motion sequences and aligned text descriptions, (2) SHIFT, a part-modality decomposed VQ-VAE tokenizer that separately encodes trajectory and pose for left and right hands, and (3) CLUTCH, an LLM-based system featuring a geometric refinement training stage that applies reconstruction losses directly to decoded motion parameters. Experiments demonstrate improvements over baselines including HumanMDM, MotionGPT, and T2M-GPT on both text-to-motion and motion-to-text tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The focus on in-the-wild hand motion generation addresses a significant gap in the literature, moving beyond studio-captured datasets with limited diversity.\n2. The 3D-HIW dataset with 32K sequences represents approximately 10× the scale of GRAB and ARCTIC, offering substantially greater diversity in actions (1045 verbs, 1355 objects) and scenarios.\n3. The two-stage approach using Parallel Chain-of-Thought prompting followed by closed-vocabulary refinement is well-motivated and demonstrates improved GPT-scores (6.9) compared to existing methods."}, "weaknesses": {"value": "1. Dataset quality concerns:\n- The reliance on HaWor for 3D reconstruction may introduce systematic errors or artifacts that propagate through the entire pipeline.\n- The filtering criteria (80% hand visibility, acceleration thresholds) may introduce biases toward certain types of motions.\n- No human evaluation or validation of the reconstructed 3D motions is provided.\n\n2. Geometric refinement stage lacks clarity:\n- The Gumbel-Softmax formulation is mentioned but not detailed.\n- The balance between cross-entropy and reconstruction loss (α, λ) appears critical but hyperparameter sensitivity is not thoroughly analyzed.\n- The comparison to EgoLM's soft-blending approach (Table 5) shows only marginal improvements, raising questions about whether the added complexity is justified.\n3. The paper primarily compares against methods designed for full-body motion (HumanMDM) or general motion (MotionGPT). Comparisons with recent hand-specific methods or SOTA human motion generation method (MoMask) are missing."}, "questions": {"value": "1. Have you conducted human evaluation of the generated annotations? What is the agreement between your automated annotations and human-written descriptions?\n2. How does the model perform on motions significantly different from the training distribution? Can it generate novel compositions of actions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ADq5NoIK49", "forum": "W7YRskO47j", "replyto": "W7YRskO47j", "signatures": ["ICLR.cc/2026/Conference/Submission7350/Reviewer_5Tyz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7350/Reviewer_5Tyz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899491316, "cdate": 1761899491316, "tmdate": 1762919486989, "mdate": 1762919486989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work introduces a large-scale dataset of human hands automatically extracted from a large-scale egocentric video dataset, consisting of motion sequences and accompanying textual activity descriptions. Additionally, a method for text-to-hand trajectory generation and hand trajectory-to-text description is proposed, benefiting from a novel hand tokenization scheme. The proposed method outperforms multiple baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The scale of the hand motion dataset is unprecedented. The work provides a thorough analysis of the introduced dataset. The proposed method outperforms multiple baselines on the introduced dataset. The design choices of the method and filtering of the dataset are quantitatively supported by ablation studies. The proposed method supports both the text-to-motion and motion-to-text tasks simultaneously."}, "weaknesses": {"value": "For a work introducing a novel dataset as its main contribution, more qualitative examples of the generated trajectories as well as hand poses and coarse/fine-grained textual descriptions are necessary. This is a major weakness, especially coupled with the following concern:\nThe noun distribution in Figure 11 shows several undesirable entries being common in the dataset, e.g. \"hand\" (hand touching a hand?) and \"cut\" (a verb?). This raises questions about the quality of the dataset's noun/verb annotations.\nThe efficacy of the method is supported by its strong performance against baselines. However, at no point does the work mention any human verification of the generated dataset. As such, it is difficult for a reviewer to ascertain its quality. A human study would have greatly benefited the work. The contribution from the dataset side is thus limited for me.\n\nIn addition to insufficient qualitative examples of the dataset, more qualitative examples of the method's output must be included.\n\nThe proposed method was only evaluated on the introduced dataset, and not on other datasets such as GigaHands or ARCTIC.\n\nIt would be good to add qualitative examples of trajectories to the rightmost t-SNE plot in Fig. 6. Merely covering a broader t-SNE range of hand poses could also be achieved by a large fraction of erroneous poses in the dataset.\n\nThe object is not at all considered in the proposed dataset and method, limiting their usefulness.\n\nThe work could benefit from a table comparing it to existing datasets in terms of scenario count, total length, diversity (number of objects), etc."}, "questions": {"value": "In Section 3.3, what is meant by \"top-200\" and \"top-3000\"?\n\nWhat is the purpose of the introduced \"testing\" split if numbers are reported on the validation split only?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hTPtO0pgOQ", "forum": "W7YRskO47j", "replyto": "W7YRskO47j", "signatures": ["ICLR.cc/2026/Conference/Submission7350/Reviewer_kcC7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7350/Reviewer_kcC7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969101778, "cdate": 1761969101778, "tmdate": 1762919486125, "mdate": 1762919486125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The researchers present two main contributions:\n1. A large-scale dataset, 3D Hands in the Wild (3D-HIW): They construct this dataset using 3D hand trackers to extract hand trajectories from egocentric videos and leverage vision-language models to generate corresponding textual annotations.\n2. The CLUTCH model: A transformer-based model trained on 3D-HIW, introducing two key innovations: (a). SHIFT: A novel tokenization method that decomposes hand motions into separate trajectory and pose components for each hand, improving generalization and yielding more accurate motion reconstructions. (b). Geometric Refinement: A fine-tuning stage applied to the language model that enhances the geometric accuracy and realism of the generated animations. This includes a reconstruction loss directly applied to the decoded 3D motion parameters."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The motion dataset represents a highly valuable contribution to the field.\n2. The SHIFT mechanism is well-motivated, effectively decoupling trajectory-level movements from fine-grained finger motions, and ablation studies demonstrate its effectiveness. Also, enabling bidirectional motion–text decoding is an innovative design, and it is noteworthy that this approach performs successfully in practice."}, "weaknesses": {"value": "1. Since the hand motions are synthesized from a single textual description, how is motion diversity ensured? Are there mechanisms in CLUTCH to generate varied hand trajectories or poses from the same caption?\n2. While the proposed approach is effective for isolated hand motions and the datasets are centered on hand-only movements, its omission of object interactions could constrain its applicability to more realistic, object-involved settings."}, "questions": {"value": "1. What \"Div →\" means ?\n2. The presentation could be improved, for example, inconsistent title capitalization (Line458), text size in tables, and line-breaking logic (Line271).\n3. Because text is discrete while motion is continuous, it may be more natural to model hand motions using architectures like TransFusion, similar to how VLMs such as Pi0?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4B3gzw6F0n", "forum": "W7YRskO47j", "replyto": "W7YRskO47j", "signatures": ["ICLR.cc/2026/Conference/Submission7350/Reviewer_oQBC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7350/Reviewer_oQBC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996822138, "cdate": 1761996822138, "tmdate": 1762919485330, "mdate": 1762919485330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a system for text-conditioned 3D hand motion generation and captioning in in-the-wild settings. The two primary contributions:\n1. A large-scale dataset of 32,000 3D hand-motion sequences paired with textual descriptions, sourced from egocentric videos (Ego4D). \n2. An LLM-based system that models motion tokens. A new VQ-VAE tokenizer that decomposes hand motion into separate codebooks for trajectory and pose\nThe authors demonstrate their experimental results on their new benchmark for both text-to-motion and motion-to-text tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The 3D-HIW dataset is a good contribution. The proposed VLM-based annotation pipeline is a clever and scalable approach to captioning this in-the-wild data.\n2. The paper does a good job of validating its design choices with the ablations for the SHIFT tokenizer and the training stages.\n3. The paper is well-written, the figures are informative, and the core ideas are articulated clearly."}, "weaknesses": {"value": "1. Lack of Hand-Object Interaction (HOI) is the most significant limitation. The paper frames its work as \"in-the-wild\"  yet the model only generates 3D hand motion. It does not model the objects being interacted with. True in-the-wild motion is almost entirely defined by HOI, which is explicitly left as future work.\n2. The model is trained and evaluated exclusively on the authors' new dataset. It is unclear how CLUTCH would perform on other public benchmarks."}, "questions": {"value": "1. How much computational resource and time does it take to generate a dataset of this size? It's key to show whether the proposed method is scalable.\n2. The paper says currently their current method doesn't support HOI generation. How is Figure 1 generated? Do you manually align the hand trajectory to fit the objects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oflSKPB11M", "forum": "W7YRskO47j", "replyto": "W7YRskO47j", "signatures": ["ICLR.cc/2026/Conference/Submission7350/Reviewer_x3VX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7350/Reviewer_x3VX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7350/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762473301742, "cdate": 1762473301742, "tmdate": 1762919484463, "mdate": 1762919484463, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}