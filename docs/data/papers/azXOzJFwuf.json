{"id": "azXOzJFwuf", "number": 12055, "cdate": 1758205433285, "mdate": 1759897536987, "content": {"title": "FuseNorm: Achieving the Best of Both Worlds from PreNorm and PostNorm", "abstract": "The success of Large Language Models (LLMs) hinges on the stable training of deep Transformer architectures. A critical design choice is the placement of normalization layers, leading to a fundamental trade-off: the PreNorm architecture ensures training stability at the cost of potential performance degradation in deep models, while the PostNorm architecture offers strong performance but suffers from severe training instability. In this work, we propose FuseNorm, a novel technique designed to resolve this dilemma by integrating the strengths of both paradigms. FuseNorm adopts the clean residual path of PreNorm to stabilize signal propagation while employing a PostNorm-style computation that normalizes the output of the residual connection, thereby enhancing model performance. We provide a theoretical analysis demonstrating that FuseNorm, combined with a principled scaling strategy, maintains bounded signal variance throughout the network, preventing the gradient issues that plague PostNorm models, and alleviating the representation collapse of PreNorm. Empirically, FuseNorm consistently outperforms standard normalization schemes in both dense and Mixture-of-Experts (MoE) scenarios, paving the way for more powerful and stable Transformer architectures.", "tldr": "We propose FuseNorm for Transformers to achieve Pre-layernorm's stability and Post-layernorm's performance without compromises.", "keywords": ["LLM", "PreNorm", "PostNorm", "Layer Normalization", "Architecture", "foundation Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d813e8d357e9d9a1092c232ece8a42c5f30594e7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the trade-off between PreNorm and PostNorm Transformer architectures. PreNorm stabilizes training but often underperforms at scale. In contrast, PostNorm achieves better final performance but is prone to instability in deep networks. To reconcile these opposing properties, the authors propose FuseNorm, a simple yet effective modification that preserves the clean gradient path of PreNorm while applying Layer Normalization at the output like PostNorm."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core ideas are well-motivated and supported by theoretical insights.\n\nThe method shows improvements in performance across various tasks and model scales."}, "weaknesses": {"value": "The performance improvement is marginal, and in some cases (e.g., LMB perplexity, LMB accuracy, and ARC-c accuracy at 740M scale) the method underperforms compared to PreNorm. Moreover, on the 5B model, HybridNorm achieves better performance on ARC-c accuracy.\n\nIn addition, the paper lacks completeness in several aspects:\n\n**Lack of comparison with relevant baselines**: The paper does not include a comparison with Peri-LN (Peri-LN: Revisiting Normalization Layer in the Transformer Architecture, ICML 2025), which is a closely related normalization method.\n\n**Confounding factors in experimental design**: The method couples FuseNorm with Scale Init. Since initialization itself can significantly impact both training stability and final performance, it is unclear how much of the gain comes from FuseNorm vs. Scale Init. For example, a comparison with PreNorm + Scale Init would provide a clearer attribution of the source of improvement.\n\n**Missing ablation experiments**: The paper does not provide standalone ablation experiments to disentangle the contribution of different architectural components (residual shortcut structure vs. normalization sequence) to training stability and performance.\n\n**Reproducibility**: No implementation or code is provided, which makes it difficult to fully verify the empirical claims."}, "questions": {"value": "Minor Errors\n\nLine 505: In in -> In\n\nLine 658: to twice as 740M -> to twice that of the 740M model\n\nLine 659: maximize -> largest\n\nLine 666: slimpajama -> SlimPajama\n\nLine 674: Slimpajama -> SlimPajama\n\nLine 676: Winograde -> WinoGrande\n\nLine 676: Hellaswag -> HellaSwag\n\nLine 675: LM harness evaluation -> the LM Harness evaluation\n\nLine 174: LN(MHA(X′_l)) + X′_l) -> LN(MHA(X′_l) + X′_l)\n\nLines 175 and 190: The prime notation appears to be inconsistently rendered in mathematical expressions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BpoyPIZWUP", "forum": "azXOzJFwuf", "replyto": "azXOzJFwuf", "signatures": ["ICLR.cc/2026/Conference/Submission12055/Reviewer_Deba"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12055/Reviewer_Deba"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761226310953, "cdate": 1761226310953, "tmdate": 1762923031577, "mdate": 1762923031577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to achieve a best of both worlds between PreNorm and PostNorm in the Transformer architecture, by modifying the skip-connection/LayerNorm placements. Specifically, in a PostNorm architecture, the authors remove the skip connection from the FFN block, instead replacing it with a longer skip connection across both Attention and FFN blocks. The authors show improvements compared to PreNorm on multiple model sizes, and compared to some prior baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper shows improvements over PreNorm architecture in pre-training across multiple model sizes, and over some prior works.\n1. The proposed method achieves improved representation diversity across layers"}, "weaknesses": {"value": "1. A lot of the derivations are extremely handwavy, with numerous approximations and assumptions throughout in the derivations. ( E.g. $\\approx$ occurs 15 times in the manuscript.).\n1. Section 4 of the paper, covering depth and width scaling, are not proposing anything new, and are extremely redundant. For example, the  same/similar scaling has often been proposed before (e.g., see section K.3 of https://arxiv.org/pdf/2403.09635), and the \"width scaling\" is the authors simply verifying that prior works for LR transfer work with their architecture as well.\n1. In section 5, the authors explicitly admit to an incomplete baseline study. A claim to \"consistently outperform...advanced variants\" is premature unless all baselines are working correctly.\n1. In section 6, the authors point to their architecture being trainable with higher LR than PostNorm as proof of \"mitigated gradient decay\", but this conclusion does not necessarily hold. For example, a model with reduced sensitivity (eg. a linear layer with very large param values, followed by a layernorm) will be trainable with a much larger LR, but this does not say anything about inherent stability or instability of that model.\n1. Even assuming all the derivations of the authors are correct, their proposed method only allows training 2x deeper post-norm models (equation 11). Prior works such as DeepNorm instead extend it to 100s of layers."}, "questions": {"value": "1. Was the learning rate and initialization hyper-parameters set from known-good values from prior works, or perhaps hyper-parameter searched for all the baselines in Table 21? Inefficient setting of these can significantly affect performance.\n1. (minor, no author rebuttal needed) The pdf seems to be somewhat bugged - searching and highlighting text is broken in several pages across multiple pdf readers. I have not observed the same in other papers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ngw4bofcqc", "forum": "azXOzJFwuf", "replyto": "azXOzJFwuf", "signatures": ["ICLR.cc/2026/Conference/Submission12055/Reviewer_hEpt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12055/Reviewer_hEpt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848798297, "cdate": 1761848798297, "tmdate": 1762923029784, "mdate": 1762923029784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FuseNorm, a normalization design that keeps a direct input–output skip connection through each Transformer block while applying normalization after both the attention and MLP sublayers. This layout aims to combine the gradient stability of PreNorm and the representational strength of PostNorm. The authors analyze its gradient and variance behavior, propose a depth-dependent initialization scheme, and validate the approach on dense and MoE models up to 16B parameters. FuseNorm yields small but consistent gains over PreNorm and maintains stable training where PostNorm becomes unstable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The method is simple, clearly defined, and easy to integrate into existing architectures. While training stability for large-scale Transformers has been extensively studied in recent months through various architectural innovations, the proposed method still contributes a refreshing perspective to this ongoing line of work.\n * Theoretical and empirical analyses are coherent: the scaling rule derived from variance control matches practical stability trends.\n * Results are consistent across model sizes and architectures, with diagnostic evidence (gradient norms, inter-layer similarity) that supports the claims."}, "weaknesses": {"value": "* The paper omits comparisons to recent normalization schemes such as Peri-LN (arXiv:2502.02732), which has been adopted by several major LLMs and addresses the same issues. The paper’s motivation and positioning substantially overlap with prior works such as MixLN and HybridLN, which limits its perceived novelty.\n * The reported improvements are modest relative to the added complexity."}, "questions": {"value": "- How does FuseNorm compare with recently proposed peri- or sandwich-style normalization schemes such as Peri-LN under identical hyperparameters and compute budgets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xutfnjBgJd", "forum": "azXOzJFwuf", "replyto": "azXOzJFwuf", "signatures": ["ICLR.cc/2026/Conference/Submission12055/Reviewer_icoP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12055/Reviewer_icoP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890305454, "cdate": 1761890305454, "tmdate": 1762923029248, "mdate": 1762923029248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FuseNorm, a normalization strategy that unifies the training stability of Pre-LayerNorm and the performance advantages of Post-LayerNorm in Transformer architectures. The key idea is to retain a residual path while applying a Post-LN-style normalization at the block output, thereby resetting variance and maintaining balanced gradients across depth. Theoretical analysis shows that FuseNorm preserves stable Jacobian spectra, and empirical studies confirm consistent stability and performance improvements across both dense and Mixture-of-Experts models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The architectural modification is simple and intuitive, replacing the FFN residual with the original block input and normalizing the final output.  \n- The theoretical condition for depth scaling provides a practical guideline for stable training.  \n- Empirical results show consistent improvement trends across dense and MoE settings, suggesting potential practical value."}, "weaknesses": {"value": "1. **Figures and visual evidence are unconvincing and inconsistently presented.** Many of the figures (Figs. 2–7) lack consistent axis ranges, normalization, and clarity in what they aim to demonstrate, making cross-figure interpretation difficult. For example, Figure 3 is intended to illustrate training collapse without scaling, yet it only shows three layer traces (layers 1, 12, 24) from a single run, without comparison to Pre-, Post-, or scaled FuseNorm variants. This makes it impossible to assess generality, effect size, or reproducibility. A unified plot contrasting “no-scale vs. scale” across normalization types with mean ± std over multiple seeds is needed. Moreover, Figure 6—claimed to demonstrate severe representation collapse in Pre-LN and its prevention by FuseNorm—does not visually support such a conclusion. When inspecting the heatmaps, the difference between Pre-LN and FuseNorm appears marginal, with large red regions (high similarity) persisting in both cases. The improvement is not visually evident enough to justify the strong qualitative statement in the text.\nEach figure and caption should clearly specify the experimental setup, number of runs, normalization scheme, and intended takeaway, ensuring visual evidence aligns with the claimed phenomena.\n\n2. **Missing strong baselines, fairness issues, and lack of comparison to Gemma-style architectures.** The paper lacks head-to-head results with strong baselines such as DeepNorm and NormFormer, both of which explicitly target the same stability–performance trade-off. In addition, the OLMO2 and LayerNorm-Scaling results are reported as “being re-run,” raising questions about hyperparameter fairness. A rigorous comparison should fix dataset, token count, learning-rate schedule, initialization, and random seeds (≥ 3), with clearly defined criteria for “training failure.” Furthermore, recent open-source models such as Gemma 2/3 employ a Peri-LN-style normalization [1], which shares similar motivations of balancing gradient flow and variance growth. A theoretical and empirical comparison between FuseNorm and such Peri-LN architectures would substantially strengthen the paper’s positioning and clarify whether the proposed method provides distinct or complementary benefits. Adding Peri-LN to Table 2, along with gradient-norm profiles or stability-transfer plots (Fig. 4), would make the evaluation more comprehensive and fair.\n\n3. **Ablation insufficiency – unclear contribution of structure vs. initialization.**  The current results do not isolate whether the observed improvements stem from the new FuseNorm block design itself or from the “Scale-Init” strategy introduced for depth scaling.\n\n4. **Missing quantitative validation of theoretical claims.** The theoretical analysis predicts that the variance of each residual branch should scale and that FuseNorm reduces gradient-decay rates relative to Post-LN. However, the paper provides no empirical evidence verifying these relationships. \n\n\n[1] Kim et al. \"Peri-ln: Revisiting normalization layer in the transformer architecture.\" ICML2025."}, "questions": {"value": "All major questions and requests for clarification are already integrated into the Weaknesses section for clarity and conciseness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gnTUyOQucK", "forum": "azXOzJFwuf", "replyto": "azXOzJFwuf", "signatures": ["ICLR.cc/2026/Conference/Submission12055/Reviewer_pvm3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12055/Reviewer_pvm3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762607704650, "cdate": 1762607704650, "tmdate": 1762923028882, "mdate": 1762923028882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **FuseNorm**, a normalization strategy that integrates the advantages of both PreNorm and PostNorm while addressing their respective drawbacks. In particular,  FuseNorm leverages the training stability benefit of PreLN and combines it with the performance benefits of PostLN.\n\nThe authors provide theoretical justifications for how FuseNorm preventing representational collapse in deeper layers, while also improving gradient flow to counteract gradient decay during training.  Empirical results show consistent performance improvements over recent hybrid normalization schemes that attempt to mix or modify PreLN and PostLN strategies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. LayerNorm placement is still a very active research topic, with several recent works exploring position-specific variants like QK-, QKV-, and FFN-LayerNorm [1,2, 3], or hybrid approaches such as MixLN that aim to balance stability and performance in deeper models. This paper fits naturally into that discussion and and provide a unified and principled way to combine the strengths of existing normalization strategies\n\n\n2. Authors have provided a clear mathematical analysis on how FuseNorm helps prevent representational collapse and mitigate gradient decay which offer an intuitive  understanding of normalization behavior in large and deep LLMs.\n\n3. The experimental evaluation is thorough, comparing FuseNorm against recent hybrid normalization strategies across both dense and MoE-based FFN architectures, with model sizes ranging from 0.74B to 5B parameters. The performance gains in deeper LLMs is convincing for showing the utility of FuseNorm.\n\n\n[1] Dehghani et al., Scaling vision transformers to 22 billion parameters, ICML 2023\n\n[2] Zhuo et al., HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization, 2025\n\n[3]  Rybakov et al., Methods of improving llm training stability, 2024"}, "weaknesses": {"value": "1. The novelty of the work is quite limited. The idea of combining the benefits of PreLN and PostLN has already been well explored in prior works such as MixLN [1] and other hybrid normalization schemes. The proposed FuseNorm is an **incremental modification** rather than a fundamentally new concept, and the paper does not convincingly articulate what distinguishes it in terms of design principle or innovation.\n\n\n2. The Authors  do not demonstrate how FuseNorm improves the **quality of internal representations**. Although it claims to prevent representational collapse, there’s no clear analysis---such as probing FFNs, or representation visualizations. Without showing how the internal representations, the paper’s claim about representational improvement is not convincing.  For example  eigen-value distribution or Rank-analysis shown in  [2,3].  \n\n\n[1] Li et al., Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN, ICLR 2025\n\n[2] Loshchilov et al., nGPT: Normalized Transformer with Representation Learning on the Hypersphere, ICLR 2025\n\n[3] Jha et al.,  Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward Networks Use Their Latent Space? EMNLP 2025"}, "questions": {"value": "Could the authors provide the eigenvalue distribution of FFN post-activations or weight matrices, or a layer-wise rank comparison between FuseNorm, PreLN, PostLN, and MixLN models? This would offer concrete evidence of how FuseNorm impacts internal representation quality and whether it genuinely prevents representational collapse in deeper layers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xCCk9drmic", "forum": "azXOzJFwuf", "replyto": "azXOzJFwuf", "signatures": ["ICLR.cc/2026/Conference/Submission12055/Reviewer_iuA5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12055/Reviewer_iuA5"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762815499119, "cdate": 1762815499119, "tmdate": 1762923028417, "mdate": 1762923028417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}