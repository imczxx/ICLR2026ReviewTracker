{"id": "rIv5ftmR3v", "number": 9080, "cdate": 1758109851663, "mdate": 1759897744750, "content": {"title": "TeMo: Temperature Modulation for Multimodal Contrastive Learning", "abstract": "Contrastive learning approaches achieve strong performance by training models to bring similar samples closer while pushing dissimilar samples apart. A crucial component of contrastive learning is the temperature hyperparameter $\\tau$, which controls the penalty strength applied to negative samples. However, most existing methods either fix this hyperparameter or learn a global value during training. In this paper, we introduce TeMo, $\\underline{Te}$mperature $\\underline{Mo}$dulation framework, a similarity-based modulation approach that adaptively adjusts the temperature for each positive-negative pair according to their similarity, enabling more fine-grained multimodal contrastive learning. Our approach seamlessly integrates temperature-modulated multimodal and unimodal losses with the standard multimodal contrastive loss by gradually transitioning between them. This design allows the model to capture both coarse- and fine-grained semantics at different training stages. Extensive experiments demonstrate that each component of TeMo consistently enhances performance across diverse zero-shot retrieval and classification tasks, establishing new state-of-the-art results.", "tldr": "", "keywords": ["Contrastive Learning", "Self-Supervised Learning", "Temperature"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6a8b9fad31c543c6e8969c8eb3b1ad77ef50d0fe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a dynamic temperature for the InfoNCE loss, calculated on a per-element-pair basis. The core idea is that the temperature value should be a function of the pair's embedding similarity. The paper's strategy is to assign pairs with high similarity a correspondingly higher temperature, with the goal of more appropriately scaling the loss contribution from different positive and negative pairs."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The primary strength of the work is its simplicity. The proposed temperature modulation technique appears straightforward to implement and is presented as a general method that can be integrated with various existing models to potentially improve performance."}, "weaknesses": {"value": "**Unclear Attribution of Performance Gains**: The central weakness of the paper is the ambiguity in attributing the reported performance gains specifically to the novel temperature modulation. The main results (e.g., in Tables 1 & 2) appear to combine the proposed modulation with additional Unimodal losses. Adding a unimodal loss is a known technique for improving performance in this domain.\n\nTo properly isolate the benefit of the proposed method, a crucial baseline is missing from the comparisons: the performance of the base model with the Unimodal loss but without the proposed temperature modulation. Without this direct comparison, it is impossible to determine whether the reported gains stem from the novel modulation or simply from the addition of the Unimodal loss.\n\nThis concern is further amplified by the ablation study, which shows that the 'Base + MM + Scheduling' method yields mixed and marginal gains compared to the 'TS*' baseline. For example, the proposed method achieves 21.74 (MS IR @1), 28.58 (MS TR @1), 44.02 (FL IR@1), and 54.90 (FL TR@1), whereas the baseline TS* achieves 22.01 (MS IR @1), 28.20 (MS TR @1), 42.90 (FL IR@1), and 53.30 (FL TR@1) , respectively. These indicate only a marginal improvement.\n\n**Insufficient Zero-Shot Evaluation**: The empirical evaluation for zero-shot classification is insufficient. The results are limited to CIFAR-10 and CIFAR-100, which are not representative. Current practice for evaluating CLIP-like models involves reporting metrics on more challenging suites, such as those included in ELEVATER [1] (e.g., Image Classification-in-the-Wild, which includes 20 zero-shot classification tasks) and other common ImageNet variants (e.g., ImageNet-Adversarial, ImageNet-R, and ImageNet-V2). The absence of these results makes it difficult to assess the method's practical utility.\n\n**Lack of Comparison to Hard Negative Mining**: The proposed mechanism, increasing the temperature for high-similarity pairs, effectively alters the gradient contribution of these samples. This approach is conceptually related to various hard negative mining strategies, which also focus training on the most informative (often most similar) negative pairs. The paper fails to discuss this connection or provide any empirical comparison against established hard negative mining techniques. This comparison is necessary to position the contribution relative to existing literature and to determine if this dynamic temperature formulation offers tangible advantages over prior methods.\n\n[1] Li, Chunyuan, et al. \"Elevater: A benchmark and toolkit for evaluating language-augmented visual models.\" Advances in Neural Information Processing Systems 35 (2022): 9287-9301."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MNL41O3PAO", "forum": "rIv5ftmR3v", "replyto": "rIv5ftmR3v", "signatures": ["ICLR.cc/2026/Conference/Submission9080/Reviewer_sR36"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9080/Reviewer_sR36"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974010376, "cdate": 1761974010376, "tmdate": 1762920786932, "mdate": 1762920786932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TeMo (Temperature Modulation), a new framework for multimodal contrastive learning that dynamically adjusts the temperature parameter on a per-pair basis according to sample similarity. TeMo leverages a similarity-driven temperature field to modulate both multimodal (image–text) and unimodal (image–image, text–text) contrastive losses, enabling more fine-grained representation alignment. Experimental results on various public benchmark datasets demonstrate consistent improvements over the InfoNCE, TS, and SLIP baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Adaptive temperature modulation: The paper proposes a per-pair, similarity-based temperature adjustment that jointly handles multimodal and unimodal contrastive learning, offering finer control of representation learning and reducing the modality gap.\n\n- Empirical clarity and ablation support: The paper provides thorough ablation studies that isolate the effect of each component—multimodal modulation, unimodal losses, and scheduling—clearly demonstrating their individual and combined contributions"}, "weaknesses": {"value": "- Limited horizontal comparison with other contrastive training SOTA methods: The paper only contrasts TeMo against InfoNCE and Temperature Schedules (TS), without benchmarking against stronger contrastive learning variants such as SoftCLIP (Gao et al., AAAI 2024) or CWCL (Cross-Weighted Contrastive Learning, 2024). Since CLIP can be viewed as an InfoNCE-based baseline, the relative gain from temperature modulation seems smaller than the other type of contrastive loss SOTA approaches. See the performance reported in SoftCLIP Table 5 (SoftCLIP vs CLIP) or CWCL Table 3 (CWCL vs LiT on MSCOCO). The authors should include or at least discuss such horizontal comparisons to justify why temperature modulation remains a competitive or complementary direction compared to other fine-grained contrastive learning approaches.\n\n- Potential manual tuning of τ parameters: Equation (4) λ(x̄, ȳ) = τ_min + τ_α * √sim(x, y) implies the proposed framework requiring manual tuning rather than being learned jointly with the model, this introduces additional hyperparameter sensitivity and weakens the claim of “adaptive” modulation. \n\n- Narrow data scale and uncertain generalization: The study pretrains TeMo only on small- to mid-scale datasets (CC3M and CC12M), making it unclear whether the temperature modulation remains effective on larger or more diverse multimodal corpora (e.g., LAION, YFCC). The method may also require dataset-specific hyperparameter tuning (e.g., τ_min, τ_α), which could limit its practicality in broader pretraining settings."}, "questions": {"value": "1. Can the authors compare TeMo with other fine-grained contrastive losses (e.g., SoftCLIP, CWCL) under the same experimental setup to clarify the relative contribution of temperature modulation?\n2. Could temperature modulation be combined with SoftCLIP or CWCL, and would such integration yield additive or complementary gains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NkEnf7HiuW", "forum": "rIv5ftmR3v", "replyto": "rIv5ftmR3v", "signatures": ["ICLR.cc/2026/Conference/Submission9080/Reviewer_f5Mr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9080/Reviewer_f5Mr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979942860, "cdate": 1761979942860, "tmdate": 1762920786492, "mdate": 1762920786492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TeMo (Temperature Modulation), a novel framework for multimodal contrastive learning (CL) that adaptively adjusts the temperature parameter $\\tau$ on a per-pair basis. Traditional contrastive learning methods such as CLIP, SimCLR, and MoCo typically use a global or learnable scalar temperature shared across all sample pairs. This uniform temperature limits the model’s ability to balance repulsive forces among diverse semantic classes—especially in long-tailed or multimodal datasets.\n\nTo address this limitation, the authors propose a similarity-based temperature modulation mechanism, where the temperature $\\tau_{ij}$ for each positive–negative pair is dynamically modulated according to their similarity score. This allows finer control of the InfoNCE loss. In addition to adaptive temperatures for the multimodal objective, TeMo incorporates temperature-modulated unimodal contrastive losses that help refine the local structure within each modality (e.g., image–image or text–text). The framework uses a progressive training schedule, starting with a lower temperature (capturing fine-grained, instance-level semantics) and gradually increasing it to encourage coarse, semantic-level alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**1. Novel temperature modulation framework:** The paper introduces TeMo, a principled framework that adaptively modulates the temperature parameter on a per-pair basis according to similarity scores. This is a meaningful advance over prior works that rely on a fixed or globally learned temperature. The method provides finer control over the contrastive learning process, improving representation quality across both unimodal and multimodal settings. This idea is conceptually simple yet broadly applicable to various contrastive learning architectures (e.g., CLIP-style models).\n\n**2. Comprehensive empirical evaluation and consistent performance gains:** The authors conduct extensive experiments on zero-shot retrieval (MSCOCO, Flickr30k) and zero-shot classification (CIFAR10, CIFAR100, ImageNet-1k), demonstrating consistent improvements over multiple temperature-adaptation baselines. The fact that TeMo outperforms across diverse datasets and training scales (CC3M and CC12M) indicates good generalization and practical utility of the approach."}, "weaknesses": {"value": "Here are some concerns for this paper:\n\n**1. Missing Discussion of Related Literature:** Some closely related literauter have not discussed in the paper. The paper overlooks several closely related studies that address temperature adaptation and modality imbalance in contrastive multimodal learning. For example, Manna et al. (2023) [1] propose a similarity-dependent temperature scaling function in the contrastive loss. Their method dynamically adjusts temperature based on the cosine similarity between samples, rather than using a fixed global $\\tau$. Yaras et al. (2024) [2] proposed a conceptually similar approach that dynamically controls the temperature to mitigate the modality gap—the discrepancy between image and text embedding distributions. This work provides both theoretical and empirical insights into how temperature modulation affects multimodal alignment and balance. \n\n[1] Manna, Siladittya, et al. \"Dynamically Scaled Temperature in Self-Supervised Contrastive Learning.\" IEEE Transactions on Artificial Intelligence (2025).\n[2] Yaras, Can, et al. \"Explaining and Mitigating the Modality Gap in Contrastive Multimodal Learning.\" Conference on Parsimony and Learning, 2024.\n\n**Lack of Principled Understanding:** While the proposed TeMo framework demonstrates promising empirical performance, the paper lacks a principled theoretical understanding of how and why temperature modulation improves multimodal contrastive learning. The method is primarily motivated from an intuitive perspective—that per-pair temperature scaling provides finer control over learning signals—but the paper does not formally analyze its effect on the InfoNCE loss landscape, gradient dynamics, or representation geometry."}, "questions": {"value": "**1.** Can TeMo be connected to existing theoretical frameworks on temperature scaling and representation uniformity (e.g., Wang & Isola, ICML 2020)?\n\nWang, T. and Isola, P. (2020). \"Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere.\" International Conference on Machine Learning (ICML 2020).\n\n**2.** How does modulating $\\tau_{ij}$ based on pairwise similarity alter the effective margin or smoothness of the contrastive objective?\n\n**3.** What guarantees, if any, can be made regarding stability or convergence when temperatures vary across pairs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6KiwAUr9Hz", "forum": "rIv5ftmR3v", "replyto": "rIv5ftmR3v", "signatures": ["ICLR.cc/2026/Conference/Submission9080/Reviewer_81f6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9080/Reviewer_81f6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136999622, "cdate": 1762136999622, "tmdate": 1762920786162, "mdate": 1762920786162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}