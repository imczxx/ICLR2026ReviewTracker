{"id": "fbGmSV6tUw", "number": 1135, "cdate": 1756847563154, "mdate": 1763623613717, "content": {"title": "Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations", "abstract": "Spatial cognition is essential for human intelligence, enabling problem-solving through visual simulations rather than solely relying on verbal reasoning. However, existing AI benchmarks primarily assess verbal reasoning, neglecting the complexities of non-verbal, multi-step visual simulation. We introduce \\textbf{STARE (Spatial Transformations and Reasoning Evaluation)}, a benchmark designed to rigorously evaluate multimodal large language models on tasks better solved through multi-step visual simulation. STARE features 3K tasks spanning foundational geometric transformations (2D and 3D), integrated spatial reasoning (cube net folding and tangram puzzles), and real-world spatial reasoning (perspective and temporal reasoning), reflecting practical cognitive challenges like object assembly, mechanical diagram interpretation, and everyday spatial navigation. Our evaluations show that models excel at reasoning over simpler 2D transformations, but perform close to random chance on more complex tasks like 3D cube net folding and tangram puzzles that require multi-step visual simulations. Humans achieve near-perfect accuracy but take considerable time (up to 28.9s) on complex tasks, significantly speeding up (down by 7.5 seconds on average) with intermediate visual simulations. In contrast, models exhibit inconsistent performance gains from visual simulations, improving on most tasks but declining in specific cases like tangram puzzles (GPT-4o, o1) and cube net folding (Claude-3.5, Gemini-2.0 Flash), indicating that models may not know how to effectively leverage intermediate visual information.", "tldr": "STARE: a benchmark designed to rigorously evaluate MLLMs on tasks better solved through multi-step visual simulation.", "keywords": ["spatial reasoning; visual reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c614938eeb3da9c882c346811c9cd4ddc1c03ac.pdf", "supplementary_material": "/attachment/ed52ce3af81470438fc714dd90c8d229b4265cce.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces STARE, a novel multimodal benchmark evaluating a model's capability on tasks that requires visual simulation. STARE features 4k tasks spanning foundational geometric transformations, integrated spatial reasoning and real-world spatial reasoning. Through experiments, the authors show that state of the art MLLMs perform well in simple 2D transformation tasks, but struggle with complex tasks like 3D cube folding. Human performance is also detailedly evaluated: although humans can achieve near-perfect accuracy, the completion time is considerable (up to 28 seconds). In addition, when providing visual simulation intermediate hints, humans can speed up their completion time while models exhibit inconsistent performance gains."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper is trying to draw attention to a relatively new field for multimodal AI: visual simulation. Which is crucial since many studies indicate that this is the core of human intelligence, and there are many important applications requiring such ability, such as arranging furniture. In addition, the lack of such a dataset further emphasizes the importance of the developed dataset.\n2. The tasks introduced in the papers are diverse and well designed. The descriptions about each task are detailed and easy to follow. The inclusion of visual simulation hints is also useful in both model and human evaluation.\n3. The experiment is comprehensive, covering multiple state of the art models, with and without visual simulation hints. Particularly, human response time is an important aspect which is not covered by previous benchmarks, this is important especially in benchmarks that are extremely difficult for current MLLMs."}, "weaknesses": {"value": "1.  The role of visual perception in this task is important and experiment in this aspect can be done better. Currently, only one task (cube folding) is further decomposed to basic perception questions, however, the difficulty of visual perception varies across different tasks, it is possible for another task that visual perception is the key bottleneck. This is crucial since it can inform model developers on how to improve their models. In addition, since the random chance is 50, 57.4 performance on 3D perception should indicate very poor performance, which is contradictory to the claim in line 422-425.\n2. Since most tasks are purely synthetic, it is questionable whether the performance in these tasks can reflect the model's performance on more real-world tasks (like object assembly, mechanical diagram interpretation as mentioned in the abstract)."}, "questions": {"value": "1.  I would like to hear the author's clarification regarding the weakness 1 above.\n2.  Have the authors tried reasoning models on table3’s experiment? It is expected that reasoning models can perform much better when text is provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DDJoB9vHPJ", "forum": "fbGmSV6tUw", "replyto": "fbGmSV6tUw", "signatures": ["ICLR.cc/2026/Conference/Submission1135/Reviewer_BDc7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1135/Reviewer_BDc7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761009594811, "cdate": 1761009594811, "tmdate": 1762915687704, "mdate": 1762915687704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces STARE, a new benchmark designed to evaluate the spatial reasoning capabilities of Multimodal Large Language Models (MLLMs). The authors argue that existing benchmarks neglect a crucial aspect of human intelligence: the ability to perform multi-step, non-verbal visual simulations. STARE is structured with a hierarchy of tasks, ranging from foundational 2D/3D geometric transformations to more integrated tasks like cube-net folding and tangram puzzles, and finally to real-world scenarios like perspective and temporal reasoning. Through extensive experiments on a suite of state-of-the-art closed and open-source models, the paper demonstrates a significant performance gap between current MLLMs and humans."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel and Well-Motivated Benchmark: The paper introduces STARE, a new benchmark that addresses a critical area of multimodal AI: multi-step spatial reasoning. It provides a structured framework for diagnosing the spatial cognition capabilities of models.\n\n2. Thorough Experimental Evaluation: The study is grounded in a comprehensive evaluation of a wide range of contemporary models, including a crucial human performance baseline. The analysis offers some valuable insights into model failure modes."}, "weaknesses": {"value": "1. **Potential Bias in the \"Simulation\" Paradigm**: The paper's core claim is about evaluating \"visual simulations.\" However, its primary method for this—providing intermediate visual steps—tests a model's ability to interpret a given sequence of images, not its ability to generate that sequence internally. A human performing mental rotation isn't shown snapshots; their mind creates the intermediate frames. This is a subtle but critical distinction. The benchmark is, therefore, a stronger test of sequential visual comprehension and context integration than of genuine, unprompted mental simulation. This could favor models that are good at processing image-text interleaved inputs over those with true non-verbal reasoning abilities.\n\n2. **Mismatch Between Core Tasks and \"Real-World\" Scenarios**: A potential weakness lies in the selection of the \"real-world\" tasks. While the paper's core thesis is to evaluate multi-step visual simulation, the chosen temporal and perspective reasoning tasks do not appear to measure this capability in the same way as the other tasks. \n\n3. **Limited Real-World Complexity**: The tasks categorized as \"Real-world Spatial Reasoning\" are highly constrained (selecting a missing frame or a rendered viewpoint). This raises questions about whether the observed model failures and successes would generalize to messier, embodied tasks like actual robot-based assembly or navigation."}, "questions": {"value": "Could the benchmark be strengthened by including real-world tasks that are more procedural and thus align better with the paper's core thesis on multi-step simulation? If sticking with the current tasks, is it feasible to frame them in a way that allows for a more consistent and meaningful evaluation under the with/without intermediate simulation settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yHcrkUAOcy", "forum": "fbGmSV6tUw", "replyto": "fbGmSV6tUw", "signatures": ["ICLR.cc/2026/Conference/Submission1135/Reviewer_Pqgq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1135/Reviewer_Pqgq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761379506212, "cdate": 1761379506212, "tmdate": 1762915687597, "mdate": 1762915687597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a benchmark known as STARE (Spatial Transformations and Reasoning Evaluation) to rigorously evaluate MLLMs on 2D and 3D geometric transformation tasks ideally solved through multi-step visual simulations. The tasks span three major varieties : (i) Foundational Geometric Transformations, (ii) Integrated spatial reasoning and (iii) Real-world spatial reasoning. Through ~4000 instances of these tasks, they demonstrate that models perform relatively well on reasoning over 2D transformations, but demonstrate random chance responses with 3D cube net folding and tangram puzzles. The authors show that when models receive intermediate visual steps, their performance improves for GPT-4o, Gemini-2.0 Flash Thinking and o1 while Gemini-2.0 Flash and Claude worsen on cube net folding. The paper also includes a detailed analysis of (i) how models understand individual transformation in 2D and 3D, showing a 3% - 8% increase, (ii) how model performance erodes with task complexity, showing a ~20% reduction in 2D tasks and ~12% reduction in 3D tasks, (iii) how model failures originate, demonstrating failure reduction on complexity removal, (iv) how do models reason spatially through textual descriptions, showing that text mainly helps in 2D tasks and (v) how models utilize visual simulations and how well do model use disconnected final visual simulations."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The paper presents the problem of composite reasoning through geometric transformation tasks clearly and concisely. I find these particular strengths of the paper interesting:\n\n1) Extensive task definition - The variety of tasks covering: (i) integrated spatial reasoning over 2D and 3D, (ii) Foundational geometric transformations, and (iii) real-world spatial reasoning encompasses a majority of reasoning behaviour abilities for MLLMs.\n2) Behavioural consistency with prior works - The paper demonstrates consistency of random memory associative behaviour with prior works in this domain, also extending to comprehensive real-world reasoning targeted towards helpful tasks such as cube folding and tangram shapes.\n3) Comprehensive error analysis and behaviour evaluation - The research questions posed and analysed cover the major behaviours responsible for reasoning. They cover the performance reduction for an increase in task complexity, failure modes for models in tasks and the effectiveness of textual descriptions in spatial reasoning."}, "weaknesses": {"value": "I have only one major weakness/ criticism to identify for the authors, although this has been highlighted in the limitations:\n\n1) Comprehensive answering for models can be incorporated into the work, i.e. moving beyond just multiple-choice and binary question answering, towards one-word or two-word answering to also analyse the probabilistic tendency to actually give the true answer as a response and intermediate reasoning, and not just choose an option."}, "questions": {"value": "I have one suggestion for authors:\n\n1) The paper can include some more relevant work on geometric reasoning benchmarks. \"Forgotten Polygons: Multimodal Large Language Models are Shape-Blind\", \"GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs\", and \"VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models\" might be some works relevant to the current paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YUkELyhbgd", "forum": "fbGmSV6tUw", "replyto": "fbGmSV6tUw", "signatures": ["ICLR.cc/2026/Conference/Submission1135/Reviewer_Egcv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1135/Reviewer_Egcv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761387358967, "cdate": 1761387358967, "tmdate": 1762915687478, "mdate": 1762915687478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the STARE benchmark, which evaluates multimodal large language models on vision-reasoning tasks involving spatial transformations and reasoning. It covers various contexts, reasoning types, and difficulty levels, showing that models excel at simple 2D transformations but struggle on more challenging tasks such as 3D transformations, real-world perspective taking, and temporal reasoning."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is well-motivated and well-written, with a clear structure and sufficient detail about the execution of both dataset construction and experiments, which makes it easy to replicate and build upon. The benchmark covers the spatial reasoning tasks the authors target, with multiple difficulty levels and checks for different types of reasoning in models. The evaluations are robust, including settings with and without visual simulations and step-by-step reasoning. The findings are presented clearly and are straightforward, making the takeaways easy to understand. Overall, the paper offers a valuable contribution in both the idea and its execution."}, "weaknesses": {"value": "I didn't notice major issues with this paper, but the few minor points I saw are as follows:\n\nThere is a problem with Figure 1: the question image does not match the step images.\n\nThe text mentions \"Fig. 10\" on line 196, but I assume the correct reference is the figure references 9 and 10. And Figure 10 has missing pieces.\n\nLine 323: \"Notably, o3 seems to better at leveraging visual simulations\" has a grammatical error."}, "questions": {"value": "I don't have any questions for the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fT6YgSQgof", "forum": "fbGmSV6tUw", "replyto": "fbGmSV6tUw", "signatures": ["ICLR.cc/2026/Conference/Submission1135/Reviewer_uKSW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1135/Reviewer_uKSW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978159814, "cdate": 1761978159814, "tmdate": 1762915687326, "mdate": 1762915687326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}