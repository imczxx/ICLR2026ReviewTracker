{"id": "yhFZcnbMwE", "number": 2744, "cdate": 1757231905435, "mdate": 1763000539786, "content": {"title": "Efficient Data Pruning Methods for Remote Sensing generative foundation Models", "abstract": "Large-scale datasets have propelled progress in generation foundation models for remote sensing, but training on such data incurs substantial storage and compute costs. In addition, globally collected raw data often exhibit redundancy, noise, and class imbalance, which undermines training efficiency and generation quality. Existing Remote Sensing generative foundation models typically aggregate multiple classification datasets or apply simplistic deduplication, thereby overlooking the distributional requirements of generation modeling as well as the inherent heterogeneity and diversity of remote sensing imagery. To address these limitations, we propose an efficient, two-stage data pruning approach for remote sensing generative foundation models. This approach simultaneously incorporates local information content with global scene-level diversity and representativeness. Specifically, an entropy-based criterion is applied initially to efficiently eliminate low-information samples. Leveraging remote sensing scene classification datasets as reference benchmarks, we then perform scene-aware clustering with stratified sampling, which enhances the effectiveness of clustering while reducing the computational cost of clustering on large-scale unlabeled data. Finally, by balancing cluster-level uniformity with sample representativeness, the method enables fine-grained selection under high pruning ratios while preserving overall diversity and representativeness. Experiments on both curated remote sensing datasets and large-scale global data demonstrate that our pruning strategy significantly improves convergence and generation quality. Moreover, generation foundation models trained with our method consistently achieve state-of-the-art performance across multiple downstream tasks, including super-resolution and semantic image synthesis. This data pruning paradigm provides practical guidance and empirical reference for the development of remote sensing generative foundation models.", "tldr": "", "keywords": ["Remote Sensing，generative foundation model，data pruning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/50696e0b4f9068b165aaf06407a336852efff505.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an efficient, two-stage data pruning strategy to address the high computational cost and performance issues caused by large, redundant, and low-quality remote sensing (RS) datasets used for training generative foundation models. The first stage employs an entropy-based filter to rapidly discard low-information samples like clouds, deserts, or open oceans. The second, novel scene-aware clustering and sampling stage avoids the extreme computational cost of clustering the entire massive dataset ; instead, it first generates representative scene centroids by over-clustering a \"prior bank\" of smaller, expert-curated RS classification datasets. The large-scale unlabeled data is then efficiently assigned to these pre-defined centroids, and a stratified, centroid-prioritized sampling method selects a final subset that balances scene diversity and representativeness. Experiments show this method significantly improves convergence and generation quality, allowing models trained on highly pruned datasets to outperform models trained on the complete datasets in both generation and downstream tasks like super-resolution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper's most significant contribution is the \"reference dataset-guided\" clustering in Stage II. Instead of incurring the massive computational cost of clustering the entire multi-million-image unlabeled dataset , the method efficiently pre-computes \"prior centroids\" from smaller, expert-curated classification datasets. This novel approach is shown to be not only much faster but also more effective, leading to better FID scores.\n2. A major practical benefit is the acceleration of model training. The paper clearly demonstrates that models trained on its pruned subset converge significantly faster than those trained on the full dataset or subsets from other methods.\n3. he paper tackles a critical, practical bottleneck in the development of foundation models: the massive, noisy, and highly redundant nature of globally-collected remote sensing data. By providing an effective and efficient data pruning pipeline, the paper offers a valuable and practical solution for the field."}, "weaknesses": {"value": "1. The conclusions drawn from the ablation studies are confusing and seem to conflict with the main results. For example, Table 3 shows that at a 70% pruning ratio Stage I-only achieves a better FID (68.38) than the full two-stage method (70.04). This leads the authors to recommend that \"Under low pruning ratios, entropy-only pruning (Stage I) is preferable\". However, the paper's main result in Table 1 is also at a 70% pruning ratio, where the two-stage method (\"Ours\") achieves the best FID.\n2. There are concerning FID scores for the main method that were not addressed. Table 2 shows that stage 1 (Entropy-based pruning) alone at a 70% pruning rate achieves an FID of 32.8622 at 60K iterations. Table 1 lists the results for the same 70% pruning rate at 100K iterations, where the best result after the main one is 39.34. This shows all the remaining methods perform worse than just standard entropy-based pruning.\n3. The ablation study in Table 5 is not a fair comparison. For 85% pruning, the authors evaluate their method under its best configuration (Stage II on DH↑30%) according to Table 3, but do not grant competing samplers an equivalent hyperparameter tuning, which gives their method an unfair edge.\n4. The paper focuses on RS generative foundation models but limits its experiments to optical (RGB) and near-infrared data. The authors explicitly state this limitation. This is a significant weakness, as a foundation model specialized remote sensing should handle the multi-modal bands. The proposed entropy-based filter, for example, may not be suitable for SAR data, which is dominated by speckle noise rather than low-information uniform color.\n5. The proposed pipeline is a straightforward combination of (i) global Shannon entropy filtering and (ii) centroid‑based clustering/sampling. The core idea of deriving prototypes/centroids from curated datasets and then pull the unlabeled pool toward those prototypes resembles DINOv2/V-JEPA 2 automatic data curation pipeline. The paper does not discuss this conceptual overlap or carefully position what is new beyond swapping to RS‑specific priors."}, "questions": {"value": "1. At 70% pruning, why does Stage I-only (FID 68.38, Table 3) outperform the full Stage I+II pipeline (FID 70.04, Table 3), while in Table 1, the full pipeline (\"Ours\") is used (FID 28.46)? Can you mention what configuration you used in table 1 for Stage I and II and also the number of clusters?\n2. It was mentioned that you used Git-RSCLIP for feature extractor, because compared with backbones such as DINOv2, it delivers more reliable and discriminative embeddings for RS imagery. It there a source to support that claim that compares those two?\n3. The sampling ablation in Table 5 appears to be biased. The \"Ours\" method uses its optimal input subset (DH↑30%) as identified in Table 3. Can the authors ensure this is a fair comparison? Was this DH↑30% subset also found to be optimal for the other sampling strategies, or your method simply benefiting from a best-case configuration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J2UpedSdYN", "forum": "yhFZcnbMwE", "replyto": "yhFZcnbMwE", "signatures": ["ICLR.cc/2026/Conference/Submission2744/Reviewer_jhCu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2744/Reviewer_jhCu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866167845, "cdate": 1761866167845, "tmdate": 1762916356011, "mdate": 1762916356011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "dMJhh5s1om", "forum": "yhFZcnbMwE", "replyto": "yhFZcnbMwE", "signatures": ["ICLR.cc/2026/Conference/Submission2744/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2744/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763000538871, "cdate": 1763000538871, "tmdate": 1763000538871, "mdate": 1763000538871, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores data pruning strategies for remote sensing generative foundation models (RS-GFMs) to reduce training cost while maintaining model performance. It introduces three pruning techniques, e.g., uncertainty-based, representation similarity-based, and gradient influence-based, to identify and discard redundant or low-value samples before large-scale generative model training. Experiments are performed using a diffusion-based RS-GFM on multispectral and hyperspectral datasets (e.g., BigEarthNet-S2, Houston HSI) to evaluate efficiency and accuracy trade-offs. The authors claim that up to 30–40% of training data can be pruned with negligible degradation in reconstruction and downstream classification performance"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Motivation: The problem of excessive data redundancy in the RS foundation model pretraining is practically important, given the scale of current multimodal EO datasets.\n\nClarity of goal: The work explicitly focuses on data efficiency, a relevant aspect often overlooked in the current GFM literature.\n\nExperimental variety: Multiple pruning criteria are tested, and quantitative evaluations include both reconstruction and downstream metrics.\n\nPotential utility: If robust, such pruning could reduce computation and storage requirements in large RS model training pipelines."}, "weaknesses": {"value": "Lack of novelty.\nThe three proposed strategies are direct adaptations of well-known data pruning and active learning techniques (e.g., influence functions, gradient norm, sample uncertainty from model entropy, and representation similarity from feature clustering). No novel algorithmic formulation or theoretical contribution is introduced. The work essentially repackages standard machine learning pruning heuristics for remote sensing data without domain-specific adaptation or innovation.\n\nNo significant performance gain or insight.\nThe results show only minor computational savings (~20–30%) with marginal accuracy drops. However, no deeper analysis is provided on why certain samples are redundant or how pruning affects spectral diversity, spatial distribution, or sensor-specific characteristics—factors crucial in RS GFMs.\n\nExperimental limitations.\n\nThe tested models are small-scale diffusion or autoencoder variants (tens of millions of parameters), not true foundation models (hundreds of millions to billions). Thus, conclusions may not generalize.\n\nThe pruning ratios, metrics, and datasets are limited; no experiments on cross-sensor generalization, multimodal inputs, or real foundation-scale data are shown.\n\nAblation settings and hyperparameters (e.g., gradient computation batch size, entropy thresholds) are under-specified.\n\nComparisons with modern data selection or dataset distillation methods (e.g., CRAIG, GradMatch, K-center, D2 pruning, ActiveDF) are missing.\n\nClarity and presentation issues.\nFigures are small and difficult to read, with low resolution and poor contrast; key visual comparisons (e.g., Figure 3 and Figure 5) are nearly illegible. The text contains overlapping abbreviations and inconsistent notation. The flow between Sections 3–5 is disorganized, mixing method and results.\n\nPositioning gap.\nThe paper fails to connect its work to the current GFM ecosystem (e.g., SpectralGPT, EarthGPT, OmniSat, CROMA, SatMAE). It does not analyze how pruning interacts with pretraining paradigms like masked modeling or diffusion-based generation. This omission makes the contribution appear detached from the state of the art."}, "questions": {"value": "How does pruning affect the spectral coverage and spatial distribution of the retained dataset? Are important edge cases or rare materials disproportionately removed?\n\nHow would the methods scale to real GFM settings with 100M+ parameters and global EO datasets?\n\nCan pruning be made sensor-aware or modality-adaptive rather than using generic uncertainty or gradient scores?\n\nWhy were state-of-the-art data selection methods like GradMatch, K-center, and D2 not compared?\n\nCan the authors provide clearer, higher-resolution figures and detailed algorithm pseudocode to improve readability and reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "txgnWoStoY", "forum": "yhFZcnbMwE", "replyto": "yhFZcnbMwE", "signatures": ["ICLR.cc/2026/Conference/Submission2744/Reviewer_isxz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2744/Reviewer_isxz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916384799, "cdate": 1761916384799, "tmdate": 1762916355818, "mdate": 1762916355818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical challenge of training generative foundation models for Remote Sensing on large-scale, noisy, and redundant datasets. The authors propose a two-stage pruning approach that combines entropy-based filtering (Stage I) with scene-aware clustering and stratified sampling (Stage II). The key innovation is leveraging existing RS scene classification datasets as reference benchmarks to guide clustering, avoiding the computational cost of clustering large-scale unlabeled data directly. Experiments demonstrate that this pruning strategy significantly improves results over training on the full dataset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a critical problem in RS generative modeling. The authors clearly articulate the unique challenges of RS data (redundancy, noise, class imbalance) and why existing pruning methods are insufficient. The idea of using a \"prior bank\" of existing classification datasets to guide the clustering of a massive, unlabeled generative dataset is creative. \n- The method is tested on multiple large-scale datasets, the ablation studies are comprehensive and robustly support the design choices.\n- The paper is well-written and clearly structured."}, "weaknesses": {"value": "1. The number of clusters, $K$ is a crucial and sensitive hyperparameter. Figure 3 shows that performance degrades if $K$ is too large or too small. However, the paper does not provide a principled method for selecting the optimal $K$. This seems to require an expensive hyperparameter sweep, which undermines the method's \"efficiency\" claim.\n2. The success of Stage II relies on the careful manual curation of the \"prior bank\". The authors note in Figure 3 that simply aggregating more datasets can hurt performance due to cross-dataset heterogeneity. This implies a user must carefully select complementary datasets, which is a non-trivial prerequisite.\n3. The paper's core contribution, Stage II, is not universally beneficial and its efficacy is questionable. As shown in Table 3, the combined Stage I+II pipeline performs worse than the entropy-only (Stage I) baseline in many scenarios (e.g., for Git-10M at 70% pruning)."}, "questions": {"value": "1. The entire Stage II pipeline (both centroid generation and sample assignment) is critically dependent on the quality and properties of the chosen feature extractor, Git-RSCLIP. Could the authors comment on how sensitive the Stage II results are to the choice of feature extractor? The paper does not investigate how this choice impacts the results. \n2. Did the authors conduct multiple runs with different random seeds? Are the reported FID improvements statistically significant? What is the performance variance across different random seeds (e.g., for K-means initialization and model training)?\n3. The paper states that FID is computed against a selected set of 5,000 \"high-quality\" images (Section 4.1). How was this test set selected? Was it a simple random sample from the full dataset, or was it filtered using criteria similar to Stage I (e.g., high entropy)? If the test set itself is \"clean,\" this could introduce evaluation bias that unfairly favors models trained on the pruned (cleaner) data over those trained on the full, noisy dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uCwXhH9JDH", "forum": "yhFZcnbMwE", "replyto": "yhFZcnbMwE", "signatures": ["ICLR.cc/2026/Conference/Submission2744/Reviewer_3XeW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2744/Reviewer_3XeW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2744/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992951567, "cdate": 1761992951567, "tmdate": 1762916355655, "mdate": 1762916355655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}