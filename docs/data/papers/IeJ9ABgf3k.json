{"id": "IeJ9ABgf3k", "number": 3730, "cdate": 1757507706196, "mdate": 1763738736363, "content": {"title": "OmniEduBench: A Comprehensive Chinese Benchmark for Assessing Large Language Models in Education", "abstract": "With the rapid development of large language models (LLMs), various LLM-based works have been widely applied in educational fields. However, most existing LLMs and their benchmarks focus primarily on the knowledge dimension, largely neglecting the evaluation of cultivation capabilities that are essential for real-world educational scenarios. Additionally, current benchmarks are often limited to a single subject or question type, lacking sufficient diversity. This issue is particularly prominent within the Chinese context. To address this gap, we introduce OmniEduBench, a comprehensive Chinese educational benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs. The data is meticulously divided into two core dimensions: the knowledge dimension and the cultivation dimension, which contain 18.121K and 6.481K entries, respectively. Each dimension is further subdivided into 6 fine-grained categories, covering a total of 61 different subjects (41 in the knowledge and 20 in the cultivation). Furthermore, the dataset features a rich variety of question formats, including 11 common exam question types, providing a solid foundation for comprehensively evaluating LLMs' capabilities in education. Extensive experiments on 11 mainstream open-source and closed-source LLMs reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro surpassed 60\\% accuracy, while in the cultivation dimension, the best-performing model, QWQ, still trailed human intelligence by nearly 30\\%. These results highlight the substantial room for improvement and underscore the challenges of applying LLMs in education.", "tldr": "", "keywords": ["OmniEduBench; Large Language Models; Educational Applications; Knowledge Dimension;  Cultivation Dimension"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f2d9b108e022411fc32383fc018c9f327bf7e796.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces OmniEduBench, a comprehensive Chinese benchmark for evaluating both knowledge mastery and cultivation capabilities in educational scenarios. The dataset is large-scale, diverse in subjects and formats, and constructed through a reasonably rigorous pipeline. It provides meaningful contributions to Chinese educational evaluation. However, concerns regarding theoretical grounding and data validity remain. Overall, the work has clear contributions and notable research value."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.Large-scale and diverse dataset covering multiple subjects and assessment formats, significantly enriching resources for evaluating educational LLMs.\n2.The construction pipeline is systematic and transparent, ensuring strong reproducibility.\n3.Extensive benchmarking with state-of-the-art LLMs across two core dimensions and multiple sub-dimensions, with clearly presented results."}, "weaknesses": {"value": "1. Some major claims are not sufficiently supported by the presented results:\nThe statement on Page 9, Lines 479–481 regarding performance drops in cultivation tasks does not fully align with the differences shown in Tables 3 and 4.\nThe claim that “the best-performing models still lag behind human-level performance by nearly 30%” lacks explicit numerical justification.\nSince the dataset is heavily filtered based on models’ incorrect predictions and over 90% of the data is private or LLM-generated, the difficulty level is inherently high. Therefore, the results cannot support broader claims that LLMs are ineffective in educational contexts.\n\n2.Although manual verification is emphasized, the paper does not provide details on annotation criteria, rubric design, or inter-rater agreement, reducing credibility.\n3. The structure could be improved by moving the Related Work section earlier (e.g., Section 2) to better contextualize the contribution."}, "questions": {"value": "1.What is the proportion of the final 24K samples originating from the three sources (public, private, and LLM-generated)?\n2.Do the three data sources exhibit measurable differences in difficulty? If so, why are performance comparisons across sources not reported?\n3.Since the dataset includes multiple educational levels, could the authors provide a breakdown of sample distribution and model performance by educational stage ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MwaIW4avNA", "forum": "IeJ9ABgf3k", "replyto": "IeJ9ABgf3k", "signatures": ["ICLR.cc/2026/Conference/Submission3730/Reviewer_dsE8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3730/Reviewer_dsE8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761527142105, "cdate": 1761527142105, "tmdate": 1762916952393, "mdate": 1762916952393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents OmniEduBench, a large-scale Chinese benchmark designed to evaluate LLMs in educational scenarios. The benchmark covers a wide range of subjects and question types across multiple educational levels, aiming to assess LLMs’ capabilities in teaching, learning assistance, and educational reasoning. The authors construct a dataset of over 24.6K samples, primarily generated by LLMs and subsequently verified by experts, and evaluate numerous state-of-the-art models using various scoring methods. The study provides comprehensive analyses of model performance and highlights challenges specific to the educational domain."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed Chinese education-oriented benchmark is valuable and timely, providing a standardized tool to assess LLMs' abilities in a socially and practically important domain.\n2. The benchmark includes diverse question types and subject areas, ensuring broad coverage and reflecting realistic educational use cases.\n3. The paper evaluates many models and offers interesting analyses and observations, contributing useful empirical insights to the community."}, "weaknesses": {"value": "1. During dataset construction, 86.3% of the data (800K/927K) are generated by LLMs, which raises concerns about authenticity and reliability. Given that education is a highly sensitive field, relying heavily on machine-generated data may introduce subtle inaccuracies. Although the authors mention that 15% of the samples were verified by experts, even small errors can have significant consequences in educational contexts.\n2. The inclusion of *cultivation value* as an expert evaluation criterion is conceptually interesting but potentially subjective. The paper should clarify how this metric is defined and measured, with concrete examples illustrating what constitutes high versus low cultivation value.\n3. In the section on \"Results using different LLM-assisted scoring methods\", the authors state that \"careful selection of scoring models is necessary\". However, if benchmark scores depend heavily on the choice of evaluation model, this raises questions about **the stability and objectivity** of the benchmark. Heavy reliance on LLM-based evaluation may compromise statistical reliability.\n4. The paper lacks verification of **data contamination**. It is unclear whether the tested models have been exposed to the benchmark data during pretraining or fine-tuning. Implementing contamination checks is essential to ensure that the evaluation results are valid and unbiased.\n5. While the benchmark is large and comprehensive, its **innovation is somewhat limited**, as it mainly extends existing benchmarks along the data and domain dimensions without introducing new perspectives or methodological advances.\n6. Minor issue: Line 323 should refer to *GPT-4o*, not *GPT-40*."}, "questions": {"value": "Please check my comments in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RP2cFabFhI", "forum": "IeJ9ABgf3k", "replyto": "IeJ9ABgf3k", "signatures": ["ICLR.cc/2026/Conference/Submission3730/Reviewer_Fmit"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3730/Reviewer_Fmit"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566824658, "cdate": 1761566824658, "tmdate": 1762916952148, "mdate": 1762916952148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniEduBench, a comprehensive Chinese benchmark designed to evaluate large language models (LLMs) in educational settings across two core dimensions: knowledge and cultivation. It comprises 24,602 question–answer pairs spanning 61 subjects (41 in knowledge, 20 in cultivation) and 11 common exam question types. Experiments on 11 mainstream LLMs reveal significant performance gaps: even the best model, Gemini-2.5 Pro, achieves only 62.76% accuracy in the knowledge dimension, while the top performer in cultivation (QwQ) lags nearly 30% behind human-level performance. The results underscore the challenges of deploying LLMs as effective educational assistants in real-world Chinese contexts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The dataset is relatively large in scale.\n\n2. The experimental setup is rigorous: both 0-shot and few-shot evaluations are conducted separately, and the impact of using different LLMs as scoring models on the results is compared."}, "weaknesses": {"value": "1. There is insufficient detailed comparison with existing educational benchmark datasets (e.g., C-Eval) in terms of data volume, dimensions, etc.\n\n2. The description of the “HARD” subset is limited. If it is defined as “the 26% of samples on which models perform worst,” this may bias the subset toward the weaknesses of specific models rather than reflecting objective difficulty.\n\n3. The paper should include more details about model inference settings—such as temperature, sampling strategy, and whether results are averaged over multiple runs—to better demonstrate the fairness of the evaluation.\n\n4. The baselines lack specialized expert models in specific domains, such as those tailored for mathematics or medical tasks."}, "questions": {"value": "1. How is the OmniEduBench HARD subset constructed and annotated? Is there a clear description of whether difficulty is determined based on human expert judgment or model performance?\n\n2. As shown in Tables 3 and 4, smaller Qwen3 models outperform larger ones. Has the authors conducted any specific analysis to explain why model performance on OmniEduBench deviates from the expected scaling law?\n\n3. In the Chinese context, several models claim to support educational applications (e.g., ERNIE and iFlytek Spark). Why were these models not included in the evaluation?\n\n4. Cultivation dimension is an abstract concept—how does this paper ensure the reasonableness and representativeness of the tasks designed for this dimension?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8o4Ig98YCl", "forum": "IeJ9ABgf3k", "replyto": "IeJ9ABgf3k", "signatures": ["ICLR.cc/2026/Conference/Submission3730/Reviewer_eaft"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3730/Reviewer_eaft"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866359906, "cdate": 1761866359906, "tmdate": 1762916951147, "mdate": 1762916951147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniEduBench, a Chinese education benchmark designed to evaluate LLMs on both knowledge understanding and cultivation skills. OmniEduBench consists of 24.602K question-answer pairs with 18.121K for knowledge and 6.481K entries for cultivation. This benchmark features a 11 common exam question types and has done extensive experiments on 11 mainstream open-source and closed-source LLMs. The benchmark highlights room for improvement of applying LLMs in education."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper introduces a highly comprehensive benchmark, commendable for its broad coverage spanning 41 distinct subjects and 11 common question types.\n2.\tOmniEduBench includes the \"cultivation dimension,\" which moves beyond mere knowledge assessment to evaluate crucial pedagogical competencies, including emotional support and value guidance.\n3.\tThe manuscript is well-organized and clearly written, making the methodology easy to follow."}, "weaknesses": {"value": "1.\tSerious Data Quality Concerns: The quality of the dataset is questionable, based on the paper's own examples. Several provided questions are flawed:\na)\tFigure 3(b) (cultivation dimension) is highly subjective; option C appears to be an equally valid approach, making the single \"correct\" answer debatable.\nb)\tFigure 4(b) (knowledge dimension) presents a proof question in part (2) but provides no proof or expected solution.\nc)\tFigure 8 (value alignment) contains ambiguous \"correct\" options. It's unclear why options A (Q1) and D (Q2) are not considered valid, raising concerns about the benchmark's objectivity. \nThese examples damage confidence in the overall quality of the 24.6K items.\n2.\tThe \"dual-model filtering\" approach to defining difficulty raises concerns about the benchmark's generalizability. By selecting only questions that two specific models(Qwen3 and QWQ) failed, the benchmark (OmniEduBench) risks being tailored to their particular weaknesses. As a result, it may not serve as a balanced test of general educational knowledge for other models.\n3.\tThe dataset's heavy reliance on LLM-generated data (800k/927k) is a major concern. This approach risks using the LLM's own biases rather than objective, \"correct values.\" The human annotator appears insufficient; it was limited to filtering by 50 students and scoring by 5 experts, with a lack of human-led question modification. Furthermore, the evaluation criteria for this human annotation are vague and not clearly explained in the paper."}, "questions": {"value": "1.\tThe authors must clarify the specific data quality issues raised in Weakness 1. This clarification is crucial to establish the dataset's objectivity and resolve the existing doubts about its quality..\n2.\tThe paper states 800K data points were LLM-generated. To assess reproducibility and potential bias, please provide more details. Which specific LLM(s) were used? What prompts or generation strategies were employed, particularly to create the nuanced scenarios for the \"cultivation\" dimension？\n3.\tThe paper needs more detail on the human annotation process. What specific guidelines and criteria were given to the 50 students for the filtering task? Similarly, what were the detailed scoring rules used by the 5 senior annotators for the quality evaluation in Table 2.\n4.\tThe paper mentions \"LLM-assisted scoring.\" Please elaborate on this method. How exactly is the LLM used to evaluate responses (e.g., what prompts are used)? Crucially, how was the reliability and accuracy of this automated scoring validated, for instance, by correlating it against human expert scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c1IsQYFNnr", "forum": "IeJ9ABgf3k", "replyto": "IeJ9ABgf3k", "signatures": ["ICLR.cc/2026/Conference/Submission3730/Reviewer_bj4W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3730/Reviewer_bj4W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088607785, "cdate": 1762088607785, "tmdate": 1762916950811, "mdate": 1762916950811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}