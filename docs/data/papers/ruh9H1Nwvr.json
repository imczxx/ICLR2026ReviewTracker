{"id": "ruh9H1Nwvr", "number": 19038, "cdate": 1758293024212, "mdate": 1763729303379, "content": {"title": "How Should We Evaluate LLM Reasoning Quality For Fact Verification?", "abstract": "The reasoning traces generated by Large Language Models (LLMs) are increasingly used to improve final predictions, enable reinforcement learning based on reasoning trace correctness, and justify model outputs to users. Their recognized utility spurred a line of works on evaluating LLM reasoning quality. However, such current reasoning evaluation methods are typically generic and do not shed light on the different reasoning types that may be required for various complex tasks. In this paper, we investigate reasoning quality for the prominent task of *Fact Verification*, where a model should determine whether a given claim is entailed by a reference source text, a fundamental process known as Natural Language Inference (NLI). Specifically, we propose a novel evaluation framework that considers the prominent types of inference steps involved in NLI reasoning: hypothesis *decomposition* into individual facts, followed by source *attribution* and *entailment* decision for each fact, and finally *aggregation* of fact level decisions into the final entailment classification. Our protocol introduces fine-grained metrics to assess both the existence (whether a step was performed) and the quality (how well it was performed) for each inference type. Following this framework, we first conduct a meticulous manual evaluation of six prominent LLMs, and then scale the evaluation using LLM-as-a-Judge. Our analysis reveals several insights, including: (1) a significant positive correlation exists between the quality of the reasoning trace and the correctness of the final prediction; (2) models often omit necessary reasoning steps, leading to incomplete justifications; and (3) guiding the LLM towards a systematic reasoning trace based on our framework often improves the quality of both the reasoning trace and the overall entailment classification, specifically for \"non-reasoning\" models. Overall, our work provides a more diagnostic and nuanced approach to understanding and evaluating LLM reasoning trace, demonstrated specifically for NLI reasoning in fact verification, proposing insights for future improvements in reasoning quality and its downstream usage.", "tldr": "This paper introduces a novel framework for evaluating LLM's NLI reasoning in fact verification, which traces the specific inference types involved, applied through both manual and automated analyses of model reasoning paths.", "keywords": ["reasoning", "CoT", "fact verifcation", "entailment", "NLI"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c7ad35469150abbfc98039f609e0514f410b3f28.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework for evaluating the reasoning quality of LLMs in fact verification, decomposing reasoning into four components: Decomposition, Attribution, Entailment, and Aggregation. The paper's novelty comes from introducing two axes of evaluation: existence (whether a step occurs) and quality (how well it’s done). The authors conduct manual evaluations of six LLMs (Llama-3, Gemini, DeepSeek) across guided and unguided reasoning settings, then scale up using an LLM-as-a-Judge for automatic evaluation. They find correlations between reasoning trace quality and final prediction accuracy, frequent omission of reasoning steps in unguided settings, and a tradeoff between soundness and completeness in decomposition."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "The paper is clearly written, visually well-presented, and easy to follow. Figures and examples effectively illustrate the framework. Prior work is well-situated, and the structure is logical and transparent.\nsound reasoning. \n\nThe type-aware distinction (decomposition, attribution, etc.) is intuitive and well-motivated and defended.\n\nThe experiments are competently executed, with clear definitions for each metric and a well-controlled manual annotation process. Inter-rater reliability is reported rigorously. The empirical evidence supports the paper’s stated observations, and no major methodological flaws are apparent."}, "weaknesses": {"value": "While the idea of decomposing reasoning evaluation by inference type is conceptually reasonable, the actual implementation of the metrics is very simple, as they are mostly presence indicators and proportion-based correctness measures. The four-step NLI reasoning structure (decomposition, attribution, entailment, aggregation) is often discussed (although often implicitly) in prior annotation and system design literature; the paper’s contribution lies only in formalizing metrics around these existing steps.\n\nThe experimental findings, though thorough, are unsurprising: guided prompting improves reasoning completeness, larger models omit fewer steps, and reasoning trace quality correlates with answer correctness. The results, as they are evaluative not experimental, provide no basis for actionable change in the use of these models. \n\nThe work lacks theoretical or methodological depth, feels incremental and does not meaningfully advance how we evaluate reasoning quality."}, "questions": {"value": "How do the authors justify novelty relative to existing reasoning evaluation work? \n\nCould any of the defined metrics actually be used to improve model training or reward shaping, or are they purely diagnostic?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NjgF4PrIcS", "forum": "ruh9H1Nwvr", "replyto": "ruh9H1Nwvr", "signatures": ["ICLR.cc/2026/Conference/Submission19038/Reviewer_ksLa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19038/Reviewer_ksLa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761281269373, "cdate": 1761281269373, "tmdate": 1762931077582, "mdate": 1762931077582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for evaluating the reasoning quality of LLMs in fact verification, decomposing reasoning into four components: Decomposition, Attribution, Entailment, and Aggregation. The paper's novelty comes from introducing two axes of evaluation: existence (whether a step occurs) and quality (how well it’s done). The authors conduct manual evaluations of six LLMs (Llama-3, Gemini, DeepSeek) across guided and unguided reasoning settings, then scale up using an LLM-as-a-Judge for automatic evaluation. They find correlations between reasoning trace quality and final prediction accuracy, frequent omission of reasoning steps in unguided settings, and a tradeoff between soundness and completeness in decomposition."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written, visually well-presented, and easy to follow. Figures and examples effectively illustrate the framework. Prior work is well-situated, and the structure is logical and transparent.\nsound reasoning. \n\nThe type-aware distinction (decomposition, attribution, etc.) is intuitive and well-motivated and defended.\n\nThe experiments are competently executed, with clear definitions for each metric and a well-controlled manual annotation process. Inter-rater reliability is reported rigorously. The empirical evidence supports the paper’s stated observations, and no major methodological flaws are apparent."}, "weaknesses": {"value": "While the idea of decomposing reasoning evaluation by inference type is conceptually reasonable, the actual implementation of the metrics is very simple, as they are mostly presence indicators and proportion-based correctness measures. The four-step NLI reasoning structure (decomposition, attribution, entailment, aggregation) is often discussed (although often implicitly) in prior annotation and system design literature; the paper’s contribution lies only in formalizing metrics around these existing steps.\n\nThe experimental findings, though thorough, are unsurprising: guided prompting improves reasoning completeness, larger models omit fewer steps, and reasoning trace quality correlates with answer correctness. The results, as they are evaluative not experimental, provide no basis for actionable change in the use of these models. \n\nThe work lacks theoretical or methodological depth, feels incremental and does not meaningfully advance how we evaluate reasoning quality."}, "questions": {"value": "How do the authors justify novelty relative to existing reasoning evaluation work? \n\nCould any of the defined metrics actually be used to improve model training or reward shaping, or are they purely diagnostic?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NjgF4PrIcS", "forum": "ruh9H1Nwvr", "replyto": "ruh9H1Nwvr", "signatures": ["ICLR.cc/2026/Conference/Submission19038/Reviewer_ksLa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19038/Reviewer_ksLa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761281269373, "cdate": 1761281269373, "tmdate": 1763757975077, "mdate": 1763757975077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission introduces a structured framework for evaluating the reasoning traces produced by LLMs in the context of fact verification, framed as the task of NLI. The authors decompose NLI reasoning into four key components: hypothesis decomposition, source attribution, entailment decision per sub-claim, and aggregation of decisions. They propose metrics assessing both the existence (whether a step occurs) and quality (e.g., correctness) of each component. The evaluation involves six LLMs (including reasoning-optimized models) tested on the ClearFacts dataset using unguided and guided CoT prompts. A manual annotation on 30 samples is complemented by scaling via LLM-as-a-Judge. Key findings include correlations between reasoning quality and final accuracy, frequent omission of steps in unguided settings, and improved performance with guided prompts for non-reasoning models. The paper claims this type-aware approach provides more diagnostic insights than generic reasoning evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The framework's decomposition into functionally distinct reasoning steps is a key contribution of the work and a thoughtful advancement over uniform, generic metrics (e.g., groundedness or faithfulness), allowing for targeted diagnosis of LLM weaknesses in complex tasks like NLI.\n- Empirical insights, such as the trade-off in decomposition granularity (higher under guidance but at the cost of soundness) and the efficiency of guided non-reasoning models, are practical and substantiated by correlation analyses and cost comparisons."}, "weaknesses": {"value": "- The key contribution of the paper is the delineation of evaluation dimensions for NLI tasks and the insights from manual evaluation based on the rubrics; but the manual evaluation is limited to only 30 samples, which is insufficient for robust conclusions about model behaviors across a diverse dataset like ClearFacts (combining 14 benchmarks). This small scale raises concerns about generalizability, especially given the observed variability in model performance.\n- For scalability the authors include results with LLM judge (Gemini-2.5-Flash); but its agreement with human annotations is only 'Moderate' for critical metrics like inference and entailment correctness (Gwet's scores ~0.56-0.58). This could introduce biases or inaccuracies, undermining claims about broader trends.\n- The paper lacks empirical comparisons to existing evaluation methods for \"reasoning\" quality (e.g., ROSCOE, arXiv:2212.07919 or ReCEval, arXiv:2304.10703), relying exclusively on conceptual critiques. Without quantitative comparison, it's unclear if this framework truly outperforms alternatives in diagnostic power or correlation with task accuracy.\n- The focus is narrowly on NLI for fact verification, ignoring broader reasoning tasks (e.g., math or multi-hop QA), which limits novelty - many insights (e.g., guided prompting helps) echo prior CoT literature. Additionally, the attribution typology (extractive vs. abstract) is introduced but underutilized, with no deeper analysis of its impact on usability or errors."}, "questions": {"value": "- Why was Gemini-2.5-Flash specifically chosen as the LLM-as-a-Judge, and were alternatives (e.g., other reasoning models) tested for higher agreement on low-performing metrics like entailment?\n- The paper mentions diagnostic completeness (errors traceable to components), but how does this hold empirically against prior methods? Could you provide ablations comparing the proposed metrics to generic ones like faithfulness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5WP1VXFMfb", "forum": "ruh9H1Nwvr", "replyto": "ruh9H1Nwvr", "signatures": ["ICLR.cc/2026/Conference/Submission19038/Reviewer_KDeQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19038/Reviewer_KDeQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761418139610, "cdate": 1761418139610, "tmdate": 1762931077098, "mdate": 1762931077098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a type-aware evaluation framework for NLI-style fact verification, breaking models’ reasoning traces into decomposition, attribution, entailment, and aggregation, scoring each for existence and quality. They annotate 360 model outputs sampled from ClearFacts, and LLM-as-a-judge after demonstrating high inter-annotator agreement, finding that models often skip steps, the correctness of entailment correlates strongly with overall accuracy, that guided prompting can narrow the gap with reasoning models, and non-reasoning models with guided CoT can be more token-efficient."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The clear separation of decomposition, attribution, entailment, and aggregation pairs well with the manual evaluation on a more principled and granular lens, and the LLM-as-a-judge helps to scale with substantial/almost-perfect agreement on the majority of metrics. \n2. The inclusion of a human study with reported IAA is valuable, using Gwet’s AC, with substantial or almost-perfect agreement, and the correlation analysis over the attributes provides important insights. \n3. The finding that extractive attribution is most reliable, followed closely by paraphrase attribution, before a major drop-off with abstract attribution, is intuitive and actionable."}, "weaknesses": {"value": "1. While the LLM-as-a-judge is effective along many lens, it only has moderate agreement for entailment and inference, which are described to be the most important quality metrics, and the LLM-as-a-judge findings drive several of the conclusions made. \n2. The aggregation logic is a bit simple, as there may be other aggregation operators that do not necessarily require all sub-claims to be obviously entailed, but can derive compositional value. \n3. Decomposition is binary per trace, but other existence metrics are defined by proportions over sub-claims, and since guided prompting increases sub-claim count, this can make later “existence” values harsher for comparison. \n4. One concern is that the judge is Gemini-2.5-Flash, while Gemini variants are also one of the generators, so it’s unclear if there may be some bias present."}, "questions": {"value": "1. Do your conclusions change if you normalize existence/quality by token budget or by a fixed number of sub-claims? \n2. Can you replicate the scaled up results with multiple judges and report cross-judge reliability, especially for inference and entailment? \n3. In l268 and l272, please do not have the subscript be the remaining characters — instead, spell out the full word “infer” and “entail” in the subscript, while the value can still be denoted by I and E, respectively, as they currently are."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IvooQmib3u", "forum": "ruh9H1Nwvr", "replyto": "ruh9H1Nwvr", "signatures": ["ICLR.cc/2026/Conference/Submission19038/Reviewer_PjAS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19038/Reviewer_PjAS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951348343, "cdate": 1761951348343, "tmdate": 1762931076743, "mdate": 1762931076743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for evaluating how LLMs reason during fact verification. The authors argue that current evaluation methods are too general, so they design a structured approach that divides reasoning into four stages: decomposition, attribution, entailment, and aggregation. They test various LLMs, comparing unguided and guided reasoning prompts, and find that models often skip key reasoning steps unless explicitly guided. Guided prompts generally improve reasoning completeness and accuracy, especially for smaller or non-reasoning models. The study shows a strong link between reasoning quality and correct predictions, and suggests that structured evaluation can help diagnose weaknesses and improve both reasoning quality and reliability of LLM outputs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors frame fact verification as a reasoning task and define a diverse set of reasoning components, which provides a valuable perspective for redefining the fact verification task in reasoning models.\n\n2. The individual reasoning components proposed by this paper, as well as the overall system design for fact verification, are reasonable.\n\n3. By involving human annotators, the authors increased the reliability of the validation of their proposed methodology."}, "weaknesses": {"value": "1. Overall, the authors decompose the fact verification task into multiple components and define it compositionally. The structure of each component and the overall process (as suggested in Figure 2) sounds reasonable. However, in order to demonstrate that this actually contributes to LLM fact verification, the paper should have provided broad analysis across a variety of models (different model families and capacities) and across multiple benchmarks. In that respect:\n\n  1-1. The authors claim that applying their method to non-reasoning models and using guided CoT can yield performance gains, but experiments were conducted on a very limited set of models (Llama and Gemini 2.0 Flash).\n\n  1-2. The ClearFacts benchmark is a good dataset for this task, but despite the existence of many other datasets for fact verification, the experiments were conducted on only a single benchmark.\n\n  1-3. A total of 360 human judgments is not enough to establish the reliability of the human annotations.\n\n2. The component taxonomy presented in the paper is reasonable, but the methodology itself is not novel. It essentially compares results before and after prompting. I’m not opposed to influencing model behavior through prompting, but it’s disappointing that the proposed complex system (which aims to perform multiple NLI tasks simultaneously) is implemented with just a single prompt. A more fine-grained, component-level analysis could have been done.\n\n3. The proposed approach appears to rely heavily on decomposition. The paper should include more qualitative examples showing how decomposition is performed for different statements."}, "questions": {"value": "Questions and suggestions are listed in Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AZm2jX7GzG", "forum": "ruh9H1Nwvr", "replyto": "ruh9H1Nwvr", "signatures": ["ICLR.cc/2026/Conference/Submission19038/Reviewer_qvPw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19038/Reviewer_qvPw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988082900, "cdate": 1761988082900, "tmdate": 1762931076295, "mdate": 1762931076295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}