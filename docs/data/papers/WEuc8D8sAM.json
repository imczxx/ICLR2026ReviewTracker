{"id": "WEuc8D8sAM", "number": 6977, "cdate": 1758004028502, "mdate": 1763641406406, "content": {"title": "SpikeGen: Decoupled “Rods and Cones” Visual Representation Processing with Latent Generative Framework", "abstract": "The process through which humans perceive and learn visual representations in dynamic environments is highly complex. From a structural perspective, the human eye decouples the functions of cone and rod cells: cones are primarily responsible for color perception, while rods are specialized in detecting motion, particularly variations in light intensity. These two distinct modalities of visual information are integrated and processed within the visual cortex, thereby enhancing the robustness of the human visual system. Inspired by this biological mechanism, modern hardware systems have evolved to include not only color-sensitive RGB cameras but also motion-sensitive Dynamic Visual Systems, such as spike cameras. Building upon these advancements, this study seeks to emulate the human visual system by integrating decomposed multi-modal visual inputs with modern latent-space generative frameworks. We named it ***SpikeGen***. We evaluate its performance across various spike-RGB tasks, including conditional image and video deblurring, dense frame reconstruction from spike streams, and high-speed scene novel-view synthesis. Supported by extensive experiments, we demonstrate that leveraging the latent space manipulation capabilities of generative models enables an effective synergistic enhancement of different visual modalities, addressing spatial sparsity in spike inputs and temporal sparsity in RGB inputs.", "tldr": "", "keywords": ["bio-inspired", "image representation learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/16538e2435f34c57dec3047289b0111c358eae62.pdf", "supplementary_material": "/attachment/bffcdc95b709d69f6e5579a7c06ebe3ef5436ba4.zip"}, "replies": [{"content": {"summary": {"value": "Mimicking the decoupling of color and light intensity perception of human into cone and rod cells, the authors proposed SpikeGen, which outperforms previous state-of-the-art methods in multiple spike-RGB tasks: 1) conditional image/video deblurring, 2) dense frame reconstruction and 3) scene novel-view sythesis. \n\nSpikeGen follows the general setup of a Masked Auto-regressive mode (MAR) pretrained with diffusion loss. It uses a frozen VAE encoder and a trainable spike encoder to encode blurry RGB input and spike stream into latent representations, respectively. \n\nSpikeGen has two training stages, a pretraining stage and a finetuning stage. During the pretraining, the loss used is the per-token diffusion loss between the faded clear RGB latent and the latent predicted by the ViT followed by a compact MLP. The spike stream loss is used during the finetuning stage.\n\nThe experiment results reported in the paper show that SpikeGen beats all other benchmarks on all three tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The empirical results reported by the authors on the three tasks show the superior performances of SpikeGen over all previous methods compared in the paper. The superiority of the proposed method and training pipeline is consistent across various tasks, metrics and threshold settings. \n\n2. The figures presented in the paper are clear, well-organized, and informative, especially Figure 2. It effectively and clearly conveys the overall training pipeline while also giving details on the architecture of the Spatial–Temporal Separable Spike Encoder. This visualization greatly improves the reader’s understanding of the specific modifications made to the original MAR pipeline.\n\n3. The adoption of the latent diffusion training for a dual modality spike-RGB self-supervised training method is both innovative and effective. The introduction of a random gamma parameter during training enables controllable modality dominance, allowing adaptation to different downstream tasks with minimal effort."}, "weaknesses": {"value": "1. Although the authors briefly mention at line 475 that SpikeGen’s novel-view synthesis pipeline is two-stage, the main text provides insufficient explanation. It remains unclear to me 1) what the two stages are specifically (deblurring + vanilla 3DGS?),  2) how they differ from other benchmarks reported in Table 3, and 3) how time efficient (or inefficient) is SpikeGen compared to other single-stage methods. A more detailed description and comparison would greatly improve the paper’s completeness.\n\n2. Table 9 in the appendix provides comparisons with other two-stage NVS methods. However, it only includes two relatively old, training-free approaches. The authors are encouraged to include other two-stage methods, e.g. those listed in Table 2, to provide a more complete and up-to-date comparison.\n\n3. An ablation study replacing MAR with a regular conditional DiT diffusion model would provide a much stronger basis for SpikeGen’s architectural design choices. \n\n4. Similarly, an ablation on design choices of the S3 encoder would also further validate the architecture's effectiveness. Nevertheless, considering the page limit, this omission is understandable and does not substantially affect my overall evaluation.\n\n5. At lines 69–70, the phrase “By reviewing current studies” appears to be a typo or editing error."}, "questions": {"value": "1. Have you tried to linearly interpolate gamma values for testing? How does gamma values affect performances?\n\n2. Please see the weaknesses section for additional questions and suggestions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DEfNwujoZ5", "forum": "WEuc8D8sAM", "replyto": "WEuc8D8sAM", "signatures": ["ICLR.cc/2026/Conference/Submission6977/Reviewer_CRQ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6977/Reviewer_CRQ3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721582749, "cdate": 1761721582749, "tmdate": 1762919196408, "mdate": 1762919196408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Mimicking the decoupling of color and light intensity perception of human into cone and rod cells, the authors proposed SpikeGen, which outperforms previous state-of-the-art methods in multiple spike-RGB tasks: 1) conditional image/video deblurring, 2) dense frame reconstruction and 3) scene novel-view sythesis. \n\nSpikeGen follows the general setup of a Masked Auto-regressive mode (MAR) pretrained with diffusion loss. It uses a frozen VAE encoder and a trainable spike encoder to encode blurry RGB input and spike stream into latent representations, respectively. \n\nSpikeGen has two training stages, a pretraining stage and a finetuning stage. During the pretraining, the loss used is the per-token diffusion loss between the faded clear RGB latent and the latent predicted by the ViT followed by a compact MLP. The spike stream loss is used during the finetuning stage.\n\nThe experiment results reported in the paper show that SpikeGen beats all other benchmarks on all three tasks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The empirical results reported by the authors on the three tasks show the superior performances of SpikeGen over all previous methods compared in the paper. The superiority of the proposed method and training pipeline is consistent across various tasks, metrics and threshold settings. \n\n2. The figures presented in the paper are clear, well-organized, and informative, especially Figure 2. It effectively and clearly conveys the overall training pipeline while also giving details on the architecture of the Spatial–Temporal Separable Spike Encoder. This visualization greatly improves the reader’s understanding of the specific modifications made to the original MAR pipeline.\n\n3. The adoption of the latent diffusion training for a dual modality spike-RGB self-supervised training method is both innovative and effective. The introduction of a random gamma parameter during training enables controllable modality dominance, allowing adaptation to different downstream tasks with minimal effort."}, "weaknesses": {"value": "1. Although the authors briefly mention at line 475 that SpikeGen’s novel-view synthesis pipeline is two-stage, the main text provides insufficient explanation. It remains unclear to me 1) what the two stages are specifically (deblurring + vanilla 3DGS?),  2) how they differ from other benchmarks reported in Table 3, and 3) how time efficient (or inefficient) is SpikeGen compared to other single-stage methods. A more detailed description and comparison would greatly improve the paper’s completeness.\n\n2. Table 9 in the appendix provides comparisons with other two-stage NVS methods. However, it only includes two relatively old, training-free approaches. The authors are encouraged to include other two-stage methods, e.g. those listed in Table 2, to provide a more complete and up-to-date comparison.\n\n3. An ablation study replacing MAR with a regular conditional DiT diffusion model would provide a much stronger basis for SpikeGen’s architectural design choices. \n\n4. Similarly, an ablation on design choices of the S3 encoder would also further validate the architecture's effectiveness. Nevertheless, considering the page limit, this omission is understandable and does not substantially affect my overall evaluation.\n\n5. At lines 69–70, the phrase “By reviewing current studies” appears to be a typo or editing error."}, "questions": {"value": "1. Have you tried to linearly interpolate gamma values for testing? How does gamma values affect performances?\n\n2. Please see the weaknesses section for additional questions and suggestions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DEfNwujoZ5", "forum": "WEuc8D8sAM", "replyto": "WEuc8D8sAM", "signatures": ["ICLR.cc/2026/Conference/Submission6977/Reviewer_CRQ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6977/Reviewer_CRQ3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721582749, "cdate": 1761721582749, "tmdate": 1763585286010, "mdate": 1763585286010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework called SpikeGen, which takes a blurry image and the corresponding spike stream as input to generate a clear image. The authors evaluated the method on three downstream task datasets, demonstrating its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The manuscript is well-structured, the figures are clear, the layout is appropriate, and the language flows smoothly.\n\n2. The approach is bio-inspired and well-justified.\n\n3. The experimental evaluation is sufficient and the results demonstrated are promising."}, "weaknesses": {"value": "1. The paper's citation format needs to be revised.\n\n2. In Figure 2, I believe the blurry RGB latent and the Clear RGB latent should show differences (or be visually distinct), instead of being represented by the same shape and color."}, "questions": {"value": "1. In lines 276 to 280, the authors describe the calculation process for the faded image $I_{faded}$. However, I could not seem to find where this result is applied within the method and Figure 2.\n\n2. In line 312, the authors claim their model was pre-trained on ImageNet. However, that dataset only includes clear images. I am curious how the blurry images and the spike streams were obtained, especially the latter.\n\n3. Based on the formula in line 270, the hyperparameter $\\gamma$ takes a value of 0 or 1 with high probability (>60%). Does such a modality drop rate seem too high? This seems to indicate that the model mainly receives single-modality input during the training process.\n\n4. How are the pixel-space similarity measures performed during fine-tuning with limited data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3NrvH55NmA", "forum": "WEuc8D8sAM", "replyto": "WEuc8D8sAM", "signatures": ["ICLR.cc/2026/Conference/Submission6977/Reviewer_QTzd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6977/Reviewer_QTzd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727887614, "cdate": 1761727887614, "tmdate": 1762919196067, "mdate": 1762919196067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SpikeGen, a novel latent generative framework for decoupled visual representation learning between RGB and spike modalities. It performs diffusion modeling in the latent space, combining VAE-encoded representations with a per-token diffusion mechanism to balance efficiency and effectiveness. The framework incorporates a configurable dual-modality latent pre-training mechanism and a spatio-temporal separable spike encoder for efficiently extracting temporal–spatial features from spike streams. Experiments conducted on multiple benchmark datasets, including REDS, GOPRO, VidarReal, and Blender-NeRF, show that SpikeGen surpasses existing methods across multiple metrics, demonstrating superior generalization and robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper introduces latent variables into such tasks for the first time, enabling the model to achieve higher-level feature modeling while maintaining both efficiency and effectiveness. The proposed method demonstrates strong originality, reliable and comprehensive experimental results, and clear exposition, making it an contribution to multimodal visual representation learning."}, "weaknesses": {"value": "1. The training cost of SpikeGen’s latent diffusion process is relatively expensive and requires substantial pre-training resources, which limits its reproducibility and practical deployability.\n\n2. During the pre-training phase on the ImageNet dataset, the spike frame configuration involves randomly sampling 8 frames from 64 generated spike frames for each image. This setup may prevent the model from fully leveraging information from the spike modality, causing it to rely primarily on RGB inputs and thus partially degrade into a single-modality model.\n\n3. Certain details in the paper are described ambiguously. In Section 3.2 (Spatial–Temporal Separable Spike Latent), the authors mention that the model generates temporal attention weights through two consecutive 1×1×1 3D convolutional layers to model the temporal dimension explicitly. However, this process lacks formal equations or explicit computational explanations, which affects the interpretability and reproducibility of the method.\n\n4. Although the model achieves impressive results on synthetic and benchmark datasets, its validation on real-world event-based data remains limited, making it insufficient to fully demonstrate the model’s generalization capability under complex real-world conditions."}, "questions": {"value": "1. During the pre-training phase on ImageNet, the model randomly samples only 8 frames from 64 generated spike frames. How do the authors ensure that such sparse temporal sampling can still effectively capture spike information? Have experiments with different sampling numbers (e.g., 16 or 32 frames) been conducted to verify that the model indeed utilizes spike information rather than primarily relying on RGB features?\n\n2. In Section 3.2, the authors mention that the model generates temporal attention weights through two consecutive 1×1×1 3D convolutional layers. Could the authors supplement this part by explaining how these weights are computed and applied, to clarify their role in the feature fusion process?\n\n3. The model is pre-trained on ImageNet using 8 A800 GPUs. Does the model's outstanding performance stem from the retraining advantage gained through ample computational resources, or from model innovation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JKF77XbvcQ", "forum": "WEuc8D8sAM", "replyto": "WEuc8D8sAM", "signatures": ["ICLR.cc/2026/Conference/Submission6977/Reviewer_xKds"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6977/Reviewer_xKds"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810820842, "cdate": 1761810820842, "tmdate": 1762919195510, "mdate": 1762919195510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SpikeGen, a biologically inspired framework that mimics the rods–cones decoupling mechanism in human vision.\nThe model fuses spike streams (representing temporal luminance information) and RGB images (representing spatial–chromatic information) in a shared latent diffusion space, thereby achieving joint visual representation learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1 The paper's multimodal fusion in the latent space rather than the pixel space is elegant and computationally efficient. The approach avoids the need for spatially precise alignment between asynchronous spikes and frames.\n\n2 The paper includes quantitative and visual comparisons on multiple datasets and modalities, covering both synthetic and real-world-like settings."}, "weaknesses": {"value": "(A) Task–Method Mismatch\n\nThe model’s design is static latent diffusion, but some tasks (e.g., motion deblurring) inherently require explicit temporal modeling (e.g., flow, exposure trajectory).\nSpikeGen’s temporal encoder uses only 3D convolutions, which might not capture fine-grained dynamics.\n\nIn novel-view synthesis, the model lacks geometric consistency constraints (e.g., ray-based volumetric modeling).\nThe improved perceptual quality may not correspond to true 3D structure preservation.\n\n(B) Evaluation and Fairness Issues\n\nSpikeGen is compared against single-modality baselines (e.g., SpkDeblurNet, DeblurGS) while itself using RGB+Spike dual input — an unfair comparison unless baselines are also dual-modality.\n\nThe absence of clear ablation studies (e.g., removing diffusion, removing spike input, removing γ-fusion) makes it difficult to identify the true source of performance gain.\n\nTraining details for each task (e.g., hyperparameters, dataset scale, latent dimensionality) are insufficient for reproducibility.\n\n(C) Data Authenticity and Realism\n\nMost spike data are synthetic (via SpikingSim) and do not represent real sensor noise, asynchronous pixel behavior, or refractory effects.\n\nFor each task, the paper lacks quantitative evaluation on real spike-camera datasets.\n\n(D) Overly Perfect Results / Missing Uncertainty\n\nSpikeGen outperforms all prior works on all metrics across all tasks — an unlikely scenario that raises concerns about overfitting or inconsistent training conditions.\n\nThe diffusion framework, by nature, introduces randomness and perceptual diversity; yet, the results show unrealistically consistent sharpness and color balance without variance analysis.\n\n(E) Efficiency and Practicality\n\nDespite operating in latent space, diffusion models remain computationally expensive.\nThe paper does not report inference speed or energy consumption, which are crucial in neuromorphic vision research that emphasizes efficiency."}, "questions": {"value": "1. How are the spike encoder’s temporal windows determined across different frame rates or datasets?  \n   Is the encoder adaptive to spike density variations?\n\n2. For the deblurring task, does SpikeGen explicitly model the exposure time or motion trajectory, or rely solely on spike event accumulation?\n\n3. How is the latent diffusion conditioned on spike features?  \n   Is it concatenation, cross-attention, or a learned fusion layer?\n\n4. How does the model generalize to **real spike data** (e.g., Vidar)?  \n   Have the authors tested domain adaptation or noise robustness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LMRED8pkvu", "forum": "WEuc8D8sAM", "replyto": "WEuc8D8sAM", "signatures": ["ICLR.cc/2026/Conference/Submission6977/Reviewer_146W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6977/Reviewer_146W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927209623, "cdate": 1761927209623, "tmdate": 1762919194883, "mdate": 1762919194883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response [Update of PDF & Point-by-Point Reply]"}, "comment": {"value": "First and foremost, we would like to express our sincere gratitude to the Reviewers, Area Chairs, and Program Chairs for their time and dedicated effort. We have carefully studied **all comments** and have provided detailed **point-by-point** responses under each Reviewers.\n\nWe are encouraged not only by the positive scores but, more importantly, by the recognition of our work from the reviewers across three key dimensions:\n* **Method:** \"Elegant and efficient\" (`Reviewer 146W`); \"efficient, effective and original\" (`Reviewer xKds`); \"well-justified\" (`Reviewer QTzd`); \"innovative and effective\" (`Reviewer CRQ3`).\n* **Evaluation:** \"Multiple datasets and modalities\" (`Reviewer 146W`); \"reliable and comprehensive\" (`Reviewer xKds`); \"sufficient and promising\" (`Reviewer QTzd`); \"consistent\" (`Reviewer CRQ3`).\n* **Presentation:** \"Clear\" (`Reviewer xKds`); \"well-structured\" (`Reviewer QTzd`); \"well-organized and informative\" (`Reviewer CRQ3`).\n\nWe also acknowledge the limitations in our initial submission. For instance, due to space constraints, we were unable to fully elaborate on our **efficiency optimizations**. This led to a misconception among some reviewers that our method incurs high computational costs simply because it is based on latent diffusion. In reality, while prioritizing performance (\"effectiveness\"), we have also improved \"efficiency\" through rigorous architectural choices and designs. We have detailed these optimizations in our responses and have prepared a **revised PDF** (with updates highlighted in **orange**) for your reference.\n\nWe are grateful for the active engagement of reviewers such as **Reviewer CRQ3**, who has already entered into multi-round discussions with us and has further acknowledged our contributions. We sincerely hope to engage in similarly constructive dialogue with other reviewers, as we believe this exchange is vital for improving the quality of our work.\n\nBest regards,\n\n**Authors of Submission 6977**"}}, "id": "Sg6mctNnAJ", "forum": "WEuc8D8sAM", "replyto": "WEuc8D8sAM", "signatures": ["ICLR.cc/2026/Conference/Submission6977/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6977/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission6977/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763641874844, "cdate": 1763641874844, "tmdate": 1763641874844, "mdate": 1763641874844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}