{"id": "vlraTIgUD3", "number": 572, "cdate": 1756748859929, "mdate": 1759898253015, "content": {"title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search", "abstract": "The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect that are crucial for real-world tasks like analyzing documents with dense charts/diagrams or navigating maps. To address this gap, we first introduce o3-bench, a new benchmark designed to evaluate multimodal reasoning while attending to visual details. O3-bench features challenging questions that require agents to gather subtle visual information from multiple distinct areas of an image while performing complex, interleaved reasoning using the gathered information. These tasks are highly challenging even for frontier systems like OpenAI o3, which only obtains 42.8% accuracy on o3-bench. To tackle these tasks, we propose InSight-o3, a multi-agent framework that divides labor between a visual reasoning agent (vReasoner) and a visual search agent (vSearcher). As a concrete first step towards o3-like systems, we focus on the latter (i.e., vSearcher) in this paper, for which we introduce the task of generalized visual search---locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent that can be directly called by other agents, our vSearcher significantly improves the performance of existing frontier multimodal models by empowering them with generalized visual search on a wide range of benchmarks.", "tldr": "", "keywords": ["thinking with images", "o3", "visual search", "multi-agent framework", "reinforcement learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/361436aea5932db9290ac28e2787bffd8325d270.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel multi-agent framework that decomposes multimodal reasoning into two agents: a visual reasoning agent (vReasoner) that performs CoT reasoning and provides answers, and a visual search agent (vSearcher) that outputs coordinates given a description. Given an input, the vReasoner processes the answer and, when needed, calls the vSearcher to find a relevant visual region. The authors also built InSight-o3-vS using a novel RL training pipeline and presented the O3-Bench to evaluate complex visual reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. I think decomposing mulitmodal reasoning model into two agents can be efficient without the need of training the whole model, especially with strong closed source models we cannot train. \n2. Training pipeline and constructing collage data for vSearcher is interesting and novel.\n3. O3-Bench can be beneficial to the community.\n4. Nice addiotional observation report from the hands-on experience."}, "weaknesses": {"value": "1. **Reliance on Closed-Source Models**: The experiments rely heavily on closed-source models (like GPT-5-mini), which are difficult to use for reproduction or further training. As a result, training the vSearcher based on a closed model lacks scalability and reproducibility for the wider research community. I think experiments with open-source models can improve this paper significantly\n2. **Lack of Experiments**: While the vSearcher is an interesting approach, its validity would be more convincing if it were tested more rigorously.\n3. **Limited Benchmark Coverage**: The O3-Bench has a relatively small scale (185 images, 318 QA samples). This is a limitation, especially since the chart images are sourced from the existing MME-RealWorld dataset."}, "questions": {"value": "1. Could you provide results for the vSearcher using different model sizes, or paired with open-source vReasoner models (such as Qwen2.5-VL or InternVL3)?\n2. The paper states that the O3-BENCH chart images are from MME-RealWorld. Could you clarify in more detail how the tasks (i.e., the questions) for this domain differ from the original MME-RealWorld chart domain?\n3. Does the number of layouts in the O3-Bench data affect model performance? (For example, is there a correlation between a higher layout count and lower model accuracy?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gFhaY2iBKS", "forum": "vlraTIgUD3", "replyto": "vlraTIgUD3", "signatures": ["ICLR.cc/2026/Conference/Submission572/Reviewer_7mrZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission572/Reviewer_7mrZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761641646538, "cdate": 1761641646538, "tmdate": 1762915549561, "mdate": 1762915549561, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to augment visual-language reasoning models with the capability to perform a visual search function. In particular, it trains an additional visual search module that predicts the bounding box coordinate related to a given description (e.g., instructions derived from the input question), and feeds the retrieved image patches to pretrained reasoning models. A new multi-modal reasoning dataset is also presented, which focuses on reasoning on complex charts and digital maps. Experimental results show that the method improves the performance of several state-of-the-art reasoning models across different datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) It is a promising direction to improve multi-modal reasoning models by incorporating external tools (i.e., the visual search module in this case), which could provide both performance boost and enhanced transparency.\n\n(2) The use of collages for visual search training reduces the reliance on large-scale naturalistic data.\n\n(3) The paper develops a new evaluation benchmark for multi-modal reasoning, and can benefit the development of subsequent models.\n\n(4) The proposed method shows generalizability across several models and datasets."}, "weaknesses": {"value": "(1)  It is not a new idea to combine VLMs with external tools (e.g., some compositional reasoning models [ref1, ref2] already explore the tool usage with reinforcement learning). The paper experiments with a single tool (i.e., visual search framed as a visual grounding task), while solving real-life problems could require diverse abilities. It is unclear whether visual search (especially when trained independently) can help generalize reasoning across different scenarios.\n\n(2) One advantage of having an additional visual search agent is to provide a transparent interface of the decision-making process. Nevertheless, the paper only reports the accuracy on datasets, without any analysis of how the improvement is achieved with visual search.\n\n(3) Looking at Table 1, it appears that the proposed method only shows consistent improvement on models from the GPT family, and can have negative effects on the best-performing Gemini model. Please justify the inconsistency in performance.\n\n(4) The proposed visual search module is trained on synthetic collages created by stitching together different images. Such a paradigm ignores the contextual relationship between different regions within a visual scene, and also introduces boundary artifacts. Since the search agent is essentially trained on a visual grounding task, I wonder how it will perform when training on naturalistic grounding datasets.\n\n(5) The new dataset contains a very limited set of stimuli (~350 images), making it difficult to be used for training or comprehensive evaluation.\n\n[ref1] ViperGPT: Visual Inference via Python Execution for Reasoning. CVPR, 2023.\n\n[ref2] HYDRA:AHyper Agent for Dynamic Compositional Visual Reasoning. ECCV, 2024."}, "questions": {"value": "(1) What are the advantages of visual search over other types of external tools?\n\n(2) How does the visual search agent help the reasoning agent? Please provide in-depth analysis of the decision-making process.\n\n(3) Why does the model only improve the GPT models but lead to worse performance on Gemini? Is it related to the use of GPT-nano for evaluation?\n\n(4) Please justify how the proposed training paradigm could accommodate the artifacts in synthetic data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x6hLikgj8t", "forum": "vlraTIgUD3", "replyto": "vlraTIgUD3", "signatures": ["ICLR.cc/2026/Conference/Submission572/Reviewer_Pra6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission572/Reviewer_Pra6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761691892811, "cdate": 1761691892811, "tmdate": 1762915549453, "mdate": 1762915549453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of current multimodal agents in visual reasoning by introducing a new benchmark, o3-bench, which requires models to integrate fine-grained information from multiple image regions while performing complex reasoning. To tackle this challenge, the researchers developed the InSight-o3 multi-agent framework, focusing on its vSearcher module with generalized visual search capabilities. Trained via reinforcement learning, vSearcher can locate conceptual visual regions based on natural language instructions and, as a plug-and-play component, significantly enhances the performance of state-of-the-art models across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies a specific weakness in current multimodal agentsâ€”their inability to perform complex reasoning that requires integrating fine-grained visual details. It presents a challenging benchmark (o3-bench) designed explicitly to measure this underdeveloped capability.\n\n2. The proposed InSight-o3 framework presents a sophisticated multi-agent architecture that decomposes the complex problem into specialized sub-tasks (reasoning and search)."}, "weaknesses": {"value": "The claim that the framework's search steps decrease with increasing resolution is not sufficiently supported, as the reported variations across resolutions are minimal. This suggests the search pattern may be overly reliant on the characteristics of the training data, raising concerns about its scalability and effectiveness in real-world, multi-step search-and-reasoning tasks involving high-resolution images.\n\nThe framework's performance on powerful yet tool-agnostic models like GPT-4o and Gemini-2.5 (as seen in VisualProbe-Hard and MME-RW-Lite results) is suboptimal. This indicates a strong dependency on models pre-equipped with tool-calling capabilities trained with multi-turn RL, highlighting a limitation in its general applicability. Further optimization is required to improve its stability and performance across a wider range of model architectures.\n\nThe ablation study on hybrid RL training reveals a trade-off: while combining static and dynamic RL improves performance on the proprietary O3-bench, it results in higher per-step inference latency without a clear balance of their respective advantages (static RL's speed vs. dynamic RL's adaptability). Furthermore, the performance gains appear primarily concentrated on O3-bench, questioning the hybrid method's generalization to broader tasks."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JU7gKAernr", "forum": "vlraTIgUD3", "replyto": "vlraTIgUD3", "signatures": ["ICLR.cc/2026/Conference/Submission572/Reviewer_EKkk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission572/Reviewer_EKkk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815165175, "cdate": 1761815165175, "tmdate": 1762915549326, "mdate": 1762915549326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of using an LLM reasoning with a specialist LLM visual reasoner which can be called as a tool. The paper makes two contributions: (1) a new, complex, yet quite small dataset of  questions which requires multi-modal reasoning. Half of these are complex questions about maps, as well illustrated in Fig 1, requiring zooming in and reading the legends of the maps. (2) optimize a special visual searcher (i.e. visual LLM) in the context of a strong LLM-based reasoner. The second contribution seems the largest. They formulate the visual reasoning problem as a cooperative task between a visual reasoner (a pre-trained LLM) and a visual searcher (a visual LLM which is called as a tool by the visual reasoner and optimized with RL for the task at hand). Since optimization in a cooperative setting is really hard, the paper chooses to fix the visual reasoner to GPT-5-mini-2025-08-07  and optimize the visual searcher (i.e.  Qw2n2.5-VL-Instruct in this paper).\n\nOn standard academic benchmarks such as V*-Bench, VisualProbe, and MME-RealWorld results demonstrate that they can adding their now-trained visual searcher to several GPT variants and Gemini Flash and obtain significant improvements on top of using those models without their visual searcher. The fact that is works on many visual reasoners shows generalization.  Additionally, their best result with GPT-5-mini and Gemini-2.5-Flash outperforms the state-of-the-art on the majority of datasets. Note that while Mini-O3 seems to be stronger, AFAIK this appeared on ArXiv only recently and should be counted as contemporary work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper decouples visual reasoning from visual search, leading to a modular system which is more understandable and whose components can be trained independently.\n* Results are state-of-the-art.\n* Good improvement using Gemini as a visual reasoner suggests that even though their visual search model was optimized on GPT-5-mini, it generalizes to multiple visual reasoners.\n* Ablation shows good improvement of RL fine-tuning and training efficiency benefit of using their static RL setup in conjunction with the dynamic RL setup.\n* New and complex visual reasoning dataset."}, "weaknesses": {"value": "* The new dataset is rather small and limited in domain.\n* Minor: Table 1 is a bit confusing: the bottom part seems the most important while the top part is hardly discussed and only used for context; I would suggest to show the bottom part either on top or maybe better shown separately as Figure 1.\n* Minor: it is unclear which tools are used by Qwen2.5-VL. Anything other than 'crop'?\n* Minor: specialization of agents and sub-agents in agentic frameworks has been shown to work in prior art. Examples are [Socratic Models, Zeng et al., ICLR'23] and [HAMMR, Castrejon et al., NeurIPS workshop 2024]. Would be good to cite some of these works."}, "questions": {"value": "I don't really have specific questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NuAluY9ANj", "forum": "vlraTIgUD3", "replyto": "vlraTIgUD3", "signatures": ["ICLR.cc/2026/Conference/Submission572/Reviewer_CxbX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission572/Reviewer_CxbX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission572/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902516935, "cdate": 1761902516935, "tmdate": 1762915549179, "mdate": 1762915549179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}