{"id": "XNar6WUIit", "number": 8362, "cdate": 1758079941086, "mdate": 1759897789606, "content": {"title": "From Bias to Benefit: Place Good Documents in Good Positions", "abstract": "Large language models (LLMs) exhibit a U-shaped positional bias in processing input information, characterized by heightened attention to tokens at the beginning and end of the prompt while ignoring information in the middle, also known as the Lost-in-the-Middle phenomenon. In this paper, we investigate the internal mechanisms underlying this phenomenon by analyzing how positional bias influences attention weights across both horizontal (input-level) and vertical (layer-level) dimensions of the model. Based on these findings, we propose U-shaped Placement, a strategy that leverages inherent positional bias of the model by assigning documents to positions that align with its attention pattern. By combining this placement strategy with the importance estimations of documents, effectively placing good documents in good positions, we enhance the model’s ability to utilize documents within two iterations. Experimental results demonstrate that our method consistently outperforms existing baselines across multiple models and datasets, indicating that leveraging positional bias can bring improved document utilization capability. Our codes are submitted with the paper and will be publicly available.", "tldr": "", "keywords": ["interpretability", "generalization", "open-domain QA"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c451a33e9382887e99c04b18887538dd9f8eb19.pdf", "supplementary_material": "/attachment/2c5487df16615115f0dbc2dfc0cc42cf321e771c.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a 2-stage method that puts more important documents on the beginning or end of the input. The motivation is to utilize the U-shaped position bias of LLMs to make the more important documents get more attention. \nIn stage 1, to rank the importance of each document, it uses averaged attention weights from answer tokens. Moreover, it separates a \"position bias score\" to decide whether a document should be placed at the beginning or end of the input. In stage 2, LLM uses the reordered input to generate the answer.\nExperiments on 3"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear structure and easy to understand\n2. Experiments on many baseline methods and datasets"}, "weaknesses": {"value": "1. This method is similar to baseline methods such as Attention Sorting, so the novelty is limited. The only difference is to put the document either on the beginning or the end based on a \"position bias score\".\n2. The findings and insights are trivial for me. Most are already found by previous works about lost-in-the-middle, but these works are not cited in Related works. For example:\n\n[1] Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization\n\n[2] Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models\n\n[3] Eliminating Position Bias of Language Models: A Mechanistic Approach\n\n[4] Mitigate Position Bias in Large Language Models via Scaling a Single Dimension\n\n3. This method relies on attention weights. However, calculating attention weights means more computing, and the inability to use FlashAttention2 in generation. This will significantly slow down the inference speed, especially when the input is very long. So it is not so practical.\n4. The improvement compared to baselines is not so great."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nOw2UMe3pV", "forum": "XNar6WUIit", "replyto": "XNar6WUIit", "signatures": ["ICLR.cc/2026/Conference/Submission8362/Reviewer_nuGU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8362/Reviewer_nuGU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760660025828, "cdate": 1760660025828, "tmdate": 1762920272900, "mdate": 1762920272900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the U-shaped position bias in LLM-based RAG systems. The authors propose a new method to isolate position bias in LLMs and design a U-shaped placement strategy to organize input documents. Extensive experiments are conducted to validate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper provides a thorough and detailed study of the U-shaped position bias phenomenon.\n\n2.The related work is well-summarized, and the connection to prior research is clearly articulated.\n\n3.The proposed solution is simple and efficient."}, "weaknesses": {"value": "1.In Step 1, the authors use attention weights to estimate document importance. However, since attention weights themselves may be influenced by position bias, it is unclear whether same documents placed in different positions would yield consistent importance scores. This potential confounding effect needs to be clarified and ideally supported with empirical evidence.\n\n2.In Lines 193–195, when discussing the relationship between position bias and the number of documents, the authors should provide the exact document counts for each dataset. Moreover, this relationship should also be tested within the same dataset by varying the number of documents to ensure a fair comparison.\n\n3.Regarding the finding in Lines 257–259—“positional distinctions are more pronounced in the lower layers”—this may not hold for ordered inputs. As shown in Appendix D.2, the curves for lower and higher layers appear nearly identical under ordered settings, suggesting that the distinction is much less evident.\n\n4.In addition to Figure 3, the accuracy and stability of the calculated position biases should be further justified. For instance, when document orders are randomized multiple times, do the computed biases remain consistent? Providing more quantitative evidence and analysis of variance across random orders would strengthen this point.\n\n5.The experiments are limited to small-scale LLMs. It would strengthen the paper to evaluate the method’s scalability on larger models (e.g., 10B or 30B parameters) to confirm its scalability."}, "questions": {"value": "1.Several figures (e.g., Figures 2 and 3) are of low resolution. They should be redrawn using vector formats such as .eps or .pdf. Additionally, increasing line weight and font size would improve readability.\n\n2.There are some typos in the paper, e.g., line 152 Dlanguage and line 758 lowercase \"we\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NUnHfzW1dW", "forum": "XNar6WUIit", "replyto": "XNar6WUIit", "signatures": ["ICLR.cc/2026/Conference/Submission8362/Reviewer_tut1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8362/Reviewer_tut1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729843824, "cdate": 1761729843824, "tmdate": 1762920272473, "mdate": 1762920272473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the \"Lost in the Middle\" phenomenon—where models tend to assign higher attention to the beginning and end of input prompts while neglecting the middle—from the perspective of the model's attention mechanism. It constructs a position score by aggregating attention weights before and after answer tokens. Based on document importance scores and position scores, the paper proposes a strategy called U-shaped Placement, which rearranges documents to align with the model’s inherent positional bias, ensuring that highly relevant content is placed in positions that receive greater attention."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The investigation of the \"Lost in the Middle\" phenomenon from the perspective of the attention mechanism may better reveal its underlying causes.\n\n2. Based on the insights gained from this investigation, the proposed U-shaped Placement strategy is supported by comprehensive experiments."}, "weaknesses": {"value": "1. There is a lack of ablation studies on the token-level scores. A more rigorous validation of the rationale behind using token-level (as opposed to document-level) scores would involve replacing only the scoring granularity while keeping all other factors constant.\n\n2. The experiments are limited to smaller-scale models, leaving it unclear whether the U-shaped Placement strategy generalizes to larger LLMs with more parameters.\n\n3. The paper's exposition could be clearer. The distinction between related prior work and the novel contributions of this work is not sufficiently emphasized."}, "questions": {"value": "1. Why do the results for \"U-shaped Original\" in Table 3 differ from those in Table 2? Are the results in Table 3 obtained by replacing the token-level score with a document-level score? If so, what distinguishes this document-level version from existing methods?\n\n2. What is the specific difference between the token-level method proposed in this paper and the token-level method(s) mentioned in Table 4?\n\n​Comment:​​\n\nIf the authors can clarify the distinctions between their designed method and existing approaches, thereby providing a clearer understanding of its advantages, I would consider raising my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c1KmTm33wg", "forum": "XNar6WUIit", "replyto": "XNar6WUIit", "signatures": ["ICLR.cc/2026/Conference/Submission8362/Reviewer_fxGj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8362/Reviewer_fxGj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831387107, "cdate": 1761831387107, "tmdate": 1762920272087, "mdate": 1762920272087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the U-shaped positional bias of large language models (LLMs) — their tendency to attend more strongly to tokens at the beginning and end of a prompt while neglecting the middle (“lost in the middle” phenomenon). The authors analyze this behavior from both horizontal (input-level) and vertical (layer-level) perspectives using attention weight analysis.\n\nBuilding on these insights, they propose a U-shaped Placement strategy that leverages positional bias by placing documents according to both their relevance and the attention distribution of the model. The first LLM pass estimates document relevance via attention weights and extracts positional bias from attention patterns. In the second pass, documents are rearranged to align important ones with high-attention positions (front and back of the prompt).\n\nExperiments on multi-document QA benchmarks (HotpotQA, Musique, 2WikiMHQA) and multiple LLMs (Vicuna-7B, Llama-3.1-8B, Qwen2.5-7B) show consistent improvements over baselines such as RankGPT, ICR, and SELFELICIT without any additional training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Turning positional bias into a benefit by explicitly optimizing for it is creative and conceptually appealing.\n\n+ The paper systematically confirms the U-shaped bias across models and layers using both horizontal and vertical attention analysis (Fig. 2), adding interpretability to a known but underexplored phenomenon\n\n+ The proposed algorithm requires no retraining or model modification, making it simple to deploy across different open-source LLMs and datasets.\n\n+ The method achieves consistent gains across datasets and models (up to +5–10% EM improvement) over comparable two-pass baselines\n\n."}, "weaknesses": {"value": "- While the empirical motivation is solid, the paper lacks a formal justification or deeper theoretical model for why the U-shaped bias emerges and persists across architectures.\n\n- The method assumes that attention weights correlate well with relevance, which has been debated. Without cross-validation (e.g., gradient-based attribution or causal probes), this assumption may not fully hold.\n\n- The approach requires two LLM inference passes. Although cheaper than fine-tuning, it doubles latency and inference cost, which may matter for real-time systems.\n\n- All datasets are QA benchmarks. It remains unclear whether the approach generalizes to non-QA tasks (e.g., summarization, reasoning, dialogue) that also suffer from “lost in the middle.”\n\n- Gains vary across models. More analysis on model scaling or prompt length sensitivity would strengthen generality claims."}, "questions": {"value": "1) How robust is attention as a measure of document importance? Could you provide any validation—e.g., correlation with relevance scores from gradient-based attribution, SHAP, or token perturbation tests—to support the assumption that attention weights reflect semantic importance rather than just syntactic salience?\n\n2) If attention occasionally misaligns with relevance (e.g., focusing on stopwords or question tokens), how does that affect the placement strategy? Is there any mechanism to filter or normalize attention signals before using them for ranking?\n\n3) Could you clarify how horizontal and vertical attention distributions are aggregated across layers and heads?\nAre all attention heads equally weighted, or are only a subset of “dominant” heads used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iSSAz9curn", "forum": "XNar6WUIit", "replyto": "XNar6WUIit", "signatures": ["ICLR.cc/2026/Conference/Submission8362/Reviewer_7FAP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8362/Reviewer_7FAP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094836994, "cdate": 1762094836994, "tmdate": 1762920271685, "mdate": 1762920271685, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}