{"id": "je6FN2TuWf", "number": 14085, "cdate": 1758228241049, "mdate": 1759897390933, "content": {"title": "Inference-Time Scaling of Diffusion Language Models with Particle Gibbs Sampling", "abstract": "Discrete diffusion models have recently emerged as strong alternatives to autoregressive language models, matching their performance through large-scale training. However, inference-time control remains relatively underexplored. In this work, we study how to steer generation toward desired rewards without retraining the models. Prior works typically focus on resampling or filtering within a single denoising trajectory, optimizing rewards step-by-step without trajectory-level refinement. We introduce particle Gibbs sampling for diffusion language models (PG-DLM), a novel inference-time sampling algorithm that performs trajectory-level refinement, which can preserve generation perplexity under reward optimization. PG-DLM constructs a Markov chain over full denoising trajectories and applies a conditional Sequential Monte Carlo kernel to resample them. Within this framework, we further analyze trade-offs across four key axes for inference-time scaling under fixed compute budgets: particle Gibbs iterations, sample count, denoising steps, and reward estimation cost. Analysis shows that scaling particle Gibbs iterations achieves the best reward–perplexity trade-off. Empirically, PG-DLM consistently outperforms prior methods on both MDLM and LLaDA-8B as base models across a wide range of compute budgets for reward-guided tasks, including toxicity and sentiment control as well as linguistic acceptability.", "tldr": "", "keywords": ["discrete diffusion models; inference time scaling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/004491e9d777d5a43abd825af6d0794f4df3fd2f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a technique for inference-time scaling of diffusion language models using particle Gibbs sampling. Their approach is empowered with conditional Sequential Monte-Carlo (SMC) technique which allows to do step refinement of sampling trajectory. On top of that, they propose to do additional sequential trajectory-level refinements after SMC steps which allows to scale up amount of steps at inference and bring additional improvements to the reward-guided sampling. The paper have extensive ablations over the hyperparameters which control the amount of inference-time compute as well as extensive empirical evidence of the advantage of their approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written\n\n2. The proposed method is simple and easy-to-understand\n\n3. The paper contains extensive ablations of hyperparameters which control the amount of compute at inference, making it very easy for a reader to understand which of these parameter matter in practice.\n\n4. The paper contains extensive experimental evidence demonstrating that this approach works very well in practice, compared to the baselines"}, "weaknesses": {"value": "* While the paper provides convergence and variance bounds in Section 3.4, there are no proofs for both Theorem 1 and Theorem 2. Can the authors provide full proofs for these both theorems?\n\n* Theorem 2 implies that the variance is divided by both k and m, but in Table 4, we see that ESS is mainly high for high values of k. In fact for m=1, k=64, it has the highest values, whereas for k=8,m=8, it has the lowest values. Theorem 2 implies that these should behave in the same way, since variance is divided by (k * m = 64 in both cases. Why is there such a discrepancy? Is the derivation in theorem 2 correct?"}, "questions": {"value": "1. Can you provide proofs for both, Theorem 1 and Theorem 2?\n\n2. Why is there a discrepancy in Table 4? (see Weaknesses). Is the derivation for the variance in theorem 2 correct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "32IGosHotL", "forum": "je6FN2TuWf", "replyto": "je6FN2TuWf", "signatures": ["ICLR.cc/2026/Conference/Submission14085/Reviewer_xrTZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14085/Reviewer_xrTZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14085/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654321372, "cdate": 1761654321372, "tmdate": 1762924562770, "mdate": 1762924562770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new method for inference-time scaling of diffusion language models. The proposed method is the application of particle Gibbs to this setting; it repeats the SMC sampling process iteratively, where at each iteration it uses the best trajectory from the previous iteration as a reference, so as to calibrate the SMC weights across iterations. These higher level iterations provide an axis for scaling in addition the existing axes of number of particles, time-steps and Monte Carlo runs for partial reward estimation inside SMC. Empirical evaluation suggests that this new axis can be beneficial at higher NFEs when compared to other axes."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "-- The paper is clearly written, there’s adequate background material and it is easy to follow.\n-- Theoretical results on consistency and variance are interesting.\n-- The scope of experiments is reasonable and the results, although not very strong, do support the general conclusions of the paper. \n-- The paper’s empirical study of different scaling axes is an interesting contribution."}, "weaknesses": {"value": "-- The main weakness seems to be about the cost-benefit of the proposed idea: the additional cost is sequential in nature and when compared to parallel scaling (e.g., number of particles), there seems to be a marginal benefit at large scale; it is hard to see even this in figure 2, which is supposed to show the benefit of larger m at larger NFE (am I right?)\n\n-- Partial reward estimate uses beam sampling in scaling applied to reward estimation of SMC. A better alternative is to use more accurate reward estimates through (partial) unrolling the reverse diffusion for reward estimation, for example in \\phi steps. Although this is a sequential process, and therefore not as efficient, so is the proposed scaling in the number of iterations. Can you consider this kind of scaling for reward estimation in your study?\n\n-- Prior work search based methods [1,2,3,4] such as MCTS applied to diffusion also revisit trajectories for refinement, and they have been applied to both continuous and discrete diffusion and language. While the paper does generally a reasonable job of discussing related literature, I think it should also discuss these works and adjust its claim about its novelty in improving trajectories in multiple passes.\n\nMinor:\n\n-- Table captions say NFE is up to 128 while it is up to 64\n-- Fix reference in algorithm 1 is repeated\n\n[1] Zhang, Xiangcheng, et al. \"Inference-time scaling of diffusion models through classical search.\" arXiv preprint arXiv:2505.23614 (2025).\n\n[2] Jain, Vineet, et al. \"Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models.\" arXiv preprint arXiv:2506.20701 (2025).\n\n[3] Tang, Sophia, et al. \"TR2-D2: Tree Search Guided Trajectory-Aware Fine-Tuning for Discrete Diffusion.\" arXiv preprint arXiv:2509.25171 (2025).\n\n[4] Guo, Yingqing, et al. \"Training-free guidance beyond differentiability: Scalable path steering with tree search in diffusion and flow models.\" arXiv preprint arXiv:2502.11420 (2025)."}, "questions": {"value": "-- Please let me know if there is a misunderstanding under “weaknesses”.\n\n-- What does “discretization error” mean in the context of theorem 2? \n\n-- Can you justify the following? “reward model has a similar computational cost to the denoiser”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iHkqaruk4W", "forum": "je6FN2TuWf", "replyto": "je6FN2TuWf", "signatures": ["ICLR.cc/2026/Conference/Submission14085/Reviewer_7ri2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14085/Reviewer_7ri2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14085/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761680179944, "cdate": 1761680179944, "tmdate": 1762924562451, "mdate": 1762924562451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PG-DLM, a particle Gibbs sampling method for discrete diffusion LMs. The method is grounded in sequential refinement using conditional SMC, by using the highest reward trajectory as “reference” for the next round to explore variations around this high-reward sample. This allows the practitioner to scale sequential refinement iterations in addition to particle size. This method enjoys the theoretical guarantees of existing SMC methods. Experiments using MDLM and LLaDA backbones show that scaling iterations and particle counts improve final scores on reward like toxicity classifier, linguistic acceptability classifier, and sentiment classifier. PG-DLM performs on par with existing methods."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Sequential refinement is an important scaling axis, and this method is a practical method to allow scaling beyond particle count.\n- The method and algorithm are presented clearly and are easy to understand.\n- PG-DLM enjoys the theoretical convergence guarantees of SMC, so it is a principled method."}, "weaknesses": {"value": "My main concerns revolve around fair comparison with baselines, evaluation metrics, as well as clarifications on certain statements and conclusions drawn from the experiments.\n\n- **Unfair comparison with baselines.** Section 5.1 says that all methods including PG-DLM resample every 20 steps, but Table 7 says PG-DLM resamples every 5 steps (as opposed to 20 for the baselines). Please update the main text to correctly reflect these differences. Perhaps even more crucially, PG-DLM uses the ReMDM backward process and compares it against disadvantaged baselines using vanilla backward process (this is again somewhat hidden and only mentioned in the appendix). For a fair comparison, please provide results by keeping factors other than the PG-DLM algorithm consistent to isolate gains. Looking at Figure 6, it seems like removing ReMDM from PG-DLM results in a substantial drop in performance.\n- **Evaluation shows high rewards, but not overall quality and coherency.** Since the goal is to sample from the reward-tilted posterior (Section 2.2), simply reporting high reward is not enough - it needs to be balanced with high likelihood under the model. In other words, gains in toxicity/CoLA/sentiment score or large PPL drops can be accompanied with a drop in overall coherency, quality and faithfulness to the prompt, since these reward functions are notoriously bad evaluators [1,2] and can be hacked by simply repeating certain words. I suggest the authors (1) report evaluation using humans or LLM-as-judge, since it has been shown that they align better with human judgements [3], and (2) provide qualitative examples to show the method results in still coherent text that respond to the prompt appropriately.\n- **Statistical significance of results.** All results are reported on a very small set of 15 prompts, which is not sufficient to draw general conclusions. Also, it seems all results are reported for a single seed. I *strongly* suggest the authors report results + variance for multiple seeds (and, where relevant, across prompts) to improve the reliability of the results.\n- **Theorem 1 and 2 are known results.** The authors note that since PG-DLM is essentially conditional SMC, existing guarantees apply to their method as well. I find it a bit odd to state these guarantees as theorems without citing what works they were adapted from. I suggest rewriting them as remarks, providing references to the existing results and explaining how they are still valid when applied to the diffusion setting.\n- **Theory–practice gap and strong assumptions.** Asymptotic guarantees require unbiased posterior means and no discretization error. These assumptions are asserted (e.g., for MDLM/LLaDA) but not empirically checked, while most experiments use finite T and a biased reward estimate (which they call beam sampling). This weakens the bridge between theory and the recommended operating point\n- **Overstating novelty.** The claim of being the “first trajectory-level inference-time sampling method for discrete diffusion models with formal convergence and variance guarantees” is overstated, since prior works [4,5] also propose trajectory-level methods with formal guarantees; these relevant methods should also be discussed and contrasted with PG-DM.\n- **Conclusions drawn from experiments.**  Some of the conclusions and discussion in the text needs more clarifications given the provided plots.\n    - Section 4 discusses that increasing the number of Gibbs iterations effectively improve accuracy at mid-to-high NFE regions, but I don’t see it from the plots in Figure 2 since across $m=1,2,4,8$, the performance at higher NFEs seems to be more or less the same, and at lower NFEs increasing $m$ is worse (as noted by authors).\n    - I am having trouble seeing that increasing $k$ helps more than increasing $T$ in Section 4. If we look at Figure 4 and Appendix C, it seems increasing beyond $k=8,16$ there is very little gain. However, in certain ranges there is large gain in performance by increasing denoising steps. I think these plots are not very well-suited for showing trade-offs between two different parameters, I suggest using scatter plots with $k,T$ on the two axes.\n\n*[1] Holtzman, Ari, et al. \"The Curious Case of Neural Text Degeneration.\" International Conference on Learning Representations, 2020.*\n\n*[2] Hashimoto, Tatsunori B., Hugh Zhang, and Percy Liang. \"Unifying Human and Statistical Evaluation for Natural Language Generation.\" Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019.*\n\n*[3] Liu, Yang, et al. \"G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment.\" Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.*\n\n*[4] Jain, Vineet, et al. \"Diffusion Tree Sampling: Scalable inference-time alignment of diffusion models.\" The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025.*\n\n*[5] Guo, Yingqing, et al. \"Training-free guidance beyond differentiability: Scalable path steering with tree search in diffusion and flow models.\" arXiv preprint arXiv:2502.11420 (2025).*"}, "questions": {"value": "- The introduction motivates PG-DLM by saying it performs “trajectory-level refinement”. But why is this desirable, and what is the underlying problem being solved here? Later, they say the “one-shot” generation of SMC is a limitation, without elaborating why.\n- The authors propose a beam sampling method to estimate intermediate rewards, however, it performs the same as random sampling for $\\phi > 1$. And for $\\phi=1$, as noted by the authors, it is equivalent to greedy/argmax decoding. Can the authors explain the benefit of this beam sampling method?\n- Could the authors explain what they meant in Line 51: SMC and particle-based methods  “operate within a single denoising trajectory” when they do have multiple trajectories that get resampled at regular intervals?\n- Line 157 “While the reward-weighted conditional structure can be derived formally from an RL perspective, similar formulations have also been adopted as sampling heuristic”. Can the authors explain how these formulation have been developed without explicit connection to RL objectives? All of these results and the authors’ own derivation in Appendix B use soft-Bellman equations and other RL concepts.\n- Line 205 “Samples evolve as independent trajectories interacting only via resampling, limiting inter-sample correlations.” SMC *does* induce dependence via resampling and shared weights, could the authors explain what is meant here?\n- I am having trouble understanding the values of NFEs reported in the experiments. Table 2 and 3 report results for 1,…,64 NFEs, while a single denoising process takes 1024 NFEs (for MDLM) and 50 NFEs (for LLaDA). So are these per-step-NFEs? Will the authors consider using total NFEs everywhere for consistency and simplicity?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "The paper proposes a general-purpose inference-time alignment method for discrete diffusion models. One set of experiments focuses on increasing toxic content in natural language generation. While this is not the main focus of the paper, I believe this should be accompanied by an ethics statement."}}, "id": "Pwng3EF1fS", "forum": "je6FN2TuWf", "replyto": "je6FN2TuWf", "signatures": ["ICLR.cc/2026/Conference/Submission14085/Reviewer_9miR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14085/Reviewer_9miR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14085/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762219449960, "cdate": 1762219449960, "tmdate": 1762924562101, "mdate": 1762924562101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}