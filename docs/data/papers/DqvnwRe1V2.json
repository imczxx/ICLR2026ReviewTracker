{"id": "DqvnwRe1V2", "number": 1374, "cdate": 1756877034459, "mdate": 1759898212082, "content": {"title": "The Illusion of Forgetting: Post-hoc Utility Recovery from Unlearned Models", "abstract": "Class unlearning seeks to remove the influence of designated training classes while retaining utility on the remaining ones, often for privacy or regulatory compliance. Existing evaluations largely declare success once the forgotten classes exhibit near-zero accuracy or fail membership inference tests. We argue this view is incomplete and introduce the notion of *the illusion of forgetting*: even when accuracy appears suppressed, the black-box outputs of unlearned models can retain residual, recoverable signals about forgotten classes. We formalize this phenomenon by quantifying residual information in the output space and show that unlearning trajectories leave statistically distinguishable signatures. To demonstrate practical implications, we propose a simple yet effective post-hoc recovery framework, which amplifies weak signals using a Yeo–Johnson transformation and adapts decision thresholds to reconstruct predictions for forgotten classes. Across 12 unlearning algorithms and 4 benchmark datasets, our framework substantially restores forgotten-class accuracy while causing minimal degradation on retained classes. These findings (i) expose critical blind spots in current unlearning evaluations, (ii) provide the first systematic evidence that forgotten-class utility can be restored from black-box access alone.", "tldr": "", "keywords": ["Class Unlearning", "Machine Unlearning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/058a1b74c687fc5bb676442aae740de9b04edf95.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper focuses on class unlearning and, in particular, on researching the possibility of recovering unlearned classes after class unlearning methods. It discusses and evaluates 12  MU methods across 4 datasets and claims to expose a ‘blind spot’ in these works and their evaluations, showing that unlearned classes can be recovered using a proposed technique (which is a black-box method, relying only on model outputs).\n\nThis is a well-researched subject and the paper appears to be uninformed by previous work. As such, the paper fails to make clear what, if any, advances are being made with this work. Specifically, prior work has already exposed this claimed new 'blind spot'.\n\nSo, given that the issue is not a 'blind spot', the paper should focus on why the proposed method is better than previous ones. And it fails to explore the relevant design space, position itself within it, and quantify/qualify the advantages of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The approach, showing the recoverability of previously-unlearned classes via an MU algorithm using only a black-box, output-only  method is interesting.\n\n2. The study of 12 algorithms across 4 datasets is comprehensive enough and the results do agree with the claimed ones.\n\n3. The paper is fairly well-written."}, "weaknesses": {"value": "Overall: \n\nAs it stands, the paper’s novelty and the significance of its results are questionable. \nThis is a well-researched subject, and the paper fails to make connections with the fairly large body of work that has addressed the same problem.\n \nSpecifically:\n1. Key related work is neither cited nor compared against. \nIn fact, the paper needs a proper/revamped Related Work Section.\n\nExamples of prominent prior research for this problem follow:\n\n-- the work by Bertran et al (NeurIPS24). \n\n-- the work by Ha et al, “Unlearning’s Blind Spots: Over-Unlearning and Prototypical Relearning Attack”, …\n\nMore relevant works exist - check the references within the above for related efforts. \nThese should/could be used as a baseline or discussed at length at the very least, clearly showcasing advances beyond the SOTA work in this area, if any (for exposing the blind spot). Why is the proposed method better? How much better is it?\n\nIt **is understood** that this work uses statistical analysis of black-box outputs alone and is different to adversarial attacks research in that regard. But both lines of work expose the same 'blind spot'. So the community is already aware of this problem. Given this, the work **must** quantify and qualify why being based on statistical analysis of black-box outputs alone is preferable, advantageous, etc. In other words, **the emphasis should not be on 'exposing a blind spot'** (as it is now) but on **better methods to do so**. And the work for this line of investigation is missing.\n\nI suspect the answer to the above revolves around being a black-box, output-only method. First, **even for black-box output-only attempts, this is not the first work**, to my knowledge. The IJCAI 2025 paper by Sui et al, achieves the same, no?. But it is neither cited nor compared against. \n\nEven the work \"Unlearning Inversion Attacks\" by Hu (which is cited) achieves the same (only it requires access to the original model), no? So then readers would benefit a lot if the paper analyzed the 'design space' of black-box vs wiite-box, with access to original model or not, etc. Black-box, output-only attacks are easier'but do not help against more powerful, stronger adversaries with access to weights, and/or to the original model,  etc. All of this should be discussed and not hidden as it is now with specific references to model deployments and different scenarios.\n\n2. The theoretical results are interesting. But they are based on **key assumptions which should be stated more prominently** in the paper. And which **should be explained and justified**. In particular, with respect to their practicality! Given these theoretical results, is it fair to claim that the method can recover unlearned classes? There appears to be a disconnect between claims and what the theory shows.\n\n3. I would urge the authors to consider other benchmarking-like efforts, such as the work by Cadet et al (/https://arxiv.org/pdf/2410.01276) and by Triantafillou et al (arXiv:2406.09073) to compare against MU methods typically used in MU evaluations.\n\n4.  There exist more recent MU methods with better performance than those presented in the set of 12 MU methods evaluated. In fact, the MU methods examined stop at 2024 and do not include well-known MU papers, such as the one by K. Zhao et al (NeurIPS24) and others published in 2024 and in 2025. I urge the authors to do a complete bibliographic search.\n\n5. The experimental results are fairly limited in the dimension of **scale**. Would the results hold on larger **datasets** (ie ImageNet)? Would they hold on larger and different **models** (e.g. vision transformers, such as ViT)?\n\n6. The paper’s title should make crisper the focus of the paper (e.g, class unlearning, in image classification…). This is important as there has been a large number of research papers dealing with the same problem in LLMs, or diffusion T2I models for example.\n\nOverall, the paper has merit and, if positioned and motivated correctly and compared against the proper baselines, with new MU methods, it could be valuable."}, "questions": {"value": "Please see the weaknesses section above.\n\nI remain open to improving my score, assuming the above are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G7DdYpjgoo", "forum": "DqvnwRe1V2", "replyto": "DqvnwRe1V2", "signatures": ["ICLR.cc/2026/Conference/Submission1374/Reviewer_7rZD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1374/Reviewer_7rZD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659242242, "cdate": 1761659242242, "tmdate": 1762915752212, "mdate": 1762915752212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that even after standard “successful” unlearning (near-zero forgotten-class accuracy, strong MIA), the outputs of unlearned models still contain recoverable signals about forgotten classes. It formalizes this with information-theoretic and geometric analyses, and demonstrates a black-box recovery pipeline that restores large amounts of forgotten-class accuracy across 12 unlearning methods and 4 datasets, with minimal hit to retained classes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposed a clear, reproducible black-box recovery pipeline that exposes residual forgotten-class signal across many unlearning methods and datasets.\n- This provides useful conceptual takeaway for practitioners/evaluators: standard \"near-zero FA + good RA\" can still leak recoverable utility, motivating stronger evaluations."}, "weaknesses": {"value": "- The scope of the paper is limited to class unlearning on vision classifiers. It would be more interesting to see such analysis on sample-/feature-level unlearning, larger scales, or non-vision tasks.\n\n- Consider a straightforward mitigation: masking/zeroing logits for the forget class or removing those output heads, and this simple mitigation approach can blunt the proposed recovery, but this is obviously not considered as an \"ideal\" unlearning approach as it doesn’t actually remove any knowledge. If such simple defenses evade your detector, it may suggest the auditing approach is incomplete for practical deployments.\n\n- The theoretical analysis includes heuristic steps (e.g., near-uniform per-class MI share, some conditioning inconsistencies), so the results read as intuition-building rather than tight guarantees."}, "questions": {"value": "Could you assess robustness under common output post-processing (e.g., temperature scaling/calibration, label smoothing, ensembling, etc)? It would help to know when recovery still works and when your detector can flag cases where outputs have been deliberately sanitized."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "31x07UVpPS", "forum": "DqvnwRe1V2", "replyto": "DqvnwRe1V2", "signatures": ["ICLR.cc/2026/Conference/Submission1374/Reviewer_j3BQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1374/Reviewer_j3BQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829927061, "cdate": 1761829927061, "tmdate": 1762915751864, "mdate": 1762915751864, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper discusses that most unlearning algorithm has the “illusion” of unlearning, while the models’ outputs still contain residual information about the forget-set class, which can be recovered by simple methods. The paper shows theoretical insights about first) the fact that there is an unavoidable tension between forgetting and the model’s utility on the retained classes, hence, making complete forgetting almost impossible, second) that unlearning algorithms leave statistical signatures in the models’ output space, and show bounds on residual information in these signatures. The paper also introduces a recovery attack that involves several steps, including identifying the forgotten class using the average probability and variance on the test dataset, and transforming probability values. Experiments on 12 unlearning methods for class unlearning tasks show that forgotten-class predictions can be recovered significantly."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**S1**.  The paper studies an important problem. Understanding where and how unlearning methods fail is crucial from many aspects. \n\n**S2**. The paper proposes a novel algorithm that is very intuitive and has been shown to work well for class unlearning. \n\n**S3**. The experiments include examining 12 unlearning algorithms."}, "weaknesses": {"value": "**W1**. Given the scope of the paper, it would be more suitable to emphasize “class unlearning” in the title.  In general, while insightful, the paper has limited applicability as it only addresses class unlearning. \n\n**W2**. The paper does not discuss defense mechanisms. The recovery method uses output probabilities, which are sometimes inaccessible or quantized in real deployments (e.g., APIs returning top-1 labels). The paper does not discuss to what extent the phenomenon persists under restricted output access (e.g., rounded logits, differential privacy noise).\n\n**W3**. Although the paper critiques current evaluation paradigms, it still primarily reports accuracy itself. Further analyses, such as calibration error, mutual-information estimates, or representation similarity, can add some angles to the results. \n\n**W4**. The failure modes you identify are very interesting, but can be presented better. For example, pointing to the results/experiments where these can be spotted. \n\n**W5**. The two-component design (Yeo–Johnson + adaptive thresholding) is intuitive, but there is little analysis isolating their individual contributions. A simple linear scaling or power transform baseline could test whether Yeo–Johnson is truly essential."}, "questions": {"value": "**Q1**. Is the theoretical lower bound measurable in practice? Have you estimated Ires empirically?\n\n**Q2**. Can the authors envision any defenses that destroy these residual traces while maintaining utility?\n\n**Q3**. Can you clarify the practical relevance of Theorem 2? The recovery framework does not seem to use trajectory analysis. Does this theorem have implications for the recovery task that"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dePijLRzhZ", "forum": "DqvnwRe1V2", "replyto": "DqvnwRe1V2", "signatures": ["ICLR.cc/2026/Conference/Submission1374/Reviewer_huCs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1374/Reviewer_huCs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911828331, "cdate": 1761911828331, "tmdate": 1762915751380, "mdate": 1762915751380, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discuss the vulnerability of current machine unlearning methods, whose prediction on the forget data could be recovered by post-processing the predicted logits. This paper covers the class-unlearning case of machine unlearning and the proposed method could recover the residual information from the unlearned model to re-recognize the forget class."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper points out an important issue in machine unlearning, simply relying on the forget accuracy is not enough.\n\n2. The post-process method successfully recovers the accuracy on the forget class, which reveals that there is a way to probe the residual knowledge in the unlearned model."}, "weaknesses": {"value": "1. It seems that the experiments only focus class unlearning, but no experiments on sample unlearning, which means that only a part of samples within a class is forget. As in general, the unlearning could be request at individual level. \n\n2. About theorem 1, the condition is alpha_r > alpha_0; however, in Table 1, most of unlearning methods can not maintain better accuracy on remaining classes than the original model. Therefore, the condition is rarely hold. That is, what is the bound for residual information in most of cases?"}, "questions": {"value": "1. It is unclear to me why the authors show OOD results and what does Ours mean there? Is it because the proposed method could be considered as to detect the OOD classes? More elaboration is needed for better clarity. \n\n2. In figure 3b, it shows that even with small set of test set, the proposed method works; however, I am more interesting to know how many classes of samples the proposed method need? As in practices, the user might not know how many classes are trained by the service provider (model), how do the users provide enough samples to justify whether or not the service provider has residual knowledge of the user self?\n\n3. how does kappa in eq 8 affect the identification accuracy?\n4. How will those regularization methods, like mixup, cutmix, label smoothing, affect the recovery? As the original probability distribution will be more smooth.\n \n5. Since theorem 1 and 3 are unrelated to the unlearned performance over forget set, is there any way to assess the quality of unlearn method purely based on the performance after unlearning?\n\n6. At line 114, isn't the standard is to be closer to the retrained model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QHvUB9ZFNX", "forum": "DqvnwRe1V2", "replyto": "DqvnwRe1V2", "signatures": ["ICLR.cc/2026/Conference/Submission1374/Reviewer_4AUq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1374/Reviewer_4AUq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1374/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957006609, "cdate": 1761957006609, "tmdate": 1762915751002, "mdate": 1762915751002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}