{"id": "3kouij8BWi", "number": 3372, "cdate": 1757412326043, "mdate": 1759898093339, "content": {"title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models", "abstract": "Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5\\% to 82.8\\% under the $2 \\times 2$ setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1\\%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. Code and datasets will be released soon.", "tldr": "", "keywords": ["VLMs", "Visual Understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/62855f4eea6c0372fa400f447a8715fb6a39aaa4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the problem that current VLMs struggle with. For basic perceptual and reasoning tasks like 2x2 jigsaw puzzles, they perform randomly. To fix this, the authors created AGILE, a framework that turns jigsaw solving into an interactive process. The model generates code to do actions like swapping pieces, cropping, or zooming in, and gets real-time visual feedback from the environment to keep improving. The model is trained with high-quality cold-start trajectories and jigsaw images, and the results are pretty impressive: the model’s accuracy on 2x2 jigsaws jumped from 9.5% to 82.8%, and it also got 3.1% better on average across 9 general vision tasks, showing it can generalize well beyond just solving puzzles."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors use a clever way to solve the data scarcity issue for multimodal reinforcement learning: code-based data generation for scalable data curation.\n\n2. Interactive training design is impressive: by making the model actively engage with the jigsaw environment step by step, it builds real perceptual and reasoning skills instead of just memorizing patterns. \n\n3. General improvement on different visual tasks validates the effectiveness and generalization of the proposed method."}, "weaknesses": {"value": "1. The author did not mentioned comparisons with direct apply GRPO or other RL training algorithms. As RL can improve the performance of VLMs, I would like to know the real contribution of jigsaw training. Current improvement is combined with RL training.\n\n2. The authors should provide more details about their experiments regarding the comparsion with general QAs. What does general QA dataset consist of? Also, I would like to know the result if I use high resolution general QA dataset to train the model. How it will perform? This will help others validate the contribution of using jigsaw for training more clear.\n\n3. I think the auhors should provide more on the jigsaw bench. What if giving each patch a unique ID and let the model to sort the patch to the correct order? Will it improve the overall performance. I wonder if the code excecution is the best way of doing this? How about using ID and sort them, and then use this to train the model. VLMs are not good at coding, which will make them harder to solve problems in such settings. More details and comparisons are needed. \n\n4. Cold-Start only use 1.6K data and RL using 16K data, what if training the model longer during SFT? will the improvements become fewer? Comparisons are needed regarding the scaling of training data."}, "questions": {"value": "As stated in weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UD8ETY5ixz", "forum": "3kouij8BWi", "replyto": "3kouij8BWi", "signatures": ["ICLR.cc/2026/Conference/Submission3372/Reviewer_yrdc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3372/Reviewer_yrdc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910967013, "cdate": 1761910967013, "tmdate": 1762916693549, "mdate": 1762916693549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AGILE, an agentic jigsaw interaction learning framework to enhance visual perception and reasoning in VLMs. It formulates jigsaw solving as an interactive environment where the model generates executable Python actions (Swap, Observe, Crop, Zoom), receives fine-grained visual feedback, and iteratively improves through GRPO-based RL. A cold-start stage uses 1.6K expert trajectories (filtered Gemini 2.5 Pro outputs) to teach instruction-following and code generation; RL is then performed on 15.6K programmatically generated 2×2 jigsaw tasks with controllable difficulty and verifiable rewards (accuracy, format, step). Results show large gains on jigsaw puzzles (2×2 Acc: 9.5%→82.8%; Score: 29.4%→89.0%) and nontrivial generalization across 9 vision benchmarks (+3.1% avg), with especially strong improvements on high-resolution understanding. The paper analyzes data scaling, compares jigsaw vs general QA data under a fixed budget, and includes case studies of emergent behaviors."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear problem framing: shows that current VLMs perform near random on simple jigsaw tasks, motivating a proxy task for fundamental perception and reasoning.\n\nInteractive, verifiable RL: uses executable code and a rule-based environment, enabling precise, fine-grained feedback and scalable supervision without expensive annotation.\n\nStrong empirical gains on target task: very large improvements on 2×2 and meaningful generalization to 3×3 despite training on 2×2.\n\nCross-task generalization: consistent improvements (+3.1% avg) across diverse downstream benchmarks (HR, real-world, fine-grained, reasoning, hallucination).\n\nInsightful analyses: comparison with General QA data at equal budget; case studies illustrate learned strategies (edge continuity, text alignment, semantic consistency)."}, "weaknesses": {"value": "**Limited task significance and generality**\n\nFixed jigsaw formulation: The current agent–environment interaction is confined to regular grid jigsaws (uniform tiling, no occlusion, no non-rigid deformation, no noise). Such strong scene priors make it hard to cover the diverse layouts and transformations in open-world vision (e.g., free-form crops, affine/perspective distortions, occlusions, and layered structures), limiting the method’s external validity for general visual reasoning.\n\n2×2 performance saturation: Training and reward design are primarily optimized for the 2×2 setting and are near the ceiling (high Acc/Score). There is limited evidence of sustained gains on more complex structures. Overly strong 2×2 metrics may mask real capability gaps on harder tasks.\n\nWeak size generalization: While there is some transfer to 3×3, the accuracy remains low (20.8%). Moreover, larger grids (3×3, 4×4) are not directly trained in the RL stage. The absence of cross-size curricula or memory mechanisms leads to insufficient generalization from small to larger scales.\n\nFixed, non-general action space: The toolset of Swap/Observe/Crop/Zoom is highly tailored to jigsaws and does not seamlessly transfer to general multimodal interactions (e.g., instance-level operations, relation editing, cross-image search). There is a lack of validation and ablation on extensible action semantics (parameterized operations, composable tools).\n\n\n**Uncertain sources of gains on general tasks**\n\nImprovements may stem more from distillation than from the jigsaw task: The cold-start phase relies on Gemini 2.5 Pro–generated and filtered expert trajectories, which may impart strong priors to the base model in instruction-following, code formatting, and visual alignment. Current controls are insufficient to disentangle the “expert distillation effect” (SFT-driven general gains) from the independent contribution of “jigsaw agentic interaction + RL.” Suggested additions:\nRL-only (without expert trajectories) or weak-expert/random-teacher controls;\nControls with distillation from general multimodal tasks at equal scale;\nAblations removing operations tightly tied to expert demonstrations (e.g., Crop/Zoom) to see whether downstream gains persist.\n\nCoupling risk in rewards/training signals: Format rewards and step penalties may encourage “normalized outputs and low-step strategies” rather than genuine perception–reasoning improvements. It is necessary to verify whether the gains reflect core abilities rather than policy biases through analyses of failure types (code errors vs. cognitive errors) and cross-task robustness under occlusion, distortion, and noise."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SmHUPRcj0T", "forum": "3kouij8BWi", "replyto": "3kouij8BWi", "signatures": ["ICLR.cc/2026/Conference/Submission3372/Reviewer_es5M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3372/Reviewer_es5M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986516626, "cdate": 1761986516626, "tmdate": 1762916693042, "mdate": 1762916693042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AGILE, a novel framework that enhances the visual perception and reasoning capabilities of Vision-Language Models through interactive jigsaw puzzle solving. AGILE formulates jigsaw solving as an iterative interaction between the model and its environment, allowing the model to progressively refine its perceptual and reasoning skills through exploration and feedback. The framework employs a scalable data generation pipeline to construct high-quality multimodal reinforcement learning datasets with controllable difficulty, effectively addressing the scarcity of such data. Experimental results show that AGILE significantly improves jigsaw-solving performance and generalizes well across diverse vision tasks, demonstrating its strong potential for advancing the development of VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Framework: The proposed AGILE framework creatively formulates jigsaw solving as an interactive process to enhance VLMs’ perceptual and reasoning abilities. This integration of interactive learning with jigsaw-based perception is highly novel and opens up a promising research direction for improving the core competencies of VLMs, particularly within self-supervised learning paradigms.\n2. Significant Performance Gains: Experimental results show that models trained with AGILE achieve substantial improvements on jigsaw tasks and exhibit strong generalization across various vision benchmarks, clearly demonstrating the framework’s effectiveness in boosting perception and reasoning.\n3. Data Generation Advantage: The proposed scalable data generation method is of notable value, as it effectively addresses the data scarcity challenge in multimodal reinforcement learning by producing high-quality, controllable training datasets.\n4. The paper is clearly written, logically organized, and easy to follow."}, "weaknesses": {"value": "1. It remains unclear whether AGILE adopts a multi-turn or single-turn evaluation setup for general downstream tasks. The paper only mentions that both AGILE and the baselines use VLMEvalKit as the evaluation framework, but provides no further details about the evaluation protocol. If AGILE employs multi-turn interactions while the baselines use single-turn ones, this may raise concerns about the fairness of the comparison.\n2. The motivation behind the Step Reward design is not well explained. It would be helpful to understand how removing this component affects training dynamics, especially its impact on general downstream vision tasks. Including ablation experiments on this aspect would strengthen the paper’s methodological soundness and justification.\n3. Since AGILE is based on multi-turn interactive reinforcement learning, it likely introduces additional training overhead compared with single-turn RL. The authors are encouraged to discuss the computational cost and efficiency trade-offs. A comparison between single-turn and multi-turn settings in terms of both performance and training cost would make the paper more comprehensive."}, "questions": {"value": "1. Could the authors clarify whether AGILE uses a multi-turn or single-turn setting during evaluation on general downstream tasks? If multi-turn interactions are involved, how is fairness ensured when comparing with single-turn baselines?\n2. What is the underlying motivation for introducing the Step Reward design? Could the authors elaborate on how this component influences the training dynamics and model performance? It would be valuable to include an ablation study to show the effect of removing this reward, particularly on general vision tasks.\n3. Since AGILE involves multi-turn reinforcement learning, could the authors provide a quantitative or qualitative analysis of the additional computational cost introduced by this setup? A comparison between single-turn and multi-turn RL in terms of performance–cost trade-offs would help clarify the practical implications of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u0kKyT6PwN", "forum": "3kouij8BWi", "replyto": "3kouij8BWi", "signatures": ["ICLR.cc/2026/Conference/Submission3372/Reviewer_ciSx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3372/Reviewer_ciSx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013158402, "cdate": 1762013158402, "tmdate": 1762916692619, "mdate": 1762916692619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AGILE (Agentic Jigsaw Interaction Learning), a framework to improve visual perception and reasoning in large Vision-Language Models (VLMs). The authors propose a RL based paradigm where the model progressively solves visual jigsaw puzzles. AGILE formulates the puzzle as interaction between the model and an external environment, with the model generating executable Python code to perform actions such as swapping, cropping, or zooming image tiles. Through iterative observation and feedback, the model develops stronger reasoning and perception capabilities. The framework includes a cold-start phase, leveraging expert trajectories collected via Gemini 2.5 Pro, followed by a reinforcement phase using Group Relative Policy Optimization (GRPO). Extensive experiments demonstrate accuracy improvement on the trained task from 9.5% to 82.8% on 2×2 puzzles. The trained model also demonstrates generalization gains (average +3.1%) across nine diverse vision benchmarks. By constructing scalable, controllable jigsaw datasets without reliance on costly human annotation, AGILE provides a scalable solution to the scarcity of high-quality multimodal RL data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s key strength lies in using jigsaw solving as an interactive proxy task for improving visual reasoning. Because of the rule-based data generation, this method prevents the need of annotations in a scalable manner. This reduces dataset scarcity in multimodal reinforcement learning. This design also enables control over task difficulty. \n\nBy integrating agentic behavior, code generation, and environmental feedback, the authors create a setting that closely simulates exploratory problem-solving. This becomes an essential step toward autonomous multimodal reasoning. \n\nAlthough some gains on the trained jigsaw task are expected over the base model, using AGILE achieves a jump from 9.5% to 82.8%. The model trained using AGILE also achieves gains on the general vision tasks.\n\nExperiments on the RL training data scale also indicate the importance of the RL step in improving model performance. Further experiments on using a mix of General QA and Jigsaw data over only General QA data to improve the performance indicating the importance of the jigsaw data."}, "weaknesses": {"value": "- While the authors report an average 3.1% generalization improvements, it is not clear whether these improvements are mere metric increases, or because of an improvement in visual perception and reasoning. While the authors provide a small case study on the jigsaw task, qualitative analysis on the general benchmarks would be useful to quantify the gains.\n- The reliance on python code generation for performing actions can limit the model capabilities for further generalization to unstructured tasks. The authors do not mention/explore the use of more traditional tool calling methods to directly call environment APIs instead of generating python code to execute the APIs."}, "questions": {"value": "- Since the model generates Python code to perform jigsaw actions, how are code generation errors handled during RL training? Are invalid executions penalized, or filtered out before reward calculation?\n- The paper defines three components (accuracy, format, and step rewards). Could the authors clarify how sensitive overall performance is to the weighting coefficients (α, β, γ)? Have ablation tests been performed to determine the optimal balance among these terms?\n- The model shows improvement across nine benchmarks. Could the authors elaborate on which specific reasoning categories (e.g., fine-grained recognition vs. scene reasoning) benefit most from AGILE’s training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "f9oZiTqCGj", "forum": "3kouij8BWi", "replyto": "3kouij8BWi", "signatures": ["ICLR.cc/2026/Conference/Submission3372/Reviewer_Jwkt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3372/Reviewer_Jwkt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762182696817, "cdate": 1762182696817, "tmdate": 1762916692383, "mdate": 1762916692383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}