{"id": "8UdCE5nhFl", "number": 13119, "cdate": 1758213791291, "mdate": 1759897463152, "content": {"title": "P3D: Highly Scalable 3D Neural Surrogates for Physics Simulations with Global Context", "abstract": "We present a scalable framework for learning deterministic and probabilistic neural surrogates for high-resolution 3D physics simulations. We introduce P3D, a hybrid CNN-Transformer backbone architecture targeted for 3D physics simulations, which significantly outperforms existing architectures in terms of speed and accuracy. Our proposed network can be pretrained on small patches of the simulation domain, which can be fused to obtain a global solution, optionally guided via a scalable sequence-to-sequence model to include long-range dependencies. This setup allows for training large-scale models with reduced memory and compute requirements for high-resolution datasets. We evaluate our backbone architecture against a large set of baseline methods with the objective to simultaneously learn the dynamics of 14 different types of PDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic turbulence with spatial resolutions of up to $512^3$. Finally, we show the versatility of our architecture by training it as a diffusion model to produce probabilistic samples of highly turbulent 3D channel flows across varying Reynolds numbers, accurately capturing the underlying flow statistics.", "tldr": "We present a scalable framework for learning neural surrogates for high-resolution 3D physics simulations", "keywords": ["neural surrogates", "physics simulations", "transformers", "3D"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a23389ff14064e5adce16de9f6ed10ffb157b55.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a network architecture and strategy for training on high resolution 3D data. The overall approach uses entirely translation equivariant blocks in the encoder and decoder to allow the model to take full advantage of cropped training. The authors then propose multiple approaches for scaling these models to high resolution 3D data. They experimentally evaluate their architecture on a multi-task learning problem, on a high resolution autoregressive task, and on a generation problem."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Overall this is a very well written paper. It makes the problem clear and proposes a solution along with varying approaches to implement that solution.\n2. Apart from the one note below, the experiments are well designed and well suited to showing the scaling performance of the proposed method. \n3. This is an enormous challenge in the field and this work is showing promising results on some extremely difficult tasks."}, "weaknesses": {"value": "Overall, this is a strong paper that makes important progress on a challenging problem in the space. Right now there are a few issues that I'd consider to be large enough that I can't mark it as a clear accept, but with some minor changes, I'd be happy to increase my score. I've included comments on how these issues could be addressed experimentally which would lead to a larger increase, but to be clear, my primary asks to recommend acceptance are the inclusion of rollout metrics and several adjustments to claims and presentation.\n\nMajor:\n\n1. 4.1 is definitely a weak spot in the paper.  The rest of the experiments are strong enough that I'm not marking down too far for this, but either experiments or claims could be adjusted to address several issues:\n    1. The biggest issue is that that the claim of 14+ PDEs does not appear to be true. It's possible I'm missing something, but Appendix B lists 7 PDEs. It's not reasonable to consider Gray-Scott multiple PDEs due to the varying macroscale behavior.\n   2. Second, there is limited evidence of performance as a \"foundation model\". While previous works like Poseidon have shown the conv+swin backbone is a strong basis for foundation models, the experimental evidence provided is insufficient to make that argument. These are largely fairly simple systems defined on periodic cubes, so this is a case where the model has a strong inductive bias and then is only evaluated for multi-task learning on problems that benefit from that inductive bias. To make this claim, I'd recommend including varying boundary conditions or geometric forcings or just generally more real-world inspired data. Otherwise, I'd recommend just being more explicit on what the limitations of the experiment are. \n2. The difference in metrics between table 3 and 4 also makes it difficult to evaluate whether the results in table 4 are \"good\" beyond the accomplishment of running a model at all on such high resolution data. While it doesn't make sense to ask for ML baselines, I think simple baselines like persistence and predicting the field average would help put the results in context. Additionally, I think evaluating step 1 on the same distribution in both spaces would allow for some comparison between the models.\n3. Generally speaking, the paper's claim of stable autoregressive rollouts does not seem well supported.  Metrics are provided for 1 and 15 steps in 4.2. Some evaluation of rollout on the scaled up model is necessary to make claims that the approach can make stable predictions on high resolution. \n4. The paper could engage more with the weaknesses of the method. For instance, many of the architectural choices are not compatible with non-uniform grids so there isn't a natural extension to many realistic problems. \n\nMinor:\n1. For data generated for a specific project, there should really be more precise information about the generating conditions in the appendix, specifically precise mathematical descriptions for each task. \n2. The context network is introduced, but it's not clear if its being used outside of 4.3. Is it being used? Would it have helped with the independent patch model <128|128> in 4.2? Similarly, there's not much comparison of the training approaches in Figure 4. \n3. This might be a personal issue, so I'm not marking down for this either, but it's a bit weird to use llama library names for standard network components, particularly given this is not a NLP submission. It would be more informative to say the standard names for the model components, then mention you use the llama implementations. \n4. Appendix C.1. links to table 4 - it seems to be an extended version of table 3 based on the caption and numbers."}, "questions": {"value": "1. Is the \"conv\" model in the ablations in 4.1 replacing the attention layers with more convolutional blocks or is it just removing components? \n2. Could the context network have eliminated the boundary artifacts in 4.2 for <128|128>? \n3. How does this approach compare to baselines in the presence of boundary conditions or geometric influence?\n4. How does the model perform over longer autoregressive rollouts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UncIbg9Hut", "forum": "8UdCE5nhFl", "replyto": "8UdCE5nhFl", "signatures": ["ICLR.cc/2026/Conference/Submission13119/Reviewer_HKuH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13119/Reviewer_HKuH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757253878, "cdate": 1761757253878, "tmdate": 1762923844555, "mdate": 1762923844555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a new architecture that uses convolution and attention modules in an encoder-decoder UNet structure and combine it with a latent global attention module that is used to exchange global information and inject region specific information into the decoder.\nThey evaluate various training and scaling strategies focusing on three setups: jointly learning multiple PDEs, pretraining on smaller patches and scaling to larger patches at inference, and a probabilistic training scenario.\nAcross all scenarios P3D generally outperforms the compared baselines, showing superior accuracy and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "## Originality\n- The paper introduces a novel combination of architectural components essential for scaling to larger grids and systematically analyzes their contributions through ablations.\n- The learned region tokens introduced in the bottleneck and the specific conditioning of the decoder regions is, to the best of my knowledge, a novel architectural design that, according to the presented results, helps to propagate global information.\n\n## Quality\n- Experiments are well designed and a rigorous description of the PDE setups as well as the architectural design and training hyperparameters are provided in the Appendix.\n- The authors benchmark against multiple strong baselines and report consistent improvements in accuracy as well as efficiency.\n\n## Clarity\n- The paper’s organization is clear and logical, moving from background and motivation to contributions, ablations and results, which makes the narrative easy to follow.\n- Main concepts are well described and most additional information that is crucial for understanding is provided in the Appendix.\n\n## Significance\n- Very interesting paper for the neural surrogate community, where scaling is essential to move towards real world problems.\n- The authors outline design choices and findings (e.g.: domain-crop pretraining or region token based conditioning) that can be integrated into other methofs. Elements of this paper can therefore can serve as building blocks for future work."}, "weaknesses": {"value": "1. Lack of uncertainty/variance reporting for the first two experiment setups. While I understand this is computationally expensive, I think it would be an important addition if the authors could at least include std across a few seeds in the main tables where they compare against other architectures.\n\n2. The paper only reports 1-step errors for the mulit-PDE setting in Section 4.1: Including rollout errors or spectral metrics (as done in Section 4.2) would provide deeper insights.\n\n3. Minor inconsistencies:\n    - In Appendix A.5, the latent token dimensions notation is slightly confusing: (B, C, H, W, D) -> (N, (H × W × D), C). Why do you use B for batch and then N?\n    - Section 4.1 mentions a maximum number of 3 channels. However Table B1 lists a maximum of two fields per PDE. Clarification would be helpful."}, "questions": {"value": "1. How is data normalization handled? I think this can be a bit nuanced in the mulit-PDE setting: could be normalised per channel per PDE, or per channel across all PDEs with the same channels, or different strategy?\n\n2. How does the P3D-shift ablation perform without the region tokens? It would be interesting to see if the information flow across windows is enough or this global information injection in the decoder is essential.\n\n3. I think an important downstream task of such scalable architectures is finetuning on an unseen PDE: It would be interesting to have experiments that show what advantage pretraining can bring if we e.g. have a new PDE that is extremely expensive to simulate and we can only afford a few samples. Is there positive transfer?\n\n4. Could you provide a breakdown of how the presented P3D model ablations (e.g. Table 2) compare with respect to their efficiency (Params, GFLOPs, Memory, Throughput)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MikcSINZYm", "forum": "8UdCE5nhFl", "replyto": "8UdCE5nhFl", "signatures": ["ICLR.cc/2026/Conference/Submission13119/Reviewer_rjE6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13119/Reviewer_rjE6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850841673, "cdate": 1761850841673, "tmdate": 1762923844254, "mdate": 1762923844254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a hybrid 3D CNN–Transformer surrogate that learns on small crops and fuses them with a lightweight global context sequence model (via adaln) to capture long-range dependencies while keeping memory/computation manageable. It scales to high-resolution 3D flows, and can also be trained probabilistically (using flow-matching) to generate channel-flow fields with faithful statistics across Reynolds numbers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. An innovative domain-decomposition neural surrogates with an effective and efficient module to fuse in global context, which is a meaningful step towards large-scale PDE modelling\n\n2. The empirical performance of the model is quite strong. After training on local crop it can be deployed to larger domain.\n\n3. Clear ablations isolate the global-context module as the source of gains over crop-only baselines."}, "weaknesses": {"value": "The paper’s design and \"why it works” intuition are not fully clear to me\n\n1. Decoder/attention choice. The text says a Transformer decoder with “LLMAttention.” Does this mean causal attention over latent/region tokens? If so, why prefer causal over bi-directional for spatial/latent context aggregation (is it solely because of saving up computeduring fine-tuning)? A more in-depth discussion (e.g., compute/memory in backprop, training stability) would clarify the design choice.\n\n2. Region token vs. simpler conditioning. For the region token, did the authors try a simpler variant: feeding a region index/ID embedding (analogous to a timestep/position embedding) directly into the diffusion model via adaln instead of introducing a learnable token?"}, "questions": {"value": "On patch artifacts in Fig. D9. I still notice some patch artifacts. Could this stem from padding choices in the conv layer (e.g., zero vs. reflect vs. circular)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2rYWsu7mrw", "forum": "8UdCE5nhFl", "replyto": "8UdCE5nhFl", "signatures": ["ICLR.cc/2026/Conference/Submission13119/Reviewer_EzRu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13119/Reviewer_EzRu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897110080, "cdate": 1761897110080, "tmdate": 1762923843930, "mdate": 1762923843930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is a solid engineering effort with many moving parts: a UNet encoder–decoder, local attention, a small global context stack, a region‑token MLP conditioning, and a probabilistic training scheme. I am slightly concerned that the paper is an incremental 3D variant of the earlier \"PDE-Transformer\" approach which the authors cite as a predecessor. I think this is an interesting paper and the insights can prove useful to the community once disentangled. However, I think many of the architectural components are not well-justified by the experiments (particularly those choices which have fallen out of practice in the field), and the current evaluation has a few asymmetries that make the improvements over baselines harder to trust. I appreciate the amount of work that went in, but I would like to see some improvements to these points before acceptance, so that the paper and experiments can present insights in a more effective way to the community. I have compiled some notes below to help you strengthen the justification for your claims and make the evaluation fairer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Comprehensive set of experimental datasets and baselines across different physical regimes.\n* Reasonable scaling argument via crop training and stitched inference.\n* Dataset visualisations look quite polished.\n* The context finetuning idea for channel flow is interesting when wall information is missing."}, "weaknesses": {"value": "Major:\n\n1. Novelty\n   1. The paper reads as a 3D variant of prior \"PDE-Transformer\" work. It is a bit challenging to identify which architectural modification is the major contribution to the performance improvements, as this is something I would be quite interested in reading about. I recommend you isolate which component it is, and demonstrate effect sizes. Right now there are a lot of moving parts with insufficient ablation support.\n   2. LLMLayer and \"hyper attention\" are mentioned but seemingly never defined despite being part of the \"core architectural blocks\". The only text I found on \"hyper attention\" in the paper is \"We use hyper attention , this is a specialized attention variant designed for efficient long-sequence processing\". Please spell out exactly what these are (I'm assuming these are from another paper you could simply cite), including complexity and memory scaling, or simply use a standard transformer block.\n\n2. Baselines\n   1. The crop axis of the evaluation feels hard to trust. P3D‑L at 32^3 beats Swin3D at 128^3 on the multi‑PDE suite. To me, this suggests under‑tuned baselines or other asymmetries in how the evaluation is being performed. It is perhaps also an issue with this type of evaluation, done by masking the data in a way that is suitable for the presented model. In addition, there is a fixed learning rate and weight regularization across all models, rather than using the reported training settings presented in each baseline's paper - which likely vary based on architecture settings. I would recommend at the very least using the same training settings as presented in baseline papers, or, even better, doing modest hyperparameter sweeps for the strongest baselines. Otherwise, baseline comparisons may lose their scientific value.\n   2. Comparing speed against an accurate DNS is misleading. Comparisons need to be done either (1) on accuracy, with fixed compute budget for all models (likely resulting in using an LES, not a DNS), or, (2) on speed, at a fixed accuracy target, which is likely to result in using a coarser LES at comparable accuracy to your model. See panel A of Figure 1 of https://www.pnas.org/doi/10.1073/pnas.2101784118 for an example of what this should look like. At the moment, you are comparing diagonally in this accuracy-runtime plot, so it is impossible to draw meaningful conclusions. (Nevertheless, I commend the authors for running the DNS on the GPU, as doing a CPU comparison is the other common mistake.)\n   3. For channel flow, from what I can tell, your model is conditioned on Re, but the baselines are not. Either condition baselines or train per‑Re baselines. In general I do not consider providing a model with an additional scalar context to be a novel contribution worthy of highlighting in a comparison. Also, the grid is non‑uniform in the wall‑normal direction; please explain how convolutions and positional information handle this and ensure baselines get the same treatment (assuming this is something you are controlling for).\n   4. In general, I would like to see some spectra for the tasks. It is hard to know if your model is doing well by staring at thumbnail images of the density fields.\n   5. The EMA reporting was a bit confusing to me. Please state which tables use EMA weights.\n\n3. Ablations\n   1. In general, there are many moving parts in this model. Any additional complexity needs to be justified with quantitative evidence, which I do not see as strong enough at the moment.\n   2. Region‑token conditioning: please cite the original AdaIN/FiLM papers (I think AdaIN is mentioned by name, but not cited); please also clarify novelty. It would also be important to quantify effect size on at least two datasets.\n   3. Context token density: each 16^3 crop becomes one latent token in the context model, so it would be useful to show sensitivity to token density (8^3/16^3/32^3) and where accuracy breaks.\n   4. This is a point I am not confident about so please correct me if I am wrong. But I am confused about the positional encoding; the paper states \"P3D does not include any absolute positional embeddings,\" yet the context model injects 3D sine–cosine PEs. But, since sine-cosine PEs are absolute, the global context actually seems to have full positional information. For relative positional encodings you would need to use RoPE.\n   5. Regarding this \"Inactive gradients\" idea, this seems like another complex idea, which is also not completely necessary for the model itself, introduced in the paper without sufficient quantitative justification. I think this would not only result in a biased gradient estimator to save some VRAM (which does not seem to be theoretically explored), but would also introduce extra stochasticity into the training process. Why not use more standard approaches like gradient checkpointing or, e.g., LayerDrop?\n   6. PixelShuffle3D: would be helpful to compare to more standard approaches to see how much this helps. It might be interesting for other contexts but at the moment it is hard to know whether this actually helps.\n   7. The splits vary by time and by simulation index s which is a bit hard to follow. It might be nice to document the splits in a small table.\n\n\nMinor:\n\n1. In general, I felt like there was too much emphasis on the datasets and not enough emphasis on the model. These datasets are indeed visually nice to look at, but this paper is about the model. Only if the figures are part of the story should they be in the main text of the paper. For example, Figure 7, despite the nice 3D renders of this fluid, is completely unnecessary. I would include some spectra there instead so I can actually see how well the model is doing. Or other analysis on the model itself. Not to mention there is no reference flow for this figure, because it is crammed in to include the 3D render.\n2. I felt like some of the math notation was a bit performative. For example, there was this $u(x, t)$ function defined with set theory notation, but then it was immediately dropped, even in the context of defining the $\\mathbf{u}$ fields (it is not concrete how these two are related). If you need this to define the fields, then do it, but otherwise I would drop it.\n3. The appendix feels too long and low-density in terms of information, it is a lot of the same figure generated for different datasets, many of which there is no discernible pattern at all that the reader should be following. The PDF is 40MB as a result of all of the figures. The dataset does not need to be a literal part of the PDF itself. The reader does not need to see all 9 timesteps, sliced through every axis, for a dataset that was (I think) not even introduced first by this paper. \n4. For filtering choices, a Hanning window is applied before some vorticity visuals. Please state clearly whether spectra are computed on filtered or raw fields and show both.\n5. \"weight regularization\" -> \"weight decay\"."}, "questions": {"value": "[Included in Weaknesses]"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G9ishL75IB", "forum": "8UdCE5nhFl", "replyto": "8UdCE5nhFl", "signatures": ["ICLR.cc/2026/Conference/Submission13119/Reviewer_1qCk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13119/Reviewer_1qCk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050931598, "cdate": 1762050931598, "tmdate": 1762923843334, "mdate": 1762923843334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}