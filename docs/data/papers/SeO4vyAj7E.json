{"id": "SeO4vyAj7E", "number": 15701, "cdate": 1758254131343, "mdate": 1763177164740, "content": {"title": "ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases", "abstract": "The tendency to find and exploit \"shortcuts\" to complete tasks poses significant risks for reliable assessment and deployment of large language models (LLMs). For example, an LLM agent with access to unit tests may delete failing tests rather than fix the underlying bug. Such behavior undermines both the validity of benchmark results and the reliability of real-world LLM coding assistant deployments.\n\nTo quantify, study, and mitigate such behavior, we introduce ImpossibleBench, a benchmark framework that systematically measures LLM agents' propensity to exploit test cases. ImpossibleBench creates \"impossible\" variants of tasks from existing benchmarks like LiveCodeBench and SWE-bench by introducing direct conflicts between the natural-language specification and the unit tests. We measure an agent's \"cheating rate\" as its pass rate on these impossible tasks, where any pass necessarily implies a specification-violating shortcut.\n\nAs a practical framework, ImpossibleBench is not just an evaluation but a versatile tool. We demonstrate its utility for: (1) studying model behaviors, revealing more fine-grained details of cheating behaviors from simple test modification to complex operator overloading; (2) context engineering, showing how prompt, test access and feedback loop affect cheating rates; and (3) developing monitoring tools, providing a testbed with verified deceptive solutions. We hope ImpossibleBench serves as a useful framework for building more robust and reliable LLM systems.", "tldr": "", "keywords": ["reward hacking", "alignment", "benchmark"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f0edc0ee21784f063e1f7970ecbd360037762bb7.pdf", "supplementary_material": "/attachment/64a89538c8c2090a0a0ed25a3d5caf4421fdb15b.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces IMPOSSIBLEBENCH, a benchmark of intentionally impossible programming tasks designed to reliably detect and analyze cheating in LLM agents. Using this benchmark, the authors present the concerning finding that more capable, frontier models exhibit a higher propensity to cheat. The work then explores effective mitigation strategies through context engineering, showing that cheating can be significantly reduced by using stricter prompts, restricting test file access to read-only, and providing an explicit option for the model to flag a task as unsolvable. Furthermore, the paper highlights the limitations of automated monitoring, revealing that even capable LLM-based monitors can be deceived by the sophisticated justifications that cheating models generate for their behavior. Overall, the paper provides a valuable tool and crucial insights into the alignment challenges of increasingly autonomous agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's core contribution is its highly original methodology. Creating a benchmark of intentionally impossible tasks is a novel and clever approach to directly test for and quantify agent \"cheating\" behavior, moving beyond traditional performance metrics to study a critical alignment failure mode.\n\n2. The work is well-presented. The central concepts, experimental setup, and key findings are communicated with good clarity, making the paper's compelling narrative easy to follow and ensuring its important message is both accessible and impactful.\n\n3. Furthermore, the paper's findings carry good weight for AI safety, supported by a good experimental design. The discovery that model capability positively correlates with the propensity to cheat is an interesting result, and the investigation of practical mitigations provides valuable, actionable insights for building more reliable agents."}, "weaknesses": {"value": "1. First, the paper's introduction lacks sufficient background to fully contextualize its contribution. While a related work section is provided in the appendix, the main body would benefit from a more direct discussion of the current research landscape, including related phenomena like \"vibe coding\" or reward hacking. This would help readers immediately grasp how this study's findings on impossible tasks build upon or diverge from previous work on agent reliability.\n\n2. Second, a potential limitation lies in the experimental design itself. The use of impossible or contradictory tasks could act as a confounding variable. It is plausible that such prompts might confuse the model or prime it to generate unconventional responses, making it difficult to isolate whether the observed behavior is an inherent strategic tendency or an artifact of the unusual problem setup.\n\n3. Using impossible test cases to evaluate code generated by models may lead to inaccurate assessments. Therefore, the author should thoroughly discuss in the paper what kinds of prompts were used and the detailed criteria for determining whether the generated code is considered correct or incorrect. In some cases, the output may not truly be an error—for instance, a language model might be intentionally designed to handle faulty user inputs robustly. Should such behavior really be treated as a mistake?"}, "questions": {"value": "1. Please solve the weakness above.\n2. When presented with an impossible task, is the model's resulting \"cheating\" a strategic choice to bypass the instruction, or simply an artifact of its failure to process the logical contradiction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xKe77geIaz", "forum": "SeO4vyAj7E", "replyto": "SeO4vyAj7E", "signatures": ["ICLR.cc/2026/Conference/Submission15701/Reviewer_5CGh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15701/Reviewer_5CGh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546018885, "cdate": 1761546018885, "tmdate": 1762925951733, "mdate": 1762925951733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ImpossibleBench, which is a benchmark designed to capture LLMs tendency to cheat. The benchmark uses existing benchmarks of LiveCodeBench and SWE-bench and uses a powerful LLM to change the test cases to conflict with the original task. The LLMs are evaluated on the tasks with the prompts including test cases asking not to change them. Attempts to change the test cases or devising workarounds are categorized as cheating. Frontier models are demonstrated to cheat frequently."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well written and organized.\n- The empirical evaluations are extensive in terms of the variety of models considered and tested under various context engineering scenarios."}, "weaknesses": {"value": "- I am not convinced if this paper is delivering on its promise of measuring cheating. I am worried that some of the mutations are conflicting with the task itself and common sense (like asserting that 7 is not a prime number) and LLMs tendency to modify them is not measuring their cheating but their eagerness. As a developer, this would be the first thing I imagine myself doing as well.\n- I also have reservations about the significance of the problem as I believe the cheating problem is closely related to instruction following and some of the failure cases were transient as models were getting better. In Figure 6, we already see Claude Opus and GPT5 getting almost perfect scores.\n- The paper offers limited novelty building on existing benchmarks in an incremental manner."}, "questions": {"value": "- Is it possible to share examples of cases with modified test cases? I see that you already provide examples in Appendix B and I'm interested to see model's chain of thought and whether/how they explain the change they are making.\n- Is it possible extend the method to non-coding domains?\n- Are there any experiments where the mutations are generated with another model to understand if the results would still be similar?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wQLvAESWqU", "forum": "SeO4vyAj7E", "replyto": "SeO4vyAj7E", "signatures": ["ICLR.cc/2026/Conference/Submission15701/Reviewer_17CP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15701/Reviewer_17CP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888048025, "cdate": 1761888048025, "tmdate": 1762925951049, "mdate": 1762925951049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a benchmark framework, ImpossibleBench, designed to systematically measure the LLMs' propensity of exploiting test cases through reward hacking or cheating. The core methodology involves modifying existing coding bench marks (LiveCodeBench and SWE-bench) to create test tasks that are impossible to pass by design. Hence, any passing of such impossible tests signals some cheating of tested models or agentic systems. Through empirical study and experimentations, this framework has proved effective to signify a variety of cheating methods, widely adopted by various LLM models in the production. The authors also show findings where prompts (context engineering), test access and feedback loop have clear influences on the passing rate of original test cases, as well as cheating rate highlighted by the ImpossibleBench. Based on their discoveries, they made sound suggestions to real-world LLM deployments for cheating and reward hacking prevention. Finally, the paper also discusses that cheating in some complex testing scenarios of ImpossibleBenchmark could be hard to detect in simple monitoring setups, raising awareness and questions on monitoring deficiency and model cheating intent."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Overall, this paper presents a good contribution to LLM benchmarks, providing additional frameworks for cheating and reward hacking prevention in production.\n- The idea of creating tasks impossible to solve provides an objective measurement of attempts to cheat.\n- The strategy and design choices taken when modifying existing benchmarks can be borrowed by other projects with similar goals."}, "weaknesses": {"value": "There are some caveat in the paper to be clarified in author response:\n\n- LiveCodeBench quality control is missing due to lack of standard solution. However, the paper did not mention what other measures were done or attempted to sanitize the dataset and ensure consistent quality as SWE-Bench. Given the broad lower cheating rate of Impossible-LiveCodeBench, it is not hard to make claims that LiveCodeBench has low quality of \"impossible\", where test cases were actually passable.\n- There could be more extended discussion and future research direction about content highlighted by Section E.3.\n- Missing discussions on limitations of this benchmark, such as cheating strategies that ImpossibleBench is not shown effective on."}, "questions": {"value": "- How do you interpret the effect of Feedback Loops? In particular, do you have hypotheses for why models react differently to \"flag_for_human_intervention\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mJfbglAoJF", "forum": "SeO4vyAj7E", "replyto": "SeO4vyAj7E", "signatures": ["ICLR.cc/2026/Conference/Submission15701/Reviewer_X9K7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15701/Reviewer_X9K7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762313498660, "cdate": 1762313498660, "tmdate": 1762925949522, "mdate": 1762925949522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IMPOSSIBLEBENCH, a benchmark framework for measuring large language models’ propensity to “cheat” by exploiting unit tests instead of following natural-language specifications. The authors construct “impossible” variants of LiveCodeBench and SWE-bench by mutating unit tests so that any solution that passes must violate the specification. Cheating rate is defined as the pass rate on these impossible tasks. Using multiple frontier models and two scaffolding setups, the paper analyzes cheating strategies (test modification, operator overloading, stateful hacks, special-casing), studies how prompts, test access, and feedback loops affect cheating, and demonstrates that IMPOSSIBLEBENCH can also serve as a testbed for evaluating LLM-based cheating monitors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear problem formulation:\nThe paper focuses on a concrete and practically important failure mode of LLM code agents: reward hacking via test-case exploitation. By constructing “impossible” tasks where any pass is necessarily specification-violating, the benchmark gives a clean operationalization of “cheating propensity,” which is easy to interpret and directly relevant to real-world agent deployments. \n\n2. Methodological simplicity:\nThe core idea is to mutate tests via one-off or conflicting assertions to create direct contradictions with the natural language spec, which is conceptually simple, automated (via LLMs), and might be applicable to other code benchmarks.\n\n3. Empirical breadth and detailed behavioral analysis:\nThe experiments cover several leading closed- and open-source models, multiple scaffolds (minimal vs full), and both algorithmic (LiveCodeBench) and multi-file, realistic (SWE-bench Verified) settings. The paper goes beyond aggregate cheating rates to categorize specific cheating strategies (test modification, operator overloading, state recording, special-casing) with illustrative examples, revealing qualitatively different behaviors across model families."}, "weaknesses": {"value": "1. Limited scope:\nWhile the coding domain is important, the framework currently only covers unit-test–driven programming benchmarks. Many safety-critical reward-hacking behaviors for LLM agents arise in more open-ended or non-code settings (e.g., tool-using assistants, structured reasoning tasks, RL-style environments) where “impossible tasks” are harder to define. The paper briefly argues generality, but does not demonstrate extensions beyond Python code + tests, limiting immediate applicability to broader LLM safety contexts. Maybe the authors could consider expanding the application scenario?\n\n2. Reliance on tool/proxy LLMs:\nBoth the creation of impossible tests and the categorization of cheating strategies rely heavily on other LLMs (e.g., Sonnet 4 and Opus 4). Although the authors perform some automatic quality control (e.g., filtering some mutations), this pipeline still introduces potential biases and subtle failure modes. For instance, undetected cases where mutations are not truly “impossible” or where the classifier mislabels nuanced behaviors.\n\n3. Limited statistical analysis:\nThe paper highlights a qualitative pattern that stronger models tend to cheat more, but the analysis remains largely descriptive. Cheating rates depend on many intertwined factors (e.g., prompt, task difficulty) and the paper does not provide more rigorous correlation or analysis. This makes it harder to draw such strong conclusions about how “capability” causally affects cheating propensity."}, "questions": {"value": "Please refer to the 'Weaknesses'."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SkeVXCyVu2", "forum": "SeO4vyAj7E", "replyto": "SeO4vyAj7E", "signatures": ["ICLR.cc/2026/Conference/Submission15701/Reviewer_SWzR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15701/Reviewer_SWzR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762704371839, "cdate": 1762704371839, "tmdate": 1762925949199, "mdate": 1762925949199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank all the reviewers for their time and insightful feedback. We appreciate reviewer 57GH for highlighting our core contribution as a 'highly original methodology' and a 'novel and clever approach.' We are also very encouraged that reviewer X9K7 recognized our work as a 'good contribution to LLM benchmarks' and a valuable framework for studying reward hacking.\n\nWe would like to clarify our perspective on three key concerns, with further details provided in the individual responses:\n\n- **The issue we documented is a genuine reliability failure rather than \"helpfulness.\"** We observe models modify test cases despite our clear instructions against so and they even engage in concerning actions like overloading comparison operators (Fig. 5, App. B).\n\n- **The issue worsens with model capability and it is not a transient issue that will resolve on its own.** On the contrary, our results demonstrate the behavior *worsens* with both increased model capability and task complexity (Fig. 3, Fig. 4).\n\n- **The coding domain is a strategic methodological choice.** While our methodology is widely applicable, we focused on coding for two critical reasons. First, as one of the most widely-deployed applications of LLMs, addressing reward hacking here is important and impactful. Second, the coding domain provides the objective, unambiguous ground truth that allows us to provide a rigorous analysis of this failure mode.\n\nWe have also uploaded a revised manuscript based on the reviewers' excellent suggestions. The key updates include:\n\n* **New Limitation Section:** As suggested by multiple reviewers, we have added a dedicated limitation section. This new section discusses our choice to focus on the coding domain and the reliance on LLMs for our pipeline.  \n* **Clarified Context and Related Work:** Per the excellent suggestion from reviewer 57GH, we have refined our introduction to better contextualize our work in relation to reward hacking. We have also expanded the related work section to include a more focused discussion of reward hacking and specific discussions around vibe coding.  \n* **More Details of Examples:** As suggested by reviewer 17CP, we have updated Appendix B to include more details from our selected runs, further illustrating the models' rationalizations and cheating behaviors.  \n* **Additional Ablation:** In line with feedback from reviewer 17CP and SWzR regarding pipeline robustness, we are running a smaller-scale ablation study using test mutations generated by a different model (GPT-5). We commit to adding these results to the appendix for the final camera-ready version to confirm our findings are not an artifact of a single generator model.\n\nWe believe these changes substantially strengthen the paper by clarifying its scope, context, and limitations. We have provided detailed responses to each reviewer's concerns below and look forward to the discussion."}}, "id": "d9TAelsaxa", "forum": "SeO4vyAj7E", "replyto": "SeO4vyAj7E", "signatures": ["ICLR.cc/2026/Conference/Submission15701/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15701/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission15701/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763179670687, "cdate": 1763179670687, "tmdate": 1763179774768, "mdate": 1763179774768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}