{"id": "MlHnpsBok3", "number": 1355, "cdate": 1756875092467, "mdate": 1763145258155, "content": {"title": "Radar-Guided Polynomial Fitting for Metric Depth Estimation", "abstract": "We propose POLAR, a novel radar-guided depth estimation method that introduces polynomial fitting to efficiently transform scaleless depth predictions from pretrained monocular depth estimation (MDE) models into metric depth maps. Unlike existing approaches that rely on complex architectures or expensive sensors, our method is grounded in a fundamental insight: although MDE models often infer reasonable local depth structure within each object or local region, they may misalign these regions relative to one another, making a linear scale and shift (affine) transformation insufficient given three or more of these regions. To address this limitation, we use polynomial coefficients predicted from cheap, ubiquitous radar data to adaptively adjust depth predictions non-uniformly across depth ranges. In this way, POLAR generalizes beyond affine transformations and is able to correct such misalignments by introducing inflection points. Importantly, our polynomial fitting framework preserves structural consistency through a novel training objective that enforces local monotonicity via first-derivative regularization. POLAR achieves state-of-the-art performance across three datasets, outperforming existing methods by an average of 24.9% in MAE and 33.2% in RMSE, while also achieving state-of-the-art efficiency in terms of latency and computational cost.", "tldr": "", "keywords": ["depth estimation", "radar", "metric depth", "multimodal", "3D vision", "fitting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/71dffe985ee5da89954c986198633a1c5def18cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a RADAR-camera-fusion-based metric depth esitmation pipeline. Given a depth prediction from monocular depth estimation methods, the framework aims to re-scale the predicted depth to an accurate metric one. The major claim here is that the standard scale-and-shift alignment (i.e., projection) is not enough for an accurate output. The authors extend the scale-and-shift alignment (ax + b) to a polynomial one. And the network naturally is designed to predict the polynomial factors in a transformer style. A regularization term is added to make sure the polynomial function is monotonically increasing. Experimental results show good improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Good motivation. I'm not an expert in the area of this fusion-based depth esitmation field. But as far as I know, many depth estimation models predict affine-invariant depth and use scale-and-shift alignment for metric calculation. It would be good to point out that it scale-and-shift alignment is not enough for an accurate output.\n\n- The overall framework is naturally and well designed with a clear motivation. Modules look seamlessly from my point view.\n\n- Good experimental results. Improvement over previous methods is satisfactory.\n\n- Well-written paper. It's easy for me to follow."}, "weaknesses": {"value": "1. Ablation study\n\n   Currently, all ablation study experiments are conducted based on a framework aiming to predict polynomial / scale and shift factors. It would be always good to see these results and the experimental results indicate the effectiveness of each module.\n\n    However, all previous work fuse RADAR and camera in the feature space and directly predict the depth result, whereas the proposed framework aims to predict rescaling factors based on the monocular depth. It would be necessary to have an experiment to demonstrate a target switch like this can lead to performance gain. Though the authors provided something about Regression Baselines in Tab.13, it's not clear for me how this baseline is designed.\n\n    I think it can be done by reversing the direction of the cross attention and adding one decoder for depth estimation.\n\n    It would be better to put the ablation study in the main paper.\n\n2. H PROOF is weird from my point of view. Scale and shift alignment is to estimate factors with a min global error and it's not for looking for best solutions without any error. \n\n    The figure for H PROOF is super miss-leading, please remove the orange \"affine from first two points\" line. It's NOT how scale and shift alignment works at least. \n\n    Adding reference for related work claiming alignment issue might also be helpful, like [1, 2]\n\n    [1] benchdepth: are we on the right way to evaluate depth foundation models? \n    [2] MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details"}, "questions": {"value": "Please check the weakness.\n\nFor me, it's hard for me to judge the novelty from an expert view in the field of fusion-based depth esitmation. I would like to check opinions from other reviewers who might have more experience. \n\nIt would be better to see a deeper discussion about the alignement issue in case we even don't have radar points as guidance (to somehow make broader impact for general depth field), or see some more rigorous experiments to demonstrate the point. If there are no other concerns from other reviewers and the authors can make this discussion better, I would like to increase the rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E6wy480F42", "forum": "MlHnpsBok3", "replyto": "MlHnpsBok3", "signatures": ["ICLR.cc/2026/Conference/Submission1355/Reviewer_MTqC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1355/Reviewer_MTqC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760821013465, "cdate": 1760821013465, "tmdate": 1762915745761, "mdate": 1762915745761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Ia7DrWonvt", "forum": "MlHnpsBok3", "replyto": "MlHnpsBok3", "signatures": ["ICLR.cc/2026/Conference/Submission1355/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1355/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763145257437, "cdate": 1763145257437, "tmdate": 1763145257437, "mdate": 1763145257437, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper resolves scale ambiguity in monocular depth estimation by converting predictions to metric depth via automotive radar. It proposes POLAR, applying a learned polynomial transformation to pretrained MDE (Monocular Depth Estimation) outputs. Unlike global scale-and-shift (uniform adjustment), polynomial fitting introduces inflection points for non-uniform refinement. A lightweight multimodal network predicts coefficients from sparse radar and image features, with a novel monotonicity regularization (first-derivative penalty) to preserve local depth order.Evaluated on nuScenes, ZJU-4DRadarCam, View-of-Delft, POLAR achieves state-of-the-art accuracy: RMSE reduced by 30% on average (up to 50% on some datasets), with real-time inference (≈40 FPS) and lower computational cost than prior radar-camera fusion methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Polynomial fitting: Uses radar-guided high-degree polynomial transformation to align monocular depth to metric scale, enabling non-uniform adjustments to correct multi-object depth misalignments that affine transforms cannot fix.\n\nRadar use: Leverages low-cost mmWave radar via lightweight feature fusion (no complex 3D fusion) to inform polynomial coefficients, gaining strong performance without heavy networks.\n\nEfficiency: Streamlined single-stage design avoids heavy operations (e.g., explicit radar-image correspondence), achieving 24.8 ms/frame inference (40+ Hz) and reduced FLOPs for practical autonomous deployment."}, "weaknesses": {"value": "Reliance on MDE local order: Assumes MDE provides reasonable local relative depth. Polynomial fitting (with monotonic constraints) cannot reorder local depths, so MDE’s local ordinal errors or structure misses remain uncorrected.\n\nDependency on radar quality: Radar’s sparsity/noise affects performance. No radar detections (range/reflectivity/occlusion) or outlier/noise reduce polynomial guidance, degrading scaling accuracy in affected depth ranges."}, "questions": {"value": "Handling local depth errors: How does POLAR perform if MDE has local mistakes (e.g., object predicted closer than background)? Does monotonic constraint block error correction? Are there observed failure modes from this assumption, and how to address them (e.g., adding local depth inversion detection) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "28UFJka4bi", "forum": "MlHnpsBok3", "replyto": "MlHnpsBok3", "signatures": ["ICLR.cc/2026/Conference/Submission1355/Reviewer_dVoq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1355/Reviewer_dVoq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819635196, "cdate": 1761819635196, "tmdate": 1762915745611, "mdate": 1762915745611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a method for depth estimation using sparse radar points. They use an off-the-shelf scale-less depth estimator, then correct its depth predictions with polynomial coefficients predicted from both input radar points and their relationship with the initial depth estimation. The authors demonstrate good performance against a number of prior works, ablate their key choices, and do due diligence to report against depth completion focused works as well."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper proposes an interesting framework for polynomial-based correction of scale-less depth predictions from an off the shelf network.\n- The authors apply this method to radar input points, demonstrating effective depth correction and improvement with more coefficients.\n- This reviewer found the monotonic regularization a good insight."}, "weaknesses": {"value": "- The main results in Table 1 appear to use UniDepth (Table 4). However, I think the TacoDepth results use DPT (explained in Table 1 of TacoDepth). Using DPT instead, the authors report 1525.6/3745.0 MAE/RMSE, which is worse than TacoDepth's 1492.4/3324.8. This raises a bit of a concern of fair evaluation. A similar concern also affects ZJU.\n- Why did the authors decide to develop such a specific type of feature aggregation method in Equation 2? For instance, given the similarities between L2 distance and dot product (similar w/ normalization), this could could have been a standard cross-attention layer instead. This ablation could be useful.\n- Equation 3 seems like a standard cross-attention layer, moving features from prototypes to the image. Why are the prototypes necessary at all? Would not a cross attention layer directly from radar points to the image suffice, when adjusted for the same # of parameters used? An ablation could give more insight.\n- While the polynomial approach is useful for aligning the overall scene depth, it is, to the best of my knowledge, still a region-agnostic correction method. To this reviewer, it seems like instead of having the same correction for all scene elements of the same predicted depth, instead, different scene elements should have their own correction.\n- For the results in Table 4, is the method trained independently for each depth estimator, or is a single model used for all? If the former, how might the latter work, to evaluate generalizability of the model to different depth estimators?\n- A core argument in the paper seems to be that such polynomial fitting/correction is better than direct depth prediction. I believe an ablation demonstrating this, possibly by trying to make direct depth prediction in the 2D image plane after the radar-to-image cross attention, would strengthen the paper.\n- While the authors did due diligence to compare with depth completion methods, there are a number of works focusing on sparse or unevenly distributed depth input that should probably be referenced [1-7]. While not radar, and it's likely too much to request comparison with these works during the rebuttal phase, some discussion of the ideas in these papers and how they may or may not be applicable could strengthen this work.\n- Building on the previous point, in Table 7, did the authors re-train the other methods? I find it surprising, for instance, that NLSPN which uses depth information fares worse than BTS, which is a pure monocular model.\n\n[1] Towards 3D Scene Reconstruction from Locally Scale-Aligned Monocular Video Depth (https://arxiv.org/pdf/2202.01470v3)\n[2] Sparsity Agnostic Depth Completion (https://openaccess.thecvf.com/content/WACV2023/papers/Conti_Sparsity_Agnostic_Depth_Completion_WACV_2023_paper.pdf)\n[3] Sparse SPN: Depth Completion from Sparse Keypoints (https://arxiv.org/pdf/2212.00987)\n[4] Flexible Depth Completion for Sparse and Varying Point Densities (https://openaccess.thecvf.com/content/CVPR2024/papers/Park_Flexible_Depth_Completion_for_Sparse_and_Varying_Point_Densities_CVPR_2024_paper.pdf)\n[5] SparseDC: Depth Completion from sparse and non-uniform inputs (https://arxiv.org/pdf/2312.00097)\n[6] SparseFormer: Attention-based Depth Completion Network (https://arxiv.org/pdf/2206.04557)\n[7] Towards Domain-agnostic Depth Completion (https://arxiv.org/pdf/2207.14466)"}, "questions": {"value": "- [Minor] L235: Just to confirm, \"projected radar features\" refers to radar features \"projected\" using the MLP, not that the radar points themselves are projected to 2D?\n- I would appreciate it if the authors could address my concerns above; for instance, some additional ablations could be helpful. Some core ideas, such as the polynomial fitting, remain not sufficiently tested, and I have some concerns about comparison fairness with TacoDepth. Given these concerns, I give an initial rating of 4, but I am open to raising my ratings if the above concerns are adequetely addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MZ5UaxG0RZ", "forum": "MlHnpsBok3", "replyto": "MlHnpsBok3", "signatures": ["ICLR.cc/2026/Conference/Submission1355/Reviewer_F7hX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1355/Reviewer_F7hX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962586137, "cdate": 1761962586137, "tmdate": 1762915745492, "mdate": 1762915745492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the metric depth estimation by leveraging the radar input and a pre-trained universal monocular depth estimator (MDE). The proposed POLAR, which starts from a frozen MDE and predicts a scene-specific polynomial. Coefficients are predicted from fused radar–image features; a first-derivative regularizer encourages the mapping to be “approximately monotone.” The method reports competitive accuracy and efficiency on several benchmarks, including nuScenes, ZJU-4DRadarCam, with best results around degree-8 polynomials."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  The idea of performing the scale correction as a non-linear, scene-level polynomial fit to an MDE’s output is clean and practically attractive. It departs from the dominant “decode dense depth from fusion” paradigm.\n\n2. The proposed method has strong empirical results. For instance, it shows consistent gains across multiple datasets and methods (e.g., Depth Pro, UniDepth). Ablations on polynomial degree and components are informative\n\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The mapping is global 1D, which is kind of limited.  Using a single scene-global polynomial maps equal (z) values to the same D everywhere. It cannot fix spatially entangled errors where two regions share similar predicted (z) but need different corrections (e.g., reflective car vs. dark wall). \n\n2. Inefficient ablation studies. Beyond showing that affine mappings can fail, there is insufficient analysis regarding the identifiability. eg, under radar sparsity/noise, stability vs. polynomial degree. \n\n3. The efficiency table is ambiguous. It’s unclear whether latency/GFLOPs include the MDE backbone for all methods. Provide backbone cost, fusion/decoder cost, and true end-to-end numbers with GPU/precision/batch/resolution details."}, "questions": {"value": "see above weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X15PkuR2qq", "forum": "MlHnpsBok3", "replyto": "MlHnpsBok3", "signatures": ["ICLR.cc/2026/Conference/Submission1355/Reviewer_3ts8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1355/Reviewer_3ts8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762119579633, "cdate": 1762119579633, "tmdate": 1762915745371, "mdate": 1762915745371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}