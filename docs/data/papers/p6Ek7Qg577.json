{"id": "p6Ek7Qg577", "number": 12203, "cdate": 1758206309205, "mdate": 1759897525492, "content": {"title": "Adaptive Width Neural Networks", "abstract": "For almost 70 years, researchers have primarily relied on hyper-parameter tuning to select the width of neural networks' layers. This paper challenges the status quo by introducing an easy-to-use technique to learn an unbounded width of a neural network's layer during training. The method jointly optimizes the width and the parameters of each layer via standard backpropagation. We apply the technique to a broad range of data domains such as tables, images, text, sequences, and graphs, showing how the width adapts to the task's difficulty. A by product of our width learning approach is the easy truncation of the trained network at virtually zero cost, achieving a smooth trade-off between performance and compute resources. Alternatively, one can dynamically compress the network until performances do not degrade. \nIn light of recent foundation models trained on large datasets, requiring billions of parameters and where hyper-parameter tuning is unfeasible due to huge training costs, our approach introduces a viable alternative for width learning.", "tldr": "We propose a strategy to learn, during a single training, the width of neural network layers depending on the task and without imposing upper bounds.", "keywords": ["Neural Networks", "Learning the Number of Neurons", "Adaptive Width Learning", "Dynamic Architectures", "Information Compression", "Variational Inference"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/401d1c3fc987d49b8c8e8c8abddf7a56d69668b1.pdf", "supplementary_material": "/attachment/8ca067240b30b144ee21555fbde2ff2b16f39c68.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Adaptive Width Neural Networks (AWNN), a probabilistic framework that maximizes a simple variational objective via backpropagation over a neural network’s parameters. Further a soft importance ordering is imposed on the neurons of the layer, whereby runtime dynamic truncation adaptively controls the width, and online compression can be realized naturally through regularization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a new approach for learning netwroks of unbounded width. The paper provides a theoretical derivation for their model as well as expermients that support their claims."}, "weaknesses": {"value": "The proposed method shares notable similarities with prior work on learning infinite depth, which may limit the perceived novelty. And there has already been some exploration in the direction of learning model's width."}, "questions": {"value": "no questions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gq2s3v7buY", "forum": "p6Ek7Qg577", "replyto": "p6Ek7Qg577", "signatures": ["ICLR.cc/2026/Conference/Submission12203/Reviewer_A2U1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12203/Reviewer_A2U1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675380759, "cdate": 1761675380759, "tmdate": 1762923153073, "mdate": 1762923153073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework that enables neural networks to dynamically learn and adjust their layer widths during training, removing the need for preset upper bounds. \nThe method leverages a probabilistic variational inference approach, adapted from “Variational Inference for Infinitely Deep Neural Networks” by Nazaret and Blei (2022), but focuses on varying layer widths rather than depths. \nBy automating the process of width selection, the approach streamlines model design and complements traditional hyperparameter search. \nExtensive experiments on both synthetic and real-world datasets demonstrate that Adaptive Width Neural Networks (AWNN) consistently match or outperform fixed-width baselines, allow efficient post-training compression via truncation, and remain robust across various hyperparameter choices and batch sizes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong Motivation and Practical Impact:\n\nThe authors address the challenge of hyperparameter tuning, a particularly arduous and resource-intensive task for large-scale networks. Automating the search for the width hyperparameter is therefore a highly practical contribution, especially in modern architectures with billions of parameters.\n\n2. Efficient Post-Training Trade-Off:\n\nThe proposed method offers the compelling ability to control model complexity after training with zero additional cost. This property is particularly valuable for practitioners who need to deploy models with varying computational or memory constraints, as it facilitates rapid reduction in training costs across architectures of different depths and widths.\n\n3. Theoretical foundation: \n\nWhile I am not fully familiar with variational inference frameworks and cannot fully verify the technical details of the problem formulation in Section 3, the method closely follows the established framework of Nazaret and Beli (2022), and the overall design seems to have no evident problem, but I hope that other reviewers with stronger expertise in this area will thoroughly assess the technical soundness of the approach."}, "weaknesses": {"value": "1. Limited Demonstration of Practicality and Scalability:\n\nAlthough the paper emphasizes practical implications, the experimental results do not fully demonstrate the method’s practical utility or scalability. Specifically, the approach is only applied to multi-layer perceptron (MLP) layers across various models; for convolutional neural networks (CNNs), it is restricted to the final MLP classifier layer. Given the paper’s claim of general applicability, it is unclear why the adaptive width mechanism has not been validated on other layer types, such as convolutional layers. Notably, there are several supernet approaches for adaptive width in CNNs such as [1][2], which would make for a valuable comparison to further highlight the novelty or limitations of this method.\n\n[1] Slimmable neural networks (ICLR 2019)\n\n[2] Alphanet: Improved training of supernets with alpha-divergence (ICML 2021)\n\n2. Scalability and Interdependence Across Layers:\n\nWhile the framework follows that of “unbounded depth neural networks” (UDN)\", the task of selecting layer widths in AWNN appears even more challenging, since truncation points must be chosen for every layer rather than just the overall depth. The paper assumes interdependence across layers but does not sufficiently address this complexity. In practice, changing the width of one layer can have significant effects on subsequent layers, and the impact of these dependencies deserves more thorough exploration and discussion."}, "questions": {"value": "1. My understanding is that $D_l$ (the truncated width of layer l) can fluctuate, increasing or decreasing, during training. For exmaple, DoubleMoon in Figure 2. In such cases, are the weights for newly added neurons initialized from scratch each time, and are discarded weights permanently removed? Alternatively, if the width decreases and then increases again later, are previously discarded neurons and their weights reinstated, or are they reinitialized? \nI am concerned that frequent initialization and discarding of neuron weights could potentially slow down the optimization process.\n\n2. The training procedure described in Algorithm 1 appears computationally efficient. Given this, would it be practical to evaluate the method on larger datasets (e.g., ImageNet) and deeper models (e.g., ResNet)? What is the actual training cost of the proposed approach in terms of runtime and memory usage? Are there any hidden or unexpected costs that might limit scalability to large datasets and architectures?\n\n3. Regarding Table 1: does “Linear\" refer to a linear classifier with the width equal to the number of dataset classes (e.g., width 10 for MNIST and CIFAR-10)? If so, does AWNN tend to learn significantly larger widths than this baseline for these tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QkbspGXkcM", "forum": "p6Ek7Qg577", "replyto": "p6Ek7Qg577", "signatures": ["ICLR.cc/2026/Conference/Submission12203/Reviewer_SBvZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12203/Reviewer_SBvZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909385419, "cdate": 1761909385419, "tmdate": 1762923151354, "mdate": 1762923151354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates growing the width of the linear layers, by considering the width to be hypothetically infinite dimensional, they then estimate the number of neurons in a layer during each batch by a single learnt latent variable $v_l$ per layer, which shifts the importance distribution (they have a fixed monotonically decreasing function to compute the probability, that is the initial neurons are more likely to be important than the new ones), any new neurons within the cumulative probability threshold ( $k$ >0.9)  are added, and the neurons with the least index (importance) can be de. Since new neurons are expected to have low importance (as the function is monotonically decreasing) the addition is expected to be stable. The experimentation is performed on tablular, image and text data on MLP, CNN and Text respectively to show adaptability of the method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The main advantage of the paper is that the latent variable which controls the width is learnt, so the addition and deletion of neurons is fast and requires significantly less compute than existing hessian based methods.\n2. The training is stable at appropriate batch sizes, since the importance is low for large widths, and usually we start with a considerably large width for large models. \n3. The strict ordering based on index ensures that old neurons are preserved, which again improves the stability of the training, but at the cost of flexibility.\n4. The results show that usually, the training yields similar or better accuracy, with similar width and in some cases, the width is dramatically increased but the performance gains too."}, "weaknesses": {"value": "1. Its harder for the neural network to change trajectory, since the importance is provided based on the index, if an old neuron has to be removed all newer neurons must be pruned as well, this limits the neural network from moving away from features it learnt.\n2. More redundant copies, the importance acts as a weight to the activation, therefore when the strength of an important neuron might not be enough, the neural network may choose to learn redundant copies to strengthen the importance of features. This may also lead to superposition of very important features on a small subset of the layer.\n3. If there are two features A and B being computed at index x and x+1, if B becomes more important than A as the training progresses, then either both A and B are dropped and re-learnt (which is hard if the index is small) or A transforms to B and B transforms to A over multiple iterations, which can slow convergence. The main drawback is we cannot effectively rearrange importance of neurons because of the hard index based importance.\n4. While appendix H shows that the learnt width gets similar performance when trained from scratch while fixed, is this the best width for the accuracy? could an ablation be done when training fixed neural networks with an increasing width in intervals of 100 until 1.5/2 x width."}, "questions": {"value": "1. A neuron can be less important for a specific subset of data but important for another subset of data, limiting this spike by applying an expected importance of the neuron , can lead to multiple neurons computing the same features to circumvent the loss in spike, this phenomenon can increase the number of redundant parameters being learnt. If we randomly drop certain neurons with similar importance do we observe minimal drop in performance? \n2. Why did the paper not explore Adaptive Depth, is it because of stability issues (like in case of depth, the deeper the layer is its expected to be more important)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sBGHeHOUBR", "forum": "p6Ek7Qg577", "replyto": "p6Ek7Qg577", "signatures": ["ICLR.cc/2026/Conference/Submission12203/Reviewer_KoUG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12203/Reviewer_KoUG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920716523, "cdate": 1761920716523, "tmdate": 1762923149714, "mdate": 1762923149714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a Bayesian method for setting the number of neurons per layer in a neural network. It starts with a (theoretically) infinite-width neural network and trains both the weights/biases and a width parameter for each layer. The chosen Bayesian method is Variational Inference.\n\nSummary of a **theoretical** training step:\n1. sample the width parameters $\\lambda_l \\sim \\mathcal{N}(\\nu_l, 1)$;\n2. use the $\\lambda_l$ to compute the neuron scalings $(f_l(i))_{i \\in \\mathbb{N}^+}$, where $f_l$ is a decreasing function parameterized by $\\lambda_l$ (the larger the $\\lambda_l$, the quicker $f_l$ decreases);\n3. given the $(f_l)_l$, compute the practical widths $(D_l)_l$: $D_l$ is the smallest integer such that $\\sum_{i=1}^{D_l} f_l(i) \\geq k = 0.9$;\n5. depending on the new widths $D_l$, prune or create neurons at each layer;\n6. provided that each neuron outputs $h_j^l = f_l(j) \\, \\sigma(\\sum_{i = 1}^{D_{l-1}} w_{ji}^l h_i^{l-1})$, that is, the output of the activation function $\\sigma$ scaled by a factor $f_l(j)$, perform a forward-backward and gradient step on the variational parameters $\\rho_{ji}^l$ of the weights $w_{ji}^l \\sim \\mathcal{N}(\\rho_{ji}^l, 1)$.\nIn practice, the variational parameters $(\\nu_l)_l, (\\rho_{ji}^l)_{lij}$ are trained directly, **without sampling** of the width parameters $\\lambda_l$ or the weights $w_{ji}^l$.\n\nThe neuron scalings $f_l(i)$ are a key component of the method, since they allow:\n* to practically deal with theoretical infinite-width layers;\n* to backpropagate the gradient of the loss up to the variational parameters $\\nu_l$: the loss does not depend continuously on the $D_l$, so $D_l$ cannot be used directly to optimize the loss. But $(f_l(i))_{li}$ works as a proxy for the \"layer width\" and: a) we have a continuous dependence of the loss on the $f_l(i)$; b) the $f_l(i)$ can be randomly generated with a continuous dependence on the variational parameters $\\nu_l$."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "# Clarity\nThe paper is well-written.\n\n# Significance\nThis paper proposes a Bayesian formulation of the problem of width selection when training a neural network, which is a well-grounded way of performing pruning and adding neurons (layer-wise). Additionally, the fact that, in theory, infinite-width neural networks are trainable (up to a decreasing scaling factor $f_l(i)$), makes this method fit several well-known theoretical frameworks (e.g., Neural Tangent Kernels).\n\n# Novelty\nThere are several Bayesian methods to select neurons or parameters, but, up to my knowledge, the method as a whole is new.\n\nPlease note that the idea of considering possibly infinite neural networks with decreasing scaling factors is not new. See for instance [1], which also cites [2] as a work introducing \"the idea of using asymmetrical scaling parameters\", which date respectively from 2023 (published in 2025) and 2020.\n\nHowever, the proposed method is a unified and original way to remove and add neurons, which is undeniably new. Additionally, the idea of **parameterized** asymmetrical scalings is new.\n\n[1] *Over-parameterised Shallow Neural Networks with Asymmetrical Node Scaling: Global Convergence Guarantees and Feature Learning*, F. Caron et al., Transactions on Machine Learning Research, 2025.\n\n[2] *Asymmetrical Scaling Layers for Stable Network Pruning*, P. Wolinski et al., [https://openreview.net/pdf?id=6GUIv9eYnD7](https://openreview.net/pdf?id=6GUIv9eYnD7), 2020."}, "weaknesses": {"value": "I do not see major weaknesses in this paper. But, on the technical side, several aspects deserve a discussion :\n1. why choosing a discretized exponential distribution for $f_l$? Is there a theoretical/heuristic argument? One could choose, as in [2], slowly decreasing scalings, such as $x \\mapsto 1/x$ or $x \\mapsto 1/(\\sqrt{x} \\ln(x))$...\n2. according to Eqn. (7), $\\lambda_l$ could be either positive or negative, since it is a Gaussian random variable. This is not acceptable, provided Eqn. (6), where $\\lambda_l$ is a scaling factor in an exponential. Is this a problem in practice? Even if it is not, would it be possible to replace the Gaussian distribution by a distribution supported on $\\mathbb{R}^+$?\n3. p 4, line 179: \"Appendix A and B provide formal requirements about $f_l$\". It seems that these Appendices are not about such \"formal requirements\", they are about \"Background notions\" on variational inference (App. A) and \"How to compute $D_l$ in practice\" (App. B). I am very curious about these \"formal requirements\", would it be possible to provide some? Anyway, this reference to Appendices A and B is not consistent with their content.\n\nClarity: are some \"minus\" missing in the exponentials in Eqn. (6)? It would be nicer to write $e^{-\\lambda_l x}$ instead of $e^{\\lambda_l(x)}$ (removing the parentheses would also makes it easier to read).\n\nStyle (minor concern): In the abstract and in the introduction, the authors **seem** to claim that there is no research in automatic hyperparameter search (which includes hyperparameters related to neural network architecture). The authors write \"For almost 70 years, researchers have **primarily** relied on hyper-parameter tuning [...] This paper challenges the status quo [...]\", which is technically correct, but slightly misleading. It is more a matter of style, which may be seen as a little fancy, than a matter of knowledge of works on neural architecture search, which are cited."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mKhLN36txy", "forum": "p6Ek7Qg577", "replyto": "p6Ek7Qg577", "signatures": ["ICLR.cc/2026/Conference/Submission12203/Reviewer_DdyY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12203/Reviewer_DdyY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762190823416, "cdate": 1762190823416, "tmdate": 1762923149250, "mdate": 1762923149250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a Bayesian method for setting the number of neurons per layer in a neural network. It starts with a (theoretically) infinite-width neural network and trains both the weights/biases and a width parameter for each layer. The chosen Bayesian method is Variational Inference.\n\nSummary of a **theoretical** training step:\n1. sample the width parameters $\\lambda_l \\sim \\mathcal{N}(\\nu_l, 1)$;\n2. use the $\\lambda_l$ to compute the neuron scalings $(f_l(i))_{i \\in \\mathbb{N}^+}$, where $f_l$ is a decreasing function parameterized by $\\lambda_l$ (the larger the $\\lambda_l$, the quicker $f_l$ decreases);\n3. given the $(f\\_l)\\_l$, compute the practical widths $(D\\_l)\\_l$: $D_l$ is the smallest integer such that $\\sum_{i=1}^{D_l} f_l(i) \\geq k = 0.9$;\n5. depending on the new widths $D_l$, prune or create neurons at each layer;\n6. provided that each neuron outputs $h_j^l = f_l(j) \\, \\sigma(\\sum_{i = 1}^{D_{l-1}} w_{ji}^l h_i^{l-1})$, that is, the output of the activation function $\\sigma$ scaled by a factor $f_l(j)$, perform a forward-backward and gradient step on the variational parameters $\\rho_{ji}^l$ of the weights $w_{ji}^l \\sim \\mathcal{N}(\\rho_{ji}^l, 1)$.\nIn practice, the variational parameters $(\\nu\\_l)\\_l, (\\rho\\_{ji}^l)\\_{lij}$ are trained directly, **without sampling** of the width parameters $\\lambda_l$ or the weights $w_{ji}^l$.\n\nThe neuron scalings $f_l(i)$ are a key component of the method, since they allow:\n* to practically deal with theoretical infinite-width layers;\n* to backpropagate the gradient of the loss up to the variational parameters $\\nu_l$: the loss does not depend continuously on the $D_l$, so $D_l$ cannot be used directly to optimize the loss. But $(f_l(i))_{li}$ works as a proxy for the \"layer width\" and: a) we have a continuous dependence of the loss on the $f_l(i)$; b) the $f_l(i)$ can be randomly generated with a continuous dependence on the variational parameters $\\nu_l$."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "# Clarity\nThe paper is well-written.\n\n# Significance\nThis paper proposes a Bayesian formulation of the problem of width selection when training a neural network, which is a well-grounded way of performing pruning and adding neurons (layer-wise). Additionally, the fact that, in theory, infinite-width neural networks are trainable (up to a decreasing scaling factor $f_l(i)$), makes this method fit several well-known theoretical frameworks (e.g., Neural Tangent Kernels).\n\n# Novelty\nThere are several Bayesian methods to select neurons or parameters, but, up to my knowledge, the method as a whole is new.\n\nPlease note that the idea of considering possibly infinite width neural networks with decreasing scaling factors is not new. See for instance [1], which also cites [2] as a work introducing \"the idea of using asymmetrical scaling parameters\", which date respectively from 2023 (published in 2025) and 2020.\n\nHowever, the proposed method is a unified and original way to remove and add neurons, which is undeniably new. Additionally, the idea of **parameterized** asymmetrical scalings is new.\n\n[1] *Over-parameterised Shallow Neural Networks with Asymmetrical Node Scaling: Global Convergence Guarantees and Feature Learning*, F. Caron et al., Transactions on Machine Learning Research, 2025.\n\n[2] *Asymmetrical Scaling Layers for Stable Network Pruning*, P. Wolinski et al., [https://openreview.net/pdf?id=6GUIv9eYnD7](https://openreview.net/pdf?id=6GUIv9eYnD7), 2020."}, "weaknesses": {"value": "I do not see any major weakness in this paper. But, on the technical side, several aspects deserve a discussion :\n1. why choosing a discretized exponential distribution for $f_l$? Is there a theoretical/heuristic argument? One could choose, as in [2], slowly decreasing scalings, such as $x \\mapsto 1/x$ or $x \\mapsto 1/(\\sqrt{x} \\ln(x))$...\n2. according to Eqn. (7), $\\lambda_l$ could be either positive or negative, since it is a Gaussian random variable. This is not acceptable, provided Eqn. (6), where $\\lambda_l$ is a scaling factor in an exponential. Is this a problem in practice? Even if it is not, would it be possible to replace the Gaussian distribution by a distribution supported on $\\mathbb{R}^+$?\n3. p 4, line 179: \"Appendix A and B provide formal requirements about $f_l$\". It seems that these Appendices are not about such \"formal requirements\", they are about \"Background notions\" on variational inference (App. A) and \"How to compute $D_l$ in practice\" (App. B). I am very curious about these \"formal requirements\", would it be possible to provide some? Anyway, this reference to Appendices A and B is not consistent with their content.\n\nClarity: are some \"minus\" missing in the exponentials in Eqn. (6)? It would be nicer to write $e^{-\\lambda_l x}$ instead of $e^{\\lambda_l(x)}$ (removing the parentheses would also makes it easier to read).\n\nStyle (minor concern): In the abstract and in the introduction, the authors **seem** to claim that there is no research in automatic hyperparameter search (which includes hyperparameters related to neural network architecture). The authors write \"For almost 70 years, researchers have **primarily** relied on hyper-parameter tuning [...] This paper challenges the status quo [...]\", which is technically correct, but slightly misleading. It is more a matter of style, which may be seen as a little fancy, than a matter of knowledge of works on neural architecture search, which are cited."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mKhLN36txy", "forum": "p6Ek7Qg577", "replyto": "p6Ek7Qg577", "signatures": ["ICLR.cc/2026/Conference/Submission12203/Reviewer_DdyY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12203/Reviewer_DdyY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762190823416, "cdate": 1762190823416, "tmdate": 1763141766787, "mdate": 1763141766787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}