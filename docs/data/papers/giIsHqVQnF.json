{"id": "giIsHqVQnF", "number": 1241, "cdate": 1756867210144, "mdate": 1759898219717, "content": {"title": "Lifted Uniform Quantization for Extreme Low-bit Large Language models", "abstract": "Pushing large language models to extreme low bit-widths (e.g., 2-bit) is a critical frontier for efficient deployment, yet it presents a daunting challenge to preserving model accuracy. Current methods are trapped in a fundamental trade-off: Vector Quantization (VQ) maintains accuracy by learning expressive codebooks but is crippled by its computationally expensive, non-parallelizable lookup operations. Conversely, Uniform Quantization (UQ) is exceptionally efficient but suffers a precipitous drop in quality at such low bit-widths. To break this impasse, we propose Lifted Uniform Quantization (LiftUQ), a new paradigm that encodes weights in an expanded latent space using ultra-low-bit uniform quantization (1-bit in our practice), and then applies a trainable dimensionality reduction linear transformation to project them into the original space, forming non-uniform codepoints without any look-up codebook. This lifted–projected representation recovers and even surpasses the expressive power of vector quantization while retaining the decoding efficiency of scalar uniform quantization. To make LiftUQ applicable to arbitrary layers, we further learn a whitening transform to produce approximately independent Gaussian-like channels, then apply the same lifted–projected encoding. LiftUQ marks a significant breakthrough in extreme low-bit quantization. Our experiments validate that it is the \\textbf{first framework to bridge the long-standing accuracy gap between uniform and vector quantization}, consistently matching or surpassing VQ performance on Llama and Qwen models—for instance, suffering less than a 2.7/1.1-point accuracy degradation on Llama-3-70B at 2/3-bit. Critically, this high accuracy is achieved with exceptional efficiency, boosting throughput up to 6.7$\\times$ over FP16 by combining the inherent speed of uniform decoding with a lightweight linear projection. This establishes LiftUQ a new, superior paradigm for practical quantization.", "tldr": "", "keywords": ["Quantization; Large language Model"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8ee4689044dd06c100c44c5eccf83385a9a01aeb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Lifted Uniform Quantization (LiftUQ), a method for extreme low-bit (1.62–3.02-bit) large language model (LLM) quantization, designed to balance the high efficiency of Uniform Quantization (UQ) and the strong accuracy of Vector Quantization (VQ). Its core approach involves lifting weights to a high-dimensional uniform lattice, projecting them to a target subspace via a pre-trained shared matrix \\( M \\), and using layer-adaptive lightweight whitening (transform \\( D \\)) to align weights with Gaussian distributions—eliminating VQ’s costly codebook lookup. LiftUQ natively supports fractional bit-widths, enabling performance-storage Pareto optimality (e.g., 2-bit LiftUQ-Llama-2-13B outperforms FP16 Llama-2-7B), and experiments on Llama/Qwen models validate its better accuracy-efficiency trade-off against baselines like AQLM and QuIP#."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed LiftUQ method in this paper has certain exploratory value in balancing accuracy and efficiency for low-bit quantization.  \n2. It directly addresses the fundamental pain point of current low-bit quantization (2-bit and below): the trade-off where Uniform Quantization (UQ) offers high efficiency but poor accuracy, while Vector Quantization (VQ) achieves high accuracy but suffers from low efficiency."}, "weaknesses": {"value": "1. The paper’s core claim that \"LiftUQ provides a new paradigm for extreme low-bit quantization\" is not well-supported. From the perspective of methodological comparison with existing works, LiftUQ is more of an incremental improvement rather than a paradigmatic innovation."}, "questions": {"value": "1. In the paper, weights are mapped to another space via linear transformation, and then weight-to-lattice-point mapping is performed in this space. This approach bears striking similarities to RabitQ—only differing by an additional dimension-lifting step. Could the authors elaborate on the specific differences between LiftUQ and RabitQ regarding using linear transformation to replace codebook lookup?  \n\n2. The paper fails to compare LiftUQ with QTIP, a state-of-the-art (SOTA) VQ method. QTIP has demonstrated impressive performance and efficiency, and omitting this comparison undermines the comprehensiveness of LiftUQ’s performance validation.  \n\n3. The paper assumes that \"a single pre - trained M matrix can be reused across all layers\" but provides no validation for this reasonableness. Transformer layers (e.g., attention layers with sparse weights and small variance vs. FFN layers with dense weights and large variance) exhibit significant differences in weight distribution. Even after whitening, is a shared M matrix still sufficient, or do layer - specific M matrices become necessary? The paper lacks ablation experiments comparing \"shared M matrix vs. layer - specific M matrices,\" making it impossible to rule out the possibility that shared M matrices cause accuracy loss in certain layers.\n\n4. The dimension selection of the M matrix (10×20 for 2-bit, 6×18 for 3-bit) is entirely based on empirical judgment, with no systematic search or justification. For example, why not use 12×24 or 8×16 for 2-bit quantization? What impact do dimension adjustments have on accuracy and training costs?  \n\n5. There is no ablation study on the three core components of LiftUQ (M matrix, whitening transformation D, and intra-block calibration). This makes it impossible to quantify the contribution of each component—e.g., how much accuracy degrades when D is removed, or whether whitening is truly necessary.  \n\n6. During model deployment, does the storage overhead of the M and D matrices (e.g., total parameters of the M matrix for a 70B model) offset the storage savings from weight quantization? The paper does not provide a comparison of **total storage volume** (quantized weights + M/D matrices vs. original FP16 weights).  \n\nBased on the above concerns, I recommend a **Reject** decision. I would be happy to revise my score if the authors can address these questions and provide sufficient explanations in a revised version of the paper.\n\n---\nRaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jeNUgXjKHI", "forum": "giIsHqVQnF", "replyto": "giIsHqVQnF", "signatures": ["ICLR.cc/2026/Conference/Submission1241/Reviewer_xAD3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1241/Reviewer_xAD3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760545850291, "cdate": 1760545850291, "tmdate": 1762915715312, "mdate": 1762915715312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposes the LiftUQ framework. The core of this framework adopts the \"high-dimensional lifting - low-dimensional projection\" paradigm. Specifically, first, 1-bit uniform quantization is used in the high-dimensional space to encode the weights, and then a trainable linear projection matrix M is used to map them back to the original space to generate non-uniform code points. At the same time, the framework is equipped with a hierarchical learnable whitening transformation D, which has a decomposed structure and can convert weights into an approximately independent and identically distributed Gaussian distribution to adapt to the projection process. In addition, it combines lattice quantization and intra-block correction techniques. From the perspective of overall architecture design, the LiftUQ framework cleverly avoids the cumbersome codebook lookup process in VQ and successfully retains the parallel computing efficiency advantages of UQ."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "LiftUQ proposes a high-dimensional lifting framework to address the trade-off between UQ and VQ. It generates non-uniform codebooks through 1-bit uniform quantization in high-dimensional space and a trainable linear projection matrix, thereby achieving arbitrary-precision quantization. It avoids the codebook caching of VQ through matrix multiplication."}, "weaknesses": {"value": "1. This method increases additional computational overhead during inference. Furthermore, online decoding relies on floating-point matrix multiplication, and edge devices often lack floating-point computing power, which limits its application scenarios.\n\n2. The article lacks a comparison with the QTIP method, and QTIP's performance is far superior to QuiP#, which makes its SOTA positioning less convincing.\n\n3. The selection of the dimension of M lacks ablation experiments. The article only makes an empirical selection rather than a heuristic or derivation-based one, which requires more in-depth exploration.\n\n4. The article lacks a systematic ablation of each component. This makes it unclear whether the source of the accuracy improvement is the lifting space or the Crockett transform.\n\n5. The paper assumes that the M matrix can be reused without restrictions, but this lacks rationality. Due to the differences in activations across different layers, even if the weights follow a Gaussian distribution, especially after the introduction of Smooth, the differences between channels of activations are often amplified. In this case, due to the differences in the distribution of activations, minimizing the weight error cannot replace the real error. This paper lacks relevant derivations and experiments."}, "questions": {"value": "1. The paper points out that there is a 0.15-bit information gap between LiftUQ and the theoretical accuracy limit, but it does not further explore the source of the gap: is it due to insufficient expressiveness of the high-dimensional projection matrix M? Or is it that the whitening transformation D fails to completely convert the weights into an ideal Gaussian distribution? Or is it that the loss function (MSE) for intra-block correction does not fully fit the characteristics of LLM tasks? The lack of analysis on the decomposition of theoretical bottlenecks makes the optimization direction of the method unclear.\n\n2. The lifting matrix in the paper is obtained based on training, and the pseudocode may pose a risk of non-convergence. Have the authors tried to derive the optimal transformation based on the Gaussian distribution? For example, the E8P codebook in Quip#. This would be very helpful for increasing the theoretical richness of the article."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U6T4K12W3D", "forum": "giIsHqVQnF", "replyto": "giIsHqVQnF", "signatures": ["ICLR.cc/2026/Conference/Submission1241/Reviewer_kiJ4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1241/Reviewer_kiJ4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657386293, "cdate": 1761657386293, "tmdate": 1762915715100, "mdate": 1762915715100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Lifted Uniform Quantization (LiftUQ), a method that generates non-uniform codebooks by projecting simple uniform lattices from a higher-dimensional lifted space to the target weight space. \nThe method achieves competitive accuracy with vector quantization methods while maintaining the computational efficiency of uniform quantization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**1. Conceptual novelty.** The core insight of generating non-uniform codebooks through linear projection from a lifted uniform space with 1-bit grids is mathematically sound and well-motivated. \nThe visualization in Figure 2 effectively demonstrates how projecting high-dimensional uniform lattices creates structured non-uniform distributions.\n\n**2. Promising empirical performance.** Tables suggest competitive or better performance than VQ baselines at extreme low-bit quantization (2-3 bits), while outperforming uniform quantization methods by significant margins.\n\n**3. Efficiency gains with native support for fractional bit-widths.** Replacing lookup-table operations with linear transformations enables practical deployment advantages.\nFurthermore, the ability to smoothly vary compression rates by adjusting the lifting dimension enables Pareto-optimal model selection for specific memory budgets."}, "weaknesses": {"value": "**1. Limited experimental validation.** While the proposed method demonstrates solid results on standard benchmarks, the experimental scope is narrow and does not fully establish the generality of the approach. \nThe evaluation centers mainly on perplexity and zero-shot accuracy of Llama-2, Llama-3, and Qwen-2.5 models. \n- Validation on other model families such as encoder–decoder or mixture-of-experts architectures (e.g., Mixtral) would strengthen the claim of broad applicability.\n- Evaluating on instruction-tuned, multilingual, or multi-modal models would further demonstrate robustness beyond the current base (non-instruction-tuned) LLaMA and Qwen settings.\n\n**2. Ablation and hyperparameter analysis.** The paper lacks comprehensive ablation studies on key design components. \nSpecifically, it should include: (1) the impact of alternative whitening transform parameterizations beyond the Kronecker product structure, (2) sensitivity to initialization choices for P1, P2, s1, and s2, (3) comparisons of different projection matrix training strategies, and (4) systematic evaluation of calibration dataset size and composition, and (5) the effect of the lifted subspace dimension and block size on model accuracy and quantization cost.\nFurthermore, a broader hyperparameter analysis is necessary—covering the effects of learning rates, fine-tuning epochs, and initialization of the whitening transform—to ensure the robustness and reproducibility of the reported results.\n\n**3. Insufficient analysis of computational overhead during quantization.** While the authors emphasize LiftUQ’s efficiency, the reported end-to-end wall-clock time of approximately 100 hours on a single A100 80GB GPU for quantizing a 70B parameter model (Section 4.3) raises doubts about its practicality for large-scale deployment.\nA more detailed empirical analysis of computational cost, including comparisons of end-to-end quantization time across different model sizes, along with a breakdown of core stages (per-layer quantization time, data loading overhead, calibration steps, etc.), would be necessary to substantiate the claimed efficiency.\nFurthermore, although Section 4.4 briefly mentions the trade-off between subspace dimension and quantization time, it lacks quantitative scaling results across various model sizes and bit-widths.\nProviding a comprehensive runtime study would clarify whether LiftUQ’s reported efficiency consistently holds across different architectures and configurations."}, "questions": {"value": "**1. Comparison with BCQ.** The proposed work introduces a structured non-uniform quantization scheme designed to achieve acceleration while maintaining representational flexibility. \nHow does this approach differ from prior methods with a similar concept, such as Binary-Coding Quantization (BCQ) [1, 2], both in design principle and empirical performance?\n\n**2. Degree of lookup-table inefficiency.** The authors argue that lookup tables are hardware-unfriendly and slow.\nHowever, LUT-GEMM [3] reports that optimized kernels can achieve comparable efficiency to uniform quantization.\nIt would be helpful to provide a quantitative comparison showing how much slower inference becomes when using the best available LUT-based kernels (including LUT-GEMM) compared to the proposed LiftUQ.\nAs LiftUQ demonstrates comparable accuracy to VQ-based methods, such a comparison would be crucial to assess the real significance of its acceleration advantage.\n\n**3. Application towards weight-activation quantization.** The experiments focus exclusively on weight-only quantization.\nHowever, the greatest acceleration potential of uniform quantization arises when both weights and activations are quantized, enabling matrix multiplications with low-bit kernels specifically designed for UQ.\nIt would be valuable to see results comparing LiftUQ and standard UQ under this weight-activation quantization setting, in terms of both accuracy and inference throughput.\n\n**Justification for Rating**\n\nThe paper presents a novel and theoretically motivated approach to extreme low-bit quantization. \nHowever, the limited scope of experimental validation and insufficient analyses make it difficult to assess the generality and scalability of the approach.\nI am open to raising the score if these concerns are adequately addressed in the rebuttal.\n\n**References**\n\n[1] C. Xu et al., “Alternating Multi-bit Quantization for Recurrent Neural Networks”, ICLR 2018\n\n[2] S. Park et al., “Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models”, ACL 2025\n\n[3] K. Park et al., “LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models”, ICLR 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xDA2JqcDEr", "forum": "giIsHqVQnF", "replyto": "giIsHqVQnF", "signatures": ["ICLR.cc/2026/Conference/Submission1241/Reviewer_d92k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1241/Reviewer_d92k"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734599215, "cdate": 1761734599215, "tmdate": 1762915714992, "mdate": 1762915714992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Lifted Uniform Quantization (LiftUQ), a weight-only quantization framework for extreme low-bit LLMs. The core idea is to encode weights with 1-bit uniform codes in a higher-dimensional “lifted” space and map them back to the original space via a learned linear projection, thereby producing non-uniform codepoints without a lookup table. A per-layer, decomposed whitening transform makes weights approximately i.i.d. Gaussian so a single projection matrix, trained once on Gaussian data, can be reused across layers. The authors claim LiftUQ matches or surpasses vector-quantization (VQ) accuracy at 2–3 bits while retaining scalar-UQ decoding efficiency, reporting <2.7/<1.1 point accuracy drops on Llama-3-70B at 2/3 bits and up to 6.7× FP16 throughput, plus native support for fractional “bit-widths.” The method is positioned as combining UQ’s hardware-friendly parallelism with VQ’s expressivity, avoiding VQ’s large codebooks and irregular memory access. The paper enumerates three contributions: the lifted codebook idea, a LUT-free linear decoding path, and state-of-the-art results in the 2–3-bit regime."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Conceptual simplicity with hardware awareness. Representing codes as 1-bit vectors in a lifted space and decoding via a single linear map is elegant and plausibly hardware-friendly relative to VQ lookups. The paper clearly contrasts dequantization pipelines and claims to preserve UQ-style efficiency. \n* Decomposed whitening transform. The layerwise transform $(D=\\mathrm{diag}(s_1)(P_1\\otimes P_2)\\mathrm{diag}(s_2))$ is light and invertible; the paper argues it achieves near-i.i.d. Gaussian channels at (O(n\\sqrt n)) activation cost, enabling reuse of a single projection (M). \n* Clear three-phase pipeline. Phase 1 trains (M) on Gaussian data, Phase 2 learns (D) per layer, Phase 3 performs lattice quantization with block-wise fine-tuning; the reconstruction uses $(o=\\mathrm{diag}(s),W_q D^* a^\\top) with (D^*=M^\\top D^{-1})$. \n* Empirical results across models and bit-widths. The paper reports improvements over UQ baselines and parity or small wins against strong VQ methods in 2–3 bits; the text highlights Llama-3-70B gaps and the 1.58-bit setting against PTQ1.61. \n* Fractional bit-width story and Pareto argument. Encoding capacity via lifted dimension enables effective “2.3-bit” or “2.74-bit” points and an appealing Pareto narrative versus discrete FP model sizes. \n* Efficiency claims. Reported quantization cost for a 70B model is ~100 GPU-hours on a single A100-80GB, and decoding throughput up to 6.69x FP16 with a Triton+BitBLAS path; the paper discusses asymptotics versus VQ."}, "weaknesses": {"value": "1. Decoding and search complexity are under-justified.\n   The paper asserts decoding complexity $(O(d^{1.5}))$ and contrasts it with VQ, but does not provide a rigorous derivation or runtime model tying constants to GEMV, cache behavior, or kernel fusion. This matters because small constant factors dominate at batch size 1. Provide a formal derivation, profiling on multiple GPUs, and apples-to-apples kernels beyond Triton prototypes. \n2. Nearest-neighbor search in the lifted space is a practical bottleneck.\n   The method’s offline phase requires exponential search in $(d_s\\cdot b)$, and the paper concedes the quantization cost grows rapidly, recommending “accepting the one-time cost.” Stronger engineering details are needed: exact search algorithm, beam widths, candidate pruning, wall-clock per layer, and how quality scales with $(d_s)$. The current text gives only a qualitative scaling law and a rough “~100 hours” single-GPU figure for 70B. Provide per-layer timing, ablations over $(d_s)$, and quality-vs-time curves.\n3. Ablations on architectural choices are thin.\n   The paper fixes (M) shapes for 2/3 bits (e.g., $(10\\times 20)$, $(6\\times 18)$) and reuses a single (M) trained on Gaussian data. It needs ablations on: per-layer (M) vs global (M), impact of (M) dimension on accuracy, latency, and memory; and how well Gaussian-trained (M) transfers to real weight distributions after whitening.\n4. Evaluation coverage and reproducibility.\n   Results focus on perplexity and a small set of zero-shot tasks; latency is reported per-layer rather than end-to-end generation throughput including KV cache effects. Include full pipeline latency at common decoding settings (bsz=1, context 2k–8k, top-p sampling). Also, code and checkpoints are not available at submission time; the paper promises anonymized release during rebuttal. For a method hinging on kernels and search, this is a high risk for reproducibility.\n5. Fine-tuning details are modest; calibration data is small.\n   The intra-block correction and end-to-end tuning use ~4,096 RedPajama samples with seq len 2,048–4,096 and 1–2 epochs. Show sensitivity to calibration corpus, size, and domain shift; report failure cases and diminishing returns.\n6. Writing quality and polish.\n   There are several noticeable typos (“limmitation,” “Conclution”), and some figures/tables are referred to without sufficient methodological context. The paper would benefit from a careful editing pass...."}, "questions": {"value": "1. Decoding cost model. Please derive the $(O(d^{1.5}))$ decoding complexity claim, clarify constants, and compare end-to-end generation latency vs FP16, AWQ, and QuIP#/VPTQ at batch size 1 and 8 with realistic prompts. \n2. Search in lifted space. What exact algorithm do you use for nearest-neighbor search over ({ \\pm 1}^{d_s\\cdot b}) during quantization, and how do you prune the search? Provide per-layer wall-clock for 7B/13B/70B and quality vs search depth. \n3. Generalization of a global (M). Why is a single (M) trained on Gaussian optimal across layers and models after whitening? Please include ablations with per-layer (M) and different Gaussian seeds. \n4. Whitening complexity. The paper states (O(n\\sqrt{n})) activation cost for $(D^{-1})$. Show the exact dataflow in attention/FFN blocks, where you fuse (M) into $(P_2^{-1})$, and quantify added memory traffic. \n5. Fractional bits. For “2.3-bit” configurations, specify the mapping between lifted dimension and effective bits, and report the true model bytes including all transforms and scales. Provide Pareto plots with error bars. \n6. Calibration sensitivity. How sensitive are results to the 4,096-sample RedPajama set? Report robustness to domain-mismatched calibration (e.g., code, math) and to 512-token windows. \n7. Kernel details. The Triton+BitBLAS implementation details are crucial. Please release kernels and show speedups on A100, H100, and consumer GPUs, with/without CUDA-level optimization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4tlTQ4OmRf", "forum": "giIsHqVQnF", "replyto": "giIsHqVQnF", "signatures": ["ICLR.cc/2026/Conference/Submission1241/Reviewer_QuoP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1241/Reviewer_QuoP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923394424, "cdate": 1761923394424, "tmdate": 1762915714880, "mdate": 1762915714880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}