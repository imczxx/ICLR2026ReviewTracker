{"id": "YlpaaYxx4t", "number": 9574, "cdate": 1758128419580, "mdate": 1763506362074, "content": {"title": "Detecting Data Contamination in LLMs via In-Context Learning", "abstract": "We present Contamination Detection via Context (CoDeC), a practical and accurate method to detect and quantify training data contamination in large language models. CoDeC distinguishes between data memorized during training and data outside the training distribution by measuring how in-context learning affects model performance. We find that in‑context examples typically boost confidence for unseen datasets but may reduce it when the dataset was part of training, due to disrupted memorization patterns. Experiments show that CoDeC produces interpretable contamination scores that clearly separate seen and unseen datasets, and reveals strong evidence of memorization in open-weight models with undisclosed training corpora. The method is simple, automated, and both model- and dataset-agnostic, making it easy to integrate with benchmark evaluations.", "tldr": "We propose Contamination Detection via Context (CoDeC), a simple, efficient, and model-agnostic method that detects training data contamination in LLMs by measuring how in‑context examples affect predictions.", "keywords": ["LLM", "Contamination", "In-context learning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ff30b4c6160ac241bb4827febefd6353e52dbd32.pdf", "supplementary_material": "/attachment/c74e2719125751bc4ca3bb5afbc9d6c61c3859b6.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a method called CoDeC (Contamination Detection via Context) for detecting dataset-level contamination. The key idea is simple yet effective, for unseen data ICL examples will boost its confidence, so Delta (=ICL - no ICL) would be positive, but for trained data the ICL examples could be confusing hence result in negative Delta. The authors conducted wide range of experiments to support their method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clean and simple idea. I always appreciate simple ideas that work, and this paper's idea is novel and effective. \n2. Extensive experiments. The authors conducted experiments on a wide range of model families (even for RWKV ones), all with strong experiment results.\n3. Well-written and of importance to the field. The writing is clear and the topic is important to the community."}, "weaknesses": {"value": "I have 2 key questions:\n1. Does CoDeC work when you rephrase the question? (either train on rephrased versions then test on original, or train on original then test on the rephrased) I think it's getting widely known that many contamination cannot detect contamination unless it's under exact same phrasing, and arguably the value is low if a method cannot work for general use cases.\n2. Did you try training with a mix of other datasets in your experiments? i.e. not only the contamination test set, but with general corpus? This might improve models general ICL ability and render CoDeC ineffective, and people usually train their model this way instead of only using the test set."}, "questions": {"value": "1. Does model's capability have a correlation with the method effectiveness? Could you also report the general eval metrics (for benchmarks other that the contaminated data) of the models before and after finetuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q0FvwmZEKT", "forum": "YlpaaYxx4t", "replyto": "YlpaaYxx4t", "signatures": ["ICLR.cc/2026/Conference/Submission9574/Reviewer_7wL6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9574/Reviewer_7wL6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763004034, "cdate": 1761763004034, "tmdate": 1762921127125, "mdate": 1762921127125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Contamination Detection via Context (CoDeC) — a simple, scalable method for detecting and quantifying training data contamination in large language models (LLMs) using in-context learning dynamics.\n\n\nThe key idea\n\n* When given in-context samples from a dataset it has not seen during pre-training, an LLM tends to increase its confidence on the target sample.\n* When the dataset has been seen (i.e., contaminated), additional context disrupts memorization and reduces confidence.\n\nThe method computes a contamination score by measuring the fraction of samples for which adding in-context examples decreases the model’s confidence.\n\n\nMain contributions\n\n* Proposes CoDeC — a model- and dataset-agnostic contamination detection method requiring only gray-box access to logits.\n* Demonstrates near-perfect separation (AUC ≈ 99.9%) between seen and unseen datasets across various models (Pythia, GPT-Neo, RWKV, OLMo, Nemotron).\n* Shows robustness to dataset diversity and training stage, and scalability to large benchmarks.\n* Highlights how contamination transfers across related datasets and how finetuning affects CoDeC scores.\n* Positions CoDeC as an interpretable, efficient alternative to classical membership inference methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality\n\n* While prior work relies heavily on membership inference, loss-based calibration, or reference models, CoDeC offers a novel formulation by exploiting in-context learning behavior as a contamination signal.\n* The idea is conceptually elegant — turning a standard model property (ICL behavior) into a contamination test.\n\nQuality\n\n* The experiments are extensive: multiple models (spanning architectures and sizes), datasets (training vs unseen), and baselines.\n* Ablation studies are included (context size, dataset size) and finetuning experiments provide additional validation.\n* Clear definition of the contamination score and rationale for why it works.\n\nClarity\n\n* The paper is very well written, with clean exposition of the problem statement and intuition.\n* Figures are informative and minimalistic, making the method easy to grasp.\n* The pipeline (Fig. 1) clearly communicates the method steps.\n\nSignificance\n\n* Data contamination detection is increasingly critical for fair evaluation of LLMs.\n* A method that is scalable, interpretable, and does not require access to training data or reference models is very valuable.\n* The simplicity and efficiency of CoDeC make it applicable at scale, which is practically important."}, "weaknesses": {"value": "Experiment rigor to test the generality of the findings\n\n* The paper claims with the help of CMA that LLMs \n* Similar unsubstantiated claims appear in Section 2.2 (Key Idea), e.g., that adding in-context samples usually improves confidence for unseen datasets — no citations or empirical backing are provided there.\n\nLimited theoretical grounding\n\n* While intuition is discussed in depth (Section 2.5), the theoretical explanation remains qualitative.\n* Formal guarantees or bounds on false positive/negative rates for contamination detection would strengthen the contribution.\n\nEvaluation scope\n\n* Although contamination transfer across related datasets (e.g., MMLU) is explored, adversarial or edge cases (e.g., near-identical but unseen datasets, noisy mixtures) are only lightly discussed.\n* The method might conflate contamination with related distribution overlap, as the authors acknowledge, but do not quantitatively evaluate how severe this is.\n\nNovelty relative to related work\n\n* Some elements overlap conceptually with loss-based or entropy-based detection. Positioning CoDeC more clearly in terms of unique advantages and trade-offs would help."}, "questions": {"value": "Empirical clarification\n\n* Can the authors provide empirical evidence or references to support the claims in Section 2.2 and Section 3.5 regarding confidence shifts and generalization ability?\n* Can they quantify how CoDeC correlates with generalization metrics across models with similar benchmark accuracy?\n\nContamination vs distributional similarity\n\n* How robust is CoDeC to partial contamination or to unseen but stylistically similar datasets?\n* Could the authors provide additional results where datasets are synthetically perturbed to vary similarity to training data?\n\nThresholding strategy\n\n* The paper mentions thresholding and comparison against other models. Could the authors elaborate on practical guidelines for selecting these thresholds in real-world evaluation scenarios?\n\nTheoretical properties\n\n* Do the authors have any theoretical guarantees (e.g., consistency or error bounds) on CoDeC scores? Or atleast outline what such analysis might require?\n\nReproducibility and usage\n\n* How sensitive is the method to the number of in-context samples and the randomness of context selection?\n* Would deterministic context selection (e.g., nearest neighbors) further improve stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e0oQGwkmwC", "forum": "YlpaaYxx4t", "replyto": "YlpaaYxx4t", "signatures": ["ICLR.cc/2026/Conference/Submission9574/Reviewer_sLbA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9574/Reviewer_sLbA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884315466, "cdate": 1761884315466, "tmdate": 1762921126783, "mdate": 1762921126783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all Reviewers for their detailed analysis and insightful, constructive feedback. We are encouraged that our idea was described as clear and elegant (all Reviewers), supported by extensive experiments (sLbA, 7wL6) providing strong empirical evidence (xj5F), and recognized as practical and scalable (xj5F, sLbA). We also appreciate your comments that the paper is well‑written (sLbA, 7wL6) and important to the field (7wL6).\n\nGuided by your feedback, we have substantially expanded our analysis, experiments, and presentation to address your questions. Key updates include:\n- Providing motivating examples and additional empirical analysis to clarify the core idea behind CoDeC.\n- Enhancing guidance on interpreting CoDeC scores and highlighting best practices in the Conclusions.\n- Extending the leaderboard to cover over 40 models. This leaderboard can also further help the reader contextualize CoDeC scores.\n- Expanding the examination of scores through ablation studies involving training on related or augmented data (e.g., rephrases, mixed datasets).\n- Adding detailed analysis of adversarial and degenerate datasets, quantifying their impact in real settings, and discussing mitigation strategies.\n- Analyzing similarity-based deterministic context selection.\n- Studying the effect of sample length on CoDeC scores.\n- Updating Related Works to better emphasize unique advantages of CoDeC.\n\nAll new experiments, charts, and supplementary analyses are gathered in the Appendix E of the paper and will be subsequently integrated into the contents.\n\nWe hope these additions fully address your questions. If any points remain unclear or you would like to explore specific topics further, we would be happy to continue the discussion."}}, "id": "kR7eo5dFrZ", "forum": "YlpaaYxx4t", "replyto": "YlpaaYxx4t", "signatures": ["ICLR.cc/2026/Conference/Submission9574/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9574/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9574/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763504867664, "cdate": 1763504867664, "tmdate": 1763504867664, "mdate": 1763504867664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CoDeC, a dataset-level method to detect training-data contamination in LLMs by measuring how few-shot, same-dataset context changes model confidence on a target sample. If added context improves confidence, the dataset is likely unseen; if it lowers confidence, the model likely memorized the dataset or closely related data. The per-sample confidence shift Δ(x) is aggregated into a dataset contamination score SCoDeC(D) = (1/N)∑1[Δ(x)<0], requiring only two forward passes per sample and gray-box access to token probabilities. The authors show near-perfect separation of seen vs. unseen datasets (dataset-level AUC ≈ 99.9%) across many models, and provide analyses on training dynamics, finetuning-induced contamination, and robustness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) Clear, elegant idea with strong intuition: leverage in-context learning as a probe—unseen datasets benefit from added context, while memorized datasets get perturbed—yielding a direct, interpretable signal. \n\n2) Practicality & efficiency: model- and dataset-agnostic; only two forward passes per sample; no threshold tuning or access to training corpora needed. \n\n3) Simple, interpretable metric: a percentage score (fraction of samples with negative Δ) that aligns with practitioner intuition and enables straightforward ranking across datasets. \n\n4) Strong empirical evidence: near-perfect dataset-level AUC over diverse models; analyses show early emergence of contamination in training, finetuning-induced contamination, and size trends (larger models memorizing less). \n\n5) Scope beyond strict membership inference: also detects contamination via related/shadow distributions, broadening practical utility for benchmark hygiene."}, "weaknesses": {"value": "1) Adversarial/degenerate datasets: repeated or highly heterogeneous mixtures can distort the score; the paper notes such edge cases but they remain a limitation for fully automatic use. \n\n2) Calibration to “absolute” labels: SCoDeC is excellent for ranking datasets by contamination risk, but stakeholders may still desire thresholded decisions (ACC/PR/F1). Adding a recommended thresholding recipe could help some users. \n\n3) Model-family anomalies: certain heavily instruction-optimized models behave atypically under CoDeC (e.g., chat/trace behaviors perturb logits broadly), suggesting architecture/task biases may require special handling."}, "questions": {"value": "1) Could you provide a small, principled thresholding guide (e.g., validated percentiles on held-out datasets) for users who need binary “contaminated/not” decisions in audits? \n\n2) How does SCoDeC behave under various context sizes n across different dataset lengths/formats? A brief cost–benefit curve would aid deployment. \n\n3) For instruction-tuned/chat-optimized models that yield universally high scores, can lightweight decoding settings (e.g., stop tokens, no-think modes) mitigate anomalies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8RMkD0uUdA", "forum": "YlpaaYxx4t", "replyto": "YlpaaYxx4t", "signatures": ["ICLR.cc/2026/Conference/Submission9574/Reviewer_xj5F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9574/Reviewer_xj5F"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9574/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977508875, "cdate": 1761977508875, "tmdate": 1762921126478, "mdate": 1762921126478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}