{"id": "kZHSvETWdi", "number": 16868, "cdate": 1758269684389, "mdate": 1763609523251, "content": {"title": "Towards Multimodal Data-Driven Scientific Discovery Powered by LLM Agents", "abstract": "Recent advances in large language models (LLMs) have enabled agents that automate scientific discovery by interpreting data, generating analysis pipelines, and executing them with computational tools. However, existing benchmarks remain largely limited to unimodal datasets and slice-level tasks, overlooking the fact that real discovery requires multimodal integration, modeling, and hypothesis-driven reasoning. To address this gap, we introduce MoSciBench, the first benchmark for multimodal scientific discovery, constructed from peer-reviewed studies through a principled four-stage pipeline. MoSciBench spans six scientific domains, seven data modalities, and five categories of discovery questions, yielding 88 individual, end-to-end, data-driven tasks. Each task is designed as a cross-modal hypothesis verification workflow, requiring agents to align and integrate heterogeneous datasets before modeling and reasoning. We further evaluate four representative agent frameworks across multiple LLM families. Results show that multimodal discovery is substantially harder than unimodal tasks: even the strongest agents achieve only 48.94\\% accuracy, with over 60\\% of failures due to cross-modal alignment. Lightweight workflow scaffolding consistently improves performance, reducing alignment errors by 5–10\\% and raising accuracy by 5.7\\% on average. Our benchmark and evaluation framework thus establish a rigorous testbed for advancing LLM agents toward realistic, multimodal scientific discovery.", "tldr": "", "keywords": ["Data-driven Scientific Discovery", "LLM Agent"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/90764cefbd5084afdccaf52488502dfd1cde34b6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This article designs a multi-model benchmark for data-driven scientific discovery, evaluating agents across five categories of discovery questions and seven data modalities. It also tests on state-of-the-art models, analyzes their performance, and proposes the “ReAct + Workflow” method to enhance agent effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  The core contribution of this paper is the introduction of MoSciBench, which is the first benchmark focused on evaluating LLM Agents in performing multimodal data-driven scientific discovery tasks.\n2.  MoSciBench itself covers five categories of discovery questions and seven data modalities, demonstrating good comprehensiveness."}, "weaknesses": {"value": "1. The article only shows that the performance of ReAct + Domain Knowledge deteriorates, but does not analyze why the performance worsens. Moreover, what would be the effect if Domain Knowledge and Workflow were added simultaneously?\n2. Based on reference [1] and the examples shown in the article, MoSciBench appears to be testing the model's ability to integrate various data for reasoning. However, according to Table 3, the performance of NoDataGuess is very poor; even with o4-mini, it hardly gets any answers correct. It would be beneficial to add an analysis regarding NoDataGuess.\n3. Figure 4 is the same as Figure 9. The text does not introduce Figure 4 and directly uses Figure 9 in line 318. The existence of Figure 4 seems to be meaningless.\n\n[1] Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles"}, "questions": {"value": "Table 6 only shows the results for React, but lines 658 - 660 mention that “the ReAct framework consistently outperforms all other methods for both Qwen3–235B and Qwen3–Coder,” which cannot be concluded from Table 6."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ezmxNWvWkq", "forum": "kZHSvETWdi", "replyto": "kZHSvETWdi", "signatures": ["ICLR.cc/2026/Conference/Submission16868/Reviewer_BRwp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16868/Reviewer_BRwp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761629093810, "cdate": 1761629093810, "tmdate": 1762926889698, "mdate": 1762926889698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MoSciBench, the first benchmark specifically designed for multimodal data-driven scientific discovery powered by LLM Agents. MoSciBench consists of 88 tasks, evaluated through a principled four-stage pipeline, which assesses agents on repository-level tasks that require alignment, modeling, and reasoning across seven data modalities and six scientific domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. MoSciBench is the first multimodal benchmark, covering 7 data modalities and 6 scientific domains. \n\n2. MoSciBench identifies the bottleneck of cross-modal alignment of  LLM agents in real-world multimodal scientific tasks. The author presents that over 30% of failures stem from misaligned data rather than flawed reasoning, which is a valuable insight.\n\n3. MoSciBench reveals the importance of data grounding in LLM-based scientific discovery. NoDataGuess approach performing close to 0% accuracy indicates that relying solely on LLMs’ internal knowledge is insufficient for solving scientific problems.\n\n4. By evaluating NoDataGuess, ReAct, Reflexion, and the proposed DataVoyager, the paper reveals critical limitations across current agent architectures."}, "weaknesses": {"value": "1. The baseline coverage is limited. Since all evaluated agents are prompt-based reasoning and code generation frameworks like ReAct and Reflexion, no domain task-specific Agents, multimodal-specific Agents, or retrieval-augmented Agents are included. \n\n2. While alignment errors are identified as the dominant failure mode, the root causes are not explored. Whether this arises from architectural limitations (e.g., lack of explicit alignment modules) or inherent model incapacity is not discussed. Without deeper mechanistic analysis and case studies, the paper does not provide further guidance beyond a generic suggestion of “better alignment is needed”.\n\n3. The evaluation relies exclusively on end-to-end exact match, which may conflate semantically correct solutions with fundamental failures. It is better to report a secondary metric (e.g., partial/subtask credit) or analysis for correct reasoning traces to better disentangle reasoning failure from execution failure.\n\n4. The 1-hour execution cap is not explained. Some failures may be due to time limits rather than methodological flaws.\n\n5. The paper includes several formatting and naming inconsistencies. e.g., in lines 278 and 646, it mixes the spelling of its self-proposing framework “DataVoyager” and “DataVoyage”. In line 295,  there is a missing space before “DeepSeek-V3.1”. In line 689, there is a missing space between the text and the period."}, "questions": {"value": "1. Could you provide a few concrete examples of alignment errors as case studies, and explain what kind of mechanism might cause them to happen?\n\n2. Have you tested whether a longer runtime significantly improves performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LMUWi8N96K", "forum": "kZHSvETWdi", "replyto": "kZHSvETWdi", "signatures": ["ICLR.cc/2026/Conference/Submission16868/Reviewer_mCwP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16868/Reviewer_mCwP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999512706, "cdate": 1761999512706, "tmdate": 1762926888820, "mdate": 1762926888820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MoSciBench, a benchmark for multimodal, data-driven scientific discovery powered by LLM agents. The benchmark includes six scientific domains, seven data modalities, and five discovery task types, totaling 88 tasks. The authors systematically evaluate several LLM-based agent frameworks and provide a detailed analysis of error sources and limitations. Overall, the paper addresses an important and underexplored problem — assessing AI agents in real-world, multimodal scientific workflows."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper introduces MoSciBench, a benchmark for multimodal, data-driven scientific discovery powered by LLM agents. The benchmark includes six scientific domains, seven data modalities, and five discovery task types, totaling 88 tasks. The authors systematically evaluate several LLM-based agent frameworks and provide a detailed analysis of error sources and limitations. Overall, the paper addresses an important and underexplored problem — assessing AI agents in real-world, multimodal scientific workflows."}, "weaknesses": {"value": "1. The paper tackles a meaningful and challenging goal — end-to-end scientific discovery from heterogeneous data sources — which is timely and relevant to the emerging intersection of AI agents and scientific reasoning.\n\n2. MoSciBench is well designed, covering a diverse set of domains and modalities. The data curation pipeline is clearly described and appears reproducible.\n\n3. The authors perform an insightful breakdown of error categories (alignment, modeling, reasoning), providing useful diagnostic information for the community."}, "questions": {"value": "1. Although the benchmark aims to emulate “scientific discovery,” most tasks are still formulated as structured, answerable queries with gold labels. The open-ended and hypothesis-generating aspects of real discovery are largely absent.\n\n2. Only a small set of existing agent frameworks are evaluated. It is unclear whether the conclusions generalize beyond the tested models.\n\n3. Reported accuracy numbers (≈50%) are low and not deeply analyzed beyond descriptive statistics. There is limited discussion of why certain domains are more difficult.\n\n4. For evaluation metrics, the reliance on exact-match accuracy is restrictive and might underestimate partial success or reasoning quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8I70bmXw5r", "forum": "kZHSvETWdi", "replyto": "kZHSvETWdi", "signatures": ["ICLR.cc/2026/Conference/Submission16868/Reviewer_Yaab"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16868/Reviewer_Yaab"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000336842, "cdate": 1762000336842, "tmdate": 1762926887148, "mdate": 1762926887148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MoSciBench, a multimodal benchmark designed for scientific discovery that enables agents to access complete repositories, integrate heterogeneous data, generate and execute code, and reason over results to verify scientific hypotheses. The experiments across 88 tasks reveal that cross-modal alignment is a significant bottleneck, while lightweight workflow scaffolding consistently enhances performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tUnlike previous unimodal benchmarks, MoSciBench explicitly targets multimodal, repository-level discovery, significantly increasing task complexity and realism. This benchmark will be valuable for evaluating the progress of AI agents within the community.\n2.\tThe experiments conducted provide valuable insights into enhancing agents in scientific domains, highlighting areas for further development."}, "weaknesses": {"value": "1.\tThe ground-truth hypotheses and answers in MoSciBench are derived from peer-reviewed publications. How rigorous is this benchmark? Additionally, if an agent can access search engines, how would that impact its ability to find answers?\n2.\tFor Figure 4, are there significant differences in error distributions among tasks with varying requirements? Which specific data models are particularly prone to failure, and is this related to the length or format of the data?\n3.\tDid the introduction of lightweight human workflow scaffolding change the proportion of model invocation tools/code execution used? Without this scaffolding, would models demonstrate a need for additional information or behave differently in their outputs?"}, "questions": {"value": "identical to the 'weaknesses'"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bggqf3alCb", "forum": "kZHSvETWdi", "replyto": "kZHSvETWdi", "signatures": ["ICLR.cc/2026/Conference/Submission16868/Reviewer_AMy8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16868/Reviewer_AMy8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006213614, "cdate": 1762006213614, "tmdate": 1762926886355, "mdate": 1762926886355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}