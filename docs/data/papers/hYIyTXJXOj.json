{"id": "hYIyTXJXOj", "number": 6869, "cdate": 1757998967826, "mdate": 1759897887076, "content": {"title": "Protein Autoregressive Modeling via Multiscale Structure Generation", "abstract": "We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. \nUsing the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. \nTo achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings.\nMoreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality.\nWe effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation.\nNotably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. \nOn the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior.\nTogether, these properties establish PAR as a promising framework for protein structure generation.", "tldr": "PAR is the first multi-scale autoregressive framework for protein backbone generation.", "keywords": ["Protein backbone generation", "Multi-scale autoregressive modeling", "Flow-based backbone decoder"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e82a0680ff06bf9bf1eb7b79347b3dc107e86173.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Protein Autoregressive Modeling (PAR), a multi-scale, coarse-to-fine framework for protein backbone generation. Instead of predicting the next residue, PAR predicts the structure at the next scale of resolution, from a coarse layout to the final backbone.\n\nThis approach cleverly avoids two major limitations of standard AR models in this domain: (1) it avoids coordinate discretization by using a continuous, flow-based atomic decoder (Proteina), and (2) it avoids unidirectional bias by modeling global-to-local dependencies. The authors also identify and mitigate \"exposure bias\" using noisy context learning (NCL) and scheduled sampling (SS). The model demonstrates strong unconditional generation (competitive FPSD) and notable zero-shot generalization for tasks like prompt-based shape generation and motif scaffolding."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This is a high-quality, well-written, and clear paper that successfully applies a multi-scale autoregressive (AR) framework, similar to VAR in image generation, to the complex task of protein structure generation. The coarse-to-fine \"sculpting\" idea is interesting, and the paper's main contribution is in demonstrating this new path for autoregressive modeling in this domain."}, "weaknesses": {"value": "However, the work is not without significant concerns. The methodology leans heavily on a large, pre-existing flow-based model (Proteina) as its core decoder, which makes it difficult to assess the true contribution of the AR-only components. Given the marginal improvements over the decoder baseline, the added complexity of the AR framework is not fully justified. The paper feels more like an incremental application of an existing algorithm to a new domain rather than a fundamental breakthrough."}, "questions": {"value": "1. **Reliance on Flow-Based Decoder**: The method is presented as an \"autoregressive model,\" but it heavily relies on a large, pre-trained flow-based model (Proteina) as its decoder. This blurs the lines of novelty. It feels less like a pure AR model and more like a complex, multi-scale conditioning scheme for a flow model.\n\n2. **Questionable Benefit**: Given the relatively limited improvement over the Proteina baseline (Table 1), what is the practical benefit of introducing this complicated multi-scale AR scheme? The experiments do not convincingly demonstrate that this hybrid AR-flow approach is substantially better than the flow-based model it is built upon.\n\n3. **Weaker Designability (sc-RMSD)**: While the model excels at global fold distribution (FPSD), its fine-grained designability (sc-RMSD) is a weakness. In Table 1, the PAR (400M) model's sc-RMSD of 1.28 is comparable to Proteina (1.09). \n\n4. For evaluating diversity, the average pairwise TM-score (Table 1) is not very discriminative. A more robust metric, such as reporting the number of Foldseek clusters (as done in the Proteina paper), would be more convincing.\n\n5. The interpretation of the attention maps (Fig. 6) concludes that each scale attends to the previous scale due to \"richer contextual information.\" However, this analysis fails to de-confound this with sequence length. Later scales correspond to much longer sequences, which would naturally receive larger attention weights. This confounding factor is not excluded from the analysis.\n\n6. The paper states that downsampling and upsampling are done via \"interpolation\" (Sec 3.1). What is the exact implementation of this? This detail is crucial for reproducibility.\n\n7. Motif Scaffolding Centering: The paper mentions superimposing the ground-truth motif (Sec 4.2). This superimposition operation may change the center of the motif and the coordinate frame of the entire structure. How is this change in centering handled by the subsequent autoregressive steps and the flow decoder, which are sensitive to the absolute coordinate system?\n\n8.  Since the flow-based decoder $v_{\\theta}$ is shared across all scales, can a model trained on a 3-scale configuration be used for inference with a 5-scale configuration (or vice-versa)? How \"agnostic\" is the trained model to the number of scales used at inference time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XRrc0fBsOS", "forum": "hYIyTXJXOj", "replyto": "hYIyTXJXOj", "signatures": ["ICLR.cc/2026/Conference/Submission6869/Reviewer_Zpd8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6869/Reviewer_Zpd8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761210234852, "cdate": 1761210234852, "tmdate": 1762919123315, "mdate": 1762919123315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Protein Autoregressive Modeling (PAR), a novel multi-scale, coarse-to-fine framework for protein backbone generation.\n\n**Key Contributions:**\n\n*   **Hierarchical Generation:** PAR generates structures by first creating a coarse topology and then refining it over multiple scales, a process analogous to sculpting.\n*   **Novel Architecture:** The model consists of three core components: (i) multi-scale downsampling operations, (ii) an autoregressive transformer for encoding information, and (iii) a flow-based backbone decoder.\n*   **Addresses Exposure Bias:** The authors mitigate a common issue in autoregressive models by employing noisy context learning and scheduled sampling, leading to more robust generation.\n*   **Strong Zero-Shot Generalization:** PAR can perform tasks like human-prompted conditional generation and motif scaffolding *without* needing to be fine-tuned, showcasing its flexibility and power."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Strengths:\n\n1.  The paper introduces a highly innovative approach by combining a **multi-scale autoregressive framework** with a **diffusion-based decoder** for protein backbone generation. This method allows for generating structures in a continuous coordinate space in a multi-scale manner.\n2.  The practical advantages of the multi-scale autoregressive model are compellingly demonstrated through the *\"Backbone generation with human prompt\"* and *\"Zero-shot motif scaffolding\"* applications. These examples effectively showcase the model's flexibility and its capacity for precise, user-guided generation."}, "weaknesses": {"value": "1.  **Lack of Analysis on Sampling Efficiency and Computational Cost:** A notable omission is the lack of analysis on sampling latency. While autoregressive (AR) models often present an advantage in generation speed over diffusion models in other domains, the PAR framework's architecture raises concerns. The model culminates in a diffusion decoder that appears computationally intensive (e.g., 1000 steps compared to Proteina's 400), and the preceding multi-scale AR stages introduce further computational overhead. This leads to a critical question: is PAR significantly slower than comparable end-to-end diffusion methods? Without this characterization, it is difficult to assess the practical trade-offs and viability of the proposed approach.\n\n2.  **Unclear Justification for the Autoregressive Framework's Contribution to Performance:** While the multi-scale AR framework is conceptually elegant, its empirical necessity is not fully substantiated by the results. The model introduces considerable architectural complexity, yet this does not translate into superior performance. In fact, as shown in *Table 1*, the model underperforms a strong diffusion baseline like Proteina on the key metric of Designability, while showing no significant advantages in other reported metrics. Moreover, the framework pairs a powerful diffusion decoder with a relatively low-parameter AR encoder, and the scaling experiments primarily seem to reflect the scaling properties of the diffusion component. This makes it difficult to disentangle the contributions of the novel AR framework from the strong performance of the underlying decoder. The manuscript would be substantially strengthened by an ablation study or analysis that clearly demonstrates the value added by the AR mechanism.\n\n3.  **Limited Demonstration on Longer Protein Chains:** The model's generalization capability is only demonstrated on proteins with lengths up to 256 residues, reflecting the scope of its training data. The generation of longer, multi-domain proteins is a crucial and challenging frontier for protein design. The performance of AR models, in particular, can degrade over long sequences due to error propagation. The paper would be more impactful if it included experiments on longer proteins or at least provided a thorough discussion of the potential challenges and limitations of scaling the PAR framework to these more complex and biologically relevant structures."}, "questions": {"value": "1.  Regarding the model's architecture, the current design utilizes a low-parameter autoregressive (AR) encoder with a powerful diffusion decoder that generates all tokens for a given scale simultaneously. Have the authors considered or experimented with an alternative architecture, perhaps more aligned with models like **MAR**, which employs a large-parameter AR model paired with a lightweight decoder that generates tokens sequentially (i.e., per-token) at each scale? Such a design might better leverage the known strengths of AR models in sequence modeling.\n\n2.  The proposed multi-scale framework relies on downsampling and upsampling operations that appear to be based on local windows in the 1D amino acid sequence. However, protein structures are defined by complex 3D relationships where residues that are distant in the 1D sequence can be close neighbors in 3D space. How does the current sequence-based sampling strategy ensure that these crucial non-local spatial relationships are effectively captured and preserved across different scales? Is there a risk of losing this vital information during the downsampling process?\n\n3.  The generation of protein backbones occurs directly in the continuous and high-dimensional coordinate space. In other domains, such as image generation, autoregressive modeling directly on the raw data space (e.g., pixel space) is often less effective than modeling in a compressed latent space (e.g., from a VAE or VQ-VAE), as latent spaces can be more dense and numerically stable. Could the authors comment on this choice? What is their perspective on the challenges of applying autoregressive generation directly to the sparse and numerically sensitive coordinate space of proteins, and was a latent-space generation approach considered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "we9ImUC2pH", "forum": "hYIyTXJXOj", "replyto": "hYIyTXJXOj", "signatures": ["ICLR.cc/2026/Conference/Submission6869/Reviewer_SoNE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6869/Reviewer_SoNE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761497950874, "cdate": 1761497950874, "tmdate": 1762919122599, "mdate": 1762919122599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Protein Autoregressive Modeling (PAR), the multi-scale autoregressive framework for protein backbone generation. PAR generates protein structures in a coarse-to-fine manner, progressively refining structural details across scales.\nTo address exposure bias and trainâ€“generation mismatches typical of autoregressive models, the authors introduce noisy context learning and scheduled sampling, enhancing robustness during backbone generation. \nEmpirically, PAR demonstrates strong zero-shot generalization, enabling conditional protein design and motif scaffolding without additional fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes the *first* multi-scale autoregressive model for protein backbone generation, integrating coarse-to-fine prediction within a single generative process.\n- The introduction of *noisy context learning* and *scheduled sampling* provides a principled way to mitigate exposure bias and mismatch between training and inference, a common challenge in autoregressive models.\n- PAR demonstrates strong *zero-shot generalization* and supports flexible conditional tasks such as motif scaffolding and human-guided protein design, showing versatility beyond unconditional generation."}, "weaknesses": {"value": "-  Although the proposed multi-scale autoregressive formulation is conceptually novel, the paper does not clearly demonstrate *quantitative or qualitative advantages* over existing diffusion-based protein generative models in terms of either *generation quality* or *generation efficiency*.\n- In the *zero-shot generalization* experiments, the paper mainly focuses on demonstrating conditional controllability (e.g., motif scaffolding) but does not evaluate the *designability* or *physical plausibility* of the generated structures. For motif scaffolding, comparisons with other *training-based scaffolding approaches* would be important to contextualize the results.\n- The paper reports ablations only on length versus ratio, while Figure 2 uses five scales and Table 1 uses three. There is no clear analysis or justification for how the number of scales influences performance, which is central to the proposed multi-scale formulation.\n- Since autoregressive and diffusion-based models have distinct sampling paradigms, *sampling time* is an important metric for practical evaluation. The paper does not report runtime comparisons or efficiency analyses, leaving uncertainty about the computational trade-offs of the proposed approach."}, "questions": {"value": "Q1   Intuitively, providing a hierarchical coarse-to-fine prior should make the final generation easier. However, the reported results suggest that PAR underperforms compared to diffusion models that directly generate the final-scale backbone. Could the authors clarify why the multi-scale autoregressive design does not yield stronger advantages in practice?\n\nQ2   The paper shows a case of zero-shot motif scaffolding involving a *continuous* motif segment. Can PAR also handle *discontinuous* or multi-segment motifs in a zero-shot setting? If not, what are the current limitations that prevent it?\n\nQ3   How is scheduled sampling concretely implemented during training? Does it require running two forward passes per iteration? If self-conditioning is also used, does that effectively multiply the computational cost (e.g., four forward passes per iteration)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lSbL1G6ccr", "forum": "hYIyTXJXOj", "replyto": "hYIyTXJXOj", "signatures": ["ICLR.cc/2026/Conference/Submission6869/Reviewer_8etu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6869/Reviewer_8etu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838398058, "cdate": 1761838398058, "tmdate": 1762919122185, "mdate": 1762919122185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Protein Autoregressive Modeling (PAR) is a multi-scale framework for protein backbone generation. PAR generates the coarse structure first then refines the structural details. PAR obtains competitive results to prior single scale methods, while also being able to structurally scaffold proteins without any finetuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- High technical novelty. The ability to define different granularities of the coarse structure to then refine is applicable to design tasks\n- Competitive performance. Demonstrates that the AR factorization of the problem works as desired without much loss of performance across any benchmarks.\n-Clear concise well written.\n- The fact that the AR process is done without explicit tokenization  driven through the CA flow loss is elegant."}, "weaknesses": {"value": "- No formal motif benchmark. Even if not SOTA it would be interesting to see.\n- No comparison of the inference speed"}, "questions": {"value": "- Do you use TriMul or just the attention pair bias Proteina architecture for the AR and flow components?\n-If one were to run PAR with just scale {L} what happens? Table 3 seems like the results are slightly harmed by more downsampling but curious if you basically get Proteina in the limit of doing no downsampling.\n- Does PAR hold to long length designability? Given its Proteina based the long length study could be extended here. it would be interesting to see if there is any value to being able to prompt a long length model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sJZyvmb1o7", "forum": "hYIyTXJXOj", "replyto": "hYIyTXJXOj", "signatures": ["ICLR.cc/2026/Conference/Submission6869/Reviewer_Knjj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6869/Reviewer_Knjj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947993972, "cdate": 1761947993972, "tmdate": 1762919121773, "mdate": 1762919121773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}