{"id": "up2LD7vVdW", "number": 23377, "cdate": 1758342808103, "mdate": 1759896818315, "content": {"title": "Do Vision Language Models Rotate in Mind? Evaluating Spatial Transformation Reasoning", "abstract": "Vision-language models (VLMs) have achieved impressive performance across diverse tasks, yet their ability to mentally perform spatial transformations—rotating, translating, and manipulating objects—remains poorly understood. We present \\textbf{TransformEval}, a systematic benchmark that evaluates spatial transformation reasoning across 2D shapes, 3D mental rotation, and multi-object scenes, uniquely distinguishing between \\textit{state prediction} (determining outcomes after transformations) and \\textit{transformation inference} (recovering operations from state changes). Our evaluation of state-of-the-art VLMs reveals fundamental limitations: models struggle with basic transformations that humans solve effortlessly, consistently perform better at inferring transformations than predicting their outcomes—opposite to human cognitive patterns—and frequently fail at translation operations and transformation ordering. These findings suggest that VLMs rely on pattern matching rather than mental simulation, lacking the spatial reasoning capabilities necessary for applications in robotics, augmented reality, and other domains requiring genuine spatial intelligence. Our benchmark provides a framework for measuring progress toward human-like spatial understanding in vision-language models.", "tldr": "Evaluate the spatial reasoning capabilities of VLMs and LLMs by introducing benchmarks that assess their understanding of geometric transformations, such as rotation and translation, across various tasks.", "keywords": ["vision-language models", "spatial reasoning", "multimodal reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00860bfd697f9d1c0c8708e9ebd3afbbfaa6e198.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses onVLMs’ ability to perform mental spatial transformations (e.g., rotation, translation) by introducing TransformEval, a comprehensive benchmark designed to evaluate such reasoning across three hierarchical tasks: 2D shape transformations (using asymmetric shapes on a 10×10 grid with compound rotation/translation operations), 3D mental rotation (using cube-chain objects with sequential axis rotations), and multi-object scene transformations (cluttered scenes with targeted object manipulations). TransformEval distinguishes between state prediction and transformation inference, two directions that reveal a striking asymmetry. Results show that VLMs consistently outperform at inference than at prediction, which directly contrasts with human cognitive patterns where forward simulation is easier. Overall, VLMs perform far below humans, with frequent failures in translation operations and transformation ordering, indicating that current VLMs rely on pattern matching rather than human-like mental simulation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Well-motivated Benchmark Design: This benchmark fills a critical gap in existing spatial reasoning evaluations. Unlike prior benchmarks focusing solely on static spatial relationships, TransformEval targets dynamic spatial transformation reasoning (e.g., mental rotation, sequential manipulation). It covers hierarchical tasks (2D shape → 3D mental rotation → multi-object scenes) and introduces a bidirectional evaluation paradigm that has state prediction vs. transformation inference, enabling precise diagnosis of VLMs’ true understanding rather than superficial pattern matching. The inclusion of controlled distractors (e.g., missed translation, wrong rotation order) further supports fine-grained error analysis.\n\nInsightful Finding: The discovery of the \"inference-prediction asymmetry\" (VLMs excel at inverse transformation inference but struggle with forward state prediction, opposite to humans) reveals a fundamental difference between VLMs’ pattern-matching mechanism and human-like mental simulation."}, "weaknesses": {"value": "The TransformEval benchmark, while targeted, suffers from excessive simplification that weakens its ability to reflect complex real-world spatial reasoning scenarios. All tasks rely on artificially rendered content: 2D tasks use simple asymmetric shapes (L/T/F) on a fixed 10×10 grid, 3D tasks use rigid \"cube-chain\" objects with no textural or structural complexity, and multi-object scenes are based on stylized Super-CLEVR renders (not natural images). This simplification avoids real-world noise but also means VLMs’ poor performance here may not fully translate to, or overstate, their weaknesses in practical scenarios. \n\nTransformation types are overly narrow. The benchmark only evaluates rotation and translation, ignoring common real-world spatial operations like scaling, shearing, reflection, or non-rigid deformation (e.g., folding paper, bending a wire, which are critical for applications like CAD design or robot-assisted packaging. \n\nTask format is static and passive. All evaluations use a 4-choice static image comparison, with no interactive or dynamic assessment (e.g., asking VLMs to simulate step-by-step transformations). This fails to capture the iterative, dynamic nature of human spatial reasoning.\n\nThe paper does not verify whether key design choices (e.g., task hierarchy, distractor types) affect results. For example, it does not test if simplifying 3D rotation to single-axis (instead of sequential multi-axis) improves VLMs’ performance, or if changing grid size in 2D tasks impacts translation error rates. Without such ablations, it is hard to confirm whether the benchmark’s complexity aligns with its goal of diagnosing spatial weaknesses, or if results are skewed by specific task parameters.\n\nThe study uses a fixed prompt template for all models but does not explore how prompt engineering (e.g., adding spatial cues like \"track the object’s center coordinate\" or breaking down transformations into steps) affects performance. Moreover, the model only allows to output a choice. Thus we cannot exactly know in depth the reason why models fail.\n\nWhile it identifies common errors (e.g., 46.1% of 2D errors relate to translation), it only speculates on causes (e.g., \"Transformer attention is position-invariant\") without empirical validation. For example, it does not test if modifying a model’s position encoding (e.g., adding absolute position embeddings) reduces translation errors, or if fine-tuning on translation-focused data mitigates the issue."}, "questions": {"value": "1. Do VLMs struggle more with non-rigid transformations than rigid rotation/translation, and does this vary by application context (e.g., CAD vs. robotics)?\n\n2. Since all evaluations use a static 4-choice image-comparison format, to what extent does an interactive, dynamic assessment paradigm (e.g., prompting VLMs to generate step-by-step visualizations of transformations, or adjust an object’s position/rotation in real time via text commands) change measured performance? \n\n3. Could such a paradigm better capture the iterative, trial-and-error nature of human spatial reasoning (e.g., assembling furniture) and reveal hidden capabilities in VLMs that static tasks miss? For instance, do VLMs perform better at breaking down complex transformations (e.g., \"rotate 90° then translate 2 units\") into sequential steps when prompted to explain each stage, compared to making a single static prediction?\n\n4. How does simplifying 3D rotation from multi-axis (sequential X/Z-axis) to single-axis affect VLMs’ accuracy, and does this reveal whether \"rotation order insensitivity\" is a fundamental limitation?\n\n5. Would adjusting 2D grid size (e.g., from 10×10 to 5×5 or 20×20) reduce or exacerbate translation errors, and does this suggest that VLMs’ struggle with translation stems from limited spatial working memory or poor absolute position tracking?\n\n6. Would allowing open-ended outputs (e.g., asking VLMs to describe why they chose a candidate, or to calculate the object’s final coordinates) reveal more granular error mechanisms (e.g., \"confused x/y axes\" vs. \"forgot translation entirely\") than the current binary choice format?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "feEJZqqNvJ", "forum": "up2LD7vVdW", "replyto": "up2LD7vVdW", "signatures": ["ICLR.cc/2026/Conference/Submission23377/Reviewer_q3yr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23377/Reviewer_q3yr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953930193, "cdate": 1761953930193, "tmdate": 1762942633918, "mdate": 1762942633918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces, TransformEval, a benchmark that tests whether VLMs can truly \"mentally\" rotate/ or translate objects and compose those transforms, using a bidirectional setup (predict end state and infer transforms) across 2D shapes, 3D mental rotation, and multi-object scenes. The authors find that state-of-the-art VLMs lag far behind humans, show an odd asymmetry (better at inference than prediction), and often fail on translation and order sensitivity, which would be an evidence of pattern matching rather than genuine spatial simulation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The paper proposes a new benchmark to assess how VLMs perform implicit \"mental simulation\" to infer the output of 3D transformations.\n\n* Based on the evaluation results on the proposed benchmark, the authors present an anlaysis on the limitation of current VLMs on the given tranformation tassks."}, "weaknesses": {"value": "* The quality and completeness of the paper is clearly low, and is not at the state of submission. The readabilty is extremely low due to excessive use of bullet points, typos (e.g., line 310), and the lack of structure in the writing. \n\n* The proposed dataset seems trivial, given that there exist mulitple previous works that propose similar or richer datasets that include tasks that require 3D mental rotations. Here I outline some critical missing related work that should be cited and discussed in order to claim novelty for TransformEval:\n\n  * Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models, Wang et al., CVPR 2025 \n\n  * 3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark, Ma et al., ICCV 2025\n\n  * SITE: towards Spatial Intelligence Thorough Evaluation, Wang et al., ICCV 2025\n\n  * Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities, Zhang et al., ICLR 2025\n\n  * ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models, Li et al., 2025\n\n  * RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics, Song et al., CVPR 2025\n\n  * Spatial Mental Modeling from Limited Views, Yin et al., 2025\n\n  * OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models, Jia et al., 2025\n\n  * Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations, Li et al., 2025\n\n* The anlayses presented in Section 6.1 seem to lack evidence and novelty. For instance, the claim \"This architectural bias may explain why they can identify ”this looks like a 90-degree rotation” but cannot generate the resulting state\" is unclear. Why would the current VLM archicture be unable to solve this issue? The claim is not supported by empirical evidence. Moreover, \"may instead rely on learned associations between transformation descriptions and visual patterns\" is also ambiguous and is not sufficiently supported by evidence.\n\n* The anlayses in Section 6.2 are also trivial and lack precise descriptions. For instance, \"Transformer architectures use position-invariant attention patterns that may inherently struggle with translation\" is confusing, as recent VLMs using 2D absolute position encoding or 2D RoPE would be aware of the relative positions between objects. Could the authors elaborate more on this issue?\n\n* Moreover, predicting the outcome of spatial transformations would be closely related to temporal reasoning based on video inputs, since they both require predicting or understanding the temporal dynamics from the input observations. Discussions on the difference between the given task and the video understanding task would help grasp the novelty of the paper's problem definition."}, "questions": {"value": "* The current dataset is mostly simple abstract shapes. Would there be an option to extend the data construction pipeline to include realistic images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concern."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "buNeYLVcCn", "forum": "up2LD7vVdW", "replyto": "up2LD7vVdW", "signatures": ["ICLR.cc/2026/Conference/Submission23377/Reviewer_PbzV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23377/Reviewer_PbzV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983556568, "cdate": 1761983556568, "tmdate": 1762942633709, "mdate": 1762942633709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Inspired by the classic Shepard-Metzler mental rotation experiments in cognitive psychology, this paper proposes a new benchmark called TransformEval for evaluating the dynamic spatial transformation reasoning abilities of vision-language models (VLMs). The core innovation of this benchmark lies in its \"bidirectional evaluation\" framework, which separately tests the models' abilities in \"state prediction\" (forward reasoning) and \"transformation inference\" (backward reasoning).\nThe experimental results reveal a fundamental flaw in current state-of-the-art VLMs (including GPT-4o and Gemini-2.5-Pro): they perform significantly worse than humans on these tasks. More importantly, the authors discovered a key \"state-transformation asymmetry\" phenomenon: VLMs perform better in inferring transformations backward than in predicting outcomes forward — which is the exact opposite of human cognitive patterns."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel & Clever Methodology: The most significant highlight of this paper is the \"bidirectional evaluation\" (state prediction vs. transformation inference). This design is highly ingenious, transcending the mere reporting of accuracy figures and instead providing a mechanism to diagnose why VLMs fail and how they internally process information (pattern matching vs. mental simulation).\n2. Rigorous Benchmark Design:\n         ---Hierarchical Complexity: The tasks progress systematically from 2D shapes to 3D rotations and then to complex multi-object scenes. This hierarchical increase in difficulty is rational.\n         ---Controlled Distractors: The incorrect options in the multiple-choice questions are carefully designed to represent specific failure types (e.g., \"ignoring translation,\" \"sequence error\"). This makes the error analysis in Section 5.2 (Table 2) insightful.\n\n3. Clear & Counter-Intuitive Finding: The core finding of the paper—the \"state-transformation asymmetry\"—is very clear and thought-provoking. The phenomenon that VLMs perform better in inference than in prediction, which is counter to human cognition, provides strong evidence for the argument that \"VLMs are just advanced pattern matchers.\" The discussion in Section 6.1 is in-depth."}, "weaknesses": {"value": "**1. The real challenge of \"state prediction\":**  \nA more rigorous and realistic test of \"state prediction\" should be *generative* in nature — for example, requiring the VLM to draw the transformed shape or to output the coordinates of the transformed object's keypoints.  \nThe current multiple-choice (MCQ) format may underestimate the true gap between prediction and inference in VLMs.\n\n**2. Lack of creativity in task design:**  \nThe tasks show limited originality and can largely be traced back to the following existing works:  \n[1] *Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models*  \n[2] *11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis*  \n[3] *SpatialViz-Bench: An MLLM Benchmark for Spatial Visualization*\n[4] *Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models*"}, "questions": {"value": "**1. On the evaluation format (MCQ vs. generative)\":**  \nI wonder whether the authors have considered adopting a generative evaluation protocol to assess “state prediction.”\nFor instance, instead of using a multiple-choice format, the model could be asked to output the vertex coordinates of the transformed shape.\nDo the authors expect that, under such a generative evaluation, the current findings—particularly the observed asymmetry—would remain consistent, or would they become even more pronounced?\nAn even more ambitious direction might involve using a unified MLLM such as bagel to directly generate images representing the predicted post-transformation state.\nIf the authors explore this avenue and uncover new insights, that would significantly strengthen the contribution. As it stands, there are already many spatial reasoning benchmarks for VLMs, and some degree of methodological novelty would make this work stand out.\n\n**2. On the source of the observed “asymmetry:**  \nThe reported “state–transformation asymmetry” is intriguing. Could this effect stem from training data biases?\nFor example, large-scale datasets used to train VLMs may contain abundant co-occurrences of (S, S′) image pairs and transformation terms (T), which could make inference (classification) tasks easier.\nIn contrast, prediction (generation) tasks may require the model to generalize beyond the observed distribution and reason about unseen states.\nIt would be valuable if the authors could discuss how the characteristics of the training data (e.g., from LAION or similar sources) might contribute to this specific asymmetry."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yNSZ8cxlLC", "forum": "up2LD7vVdW", "replyto": "up2LD7vVdW", "signatures": ["ICLR.cc/2026/Conference/Submission23377/Reviewer_ZZxL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23377/Reviewer_ZZxL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987593480, "cdate": 1761987593480, "tmdate": 1762942633502, "mdate": 1762942633502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TransformEval, a new benchmark designed to evaluate the spatial transformation reasoning abilities of VLMs. The authors argue that existing benchmarks focus on static spatial relationships, whereas their work, inspired by mental rotation in cognitive science, tests the dynamic ability to \"mentally rotate\" and manipulate objects. The benchmark's main contributions are: \n - A hierarchical set of tasks: From 2D to 3D to scene-level.\n - A bidirectional evaluation paradigm: Both forward and backward evaluation.\n - Compositional analysis\nEvaluating state-of-the-art VLMs, the paper finds that they perform far below human levels. The key finding is a \"state-transformation asymmetry\": models are consistently better at inferring transformations (backward) than predicting their outcomes (forward). This is the opposite of human cognitive patterns, where forward simulation is easier. \nThe error analysis shows models systematically fail at translation and transformation ordering. The authors conclude that VLMs rely on \"pattern matching\" rather than human-like \"mental simulation\"."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Forward and Backward Evaluation: The paper's strongest contribution is its distinction between forward (state prediction) and backward (transformation inference) reasoning. This is a highly insightful and novel way to probe VLM capabilities, and also highly related to the internal world model in VLMs. \n\n2. Principled, Cognition-Grounded Benchmark: The work is well-grounded in classic cognitive science, explicitly building on the Shepard & Metzler paradigm. The task design is systematic, with a clear hierarchy of complexity and carefully controlled distractors that enable a detailed error analysis."}, "weaknesses": {"value": "1. Overstatement of Cognitive Claims: The strong claim that VLMs \"lack mental simulation\" in favor of \"pattern matching\" is an overstatement. While the state-transformation asymmetry is consistent with this hypothesis, it is not definitive proof, and the paper lacks direct analysis to substantiate this cognitive-level conclusion. A direct analysis can be like: giving two samples with the same action sequence, and provide the model with one of the sample, ask it to match another.\n\n2. Insufficient Dataset Scale: The dataset size of 300 examples per task is too small, especially for a synthetic benchmark where scaling is straightforward. A larger dataset (e.g., >1k examples per setting) is needed for more robust and reliable conclusions.\n\n3. Missing SOTA Model Evaluation: The evaluation is missing key state-of-the-art VLMs released months ago, specifically GPT-5 and InternVL 3.5. Their omission limits the paper's conclusions about the capabilities of current frontier models.\n\n4. Missing Related Work: The paper omits recent related work on multi-view spatial reasoning. Furthermore, relying exclusively on synthetic data is a limitation; using or comparing against real-world multi-view datasets would significantly strengthen the evaluation's generalizability.\n\n5. Limited Novelty of the Task: The novelty of investigating mental rotation is limited. This problem has already been explored in several existing benchmarks (e.g., SPACE, SpatialViz-Bench, Omnispatial), which are not sufficiently contrasted to highlight this paper's unique contribution beyond the bidirectional evaluation paradigm."}, "questions": {"value": "1. What is the performance of more recent SOTA models, specifically GPT-5 and InternVL 3.5 (also include Qwen3-VL if possible), on the benchmark?\n\n2. Human mental rotation reaction time scales linearly with the rotation angle. Does VLM accuracy show any similar correlation? For instance, do models perform significantly worse as the rotation angle increases?\n\n3. Regarding the human performance baseline: How many participants were involved in the study, and what was their inter-annotator agreement or consistency score?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ESGri2Q5pd", "forum": "up2LD7vVdW", "replyto": "up2LD7vVdW", "signatures": ["ICLR.cc/2026/Conference/Submission23377/Reviewer_mgrU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23377/Reviewer_mgrU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014842992, "cdate": 1762014842992, "tmdate": 1762942633274, "mdate": 1762942633274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}