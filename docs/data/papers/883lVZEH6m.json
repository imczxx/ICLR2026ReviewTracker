{"id": "883lVZEH6m", "number": 4371, "cdate": 1757668089456, "mdate": 1759898036467, "content": {"title": "MXSens: Sensitivity-Aware Mixed-Precision Quantization for Efficient LLM Inference", "abstract": "4-bit quantization enables efficient LLM inference, but suffers from significant accuracy degradation due to outliers. Prior work addresses this problem via data rotation or mixed-precision integer quantization, but often relies on software-managed scaling and frequent dequantization, incurring substantial overhead. Microscaling formats, such as MXINT, eliminate these inefficiencies by encoding scales in hardware, yet remain incompatible with rotation-based methods. Our analysis reveals that outliers vary in severity, from rare extremes to frequent mild deviations, and that quantization sensitivity is unevenly distributed across layers and columns. These insights motivate a fine-grained, sensitivity-guided approach. We introduce MXSens, a training-free method that assigns mixed mantissa bitwidths (4/6/8) based on column- and layer-wise sensitivity, naturally leveraging the block-wise structure of MXINT. MXSens outperforms state-of-the-art quantization methods across a range of models and tasks. Under the W4A4KV4 setting, MXSens achieves perplexities of 3.77 and 7.63 on LLaMA-2-70B and LLaMA-3-8B, respectively, substantially improving over existing baselines on WikiText-2. Our work establishes a new balance between accuracy and resource efficiency for LLM quantization.", "tldr": "", "keywords": ["quantization", "model compression", "4-bit inference", "efficient llm inference", "microscaling", "MX formats"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/843e5c747d3554731973b2bb0fcf7396c28627ba.pdf", "supplementary_material": "/attachment/be55be71cfb0b2517a7440b8d641a9c579242633.zip"}, "replies": [{"content": {"summary": {"value": "In this work, the authors introduce MXSens, in which they use the diagonal Hessian values for determining the sensitivity of each column, and assign outlier columns with higher precision, such as 8-bit or 6-bit. Essentially, the precision is mixed across layers and also within layers, which makes MXSens a fine-grained approach. However, the reviewer is particularly concerned with its hardware compatibility and novelty."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Well-explained approach with clear motivation and algorithmic code."}, "weaknesses": {"value": "My major concern for this work is its novelty. \n\n+ Mixed-precision quantization has been a well-explored topic, with the first work (HAQ) published in 2018. In this work, the authors used a standard bit-search strategy to assign the bitwidth across layers. The outlier column, with higher precision, will gain significant benefit as shown in SpQR. After reading the methodology, one can already expect the experimental results. \n\n+ A different motivation from work seems to be the mixed-precision on microscaling formats. However, the reviewer raised significant concerns about this design choice. Indeed, microscaling formats are supported on hardware, but that does not mean mixed-precision microscaling formats are supported. I can imagine huge kernel design efforts are needed when implementing your formats on Blackwell GPUs. In your W4A4 microscaling formats, the key is that each tile can perform matmul for each scaling. But with mixed-precision, each tile is not synchronous. \n\n+ Hessian diagonal does not seem to be a reasonable metric for your mixed-precision, as the weight quantization error also affects the overall layer output error. Moreover, Slim-LLM has proposed other metrics to allocate mixed-precision for different columns. \n\n+ In current LLM literature, perplexity has become less important. I'd like to see reasoning task accuracy and MMLU results to evaluate the method's effectiveness."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GOGkHqDdrg", "forum": "883lVZEH6m", "replyto": "883lVZEH6m", "signatures": ["ICLR.cc/2026/Conference/Submission4371/Reviewer_DuvT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4371/Reviewer_DuvT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760920385137, "cdate": 1760920385137, "tmdate": 1762917321206, "mdate": 1762917321206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the mixed-precision quantization for LLMs. The background is that different columns/layers matter differently to the quantization. It uses the per-layer Hessian information to identify the precision of each column and assign different portions of higher-bits to different layers according to their sensitivity to the end-to-end loss. It also explores the utilization of microscaling data format to support effective quantization. It presents the evaluation to demonstrate its efficacy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. It focuses on a very important topic, the compression of LLMs.\n2. The presentation is very clear."}, "weaknesses": {"value": "1. The idea is not new, e.g., the sensitivity identification of the columns within each layer is exact the same with Atom [1]\n2. It has missed the discussion and evaluation of some highly related mixed-precision quantization work, e.g., SliM-LLM [2], MixLLM[3]\n3. It has overclaimed the contribution, e.g., assigning different portion of bits for different layers is already explored in MixLLM [3]\n4. It has extensively discussed the microscaling format, but there is no noval contribution based on this data format. This is just to make use of a data format.\n\n[1] Atom: Low-Bit Quantization for Efficient and Accurate LLM Serving\n\n[2] SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models\n\n[3] MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design"}, "questions": {"value": "The mixed-precision quantization is extensively studied in recent years. This paper does not provide new insights upon the state-of-the-art.\n- It could be better to compare this work with the recent state-of-the-art, like SliM-LLM and MixLLM mentioned above.\n- Section 3.1 uses the same method with Atom. You can check the code of Atom to confirm it. The Atom team just did not describe this detail in the paper, but have the details in the opensourced code.\n- Using different bit-width for different layers is not new, e.g., MixLLM has already proposed this idea (even more fine-grained) and an end-to-end solution. The claim in this paper is not correct: \"Unlike prior methods that apply uniform or layer-wise fixed precision\".\n- Does this paper have any noval technique for microscaling format? It seems this paper only has utilized this data format, but does not have new contribution besides just using it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6wNrbwE9Sz", "forum": "883lVZEH6m", "replyto": "883lVZEH6m", "signatures": ["ICLR.cc/2026/Conference/Submission4371/Reviewer_SpFs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4371/Reviewer_SpFs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891660696, "cdate": 1761891660696, "tmdate": 1762917320927, "mdate": 1762917320927, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MXSens, a training-free, sensitivity-guided mixed-precision quantization framework designed for microscaling formats (MXINT). The approach combines column-wise (using hessian) and layer-wise (using model error after quantization) sensitivity  to allocate 4, 6, and 8-bit precisions efficiently, enabling compatibility with hardware accelerators that support block-wise scaling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to follow, well-structured, and includes meaningful ablations on bitwidth configurations.\n- **Fine-grained, sensitivity-aware allocation:** Using Hessian-based sensitivity at both column and layer levels is methodologically sound and yields strong empirical performance.\n- **Controlled precision budget:** formulating a fixed average bitwidth while improving accuracy is a useful engineering contribution."}, "weaknesses": {"value": "### Weaknesses & Questions\n\n1. **Limited novelty of sensitivity metric:**\n    \n    The use of the Hessian diagonal for quantization sensitivity has been well explored in works like **HAWQ-V2** and **APTQ**. The novelty here mainly lies in its adaptation to microscaling rather than the metric itself. The paper should clarify this distinction explicitly.\n    \n2. **Missing baselines:**\n    \n    Discussion and comparisons exclude recent methods such as **SpinQuant**, **FlatQuant**, **OSTQuant**, **KurTail and DartQuant**.\n    \n3. **Lack of comparison with more recent mixed-precision methods:**\n    \n    State-of-the-art approaches like **ResQ** show significant gains over **QUIK**. Including at least ResQ would strengthen claims of superiority.\n    \n4. **lack of ablation study on design parameters:**\n    \n    The hard-coded choice of “top 32 columns per layer → MXINT8” is only loosely justified by block size. An ablation on this or exploration of adaptive thresholds would be interesting.\n    \n5. **Hardware feasibility concerns:**\n    \n    The paper assumes minimal overhead when columns have heterogeneous bitwidths (4/6/8). How are such columns grouped or stored to avoid bandwidth and alignment penalties?\n    \n    To better clarify, different columns could have different number of bit(4,6,8 bits). Isn’t it necessary for all columns with the same bitwidth to be grouped together for the hardware implementation? Further discussion or empirical hardware profiling is needed\n    \n6. **Rotation incompatibility not well explained:**\n    \n    The claim that rotation “hurts” MXINT quantization due to group-wise asymmetry is mentioned but not analyzed experimentally and at least intuitively in worse case, it should have the same performances as int4. A brief explanation or intuitive justification would clarify this point.\n    \n- HAWQ-V2: Hessian Aware trace-Weighted Quantization of Neural Networks, NeurIPS 2020.\n- APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models, (arXiv preprint) 2024\n- SpinQuant: LLM Quantization with Learned Rotations, arXiv/I CLR (pre-print) 2024\n- FlatQuant: Flatness Matters for LLM Quantization, ICML 2025\n- ResQ: Mixed-Precision Quantization of Large Language Models with Low-Rank Residuals, ICML 2025\n- KurTail: Kurtosis-based LLM Quantization, ICLR Workshop (SLLM) 2025\n- OSTQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting, (arXiv/venue) 2025\n- DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization, NeurIPS 2025"}, "questions": {"value": "Check weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rq9ohN0UMe", "forum": "883lVZEH6m", "replyto": "883lVZEH6m", "signatures": ["ICLR.cc/2026/Conference/Submission4371/Reviewer_DufF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4371/Reviewer_DufF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962608033, "cdate": 1761962608033, "tmdate": 1762917320626, "mdate": 1762917320626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}