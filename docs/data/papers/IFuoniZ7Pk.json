{"id": "IFuoniZ7Pk", "number": 13267, "cdate": 1758215830914, "mdate": 1763454182488, "content": {"title": "You Are What You Train: Rethinking Training Data Quality, Targets, and Architectures for Universal Speech Enhancement", "abstract": "Universal Speech Enhancement (USE) aims to restore the quality of diverse degraded speech while preserving fidelity. Despite recent progress, several challenges remain. In this paper, we address three key issues. (1) In speech dereverberation, the conventional use of early-reflected speech as the training target simplifies model training, but we found that it still harms perceptual quality. We therefore apply time-shifted anechoic clean speech as a simple yet more effective target. (2) Regression models preserve fidelity but produce over-smoothed outputs under severe degradation, while generative models improve perceptual quality but risk hallucination. We provide theoretical analysis and introduce a two-stage framework that effectively combines the strengths of both approaches. (3) We study the trade-off between training data scale and quality, a critical factor when scaling to large, imperfect corpora. Experimental results demonstrate that using time-shifted anechoic clean speech as the learning target significantly improves both speech quality and downstream automatic speech recognition (ASR) performance, while the two-stage framework further boosts quality without compromising fidelity.  In addition, our model demonstrates strong language-agnostic capability, making it well-suited for enhancing training data in other speech generative tasks. To ensure reproducibility, the code will be made publicly available\nupon acceptance of the paper. Several enhanced real noisy speech examples are provided on the demo page: \\url{https://anonymous.4open.science/w/USE-5232/}", "tldr": "", "keywords": ["Universal Speech Enhancement", "Speech Generation", "Fidelity and Quality", "Training Data Quality"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5409c76cf9467ec67955cd13862fa1504f43394.pdf", "supplementary_material": "/attachment/c5337c01b2a1122179f8fd361c481399dea6b7a6.zip"}, "replies": [{"content": {"summary": {"value": "The paper investigates the causes of performance degradation in universal speech enhancement (SE) methods. It considers alignment mismatch between input and target recordings, compares regression-based and generative approaches to SE, and investigates the effects of low-quality training data on the final model's performance."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper correctly identifies key areas that crucially affect the performance of SE methods."}, "weaknesses": {"value": "The paper has the following weaknesses: \n1. The paper claims that aligning target and input audio recordings improves the overall performance of the model. This is not a new observation; e.g. [2] also aligns target and input audios for training. Moreover, this observation is quite obvious, since any impulse response used for augmentation introduces a time shift that is known and can be manually corrected relatively simply. Therefore, the observation that using $s[n-n_0] = s[n] \\ast \\delta[n - n_0]$ yields better results is not a novelty.\n\n2. The authors discuss the benefits and downsides of using regression-based and generative methods for SE. They argue that using generative modelling can help reduce over-smoothing, yet preserve the fidelity. **Firstly**, the problem of over-smoothing is already well-studied, among others, in [1, 2, 3]. Other GAN-based methods deal with over-smoothing using pre-training with regression-based pre-training and adversarial fine-tuning. It is not clear from the paper why training a separate generative model is better than applying fine-tuning. **Secondly**, the authors claim to provide a theoretical argument based on equation (3) in the paper that links the proposed method to the optimal transport. However, the presented argument lacks proper rigour. The conclusion \"... *the generative model can mainly focus on correcting the over-smoothed regions of the regression model output*\" is substantiated only with an intuitive explanation and lacks a formal proof. **Thirdly**, the provided analysis assumes that the SE method is based on adversarial training, which excludes a large body of work [4, 5, 6, 7] that uses diffusion-based and bridge methods for SE. It is unclear how the conclusions from the paper can be applied to these methods.\n\n3. The authors observe that the URGENT 2025 Challenge training dataset contains some recordings that degrade the performance of the SE model. Although potentially useful for challenge participants, this observation, in my opinion, constitutes only a marginal contribution. Moreover, to fully measure the effects of the degraded recordings, it would be beneficial to train various models -- both GAN-based and diffusion-based -- on the original and cleaned data. That would show that the impact of the degraded data is significant; otherwise, the loss in quality might be attributed to architectural inefficiencies and training setup"}, "questions": {"value": "How can the analysis of the trade-off between the generative and regression-based paradigms be generalised to other types of SE methods, such as bridge models or diffusion-based models?\n\n#### **References:**\n\n[1] Andreev et al., \"HiFi++: a unified framework for bandwidth extension and speech enhancement\".\n\n[2] Babaev et al., \"FINALLY: fast and universal speech enhancement with studio-like quality\".\n\n[3] Su et al., \"HiFi-GAN-2: studio-quality speech enhancement via generative\".\n\n[4] Lemercier et al., \"StoRM: a diffusion-based stochastic regeneration model for speech enhancement and dereverberation\".\n\n[5] Scheibler et al.,  \"Universal score-based speech enhancement with high content preservation\".\n\n[6] Jukíc et al., \"Schrödinger bridge for generative speech enhancement\".\n\n[7] Wang et al., \"Diffusion-based Speech Enhancement with Schrödinger Bridge and Symmetric Noise Schedule\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xIvAmvCygW", "forum": "IFuoniZ7Pk", "replyto": "IFuoniZ7Pk", "signatures": ["ICLR.cc/2026/Conference/Submission13267/Reviewer_CvYq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13267/Reviewer_CvYq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763615104, "cdate": 1761763615104, "tmdate": 1762923942089, "mdate": 1762923942089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates three approaches that aim at improving the training of universal speech enhancement (USE) systems. These three approaches are (a) enforcing higher quality training data by curating files with a high VQScore, (b) employing unreverberated training targets instead of such including early reflections, and (c) a two-stage training framework that combines discriminative and generative architectures. The authors show that a smaller, but higher quality dataset improves performance, and that non-reverberated targets as well as their two-stage approach raise scores on non-intrusive and task-dependent metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The author’s train of thought is well described and easy to follow. The paper compares with both state of the art and open-source models on a publicly available test set. Furthermore, the authors did state their intent to publish code, which would ensure reproducibility of the presented results."}, "weaknesses": {"value": "While the paper is at first sight well written and presents interesting results, there are multiple shortcomings. A major issue lies with the novelty and presentation of the discussed three “critical aspects”:\n\n1.\tTraining targets: There are two issues with the authors’ claim here. Firstly, the assumption that the use of slightly reverberated targets is due to the difficulty of removing them is disputable, especially since the authors themselves contradict that argument by presenting models trained on (time-shifted) anechoic speech as superior in performance. The main reason for maintaining early reflections in the targets is that they are regarded as beneficial to speech intelligibility [Bradley 2003]. Secondly, the anechoic clean speech approach in Figure 3 (b) without consideration of the direct path delay is no sensible approach to begin with. The proposed solution with time-shifted targets (or vice-versa, removing direct path delay in the room impulse response) is not novel, but should rather be the standard for any reasonable training employing anechoic targets.\n[Bradley, et al. “ On the importance of early reflections for speech in rooms,” 2003.]\n\n2.\tModel architecture: The results in Table 1 are unconvincing. Employing the GAN correction degrades more metrics than it improves. If the authors had adopted the overall ranking from URGENT, as they did with the other metrics, these models would have fallen behind their regression-only counterparts. Furthermore, improving non-intrusive metrics, which cannot detect hallucinations, is not surprising for a generative model. Only the slight improvement in CAcc seems interesting. Regarding novelty, combining regression and generative stages is nothing new and has been done in much more sophisticated ways before (see UNIVERSE++).\n[Scheibler et al., “Universal Score-based Speech Enhancement with High Content Preservation”, 2024.]\n\n3.\tTraining data quality: No novelty at all. This is basically the same which was already investigated in more detail and with a more sophisticated data curation strategy in the paper by [Li 2025], which was even cited by the authors. Furthermore, why would the authors rely solely on the seemingly not entirely suitable VQScore when samples of the dataset could be out of domain for this evaluation as mentioned in Section 2.3 (e.g., expressive speech)?\n[Li et al., “Less is More: Data Curation Matters in Scaling Speech Enhancement”, 2024.]\n\nFurthermore, the presentation of results is questionable. Why would the authors present a table with 12 metrics, only to argue that 8 of them cannot be considered for comparison due to different training targets? The complete lack of comparable intrusive metrics greatly reduces the significance of the results. A subjective degradation category rating (DCR) listening test between the “Early reflected” and “Shifted anechoic” approaches could have helped."}, "questions": {"value": "Minor remarks:\n-\tSection 2.2: missing article before bold phrases (twice)\n-\tFonts in figures are often too small\n-\tSome figures are just screenshots; labels contain compression artifacts\n-\tFigure 1: output -> Output\n-\tTable 3 would be more conclusive if the output of the models regression stage would also be reported\n-\tConsistently missing capitalizations in references (e.g., line 513: Ecapa-tdnn – ECAPA-TDNN, line 518: Icassp -> ICASSP, line 636: perceptual –> Perceptual)\n-\tInconsistent formatting of references, e.g., line 603 Rix [Rix et al.] vs line 673 [Zhao et al.] – both are ICASSP papers\n-\tFigures 6 and 7 are missing axis descriptions (time, frequency range)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bV8lKZtuY2", "forum": "IFuoniZ7Pk", "replyto": "IFuoniZ7Pk", "signatures": ["ICLR.cc/2026/Conference/Submission13267/Reviewer_UWo4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13267/Reviewer_UWo4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764190820, "cdate": 1761764190820, "tmdate": 1762923941480, "mdate": 1762923941480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of universal speech enhancement, which is to restore the quality of diverse degraded speech while preserving fidelity. To address the problem that the use of early-reflected speech as the training target harms perceptual quality, the authors consider time-shifted anechoic clean speech as the target. To address the problem that regression models preserve fidelity but produce over-smoothed outputs while generative models improve perceptual quality but risk hallucination, the authors introduce a two-stage training approach that first trains a regression model and then freezes it to train a generative model. Provided experiment results demonstrated the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method of using time-shifted anechoic clean speech as training target and the two-stage training approach is interesting, and the provided experimental results demonstrated the effectiveness of the proposed method."}, "weaknesses": {"value": "The novelty of this paper is somewhat limited. The contributions of this paper seems incremental in system design rather than proposing fundamentally new ideas. The use of time-shifted anechoic clean speech as the training target is an incremental modification of existing practices. The two-stage training approach is not new, similar approaches have been explored in both speech and image enhancement, e.g., prior work such as [1] also consider such a two-stage training approach for speech enhancement.\n\nWhile the authors claim to provide a theoretical analysis of the two-stage framework (in Abstract), this is not substantiated. Section 2.2 mostly summarizes conclusion from prior work rather than presenting any new theoretical results. In particular, eq. (3) is quite trivial and does not offer any deeper understanding of why or how the two-stage approach works. Therefore, it is difficult to consider this as a theoretical contribution.\n\n[1] Huang J, Yan Z, et al. A Two-Stage Training Framework for Joint Speech Compression and Enhancement. arXiv preprint arXiv:2309.04132, 2023."}, "questions": {"value": "See the above Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2WXCnScsMK", "forum": "IFuoniZ7Pk", "replyto": "IFuoniZ7Pk", "signatures": ["ICLR.cc/2026/Conference/Submission13267/Reviewer_43Aa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13267/Reviewer_43Aa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977608859, "cdate": 1761977608859, "tmdate": 1762923941119, "mdate": 1762923941119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of the Contributions of This Paper"}, "comment": {"value": "We thank all reviewers for their valuable comments and have uploaded the revised paper with more audio examples in our demo page. \n\nIn summary, this paper aims to encourage the related community to **rethink** some conventional practices that may not be optimal. By adopting **better learning targets** and **improving training data quality,** and by clarifying the roles of regression and generative models through theoretical support for achieving a **better distortion–perception tradeoff**, our simple yet effective model achieves **state-of-the-art performance** across several metrics in the URGENT Challenge benchmark.\n\n1.\t**Training target analysis**: We demonstrate that **time-shifted anechoic clean** speech is a more suitable training target for speech dereverberation than conventional **early-reflected speech.** While some prior works have used time-shifted anechoic speech as a target, to the best of our knowledge, **none** have explicitly discussed or compared these two target types. **Since many existing speech dereverberation studies still adopt early-reflected speech as the learning target, we aim to highlight the importance of this choice.**\n\n2.\t**Two-stage framework design:** Recent studies (see the Model Architecture Part in the Introduction section of our paper) have **heuristically** explored combining regression and generative models in various ways, but without theoretical support. Motivated by the recent theoretical finding that **“optimally transporting the posterior mean prediction to the true data distribution can achieve a better tradeoff between quality and fidelity,”** we adopt a simple two-stage framework based on this principle and show it can better **keep fidelity** than other combining methods.\n\n3.\t**Training data quality**: We want to draw the community’s attention to the importance of training data quality, particularly for widely used datasets that are often **assumed to be “clean”** (e.g., Libri-TTS, DNS5, etc.). As illustrated in Figure 8, models trained on such data struggle to remove artifacts like electrical microphone hiss, underscoring the need for higher-quality training data."}}, "id": "mgEyfLsmzW", "forum": "IFuoniZ7Pk", "replyto": "IFuoniZ7Pk", "signatures": ["ICLR.cc/2026/Conference/Submission13267/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13267/Authors"], "number": 11, "invitations": ["ICLR.cc/2026/Conference/Submission13267/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763453828894, "cdate": 1763453828894, "tmdate": 1763453828894, "mdate": 1763453828894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}