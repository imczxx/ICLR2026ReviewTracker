{"id": "zfuGXx4059", "number": 15895, "cdate": 1758256741316, "mdate": 1759897274727, "content": {"title": "Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning", "abstract": "Action-dependent policies, which condition decisions on both states and other agents' actions, provide a powerful alternative to independent policies in multi-agent reinforcement learning. Most existing studies have focused on auto-regressive formulations, where each agent's policy depends on the actions of all preceding agents. However, this approach suffers from severe scalability limitations as the number of agents grows. In contrast, sparse dependency structures, where each agent relies only on a subset of other agents, remain largely unexplored and lack rigorous theoretical foundations. To address this gap, we introduce the action dependency graph (ADG) to model sparse inter-agent action dependencies. We prove that action-dependent policies converge to solutions stronger than Nash equilibria, which often trap independent policies, and we refer to such solutions as $G_d$-locally optimal policies. Furthermore, within coordination graph (CG) structured problems, we show that a $G_d$-locally optimal policy attains global optimality when the ADG satisfies specific CG-induced conditions. To substantiate our theory, we develop a tabular policy iteration algorithm that converges exactly as predicted. We further extend a standard deep MARL method to incorporate action-dependent policies, confirming the practical relevance of our framework.", "tldr": "", "keywords": ["action-dependent policy", "multi-agent reinforcement learning", "global optimality", "coordination graph"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/95f033132fb7011d407e3602135e2fb381457081.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper considers the problem of optimizing action-dependent policies in MAMDPs where the global Q-value can be decomposed by a coordination graph (which can be policy- and state-dependent). The paper contributes a sufficient condition (regarding the action dependency graph) for the action-dependent policy to ensure the global optimality, along with a tabular algorithm to optimize such action-dependent policies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is exceptionally well-written with clear use of notions and illustrative diagrams (Figures 1-4) that help understanding.\n\n\nThe paper reveals an important link between coordination graphs and action-dependent policies, which has been a missing piece in the cooperative MARL literature."}, "weaknesses": {"value": "1. While the clarity is overall good, there is room for improvement:\n    - *Any* deterministic policy can be decomposed into independent policies as  $\\pi(s)=(\\pi_1(s),\\ldots,\\pi_n(s))$ can therefore some statements in the first two paragraphs of Section 4.1 are somewhat sloppy. \n    - The notation used in (8) and (9) is hard to digest, which could benefit with more explanations and examples. Without understanding (8), it’s hard to understand (10).\n\n2. A reader might appreciate a discussion on (the difficulty of) extending the idea and techniques to stochastic policies.\n\n\n3. In Figure 5, it’s unclear how sparse the graphs generated by Algorithm 2 are. It’s unclear in general what kind (e.g., how spare) of graphs are generated by Algorithm 2 and how to let them satisfy (11)."}, "questions": {"value": "All my concerns are in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ooykXtKe9P", "forum": "zfuGXx4059", "replyto": "zfuGXx4059", "signatures": ["ICLR.cc/2026/Conference/Submission15895/Reviewer_4W8F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15895/Reviewer_4W8F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761764913397, "cdate": 1761764913397, "tmdate": 1762926112104, "mdate": 1762926112104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work formulates the notion of a Gd-locally optimal policy, which characterizes convergence behaviour of action-dependent policies, and unifies coordination graphs with action-dependent policies to derive optimality conditions. They define a policy iteration algorithm for sparse action dependency graphs that guarantees convergence to Gd-optimal policies under certain conditions. The authors implement sparse and dense ADGs for coordination games and demonstrate their viability in both a dynamic programming setting and deep setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The formulation of action-dependent policies and action dependency graphs is clear.\n2. Algorithm 1 is clear and understandable.\n3. The example in Section 4.2 is helpful in understanding the relationship between independent policy learning and local equilibrium convergence. \n4. The coordination polymatrix games clearly elucidate why ADGs are preferable to CGs.\n5. The proofs in the Appendix and main text appear correct and complete. The proof by induction in Appendix D is particularly well-explained.\n6. The ATSC benchmark examines the performance of ADGs and clearly demonstrates the usefulness of action dependency in this setting.\n7. The logarithmic cost of running dense ADGs is clearly demonstrated in Figure 6 (right), motivating sparse ADGs further."}, "weaknesses": {"value": "1. Lack of baselines - only the proposed method was benchmarked in this work. Why were other forms of action dependency not benchmarked? [1]. The viability of ADGs needs to be framed within the context of action-dependent policies and MARL in general - while the latter can be achieved by a simple ablation with regular MAPPO, it should be clearly demonstrated why action-dependency proposed is superior to message passing, autoregressive action dependency, or action memory. \n2. Lack of environments in the deep setting - to fully demonstrate the viability of ADGs in coordination tasks, additional environments such as [2] should be considered. Why was ATSC the only grid environment considered for the deep setting?\n3. The statistical significance of the results is unclear. What are the shaded areas in Figures 5,6, and 7? \n\n[1] Wei Fu, Chao Yu, Zelai Xu, Jiaqi Yang, and Yi Wu. Revisiting some common practices in cooperative\nmulti-agent reinforcement learning. In International Conference on Machine Learning, pp. 6863–\n6877. PMLR, 2022.\n\n[2] Wang, Tonghan, et al. \"Context-aware sparse deep coordination graphs.\" arXiv preprint arXiv:2106.02886 (2021)."}, "questions": {"value": "1. Can a MARL algorithm other than MAPPO be incorporated into the ADG setting? \n2. Can ADGs be used for competitive or team-based settings?\n3. Is there known reason why the payoffs for empty ADGs settle on those particular suboptimal values in Figure 5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iLZLqYg6f3", "forum": "zfuGXx4059", "replyto": "zfuGXx4059", "signatures": ["ICLR.cc/2026/Conference/Submission15895/Reviewer_yfqy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15895/Reviewer_yfqy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935484803, "cdate": 1761935484803, "tmdate": 1762926111376, "mdate": 1762926111376, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Action Dependency Graph (ADG) framework to formalize structured dependencies among agents in multi-agent reinforcement learning. The authors introduce a sufficient condition under which a policy that is locally optimal with respect to a directed acyclic dependency graph (ADG) is also globally optimal with respect to the underlying coordination graph (CG). Building on this result, they develop an Action-Dependent Multi-Agent Policy Iteration (AD-MPI) algorithm and prove its finite-step convergence to a $G_d$-locally optimal (and, under certain conditions, globally optimal) policy. A greedy construction method for sparse ADGs is also proposed to reduce computational complexity while preserving optimality guarantees. Empirical results on coordination games and traffic signal control tasks show that sparse ADGs match the performance of dense graphs."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a clear and rigorous theoretical analysis establishing when $G_d$-local optimality implies global optimality.   \n- The theoretical results are technically sound, with well-defined assumptions and consistent notation across sections.  \n- The paper is well written and organized, with smooth logical flow from definitions to theorems and experiments.\n- How to utilize a sparse coordination structure is an important problem in multi-agent reinforcement learning."}, "weaknesses": {"value": "- The experimental evaluation is limited to small-scale and largely toy environments, which do not convincingly demonstrate the framework’s scalability or practical relevance.  \n- No comparisons are made against standard MARL baselines (e.g., QMIX[1], MAPPO[2], and other Sequential or Bayesian Network-based methods[3,4]), making it unclear whether ADG-based methods provide empirical advantages beyond theoretical guarantees.\n- The experiments focus on verifying theorems rather than exploring performance under realistic stochasticity, partial observability, or dynamic coordination graphs.  \n- There is no ablation on how the choice of the dependency structure or greedy ordering affects performance or computational cost.\n\n[1] Rashid, T., Samvelyan, M., De Witt, C. S., Farquhar, G., Foerster, J., & Whiteson, S. (2020). Monotonic value function factorisation for deep multi-agent reinforcement learning. Journal of Machine Learning Research, 21(178), 1-51.\n\n[2] Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A., & Wu, Y. (2022). The surprising effectiveness of ppo in cooperative multi-agent games. Advances in neural information processing systems, 35, 24611-24624.\n\n[3] Ye, Jianing, Chenghao Li, Jianhao Wang and Chongjie Zhang. “Towards Global Optimality in Cooperative MARL with the Transformation And Distillation Framework.” (2022).\n\n[4] Wen, Muning, Jakub Grudzien Kuba, Runji Lin, Weinan Zhang, Ying Wen, J. Wang and Yaodong Yang. “Multi-Agent Reinforcement Learning is a Sequence Modeling Problem.” ArXiv abs/2205.14953 (2022): n. pag."}, "questions": {"value": "1. The paper motivates the study by stating that sequential decision making is computationally expensive. However, there is no quantitative comparison of computational efficiency against any existing MARL baselines or even against standard decentralized methods. Could the authors provide empirical evidence supporting the claimed computational advantage of ADG-based policy iteration?\n\n2. The experiments are confined to toy coordination games and small ATSC grids, without evaluation on standard MARL benchmarks such as SMAC, Google Research Football, or MPE. Have the authors considered testing ADG-based methods on these widely used benchmarks, or are there limitations preventing such evaluations?\n\n3. Since the proposed framework is conceptually concise and relies on a predefined DAG structure, I am curious whether a fixed DAG could generalize to complex and dynamic environments (and how much will it affect the performance/computational cost, compared to other sequential decision making methods like [3,4]). If not, could the authors discuss possible extensions for learning or adapting the DAG structure online, rather than fixing it a priori?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HSDcottoyV", "forum": "zfuGXx4059", "replyto": "zfuGXx4059", "signatures": ["ICLR.cc/2026/Conference/Submission15895/Reviewer_o4Q3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15895/Reviewer_o4Q3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972096871, "cdate": 1761972096871, "tmdate": 1762926110951, "mdate": 1762926110951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduces Action Dependency Graphs (ADGs) as a formal mechanism to model sparse inter-agent action dependencies in multi-agent RL (MARL). Particularly, the proposed approach advances the field by rigorously studing sparsity over the dependencies of an agent's policy over the actions of the other agents. The authors demonstrate with both theoretical and empirical evidence that the proposed approach can converge to stronger decentralized agents, while being interoperable with current MARL algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- the authors make a relevant contribution by filling the gap both theoretically and empirically on the usage of action-dependent policies in MARL, which is an open relevant setting in the community.\n- the paper is clearly written and technically sound. It is relatively easy to follow.\n- the authors propose a method that is scalable because it can be integrated into existing MARL algorithms"}, "weaknesses": {"value": "- The experimental evaluation is limited to simple toy cooperative MARL tasks. I believe the paper could provide a much more relevant contribution to the field if tested on notorious problems such as SMACv2 [1] or MaMuJoCo [2].\n\n[1] Ellis, Benjamin, et al. \"Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 37567-37593.\n\n[2] Peng, Bei, et al. \"Facmac: Factored multi-agent centralised policy gradients.\" Advances in Neural Information Processing Systems 34 (2021): 12208-12221."}, "questions": {"value": "- Could you please provide quantitative metrics on the benefits of sparse ADGs vs. dense ADGs when these have comparable performance? E.g. do you get faster inference time or more efficient computational needs?\n- Could you clarify why the experimental evaluation did not include more complex and recent benchmarks such as SMACv2 or MaMuJoCo?\n- How robust is the overall algorithm w.r.t. the choice of index order?\n- Does the assumption of having to define an index order limit the applicability of the method at all? Are there any tasks where such order may not be imposed due to the nature and constraint of the task itself? E.g. when actions must occur syncronously."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "x7sxaUaHW8", "forum": "zfuGXx4059", "replyto": "zfuGXx4059", "signatures": ["ICLR.cc/2026/Conference/Submission15895/Reviewer_KgtT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15895/Reviewer_KgtT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997724174, "cdate": 1761997724174, "tmdate": 1762926110494, "mdate": 1762926110494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}