{"id": "mYP33u1QBK", "number": 8742, "cdate": 1758096744383, "mdate": 1763636059598, "content": {"title": "Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle", "abstract": "Reinforcement learning (RL) has emerged as an effective post-training paradigm for enhancing the reasoning capabilities of multimodal large language model (MLLM). However, current RL pipelines often suffer from training inefficiencies caused by two underexplored issues: Advantage Collapsing, where most advantages in a batch concentrate near zero, and Rollout Silencing, where the proportion of rollouts contributing non-zero gradients diminishes over time. These issues lead to suboptimal gradient updates and hinder long-term learning efficiency. To address these issues, we propose Shuffle-R1, a simple yet principled framework that improves RL fine-tuning efficiency by dynamically restructuring trajectory sampling and batch composition. It introduces (1) Pairwise Trajectory Sampling, which selects high-contrast trajectories with large advantages to improve gradient signal quality, and (2) Advantage-based Trajectory Shuffle, which increases exposure of valuable rollouts through informed batch reshuffling. Experiments across multiple reasoning benchmarks show that our framework consistently outperforms strong RL baselines with minimal overhead. These results highlight the importance of data-centric adaptations for more efficient RL training in MLLM.", "tldr": "We propose Shuffle-R1, a simple and effective RL post-training framework that significantly improves RL training efficiency and model performance.", "keywords": ["multimodal large language model", "reinforcement learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3723472cc2282388c7aae6309426a0344852d54c.pdf", "supplementary_material": "/attachment/7b5e328ca9dc91d8d40775c7c3765b9391944e24.zip"}, "replies": [{"content": {"summary": {"value": "This paper identifies two key inefficiencies in reinforcement learning (RL) fine-tuning for multimodal large language models (MLLMs): Advantage Collapsing—where most advantage estimates cluster near zero, weakening gradient signals—and Rollout Silencing—where fewer rollouts contribute useful gradients as training progresses. To address these, the authors propose Shuffle-R1, a data-centric RL framework featuring: (1) Pairwise Trajectory Sampling, which forms high-contrast trajectory pairs by matching high- and low-advantage rollouts to amplify informative signals, and (2) Advantage-based Batch Shuffle, which dynamically reshuffles training batches to prioritize high-value samples based on their advantage magnitudes. Experiments show Shuffle-R1 consistently outperforms strong baselines (e.g., GRPO, DAPO) across multiple multimodal reasoning benchmarks, achieves competitive results with leading closed-source models like GPT-4o and Claude-3.7, and even matches prior methods using only half the training steps—all with minimal computational overhead. The framework also generalizes to text-only LLMs, highlighting its broad applicability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "It demonstrates high quality through rigorous experiments across model scales, datasets, and multimodal benchmarks, supported by thorough ablations and efficiency analyses.\nThe writing is clear: concepts are intuitively explained, methods are well-structured, and figures effectively illustrate key ideas."}, "weaknesses": {"value": "Unfair rollout budget: The method uses 16 rollouts per query but only trains on 8, while baselines like DAPO likely use only 8 total. The gains may come from more exploration, not smarter sampling. A fair comparison—using the same total number of rollouts (e.g., 16 for both)—is missing. Also, as N grows large, discarding (1−α) samples wastes potentially useful signals.\n\nResampling vs. reweighting: The paper uses advantage-based resampling (ABS), but reweighting the loss by advantage magnitude would achieve nearly the same effect with zero extra overhead and less variance. No justification or comparison is provided for choosing the more complex resampling approach.\n\nMarginal gains vs. added cost: Improvements over strong baselines (e.g., DAPO, GSPO) are small—often <1% on average—yet the method adds rollout generation cost (2× more rollouts) and pipeline complexity. If the real benefit is faster convergence (e.g., 2× fewer steps), that should be the highlighted advantage, not final accuracy. As-is, the practical value is unclear."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w8yUc6ecJM", "forum": "mYP33u1QBK", "replyto": "mYP33u1QBK", "signatures": ["ICLR.cc/2026/Conference/Submission8742/Reviewer_u5Bq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8742/Reviewer_u5Bq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760933843303, "cdate": 1760933843303, "tmdate": 1762920534763, "mdate": 1762920534763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Shuffle-R1, a data-centric reinforcement learning framework that targets two major issues in current RL fine-tuning pipelines: Advantage Collapsing (most advantages cluster near zero) and Rollout Silencing (the share of rollouts with non-zero gradients keep dropping).\n\nThe method introduces two simple modules: Pairwise Trajectory Sampling, which selects high-contrast trajectory pairs to strengthen gradient signals, and Advantage-based Batch Shuffle, which dynamically reshuffles batches to reuse more informative samples.\n\nExperiments across multimodal reasoning benchmarks show consistent gains over GRPO, DAPO, and GSPO with almost no extra computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper pinpoints two concrete and observable issues, \"Advantage Collapsing and Rollout Silencing\", which intuitively explain why current RL pipelines waste computation and fail to leverage informative signals. This diagnostic perspective is well-motivated.\n\n2. Instead of modifying the reward model or policy objective, Shuffle-R1 improves RL efficiency purely from the data side through Pairwise Trajectory Sampling (PTS) and Advantage-based Batch Shuffle (ABS). Both modules are lightweight, easy to implement, and can plug into existing frameworks without architectural changes. And the proposed framework shows consistent improvements across different datasets, model scales, and reasoning benchmarks with less training steps and GPUs.\n\n3. The paper presents clear ablation studies and analyses showing how PTS mitigates advantage collapse and how ABS maintains token utilization over time. This makes the method’s effectiveness both transparent and reproducible."}, "weaknesses": {"value": "1. **Overfitting to high-advantage samples**\nSince ABS repeatedly exposes high-value trajectories, the framework might bias the model toward a narrower distribution of “reward-dense” samples, reducing exploration and long-term diversity. \n\n2. **Scope of benchmarks**\nMost experiments are on math or visual reasoning tasks; while results are strong, these domains already have dense reward signals. It remains unclear whether Shuffle-R1 would bring similar benefits on tasks with sparse or noisy rewards (e.g. open-ended QA, safety...)\n\n3. **Lack of comparison to recent adaptive-sampling paradigms**\nRecent works like LIMO also tackle signal efficiency through adaptive data selection and contrastive training. Including these would further enhance impact within the community."}, "questions": {"value": "Overall, this paper provides a well-motivated and practically useful data-centric perspective on improving RL efficiency for multimodal LLMs, there are several questions:\n1. Since ABS repeatedly exposes high-advantage trajectories, how does the framework prevent over-exploitation of a narrow subset of rollouts? \n\n2. The experiments are convincing but domain specific. Does the author expect the same efficiency gains for RL tasks with sparse, delayed, or non-verifiable rewards (e.g., open-ended QA)? If not, what modifications would be necessary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pNYVYqAHSb", "forum": "mYP33u1QBK", "replyto": "mYP33u1QBK", "signatures": ["ICLR.cc/2026/Conference/Submission8742/Reviewer_gi6w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8742/Reviewer_gi6w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694927378, "cdate": 1761694927378, "tmdate": 1762920534344, "mdate": 1762920534344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Shuffle-R1 is a data-centric RL fine-tuning method for multimodal LLMs that combats Advantage Collapsing and Rollout Silencing. It introduces Pairwise Trajectory Sampling to extract high-contrast advantage pairs and Advantage-based Batch Shuffle to over-sample valuable pairs during mini-batch construction. On 2–30 k training samples, 3/7 B models outperform GRPO, DAPO, GSPO and match GPT-4o/Claude-3.7 on MathVerse, MathVista, etc."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Diagnose Advantage Collapsing & Rollout Silencing in MLLM-RL; proposes contrastive pairing + advantage-weighted reshuffle instead of larger rollouts or reward re-design.\n- Extensive ablations (α, S, PTS variants), 8 datasets, 2 model scales; statistical gains significant; extend to LLMs; code & pseudo-code provided."}, "weaknesses": {"value": "- While the empirical results are strong, the paper lacks formal analysis or theoretical justification for why PTS and ABS improve training dynamics. For example, it would be helpful to show (even intuitively) how contrastive sampling improves gradient variance or convergence rates.\n- While Shuffle-R1 outperforms GRPO, DAPO, and GSPO, it does not compare with other data-centric RL methods such as curriculum-based sampling, which are relevant to the idea of reusing or reweighting data. A short discussion or comparison would strengthen the positioning of the work.\n- While ablations on α and S are provided, other design choices (e.g., max-min pairing, absolute advantage weighting) are not thoroughly explored. For example, would cosine similarity or entropy-based weighting perform better?"}, "questions": {"value": "- Have you empolyed Shuffle-R1 on larger models (e.g., 30B+ parameters) or other domains beyond math and vision reasoning? What are the anticipated challenges?\n- Can you provide examples where Shuffle-R1 underperforms or fails to improve over baselines? What are the limitations of your method in its current form?\n- Can you provide a more formal or intuitive explanation of how PTS improves gradient estimation? For example, how does selecting high-contrast pairs reduce gradient variance or improve signal-to-noise ratio?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W7V4QDGe7g", "forum": "mYP33u1QBK", "replyto": "mYP33u1QBK", "signatures": ["ICLR.cc/2026/Conference/Submission8742/Reviewer_Codo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8742/Reviewer_Codo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998040797, "cdate": 1761998040797, "tmdate": 1762920533838, "mdate": 1762920533838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Shuffle‑R1, a data-centric RL training wrapper for multimodal LLMs that addresses advantage collapsing and rollout silencing by pairing high-contrast trajectories and shuffling batches based on advantage magnitude. The method is simple, practical, and empirically effective, showing improved accuracy and efficiency over strong RL baselines on math and vision-language benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of the paper are:\n\n- Simple, practical method that is easy to implement on top of GRPO, which addresses a real pain point in RL fine-tuning: many trajectories are statistically uninformative.\n- Results are strong and it shows effectiveness on the good coverage of in-domain and out-of-domain benchmarks.\n- Detailed experiments with ablation studies."}, "weaknesses": {"value": "The weaknesses of the paper are:\n\n- Theoretical analysis of bias/variance under selective sampling is limited; unbiasedness is not proven.\n- Missing some clarifications and ablation study"}, "questions": {"value": "1. While steps and batch sizes are reported, it is not fully clear that all baselines (GRPO/DAPO/GSPO/RLOO/Reinforce++) were run with identical token budgets, rollout counts (2N=16), and decode temperatures. Small differences can swing math benchmarks materially. Is it possible to add a compute-matched table and a training curve wall-clock plot to substantiate the \"7% overhead\" claim across settings?\n\n2. HallusionBench results improve, but do PTS/ABS reduce refusal or increase over‑assertion? Any calibration metrics or abstention analysis?\n\n3. How does the pair selection affect solution diversity? Any evidence of collapse in reasoning styles (e.g., fewer distinct CoT patterns)?\n\n4. Are there tasks where advantage distributions are already well‑spread (e.g., dense/step rewards), making PTS/ABS less helpful? Negative results would help practitioners choose whether to use the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bffaTuUFvE", "forum": "mYP33u1QBK", "replyto": "mYP33u1QBK", "signatures": ["ICLR.cc/2026/Conference/Submission8742/Reviewer_5QmF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8742/Reviewer_5QmF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762219394521, "cdate": 1762219394521, "tmdate": 1762920533258, "mdate": 1762920533258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response"}, "comment": {"value": "Dear reviewers, ACs and PCs:\n\nWe sincerely appreciate your reviews and valuable feedback. We have responded each comment individually in the main response. Below, we summarize the key modifications in the revised paper and provide clarifications for concerns raised by multiple reviewers.\n\n## Modifications of revised paper\n\nWe have added a detailed training curve wall-clock plot and discussion in Section 4.2\n\nWe have conducted a full setting 30k-data experiment on Qwen2.5-VL-7B to demonstrate the scalability of Shuffle-R1. Results are reported in Appendix C.\n\nWe have added additional theoretical analysis of proposed algorithm. An approximated training dynamics analysis and bias analysis in Appendix H.1 and H.2.\n\nThe structure of Section 4 and Appendix is adjusted for added content.\n\n## Common Concerns and Questions\n\nBoth reviewer 5QmF and Codo request for additional theoretical analysis. We have presented the full analysis process, including propositions and proofs in Appendix H.1 and H.2. We only provide key conclusion in the main response for conciseness.\n\nReviewer 5QmF, Codo and gi6w request for broader discussion beyond math/visual reasoning tasks or sparse outcome reward. We include a representative study on Referring Expression Comprehension (REC), a subset of Visual Grounding task with IoU-based soft rewards. Following VLM-R1’s setup, we train Qwen2.5-VL-3B on 60k samples randomly selected from RefCOCO/RefCOCOg/RefCOCO+ for 500 steps, and report accuracy on corresponding test sets.\n\nWe use the widely adopted prompt for REC: \"Please provide the bounding box coordinate of the region this sentence describes: {QUERY} Output the answer in pixel coordinates in the format of [x1, y1, x2, y2]. Minimum value is 0 and maximum value is the width/height of the image. Output the answer in JSON format.\" \n\nNote: the base model uncontrollably outputs boxes in different formats (absolute, 0-1 relative, or 0-1000 normalized). We convert relative boxes to absolute coordinates during evaluation, and treat normalized bboxes were treated as failure because they cannot be reliably detected. This leads to lower accuracy compared with the Qwen2.5 technical report, and results are not directly comparable to VLM-R1 due to reduced training data.\n\nFor open-ended QA and wider tasks/scenarios, they can be either: (1) framed as outcome-reward RL (using rule based reward functions, or LLM judges); or (2) framed as token-wise process reward RL (using reward model). Since application on these topics primarily concern reward function/reward model design, rather than improving current RLVR paradigms, both cases are beyond the scope of the paper. Consequently, we did not include them in the response, and we hope the reviewers would understand."}}, "id": "SkxTvpCU2s", "forum": "mYP33u1QBK", "replyto": "mYP33u1QBK", "signatures": ["ICLR.cc/2026/Conference/Submission8742/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8742/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission8742/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763639067394, "cdate": 1763639067394, "tmdate": 1763639103934, "mdate": 1763639103934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}