{"id": "UMUYpeXtJQ", "number": 21779, "cdate": 1758321660125, "mdate": 1759896903616, "content": {"title": "Post-Hoc Reasoning in Chain-of-Thought: Evidence from Pre-CoT Probes and Activation Steering", "abstract": "Chain‑of‑thought (CoT) can improve performance in large language models (LLMs) but does not always accurately represent a model's decision process. Prior work has shown one way CoT may be unfaithful is via \\textit{post-hoc reasoning}, where the model pre-commits to an answer before generating CoT. We extend this line of inquiry by exploring \\emph{mechanisms} of post-hoc reasoning in five language models (Gemma 2: 2B, 9B; Qwen 2.5: 1.5B, 3B, 7B) and four binary question answering tasks (Anachronisms, Logical Deduction, Social Chemistry, Sports Understanding). We first show that the model already knows its answer before the CoT, by linearly decoding it from residual stream activations at the last pre‑CoT token obtaining an area under the ROC curve (AUC) above 0.9 across most tasks and all models. We then show the model actually uses this representation by steering activations along the learned direction during generation, which causes the model to change its answer in more than $50\\%$ of originally‑correct examples in most model--dataset pairs. Finally, under steering we classify structured CoT pathologies, finding \\emph{confabulation} (false premises supporting the steered answer) and \\emph{non-entailment} (true premises with a non sequitur conclusion) at roughly equal rates. Together, our results describe pre-CoT features that both predict and causally influence final answers, consistent with post-hoc reasoning in LLMs. This may suggest avenues to monitor and modulate unfaithful CoT via probing and activation steering.", "tldr": "Contrastive probes can predict the answer before CoT; steering with these probes causes the model to change its answer, and often confabulate reasoning in support.", "keywords": ["Chain of Thought", "Reasoning", "Probing", "Steering", "Interpretability", "Faithfulness"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b081281ddae90b7441a5deb4fcc8728bd132b5d8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies post-hoc reasoning, i.e. the tendency of LLMs to commit to a prediction before additional chain of thought processes in tasks thought to require reasoning. In particular, it aims to uncover the latent mechanisms of post-hoc reasoning. To this end, the authors consider five language models (two from the Gemma 2 and three from the Qwen 2.5 families) across five binary QA tasks. There, through erasing or falsifiying given CoTs, the paper shows that the models exhibits post-hoc reasoning. This is furhter reinforced by linear probes of the model activations that show that the final model answer after CoT is reliably predictable across models and tasks at the last pre-CoT position.\n\nThe paper then reports steering experiments, where the steering vectors are given by the probing directions found. Through this, the binary model answers can be steered successfully causing the model to change its predictions. Finally, the CoT pathologies are classified through an LLM as a judge approach finding that confabulation and non-entailment happen equally often."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper looks at an interesting hypothesis of failure of CoT-style prediction in language models.\n\nThe probing experiments provide an interesting new insight, namely, that the final post- CoT prediction is reliably predictable from the last pre-CoT state.\n\nClassifying the types of reasoning behavior in the steered predictions is an interesting approach to quantify the changes in behaviour resulting from the steering.\n\nWhile there are several crucial weaknesses listed below, I'd encourage the authors to continue working on this interesting idea."}, "weaknesses": {"value": "(W1) The models analysed are both instruction tuned versions. In the related work, Vernhoff et al., 2025 and Zhang et al., 2025 report studying ‘reasoning' or ‘thinking’ models, which refers to distilled versions of DeepSeek R1. Given that the core research question of this paper is to study CoT behavior, it is unclear why the authors did not decide to also study models trained for CoT processing. Can the results from models not trained for CoT be expected to generalize to such models?\n\n(W2) One of the major weaknesses relates to the second key contribution. Firstly, it is not clear what the authors actually show with the steering experiments. In section 3.4, they state they want to test whether \"the pre-CoT probes are merely correlated with the final answer, and do not themselves represent the final answer or causally influence it”. In Section 4.5 they state that \"The results of the steering experiments are consistent with an interpretation of the pre-CoT probe as a causal representation of the pre- committed answer”. The preceeding Section 4.4 however does not make clear how the results actually imply this interpretation.\n\nThe way I understand it, the probe learns the directional difference of samples predictied “yes” versus samples predicted “no”. If we add/subtract this direction across all positions of the CoT chain, getting the answer to flip from “yes” to “no” and vice versa seems unsurprising. For the causal claim “the pre-CoT activation along the probe direction $w^(l*)$ is causing the final model answer” (i.e. post-hoc commitment) it should be shown that “in the absence of the probe direction v* from the final pre-CoT position, the model flips it’s prediction”, i.e. proving causality by showing the counterfactual to reliably result in the opposite behaviour.\n\nTo create this counterfactual, one could steer as $x_0 = x_0 - \\langle w^{l*}, x_0 \\rangle x_0$, i.e. eliminate the linear component of \"x_0\" should along the “yes direction\" $w^{l*}$ (and vice versa $-w^{l*}$ for the “no direction”).\n\n(W3) Besides, the predictions seem to flip already a lot at random without any steering (alpha=0 in Figure 4), the orthogonal baseline results in even more flips, and the most reliable steering success usually happens at alpha levels with high parse failures. This further weakens the steering results itself as well as the interesting results from Section 4.5, as only the portion of successfully steered examples is studied. The high level of hallucination there further calls into question how general classification of steered CoT behavior really is.\n\n\n\n**Minor weaknesses, questions, and suggestions:**\n\n– Section 4.2 references Appendix B but then proceeds with the same text as in the\nAppendix. Would it make sense to leave out this refence and just reference\nAppendix Figure 6?\n\n– Some citations seem to be formatted incorrectly, e.g. line 46 “nostalgebraist\n(2024)” should be “(nostalgebraist, 2024)”, same goes for line 047 “Bai et al.\n(2022)”\n\n– Is it possible that the models studied have already seen the evaluation data during\ntraining thus making successful post-hoc prediction more likely through data\nleakage?\n\n– The link to the limitations in §5.2 does not point to the reported discussion of the\nprobe caveats. Those appear in §5.1 (but probably should be in §5.2 as they also\nread like limitations).\n\n– The Related Work section could use a clearer differentiation to the work presented in\nthe paper. What are the gaps/limitations addressed through this work compared to\nexisting works?\n\n– Antropomorphisations like \"To determine if the model is thinking about the final\nanswer” seem unscientific. Personally, I would use a more exact wording like “if the\nfinal answer is represented”.\n\n– Why are some values for alpha=0 missing in FIgure 5?"}, "questions": {"value": "1. Why did you not use reasoning models like Venhoff et al., 2025 (DeepSeek-R1-Distill) and Zhang et al., 2025 (R1-Distill-Llama and R1-Distill-Qwen families)? Can the results from models not trained for CoT be expected to generalize to such models?\n\n2. Could you please clarify the hypothesis tested through the probing experiments and how the results imply this hypothesis exactly? Optimally, a mathematical definition of the variables thought to have a causal relation and how you test it.\n\n3. Would it make sense to look into further settings/hyperparameters for the steering to obtain results with a better ratio of successfully steered samples? In the light of the questions studied, focussing on one, optimal setting for alpha seems like a good idea. Right now, the results of 4.4 and 4.5 contain ablations for different alpha values of which most result in high parse failures or low steering success, making it hard to interpret them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3nHDPb98or", "forum": "UMUYpeXtJQ", "replyto": "UMUYpeXtJQ", "signatures": ["ICLR.cc/2026/Conference/Submission21779/Reviewer_aHJG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21779/Reviewer_aHJG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761316978307, "cdate": 1761316978307, "tmdate": 1762941929045, "mdate": 1762941929045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates whether CoT faithfully reflects the model reasoning capabilities. This work shows that models pre-commit to an answer before generating the CoT and train simple contrastive linear probes on the activation of the model and perform activation steering. Through steering the model flips its answer in more than 50% of the original examples, showing that model actually uses this representation.\nThe paper further studies how the intervention through activation steering causes the model to change its answer."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The high pre-COT probe performance shows an interesting observation of the existence of unfaithful COT.\n2. I found the CoT classification analysis interesting in how the model changes its CoT, which leads to a change in the answer."}, "weaknesses": {"value": "1. Even though authors show the existence of the final answer pre-CoT, they use simple classification tasks. I am doubtful that this might not hold up for tougher datasets. Does the author show the performance difference between CoT and Non-CoT answer?\n2. The paper only studies normal models; however, results might be different in reasoning models that undergo multiple rounds of self-verification and backtracking to reach an answer.\n3. While testing for CoT sensitivity, how is the swapping of the CoT taking place? Specifically, how is CoT modified to introduce mistakes?\n4. In Figure 4, why is there a difference in trends across models, for example in task sports understanding gemma 2 2B shows how count of flips even in large steering coefficient, however this does not hold up for qwen2.5-1.5B.\n5. What is the motivation to pick these 4 tasks?"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2GUIc03t9p", "forum": "UMUYpeXtJQ", "replyto": "UMUYpeXtJQ", "signatures": ["ICLR.cc/2026/Conference/Submission21779/Reviewer_bnMT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21779/Reviewer_bnMT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944377438, "cdate": 1761944377438, "tmdate": 1762941928677, "mdate": 1762941928677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the faithfulness of chain-of-thought (CoT) reasoning in large language models—whether the model’s verbalized reasoning truly reflects its decision-making process. While CoT improves interpretability and performance, prior work shows models often generate misleading or post-hoc rationales. The authors study post-hoc reasoning, where answers are determined before reasoning begins. Through experiments, they identify two phenomena: (1) Pre-CoT probes reveal that final answers are linearly decodable from activations before reasoning starts, implying answer pre-commitment; and (2) Answer steering shows that manipulating these activations can flip answers, often producing incoherent or non-entailing CoTs, evidencing unfaithful reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important question regarding the faithfulness of chain-of-thought (CoT) reasoning. Its use of pre-CoT probes and answer steering as tools to examine post-hoc reasoning offers a creative and insightful approach that goes beyond merely identifying unfaithful CoT. The answer steering experiments, in particular, provide a meaningful and valuable extension to current studies on CoT faithfulness. The work could make a more substantial contribution to the field with a deeper comparative analysis encompassing both LLMs and LRMs."}, "weaknesses": {"value": "1. The paper’s writing requires significant improvement. The related work section is incomplete, missing key prior studies on chain-of-thought (CoT) faithfulness (as mentioned in questions). The presentation of results and conclusions lacks logical flow, making it difficult to understand the causal link between hypotheses, methods, and findings.\n2. The experimental design is shallow relative to prior work. The “model knows the answer before CoT” phenomenon has been well documented, and the presented “pre-CoT probe” and “answer steering” analyses do not substantially advance the empirical understanding beyond existing results.\n3. The paper does not concretely connect its findings to practical principles or solutions that could guide future research to improve the reasoning ability of LLMs."}, "questions": {"value": "LN076: What is the novel part of the work compared to existing related work?\n\nLN086: Missing high relevant related work: \"How Likely Do LLMs with CoT Mimic Human Reasoning?\" which also reports the issue that LLMs may know the answer before the CoT and studies it empirically.\n\nLN131: It is also very similar to the intervention used by the related work mentioned above.\n\nLN229: These accuracies are before interventions. What is the point? How does it relate to your claim?\n\nLN355: What is the unit for the y-axis?\n\nLN350: Are these models working in think (long CoT) or non-think mode?\n\nLN417: The Conclusion section actually contains discussions instead of any conclusion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8ixNlo6e04", "forum": "UMUYpeXtJQ", "replyto": "UMUYpeXtJQ", "signatures": ["ICLR.cc/2026/Conference/Submission21779/Reviewer_Hauf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21779/Reviewer_Hauf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762068504075, "cdate": 1762068504075, "tmdate": 1762941928164, "mdate": 1762941928164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the post-hoc reasoning phenomena in language models, where the language model uses its CoT to reason in favor of a pre-determined answer (instead of doing computations that lead to an answer). It shows that the final answer of model is linearly decodable from the pre-CoT activations inside the model. Moreover, it shows that steering those activations could result in the model changing its originally correct answer, hence concluding that the model actually uses the extracted features to determine its final answer. In these steering cases, the paper classifies CoT pathologies into confabulation (false premises supporting the flipped answer) and non-entailment (true premises that do not lead to the conclusion) cases, giving insight into how post-hoc reasoning is reflected in the CoT."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper studies an important problem in reasoning literature that has implications about faithfulness of language models. It is comprehensive in the study, by studying both prediction from representations, as well as their causal role in determining the final answer. Moreover, the paper gives insight into how the post-hoc reasoning manifests in the CoT by classifying the steered cases into two cases, making it useful for studying potentials and limitations of CoT monitoring methods."}, "weaknesses": {"value": "1. In the steering experiments, the best layer for steering is chosen using the test dataset, which might cause selection bias (especially with the relatively small test dataset). Instead, a validation dataset should have been used to select the best layer and then report the results on test set. \n2. The tasks are chosen such that CoT does not help much with accuracy, it would be nice to include a datasets where CoT actually improves performance. The current datasets are limited to binary classification. \n3. The generations are limited to correct ones, while the post-hoc reasoning behavior could happen no matter the model’s pre-determined answer is correct or not."}, "questions": {"value": "1. In the logical decuctoin experiments, the pre-CoT probe achieves less AUC, potentially because the final answer depends more on the CoT. However, the “Ellipses” intervention does not affect it much. How do you explain this?\n2. Why have you limited the generations to correct ones?\n3. Could you provide examples of the cases labeled as confabulation and non-entailment? Did you verify that the labeling is accurate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sFu9XWSiGA", "forum": "UMUYpeXtJQ", "replyto": "UMUYpeXtJQ", "signatures": ["ICLR.cc/2026/Conference/Submission21779/Reviewer_WttT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21779/Reviewer_WttT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762445017793, "cdate": 1762445017793, "tmdate": 1762941927699, "mdate": 1762941927699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}