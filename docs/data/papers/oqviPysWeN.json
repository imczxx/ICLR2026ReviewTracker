{"id": "oqviPysWeN", "number": 11261, "cdate": 1758194601487, "mdate": 1759897597552, "content": {"title": "Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in LLMs", "abstract": "Large language models (LLMs) can express different values in two distinct ways: (1) $\\textit{intrinsic}$ expression, reflecting the model's inherent values learned during training, and (2) $\\textit{prompted}$ expression, elicited by explicit prompts. Given their widespread use in value alignment and persona steering, it is paramount to clearly understand their underlying mechanisms, particularly whether they mostly overlap (as one might expect) or rely on substantially different mechanisms, but this remains largely understudied.\nWe analyze this at the mechanistic level using two approaches: (1) $\\textit{value vectors}$, feature directions representing value mechanisms extracted from the residual stream, and (2) $\\textit{value neurons}$, MLP neurons that contribute to value expressions. We demonstrate that intrinsic and prompted value mechanisms partly share common components that are crucial for inducing value expression, but also possess unique elements that manifest in different ways.\nAs a result, these mechanisms lead to different degrees of value steerability ($\\textit{prompted}$ > $\\textit{intrinsic}$) and response diversity ($\\textit{intrinsic}$ > $\\textit{prompted}$). In particular, components unique to the intrinsic mechanism seem to promote lexical diversity in responses, whereas those specific to the prompted mechanism primarily strengthen instruction following, taking effect even in distant tasks like jailbreaking.", "tldr": "", "keywords": ["values", "interpretability", "large language model"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72ebdca82d3960adf3860ee27f188a1ec55afcc9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to analyze the internal differences between LLMs with and without prompts using an interpretable approach. By combining Concept Activation Vectors (CAVs) and MLP neuron analysis, the authors investigate how prompting affects internal neuron activations and conceptual representations."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an interpretable approach to analyzing the internal differences between no-prompt and prompted model behaviors. The integration of CAVs with MLP neuron analysis is well executed and conceptually appealing—using CAVs to interpret neuron activations is both novel and insightful.\n2. The paper offers concrete takeaways that deepen our understanding of how adding prompts alters internal representations in LLMs, which I believe is beneficial to the broader research community."}, "weaknesses": {"value": "The experiments mainly rely on value vectors to verify hypotheses, while value neurons are only used as motivation. I would have liked to see experiments exploring which concepts the shared neurons correspond to, and what kinds of meanings are captured by unique neurons (intrinsic) and unique neurons (prompted).\nRecent works have shown progress in automatically generating neuron explanations [1][2]. The absence of such analysis here is somewhat disappointing — if the authors could include experiments visualizing or describing these neuron-level concepts, I would consider increasing my score."}, "questions": {"value": "1. Why is the value vector computed by averaging and subtracting embeddings? As far as I know, one could directly train a linear classifier and use its normal vector as the Concept Activation Vector (CAV). Wouldn’t this yield a more principled representation that can still be used for identifying value neurons?\n2. Line 297: “This may partly reflect limitations of our win rate metric, which compares steered outputs to strong original responses (e.g. Benevolence), making large improvements inherently difficult to observe.” Could this limitation be observed empirically? I’m also curious why the metric drops after Layer 20 — if large improvements exist, would they be noticeable through human evaluation?\n3. Line 367: “Previous experiments suggest that shared components are crucial for expressing values.” Which previous experiments does this refer to? Any inference or reference?\n\n[1]https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html\n[2]The Importance of Prompt Tuning for Automated Neuron Explanations NIPSW"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sBqChDmhMj", "forum": "oqviPysWeN", "replyto": "oqviPysWeN", "signatures": ["ICLR.cc/2026/Conference/Submission11261/Reviewer_NTL2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11261/Reviewer_NTL2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547938299, "cdate": 1761547938299, "tmdate": 1762922419570, "mdate": 1762922419570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how LLMs express values intrinsically (from training) versus via prompts. It extracts “value vectors” from residual stream activations and attributes them to “value neurons” in MLP layers, finding overlapping but distinct mechanisms. Prompted mechanisms are more steerable; intrinsic ones yield greater lexical diversity. After orthogonalizing shared components, intrinsic-unique parts still drive diversity, while prompted-unique parts behave like instruction-following mechanisms, even influencing distant tasks (e.g., jailbreak susceptibility). Experiments span Qwen and Llama-3 models and multiple languages using PVQ questionnaires and situational dilemmas, with GPT-4o-mini as an evaluator. Overall, intrinsic and prompted values are mechanistically separable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper presents a novel mechanistic analysis of LLM values, in contrast to most related papers, which conduct only behavioral evaluations.\n\nInteresting empirical findings, such as prompted > intrinsic in steerability; intrinsic > prompted in lexical diversity; unique prompted components align with instruction following. \n\nThis paper presents a broad, multilingual evaluation using PVQ and situational dilemmas across several open models, as well as robustness analyses across layers. \n\nIt holds practical implications for alignment and persona steering (e.g., steerability vs. diversity trade-offs; jailbreak effects)."}, "weaknesses": {"value": "I am particularly interested in how we can leverage these findings to align LLM values. It would be beneficial to compare steering LLM value activations with behavioral alignment in terms of their effectiveness in aligning LLMs.\n\nLinear difference-in-means vectors and neuron projections may miss nonlinear mechanisms. The preconceptualization of the intrinsic value mechanism requires further validation."}, "questions": {"value": "How does value steering based on your discovery compare against behavioral alignment (e.g., RLHF and DPO) in aligning LLM values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "psTe6rzP4Z", "forum": "oqviPysWeN", "replyto": "oqviPysWeN", "signatures": ["ICLR.cc/2026/Conference/Submission11261/Reviewer_cWMP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11261/Reviewer_cWMP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896328833, "cdate": 1761896328833, "tmdate": 1762922418931, "mdate": 1762922418931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper advances a dual‑pathway account of value expression in LLMs --intrinsic (without value prompts) versus prompted (with explicit value prompts)--and links their representational differences to controllability, linguistic diversity, and safety. It extracts valeu directions for ten Schwartz dimensions from residual streams via a difference‑in‑means procedure, orthogonalizes to separate shared and unique components, and uses SVD to define shared/difference neuron axes and identify shared, intrinsic‑unique, and prompted‑unique neurons. Causal interventions (vector injection, neuron amplification) are run across Qwen2.5‑7B/1.5B and Llama‑3.1‑8B in five languages while capping MMLU degradation at ≤5 points. The shared component approximates the Schwartz circular geometry and carries value semantics; prompted‑unique components deliver stronger steerability and reveal a model‑agnostic “prompted - intrinsic” compliance direction that increases adherence -- including to unsafe prompts and jailbreak rates on HarmBench/AdvBench--whereas intrinsic‑unique components increase lexical/semantic diversity and naturalness. Its contributions are: a unified, reproducible vector ‑ and neuron‑level framework with causal evidence for disentangling shared vs. unique value representations; mechanistic insight that the prompted‑unique pathway chiefly mediates instruction‑following while the intrinsic‑unique pathway drives diversity over a shared value core; and an interpretable compliance channel that enables monitoring and mitigation for alignment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Quality. The empirical study is thorough. It spans multiple model families and scales, multiple languages, and diverse behavioral evaluations (PVQ, free-form PVQ, situational dilemmas, Value Portrait). The authors couple vector-level interventions with neuron-level amplification, which supports causal interpretation beyond correlation. The use of an MMLU-based constraint on intervention strength is a practical and responsible step toward assessing capability trade-offs. The convergence of evidence -- from geometric similarities, behavioral steering outcomes, diversity metrics, and logit-lens vocabulary projections -- creates a compelling narrative.\n\nClarity. The conceptual pipeline is clearly explained, the orthogonalization and SVD projections are well motivated, and the layer/strength selection procedure (with a cap on MMLU drop) is transparently described. Figures are informative and help connect geometry to behavior.\n\nSignificance. The work has immediate value for both controllable generation and safety alignment. It offers a principled way to decide when to favor prompted mechanisms (for strong steerability) versus intrinsic mechanisms (for naturalness/diversity). The identification of a general “compliance direction” suggests practical monitoring and mitigation strategies against jailbreaks. Methodologically, the shared-unique-neuron framework appears portable to other controllable attributes (e.g., politeness, emotion, persona)."}, "weaknesses": {"value": "Evaluation and labeling bias. \n- Much of the labeling and evaluation rests on a single automated judge (GPT-4o-mini), with a relatively small human validation sample (n = 100 items × 3 annotators). While the reported 92.33% agreement is encouraging, it is limited in size and granularity. \n- Suggestions: please expand human evaluation across languages and value dimensions, report inter-annotator agreement (e.g., κ) with confidence intervals, and include robustness checks with multiple, diverse evaluator models (including open-source judges).\n\nGeneralization of the “compliance channel.” \n- The evidence for a general instruction-following direction is strong in the value domain, with positive/negative prompts and jailbreak scenarios. However, it remains unclear whether this direction broadly improves task compliance on unrelated instruction tasks (summarization, translation, extraction) without harming factuality or increasing hallucination. \n- Suggestions: a small suite of atomic, non-value tasks would help establish boundary conditions and practical utility.\n\nLinear difference-in-means and temporal information. \n- The approach assumes linear separability in the residual stream after token-wise averaging, potentially discarding positional and syntactic information that could carry value cues.\n- Suggestions: consider adding position-weighted or span-aligned variants of the extraction, and compare with simple linear probes or sparse regression probes to test linearity. An ablation on time-sensitive signals (e.g., clause-initial vs. clause-final tokens) would strengthen the case.\n\nStatistical robustness of the Schwartz-circle recovery. \n- The PCA visualization of shared axes is compelling, but significance versus randomized baselines is not fully quantified. \n- Suggestions: please add statistical tests (e.g., Procrustes alignment) to the theoretical circle, circular correlation, or Mantel tests -- along with confidence intervals, to quantify alignment with Schwartz’s structure across layers and models.\n\nDiversity analyses and decoding confounds. \n- Although settings were held constant, distinct-n and entropy are sensitive to output length, temperature, and sampling strategy.\n- Suggestions: please add controls: length-matched comparisons, temperature sweeps, and sampling-strategy ablations, together with effect sizes and confidence intervals. This would isolate representational causes from decoding artifacts.\n\nTemplate and vocabulary leakage. \n- Since the prompted templates explicitly assert “you value {value},” they may directly inflate value-specific keywords and exaggerate the “templated” effect attributed to prompted mechanisms. \n- Suggestions: please include implicit/indirect prompts and counterfactual templates (that avoid direct value keywords but elicit similar behaviors) as controls to show that findings are not an artifact of lexical priming.\n\nMissing related works. For example, related research using SAE features for value steering such as:\n- https://www.lesswrong.com/posts/k8bBx4HcTF9iyikma/sae-features-for-refusal-and-sycophancy-steering-vectors\n- https://arxiv.org/pdf/2501.00581\nMaybe you could also consider comparing your method to these techniques (as well as those you mentioned in Section 5: Steering values through activation engineering). This comparison would offer a more robust way of validating your way of representing values as vectors."}, "questions": {"value": "- The description of the value vector/neuron extraction method is vague. In lines 103-105, the calculation method for the value vector is briefly mentioned, but how can it be guaranteed that a method like \"token-averaged mean difference between residual stream activations\" truly yields a vector related to the intended value, and not just random differences between samples? Is there any subsequent proof for this?\n\n- Safety trade-offs. If one suppresses the compliance direction to harden models against jailbreaks, what is the impact on desirable “policy-compliant” adherence (e.g., following benign safety instructions)? Is there measurable collateral damage to safe refusal behavior?\n\n- Labeling robustness. Can you report cross-language, cross-value human agreement statistics with confidence intervals? How sensitive are results to the choice of evaluator model(s)? Do you observe systematic biases (e.g., over-penalizing or over-rewarding “safety” or “conformity”)?\n    \n- Statistical alignment of shared axes. Can you quantify the match to the Schwartz circle using formal statistics (e.g., circular correlation, Procrustes alignment) across layers/models and provide p-values vs. randomized baselines?\n    \n- Scope of the compliance channel. Does steering along the mean “prompted − intrinsic” direction improve adherence on standard instruction tasks (summarization, translation, extraction) without degrading faithfulness? Are there conflicts with refusal strategies on safety-related prompts?\n    \n- Decoding controls. Under length-matching and with temperature/sampling sweeps, does the intrinsic > prompted diversity advantage persist? A figure/table summarizing these robustness checks would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wSf5cAv55d", "forum": "oqviPysWeN", "replyto": "oqviPysWeN", "signatures": ["ICLR.cc/2026/Conference/Submission11261/Reviewer_DfyZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11261/Reviewer_DfyZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987673881, "cdate": 1761987673881, "tmdate": 1762922418362, "mdate": 1762922418362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper mechanistically contrasts intrinsic and prompted value expression in LLMs.Using difference-in-means and vector orthogonalization, the authors isolate shared and unique components for both mechanisms.Intervention experiments show that while a shared component encodes general value semantics, the unique components diverge functionally.The intrinsic-unique component enhances lexical diversity, while the prompted-unique component is identified as a general \"instruction following\" mechanism."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Interesting question: The paper addresses an interesting question about value alignment mechanisms in LLMs.\n\n2.Practical application: The findings may be useful for AI safety."}, "weaknesses": {"value": "1.Limited theoretical framework: The paper lacks a clear theoretical explanation for why intrinsic and prompted mechanisms differ in these specific ways.\n\n2.Relatively simple decomposition method: The SVD-based decomposition using only two vectors to identify shared and unique axes is relatively simple.\n\n3.Insufficient jailbreaking evaluation: The jailbreaking experiments use only 500 samples per benchmark, limiting statistical power and preventing fair comparison with baselines. So, claiming to “match or exceed state-of-the-art” is premature.  \n\n4.Incomplete mechanistic understanding: The paper only studies what differs between mechanisms but provides limited insight into how and why they emerge in the LLMs.\n\n5.Limited model coverage: Testing only two model families (Qwen and Llama) and three models. Lack of larger models, different model families and newer models.\n\n6.Confounding from instruction-tuning: All tested models are instruction-tuned, which explicitly trains models to follow system prompts. The finding that \"prompted-unique mechanisms strengthen instruction-following\" may simply reflect instruction-tuning artifacts rather than fundamental properties of value expression. Without base model experiments, it's unclear whether these distinct mechanisms exist inherently or emerge specifically from instruction-tuning procedures."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6FsJZnmzCl", "forum": "oqviPysWeN", "replyto": "oqviPysWeN", "signatures": ["ICLR.cc/2026/Conference/Submission11261/Reviewer_1xKU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11261/Reviewer_1xKU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989166741, "cdate": 1761989166741, "tmdate": 1762922417899, "mdate": 1762922417899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}