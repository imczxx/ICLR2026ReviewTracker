{"id": "MtsmJZXIu5", "number": 13550, "cdate": 1758219116641, "mdate": 1759897429080, "content": {"title": "Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?", "abstract": "We study the corruption-robustness of in-context reinforcement learning (ICRL), focusing on the Decision-Pretrained Transformer (DPT, Lee et al., 2023).\nTo address the challenge of reward poisoning attacks targeting the DPT, we propose a novel adversarial training framework, called Adversarially Trained Decision-Pretrained Transformer (AT-DPT).\nOur method simultaneously trains an attacker to minimize the true reward of the DPT by poisoning environment rewards, and a DPT model to infer optimal actions from the poisoned data.\nWe evaluate the effectiveness of our approach against standard bandit algorithms, including robust baselines designed to handle reward contamination.\nOur results show that the proposed method significantly outperforms these baselines in bandit settings, under a learned attacker.\nWe additionally evaluate AT-DPT on an adaptive attacker, and observe similar results.\nFurthermore, we extend our evaluation to the MDP setting, confirming that the robustness observed in bandit scenarios generalizes to more complex environments.", "tldr": "", "keywords": ["in-context reinforcement learning", "reward poisoning attacks", "meta-learning", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f238607aee681bbcfa8e414a1817d6495dcb91e1.pdf", "supplementary_material": "/attachment/9de5198f17ec0857cb584f4009946cb729ceafe1.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses adversarial robustness from reward poisoning attacks for in-context reinforcement learning, focusing on the decision pretrained transformer (DPT) framework. The paper introduces an adversarial training framework to find optimally worst-case perturbations for the adversary to make to the target's rewards, and find that training DPT against this adversary makes the model robust."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper develops a strong formal baseline for adversarial robustness in in-context reinforcement learning, a growing and increasingly pertinent field of study. They demonstrate the effectiveness of adversarial training in the newer setting, expanding upon an important conclusion found in older works [1,2].\n\nThe paper is well written, clearly describes implementation details, and motivates design choices.\n\n\n\n\n[1] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu: Towards Deep Learning Models Resistant to Adversarial Attacks. ICLR (Poster) 2018\n\n[2] Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, Girish Chowdhary: Robust Deep Reinforcement Learning with Adversarial Attacks. AAMAS 2018: 2040-2042"}, "weaknesses": {"value": "#### Motivation\n- Given the success of adversarial training in past work, its direct application to DPT seems straightforward, and its effectiveness unsurprising. Further, the comparisons to prior work mention a difference in setting (in-context, multi-task) but do not relate these differences to a motivation. Thus, it is not immediately clear how AT-DPT addresses challenges present in prior works.\n\n#### Evaluation \n- Tables 1 & 2 state that lower regret is better; however report a higher regret for the proposed methods in clean and random environments as compared to baselines. This is noted in the text, but it is not discussed as to why this might happen. For clean settings, this usually amounts to a form of catastrophic forgetting. The disparity under random attacks is counterintuitive and should be discussed.\n- The proposed method maximizes reward as an optimization criterion; however, the experiments mostly discuss regret when compared to other robust baselines. It is worth noting the raw performance of the proposed method versus other robust methods.\n- Table 4 shows raw performance, but excludes robust baselines."}, "questions": {"value": "- Is there an intuitive explanation (or a good guess) as to why AT-DPT underperforms against random attacks specifically?\n- The proposed method uses adversaries that perturb the reward model. Can it be applied directly to a setting with observation-perturbing adversaries?\n\n#### Main Question\n- How does AT-DPT address challenges in prior works?\nThe score may be revised upwards if this information is provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gkyp54tn8E", "forum": "MtsmJZXIu5", "replyto": "MtsmJZXIu5", "signatures": ["ICLR.cc/2026/Conference/Submission13550/Reviewer_baJq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13550/Reviewer_baJq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746905393, "cdate": 1761746905393, "tmdate": 1762924147507, "mdate": 1762924147507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses reward poisoning attacks. In-context learning has been particularly popular and in specific Few-shot learning has achieved great accolades in this field. Just by observing a handful of data it can adapt to multiple tasks of that type. However, adversarial attacks and reward poisoning affect the policy output and this paper proposes an efficient algorithm to recover from such reward poisoning attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The technique is very nice, and the motivation is very clear\n2. The extensions of the algorithm to a wide variety of settings depict its effectiveness."}, "weaknesses": {"value": "1. Page 3, under section 3.1, the description of the reward function might not have a distribution over real numbers. I get it that the paper aligns the reward to a Normal or Gaussian distribution, but in general, making it a probability simplex is not very justified and might contrast with normal terminology. Same goes for $\\pi^{\\dagger}_{\\phi}$ in page 4\n\n2. Algorithm 1 line 4. It might be M and not $\\mathcal{M}$\n\n3. It might be helpful to know why is it necessary for DPT parameters to be frozen\n\n4. Why is the experiments restricted to only one environment? The extension to other environments such as Machine replacement, River swim, Frozen Lake in tabular settings, etc. or Cartpole, Mountain car, etc., 1 or 2 other environments would give a major understanding. Moreover there are much more possibilities for introducing randomness there."}, "questions": {"value": "1. It might be interesting to see how AT-DPT performs against robust variants of RL algorithms like Robust variants of Q-learning or Robust Natural Actor Critic variants, upon viewing the effectiveness in the Bandit setting.\n\n2. What happens when the architecture of the attacker and the Agent is exactly the same?\n3. What happens when extended to other environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nL7C8KrIFx", "forum": "MtsmJZXIu5", "replyto": "MtsmJZXIu5", "signatures": ["ICLR.cc/2026/Conference/Submission13550/Reviewer_LCz1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13550/Reviewer_LCz1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795952486, "cdate": 1761795952486, "tmdate": 1762924147049, "mdate": 1762924147049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on test-time reward poisoning attacks against in-context reinforcement learning, where an adversary manipulates the reward signals observed by the agent during inference to alter its in-context learning behavior. To mitigate this issue, this paper adapts adversarial training to the in-context RL setting, proposing the Adversarially Trained Decision-Pretrained Transformer (AT-DPT). \n\nThis framework can be formalized as a bi-level optimization problem. At the inner level, an adversarial attacker is optimized to learn a reward-corruption policy that perturbs the rewards within a soft budget so as to degrade the agent’s true return and expose its vulnerability. At the outer level, the transformer-based agent (DPT) is optimized to recover the correct actions and maintain high performance despite the corrupted feedback observed in its context. By alternating these two objectives, AT-DPT effectively learns to perform in-context adaptation that is resilient to both non-adaptive and adaptive reward-poisoning strategies.\n\nExperiments across bandit and MDP environments demonstrate that this adversarially trained model substantially improves robustness compared to standard and corruption-robust RL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles an interesting question concerning the robustness of in-context reinforcement learning under test-time reward poisoning. This threat model has received little attention so far, and the authors provide a clear and well-defined solution to address it.\n\n2. The proposed AT-DPT framework extends adversarial training to the in-context setting. The idea is clear and makes sense. The bi-level setup, where the attacker changes the rewards and the agent learns to handle these changes, matches well with how in-context learning works.\n\n3. The empirical evaluation is thorough, covering several environments and both adaptive and non-adaptive adversaries. Among these experiments, the results consistently show that AT-DPT outperforms both standard and robust baselines, indicating that the proposed method effectively enhances robustness."}, "weaknesses": {"value": "1. While the paper introduces a new problem setting, the core idea of applying adversarial training to improve robustness is not particularly new. Extending this idea to the in-context reinforcement learning scenario is interesting, but the paper does not clearly explain what makes this adaptation non-trivial. As a result, it is difficult to determine whether the paper makes a genuinely non-trivial contribution or merely presents a straightforward adaptation.\n\n2. The paper mainly shows that AT-DPT works empirically, but it lacks a deeper analysis of why it works. There is no theoretical explanation or detailed experimental study that helps us understand the source of its robustness. A more in-depth analysis, either theoretical or empirical, would make the contribution much stronger and more convincing.\n\n3. The paper does not discuss enough about the real-world relevance of the proposed problem. It would be helpful to provide concrete examples or scenarios showing where test-time reward poisoning could actually happen. Without such discussion, it is hard to judge how realistic or important this threat model is in practice."}, "questions": {"value": "1. Could the authors further clarify what specific challenges make the adaptation of adversarial training to the in-context reinforcement learning setting non-trivial? \n\n2. Could the authors provide a deeper analysis to justify why AT-DPT works?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9PBEqUclYc", "forum": "MtsmJZXIu5", "replyto": "MtsmJZXIu5", "signatures": ["ICLR.cc/2026/Conference/Submission13550/Reviewer_4dN9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13550/Reviewer_4dN9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891911266, "cdate": 1761891911266, "tmdate": 1762924146770, "mdate": 1762924146770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies adversarial in-context RL, where the reward signal can be corrupted by a possibly adaptive attacker. It proposes an attacker that aims to minimize the expected return on a task, and a robust variant of the Decision Pretrained Transformer (AT-DPT) to maintain performance against such attacks across different target tasks, which is validated empirically."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The work formalizes a reward poisoning model for in-context learners and introduces a simple min–max training procedure to achieve robustness against both nonadaptive and adaptive attackers."}, "weaknesses": {"value": "1. The paper focuses on empirical corruption robustness of AT-DPT without theoretical guarantees. This is unusual given the well-specified corruption model. At minimum, for the bandit setting emphasized in the experiments, it would be important to prove that regret degrades at most linearly in the corruption budget (in the spirit of guarantees in [1,2,3]). Without such bounds, the contribution feels incomplete.\n\n2. Many baselines are not designed to be adversarially robust, so their underperformance under attacks is expected. In the MDP setting, no robust baseline is included. I recommend adding robust baselines for both bandits and MDPs, and reporting results with appropriately tuned corruption parameters to ensure a fair comparison.\n\n[1] Nika, A., Singla, A. &amp; Radanovic, G.. (2023). Online Defense Strategies for Reinforcement Learning Against Adaptive Reward Poisoning. Proceedings of The 26th International Conference on Artificial Intelligence and Statistics.\n\n[2] Ye, C., Xiong, W., Gu, Q., & Zhang, T. (2023). Corruption-robust algorithms with uncertainty weighting for nonlinear contextual bandits and markov decision processes. In International Conference on Machine Learning (pp. 39834-39863).\n\n[3] Liu, H., Tajdini, A., Wagenmaker, A., & Wei, C. Y. (2024). Corruption-robust linear bandits: Minimax optimality and gap-dependent misspecification. Advances in Neural Information Processing Systems, 37, 24277-24325."}, "questions": {"value": "1. What is the cumulative regret of DPT (and other baselines) as a function of the corruption ratio $\\varepsilon$? At what threshold would AT-DPT start outperforming it?\n2. AT-DPT is trained against the attacker class defined in Section 3.2. How robust is it to different reward-poisoning mechanisms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "y6Lse3jtI1", "forum": "MtsmJZXIu5", "replyto": "MtsmJZXIu5", "signatures": ["ICLR.cc/2026/Conference/Submission13550/Reviewer_HT3j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13550/Reviewer_HT3j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762102914624, "cdate": 1762102914624, "tmdate": 1762924146518, "mdate": 1762924146518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}