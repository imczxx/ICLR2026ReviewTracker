{"id": "W2gumMWtWf", "number": 9806, "cdate": 1758141487796, "mdate": 1759897694482, "content": {"title": "CoSA: Compressed Sensing-Based Adaptation of Large Language Models", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has emerged as a practical paradigm for adapting large language models (LLMs) without updating all parameters. Most existing approaches, such as LoRA and PiSSA, rely on low-rank decompositions of weight updates. However, the low-rank assumption may restrict expressivity, particularly in task-specific adaptation scenarios where singular values are distributed relatively uniformly. To address this limitation, we propose CoSA (Compressed Sensing-Based Adaptation), a new PEFT method extended from compressed sensing theory. Instead of constraining weight updates to a low-rank subspace, CoSA expresses them through fixed random projection matrices and a compact learnable core. We provide a formal theoretical analysis of CoSA as a synthesis process, proving that weight updates can be compactly encoded into a low-dimensional space and mapped back through random projections. Extensive experimental results suggest that CoSA provides a principled perspective for efficient and expressive multi-scale model adaptation. Specifically, we evaluate CoSA on 10 diverse tasks including natural language understanding and generation, employing 5 models of different scales from RoBERTa, Llama, and Qwen families. Across these settings, CoSA consistently matches or outperforms state-of-the-art PEFT baselines while requiring over 68.4% fewer trainable parameters than LoRA and PiSSA.", "tldr": "We introduce CoSA, a compressed sensing–based adaptation method that achieves state-of-the-art performance on large language models with strong parameter efficiency.", "keywords": ["Compressed Sensing", "Parameter-Efficient Fine-Tuning", "Large Language Models", "Restricted Isometry Property"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ddb8a945600072e5d8dd93d1fa06161d59691c31.pdf", "supplementary_material": "/attachment/8ad5f03fa790087cad9b24300ddd930885aa6151.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes CoSA (Compressed Sensing Adapter), a compressed sensing-based adapter architecture for parameter-efficient fine-tuning. Unlike low-rank approaches, CoSA expresses weight updates in a compressed form using a learnable core matrix \\( Y \\) and fixed random projection matrices \\( L \\) and \\( R \\). The method is theoretically motivated by the Restricted Isometry Property (RIP), which is used to justify both training stability and the preservation of expressivity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The idea of applying compressed sensing to parameter-efficient fine-tuning is interesting.\n- The paper is clearly written and easy to follow.\n- There is an effort to ground the method in compressed sensing theory, including the use of the Restricted Isometry Property (RIP)."}, "weaknesses": {"value": "- The proposed method lacks novelty. Several works have already explored tri-matrix adapter structures. In particular, TLoRA [1] (in arxiv) presents a structurally identical approach, using frozen random matrices $A$ and $C$, and a learnable small matrix $B$.  Additionally, PMSS [2], which trains frozen A, B, and learnable cores in the same way, but with different initialization methods, was proposed in COLING'25. The authors did not provide a sufficient comparison of these tri-matrix adapters.\n\n- Although the paper claims that RIP leads to stable training, it is unclear whether such constraints are always beneficial. Since RIP restricts the amount of change after projection, it could potentially limit the expressivity of the model during fine-tuning, especially in low-data or few-shot scenarios where greater flexibility might be required. For instance, LoRA-GA [2] aims to better approximate full fine-tuning by mimicking full gradients, while CoSA instead constrains the update space, which may actually hinder learning. While the paper emphasizes the benefits of RIP, it lacks concrete theoretical, empirical, or quantitative evidence to support this claim.\n\n- The set of baseline comparisons is too narrow. The paper does not compare CoSA against recently proposed methods such as NoLA [3] and VeRA [4], which also use frozen/random bases or $AB$-structured adapters. In addition, there is no experimental comparison with TLoRA, which appears to be the most closely related work. Such comparisons are essential to fairly assess CoSA’s effectiveness.\n\n- Unlike standard LoRA, CoSA introduces \\( ab \\) parameters. Therefore, to ensure fair comparison with LoRA, the number of trainable parameters should be explicitly reported in each experiment. For example, in the NLG task with LLaMA, the paper mentions using (a, b) = (1024, 256), while the LoRA rank is set to 128. Since LLaMA-3B has a hidden dimension of 2048, this results in:\n    - LoRA: 2048 × 128 × 2 parameters\n    - CoSA: 1024 × 256 parameters\n    \nTherefore, CoSA uses about half the parameters compared to LoRA. However, other methods such as VeRA [4] or Vb-LoRA [5] introduce even fewer parameters, which makes CoSA to be less contributed. Therefore, the paper should clearly report the exact parameter counts and include comparisons against a wider range of parameter-efficient baselines.\n\n\n\n---\n\n>[1]Islam, Tanvir. \"TLoRA: Tri-Matrix Low-Rank Adaptation of Large Language Models.\" arXiv preprint arXiv:2504.18735 (2025).\n>\n>[2] PMSS: Pretrained Matrices Skeleton Selection for LLM Fine-tuning, COLING, 2025\n>\n>[3]Wang, Shaowen, Linxi Yu, and Jian Li. \"Lora-ga: Low-rank adaptation with gradient approximation.\" Advances in Neural Information Processing Systems 37 (2024): 54905-54931.\n>\n>[4] Koohpayegani, Soroush Abbasi, et al. \"NOLA: Compressing LoRA using Linear Combination of Random Basis.\" The Twelfth International Conference on Learning Representations. 2024\n>\n>[5] Kopiczko, Dawid Jan, Tijmen Blankevoort, and Yuki M. Asano. \"VeRA: Vector-based Random Matrix Adaptation.\" The Twelfth International Conference on Learning Representations. 2024\n>\n>[6] Li, Yang, Shaobo Han, and Shihao Ji. \"Vb-lora: Extreme parameter efficient fine-tuning with vector banks.\" Advances in Neural Information Processing Systems 37 (2024): 16724-16751."}, "questions": {"value": "- In Table 1, the paper claims that the storage requirement is $\\mathcal{O}(1)$, but the learned core matrix $Y \\in R^{a \\times b}$ still needs to be stored. To me, this suggests that the storage complexity should be $\\mathcal{O}(ab)$, not $\\mathcal{O}(1)$. If I’m misunderstanding something, clarification would be appreciated. Moreover, in the NLG task, it seems that $a, b$ can be as large as 1024, which is not negligible in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WMz9GDwKDR", "forum": "W2gumMWtWf", "replyto": "W2gumMWtWf", "signatures": ["ICLR.cc/2026/Conference/Submission9806/Reviewer_jcn4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9806/Reviewer_jcn4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761532338106, "cdate": 1761532338106, "tmdate": 1762921291956, "mdate": 1762921291956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Compressed Sensing–based Adaptation (CoSA), a PEFT method for LLMs.\n\nInspired by compressed sensing thoery, the general idea of this work is to parameterize every weight update as are fixed random projection matrices and only the compact core is trained. \n\nCoSA is compared against LoRA, AdaLoRA, and PiSSA on GLUE and on math/code generation, on top of Llama-3.2-1B, Llama-3.1-8B, Qwen2-7B, showing competitive or better accuracy with substantially fewer parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ This paper is overall well-written and clearly-presented, making the readers easy to follow.\n\n+ The proposed method shows a clear parameter and memory benefits over LoRA, AdaLoRA, PiSSA.\n\n+ The ablation study is extensive."}, "weaknesses": {"value": "- The technique soundness is open to doubt, at least in its current form. For example, the framing is not tied to an actual sparsity prior or to constraints. Besides, there is no theory level proof to justify the stability guarantees.\n\n- The core idea to fix random $L$, $R$ and learn a compact core is not sufficiently distinguished from VeRA and/or other related random-projection PEFT methods, making the contribution to the community difficult to justify.\n\n- This paper does not provide a theory-level proof on the emperical risk bound of either sparisty or the regularized training.\n\n- The compared state-of-the-art PEFT methods are significantly missing. Some more recent and much stronger PEFT methods are mssing for comparison, for example:\n\n[1] DoRA: Weight-Decomposed Low-Rank Adaptation. ICML 2024.\n\n[2] VeRA: Vector-based Random Matrix Adaptation. ICLR 2024.\n\n[3] Foura: Fourier low-rank adaptation. NeurIPS 2024.\n\n[4] SSH: Sparse Spectrum Adaptation via Discrete Hartley Transformation. NAACL 2024.\n\n- In the $(a, b)$ ablation, is the rank $r$ rigorously matched? Please clarify. \n\n- If comparing with these more recent PEFT methods, the performance of the proposed method is rather limited and even inferior. \n\n- The experimental validation, to be honest, is rather limted. The authors only validate on two benchmarks, where GLUE is already out-of-date. It should be benchmarked on more recent yet more challenging instruction-tuning or multi-task mixture benchmarks, like [1-4] do. \n\n- Still regarding the performance, this paper lacks a convincing discussion on the performance variance caused by the random seeds.\n\n- The training time and latency is not either reported or compared. \n\n- Some typos and presentation issues still remain."}, "questions": {"value": "Please refer to the weakness section, and address them point-by-point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Fk1JrpT0NP", "forum": "W2gumMWtWf", "replyto": "W2gumMWtWf", "signatures": ["ICLR.cc/2026/Conference/Submission9806/Reviewer_bos4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9806/Reviewer_bos4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744280917, "cdate": 1761744280917, "tmdate": 1762921291578, "mdate": 1762921291578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new PEFT method for LLMs. The authors argue that the low-rank assumption in LoRA limits expressivity. Inspired by compressed sensing, they propose CoSA, which treats the target weight update matrix as sparse. CoSA employs frozen projection matrices as the sensing matrices and fine-tunes a lower-dimensional measurement matrix. The key contribution is framing the compression of the target weight update matrix through the lens of compressed sensing. The authors also prove that the frozen projection matrices L and R satisfy RIP with high probability. Experiments show that CoSA matches or outperforms strong PEFT baselines while using over 68% fewer trainable parameters, with consistent improvements across NLU and reasoning/code benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Viewing the PEFT problem through the lens of compressed sensing is an interesting and novel perspective.\n2. The writing and presentation is clear.\n3. The experiments are comprehensive and include tasks of different domains."}, "weaknesses": {"value": "1. The proposed approach substantially overlaps with existing methods such as Tied-LoRA and VeRA [1,2], yet the paper makes no mention of them. Both Tied-LoRA and VeRA also employ frozen random matrices as down- and up-projection matrices, making it unclear how CoSA differs conceptually or empirically from these prior works.\n2. The claim of O(1) complexity for CoSA in Table 1 appears inaccurate. Given the formulation, the complexity should be O(ab).\n3. The method assumes that the target weight update matrix is sparse, which may not hold in practice. The authors should provide justifications for this sparsity assumption.\n4. Theorem 1 offers only a superficial guarantee that the Kronecker product of two sensing matrices satisfies RIP with high probability. This result does not provide deeper insights into why the proposed approach should work better than existing PEFT methods.\n5. The baseline comparisons are limited. Stronger and more recent baselines such as DoRA are not included. It would also strengthen the paper to evaluate CoSA on instruction-tuning tasks to demonstrate its generality.\n\nReferences\n1. Tied-LoRA: Enhancing parameter efficiency of LoRA with Weight Tying\n2. VeRA: Vector-based Random Matrix Adaptation"}, "questions": {"value": "1. Why would a sparsity assumption work better for the $\\Delta W$ than low rank assumption?\n2. Is there any computational overhead from the additional matrix multiplication of CoSA compared to LoRA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DOW97YWLiF", "forum": "W2gumMWtWf", "replyto": "W2gumMWtWf", "signatures": ["ICLR.cc/2026/Conference/Submission9806/Reviewer_ZSj7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9806/Reviewer_ZSj7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898030837, "cdate": 1761898030837, "tmdate": 1762921291063, "mdate": 1762921291063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a new PEFT method inspired by compressed sensing. They parameterize the update as a sequence of three matrices \ndeltaW = L Y R, where L and R are independent random matrics and Y is learnable core matrix. Using random projections reduces the number of training parameters and their RIP property ensures that training is not destabilized. The resutls show improvements over LoRA based methods showing that there are cases where Low-rank is not a correct hypothesis over updates."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The observation that Low rank is not always a good hypothesis is valuable (although it appears in some recent works)\n2. The paper is well written and generally a good read with discussion around compressed sensing,etc"}, "weaknesses": {"value": "1. Lack of baselines ( and hence related work)  (my main concern is this)\n\nThe experiments are okay (benchmark wise) but are lacking baseline wise. For instance, very similar and more recent PEFT baselines are excluded. SketchTune, for instance is also based on sketching matrices (a special case of projection matrices which also have RIP property). Also, some other baselines such as S2FT etc are missing. It is important to compare against these methods to ensure that we are indeed making progress in PEFT domain. \n\nZhang, Tianyi, Junda Su, Aditya Desai, Oscar Wu, Zhaozhuo Xu, and Anshumali Shrivastava. \"Sketch to Adapt: Fine-Tunable Sketches for Efficient LLM Adaptation.\" arXiv preprint arXiv:2410.06364 (2024).\n\nYang, Xinyu, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, and Beidi Chen. \"S $^{2} $ FT: Efficient, scalable and generalizable LLM fine-tuning by structured sparsity.\" Advances in Neural Information Processing Systems 37 (2024): 59912-59947.\n\n2. The current formulation also is low-rank (rank = min(a,b)). Am i missing something?\n       a. why do you expect CoSA to handle cases when deltaW is not low rank\n       a. related but different, can authors elaborate how does CoSA provide extra expressive power."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GeYruFqFkg", "forum": "W2gumMWtWf", "replyto": "W2gumMWtWf", "signatures": ["ICLR.cc/2026/Conference/Submission9806/Reviewer_8TDm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9806/Reviewer_8TDm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042690053, "cdate": 1762042690053, "tmdate": 1762921290787, "mdate": 1762921290787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}