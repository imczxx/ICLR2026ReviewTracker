{"id": "qxt6glz9A8", "number": 15790, "cdate": 1758255268023, "mdate": 1759897281848, "content": {"title": "MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation", "abstract": "Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards. Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and text–motion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-of-Motion (CoM), a test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets, MoUnd-CoT-140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over state-of-the-art baselines.", "tldr": "", "keywords": ["human motion", "motion understanding", "motion generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a4c3313e3dc01ccfc20a7309a6f0e8b7d709f3a8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents MoRL, a unified framework for human motion understanding and generation, leveraging a multi-modal large language model enhanced by reinforcement learning and step-by-step reasoning strategies. The approach introduces specific rewards for semantic alignment, reasoning coherence, physical plausibility, and text-motion consistency, aiming to improve both the interpretability and realism of generated motions. MoRL is trained and evaluated on large human motion-language datasets, and is complemented by a new Chain-of-Motion (CoM) test-time inference scheme and two synthetic chain-of-thought (CoT) motion datasets (MoUnd-CoT-140K and MoGen-CoT-140K). Experiments on benchmark datasets demonstrate gains over a wide range of baselines, and ablation studies dissect the value of individual model components."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- MoRL presents an integrated approach for both motion understanding and generation, offering a single framework that closes the gap between perception and synthesis in motion-language modeling.\n- The reward functions go beyond prior generic similarity metrics: the logical coherence reward for reasoning traces and explicit physical plausibility for motion generation are clear step-ups over most existing motion-language systems.\n- Quantitative results across linguistic (BLEU, ROUGE, CIDEr, BERTScore) and distributional metrics (R-Precision, FID, MM Dist, Diversity) demonstrate consistent improvements."}, "weaknesses": {"value": "- **Lack of supplementary materials and qualitative evidence (visualizations/videos)**： The submission provides neither a supplementary document nor qualitative visualizations (e.g., trajectory plots, attention/activation maps, sample reasoning traces aligned with frames) or videos of generated motions. For a generative motion system, the absence of side-by-side videos against baselines and ablations makes it difficult to assess realism, physical plausibility, temporal coherence, and failure modes, which **materially undermines credibility**. Please include a supplementary PDF and a project page with videos (paired with CoM/CoT reasoning snippets), as well as qualitative comparisons to Motion-R1/MotionRL.\n- **Novelty Relative to Recent Work**: While the reinforcement learning and CoT/CoM techniques are assembled well, the combination does not significantly depart from recent trends exemplified by MotionRL, Motion-R1, and Open-Reasoner-Zero; the primary novelty appears incremental rather than foundational. This modest advancement is particularly notable considering the reliance on recently published reinforcement learning pipelines with similar reward shaping and reflection-style reasoning.\n- **Reward Implementation Ambiguities**: The reinforcement learning reward structure is stated clearly but lacks necessary specifics. For example, details on normalization strategies, the precise form of the entailment model $f_{\\mathrm{NLI}}$, the codebook size ($N$) and its impact on representation granularity, and hyperparameter $\\lambda_1$, $\\lambda_2$ selections for physical plausibility rewards are omitted (see Section 4.4, page 5-6). This hampers reproducibility and undermines claims regarding robustness and optimality.\n- **Inadequate Differentiation from Motion-R1 and MotionRL**: Both these works (see Related Work, page 2) have explored RL-based and CoT-infused motion generation. It's unclear to what extent MoRL substantially improves over or is distinct from these baselines, especially lacking direct, head-to-head empirical or qualitative comparison in Table 1 or Table 2 (no explicit Motion-R1 or MotionRL entries)."}, "questions": {"value": "1. Can the authors clarify the selection and training of the NLI model $f_{\\mathrm{NLI}}$ used for the reasoning coherence reward? Is it frozen or fine-tuned? What is its domain (textual or motion-natural language mixed)?\n2. Are there examples or a qualitative analysis of the generated reasoning traces for typical and failure cases? How do they compare in logical structure and relevance to those produced by recent LLM or multimodal CoT-based baselines?\n3. Could the authors provide more details about the quantization mechanism in the motion tokenizer, including how ambiguous cases or codebook collisions are handled?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fsN0GhHrxc", "forum": "qxt6glz9A8", "replyto": "qxt6glz9A8", "signatures": ["ICLR.cc/2026/Conference/Submission15790/Reviewer_ErN1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15790/Reviewer_ErN1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915185996, "cdate": 1761915185996, "tmdate": 1762926022837, "mdate": 1762926022837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a unified multimodal motion model that handles both motion understanding and text-to-motion generation, it combines supervised CoT tuning with reinforcement learning. The reward design is dual-headed: semantic alignment + reasoning coherence for understanding, and physical plausibility + text–motion consistency for generation. MoRL uses a Qwen3-4B backbone with a VQ-VAE motion tokenizer, and is optimized via a GRPO-style objective with KL regularization. On HumanML3D and KIT-ML, MoRL reports consistent gains compared to extensive baselines, with ablations showing each reward and CoM contribute additively."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper unifies motion understanding and generation with task-specific rewards. The paper adopts a simple but effective RL recipe, using GRPO-style group sampling with KL to a frozen reference avoids heavy heuristics yet yields consistent gains.\n\n- The work has strong CoT data engine. Two large CoT datasets align motions with reasoning traces and concise answers, providing good supervision for both directions.\n\n- Evaluations cover comprehensive metrics for both understanding and generation comparing with extensive baselines."}, "weaknesses": {"value": "- Results are only on HumanML3D and KIT-ML, harder and more diverse settings (like long sequences and multi-person) aren’t covered. \n\n- CoM samples K=8 candidates with T=2 refinement, the paper calls overhead “modest” but reports no latency/throughput numbers.\n\n- Headline gains of experimental results are 4.17% BERT and 3% FID, and MoRL is not best on all metrics (like FID vs diffusion baselines)."}, "questions": {"value": "- Could the author provide a human study for realism and text–motion alignment to complement automatic metrics?\n\n- Could the authors report end-to-end inference latency/throughput with CoM vs single-pass decoding?\n\n- The VQ-VAE motion tokenizer lacks concrete hyperparameters like codebook size that affect generation quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yo89LfGteT", "forum": "qxt6glz9A8", "replyto": "qxt6glz9A8", "signatures": ["ICLR.cc/2026/Conference/Submission15790/Reviewer_Dwdq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15790/Reviewer_Dwdq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981636227, "cdate": 1761981636227, "tmdate": 1762926022245, "mdate": 1762926022245, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MoRL, a unified model for human motion understanding and generation trained with SFT and RL. The approach has three main components: (1) task-specific reward functions combining semantic alignment and reasoning coherence for understanding (2) two synthetic Chain-of-Thought datasets (MoUnd-CoT-140K and MoGen-CoT-140K) created using Gemini to align motion sequences with reasoning traces (3) Chain-of-Motion (CoM), a test-time decoding strategy with iterative refinement. Experiments on HumanML3D and KIT-ML show improvements over baselines; ablations isolate contribution of each reward and of CoM."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- well motivated problem. Motion understanding and generation have been studied separately but unifying them with a shared representation is valuable. Table 3 shows that joint training improves both understanding and generation, suggesting these tasks reinforce each other.  \n- the four reward (semantic, coherence, physical, alignment) all cover complementary aspects and I appreciate the ablations in table 3 to show each contributes meaningfully\n- strong experimental validation with two benchmarks with comprehensive metrics + comparison with over 15 baselines \n- another strength is the large-scale dataset contributions with both MoUnd-CoT-140K and MoGen-CoT-140K providing valuable resources for future research \n- CoM is a very interesting idea and I'm glad to see it worked out. Its success shows that we can adapt the paradigm of test-time compute scaling from LLMs to motion domain, showing its generalizability"}, "weaknesses": {"value": "-Missing an analysis of the quality of the synthetic CoT data as there is no human evaluation of the dataset provided. We have no idea which gemini reasoning traces are actually correct. I hope the authors can share more data quality metrics and analysis. \n- I would like to see when does MoRL fail. what types of motions or captions are challenging. A qualitative and quantitative error analysis would be nice."}, "questions": {"value": "- Why did you use GRPO specifically? Can you compare with PPO, DPO, and SFT on CoT data w/o RL?\n- a 4B model size seems small. Did you try smaller and larger models and see how it affects performance?\n- The paper states that SFT uses MoUnd-CoT-140K and MoGen-CoT-140K, but doesn't explicitly specify which dataset is used for the RL stage. I would imagine MoUnd-CoT-140K and MoGen-CoT-140K would also be used for RL, but then Section 5.1 mentions \"both datasets are divided into training, validation, and test splits with a ratio of 0.8/0.15/0.05,\" referring to HumanML3D and KIT-ML, which suggests these might be used for training. Is this correct? Can you in general clarify which datasets are used at what point in the training pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lSUnwystQH", "forum": "qxt6glz9A8", "replyto": "qxt6glz9A8", "signatures": ["ICLR.cc/2026/Conference/Submission15790/Reviewer_VD7i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15790/Reviewer_VD7i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013751590, "cdate": 1762013751590, "tmdate": 1762926021841, "mdate": 1762926021841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MoRL, a unified multimodal motion model designed to enhance human motion understanding and generation through both supervised fine-tuning and reinforcement learning with verifiable, task-specific rewards. The reward framework integrates semantic and reasoning coherence for understanding tasks and physical plausibility with text–motion consistency for generation tasks, thereby improving logical reasoning and perceptual realism. To strengthen test-time reasoning, the authors propose Chain-of-Motion (CoM), a method that supports step-by-step planning and reflection during inference. They also build two large-scale reasoning datasets—MoUnd-CoT-140K and MoGen-CoT-140K—linking motion sequences to reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML demonstrate that MoRL outperforms state-of-the-art baselines in both understanding and generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a unified multimodal framework that effectively integrates semantic, reasoning, and physical consistency rewards, leading to more logically coherent and perceptually realistic motion understanding and generation.\n2. It introduces Chain of Motion reasoning and large-scale CoT datasets, which significantly enhance the model’s interpretability and performance, achieving measurable improvements over state-of-the-art baselines on standard benchmarks.\n3. The paper is well-written, presenting a complex technical system with conceptual clarity and a logical narrative that is easy to follow."}, "weaknesses": {"value": "1. The proposed method fails to outperform or at least reach comparable performance to all the baselines, especially in terms of CIDEr, where it was significantly surpassed by certain baselines.\n2. In the ablation study, it seems that the performance of full MoRL and that without certain reward item or CoM are comparable. Moreover, it does not even show a significant improvement of full MoRL over SFT only."}, "questions": {"value": "I am curious about the approximate probability of triggering the iterative reflection and selection process in CoM. Typically, how many rounds does it take to successfully generate a result? During the reflection process, does the model produce new modalities of behavior, or does it mainly modify inconsistent details?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tYROLroX8D", "forum": "qxt6glz9A8", "replyto": "qxt6glz9A8", "signatures": ["ICLR.cc/2026/Conference/Submission15790/Reviewer_oK66"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15790/Reviewer_oK66"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762057586048, "cdate": 1762057586048, "tmdate": 1762926021448, "mdate": 1762926021448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}