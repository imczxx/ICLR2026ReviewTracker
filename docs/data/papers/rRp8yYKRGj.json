{"id": "rRp8yYKRGj", "number": 8148, "cdate": 1758070153673, "mdate": 1759897803639, "content": {"title": "Consistent Text-to-Image Generation via Scene De-Contextualization", "abstract": "Consistent text-to-image (T2I) generation seeks to produce identity-preserving images of the same subject across diverse scenes, yet it often fails due to a phenomenon called identity (ID) shift. Previous methods have tackled this issue, but typically rely on the unrealistic assumption of knowing all target scenes in advance. This paper reveals that a key source of ID shift is the native correlation between subject and scene context, called scene contextualization, which arises naturally as T2I models fit the training distribution of vast natural images. We formally prove the near-universality of this scene-subject correlation and derive theoretical bounds on its strength. On this basis, we propose a novel, efficient, training-free prompt embedding editing approach, called Scene De-Contextualization (SDeC), that imposes an inversion process of T2I’s built-in scene contextualization. Specifically, it identifies and suppresses the latent scene-subject correlation within the ID prompt’s embedding by quantifying SVD directional stability to re-weight the corresponding eigenvalues adaptively. Critically, SDeC allows for per-scene use (one prompt per scene) without requiring prior access to all target scenes. This makes it a highly flexible and general solution well-suited to real-world applications where such prior knowledge is often unavailable or varies over time. Experiments demonstrate that SDeC significantly enhances identity preservation while maintaining scene diversity.", "tldr": "", "keywords": ["Text-to-Image generation", "Identity-preserving", "Prompt embedding editing", "Scene contextualization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/222e5d3cd77bd589c6709d2950804c2a7285b02c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to prevent ID drift in text-to-image based personalized storytelling. This paper theoretically analyze the cause of ID shift, which is named scene contextualization. Based on the theory, this paper further quantifies the contextualization, and proposes scene de-contextualization. The idea is to quantify the extent to which each direction is influenced by contextualization and then selectively reinforce those that are less affected. \n\nIn practice, it first pull the eigenvalue of SVD of ID embedding towards scene, and then pull the eigenvalue back towards the ID. The directions whose corresponding eigenvalues remain nearly unchanged (resistant to both pull and restoration) are treated as robust directions against contextualization. In contrast, those with large variations are the latent scene-ID correlation subspace. To suppress the scene influence on ID, it suppresses eigenvalue with large variations by using adaptive weighting. The modified ID embedding are fed into the model along with the original scene embedding.\n\nExperiments show that this method can be a plug-in into diverse tasks, e.g., integrating pose map and personalized photo, and generative backbones such as PlayGround-v2.5, RealVisXL-V4.0 and Juggernaut-X-V10."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper seems to strike a new balance between ID consistency and scene diversity.\n2. This paper derives the method from theory, which may inspire future researchers."}, "weaknesses": {"value": "1. Does it really make sense to suppress $\\sigma_{\\cap}$? What if the scene has very strong information on the subject such as colorful lighting?\n2. Related to 1), the method seems sensitive to $\\Omega$ as shown in Figure 8. As both works on SVD eigenvalues, this paper seems an improvement on 1Prompt1Story. In this sense, sensitivity to  $\\Omega$ can significantly harm the contribution."}, "questions": {"value": "1. Maybe more math: what might be the (possibly) deepest mathematical reason of there being always a balance between ID and scene? Since this paper already contains a lot of math, digging into the reason can make this paper in a higher stage.\n2. Maybe more intuition: I feel visualizing more about the pull and restoration process and playing with $\\Omega$ might give some more insight."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sxJaTDUj5X", "forum": "rRp8yYKRGj", "replyto": "rRp8yYKRGj", "signatures": ["ICLR.cc/2026/Conference/Submission8148/Reviewer_Hj37"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8148/Reviewer_Hj37"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760914942274, "cdate": 1760914942274, "tmdate": 1762920118455, "mdate": 1762920118455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the cause of identity (ID) shift in text-to-image (T2I) models and introduces a training-free method called Scene De-Contextualization (SDeC). The authors theorize that ID shift arises from *scene contextualization*—an intrinsic correlation between identity and scene semantics induced by the attention mechanism. They formally prove this phenomenon’s inevitability and bound its strength. Building on this, SDeC edits prompt embeddings to suppress latent scene-ID correlations by analyzing singular value stability and adaptively reweighting eigenvalues, allowing per-scene use without knowing all target scenes. Experiments on ConsiStory+ demonstrate that SDeC improves identity consistency while preserving scene diversity and works complementarily with prior methods like ConsiStory. The paper’s theoretical analysis and efficient, training-free implementation make it a notable step toward understanding and mitigating ID shift in T2I generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a clear and rigorous formulation of *scene contextualization* as the root cause of identity shift in T2I models, supported by formal theorems and bounds. This theoretical framing is original and intellectually valuable.\n2. The proposed SDeC method operates purely at the prompt-embedding level, requiring no model retraining or access to all target scenes, which makes it lightweight, general, and practical.\n3. The paper is well structured, linking theoretical findings to practical implications, and provides convincing visual and analytical evidence for the proposed claims."}, "weaknesses": {"value": "1. The Introduction could be made clearer for readers unfamiliar with this research area. For example, in *line 47*, the authors argue that “target scenes are not always available,” but it is unclear whether this refers to the incompleteness of scene samples in the training data. When introducing *scene contextualization*, it would also help to include a comparative visualization in Figure 1 showing scenes with and without ID shift, to better illustrate the motivation.\n2. The theoretical analysis in Section 3 is difficult to follow, especially for non-expert readers. From a high-level understanding (as shown in *Figure 2*), *scene contextualization* arises because the text encoder’s causal attention mechanism allows scene tokens to capture partial semantics from ID tokens, thereby causing ID shift. However, I have two questions: (i) does this phenomenon only occur in CLIP-based T2I models, since newer architectures such as SD3and Flux use T5-based encoders with *bidirectional attention*? Would the proposed method still apply under such architectures? (ii) How sensitive is this effect to the order of the scene and ID prompts? For instance, if the scene prompt precedes the ID prompt, would scene contextualization still emerge, and would SDeC remain effective in that case?\n3. As I am not fully up to date with the latest progress in this field, I am unable to fully assess the choice of baselines and the reported performance comparisons, and therefore defer to other reviewers for judgment on that aspect."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pBDVmZDMYh", "forum": "rRp8yYKRGj", "replyto": "rRp8yYKRGj", "signatures": ["ICLR.cc/2026/Conference/Submission8148/Reviewer_GF6P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8148/Reviewer_GF6P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761378235633, "cdate": 1761378235633, "tmdate": 1762920118023, "mdate": 1762920118023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a traiing-free prompt embedding editing framework, i.e.,  Scene De-Contextualization (SDeC), to address the issue of identity (ID) shift in consistent text-to-image (T2I) generation. The authors identify that a key source of ID shift is the inherent correlation between the subject and the scene context, termed \"scene contextualization.\" They provide a theoretical formulation to characterize this phenomenon and derive bounds on its strength. Based on these insights, SDeC is introduced as a training-free prompt embedding editing method that suppresses the latent scene-ID correlation within the ID prompt's embedding. The method is shown to be effective in enhancing identity preservation while maintaining scene diversity across various experiments and benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Technically effective and efficient framework: the proposed SDeC is a training-free method that can be applied per scene without requiring prior knowledge of all target scenes. This makes it highly flexible and suitable for real-world applications where target scenes may vary over time. The experimental results demonstrate significant improvements in identity preservation and scene diversity compared to existing methods.\n2. The paper provides a new perspective on the problem of ID shift by formally defining and analyzing scene contextualization. The theoretical framework, including Theorem 1 and Corollary 1, offers a deeper understanding of the underlying mechanisms of ID shift in T2I models.\n3. The authors conduct extensive experiments on a benchmark dataset (ConsiStory+) and compare their method with both training-based and training-free state-of-the-art approaches. The results are supported by both quantitative metrics (CLIP-I, DreamSim-F, CLIP-T, DreamSim-B) and qualitative visual comparisons, showing the superiority of SDeC in various scenarios. Also, the study of Nano banana's ID preservation provide interesting insight."}, "weaknesses": {"value": "1. Evaluation: both the proposed method and compared baselines are unet-based models, mainly SD1.5 and SD-XL. However, DiT-based models, especially the MMDiT-based variants, significantly advances synthesis quality. Thus, it would be better to compare with some DiT-based models and implement the proposed method on these models to further investigate the effectiveness. Moreover, the paper mentions that SDeC can be integrated with different generative models, but it does not explore its compatibility with other similari methods that also aim to improve ID consistency, such as ID-Aligner, InstantID, IP-adapter (flux based), etc.. Comparing SDeC with these methods could provide a more comprehensive evaluation of its performance.\n2. The performance of SDeC is sensitive to parameters like weighting strength (Ω) and trade-off parameter (βM). While the paper provides a sensitivity analysis, further optimization of these parameters could improve performance.\n3. Time Complexity: SDeC requires performing SVD and a forward-backward optimization for each scene, resulting in a time complexity of O(d^3). This can become a bottleneck in real-time generation scenarios, while the paper mentions that the inference time only increases by 0.61 seconds, but this is for single-image generation.\n4. Diverse Dataset Evaluation: Evaluate SDeC on additional datasets with varying levels of complexity and compositional density to validate its generalization ability."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ki5WxhVn4M", "forum": "rRp8yYKRGj", "replyto": "rRp8yYKRGj", "signatures": ["ICLR.cc/2026/Conference/Submission8148/Reviewer_xuex"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8148/Reviewer_xuex"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924226563, "cdate": 1761924226563, "tmdate": 1762920117626, "mdate": 1762920117626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the issue of identity shift in consistent text-to-image generation, where T2I models often fail to produce identity-preserving images of the same subject across different scenes. It points out that previous methods addressing ID shift rely on the unrealistic assumption of pre-knowing all target scenes, and reveals that a key cause of ID shift is the inherent correlation between subject and scene context, which emerges when T2I models fit the training distribution of large-scale natural images. The paper formally proves the near-universality of this scene-ID correlation and derives theoretical bounds on its strength. Based on this, it proposes a novel, efficient, training-free prompt embedding editing approach named Scene De-Contextualization, which inverts T2I’s built-in scene contextualization—specifically, it identifies and suppresses the latent scene-ID correlation in the ID prompt’s embedding by quantifying SVD directional stability to adaptively re-weight corresponding eigenvalues."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper targets a critical limitation in text-to-image  models: identity shift—where a subject’s core appearance changes unexpectedly when scene prompts are modified. To solve this, the authors propose the Scene-Subject Decoupling  module, a lightweight addition to pre-trained T2I models that uses: (1) a contrastive loss to separate scene and subject representation spaces; (2) dynamic attention gating to prioritize subject features during scene updates. Claimed contributions include: (1) a novel framework for scene-subject decoupling in T2I; (2) empirical validation of ID preservation without sacrificing scene quality; (3) a lightweight design compatible with pre-trained models."}, "weaknesses": {"value": "The quantitative results shown in table 1 is not convincing that the proposed method outperforms others sota method like 1P1S."}, "questions": {"value": "The scene description and the subject in the image generation process should be coupled; for instance, the lighting and style of the scene influence the subject’s clothing, appearance, and so on. The motivation of this paper is to reduce the correlation between the scene and the subject.\n\nthe scene prompts in your provided cases include the verbs, why the verbs are included as part of the scene description?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ub0KcSByhC", "forum": "rRp8yYKRGj", "replyto": "rRp8yYKRGj", "signatures": ["ICLR.cc/2026/Conference/Submission8148/Reviewer_zjEc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8148/Reviewer_zjEc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762054584720, "cdate": 1762054584720, "tmdate": 1762920117256, "mdate": 1762920117256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}