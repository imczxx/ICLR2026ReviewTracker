{"id": "UxIRc97ecL", "number": 19493, "cdate": 1758296730593, "mdate": 1759897035932, "content": {"title": "Understanding the Learning Phases in Self-Supervised Learning via Critical Periods", "abstract": "Self-supervised learning (SSL) has emerged as a powerful pretraining strategy to learn transferable representations from unlabeled data. Yet, it remains unclear how long SSL models should be pretrained for such representations to emerge. Contrary to the prevailing heuristic that longer pretraining translates to better downstream performance, we identify a transferability trade-off: across multiple SSL methods, architectures, and datasets, we observe intermediate checkpoints yielding stronger out-of-domain (OOD) generalization, while models pretrained longer tend to instead only improve in-domain (ID) accuracy. From this observation, we hypothesize that SSL progresses through learning phases that can be characterized through the lens of critical periods (CP). Prior work on CP has shown that neural networks trained under supervised learning exhibit early phases of high plasticity, followed by a consolidation phase where adaptability declines but task-specific performance keeps increasing. Since traditional CP analysis depends on supervised labels, for SSL we rethink CP in two ways. First, we inject deficits to perturb the pretraining data and measure the quality of learned representations via downstream tasks. Second, to estimate network plasticity during pretraining we compute the Fisher Information matrix on pretext objectives, quantifying the sensitivity of model parameters to the supervisory signal defined by the pretext tasks. We conduct several experiments to demonstrate that SSL models do exhibit their own CP, with CP closure marking a sweet spot where representations are neither underdeveloped nor overfitted to the pretext task. Leveraging these insights, we propose CP-guided checkpoint selection as a mechanism for identifying intermediate checkpoints during SSL that improve OOD transferability. Finally, to balance the transferability trade-off, we propose CP-guided self-distillation, which selectively distills layer representations from the sweet spot (CP closure) checkpoint into their overspecialized counterparts in the final pretrained model.", "tldr": "SSL models pass through critical periods where representations are highly plastic, early training shapes transferability, and prolonged pretraining leads to overspecialization that hurts OOD generalization.", "keywords": ["Learning Phases", "Critical Periods", "Self-Supervised Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5302c6916935c03ba644f3434a260e4cd5c70c4b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the temporal dynamics of self-supervised learning (SSL) and its effect on transferability. The authors identify a transferability trade-off: intermediate checkpoints during SSL pretraining often yield stronger out-of-domain (OOD) generalization, whereas longer training enhances in-domain (ID) accuracy. Authors characterize SSL pretraining into three phases—plasticity, consolidation, and overspecialization—tracked via Fisher Information dynamics and deficit injection experiments. They further propose 2 training scheme for improvement:\n1. CP-guided Checkpoint Selection (CPCS) for identifying checkpoints near CP closure, improving OOD transfer;\n2. CP-guided Self-Distillation (CPSD) to distill representations from CP checkpoints into later ones, mitigating overspecialization effects"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Proposed that the learning process of SSL is stage-by-stage and provide evidences for verification.\n2. Proposed 2 training improvement scheme CPCS and CPSD without uesag of extra label.\n3. Propose novel explanation for OOD generalization decay at the end of training."}, "weaknesses": {"value": "1. The paper defines Critical Period closure as the epoch when the Fisher Information (FI) curve stabilizes—operationalized as a near-zero slope. It better comes up with a formal metric consider many other variables like batch size, hyperparameters and even downstream tasks.\n2. Need to provide more insights about why the FI dynamic performance differently among different SSL architectures. Are these differences come from the loss term or the contrastive and reconstruction-based SSL methods？"}, "questions": {"value": "1. Can Fisher Information be replaced with a more stable unsupervised proxy?\n2. Do critical periods also exist in language or multi-modal SSL?\n3. How the model capacity will influence the CP closure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eld1DC24de", "forum": "UxIRc97ecL", "replyto": "UxIRc97ecL", "signatures": ["ICLR.cc/2026/Conference/Submission19493/Reviewer_EgnN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19493/Reviewer_EgnN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572434266, "cdate": 1761572434266, "tmdate": 1762931398040, "mdate": 1762931398040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how self-supervised learners evolve over training and argues that there exist “critical periods” (CP) during which representations are highly plastic and best for transfer. The authors (i) document a transferability trade-off: longer pretraining continues to improve in-domain accuracy while hurting out-of-domain generalization, (ii) operationalize CP analysis for SSL via two tools, and (iii) propose two practical mechanisms: CP-guided checkpoint selection and CP-guided self-distillation that transfers intermediate “sweet-spot” layer features into the final model to balance ID and OOD performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of rethinking critical periods without labels by analyzing deficits and Fisher Information on the pretext loss is conceptually elegant and broadly applicable.\n2. The CP-guided checkpoint selection provides a simple yet practical tuning mechanism. The selective self-distillation strategy offers a concrete approach to restore transferability while retaining late-stage in-domain performance gains."}, "weaknesses": {"value": "1. The paper directly equates Fisher Information on the self-supervised pretext objective with parameter sensitivity to supervision signals to quantify plasticity, but it does not establish a theoretical connection between this proxy and transfer performance or generalization error.\n2. The paper posits a trade-off between sustained in-domain improvement and degraded out-of-domain transferability, which motivates the critical-period perspective, but lacks experiments linking this phenomenon to representation drift or task specialization.\n3. The use of broad phrases such as “across datasets or distributions” suggests generality, yet the paper neither defines nor categorizes the types of distribution shifts considered, nor decomposes which shift types benefit most, making the scope of conclusions unclear.\n4. The abstract claims coverage “across multiple SSL methods, architectures, and datasets,” but omits explicit statements on systematic coverage and boundary conditions. Without clarifying the circumstances under which the findings fail, the general conclusions risk overextension."}, "questions": {"value": "1. Why do different SSL methods in Section 2.2 exhibit distinct trends, and why do models such as SimCLR not show a clear “critical period” phase?\n2. How is Fisher Information computed in practice? Do you use full, block-diagonal, or diagonal approximations, per layer or per parameter group, and how frequently is it recomputed during pretraining?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s6kLgnCX3w", "forum": "UxIRc97ecL", "replyto": "UxIRc97ecL", "signatures": ["ICLR.cc/2026/Conference/Submission19493/Reviewer_2iUr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19493/Reviewer_2iUr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792532230, "cdate": 1761792532230, "tmdate": 1762931397339, "mdate": 1762931397339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the question of how long to train a visual self-supervised learning model, by looking at the critical periods happening throughout training. The paper presents metrics that allow to identify these periods, and studies how they correlate to downstream tasks. Finally, from these metrics the paper describes methods to reconcile optimal in-domain and out-of-domain performance on downstream tasks. Experiments are conducted on the ImageNet-1K and fMoW-RGB satellite imagery datasets, and several popular SSL methods are studied (SimCLR, VICReg, DINO and MAE)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper highlights a very interesting phenomenon in SSL, the fact that there exists learning phases that correlate with different levels of performance in out-of-domain or in-domain tasks.  The paper calls this phenomenon, which is the counterpart of overfitting in supervised learning, but for self-supervised learning: “critical periods”. The paper clearly explains how they found these critical periods with precise metrics, how these metrics correlate to downstream tasks and how to leverage these metrics to derive SSL models with better generalization capabilities.\n\n- There is existing large-scale evidence of this phenomenon in the SSL literature. Most SSL practitioners have already encountered these critical phases without putting a name on it.\n\n- The paper is well-written, easy to follow and with good presentation, the message and finding is simple and presented with clear experiments."}, "weaknesses": {"value": "- The paper focuses on two datasets: ImageNet, which make sense as a general pretraining datasets to learn visual representation, and fMoW-RGB, a satellite imagery datasets, so not generalist. But the use fMoW-RGB is not well motivated and does not help make the experiment convincing. Is the motivation to clearly identify what is ID and what is OOD ? I would appreciate more a focus on ImageNet data or even larger scale generalist data, with the objective to see if these critical periods are also observed in real-world or large-scale scenarios. There is some evidence in the literature that this is the case, in DINOv3, they use gram anchoring at the end of their pretraining to retrieve pic segmentation performance that the model has towards the beginning of training, this is very similar to what the authors describe line 78 “Intermediate checkpoints often achieve better out-of-domain (OOD) transfer than later checkpoints”.\n\n- The paper does not study critical phases with the prism of data overfitting, or doing too many passes on the same data. What would happen in an infinite data regime ? Would we still observe the same behaviour ? It would be great to have more experiments controlling the data distribution and studying the impact on these critical periods.\n\n- Other than the impact of data, the paper also misses the fact that a schedule is used on the learning rate and weight decay. Both are brought to 0 progressively throughout the training and that could be correlated with the critical period observed. I think some experiments with a fixed schedule should be conducted to remove this confounding factor.\n\n- Other similar metrics that have the same objective of characterizing SSL features are not acknowledged. For example, RankMe or LiDAR.\n\n- The introduction should say a little more about the experimental setup, in terms of dataset used and concrete results obtained."}, "questions": {"value": "Is there a link between critical periods and double descent ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kEPB7OcIuo", "forum": "UxIRc97ecL", "replyto": "UxIRc97ecL", "signatures": ["ICLR.cc/2026/Conference/Submission19493/Reviewer_HAzr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19493/Reviewer_HAzr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846963720, "cdate": 1761846963720, "tmdate": 1762931396816, "mdate": 1762931396816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the training dynamics of self-supervised learning methods and how performance evolves both in domain and out of domain during training. To understand why OOD performance drops even though ID performance increases during training the authors people to study this behavior through the lens of critical periods. But using two different criteria, the authors are effectively able to detect when overspecialization starts to happen which helps select the best model for OOD performance, or a more balanced model.\nFinally, the authors motivate and experiment with a distillation technique to help with overspecialization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The authors provide clear evidence of the studied problem, notably the drop in OOD performance during longer training\n\n- The proposed metrics, in particular the one based on Fisher Information, correlate really well with this behavior. This leads to an effective method to perform early-stopping\n\n- Experiments are performed at a good scale (R50/ViT-B, trained up to ImageNet for 1000 epochs) which adds to the relevance of the results\n\n- The proposed distillation method is useful and very relevant to current SSL research. A similar problem was shown to be present in DINOv3[1], which concurrently proposed another solution based on earlier checkpoint distillation.\n\n[1] Siméoni, Oriane, et al. \"Dinov3.\" arXiv preprint arXiv:2508.10104 (2025)."}, "weaknesses": {"value": "1) Focus on Satellite dataset in the main paper. The same results as in the main paper are performed on ImageNet in the appendix but should be emphasised more in the main paper to appeal to a broader audience.\n\n2) Throughout the paper, the considered evaluation is finetuning. However, the considered methods are more commonly used with lighter evaluations such as training a linear classifier. This would help make the results more relevant and may shed different insights."}, "questions": {"value": "1) For Probe 1, how much do you think that the sensitivity is correlated with a lower learning rate later in training ?\n2) Lines 254-255: when using noise during the deficit window, are data augmentation (jitter,crop,masking etc) still applied ? If not, how are the input image pairs constructed ?\n3) Figure 4: All methods except VICReg have their Fisher Information drop to zero, do you have any intuition why it does not for VICReg ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Avcaj4oIag", "forum": "UxIRc97ecL", "replyto": "UxIRc97ecL", "signatures": ["ICLR.cc/2026/Conference/Submission19493/Reviewer_12fS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19493/Reviewer_12fS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929754763, "cdate": 1761929754763, "tmdate": 1762931396308, "mdate": 1762931396308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to answer the following question: \"how long should SSL models be pretrained?\". They relate this problem to the notion of critical periods: where models exhibit high plasticity in early training stages, and go through a consolidation phase where OOD adaptability declines but ID performance improves. This phenomenon has been observed in supervised learning and this work claims to be the first systematic investigation of critical periods in SSL. They make three main contributions:\n- how to do CP analyses for SSL without requiring labels\n- how CP closure can guide the process of selecting intermediate checkpoints that show stronger OOD robustness\n- propose a distillation technique that uses sweet spot CP checkpoints as teacher and overspecialized networks as student, which leads to improved OOD generalization while maintaining ID performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. **Reformulation of CP for SSL**: This seems to be the main novelty of this work. To show critical periods in SSL, the authors have proposed two analyses techniques: (a) perturbations at different learning stages (b) FI matrix with respect to pretext tasks.\n2. **Practical Impact**: Contributions in Section 4 (CPCS and CPSD) can potentially have good practical impact providing guidance to select checkpoints based on transferability tradeoff, and improving OOD robustness. \n3. **Experiments**: This work is backed well with strong experiments. They have considered two real-world datasets (IM-1K and fMoW-rgb) and four methods (SimCLR, VICReg, MAE, DINO). There're some DINOv2 results as well but I am not sure why it's only mentioned in Appendix.\n4. **Reproducibility**: Hyperparameters and dataset details are well documented, and the methodology seems reproducible given the provided information.\n5. **Quality & Clarity**: Overall, the paper is well-written and has a logical flow. The schematic in Figure 1 is particularly effective in summarizing the conceptual framework, which is later supported by quantitative results."}, "weaknesses": {"value": "1. **Lack of Mathematical Rigor / Theoretical Depth**: The study is primarily empirical. While the FI metric offers some analytical grounding, the paper does not provide a principled explanation for why SSL exhibits critical periods or overspecialization. Some lightweight theoretical reasoning could strengthen the argument. \n\n2. **Issues with CP-guided self-distillation**:\n\n    (a) L117: it is not explained why the authors chose to distill early layers only. It's only later in Section 4.3 where the rationale behind this choice is addressed. \n\n    (b) Figure 6 is not convincing enough to justify distilling early layers. I think it will be useful to verify their claims by distilling the entire network and comparing performance gains.\n\n    (c) **Missing Ablations**: The number of layers distilled (L) and the distillation weight (λ) are unspecified and unexplored. Both likely affect results and reproducibility.\n\n3. **Figure 6**: It is not clear what the authors imply by \"stage\". \n4. **SimCLR's behavior**: I appreciate the attempt to explain why SimCLR's critical period closes much later (L375-376). I think this section needs more clarification. What are the differences in objectives of all the methods taken into consideration that leads to this behavior?\n5. **Definition of CP closure**: The paper briefly mentions detecting CP closure “L388- when the FI slope stabilizes (e.g., below a tolerance for p consecutive epochs)” (in Sec. 4.2). However, it is unclear whether this rule was actually used to determine the CP checkpoints reported in the experiments. For reproducibility and clarity, it would be valuable for the authors to specify the exact tolerance threshold and window length used in practice (if any)."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lywj8SYtuP", "forum": "UxIRc97ecL", "replyto": "UxIRc97ecL", "signatures": ["ICLR.cc/2026/Conference/Submission19493/Reviewer_U7ZN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19493/Reviewer_U7ZN"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762451539863, "cdate": 1762451539863, "tmdate": 1762931395775, "mdate": 1762931395775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}