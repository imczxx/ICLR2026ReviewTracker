{"id": "7SUUW2P9lK", "number": 4098, "cdate": 1757600030694, "mdate": 1763066295046, "content": {"title": "VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes", "abstract": "Is basic visual understanding really solved in state-of-the-art VLMs? We present VisualOverload, a slightly different visual question answering (VQA) benchmark comprising 2,720 question–answer pairs, with privately held ground-truth responses. Unlike prior VQA datasets that typically focus on near global image understanding, VisualOverload challenges models to perform simple, knowledge-free vision tasks in densely populated (or, overloaded) scenes. Our dataset consists of high-resolution scans of public-domain paintings that are populated with multiple figures, actions, and unfolding subplots set against elaborately detailed backdrops. We manually annotated these images with questions across six task categories to probe for a thorough understanding of the scene. We hypothesize that current benchmarks overestimate the performance of VLMs, and encoding and reasoning over details is still a challenging task for them, especially if they are confronted with densely populated scenes. Indeed, we observe that even the best model (o3) out of 37 tested models only achieves 19.8 % accuracy on our hardest test split and overall 69.5 % accuracy on all questions. Beyond a thorough evaluation, we complement our benchmark with an error analysis that reveals multiple failure modes, including a lack of counting skills, failure in OCR, and striking logical inconsistencies under complex tasks. Altogether, VisualOverload exposes a critical gap in current vision models and offers a crucial resource for the community to develop better models.", "tldr": "A benchmark for visual reasoning in highly dense scenes.", "keywords": ["visual question answering", "dense scenes", "dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/63bc79ad2743183799708ae800687e40cf27c27c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a new benchmark for evaluating VLMs focusing on activity, reasoning, OCR, scene, attribute, and counting tasks for dense images/paintings (i.e. complex images containing many objects). These are basic perception tasks that VLLMs should be well-performing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- the idea of finding failure models of VLMs is interesting and of value to the community"}, "weaknesses": {"value": "- It seems to me that the paper results contradict the paper claims. From Table 2 we can see pretty decent performance of VLLMs for easy and medium tasks, also scaling with model size. There's a hard set where most of the VLLMs don't do well but i think even in well established benchmarks there are test samples that the models don't work. \n- Some issues identified in the paper (e.g. counting, attributes, OCR) are already known  to be existing issues with current VLLMs. \n- The contribution of such a small size benchmark is not enough for ICLR"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8UZkg5kJ8C", "forum": "7SUUW2P9lK", "replyto": "7SUUW2P9lK", "signatures": ["ICLR.cc/2026/Conference/Submission4098/Reviewer_Hz6e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4098/Reviewer_Hz6e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761340608218, "cdate": 1761340608218, "tmdate": 1762917177737, "mdate": 1762917177737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "k5UTv4XnLj", "forum": "7SUUW2P9lK", "replyto": "7SUUW2P9lK", "signatures": ["ICLR.cc/2026/Conference/Submission4098/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4098/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763066294173, "cdate": 1763066294173, "tmdate": 1763066294173, "mdate": 1763066294173, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VisualOverload, a Visual Question Answering (VQA) benchmark designed to test state-of-the-art Vision-Language Models (VLMs) in dense, high-resolution scenes—addressing a gap in existing benchmarks that overestimate VLMs’ visual understanding by focusing on global or simple tasks.  The benchmark comprises 150 public-domain high-resolution paintings (mostly ~4K resolution) with dense, detail-rich scenes, paired with 2,720 manually annotated question-answer pairs across 6 tasks: activity recognition, attribute recognition, counting, OCR, reasoning, and scene classification. Binary questions are paired with logical opposites to measure consistency, and questions are split into easy/medium/hard based on model performance; ground truths are private to avoid leakage.  Evaluations of 37 VLMs (open-weight: 0.4B–109B params; proprietary: e.g., o3, Gemini 2.0 Flash) show even the top model (o3) achieves only 19.6% accuracy on the hard split and 69.5% overall. VLMs struggle most with counting (top accuracy: 41.7%) and OCR (top accuracy: 62.7%), and exhibit logical inconsistencies in reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces the VisualOverload benchmark, which specifically focuses on testing VLMs’ fine-grained visual understanding in dense, high-resolution scenes—an area where existing VQA benchmarks (which prioritize global scene understanding or simple retrieval tasks) fail. By using 150 public-domain high-resolution paintings (mostly ~4K) with manually annotated questions across 6 core tasks (e.g., counting, OCR, reasoning) and pairing binary questions with logical opposites, the benchmark effectively exposes VLMs’ limitations in handling real-world complex visual scenarios, filling a key gap in VLM evaluation \n2. The study evaluates 37 diverse VLMs (covering open-weight models of varying sizes and proprietary frontier models) and uses a well-calibrated difficulty split (easy/medium/hard based on average model performance) to ensure meaningful performance differentiation. Additionally, its detailed error analysis uncovers systematic VLM failures—such as poor counting accuracy for large numbers, severe OCR errors (high normalized Levenshtein edit distances), and logical inconsistencies in paired questions—providing actionable insights for improving VLM design, rather than just reporting performance metrics"}, "weaknesses": {"value": "1. Insufficient Validation of Benchmark Generalization and Fairness: The benchmark’s image corpus is limited to 150 public-domain paintings, which are culturally specific (curated from museum collections) and may not represent the diversity of real-world dense scenes (e.g., urban crowds, medical imaging, industrial layouts). The paper does not evaluate whether VisualOverload’s findings generalize to non-artwork dense scenarios, raising concerns about its external validity. Additionally, while it mentions removing language biases (Section 1-51), it does not assess potential cultural biases in annotations (e.g., questions tailored to Western art contexts) or how these might impact model performance across diverse VLMs trained on different data.\n2. The paper’s discussion of related work (Section 5) remains relatively superficial, failing to deeply contextualize VisualOverload against recent high-resolution or dense-scene VQA benchmarks (e.g., Wu & Xie, 2024; Shi et al., 2025). It only briefly mentions that prior high-resolution benchmarks focus on \"needle-in-the-haystack\" retrieval rather than full-scene complexity but lacks a rigorous, side-by-side comparison of benchmark design (e.g., image source, task scope, annotation methodology) or quantitative performance contrasts with these existing datasets."}, "questions": {"value": "The benchmark’s image corpus is limited to 150 public-domain paintings, which are mostly from museum collections and may have cultural specificity. Could you elaborate on whether there are plans to expand the image sources to include more diverse real-world dense scenes (such as urban crowds, medical images, or industrial layouts) in the future? And how do you intend to verify whether the limitations of VLMs revealed by the current painting-based benchmark can be generalized to these non-artwork dense scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NVNC69VkiR", "forum": "7SUUW2P9lK", "replyto": "7SUUW2P9lK", "signatures": ["ICLR.cc/2026/Conference/Submission4098/Reviewer_48xJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4098/Reviewer_48xJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750551090, "cdate": 1761750551090, "tmdate": 1762917177512, "mdate": 1762917177512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a VQA benchmark in dense, high-resolution (visually overloaded) scenes with 2,720 manually curated question–answer pairs over activity recognition, attribute recognition, counting, OCR, visual reasoning, and global scene classification tasks. The evaluation results show that the sate-of-the-art models struggle significantly in fine-grained understanding in dense settings, particularly for counting and OCR. The analysis of this paper finds that systematic inconsistencies and shortcut biases hinder robust performance in visually overloaded settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The images are new from existing dataset with high resolution and fine-grained details, and annotations are labeled manually.\n2. The quality of the data is well controlled, while 37 VLMs are evaluated on proposed dataset and manually verified the correctness of ground truths."}, "weaknesses": {"value": "1. It is a consensus within the community that current VLMs have shortcomings in OCR and counting tasks, while this paper lacks discussions comparing related benchmarks focusing on these tasks to demonstrate the novelty to propose VisualOverload.\n2. Some results of state-of-the-art models are missing, such as GPT-4o and Gemini-2.5 Pro.\n3. The image are collected from Google Arts & Culture, the evaluation field of images may be limited."}, "questions": {"value": "1. Does the conclusion in Section 4 hold consistent across the evaluation results of all the state-of-the-art models, or does it only apply to some of the current models? Are there some differences in the performance over these tasks of different models within the community?\n2. For easy questions, as shown in Table 2, nearly all models achieve above 98 points. Is this easy part of the dataset necessary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wvJ1uJ22ji", "forum": "7SUUW2P9lK", "replyto": "7SUUW2P9lK", "signatures": ["ICLR.cc/2026/Conference/Submission4098/Reviewer_1ZXV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4098/Reviewer_1ZXV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818163790, "cdate": 1761818163790, "tmdate": 1762917177290, "mdate": 1762917177290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VisualOverload, a new benchmark designed to evaluate visual question answering (VQA) models, specifically targeting their performance in densely populated, high-resolution scenes. The aim is to probe basic visual understanding in models, particularly their ability to handle fine-grained recognition in complex scenes. The authors identify that while state-of-the-art vision-language models (VLMs) perform well on simpler VQA tasks, they struggle with these more nuanced, visually overloaded contexts, revealing significant weaknesses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed dataset and the specific focus on dense, high-resolution images are innovative. The focus on challenging tasks like counting and OCR in complex settings provides important insights into the limitations of current VLMs.\n- The evaluation of 37 models is thorough."}, "weaknesses": {"value": "- Although the paper discusses the impact of resolution on model performance, the analysis is relatively simple. Different models may have varying processing capabilities to respond differently to high-resolution images, possibly due to differences in the amount of pre-trained data; the paper does not delve into these architectural differences.\n- The error analysis section reveals the model's failure modes in tasks such as counting, OCR, and logical consistency. However, this analysis mainly stays at the task level and lacks in-depth analysis of the root causes of model failures.\n- While the paper reveals the limitations of current VLMs in complex scenarios, it lacks specific suggestions on how to improve these models. For example, what problems were discovered that could provide insights for the future development of these models?\n- While Figure 1 shows an example problem and corresponding image for one task, there aren't enough images to demonstrate the diversity of the dataset. Several example images for different task categories could be added, showing how these images are designed and how different tasks combine these images to evaluate model performance.\n- The difficulty level of the dataset (easy, medium, hard) is an important point of analysis in the paper, but there is currently no clear visualization in the paper to show how these difficulty levels affect the model's performance."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9b1Mew1J1L", "forum": "7SUUW2P9lK", "replyto": "7SUUW2P9lK", "signatures": ["ICLR.cc/2026/Conference/Submission4098/Reviewer_Mb4w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4098/Reviewer_Mb4w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023736487, "cdate": 1762023736487, "tmdate": 1762917177095, "mdate": 1762917177095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors develop VisualOverload, a visual question answering (VQA) benchmark visual language models (VLMs) on a focus of visually dense images. The paper motivates this by proposing a possible bottleneck in modern VLMs, namely the visual encoder, which may limit the amount of information that can be fit in a fixed set of tokens, and argue VisualOverload helps test this. The authors collect 150 photoscans of real artworks from Google Arts & Culture and develop 2k+ questions, including counting, global scene understanding, and OCR -type tasks. The authors find that the best performing model, o3, was able to only achieve 69.5% overall."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The research question is well-motivated. Many benchmarks really do tend to focus on global scene understanding. While visually dense images do occur in many benchmarks, to the best of my knowledge it is true that this has never been a central focus of any other benchmark to this date.\n2. The research hypothesis that the vision encoder is a bottleneck is really interesting. It's definitely a point of concern, and a study on that hypothesis would be very useful.\n3. The authors evaluate on a comprehensive set of vision language models.\n4. The authors clearly put significant effort in the curation of the images and high-quality questions."}, "weaknesses": {"value": "1. In the introduction, the authors claim a motivation to test visually dense scenes is a hypothesis that the vision encoder is a bottleneck for VLMs. However, their experimental setup does not do a good job of directly testing this:\n\t* While encoders might compress images to a fixed number of tokens, not all *VLMs* encode into a fixed number of tokens.\n\t\t* Authors use o3 as an example of a frontier model that does poorly - o3 has an option to use either low fidelity or [high fidelity](https://platform.openai.com/docs/guides/images-vision?api-mode=responses#gpt-4o-gpt-4-1-gpt-4o-mini-cua-and-o-series-except-o4-mini)\n\t\t* InternVL3 also uses [dynamic resolution](https://internvl.github.io/blog/2025-04-11-InternVL-3.0/)\n\t\t* This would have been an interesting experimental setup to compare fixed resolution performance vs dynamic resolution\n\t* Error analysis and conclusion do not study whether or not visual cluttering was the cause of errors.\n2. The paper lacks a formal definition of image complexity as a metric for selection. While the authors state that it is hard to formally define it, an alternative that might be useful would be some statistics about the number of subjects or objects in each photo, and compare it against other VQA benchmarks in a more quantitative matter.\n3. Some more clarification on how the easy/medium/hard test splits were created would be helpful.\n   \"We divide our questions into three difficulty levels—easy, medium, and hard—based on model performance in Sec. 3. The thresholds are defined by the percentage of correct responses: \\[0, 20\\] for hard, (20, 90) for medium, and \\[90, 100\\] for easy.\"\n\t* Does this mean:\n\t\t* a) if average model performance on a question is lower than 20%, then the question is in the hard split, or\n\t\t* b) hard test split is designed so that all models perform <= 20%.\n\t* In either case, this makes the claim that \"even the best model (o3) … only achieves 19.8% accuracy on our hardest test split\" somewhat circular.\n4. Lack of image diversity:\n\t* Despite a focus on visual clutterness, the paper also happens to only include historical artworks, which may result in an important confounding variable during evaluation.\n\t* Most paintings also have humans as the main subjects. While not as large of a concern, some statistics about the dataset and its subjects would also have been useful.\n5. The claims of comparisons against existing benchmarks are not substantiated: \n   \"Existing VQA benchmarks underestimate the true difficulty of visual reasoning. They rely on low-resolution images, recycled content, and automatically generated questions that encourage shallow pattern matching rather than genuine scene understanding\"\n\t* Do you mean VQA benchmarks as in the VQA v1/VQA v2 *datasets/benchmarks*, and not VQA as a task? This is more than a bit unfair to others' works on VQA *tasks*. For example, the below are more recent VQA benchmarks that use high resolution photos and is not \"recycling\" COCO data. Just a few examples\n\t\t* [HRVQA](https://arxiv.org/abs/2301.09460): high-resolution images (1024x1024).\n\t\t* [DocVQA](https://openaccess.thecvf.com/content/WACV2021/html/Mathew_DocVQA_A_Dataset_for_VQA_on_Document_Images_WACV_2021_paper.html): most photos have more than [1.68k px](https://huggingface.co/datasets/lmms-lab/DocVQA) in width.\n\t\t* [SlideVQA](https://huggingface.co/datasets/NTT-hil-insight/SlideVQA): larger than 1k px in width.\n\t\t* [Blink](https://huggingface.co/datasets/BLINK-Benchmark/BLINK): very large images, high quality questions, not recycled content.\n\t* This should be completely reworded and have a fair comparison."}, "questions": {"value": "1. Could you clarify how the easy/medium/hard test splits were created?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KamsF2agk5", "forum": "7SUUW2P9lK", "replyto": "7SUUW2P9lK", "signatures": ["ICLR.cc/2026/Conference/Submission4098/Reviewer_sMgK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4098/Reviewer_sMgK"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission4098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050314316, "cdate": 1762050314316, "tmdate": 1762917176929, "mdate": 1762917176929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}