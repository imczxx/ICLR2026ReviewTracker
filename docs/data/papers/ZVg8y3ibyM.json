{"id": "ZVg8y3ibyM", "number": 10493, "cdate": 1758173692093, "mdate": 1759897647537, "content": {"title": "GuidedBench: Measuring and Mitigating the Evaluation Discrepancies of In-the-wild LLM Jailbreak Methods", "abstract": "Despite the growing interest in jailbreaks as an effective red-teaming tool for building safe and responsible large language models (LLMs), flawed evaluation system designs have led to significant discrepancies in their effectiveness assessments. With a systematic measurement study based on 37 jailbreak studies since 2022, we find that existing evaluation systems lack case-specific criteria, resulting in misleading conclusions about their effectiveness and safety implications. In this paper, we introduce GuidedBench, a novel benchmark comprising a curated harmful question dataset and GuidedEval, an evaluation system integrated with detailed case-by-case evaluation guidelines. Experiments demonstrate that GuidedBench offers more accurate evaluations of jailbreak performance, enabling meaningful comparisons across methods. GuidedEval reduces inter-evaluator variance by at least 76.03%, ensuring reliable and reproducible evaluations. We reveal why existing jailbreak benchmarks fail to evaluate accurately and suggest better evaluation practices.", "tldr": "This paper introduces a new guideline-based evaluation benchmark GuidedBench, to accurately test LLM jailbreaks.", "keywords": ["Large Language Models", "Jailbreak Attacks", "Evaluation System", "Benchmark"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f453afca2d0809d90f5fbf415f05adbbcd14e75.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents GuidedBench and GuidedEval, a benchmark and an accompanying evaluation framework for assessing LLM jailbreak methods. The authors curate and filter a set of high-quality harmful questions spanning 180 core topics plus 20 additional, emerging topics. GuidedEval supplies case-specific checklists of entities and actions; a victim model’s response is scored by detecting whether those checklist items appear. Experiments indicate that GuidedBench is a more challenging benchmark for jailbreaks, and that GuidedEval yields more accurate and robust evaluations than existing approaches."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper clearly identifies concrete shortcomings of current jailbreak benchmarks and evaluation practices, and proposes a generally stronger alternative.\n\n- It contributes a carefully curated, high-quality dataset for LLM jailbreak evaluation, which is a valuable infrastructure for this research area.\n\n- The empirical study probes failure modes in current evaluation methods and investigates why those methods misjudge jailbreak effectiveness."}, "weaknesses": {"value": "- The final GuidedBench dataset contains 200 questions distilled from roughly 1,800 candidates, which may still be small relative to the breadth and diversity of real-world jailbreak intents.\n\n- Although the experiments are comprehensive, the paper’s novelty primarily resides in GuidedEval. Since this paper is outside the \"datasets and benchmarks\" track, it may lack of method novelty."}, "questions": {"value": "In Equation (1), ASR currently gives equal weight to all checklist items. Have you considered alternative weighting schemes (e.g., TF-IDF, risk-based weights, or topic-specific priors)? Since in practice, not all entities/actions contribute equally to the harmful capability or risk of a response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3mMrTwSyqk", "forum": "ZVg8y3ibyM", "replyto": "ZVg8y3ibyM", "signatures": ["ICLR.cc/2026/Conference/Submission10493/Reviewer_F8xn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10493/Reviewer_F8xn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761669180333, "cdate": 1761669180333, "tmdate": 1762921781015, "mdate": 1762921781015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents GuidedBench and GuidedEval, a harmful question dataset, and GuidedEval, an evaluation system integrated with detailed case-by-case guidelines for assessing julibreak capabilities. GuidedEval shows good agreement with humans and low variance. The authors use GuidedBench and GuidedEval to run a large set of experiments and compare different attacks, different evaluation methods, Misjudged Cases, and score distribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The data and system are effective in assessing jailbreak capabilities.\n2. The method addresses a gap in the current evaluation of jailbreak attacks.\n3. Analysis and results aim to provide a more accurate view of the field compared to other methods."}, "weaknesses": {"value": "1. Many parts of the paper are lacking in description or rely on the appendix, making them hard to follow. For example, there is no explanation of the experimental setup for the \"Misjudged Cases\" part.\n2. The writing makes it hard to understand how the conclusions are drawn from the raw results. For example, after Tables 2 and 3, there are very general takeaways that do not refer to the table and the results.\n3. This claim: \"Our leaderboard analysis reveals that prior evaluation systems, especially keyword-based ones, have inaccurately assessed the performance of many jailbreak methods, whereas GuidedEval provides a reasonable and accurate evaluation.\" Which is important, but seems unsupported. There is no direct comparison of the evaluation methods based on external evaluation. The human assessment is a standalone one. The ranking comparison reveals decent similarity. It appears that the authors believe their method is more accurate and less prone to mislabeling responses as successful attacks at the instance level, but no such comparison is provided.\n4. The \"Misjudged Cases\" context is unclear. It seems that the authors assume their method is correct and use it to assess the other methods' misjudgment cases pattern.\n5. The baseline, for example, in table 4, e.g,. Simple LLM-based, seems like a \"straw man\" and interesting to see a comparison with LLMaaJ, but with a detailed prompt that instructs the model what qualifies as a successful attack.\n6. Why do the human validations not compare the other method to GuidedBench?"}, "questions": {"value": "1. No discussion on cost, as your method requires more calls per instance.\n2. No reference to other works that look into rubric-based evaluation. For example:\n  a. \"Checklists Are Better Than Reward Models For Aligning Language Models\": https://arxiv.org/abs/2507.18624\n  b. \"WildIFEval: Instruction Following in the Wild\": https://arxiv.org/abs/2503.06573\n3. Figure 4 seems to be a bit gamed, not the same scales. Adding a numeric score to compare the distributions will be helpful.\n4. Probably the parts that ensure GuidedBench reliability need to come before the main results.\n5. I am open to raising the score upon a good response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1k9QftOAKg", "forum": "ZVg8y3ibyM", "replyto": "ZVg8y3ibyM", "signatures": ["ICLR.cc/2026/Conference/Submission10493/Reviewer_PHZy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10493/Reviewer_PHZy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775922031, "cdate": 1761775922031, "tmdate": 1762921780336, "mdate": 1762921780336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Guided bench addresses an important problem, which is the evaluation of jailbreak attacks as quite inconsistent. So the authors analyze several jailbreak papers and identify a fair bunch of systematic issues with the current evaluation methods. And they propose an improved method called guided bench, which is a benchmark with case-specific evaluations.\n\nThe very fact that this is a systematic study revealing evaluation discrepancies is quite valuable in of itself. Perhaps focusing on entities and actions that will actually assist attackers is more meaningful than simple keyword matching or wave harmfulness measurements and judgments. The reduction in variance is quite substantial."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It's an important problem, that current jailbreak methods produce wildly inconsistent results with often a large number of attempts. Keyword based methods can even reverse rankings of attack efficiency. The paper is quite comprehensive and evaluates a large number of cases across 10 attack methods and gives thorough evidence. There are concrete entities and actions to actually help attacks, which makes it more objective as well as interpretable, and the 76% reduction is quite important. The agreement with human annotators is quite strong. Fewer false positives on “misjudged” cases is a strength as well."}, "weaknesses": {"value": "Assumes all scoring points equally important, somewhat unrealistic practically - some entities are clearly more critical for harm. \n\nThe dataset size is fairly small with no strong analysis on that. \n\nThere is not any evaluation on completely novel harmful question types outside the predefined categories. \n\nThe dataset filteration could introduce selection bias and miss important information. (authors admit guidelines can miss non-essential but relevant details).\n\nEvaluators are “less safety-restricted,” so findings may shift under stricter judges."}, "questions": {"value": "Your scoring treats entities and actions with equal weight. How resilient is this to shallow name-dropping or generic steps that look like hits but provide little to no operational value? \n\nThe topic mix could hide instability; could you provide more detail on the reduction?\n\nRegarding the dataset (200 cases), please provide a simple power analysis and confidence intervals for the additional set of 20."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2gi6Gtjapu", "forum": "ZVg8y3ibyM", "replyto": "ZVg8y3ibyM", "signatures": ["ICLR.cc/2026/Conference/Submission10493/Reviewer_ZA2A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10493/Reviewer_ZA2A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932598369, "cdate": 1761932598369, "tmdate": 1762921779524, "mdate": 1762921779524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GuidedBench, a benchmark and accompanying the GuidedEval framework, which is designed to improve the reliability and interpretability of large language model (LLM) jailbreak evaluation. The authors argue that existing evaluation systems, particularly keyword-based methods or generic LLM-as-a-judge pipelines, produce inconsistent and inflated attack success rates (ASRs), with substantial variance across evaluators, target models, and topics.\n\nTo address this, GuidedEval replaces binary success judgments with a structured, guideline-based scoring process. Each harmful prompt is annotated with explicit “entity” and “action” criteria describing the concrete elements that would constitute a successful jailbreak. This design enables partial credit and measures actionable harmfulness rather than surface-level lexical overlap.\n\nThe benchmark comprises 200 curated harmful questions (180 core and 20 additional topics) derived from an initial pool of 1,823 items. Ten jailbreak methods are evaluated against five victim LLMs (e.g., GPT-4-turbo, Claude-3.5, Llama-3) and three evaluator LLMs. Results show that GuidedEval yields lower ASRs (~30% versus >90% in prior reports), reduces evaluator variance by 76–88%, and produces smoother, graded score distributions.\n\nThe paper positions GuidedEval as a high-precision, reference benchmark, favoring consistency, human alignment, and interpretability over broad coverage, to enable reproducible measurement of jailbreak success and provide a stable foundation for future safety evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "(1) Clear motivation and problem identification. \\\nThe paper isolates a key weakness in current jailbreak research: inflated or unstable ASR estimates caused by loosely defined evaluation metrics. It argues persuasively that improving the precision and reproducibility of evaluation is essential to progress in this area. The problem framing is clear and well justified.\n\n(2) Rigorous, interpretable, and actionability-based scoring design. \\\nThe entity/action rubric formalizes jailbreak success in terms of what would be required for an adversary to act on the model’s output. By requiring that a successful response include specific entities and concrete actions, GuidedEval ensures that a “successful jailbreak” reflects genuinely actionable information, not just surface-level mentions of dangerous terms. This distinction addresses a common criticism of LLM safety research, namely, that models can appear harmful for reproducing information easily obtainable through a search engine. The rubric therefore defines success more meaningfully: a model is only penalized when its response actually assists a harmful intent, providing a stronger conceptual basis for measuring model safety.\n\n(3) Strong empirical coverage and transparent comparisons. \\\nThe experiments evaluate 10 jailbreak methods across 5 victim LLMs and 3 evaluator LLMs, offering a comprehensive and replicable study. The figures and tables (especially Table 6, Table 7, and Figure 4) clearly demonstrate GuidedEval’s improvements in evaluator consistency, variance reduction, and distributional granularity.\n\n(4) Demonstration of good practices for validating LLM-as-a-judge evaluation. \\\nThe work provides a useful example of how to validate an LLM-based judging pipeline through variance analysis, inter-evaluator agreement, and human validation. These methodological checks reflect careful experimental design and could inform best practices for future LLM evaluation benchmarks."}, "weaknesses": {"value": "A recurring theme in the paper’s claims is the improved stability and reliability of GuidedEval across multiple axes of variation. These axes can be grouped into three distinct dimensions: (A) stability across victim LLMs and harmful-topic categories, (B) consistency across evaluation schemes and their resulting method rankings, and (C) agreement across evaluator LLMs used to judge the same responses. Each dimension captures a different aspect of robustness: (A) tests whether the measured effectiveness of a jailbreak method depends on which model or topic is chosen, (B) tests whether different evaluation systems produce comparable leaderboards, and (C) tests whether evaluations remain stable when different LLM judges are used.\n\nWhile the qualitative evidence for these forms of stability is persuasive, the paper largely presents them descriptively—through heatmaps, case studies, and variance tables—rather than with concise quantitative summaries. Because reproducibility and reliability are central to evaluation methodology, it would strengthen the empirical case to quantify how much of the variation in attack success rates (ASRs) arises from true differences between jailbreak methods versus contextual or evaluator-dependent noise. The following comments outline specific analyses and clarifications that could make the results clearer and more convincing.\n\n(1) Clarifying the quantitative relationship between GuidedEval and other LLM-as-a-judge systems (Figure 2; Table 5) \\\nFigure 2 shows that other LLM-as-a-judge evaluation systems are positively correlated with GuidedEval, suggesting broad alignment. Table 5, however, indicates that the main distinction lies in GuidedEval’s ability to grade borderline or “tricky” responses more harshly and accurately. If this interpretation is correct, emphasizing that contrast explicitly would clarify the main advantage: GuidedEval aligns with other evaluators on most examples but better identifies subtle evaluation errors.\n\nIt remains unclear whether this improvement primarily reflects stricter dataset curation and human-authored guidelines, or whether aspects of the pipeline could, in principle, be automated. Are the guideline-writing and filtering steps fully manual, partially assisted by LLMs, or more automated? More detail would help readers understand whether the gains in stability and precision come at the cost of scalability and generalization to new prompts or domains.\n\nFinally, the paper could briefly situate GuidedEval relative to recent work that propagates or corrects LLM-as-a-judge error statistically (e.g., Prediction-Powered Inference, Using Large Language Model Annotations for the Social Sciences). These frameworks show that LLM-based evaluation error can, in principle, be corrected statistically rather than solely through more specific prompts or guidelines. If GuidedEval’s benefit lies primarily in lowering annotation variance and reducing obvious evaluation errors, its broader significance could be articulated as improving the stability of evaluations across models, topics, and evaluators—something that complements, rather than substitutes for, statistical error-correction frameworks. A short discussion of this connection would situate the method more clearly within the broader evaluation literature.\n\n(2) Quantifying cross-victim and cross-topic stability (Tables 2–3). \\ \nThe paper claims that \"some jailbreak methods show dependence on specific victim LLMs or harmful topics\", but does not quantify how much a fixed jailbreak method’s scores vary along these axes under different evaluation schemes. Two lightweight quantitative analyses would make this claim concrete.\n\nFor each jailbreak method \\$m\\$ and evaluation scheme \\$e\\$, compute the standard deviation or coefficient of variation of ASR scores across victims (pooled over topics) and across topics (pooled over victims):\n\n$$\n\\sigma\\_{m,e}^{(\\mathrm{victim})} = \\mathrm{sd}\\_v\\big(\\bar{S}\\_{m,e}(v)\\big), \\quad \n\\sigma\\_{m,e}^{(\\mathrm{topic})} = \\mathrm{sd}\\_t\\big(\\bar{S}\\_{m,e}(t)\\big)\n$$\n\nwhere \\$\\bar{S}\\_{m,e}(v)\\$ is the mean ASR of method \\$m\\$ under scheme \\$e\\$ for victim \\$v\\$, and \\$\\bar{S}\\_{m,e}(t)\\$ is the mean ASR for topic \\$t\\$. Averaging these dispersions across methods yields a per-scheme stability measure. A test such as Levene’s or Brown--Forsythe could then check whether GuidedEval’s cross-victim/topic variance is significantly lower than that of PAIR, StrongREJECT, or Keyword.\n\nA brief two-way ANOVA (factors = Victim and Topic) or a mixed-effects model could then partition total variance:\n\n$$\n\\bar{S}\\_{m,e}(v,t) = \\mu + \\alpha\\_v + \\beta\\_t + (\\alpha\\beta)\\_{v,t} + \\epsilon\n$$\n\nor\n\n$$\nS \\sim 1 + (1 \\mid \\mathrm{victim}) + (1 \\mid \\mathrm{topic}) + (1 \\mid \\mathrm{prompt})\n$$\n\nestimating \\$\\mathrm{Var(victim)}\\$, \\$\\mathrm{Var(topic)}\\$, \\$\\mathrm{Var(prompt)}\\$, and \\$\\mathrm{Var(residual)}\\$.  \nIf GuidedEval truly stabilizes evaluation, the proportion of variance explained by contextual factors (victim LLM, topic) should be smaller than for other systems. These analyses would quantify how much of the observed ASR fluctuation comes from genuine differences in jailbreak method performance versus contextual effects (victim LLM, topic) or random noise, directly testing the claim that GuidedEval reduces dependence on the evaluation setting.\n\n(3) Providing quantitative summaries of cross-evaluation ranking consistency (Figure 3; Tables 4 & 7) \\\nThe heatmaps in Figure 3 show that evaluation choice changes the ordering of jailbreak methods, with keyword-based systems distorting rankings relative to GuidedEval. To make this point quantitative, the authors could report pairwise rank-correlation coefficients (Spearman $\\rho$ or Kendall $\\tau$) between evaluation systems. Including a small table of $\\rho$/$\\tau$ values would measure how closely different evaluation methods agree and highlight where disagreements arise.\n\nIf the human validation subset in Table 7 is large enough to derive jailbreak method success rankings as in Figure 3, the authors could also compare the human-based rankings to the rankings of GuidedEval, other LLM-as-a-judge systems (non-guideline-based), and keyword-based evaluations.\n\n(4) Reporting standardized reliability metrics for inter-evaluator agreement (Table 6). \\\nTable 6 shows that GuidedEval substantially lowers score variance across evaluator LLMs. To make this result more interpretable, a single reliability coefficient such as an intra-class correlation (ICC) or Krippendorff’s $\\alpha$ could be reported over the three evaluator LLMs (GPT-4o, DeepSeek-V3, Doubao). These statistics quantify the degree to which different evaluators produce consistent judgments when scoring the same items.  \n\nA high ICC or $\\alpha$ would indicate that most of the variation in scores comes from true differences in the items being evaluated, rather than from evaluator-specific disagreement or randomness. In other words, a high value implies that the evaluation procedure is robust to the choice of judge model, whereas a low value would suggest instability or bias introduced by the particular evaluator LLM. Reporting one of these coefficients would therefore situate GuidedEval’s variance reduction within a familiar inter-rater reliability framework and provide a single, interpretable measure of how judge-agnostic the method truly is.\n\n(5) Clarifying the trade-off between dataset curation, guideline specificity, and scalability \\\nFigure 2 shows that GuidedEval’s outputs are strongly correlated with those of other LLM-as-a-judge systems, while Table 5 suggests that its main advantage lies in assigning lower ASR scores to borderline or ambiguous “mistake” cases. If this interpretation is correct, it would help to state this contrast explicitly: GuidedEval generally agrees with other evaluators but more effectively penalizes subtle misjudgments. However, it remains unclear whether this improvement primarily stems from human-authored, case-specific guidelines and manual filtering, or whether parts of the process are automated. More detail on the extent of human versus automatic curation and guideline generation would clarify whether GuidedEval’s gains in precision come at the cost of scalability and adaptability to new prompts or domains.\n\nThese questions are important for understanding whether GuidedEval’s improvements are intrinsic to its methodology or simply the result of intensive human curation. This discussion could also briefly connect to statistical methods for propagating LLM-as-a-judge error (e.g., Prediction-Powered Inference, Using Large Language Model Annotations for the Social Sciences), which demonstrate that LLM-based evaluation error can, in principle, be corrected statistically rather than solely through more specific prompts or guidelines. If GuidedEval’s benefit lies primarily in lowering annotation variance and reducing clear-cut misclassifications, its broader contribution could be framed as improving the stability of evaluation outcomes across models, topics, and evaluators—complementing, rather than substituting for, statistical error-correction frameworks.\n\n(6) Clarifying the trade-off between benchmark reliability and adversarial realism \\\nThe dataset-construction principles (Model Refusal, Direct Requirement, Malicious Intent, Answerable Structure) emphasize clean, unambiguous, answerable harmful queries that models reliably refuse without jailbreaks. This choice enhances consistency and interpretability but narrows the scope of what is being tested. The benchmark thus evaluates jailbreaks on explicit refusals, not on ambiguous or dual-use scenarios that real adversaries might exploit.\n\nThis reflects a reasonable design trade-off between reliability and realism, but the rationale could be stated more explicitly. Many of the most consequential jailbreaks occur in ill-posed or simulation-based settings, the very cases excluded here. From a minimax or worst-case perspective on model safety, these ambiguous cases may pose higher real-world risk. Clarifying early that GuidedBench serves as a calibration benchmark for consistent measurement, rather than a stress-test for adversarial realism, would make this framing clearer and position it as complementary to open-ended, “in-the-wild” jailbreak studies."}, "questions": {"value": "1. Table 5 seems to suggest that GuidedEval better identifies borderline or “mistake” examples than other LLM-as-a-judge systems. Is this interpretation correct, and how were these tricky cases identified or sampled?\n\n2. Could you clarify the degree of human versus automated involvement in dataset curation and guideline generation? Were entities and actions drafted manually, LLM-assisted, or generated automatically? Understanding this would help readers assess how scalable and generalizable the framework is.\n\n3. In addition to Tables 2–3 (which report only GuidedEval), could you include some quantitative comparison across evaluation systems to support the claim that “notably, some jailbreak methods show dependence on specific victim LLMs or harmful topics”? For example, reporting cross-victim/topic variance metrics ($\\sigma_{m,e}^{(\\mathrm{victim})}$, $\\sigma_{m,e}^{(\\mathrm{topic})}$) or ANOVA-style variance decomposition results would substantiate this point.\n\n4. Table 6 demonstrates reduced variance across evaluator LLMs; would you consider reporting a standardized inter-rater reliability coefficient, such as an intra-class correlation (ICC) or Krippendorff’s $\\alpha$, to provide a single interpretable measure of evaluator agreement?\n\n5. The curated dataset contains $n = 200$ questions, which places this work in a relatively small-sample regime. I recognize the difficulty of scaling expert-authored, guideline-based scoring to larger datasets and that many existing jailbreak benchmarks operate at similar scales. Still, it would be valuable to discuss how sample size might affect the robustness of the reported findings and what directions could enable future scaling of this approach—either to improve statistical power or to test whether the observed stability trends hold for larger and more diverse benchmarks.\n\nSmall line-level edits\n- Line 42: \"which directly hinders comparisons\" -> \"which directly hinder comparisons\"\n- Line 47: \"also leads evaluator LLMs fail\" -> \"also leads evaluator LLMs to fail\"\n- Line 124: \"jailbreak work\" -> \"jailbreak works\"\n- Line 494: \"IRB\" -> \"the IRB\"\n- Figure 7: \"Entitiy 1-3\" -> \"Entity 1-3\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HWrfByZm1c", "forum": "ZVg8y3ibyM", "replyto": "ZVg8y3ibyM", "signatures": ["ICLR.cc/2026/Conference/Submission10493/Reviewer_UgNJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10493/Reviewer_UgNJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044782761, "cdate": 1762044782761, "tmdate": 1762921778826, "mdate": 1762921778826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}