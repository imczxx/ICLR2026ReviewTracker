{"id": "bPM2KgvK53", "number": 8138, "cdate": 1758068370868, "mdate": 1759897804876, "content": {"title": "TF-Restormer: Complex Spectral Prediction for Speech Restoration", "abstract": "Speech restoration in real-world conditions is challenging due to compounded distortions such as clipping, band-pass filtering, digital artifacts, noise, and reverberation, and low sampling rates. Existing systems, including vocoder-based approaches, often sacrifice signal fidelity, while diffusion models remain impractical for streaming. Moreover, most assume a fixed target sampling rate, requiring external resampling that leads to redundant computations. We present TF-Restormer, an encoder-decoder architecture that concentrates analysis on input-bandwidth with a time-frequency dual-path encoder and reconstructs missing high-frequency bands through a light decoder with frequency extension queries. It enables efficient and universal restoration across arbitrary input-output rates without redundant resampling. To support adversarial training across diverse rates, we introduce a shared sampling-frequency-independent (SFI) STFT discriminator.\nTF-Restormer further supports streaming with a causal time module, and improves robustness under extreme degradations by injecting spectral inductive bias into the frequency module. Finally, we propose a scaled log-spectral loss that stabilizes optimization under severe conditions while emphasizing well-predicted spectral details. As a single model across sampling rates, TF-Restormer consistently outperforms prior systems, achieving balanced gains in signal fidelity and perceptual quality, while its streaming mode maintains competitive performance for real-time use. Anonymous code and demos are available at https://tf-restormer.github.io/demo.", "tldr": "", "keywords": ["speech restoration", "general speech enhancement", "Transformer", "Time-Frequency dual-path model", "sampling-frequency-independent"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3b7846356d0d2ad7f482aee2f7ca0ee57c8100b9.pdf", "supplementary_material": "/attachment/f004aa0ec3bfa4a0e75478cea6355c4a7a41e799.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a universal speech enhancement model called TF-Restormer, which reconstructs missing high-frequency bands through a lightweight decoder with frequency extension queries. It enables efficient and universal restoration across arbitrary input–output rates without redundant resampling and supports streaming with a causal time module. The results shown in the experimental tables and demo audio are good. However, several important concerns regarding the experimental setup and comparisons should be addressed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1)\tThe paper tackles an important and often overlooked issue in speech restoration: achieving a good balance between signal fidelity and perceptual quality. The authors explicitly consider both aspects during training.\n2)\tThe concept of frequency extension queries is interesting and reasonable, though some implementation details are missing.\n3)\tThe proposed frequency cross-self module and the use of inductive bias for structural consistency are also intriguing (but I feel the presentation is not very clear)."}, "weaknesses": {"value": "1)\tThe most serious concern is that the baselines reported in Tables 1 to 3 appear to be trained on different datasets and degradation conditions, making it unclear whether the performance gains come from the model architecture or the training data. I suggest training and evaluating TF-Restormer on a standardized benchmark, such as the URGENT Challenge (https://urgent-challenge.github.io/urgent2025/), for a fair comparison.\n2)\tThe explanation of the shared projection layer for structural bias is unclear. While it may reduce model size, it’s not obvious for me how it improves performance. Although the ablation in Table 4(c) supports its effectiveness, it would be informative to show results when frequency projection is applied but **not** shared across all modules and key–value mappings.\n3)\tAlthough many previous works have already used log1p to compress the dynamic range so that the model is not dominated by high-energy regions, the rationale behind the proposed scaled log-spectral loss does not seem entirely convincing. It produces “gradients that are strongest near d≈0 and gradually diminish”, which somewhat appears to contradict the claim that it “emphasizes regions where the spectrum is already well-aligned.” In fact, it may have the opposite effect. How do you ensure that the large gradients do not cause the loss in “well-aligned regions” to oscillate around the optimal point?\n4)\tIn Table 3 (super-resolution evaluation), UTMOS scores should not be reported since the metric always resamples audio to 16 kHz. It would be better to report NISQA or other suitable metrics instead.\n\n5)\tSeveral references related to the URGENT Challenge and the universal speech enhancement models submitted to it are missing:\nEx: \n\n\n  [1] “Interspeech 2025 urgent speech enhancement challenge.” In Proc. Interspeech, 2025.\n\n\n  [2] “Scaling beyond denoising: Submitted system and findings in urgent challenge 2025”, In Proc. Interspeech, 2025.\n\n\n  [3] “Universal speech enhancement with regression and generative mamba”, In Proc. Interspeech, 2025."}, "questions": {"value": "1)\tFor the ablation study, did you use fixed training steps (i.e.,100,000 for pretraining and 10,000 for adversarial training)? For a fair comparison, I would suggest determining the training steps based on a validation set, as different architectures may have different optimal training steps.\n2)\tFor the learned frequency extension queries, do you learn only a single vector (as you mentioned shared across frames)? Under different input/target sampling rate settings, do you use different vectors (e.g., one for 16→32 kHz and another for 8→48 kHz)?\n3)\tWhy MHCA only “operates when extension query is padded, otherwise, bypassed.”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Gu5XNJNXCT", "forum": "bPM2KgvK53", "replyto": "bPM2KgvK53", "signatures": ["ICLR.cc/2026/Conference/Submission8138/Reviewer_HvyR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8138/Reviewer_HvyR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761106886976, "cdate": 1761106886976, "tmdate": 1762920109156, "mdate": 1762920109156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an encoder-decoder speech enhancement method that is capable of working with input audio with different sampling rates. The presented model is based on the Transformer architecture and consists of a heavy encoder and a light decoder. The training comprises two stages: the first stage involves the perceptual loss and the newly proposed log-spectral loss, which, as the authors claim, accounts for a better balance between well-predicted and poorly-predicted regions during training; the second stage uses SFI-STFT discriminators that enable adversarial training for varying frequencies. The model is evaluated on several key speech enhancement benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "There are several noteworthy strengths of the paper:\n\n1. The presentation of the newly developed method is well-structured and clear, and the paper is well written. All method components are well-described, including detailed explanations of the model architecture, training procedure, and training losses. The provided codebase is very helpful in understanding the internal structure of the model. \n2. The capability of the model to work on various sampling rates is a valuable contribution to the field. \n3. The paper provides a thorough ablation study that justifies most of the design choices made in the paper."}, "weaknesses": {"value": "However, there are several weak points: \n\n1. The paper could benefit from additional benchmarking on several datasets, including VoxCeleb [1], LibriTTS [2], and DNS [3]. Moreover, other recent methods, such as GenHancer [5] and MIIPHER [4] could also be considered. \n2. The padding in the decoder, which allows the model to be trained on recordings with various sampling rates, crucially relies on the model being trained on samples with both the smallest and the largest sampling rate. Otherwise, the pad tokens that correspond to the lowest or highest frequencies might be undertrained, resulting in worsened performance. This is a potential vulnerability of the model, and, therefore, should be flagged and investigated."}, "questions": {"value": "1. It would be interesting to conduct an ablation study to see how the undertrained padding tokens can impact performance.\n2. It would also be interesting to see how the use of perceptual loss impacts the quality of the model. I would welcome an additional ablation study on the use of perceptual loss in both pre-training and adversarial fine-tuning.\n3. Since the authors claim that their method can be easily turned into a streaming algorithm, it would be interesting to compare the model to other streaming baselines, both in terms of quality and inference speed.\n\n#### **References:**\n\n[1] Nagrani et al., \"VoxCeleb: a large-scale speaker identification dataset\" \n\n[2] Zen et al., \"LibriTTS: A corpus derived from LibriSpeech for text-to-speech\"\n\n[3] Dubey et al., \"ICASSP 2023 Deep Noise Suppression Challenge\" \n\n[4] Koizumi et al., \"MIIPHER: a robust speech restoration model integrating self-supervised speech and text representations\" \n\n[5] Yang et al., \"Genhancer: high-fidelity speech enhancement via generative modeling on discrete codec tokens\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kTfND9YutG", "forum": "bPM2KgvK53", "replyto": "bPM2KgvK53", "signatures": ["ICLR.cc/2026/Conference/Submission8138/Reviewer_NvNb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8138/Reviewer_NvNb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741713828, "cdate": 1761741713828, "tmdate": 1762920108708, "mdate": 1762920108708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a transformer-based architecture, TF-RESTORMER, for tackling the general speech restoration task. Inherited from the commonly adopted sampling-frequency independent strategy, this paper further extends to supporting arbitrary input-output sampling rates with a well-designed asymmetric TF dual path structure and proposes a projection-based spectral inductive bias. To further enhance speech quality, this paper proposes a scaled log-spectral loss. Extensive experiments have been conducted on general speech restoration, denoising, and super-resolution tasks to verify the effectiveness of TF-RESTORMER."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper further extends the commonly adopted sampling-frequency independent strategy with supporting arbitrary input-output sampling rates. Additionally, the proposed projection-based spectral inductive bias appears to be effective based on the experiments.\n\n2. This paper proposes a scaled log-spectral loss for spectral supervision, which seems to be beneficial for improving the performance of speech restoration tasks and for the development of loss designs within the community."}, "weaknesses": {"value": "1. This paper only uses clean source from the VCTK dataset, which is relatively small, comprising merely 44 hours of recordings. For the general speech restoration task, it is more common to employ a large-scale corpus augmented with diverse degradations. Additionally, the distortions added for mixing up the degraded signal are moderate. For example, while this paper limits the signal-to-noise ratio (SNR) to 5-20 dB, comparable studies evaluate severe noise occasions with SNR down to -5 dB or even -10 dB. The results obtained from such a small dataset and trained with moderate distortions, are not sufficiently convincing. And some testsets that are more widely adopted in the general speech restoration task, such as the testset from the URGENT Challenge, should be considered. Additionally, given that the TF-Restormer can simultaneously perform multiple tasks, including denoising and super-resolution, it is recommended to evaluate the model on different test sets using a single trained model.\n\n2. This paper proposes an SFI-STFT Discriminator, yet it provides insufficient experimental results to prove its effectiveness. It's unclear whether using a shared SFI-STFT Discriminator for all the sampling rate settings is more effective than using a set of independent discriminators, each targeting a specific sampling rate. Without this ablation, the claimed superiority of the shared design in the discriminator remains unsubstantiated.\n\n3. Although the paper introduces a scaled log-spectral loss, its effectiveness is not adequately demonstrated. The ablation study should be extended to include experiments in which the scale factor is held constant, thereby verifying the contribution of the scaling mechanism."}, "questions": {"value": "1. The effectiveness of the shared SFI-STFT Discriminator remains unclear to me. I am skeptical that a shared SFI-STFT Discriminator can outperform a set of independent discriminators, each targeting a specific sampling rate, given that the latter actually involves the same training costs as the proposed SFI-STFT Discriminator but with specialized capacity for supervising the performance in each sampling rate. Therefore, I suggest conducting auxiliary experiments to compare these two approaches.\n\n2. Does the author conduct auxiliary experiments to further investigate the proposed scaled log-spectral loss? For instance, fixing the value of the scale factor. \n\n3. The formatting of the references is inconsistent and should be standardized."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "14Hl6O32Fy", "forum": "bPM2KgvK53", "replyto": "bPM2KgvK53", "signatures": ["ICLR.cc/2026/Conference/Submission8138/Reviewer_7rbh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8138/Reviewer_7rbh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971728901, "cdate": 1761971728901, "tmdate": 1762920108440, "mdate": 1762920108440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TF-Restormer, an encoder-decoder architecture designed for universal speech restoration. The key innovation is its ability to handle arbitrary input-output sampling rates without external resampling. The model uses a Time-Frequency (TF) dual-path encoder to analyze the input bandwidth and a lightweight decoder that uses extension queries to reconstruct missing high-frequency bands. It also introduces a streaming variant using causal modules (Mamba blocks) and a scaled log-spectral loss to stabilize training under severe degradations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Practicality & Flexibility: The ability to handle various input/output rates in a single model without external resampling is a significant practical advantage over models that require fixed rates.\n- Performance Claims: It claims to achieve a better balance of signal fidelity (PESQ, SDR) and perceptual quality (MOS metrics) compared to pure vocoder or diffusion baselines"}, "weaknesses": {"value": "- **Novelty**: While the combination is novel, the core components (encoder-decoder model, TF-dual path, conformer/mamba blocks, adversarial training) are established. \n\n- **Fairness**: In the super-resolution task, standard dedicated models were compared. It is not clear if these baselines were also allowed to use similar advanced training data augmentations to ensure a fair comparison.\n\n- **Complexity vs. Performance**: The ablation study (Table 4b) shows the encoder-decoder structure is better than a \"decoder-only\" approach. It is not clear if the added complexity of the asymmetric design is fully justified by the performance gains across all tasks."}, "questions": {"value": "- **Novelty**: To what extent do you consider the TF-Restormer a novel architectural contribution versus a clever application of existing TF dual-path and seq2seq principles to the SFI problem? specifically, is the 'extension query' mechanism a significant departure from standard decoder practices?\n\n- **Generalizability**: \"The paper focuses heavily on speech-specific properties (e.g., frequency structures). In your view, are the core contributions—like the SFI formulation or Scaled Log-Spectral Loss—generic enough to benefit other fields dealing with time-series or spectral data?\n\n- **Reproducibility**: Given the complexity of the training data simulation pipeline, are there any nuanced implementation details not currently covered in Appendix B that would be critical for exact reproduction of the reported baselines and results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n91SeoEolH", "forum": "bPM2KgvK53", "replyto": "bPM2KgvK53", "signatures": ["ICLR.cc/2026/Conference/Submission8138/Reviewer_5YCi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8138/Reviewer_5YCi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147484440, "cdate": 1762147484440, "tmdate": 1762920108030, "mdate": 1762920108030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}