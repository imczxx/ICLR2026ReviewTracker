{"id": "SjqMQ9Gsbx", "number": 13065, "cdate": 1758213227329, "mdate": 1759897467634, "content": {"title": "Global optimization of graph acquisition functions for neural architecture search", "abstract": "Graph Bayesian optimization (BO) has shown potential as a powerful and data-efficient tool for neural architecture search (NAS). Most existing graph BO works focus on developing graph surrogate models, i.e., metrics of networks and/or kernels to quantify the similarity between networks. However, optimization of the resulting acquisition functions over graph structures is less studied due to their complexity and formulations over the combinatorial graph search space. This paper presents explicit optimization formulations for graph input spaces, including properties such as reachability and shortest paths, which can then be used to formulate graph kernels and associated acquisition functions. We theoretically prove that the proposed encoding is an equivalent representation of the original graph space and provide a general formulation for neural architecture cells that incorporates node and/or edge-labeled graphs with multiple sources and sinks regardless of connectivity. Numerical results over several NAS benchmarks show that our method efficiently finds the optimal architecture for most cases.", "tldr": "We introduce mathematical programming formulations for acquisition functions over graph input spaces for graph BO-based neural architecture search.", "keywords": ["Graph Bayesian optimization", "Mixed-integer programming", "neural architecture search", "global optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22e18a983d292e213159bec90dd702f2157085ef.pdf", "supplementary_material": "/attachment/95d714c97bedb655ce00b2816c9c137530ef0295.zip"}, "replies": [{"content": {"summary": {"value": "Briefly summarize the paper and its contributions. You can incorporate Markdown and Latex into your review.\nThis article proposes an equivalent representation of a general labeled graph in an optimized variable space, where each graph corresponds to a unique feasible solution. It further introduces a universal kernel formula to measure graph similarity, which is compatible with the proposed encoding. This method achieves global acquisition optimization based on graph Bayesian optimization in neural structure search."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper proposes an equivalent representation of general labeled graphs in the optimization variable space, ensuring that each graph corresponds to a unique feasible solution. Moreover, it introduces a unified kernel formulation that quantifies the similarity between two labeled graphs at the levels of graph structure, node labels, and edge labels.  The advantages over baselines were demonstrated in NAS Bench 101, NAS Bench 201, and NAS Bench 301.\n2.\tThe formulas and derivation proofs in the article are very detailed and accompanied by complete code."}, "weaknesses": {"value": "1.\tThe benchmarks used (NAS Bench 101, NAS Bench 201, and NAS Bench 301) are all from before 2022. Similarly, the baseline methods such as GCN, NAS BOT, and NAS BOWL are also from before 2021. No experiments were conducted on the latest benchmarks or with more recent baseline methods.\n2.\tThis paper lacks an analysis of the algorithm's time complexity.\n3.\tThe evaluated benchmark is limited to NAS, lacking experiments on real-world tasks, which makes the contribution relatively limited."}, "questions": {"value": "1.\tCould experiments be added on more recent and broader benchmarks and baselines?\n2.\tCould an analysis of the algorithm’s time complexity be provided?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9WmJDZoDSa", "forum": "SjqMQ9Gsbx", "replyto": "SjqMQ9Gsbx", "signatures": ["ICLR.cc/2026/Conference/Submission13065/Reviewer_tjo5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13065/Reviewer_tjo5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623121212, "cdate": 1761623121212, "tmdate": 1762923793772, "mdate": 1762923793772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Response to common comments"}, "comment": {"value": "Sincere thanks to all Reviewers for the helpful and detailed comments provided. First, we address the following points that are mentioned by more than one Reviewer:\n\n**[Complexity analysis]**\n\nComplexity analysis can refer to two different NAS-GOAT components: (1) complexity of calculating the kernel values and (2) complexity of solving the acquisition optimization problem (MIP):\n\n(1) The computational complexities of calculating the graph kernel values on a graph with size $n$ are:\n\n- Random Walk (RW): $O(n^3)$\n\n- Weisfeiler-Lehman (WL): $O(hm)$ where $h$ is the number of iterations and $m$ is the number of edges\n\n- Shortest Path (SP): $O(n^3)$\n\nWhile there are graph kernels with lower complexities than the shortest-path kernel, the complexity in computing kernel values is much less significant than the complexity of the graph optimization (MIP). \n\n(2) For the acquisition optimization, MIP formulations for other graph kernels are not established, making the comparison or complexity analysis over different graph kernels unrealistic. \n\nSolving large MIPs is indeed a long-standing challenge, but modern MIP solvers effectively reduce the solving time by combining many algorithms and best practices. For the scope of NAS-GOAT, cell-based NAS does not involve very large graphs, making global acquisition optimization useful (from an application perspective) and tractable (from a computational perspective). When graph size increases and global optimization becomes computationally intractable, NAS-GOAT is still useful because we can limit the solver runtime and still obtain good feasible solutions with theoretical quality guarantees, i.e., the primal-dual gap. We will add this discussion of complexity into the Appendix of a revised version to help readers better understand our method.\n\n**[Choice of benchmarks]**\n\nWe conducted experiments on NAS-Bench-101, NAS-Bench-201 and NAS-Bench-301 as these benchmarks are classic choices in NAS research. Moreover, together they cover the most common and challenging cases in NAS tasks, including node-labeled DAGs, edge-labeled DAGs and disconnected digraphs with multiple sinks and sources. Section 4.1 introduces the graph types involved in each benchmark. Each benchmark also represents graph spaces with different sizes: NAS-Bench-101 and NAS-Bench-201 serve as tabular benchmarks and NAS-Bench-301 includes the challenging open-domain DARTS search space. These benchmarks result in a comprehensive set of NAS cases (as noted by Reviewer uXfa) to study the performance of NAS-GOAT."}}, "id": "JdWSRbCIKA", "forum": "SjqMQ9Gsbx", "replyto": "SjqMQ9Gsbx", "signatures": ["ICLR.cc/2026/Conference/Submission13065/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13065/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13065/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763736778478, "cdate": 1763736778478, "tmdate": 1763736778478, "mdate": 1763736778478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "NAS-GOAT casts cell-based neural architecture search as a Mixed-Integer Program in which graph topology, reachability, shortest-path features and a GP acquisition function are jointly optimized. The resulting MIP is solved to global optimality at every BO step, eliminating hand-crafted mutations and providing certificates of optimality under the surrogate model. Experiments on three public NAS benchmarks demonstrate competitive or superior query efficiency versus recent sampling-based or evolutionary BO baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n2. The authors design a full condition plan of NAS graph space.\n3. The code is supplied, and the hyper-parameters are reported."}, "weaknesses": {"value": "1. The complexity of the method should be analyzed.\n2. The main content in Theorem 1 is more likely a modeling plan of the graph space, but it takes too much space in the paper, which makes readers uncomfortable. In addition, Theorem 1 is unnecessary to be a theorem.\n3. The experiments are all conducted on NB101~301, it is better to evaluate the method on more datasets. Besides, the method cannot achieve SOTA in some of cases."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vHZmq3sqFX", "forum": "SjqMQ9Gsbx", "replyto": "SjqMQ9Gsbx", "signatures": ["ICLR.cc/2026/Conference/Submission13065/Reviewer_SBUm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13065/Reviewer_SBUm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753050897, "cdate": 1761753050897, "tmdate": 1762923793479, "mdate": 1762923793479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NAS-GOAT, a framework for globally optimizing graph-based acquisition functions in Bayesian optimization (BO) for neural architecture search (NAS). The authors formulate the graph search space—including reachability, shortest paths, and node/edge labels—as a mixed-integer program (MIP), enabling exact optimization of acquisition functions. The method generalizes prior graph BO formulations (e.g., BoGrape) to handle weakly-connected or disconnected DAGs common in NAS. Experiments on NAS-Bench-101, 201, and 301 show that NAS-GOAT efficiently finds near-optimal architectures, often outperforming or matching state-of-the-art baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "++ This method extends graph BO to NAS by relaxing the strong connectivity assumption of BoGrape.\n\n++ Comprehensive experiments on three major NAS benchmarks under both deterministic and noisy settings demonstrate robustness and efficiency."}, "weaknesses": {"value": "-- The MIP encoding for graph structures builds heavily on BoGrape, with the main adaptation being the relaxation of strong connectivity. While this is non-trivial, the paper could better highlight what specific constraints were modified or added to handle NAS-specific DAGs. \nSpecifically, the claim that BoGrape is unsuitable due to strong connectivity is not followed by a clear explanation of how this is resolved beyond \"generalizing the graph encoding.\"\n\n-- I am afraid that this method is not a \"plug-and-play\" solution. The MIP model must be manually re-derived and re-implemented for each new search space topology. This creates a significant barrier to practical adoption and limits its applicability to new or evolving NAS problems."}, "questions": {"value": "1. I suggest the authors provide more analyze about the differences between this method and BoGrape. As I am concerned, the contribution of this work lies in the adoption of BoGrape for NAS tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3FW6Yl2jdQ", "forum": "SjqMQ9Gsbx", "replyto": "SjqMQ9Gsbx", "signatures": ["ICLR.cc/2026/Conference/Submission13065/Reviewer_uXfa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13065/Reviewer_uXfa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992902355, "cdate": 1761992902355, "tmdate": 1762923793188, "mdate": 1762923793188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}