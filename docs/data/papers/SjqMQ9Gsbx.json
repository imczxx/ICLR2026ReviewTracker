{"id": "SjqMQ9Gsbx", "number": 13065, "cdate": 1758213227329, "mdate": 1759897467634, "content": {"title": "Global optimization of graph acquisition functions for neural architecture search", "abstract": "Graph Bayesian optimization (BO) has shown potential as a powerful and data-efficient tool for neural architecture search (NAS). Most existing graph BO works focus on developing graph surrogate models, i.e., metrics of networks and/or kernels to quantify the similarity between networks. However, optimization of the resulting acquisition functions over graph structures is less studied due to their complexity and formulations over the combinatorial graph search space. This paper presents explicit optimization formulations for graph input spaces, including properties such as reachability and shortest paths, which can then be used to formulate graph kernels and associated acquisition functions. We theoretically prove that the proposed encoding is an equivalent representation of the original graph space and provide a general formulation for neural architecture cells that incorporates node and/or edge-labeled graphs with multiple sources and sinks regardless of connectivity. Numerical results over several NAS benchmarks show that our method efficiently finds the optimal architecture for most cases.", "tldr": "We introduce mathematical programming formulations for acquisition functions over graph input spaces for graph BO-based neural architecture search.", "keywords": ["Graph Bayesian optimization", "Mixed-integer programming", "neural architecture search", "global optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22e18a983d292e213159bec90dd702f2157085ef.pdf", "supplementary_material": "/attachment/95d714c97bedb655ce00b2816c9c137530ef0295.zip"}, "replies": [{"content": {"summary": {"value": "Briefly summarize the paper and its contributions. You can incorporate Markdown and Latex into your review.\nThis article proposes an equivalent representation of a general labeled graph in an optimized variable space, where each graph corresponds to a unique feasible solution. It further introduces a universal kernel formula to measure graph similarity, which is compatible with the proposed encoding. This method achieves global acquisition optimization based on graph Bayesian optimization in neural structure search."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper proposes an equivalent representation of general labeled graphs in the optimization variable space, ensuring that each graph corresponds to a unique feasible solution. Moreover, it introduces a unified kernel formulation that quantifies the similarity between two labeled graphs at the levels of graph structure, node labels, and edge labels.  The advantages over baselines were demonstrated in NAS Bench 101, NAS Bench 201, and NAS Bench 301.\n2.\tThe formulas and derivation proofs in the article are very detailed and accompanied by complete code."}, "weaknesses": {"value": "1.\tThe benchmarks used (NAS Bench 101, NAS Bench 201, and NAS Bench 301) are all from before 2022. Similarly, the baseline methods such as GCN, NAS BOT, and NAS BOWL are also from before 2021. No experiments were conducted on the latest benchmarks or with more recent baseline methods.\n2.\tThis paper lacks an analysis of the algorithm's time complexity.\n3.\tThe evaluated benchmark is limited to NAS, lacking experiments on real-world tasks, which makes the contribution relatively limited."}, "questions": {"value": "1.\tCould experiments be added on more recent and broader benchmarks and baselines?\n2.\tCould an analysis of the algorithm’s time complexity be provided?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "9WmJDZoDSa", "forum": "SjqMQ9Gsbx", "replyto": "SjqMQ9Gsbx", "signatures": ["ICLR.cc/2026/Conference/Submission13065/Reviewer_tjo5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13065/Reviewer_tjo5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623121212, "cdate": 1761623121212, "tmdate": 1762923793772, "mdate": 1762923793772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "NAS-GOAT casts cell-based neural architecture search as a Mixed-Integer Program in which graph topology, reachability, shortest-path features and a GP acquisition function are jointly optimized. The resulting MIP is solved to global optimality at every BO step, eliminating hand-crafted mutations and providing certificates of optimality under the surrogate model. Experiments on three public NAS benchmarks demonstrate competitive or superior query efficiency versus recent sampling-based or evolutionary BO baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow.\n2. The authors design a full condition plan of NAS graph space.\n3. The code is supplied, and the hyper-parameters are reported."}, "weaknesses": {"value": "1. The complexity of the method should be analyzed.\n2. The main content in Theorem 1 is more likely a modeling plan of the graph space, but it takes too much space in the paper, which makes readers uncomfortable. In addition, Theorem 1 is unnecessary to be a theorem.\n3. The experiments are all conducted on NB101~301, it is better to evaluate the method on more datasets. Besides, the method cannot achieve SOTA in some of cases."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vHZmq3sqFX", "forum": "SjqMQ9Gsbx", "replyto": "SjqMQ9Gsbx", "signatures": ["ICLR.cc/2026/Conference/Submission13065/Reviewer_SBUm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13065/Reviewer_SBUm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761753050897, "cdate": 1761753050897, "tmdate": 1762923793479, "mdate": 1762923793479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NAS-GOAT, a framework for globally optimizing graph-based acquisition functions in Bayesian optimization (BO) for neural architecture search (NAS). The authors formulate the graph search space—including reachability, shortest paths, and node/edge labels—as a mixed-integer program (MIP), enabling exact optimization of acquisition functions. The method generalizes prior graph BO formulations (e.g., BoGrape) to handle weakly-connected or disconnected DAGs common in NAS. Experiments on NAS-Bench-101, 201, and 301 show that NAS-GOAT efficiently finds near-optimal architectures, often outperforming or matching state-of-the-art baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "++ This method extends graph BO to NAS by relaxing the strong connectivity assumption of BoGrape.\n\n++ Comprehensive experiments on three major NAS benchmarks under both deterministic and noisy settings demonstrate robustness and efficiency."}, "weaknesses": {"value": "-- The MIP encoding for graph structures builds heavily on BoGrape, with the main adaptation being the relaxation of strong connectivity. While this is non-trivial, the paper could better highlight what specific constraints were modified or added to handle NAS-specific DAGs. \nSpecifically, the claim that BoGrape is unsuitable due to strong connectivity is not followed by a clear explanation of how this is resolved beyond \"generalizing the graph encoding.\"\n\n-- I am afraid that this method is not a \"plug-and-play\" solution. The MIP model must be manually re-derived and re-implemented for each new search space topology. This creates a significant barrier to practical adoption and limits its applicability to new or evolving NAS problems."}, "questions": {"value": "1. I suggest the authors provide more analyze about the differences between this method and BoGrape. As I am concerned, the contribution of this work lies in the adoption of BoGrape for NAS tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3FW6Yl2jdQ", "forum": "SjqMQ9Gsbx", "replyto": "SjqMQ9Gsbx", "signatures": ["ICLR.cc/2026/Conference/Submission13065/Reviewer_uXfa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13065/Reviewer_uXfa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992902355, "cdate": 1761992902355, "tmdate": 1762923793188, "mdate": 1762923793188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}