{"id": "CuNHz3zxgm", "number": 12457, "cdate": 1758207965469, "mdate": 1759897508917, "content": {"title": "WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling", "abstract": "Video world models have attracted significant attention for their ability to produce high-fidelity future visual observations conditioned on past observations and navigation actions.\nTemporally- and spatially-consistent, long-term world modeling has been a long-standing problem, unresolved with even recent state-of-the-art models, due to the prohibitively expensive computational costs for long-context inputs.\nIn this paper, we propose WorldPack, a video world model with efficient compressed memory, which significantly improves spatial consistency, fidelity, and quality in long-term generation despite much shorter context length.\nOur compressed memory consists of trajectory packing and memory retrieval; trajectory packing realizes high conctext efficiency and memory retrieval maintains the consistency in rollouts and helps long-term generations that require spatial reasoning.\nOur performance is evaluated with LoopNav, a benchmark on MineCraft, specialized for the evaluation of long-term consistency, and we verify that WorldPack notably outpeforms strong state-of-the-art models.", "tldr": "", "keywords": ["world models", "memory", "video diffusion models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bb00441d8ee8010f41a800079618ab1f3b86453a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces WorldPack, a video world model targetting on long-term spatial consistency. According to the authors, there are two key components: trajectory packing, which compresses past frames hierarchically to retain long-term information efficiently; memory retrieval, which selectively recalls relevant past scenes for better spatial understanding. The evaluatiion on LoopNav (a benchmark on Minecraft) shows that WorldPack clearly outperforms prior methods including Oasis, MineWorld, DIAMOND, and NWM."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Leveraging the idea of FramePack is effective in enabling long video generation, which significantly reduces the context and improves the context efficiency. The constant memory is a good attribute for long, auto-regressive video generation.\n2. The experiment on LoopNav is impressive.\n3. The drawing is good and clear."}, "weaknesses": {"value": "1. Firstly, I'm not clear about the performance gained from the FramePack design, which aims for better / longer video generation. But in your experiments, the number of frames in the generated videos are similar with NWM, can you show some comparisons with NWM on longder video generations? Like minutes with early 1,000 frames.\n2. The FramePack design has a clear drawback is that the importance is manually assigned rather than learned from data. Can you add some additional discussions on that? Why in the field of NWM, this manually assigned importance works well or at least better than prior methods?\n3. Secondly, though the results on LoopNav is promising, the experiments on real-world scenarios is missing, e.g., Go Stanford. I'm absolutely preferring a real-world benchmark than a Minecraft benchmark, which is much more practical.\n4. Following 1, I encourage the authors to conduct the experiments between WorldPack and NWM on long, real-world cases, and report the results following the Figure 4 of NWM, and of course, the x-axis should represent longer period. I think it will be more clear to demonstrate the improved \"long-term spatial consistency\" over NWM.\n5. The generated videos on both real-world and Minecraft cases is missing. Can you provide some videos compressed in *.zip?\n6. The FVD in LoopNav is missing.\n7. For the writing, I don't really get why \"Compressed Memory Improves Spatial Consistency\". This is anti-intuitive. You can claim your method brings compressed memory and improved spatial consistency if your experiments well support that.\n8. Is the efficiency on running time also compared with NWM? I saw only the \"context efficiency\"."}, "questions": {"value": "Please see the Weaknesses*."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "712WAAsbk0", "forum": "CuNHz3zxgm", "replyto": "CuNHz3zxgm", "signatures": ["ICLR.cc/2026/Conference/Submission12457/Reviewer_wZPH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12457/Reviewer_wZPH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761007345435, "cdate": 1761007345435, "tmdate": 1762923339696, "mdate": 1762923339696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the challenge of long-horizon generation in video world models. \n\nThis paper introduces WorldPack, which achieves long-term temporal and spatial consistency through an efficient compressed memory design. \n\nOn the LoopNav benchmark in Minecraft, WorldPack demonstrates good performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Introduces a compressed memory approach (trajectory packing + memory retrieval) that creatively addresses long-horizon consistency without increasing context length.\n\nEmpirically outperforms baselines on LoopNav, improving spatial consistency, fidelity, and long-term generation quality."}, "weaknesses": {"value": "**Limited innovation in memory design**: The proposed trajectory packing and memory retrieval are common ideas that have been widely used, including in RNNs and LSTMs via memory banks. The paper does not articulate a fundamentally new algorithmic principle or provide a theoretically grounded formulation that distinguishes its approach from prior methods. Moreover, the related work section lacks a thorough discussion of these connections.\n\n**Incremental engineering on existing backbones**: Using a CDiT backbone with RoPE-based temporal embeddings is a straightforward integration of known components that enhance long-range conditioning in Transformers. The gains appear to stem from combining established techniques rather than introducing a new architecture or training objective that advances the state of the art conceptually. These do not constitute genuine novelty for the paper.\n\n**Missing references**: In the Video World Models section of related work, there has already been substantial exploration of long-term context in autonomous driving, yet the authors overlook these approaches [1,2]. Why is that? Was it an oversight, or a deliberate omission?\n\n**Results**: The results in Table 1 are not particularly impressive. Moreover, the last two panels of ABCA-50 in Figure 4 suggest that the memory component may have limited impact; in fact, adding memory appears to underperform compared to not using it, especially over long horizons.\n\n[1] DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT\n\n[2] InfinityDrive: Breaking Time Limits in Driving World Models"}, "questions": {"value": "1. Do the authors have better results to demonstrate the superiority of their method?\n\n2. Could the author explain their contribution and novelty more clearly, as well as their previous work, and the difference between their network structure and theirs (using the memory bank in RNN and LSTM is not a novelty in my opinion)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qzrkapyLVo", "forum": "CuNHz3zxgm", "replyto": "CuNHz3zxgm", "signatures": ["ICLR.cc/2026/Conference/Submission12457/Reviewer_42nA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12457/Reviewer_42nA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655127486, "cdate": 1761655127486, "tmdate": 1762923337326, "mdate": 1762923337326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes WorldPack, a video world model with compressed memory. Specifically, the method retrieves relevant past frames, then downsamples them to lower resolutions according to their estimated importance — a technique inspired by FramePack used in video generation. Experiments are conducted on LoopNav, a Minecraft-based benchmark designed to evaluate long-term cycle consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is technically sound, and the experimental results clearly demonstrate its superiority over provided baseline approaches.\n2. The paper is well written and easy to follow."}, "weaknesses": {"value": "The technical novelty of the paper is somewhat limited. The central idea, importance-based frame compression, has already been explored in next-frame prediction and video diffusion models (FramePack). The paper’s main contribution appears to be in adapting this idea to the world modeling context through specific retrieval strategies and benchmark evaluations.\n\n- I suggest that FramePack be clearly highlighted in the Preliminaries section to better situate this work within existing literature and make its incremental contributions more transparent."}, "questions": {"value": "1. Could the authors clarify the key differences between FramePack and trajectory packing?\n2. In Section 4.2, it is stated that the proposed method does not require explicit camera parameters. However, aren’t Euler angles of the camera still considered part of its parameters? Please elaborate.\n3. Why does WorldPack not include comparisons with memory-related works in world model or video generation literature (as cited in Line 171)? It would be valuable to include a controlled, head-to-head comparison, e.g., under fair conditions such as with or without camera parameters.\n4. In Figure 5, why does adding packing improve overall performance (+Packing & Memory > +Memory)? My understanding is that packing should primarily improve efficiency, not necessarily task performance. Could the authors explain this effect?\n5. Typo: Line 221: trajectory packing => Trajectory packing"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oxMKut4uIc", "forum": "CuNHz3zxgm", "replyto": "CuNHz3zxgm", "signatures": ["ICLR.cc/2026/Conference/Submission12457/Reviewer_W2JJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12457/Reviewer_W2JJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917017250, "cdate": 1761917017250, "tmdate": 1762923336818, "mdate": 1762923336818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces WorldPack, a video world model that combines trajectory packing and memory retrieval to achieve long-horizon spatial consistency with short context lengths, building on a CDiT backbone with RoPE-based temporal embeddings to integrate memories from arbitrary distances. Trajectory packing hierarchically compresses past frames so that recent frames are high-resolution while older and retrieved memory frames are progressively downsampled, and the retrieval module selects context frames using a geometric score from agent positions and orientations when camera parameters are unavailable. Evaluation on the LoopNav benchmark in Minecraft shows consistent gains over previous methods across SSIM, LPIPS, PSNR, and DreamSim on both ABA and ABCA tasks with shorter contexts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed idea of combining trajectory packing with retrieval-based compression intuitively addresses the limitations of fixed-context world models.\n2. Retrieval formulation does not rely on camera intrinsics and explicitly handles opposite-facing views at characteristic distances, improving practical applicability in game/sim settings.\n3. Consistent gains on LoopNav across metrics and qualitative rollouts, with some ablations isolating the impact of retrieval vs packing and showing their complementarity."}, "weaknesses": {"value": "1. My main concern is the latency/memory cost of the introduced retrieval-based compression approach, because it introduces many additional operations. Although claimed as “computationally efficient,” the paper does not analyze actual memory savings or inference latencies of the compressed memory versus previous baselines.\n2. Evaluation is confined to a single simulator benchmark (Minecraft/LoopNav), and it is more important to test beyond simulators and towards real-world data to validate generality, like realestate10K​[1].\n3. Improvements in metrics such as SSIM and PSNR are relatively modest, and perceptual evaluations rely heavily on qualitative claims. More statistical analysis, such as FID, FVD, or user studies, would lend stronger evidence.\n\n[1] Zhou, Tinghui, et al. \"Stereo magnification: Learning view synthesis using multiplane images.\" arXiv preprint arXiv:1805.09817 (2018)."}, "questions": {"value": "1. How does retrieval behave under significant localization noise or drift in agent pose, and can visual similarity cues be integrated when poses are partially wrong or missing?\n2. What is the compute/time tradeoff of packing and retrieval at inference, and how does total context token count scale in practice across navigation ranges?\n3. How far can the world model see? Or how many frames can the world model generate? When or at what number of frames do the generation results become blurry or unrealistic\n\nI will raise my score if the author could address all of my concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BzxIF9tajO", "forum": "CuNHz3zxgm", "replyto": "CuNHz3zxgm", "signatures": ["ICLR.cc/2026/Conference/Submission12457/Reviewer_uLtW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12457/Reviewer_uLtW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986213925, "cdate": 1761986213925, "tmdate": 1762923336326, "mdate": 1762923336326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}