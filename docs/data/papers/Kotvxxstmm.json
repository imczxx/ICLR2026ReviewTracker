{"id": "Kotvxxstmm", "number": 14655, "cdate": 1758240881769, "mdate": 1759897356978, "content": {"title": "Learning to Rank Chain-of-Thought: Using a Small Model", "abstract": "Large Language Models (LLMs) struggle with reliable mathematical reasoning, and current verification methods are often computationally expensive. This paper introduces the Energy Outcome Reward Model (EORM), a highly efficient, lightweight post-hoc verifier designed to address this challenge. EORM uses an energy-based framework to rank Chain-of-Thought (CoT) solutions, learning to distinguish correct from incorrect reasoning using only simple outcome labels, thus eliminating the need for expensive annotations. With only 55M parameters, over 127 times smaller than typical reward models, EORM boosts the accuracy of Llama 3 8B to 90.7\\% on GSM8k and 63.7\\% on MATH. This performance is achieved by efficiently selecting the optimal reasoning path from a pool of candidates, allowing it to match or exceed the accuracy of far more resource-intensive Best-of-N sampling techniques. Crucially, our experiments show that EORM generalizes effectively to out-of-distribution problems and unseen models, indicating it learns fundamental principles of valid reasoning. This robustness, combined with its efficiency, establishes EORM as a practical tool for deploying more dependable LLMs in complex, real-world applications.", "tldr": "", "keywords": ["large language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eb3d2a40a27d4eb138de0857256248695eda9aed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work shows that a relatively small (~55 M parameters) energy-based transformer encoder can be used as an efficient reward model for math-oriented tasks.\nIt provides a comparison with an established majority voting baseline, along with fine-tuned models and methods such as test-time RL.\nThe results show that there seems to be a strong case for the EBM reward model, showing its ability to generalize beyond the distribution of training data and model completions.\nHowever, the work seems to underexplore several important aspects:\n+ importance of Bradley-Terry loss on logits vs classification approach\n+ comparison to reward models created by tuning a reward head on top of an autoregressive transformer (or maybe this is the ORM type used in Figure 3)"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ The proposed method is evaluated against a large collection of fine-tuned models (Mistral, Llama2, DeepSeekMath, Llama 3, and Qwen 2.5) with parameters ranging around 7B.\n+ The work shows how to create a relatively small and powerful reward model that beats majority voting and fine-tuned models"}, "weaknesses": {"value": "Presentation:\n\n- Figure 1: Using an incorrect solution (70) to the \"daps baps\" problem as an example of something that an EORM thinks is correct and marking it in green can be misleading.\nNote that the correct answer is 40, as 4 * 10 daps = 7*10 yaps and 5*14 yaps = 70 yaps = 3*14 baps = 42 baps\n- line 188-189 \"where Zθ is the partition function, a normalization constant that ensures pθ (y) sums to unity:\" -> where Zθ is a normalization constant that ensures pθ (y) sums to unity:\n- 264-265 \"For each training problem, we generated n = 256 CoT candidates with a temperature of 0.7 and a sampling probability of 0.9, ensuring all attempts were included, regardless of their correctness.\" - this is a little bit unclear way of saying that authors use nucleus sampling with temperature 0.7 and top-p=0.9\n- 360-361 \"Additionally, the Qwen-2.5 base model also finds similar findings.\"\n- Table 3: lines 374-377: \"Comparison of EORM with other reasoning methods. We evaluate EORM against two reasoning baselines, TTRL and MathWizard, using accuracy as the metric across four mathematical datasets.\": However, only TTRL (Zuo et al., 2025), DART-MATH (Prop2Diff) (Tong et al., 2024), and EORM are presented\n- line 907 has a citation error for Layer Norm (? instead of citation)\n\nOther:  \n- A proper description of ORM in Figure 3 would be beneficial (how exactly does it differ from EORM)\n- A proper description of how the input feeds to the MLP verifier (instead of the transformer encoder) in the ablation section would be beneficial\n- minor: work does not evaluate recent \"thinking\" models"}, "questions": {"value": "1. For the ablation of the architecture, how is the input feed to the MLP (instead of the transformer encoder) verifier? That is how different output lengths are handled?\n2. How does the ORM in Figure 3 differ from EORM?\n3. What parts are really important for EORM? That is:\n    + What makes it distinct from ORM trained as a small transformer model instead of a head on top of a large model?\n    + How does the Bradley-Terry loss affect the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3Sqn4lJbmq", "forum": "Kotvxxstmm", "replyto": "Kotvxxstmm", "signatures": ["ICLR.cc/2026/Conference/Submission14655/Reviewer_sf5M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14655/Reviewer_sf5M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761063785257, "cdate": 1761063785257, "tmdate": 1762925027351, "mdate": 1762925027351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the **Energy Outcome Reward Model (EORM)** — a lightweight, energy-based verifier for Chain-of-Thought (CoT) reasoning.  \nEORM reframes the *Best-of-N selection* problem as a ranking task: it assigns a scalar “energy” score to each reasoning trace, preferring lower energies for correct reasoning.  \nBy training with only binary outcome labels, EORM avoids costly preference or process supervision.  \nDespite having only **55M parameters** (≈127× smaller than typical reward models), it significantly improves reasoning accuracy on GSM8k and MATH, reaching **90.7%** and **63.7%** respectively when paired with Llama 3 8B.  \nNotably, EORM generalizes well across unseen models and out-of-distribution tasks (e.g., AIME 2024, AMC), suggesting it learns transferable reasoning principles.  \nThe contribution is practical, technically sound, and experimentally convincing, though conceptually incremental relative to prior verification frameworks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Strong empirical results:** State-of-the-art accuracy on GSM8k and MATH, with solid OOD generalization.  \n- **Efficiency:** 55M-parameter verifier outperforms 7B reward models, highlighting excellent cost–performance tradeoffs.  \n- **Clarity:** Methodology, loss function, and data preparation are explicitly described, enabling replication.  \n- **Robust ablations:** Demonstrate architecture sensitivity and tokenizer invariance, strengthening the argument for generalizability.  \n- **Practical impact:** Provides a scalable and plug-in verifier for reasoning tasks that could complement or replace Best-of-N sampling."}, "weaknesses": {"value": "- **Limited theoretical depth:** The approach is primarily empirical; the energy formulation does not introduce new learning theory.  \n- **Incremental conceptual novelty:** Extends known EBM and reward-modeling ideas rather than introducing new reasoning paradigms.  \n- **Evaluation scope:** Focused on math reasoning; broader reasoning domains (commonsense, logical entailment) are not tested.  \n- **No human interpretability study:** While effective, it is unclear what the model learns as a “signal of correctness.”  \n- **Formatting issue:**  Figure 2 is disproportionately large relative to the information it presents. The visual content is minimal and could be conveyed more effectively as a compact table. The current formatting distracts from the narrative and gives the impression of filler space rather than substantive insight. Figure 5 can also be more compact. I would also recommend using PDF for your figure."}, "questions": {"value": "1. The proposed Energy Outcome Reward Model (EORM) introduces a ranking-based energy formulation for verifying reasoning traces. Could the authors clarify how this approach conceptually differs from prior reward modeling frameworks (e.g., PRM, ORM) beyond model size and supervision type?\n\n2. The paper highlights strong cross-model generalization. Could the authors discuss whether this robustness arises from the model architecture, the training objective, or specific properties of the reasoning data distribution?\n\n3. While EORM focuses on outcome-based supervision, have the authors considered hybridizing it with *process supervision* signals (e.g., step-level correctness annotations) to improve interpretability or reasoning faithfulness?\n\n4. Could the authors provide variance or confidence intervals for key metrics (e.g., reasoning accuracy, ranking precision) across multiple runs to demonstrate the statistical robustness of EORM’s reported improvements?\n\n5. How sensitive is EORM to noisy or imperfect correctness labels during training, and did the authors evaluate its stability under controlled label corruption experiments?\n\n6. The paper claims strong cross-model generalization (e.g., from Llama to Mistral and Qwen). Were the reasoning traces normalized or token-aligned across models before training, and how does this preprocessing affect ranking performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "atNEwQA6Sq", "forum": "Kotvxxstmm", "replyto": "Kotvxxstmm", "signatures": ["ICLR.cc/2026/Conference/Submission14655/Reviewer_E5Nw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14655/Reviewer_E5Nw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761337047858, "cdate": 1761337047858, "tmdate": 1762925026690, "mdate": 1762925026690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The introduces EORM (Energy Outcome Reward Model) -- a small model assessing the correctness of CoT-style LLM responses to mathematical queries. EORM is parametrized as a small (55M parameters) transformer with an MLP head, which is lightweight compared to typical outcome reward models trained by others (e.g., LLMs with 7B parameters). EORM receives a complete CoT on the input, returns a real-valued score representing the correctness assessment. EORM is trained via a special pairwise loss which promotes ranking correct responses higher than the incorrect ones."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper tackles an interesting and practically important problem: how to approximately verify the CoT responses from LLMs.\n* The EORM model trained by the authors is indeed lightweight compared to typical ORMs, which has practical implications.\n* EORM is trained using an original approach with a special pairwise loss.\n* The authors show that EORM generalizes (to a degree) between different base LLMs as well as between datasets."}, "weaknesses": {"value": "In general I like the idea of training a lightweight ORM, and using an special loss for that. However, a significant weakness of the paper is its poor experimental setup which makes assessing the performance of EORM difficult.\n\n1. In Table 2, you present a large set of results comparing EORM with other methods / base models. However, these results are not aligned on generation budgets, so comparing them is not meaningful. For instance, it may turn out that if some of the considered base models we sample 256 times and perform simple majority voting, the results are better than with EORM (which also samples 256 times). The same remark applies to Table 3. Also, the results change dramatically with different generation budgets, whereas the authors use a fixed budget of 256 (or 64 for OOD benchmarks) samples.\n\n2. EORM should be compared with existing, open-source ORMs, and perhaps also with PRMs. In Figure 3 there appears some ORM, but it is not specified what exactly is this model. Also, the comparison between different verifiers could be expressed in terms of the AUC metric which would provide more direct comparison than downstream-task accuracy.\n\n3. EORM uses non-standard loss, which is one of more important \"selling points\" of the paper. But it would be important to see if this way of training is better that, say, using simple binary cross-entropy loss, or contrastive learning approach. \n\n4. The authors train EORM on just two math datasets (GSM8K, MATH). It would strengthen the paper if more benchmarks would be considered (especially that these two are likely leaked into pre-training of the mainstream LLMs.) For instance, what about training on older AIME problems and evaluating on AIME 2024/2025?\n\n5. The effectiveness of any ORM is conditioned on the fact that the correct CoT is often present among the generated samples. Therefore, it would be important to see for different models and datasets how often it is the case.\n\n6. Some related works seem to be missing. For instance, [1] has a very similar theme of training a lightweight CoT ranker for math problems, so it would be good to include a comparison with this work.\n\n**Minor**\n* In line 389-390 you say Llama 3 8B gets 42.9% on GSM8K, which is inconsistent with Table 2 and 4 (76.6%).\n* Figure 3 uses a strange scale on x axis: looks logarithmic, but not exactly: why x = 1 and x = 196 is included?\n* In Figure 4, to make it more readable, datasets should be represented as points on x axis, not colors.\n\n[1] Lightweight Latent Verifiers for Efficient Meta-Generation Strategies (https://arxiv.org/abs/2504.16760)"}, "questions": {"value": "1. I suspect that stronger reasoning models, like Qwen 2.5 7B, often \"know\" that they cannot answer correctly and express it in its CoT explicitly. This means that an outcome reward model may not be useful, and it's sufficient to check if the model didn't respond \"I don't know the correct answer\". Do you observe such phenomena? How often such models respond with a number which is incorrect, as opposed of not giving any number?\n\n2. When describing the Bradley-Terry loss you cite (Liu, 2009). However, I couldn't find Bradley-Terry loss defined there. Could you point to the exact place where this loss is defined?\n\n3. You write that majority voting \"tends to reflect the model’s biases rather than identifying the truly correct answer\". What does it mean?\n\n4. In Table 1, you indicate that EORM \"Uses Internal Indicators as Reward\". What does it mean?\n\n5. In line 209 you write that the CLS tokens is prepended. Shouldn't it be appended?\n\n6. What is \"sampling probability\" in line 265?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bZKm2csADs", "forum": "Kotvxxstmm", "replyto": "Kotvxxstmm", "signatures": ["ICLR.cc/2026/Conference/Submission14655/Reviewer_gqh4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14655/Reviewer_gqh4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922449639, "cdate": 1761922449639, "tmdate": 1762925026116, "mdate": 1762925026116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose the Energy Outcome Reward Model (EORM), a lightweight post-hoc verifier for Chain-of-Thought (CoT) reasoning. The approach employs an energy-based framework that assigns scalar energy scores to CoT outputs, allowing the model to rank reasoning chains and select the most plausible ones. In contrast to process- or preference-based reward models, EORM uses only binary outcome labels, reducing annotation costs. The model is significantly smaller than typical reward models while maintaining strong performance and generalization across different base models and out-of-distribution reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "[S1] Methodological Modification. The paper provides a clear and focused description of its methodological modification of the energy-based modeling framework for ranking Chain-of-Thought outputs. The adaptation of EBM to reasoning verification is reasonable and technically sound.\n\n[S2] Efficiency and Scalability. The proposed model is lightweight, using only 55M parameters compared to multi-billion-parameter reward models, which demonstrates strong potential for efficient and scalable deployment as a post-hoc verifier."}, "weaknesses": {"value": "[W1] Unclear and Potentially Unfair Comparison Setup (Table 2).\nThe main quantitative comparison in Table 2 does not clearly specify how the baselines were selected or evaluated. The listed models, including WizardMath, DART-Math, and MetaMath, incorporate different forms of instruction tuning or reinforcement learning (for example, RLHF, RLEIF, or preference optimization), but the paper does not clarify whether they were re-evaluated under the same experimental setup, dataset splits, or sampling conditions as EORM.\nAs a result, it is difficult to determine whether the reported performance gains stem from the proposed reranking mechanism or from differences in training data, or evaluation methodology. A fair comparison would require verifying that all models were tested on identical candidate sets and reasoning samples, or at least discussing the limitations of using heterogeneous pre-trained baselines.\nIn addition, given that GRPO and related reinforcement learning frameworks have recently become standard approaches for improving LLM reasoning, the paper should include a discussion or comparison with such methods to position EORM more clearly within the current landscape.\n\n[W2] Questionable Necessity of EORM Given Its Supervision on Correctness Labels.\nEORM is trained using binary correctness labels (correct or incorrect) for each Chain-of-Thought (CoT) output. These labels require access to ground-truth answers, which are often unavailable in realistic reasoning settings.\nWhile outcome-level supervision is cheaper than step-level annotation, this advantage only holds for structured math benchmarks such as GSM8K and MATH where answers are explicitly known. In broader domains such as commonsense or scientific reasoning, correctness labels are difficult or costly to obtain, limiting the method’s general applicability.\nMoreover, if correctness labels already exist, it is unclear why a separate verifier model is needed to approximate an evaluation signal that can be computed directly. The paper would benefit from clarifying whether EORM is intended for use on unlabeled reasoning data and how it could generalize without direct correctness supervision."}, "questions": {"value": "- Could the authors clarify how the baselines in Table 2 were selected and evaluated?\nSpecifically, were all models re-evaluated under the same data splits, sampling settings, and inference conditions as EORM, or were the reported numbers taken from prior work? A detailed explanation would help assess the fairness and validity of the comparison, especially given that several baselines use reinforcement learning or instruction-tuning procedures. (see W1)\n\n- Since EORM is trained on final correctness, one could simply select candidates by checking answer accuracy when ground-truths are available. In what scenarios does EORM offer an advantage over such direct answer-based selection? Process reward models (PRM) provide additional step-wise signals that typical labeling cannot capture, so it would be helpful to clarify how EORM’s practical value compares when such signals are or are not available. (see W2)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9RSy9rQVsg", "forum": "Kotvxxstmm", "replyto": "Kotvxxstmm", "signatures": ["ICLR.cc/2026/Conference/Submission14655/Reviewer_suFK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14655/Reviewer_suFK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973172489, "cdate": 1761973172489, "tmdate": 1762925025624, "mdate": 1762925025624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of reliably and efficiently verifying mathematical reasoning in Large Language Models (LLMs). The authors introduce the **Energy Outcome Reward Model (EORM)**, a novel and highly efficient post-hoc verifier for ranking Chain-of-Thought (CoT) solutions.\n\nThe core of the method is an **Energy-Based Model (EBM)** implemented as a lightweight (55M parameter) Transformer. This model is trained to assign a scalar \"energy\" score to any given CoT solution, where correct reasoning paths are assigned lower energy than incorrect ones. A key advantage of this approach is its training data requirement: EORM learns effectively using only simple **binary outcome labels** (i.e., whether the final answer is correct or incorrect), completely avoiding the need for expensive step-by-step annotations (like PRMs) or preference pairs (like POs).\n\nThe paper's main contributions are:\n1.  A novel, lightweight (55M) and efficient verifier architecture (EORM) based on EBM principles, which is over 127 times smaller than typical 7B reward models.\n2.  State-of-the-art reranking performance on mathematical benchmarks. By selecting the best from a pool of candidates, EORM boosts the accuracy of Llama 3 8B to **90.7% on GSM8k** and **63.7% on MATH**.\n3.  Strong empirical evidence of **robust generalization**. The model, trained on GSM8k and MATH, generalizes effectively to out-of-distribution (OOD) datasets (like AIME and Gaokao Math) and, critically, to outputs from LLM architectures not seen during training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* **Exceptional Efficiency:** The most significant strength is the model's size. At only **55M parameters**, EORM is over 127 times smaller than standard 7B-8B parameter reward models. This makes it an incredibly practical and lightweight tool that can be cheaply deployed for inference alongside a generator LLM.\n\n* **Low-Cost Supervision:** The model is trained *only* on binary outcome labels (correct/incorrect). This is a massive practical advantage over Process Reward Models (PRMs), which require costly and labor-intensive step-by-step human annotations, and is simpler than collecting preference pairs for Preference Optimization (PO).\n\n* **Strong Empirical Performance:** Despite its small size, EORM achieves outstanding results. It significantly boosts the performance of all base LLMs it is applied to. For instance, it improves Llama 3 8B's accuracy from 76.6% to 90.7% on GSM8k and from 28.9% to 63.7% on MATH. It consistently outperforms both majority voting and a baseline ORM in reranking.\n\n* **Demonstrated Generalization (Robustness):** The paper provides a very thorough validation of its generalization claims, which is a critical aspect for a practical verifier.\n    * **OOD Tasks:** When trained only on GSM8k and MATH, EORM generalizes well to more difficult, unseen datasets like AIME 2024 and Gaokao Math.\n    * **Unseen Models:** The leave-one-out experiments are highly convincing. A variant trained on 4/5 models (e.g., excluding Llama 3) can still dramatically improve the held-out model's accuracy (e.g., Llama 3 GSM8k base 42.9% -> 76.6%). This strongly supports the claim that EORM learns fundamental principles of valid reasoning rather than just overfitting to the stylistic quirks of its training models.\n\n* **Clarity and Soundness:** The paper is exceptionally well-written. The problem is clearly motivated and contrasted with existing work. The methodological choice of using an EBM for ranking (which avoids computing the partition function $Z_{\\theta}$) and training with a Bradley-Terry loss is elegant, well-justified, and theoretically sound."}, "weaknesses": {"value": "* **Confusing \"ORM\" Baseline:** In Figure 3, the paper compares EORM against \"ORM\". This is confusing because the paper's own model is an \"Energy **Outcome Reward Model**\" (EORM). The paper also mentions \"traditional Outcome Reward Models\", but the specific architecture and loss function (e.g., standard classification cross-entropy?) of this \"ORM\" baseline are not defined. This makes the comparison in Figure 3 difficult to interpret.\n\n* **Massive Training Data Requirement:** The model's success appears to be supported by a very large dataset of approximately 15 million (14958k) (question, CoT, label) triplets. The \"low-cost supervision\" (outcome labels) is thus traded for \"high-volume data generation.\" While this is a *far better* trade-off than PRMs (which need high-cost supervision), the computational cost of generating and auto-labeling 15 million samples is still significant. The paper would be strengthened by a study of performance vs. training data scale.\n\n* **Weak Ablation Baseline (MLP):** The ablation study in Figure 4 (left) compares the Transformer-based EORM to an \"MLP Verifier\". This is a weak baseline, as a simple MLP cannot effectively process variable-length, sequential token data. The conclusion that the \"Transformer's ability... is crucial\" is almost certainly correct, but a stronger baseline (e.g., a GRU/RNN, or an MLP on-top-of-average-pooled-embeddings) would make this point more definitively."}, "questions": {"value": "1.  **\"ORM\" Baseline:** The comparison in Figure 3 against \"ORM\" is a key result. Could you please clarify what this \"ORM\" baseline model is? What is its architecture, and what loss function is it trained with?\n\n2.  **Training Data Scaling:** The model was trained on a very large dataset of ~15 million samples. How does EORM's performance scale with the *number* of training examples? Is this massive dataset essential for the 55M model, or can it achieve strong (e.g., 90% of final) performance with, say, 1M or 5M examples?\n\n3.  **Generalization Test Setup:** The \"EORM Generalize\" (leave-one-out) results in Table 4 are very strong. Did you also experiment with a more practical \"general-purpose verifier\" scenario, such as training on a *subset* of models (e.g., Llama 2, Mistral) and testing on a completely *disjoint* set (e.g., Llama 3, Qwen 2.5)?\n\n4.  **MLP Ablation Details:** For the \"MLP Verifier\" ablation, how was the variable-length CoT sequence (token embeddings) fed into the MLP? Was it truncated, or was some form of pooling (e.g., average pooling) used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Um3CuLctnV", "forum": "Kotvxxstmm", "replyto": "Kotvxxstmm", "signatures": ["ICLR.cc/2026/Conference/Submission14655/Reviewer_RSFF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14655/Reviewer_RSFF"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762448851001, "cdate": 1762448851001, "tmdate": 1762925024993, "mdate": 1762925024993, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}