{"id": "5d1M87NQDq", "number": 22917, "cdate": 1758337054685, "mdate": 1759896840207, "content": {"title": "Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents", "abstract": "Mobile-use agents powered by vision-language models (VLMs) have shown great potential in interpreting natural language instructions and generating corresponding actions based on mobile graphical user interface. Recent studies suggest that incorporating chain-of-thought (CoT) reasoning tends to improve the execution accuracy. However, existing evaluations emphasize execution accuracy while neglecting whether CoT reasoning aligns with ground-truth actions. This oversight fails to assess potential reasoning-execution gaps, which in turn foster over-trust: users relying on seemingly plausible CoTs may unknowingly authorize harmful actions, potentially resulting in financial loss or trust crisis. In this work, we introduce a new evaluation framework to diagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment (GTA), which measures whether the action implied by a CoT matches the ground-truth action. By combining GTA with the standard Exact Match (EM) metric, we jointly assess both the reasoning accuracy and execution accuracy. This joint perspective reveals two types of reasoning-execution gaps: (i) Execution Gap (EG), where the reasoning correctly identifies the correct action but execution fails, and (ii) Reasoning Gap (RG), where execution succeeds but reasoning process conflicts with the actual execution. Experimental results across a wide range of mobile interaction tasks reveal that reasoning-execution gaps are prevalent, with execution gaps occurring more frequently than reasoning gaps. Moreover, while scaling up model size reduces the overall gap, sizable execution gaps persist even in the largest models. Further analysis shows that our framework reliably reflects systematic EG/RG patterns in state-of-the-art models. These findings offer concrete diagnostics and support the development of more trustworthy mobile-use agents. Our data and code are publicly available at \\textit{anonymity}.", "tldr": "We propose a new evaluation framework for mobile-use agents powered by vision–language models that diagnoses reasoning–execution gaps.", "keywords": ["Vision–Language Models", "Mobile-use agents", "Chain-of-Thought reasoning", "Reasoning–Execution Gap"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88e248f0297a32564bb0a8098916602b9f4052e7.pdf", "supplementary_material": "/attachment/a3e1bb56697551674026f4830e8fe2197cae8d8a.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a diagnostic framework that separates reasoning quality from execution accuracy for VLM-based mobile GUI agents. Beyond the standard Exact Match (EM) for action execution, the authors introduce Ground-Truth Alignment (GTA) to check whether the chain-of-thought (CoT) logically *implies* the correct action. EM and GTA are combined into a four-quadrant analysis (Ideal / Execution Gap / Reasoning Gap / Both-Wrong). A GTA Evaluator maps free-form CoT to an implied action and compares it with ground truth. Experiments on AITZ, CAGUI, and AndroidControl analyze the prevalence of Execution Gap (EG) versus Reasoning Gap (RG) across several agents."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The EM–GTA decomposition is intuitive and yields a useful vocabulary for analyzing GUI-agent failures (EG vs. RG).  \n2. In many model–dataset pairs, GTA ≥ EM, suggesting that translating reasoning into concrete UI actions is a common bottleneck (EG)."}, "weaknesses": {"value": "1. The GTA Evaluator lacks crucial implementation details (prompt schema, action ontology, disambiguation rules, handling of multi-intent CoTs). This hinders reproducibility and makes it hard to assess validity.  \n2. Since the evaluator is a learned VLM trained on GUI data, there is a risk of bias/circularity. Cross-evaluator agreement (rule-based parser, a different VLM, humans) is not reported.  \n3. The paper diagnoses EG/RG but offers few concrete remedies or controlled interventions. \n4. All results hinge on one evaluator model. Without cross-model validation, the findings may reflect evaluator bias rather than ground truth and limit the generality of the conclusions."}, "questions": {"value": "1. How is the action implied by the chain-of-thought (CoT) extracted automatically in practice?\n2. Figure 3 shows evaluator accuracy on CAGUI and AITZ below 90%. Is this level sufficient for trustworthy conclusions?\n3. Figure 4 exhibits divergent trends: on AITZ, UI-TARS and GUI-Owl have similar EG and RG, whereas on CAGUI and AndroidControl, GUI-Owl’s EG (and the “both-wrong” rate) increase substantially. Does this indicate evaluator instability, dataset-specific bias, or genuine differences in model behavior?\n4. Do the findings hold when using a different evaluator model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Gu8kBbKPQu", "forum": "5d1M87NQDq", "replyto": "5d1M87NQDq", "signatures": ["ICLR.cc/2026/Conference/Submission22917/Reviewer_ScfG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22917/Reviewer_ScfG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760936337626, "cdate": 1760936337626, "tmdate": 1762942436032, "mdate": 1762942436032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a diagnostic framework for evaluating the faithfulness of reasoning and action execution in VLM-powered mobile agents. The key contribution is the Ground-Truth Alignment (GTA) metric, which measures whether the chain-of-thought (CoT) reasoning aligns with the ground-truth action. By combining GTA with execution accuracy (Exact Match, EM), the authors propose a four-quadrant taxonomy to identify failure modes, including Execution Gaps (EG) and Reasoning Gaps (RG). Experiments show that this framework is demonstrated to be reproducible and aligns strongly with human judgments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper proposes a new evaluation framework, Ground-Truth Alignment (GTA), which combines reasoning consistency (CoT) and execution accuracy (Exact Match, EM), providing a more detailed assessment of reasoning-execution alignment for mobile agents.\n\n2. Experimental results demonstrate that the framework's outcomes align strongly with human annotations, ensuring the reliability of the evaluation results."}, "weaknesses": {"value": "1. Although the paper proposes a new evaluation method, GTA (Ground-Truth Alignment), and analyzes reasoning-execution gaps using a quadrant framework, the overall novelty of the paper is limited.\n2. While the paper mentions the GTA Evaluator as a tool for evaluation, the details of its implementation are insufficient. For example, has the GTA Evaluator been fine-tuned? If so, what datasets were used for training and optimization? If not, how does the evaluator generate reasoning, and does it use any special prompts (e.g., zero-shot or few-shot learning) to enhance reasoning?\n3. The proposed evaluation method heavily relies on the performance of the evaluator model, which may introduce biases in the evaluation results, such as bias or fitting to specific phrasing. If this is the case, it should be discussed in the limitations section.\n4. Although the paper defines Execution Gap (EG) and Reasoning Gap (RG) and conducts quadrant analysis, the in-depth analysis of these phenomena is lacking. For example, the causes, specific manifestations, and impacts of EG and RG on model performance are not thoroughly discussed."}, "questions": {"value": "See the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pkBU24l8AI", "forum": "5d1M87NQDq", "replyto": "5d1M87NQDq", "signatures": ["ICLR.cc/2026/Conference/Submission22917/Reviewer_2Hkw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22917/Reviewer_2Hkw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807643281, "cdate": 1761807643281, "tmdate": 1762942435701, "mdate": 1762942435701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the reasoning-execution gap in VLM powered mobile-use agent systems with CoT reasoning. While current benchmarks mainly evaluate execution accuracy via Exact Match (EM), this work argues that plausible CoT reasoning does not necessarily imply correct action and execution. To address this, the authors propose a new evaluation framework incorporating a novel metric, Ground-Truth Alignment (GTA), which measures whether the action implied by the CoT aligns with the ground-truth action. Combining GTA and EM metrics, the authors provide a four-quadrant diagnostic framework for reasoning-execution gaps. According to the experimental results on three benchmarks, the authors claim that the reasoning-execution gaps are prevalent, particularly execution gaps. The analysis further discusses scaling effects and provides insights for future studies about trustworthy VLM-powered mobile-use agent systems."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper focuses on deepen the understanding of the faithfulness and reliability of mobile-use agents with CoT reasoning, which is critical for safe deployment in real-world applications.\n\n- The GTA metric enables the evaluation of reasoning gap and can provide more perspectives for understanding the faithfulness of mobile-use agents together with the exact match evaluation.\n\n- This benchmarking provides the insight that the reasoning-execution gaps, particularly the execution gap is common in VLM-powered mobile-use agent systems. It clearly exposes this overlooked problem to the community and can benefit following research.\n\n- The paper shows the analysis regarding parameter scaling effect, which indicates that the larger models diminish returns in closing the reasoning-execution gaps. It provides some useful insights for the community"}, "weaknesses": {"value": "- The GTA evaluator uses local history and current observation as priors for decoding the free-text CoT outputs into actions, which can introduce bias.\n\n- I would suggest adding some analysis/ablation studies regarding the effect of local history and current observation in mapping CoT to actions.\n\n- While it studies the parameter scaling effect, it would be more convincing to show how stronger reasoning models might help mitigate the reasoning-execution gaps.\n\n- The technical innovation is limited.\n\n- It would be better to show some case studies."}, "questions": {"value": "- In Figure 4, why does IDEAL show lower values (%)? What do the values represent on the y-axis?\n\n- Will the GTA evaluator be sensitive to the CoT mapping/parsing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pnJ8uHIgPC", "forum": "5d1M87NQDq", "replyto": "5d1M87NQDq", "signatures": ["ICLR.cc/2026/Conference/Submission22917/Reviewer_VPV5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22917/Reviewer_VPV5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994043267, "cdate": 1761994043267, "tmdate": 1762942435191, "mdate": 1762942435191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a diagnostic framework for diagnosing reasoning-execution gaps in VLM-powered mobile-use agents. The framework proposes the Ground-Truth Alignment (GTA) metric to assess whether the action implied by chain-of-thought (CoT) reasoning aligns with the ground-truth action. By combining GTA with the standard Exact Match (EM) metric, this diagnostic framework categorizes model outputs into four quadrants: (i) Ideal, where both reasoning and action are correct; (ii) Execution Gap (EG), where reasoning is correct but execution fails; (iii) Both Wrong, where both reasoning and action are incorrect; and (iv) Reasoning Gap (RG), where the action is correct but reasoning is inconsistent. In addition, the paper presents an automatic GTA evaluator validated through stratified sampled human annotation, and conducts experiments on three mobile-interaction benchmarks (AITZ, CAGUI, and AndroidControl) using multiple state-of-the-art models. Results reveal that reasoning-execution gaps are prevalent, with execution gaps occurring more frequently than reasoning gaps. Besides, causal CoT models typically exhibit RG > EG. Most models are capable of correct reasoning, at least in generating valid thought chains, but encounter difficulties in mapping these intermediate reasoning steps to the precise executable actions required by the task. Furthermore, scaling up model size reduces the overall gap but does not eliminate it."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper investigates an less-studied aspect of faithfulness in mobile-use agents: the inconsistency between CoT and its executed actions. In addition, the proposed GTA metric and four-quadrant framework provide a novel and principled method to disentangle reasoning accuracy from execution accuracy, surpassing the conventional EM metric in interpretability. The automatic GTA evaluator, built upon AgentCPM-GUI-8B and validated through human annotations, achieves accuracies ranging from 78% to 93% across diverse datasets and models, enabling scalable analysis without extensive manual effort. Experimental results across various benchmarks and model families (e.g., AgentCPM, UI-TARS, GUI-Owl) further demonstrate that models face a bottleneck in correctly executing the outcomes of CoT reasoning. Overall, this work demonstrates originality by offering a clear categorization within the diagnostic framework for mobile-use agents, while intuitively highlighting the primary and secondary issues in existing mobile-use agents, thereby informing future enhancements in agent reliability."}, "weaknesses": {"value": "1. The work primarily emphasizes diagnosis without proposing methods to mitigate the identified gaps (e.g., through targeted fine-tuning), thereby leaving a gap between analytical insights and actionable improvements.\n\n2. Certain visualizations, including charts and tables, exhibit presentation issues; for example, Figure 4 employs a line chart to illustrate differences among GTA, EM, and IDEAL, which may mislead readers due to implied trends—replacing it with a bar chart or another non-trend-oriented format would enhance clarity.\n\n3. Compared to Execution Gap (EG), the Reasoning Gap (RG) addressed in the paper may not represent a highly significant research focus, potentially diminishing the work's contribution. Because this seems to be a relatively minor point in mobile-use agents."}, "questions": {"value": "1. I would like to see some execution examples of the GTA Evaluator, including both successful and erroneous cases, to provide readers with a clearer understanding of how GTA operates. \n2. Why does Figure 9 in the Appendix fall into the RG category? What errors are present in the model’s CoT results? Could you explain the rationale behind Figure 9 or provide additional examples of RG for further clarity?\n3. The evaluation result appears to emphasize three intuitively plausible categories—EG, Ideal, and Both Wrong—as the most prevalent scenarios. In contrast, the RG scenario, presented in the diagnostic framework, seems less critical in practice. If the authors could provide additional examples and in-depth analysis of RG to better underscore its significance, I would consider raising my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5xIq8uqmA0", "forum": "5d1M87NQDq", "replyto": "5d1M87NQDq", "signatures": ["ICLR.cc/2026/Conference/Submission22917/Reviewer_LPGH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22917/Reviewer_LPGH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22917/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088864649, "cdate": 1762088864649, "tmdate": 1762942434580, "mdate": 1762942434580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}