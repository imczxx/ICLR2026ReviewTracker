{"id": "DK4kl9eTIo", "number": 14037, "cdate": 1758227444095, "mdate": 1759897394414, "content": {"title": "Boltzmann Graph Networks: Efficient Energy-Based Framework for Graph Representation Learning", "abstract": "With the rapid growth of interconnected data, graph-structured representations have become essential for modeling complex relational systems. Graph Neural Networks (GNNs) are widely used architectures for leveraging semantic information encoded in nodes and edges. The current GNN models encounter multiple challenges, including over-smoothing along with restricted ability to model distant dependencies and excessive computational requirements from complex graph structures. This paper introduces Boltzmann Graph Network (BGN), a novel and efficient GNN architecture that integrates energy-based probabilistic models with deterministic graph convolution techniques. By conceptualizing the graph as an energy landscape, BGN employs a Boltzmann-inspired energy function to capture intricate node and edge interactions, enabling robust representation learning. The use of k-step persistent contrastive divergence while ensuring compatibility with gradient-based optimization, mitigates over-smoothing, and enhances long-range dependency modeling. Comprehensive evaluations for node prediction on citation network benchmarks show that BGN achieves state-of-the-art performance on both random and Geom-GCN splits. With test accuracies of 88.0% (Cora), 75.6% (CiteSeer), and 85.6% (PubMed) on random splits, while on Geom-GCN splits the model attains 85.8% (Cora), 75.5% (CiteSeer), and 88.2% (PubMed), demonstrating consistent improvements over existing methods. These results highlight BGN’s scalability, robustness, and efficiency, positioning it as a powerful framework for advancing graph-based learning across diverse applications.", "tldr": "Boltzmann Graph Networks (BGNs) introduce an efficient, energy-based GNN for robust node representation learning in citation network node classification.", "keywords": ["Graph Neural Networks", "Energy-Based Learning", "Restricted Boltzmann Machines", "Node Representation Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d07254ee57f7bf4bd619be9aab34f47881d26c55.pdf", "supplementary_material": "/attachment/789d81132325f857c307fada8da1fd0ea4c1917f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the Boltzmann Graph Network (BGN), a novel GNN architecture designed to address limitations like over-smoothing and difficulty modeling long-range dependencies in traditional GNNs. BGN integrates energy-based models, inspired by Restricted Boltzmann Machines (RBMs), with deterministic graph convolution methods. It employs an energy function and k-step Persistent Contrastive Divergence (PCD) during training to learn node representations, aiming to capture intricate interactions and mitigate common GNN issues. The model achieves state-of-the-art results on node classification tasks for citation networks like Cora, CiteSeer, and PubMed, particularly under the challenging random and Geom-GCN data splits."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces the Boltzmann Graph Network (BGN), a novel architecture that integrates energy-based probabilistic models inspired by Restricted Boltzmann Machines (RBMs) with deterministic graph convolution techniques .\n\n- BGN achieves state-of-the-art performance on node classification benchmarks for citation networks under the challenging random and Geom-GCN data splits, surpassing several existing methods"}, "weaknesses": {"value": "- The model shows inconsistent performance, achieving state-of-the-art results on random and Geom-GCN splits but performing poorly compared to baselines on the standard public splits .\n\n- Claims of computational efficiency are questionable as the training process involves iterative k-step Persistent Contrastive Divergence, which appears computationally intensive, and no direct runtime comparisons to baselines are provided .\n\n- The paper suffers from weak theoretical motivation connecting the RBM mechanisms specifically to mitigating GNN issues like over-smoothing, and it lacks ablation studies to fully justify the complexity of the added components"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jWLuteZZmv", "forum": "DK4kl9eTIo", "replyto": "DK4kl9eTIo", "signatures": ["ICLR.cc/2026/Conference/Submission14037/Reviewer_r93Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14037/Reviewer_r93Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761452299617, "cdate": 1761452299617, "tmdate": 1762924527484, "mdate": 1762924527484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **Boltzmann Graph Networks (BGN)**, a novel framework that combines **Restricted Boltzmann Machine (RBM)**-style energy-based modeling with **Graph Neural Networks (GNNs)**. The authors propose a **Boltzmann Graph Layer** that replaces standard message-passing convolutions with bidirectional stochastic sampling using **k-step Persistent Contrastive Divergence (PCD)**. This allows iterative propagation between “visible” and “hidden” node states, capturing complex dependencies and mitigating over-smoothing. Empirical evaluations on **Cora, CiteSeer, and PubMed** show that BGN achieves **state-of-the-art performance on Geom-GCN and random splits**, while being competitive on public semi-supervised splits."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper offers an **original hybridization** of energy-based models and graph representation learning. While both RBMs and GNNs are mature paradigms, their **integration into a unified, differentiable, bidirectional framework** is novel and non-trivial.\n\n* The paper presents a **comprehensive technical formulation** with well-structured derivations, including clear algorithmic pseudocode (Algorithm 1) and explicit forward/backward sampling equations.\n\n* The exposition is clear and pedagogical, with careful definitions of the energy function, probabilistic sampling, and training procedure."}, "weaknesses": {"value": "* While the paper claims efficiency and scalability, the computational overhead of Gibbs sampling and PCD iterations is not quantified. A runtime comparison versus standard GCN or GAT under equal hardware settings would strengthen the claims.\n* The convergence stability of PCD-trained stochastic layers within gradient-based optimization is only qualitatively discussed. A more rigorous analysis (e.g., stability bounds or gradient variance trends) would improve credibility.\n\n* The experiments are limited to small citation networks (Cora, CiteSeer, PubMed). Demonstrating scalability on large (OGB datasets) or heterophily graphs (TEXAS,WISCONSIN,ACTOR,CORNELL) would enhance the paper’s impact.\n* A lot of old baselines are missing, like [1-4].\n\n[1] Graph Neural Networks Inspired by Classical Iterative Algorithms, ICML\n[2] Simple and Deep Graph Convolutional Networks, ICML\n[3] Predict then Propagate: Graph Neural Networks meet Personalized PageRank. ICLR\n[4] Representation Learning on Graphs with Jumping Knowledge Networks, ICML"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VphVloYjH4", "forum": "DK4kl9eTIo", "replyto": "DK4kl9eTIo", "signatures": ["ICLR.cc/2026/Conference/Submission14037/Reviewer_eTeM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14037/Reviewer_eTeM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634610977, "cdate": 1761634610977, "tmdate": 1762924526955, "mdate": 1762924526955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel kind of GNN inspired by Restricted Boltzmann Machines (RBMs) and uses special training, a gradient-based variant of Persistent Contrastive Divergence. It shows that this yields good performance on citation data compared to the conventional message-passing GNNs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper considers an interesting topic.\n- The results on the citation data are encouraging"}, "weaknesses": {"value": "- The paper is missing to give a good introduction of the preliminaries (RBMs) so it is hard to understand and needs extra googling. The method section could be written much more shortly and concisely so that there would be space for that.\n- My main criticism is the empirical evaluation. Expriments are only conducted over citation data, which is hardly enough to promote the model as a new, general GNN architecture.\n- Moreover, no more analysis is given. The results cover the standard numbers and ablation studies, but it is unclear where or why this GNN shines particularly - or is it citation data due to its shape?\n\n--------------------------\n- Note: There is some related work looking at energy-based transformers over graphs but, after all, they are just fully-connected GNNs:\nHoover et al. Energy Transformer, Neurips'23"}, "questions": {"value": "---"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R8vZ7ngjyg", "forum": "DK4kl9eTIo", "replyto": "DK4kl9eTIo", "signatures": ["ICLR.cc/2026/Conference/Submission14037/Reviewer_k7hG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14037/Reviewer_k7hG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918422542, "cdate": 1761918422542, "tmdate": 1762924526375, "mdate": 1762924526375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Boltzmann Graph Network (BGN). It integrates energy-based probabilistic models with deterministic graph convolution techniques. The Boltzmann Graph Layers performs iterative forward–backward propagation between visible and hidden units. This enables bidirectional information flow, capturing both local and global dependencies without requiring very deep layers. Empirical results on citation network benchmarks demonstrate BGN's superiority in accuracy, robustness, and computational efficiency. The main contributions are: 1. it is the first GNN integrating RBM-style energy minimization into graph learning, 2. the new bidirectional propagation mechanism improves learning on higher-order dependencies, 3. The adaptation of k-step Persistent Contrastive Divergence (PCD) for graph data enables gradient-based optimization while mitigating oversmoothing. Empirical results show improvement compared to GNN baselines. Ablation studies show how different k PCD steps impact both performance measures and execution time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a novel hybrid work which successfully bridges energy-based modeling (RBM) and GNNs.  \n2. The iterative forward–backward propagation enables richer feature interactions compared to single pass message aggregation in traditional GNNs.  \n3. The Boltzmann energy mechanism and PCD training help mitigate oversmoothing and enhance long-range dependency modeling.  \n4. Empirical results demonstrate strong performance and consistent improvements across both random and geometrically split benchmark datasets."}, "weaknesses": {"value": "1. Experiments focus mainly on small citation networks.  \n2. BGN might not be scalable since iterative sampling and bidirectional updates could become costly for large-scale graphs with millions of nodes.  \n3. While the model claims improved efficiency and reduced computational complexity, detailed runtime or memory comparisons are limited."}, "questions": {"value": "1.Is BGN scalable in large graph settings?  \n2.How sensitive is BGN to the number of CD steps?  \n3.The authors suggests the RBM-based architecture mitigates oversmoothing, but the ablation study shows a decreasing trend of performance with increasing K. Will oversmoothing still exist with large K like 100? Can we use energy-based framework as a regularizer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8CWdBxhcWH", "forum": "DK4kl9eTIo", "replyto": "DK4kl9eTIo", "signatures": ["ICLR.cc/2026/Conference/Submission14037/Reviewer_AzV2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14037/Reviewer_AzV2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961274359, "cdate": 1761961274359, "tmdate": 1762924525657, "mdate": 1762924525657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}