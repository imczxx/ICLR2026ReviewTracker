{"id": "sAzUQkP47r", "number": 21495, "cdate": 1758318265938, "mdate": 1763388460000, "content": {"title": "OpenEstimate: Evaluating LLMs on Probabilistic Estimation with Real-World Data", "abstract": "Real-world settings where language models (LMs) are deployed -- in domains spanning healthcare, finance, and other forms of knowledge work -- require models to grapple with incomplete information and reason under uncertainty. Yet most LM evaluations focus on problems with well-defined answers and success criteria. This gap exists in part because natural problems involving uncertainty are difficult to construct: given that LMs have access to most of the same knowledge as humans, it is non-trivial to design questions for which LMs will struggle to produce correct answers, but which humans can answer reliably. As a result, LM performance on reasoning under uncertainty remains poorly characterized. To address this gap, we introduce OpenEstimate, an extensible, multi-domain benchmark for evaluating LMs on numerical estimation tasks that require models to synthesize significant amounts of background information and express predictions as probabilistic priors. We assess these priors for accuracy and calibration, quantifying their usefulness relative to samples from the true distribution of interest. Across six frontier LMs, we find that LM-elicited priors are often inaccurate and overconfident. Performance improves modestly depending on how uncertainty is elicited from the model, but is largely unaffected by changes in sampling strategy, reasoning effort, or prompt design. The OpenEstimate benchmark thus offers a challenging evaluation for frontier LMs and a platform for developing models that are better at probabilistic estimation and reasoning under uncertainty.", "tldr": "Language models (LMs) excel at reasoning on tasks with clear answers and complete information. Yet many real-world applications are open-ended and uncertain, and require reasoning about incomplete or noisy data.", "keywords": ["probabilistic estimation", "reasoning", "uncertainty", "calibration"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4ac249f575c12762c87a71654d318b288d597b1f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new benchmark designed to evaluate the ability of LLMs to perform probabilistic estimation, as opposed to merely extracting point estimates from such foundation models. The **OpenEstimate** benchmark involves asking LLMs to generate parametric Bayesian priors, specifically parametrized as Normal or Beta distributions for a set of of \"derived variables\" from real-world datasets in various fields.\n\nThe evaluation is twofold: firstly **accuracy**, which uses a normalized mean absolute error to verify whether the location of the mode of the distribution is correct, while accounting for the probability mass of the 'ground truth' distribution at that point.\nSecondly, the authors measure **calibration**, a kind of empirical earth-mover's distance between the observed frequencies and the predicted distribution.\n\nExperiments with several recent general-purpose LLMs suggest no systematic patterns by any particular LLM on any particular task, but that most models tend to be overconfident and do worse than 5 draws from the empirical distribution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important and topical problem of evaluating LLM quantification of uncertainty, which is an emerging and until recently underexplored area of research. Generating full, parametrized priors is a relatively novel idea compared to many other works in the literature, which rely on less 'statistical' approaches, such as simple point estimates or series of questionnaires.\n\nThe proposal of \"derived variables\" is a good idea, as it brings the benchmark closer to measuring the utility of priors on realistic tasks, rather than estimating superficial summaries.\n\nIf the code and benchmark datasets are made available, this could be a useful resource for other researchers and practitioners. [NB: the code/data was not visible to reviewers at this stage.]\n\nThe motivation for the paper is well articulated and overall the writing and presentation of the article are clear. The figure labels are readable (with caveats, see below)."}, "weaknesses": {"value": "The most significant weakness of the paper is the restriction to normal and beta distributions for continuous variables and proportions, respectively. While this is noted in the limitations section, it undermines the validity of the benchmark. For any continuous variable with a skewed distribution or heavy tails---which is indeed likely to include those in one or more of the chosen benchmark datasets---the prior will be fundamentally mis-specified. The use of the mode for MAE should be better justified; while it coincides with the median/median of the normal distribution, this is not true more generally.\n\nOn a related note, the beta distribution has more than one parametrization, and so (without seeing the prompt, which is missing from the paper), it will be impossible to disentangle the performance of the LLM from its 'understanding' of the choice of parametrization. Indeed, the first line of the appendix hints at an issue that might be related, and which may have been resolved using 'function calling' capabilities to constrain the format of the output.\n\nThe choice of **expected calibration error** (ECE), based on four coarse bins seems like reinventing the wheel. The construction of this metric is ad-hoc, lossy and non-standard for this type of analysis. A graphical method such as a QQ plot would have provided a far more rigorous and informative assessment of calibration. Splitting data into quarters (incorrectly referred to in the paper as 'quartiles', but quartiles/quantiles are *points*, not *intervals*; a common mistake) seems arbitrary and discards information within those bins. The number of models being compared is small enough to allow for a visual method, and if not then other, more granular numerical metrics are available, as well as standard statistical tests for comparison of empirical and expected distributions.\n\nPoor visualization. Bar charts with error bars, sometimes called \"dynamite plunger plots\", should never be used, and hide the true distribution of performance across the different tasks. Box plots or simple dots and error bars would be better. See Drummond & Vowler (2011; doi:10.1111/j.1476-5381.2011.01251.x).\n\nThe distinction between 'reasoning' and 'non-reasoning' models in the ablation study needs to be more clearly defined and justified. What is the basis for classifying, e.g. `GPT-4o` as 'non-reasoning'?\n\nSome related works need to be mentioned in the literature review. This is not the first work to propose eliciting parametric Bayesian priors from LLMs, see for example Selby et al (2025; doi:10.1002/sta4.70054), which explores the problem of evaluating the quality of LLM priors on real-world datasets. Data augmentation approaches (i.e. sampling pseudo-observations from the LLM and then looking at the empirical distribution) should also get a mention: see Huynh et al (2025; https://openreview.net/forum?id=2Q3gFNbpAr).\n\nFinally, there are one or two minor typos: e.g. \"empricial\" at bottom of page 6."}, "questions": {"value": "1. The central claim is that LLMs appear to be 'overconfident'. How can we be sure that this is not an artifact of the experimental design? The restriction to Gaussian priors for heavy-tailed or skewed data (e.g. funding) is likely to enforce model mis-specification.\n\n2. Why split the data into four coarse bins to calculate ECE instead of standard, more informative methods for assessing calibration, such as quantile--quantile plots?\n\n3. What is the definition of 'reasoning' and 'non-reasoning'. Can you justify placing powerful GPT models in the 'non-reasoning' category?\n\n4. What is the justification for using the *mode* of the distribution for the MAE calculation?\n\n5. What is the formula or citation used for the correction term from the Jeffreys prior used in the scale-adjusted log-probability metric?\n\n6. How does this work compare with others on eliciting Bayesian priors from LLMs (see above)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LziGa14WoE", "forum": "sAzUQkP47r", "replyto": "sAzUQkP47r", "signatures": ["ICLR.cc/2026/Conference/Submission21495/Reviewer_H3RD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21495/Reviewer_H3RD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761817943653, "cdate": 1761817943653, "tmdate": 1762941806313, "mdate": 1762941806313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OpenEstimate, a benchmark for testing large language models on probabilistic estimation tasks. Instead of giving point estimates, models must express beliefs as Bayesian priors (Gaussian or Beta distributions) for real-world quantities drawn from datasets in labor economics, finance, and public health. Results show that even advanced models like GPT-4 and LLaMA 3 are poorly calibrated and overconfident, performing no better than simple statistical baselines built from a few real samples."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper fills a clear gap by evaluating LLMs’ ability to reason under uncertainty, focusing on probabilistic estimation rather than deterministic prediction.\n\n2. The study spans multiple domains and models, using clear metrics for accuracy and calibration and comparing results against interpretable statistical baselines."}, "weaknesses": {"value": "1. The paper mainly reports performance differences without probing why models fail to calibrate uncertainty.\n2. Restricting distributions to Gaussian and Beta forms limits realism for complex or multimodal uncertainty.\n3. The benchmark is not used to improve models or study uncertainty learning, missing the opportunity to explore how training on such data could enhance self-calibration.\n4. Only zero-shot inference is tested. There’s no attempt to improve performance through structured prompting, self-consistency, or retrieval, which would make the benchmark more actionable."}, "questions": {"value": "Do you expect that supervised or reinforcement learning using OpenEstimate could improve uncertainty calibration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8SBDYUaeJB", "forum": "sAzUQkP47r", "replyto": "sAzUQkP47r", "signatures": ["ICLR.cc/2026/Conference/Submission21495/Reviewer_5LwK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21495/Reviewer_5LwK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839855649, "cdate": 1761839855649, "tmdate": 1762941806042, "mdate": 1762941806042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper builds a benchmark dataset OPENESTIMATE, which evaluates LLMs' ability to provide a prior probability distribution over uncertain variables. The variables are constructed from real-world datasets on labor economics, private markets, and human health. LLMs are prompted to specify a Gaussian or Beta distribution for the variables. Results show that state-of-the-art LLMs perform poorly on this benchmark; for example, the LLM-generated priors are often less accurate than posteriors formed from 5 real samples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper studies a practically important problem, namely LLMs' ability to give a good prior over random quantities in the real world.\n2. The paper gives a reasonable approach to generate derived variables that are unlikely to have been explicitly documented in LLMs' pretraining data, by conditioning on attributes that change the target statistic sufficiently.\n3. The performance metrics for accuracy and calibration are reasonable and intuitive."}, "weaknesses": {"value": "1. My major concern is that the paper uses only the Gaussian and Beta distributions, which may be highly misspecified probabilistic models for the real-world data. This raises the question of whether the LLMs' poor performance is true reflection of its probabilistic reasoning skills, or an artifact of being forced to specify an inappropriate model. I think an important evaluation would be to let an LLM propose a distribution form on its own, and see if the accuracy and calibration improve.\n2. The derived variables are constructed by conditioning on randomly sampled attributes. While it can effectively avoid data leakage, I wonder if the derived variables are always practically relevant, e.g., ones that a real-world analyst would actually ever estimate."}, "questions": {"value": "1. In Figure 2b, how can the expected calibration error (ECE) be as large as 10? By definition, the maximum value that ECE can take is $|1-0.25| / 4 = 0.175$. Are the numbers supposed to be *percentage* ECEs?\n2. In constructing the derived variables, the paper conditions only on attributes that alters the target statistic by at least 5%. Would this create overly difficult estimation targets with high variability, and thus make the benchmark more difficult than common real-world tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "okafyvbr5u", "forum": "sAzUQkP47r", "replyto": "sAzUQkP47r", "signatures": ["ICLR.cc/2026/Conference/Submission21495/Reviewer_7ciD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21495/Reviewer_7ciD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868594395, "cdate": 1761868594395, "tmdate": 1762941805642, "mdate": 1762941805642, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OPENESTIMATE, a benchmark designed to evaluate LLMs on their ability to generate well-calibrated Bayesian priors for real-world quantities in domains such as labor economics, finance, and public health. Models are asked to express uncertainty as Gaussian or Beta distributions, and their outputs are assessed for accuracy and calibration against empirical data.\n\nExperiments across six frontier LLMs (including GPT-4 and Qwen3-235B) show that current models are generally inaccurate and overconfident, often performing no better than using five random samples from real data. While some reasoning models (e.g., o3-mini, o4-mini) better capture probability mass near true values, calibration remains poor and domain-dependent. Ablation studies reveal that elicitation method (how uncertainty is prompted) affects results more than temperature or system prompt settings.\n\nOverall, the study finds that LLMs’ probabilistic reasoning is weak but not random, showing structured, domain-aware uncertainty that could serve as a foundation for improving AI systems that reason under uncertainty."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel Benchmark Design: The paper introduces OPENESTIMATE, a first-of-its-kind benchmark that evaluates LLMs on probabilistic estimation using real-world tabular data. Unlike prior work focused on deterministic or forecasting tasks, it systematically measures both accuracy and calibration of Bayesian priors.\n\n\n2. Comprehensive Empirical Evaluation: It provides a cross-domain assessment (labor economics, finance, and public health) across multiple frontier models (GPT-4, Llama 3.1, o3/o4-mini, Qwen3). The inclusion of strong statistical baselines gives the results clear interpretability and robustness."}, "weaknesses": {"value": "1. The task definition appears to lack clear motivation. Under a zero-shot setting, the LLM has no prior exposure to the specific datasets used in the benchmark, making it unclear how it could produce meaningful estimations. Moreover, it is debatable whether the model’s outputs can truly be considered “priors,” since they effectively reflect the posterior knowledge embedded during pretraining. The way an LLM is trained or fine-tuned likely has a substantial influence on these results, and this issue should be explicitly acknowledged and discussed.\n\n2. The paper focuses on three domains: labor economics, private markets, and public health, but it is unclear why these specific areas were chosen over other possible domains. The authors should clarify the rationale for selecting datasets exclusively from the social sciences and explain why natural sciences, medicine, or engineering data were not considered. In addition, it would be helpful to articulate what types of real-world reasoning or uncertainty these three domains are intended to represent, and what general conclusions can meaningfully be drawn from results confined to these areas."}, "questions": {"value": "1. The authors could provide a discussion of the differences across models, such as distinctions between open- and closed-source models, and how model size may affect performance on the benchmark.\n\n2. Clarify the motivation and reasonableness of the zero-shot task setting, and whether the model outputs can truly be interpreted as Bayesian priors.\n\n3. Can you justify the choice of focusing only on three social-science domains or expand to other fields."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L5Y51MQM6m", "forum": "sAzUQkP47r", "replyto": "sAzUQkP47r", "signatures": ["ICLR.cc/2026/Conference/Submission21495/Reviewer_oFRa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21495/Reviewer_oFRa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930954508, "cdate": 1761930954508, "tmdate": 1762941805187, "mdate": 1762941805187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}