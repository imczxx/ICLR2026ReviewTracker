{"id": "Gq7mjFEoDm", "number": 16096, "cdate": 1758259936323, "mdate": 1763606320964, "content": {"title": "Hierarchical Encoding Tree with Modality Mixup for Cross-modal Hashing", "abstract": "Cross-modal retrieval is a significant task that aims to learn the semantic correspondence between visual and textual modalities. Unsupervised hashing methods can efficiently manage large-scale data and can be effectively applied to cross-modal retrieval studies. However, existing methods typically fail to fully exploit the hierarchical structure between text and image data. Moreover, the commonly used direct modal alignment cannot effectively bridge the semantic gap between these two modalities. To address these issues, we introduce a novel Hierarchical Encoding Tree with Modality Mixup (HINT) method, which achieves effective cross-modal retrieval by extracting hierarchical cross-modal relations. HINT constructs a cross-modal encoding tree guided by hierarchical structural entropy and generates proxy samples of text and image modalities for each instance from the encoding tree. Through the curriculum-based mixup of proxy samples, HINT achieves progressive modal alignment and effective cross-modal retrieval. Furthermore, we conduct cross-modal consistency learning to achieve global-view semantic alignment between text and image representations. Extensive experiments on a range of cross-modal retrieval datasets demonstrate the superiority of HINT over state-of-the-art methods.", "tldr": "", "keywords": ["Cross-modal Hashing", "Unsupervised Hash Retrieval", "Cross-modal Retireval"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f260b39fa759c2789969f32f6118d98ac483dfc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes HINT (Hierarchical Encoding Tree with Modality Mixup) for cross-modal hashing. The core contribution of HINT is the construction of a cross-modal encoding tree, guided by hierarchical structural entropy, to recover implicit semantic communities and hierarchical relationships. Based on this tree, the method generates same-modality and cross-modality \"proxy samples\" for each instance. Subsequently, a curriculum-based modality mixup strategy is employed to progressively align these proxy samples, thereby gradually bridging the modality gap. The framework is further enhanced with a cross-modal consistency learning objective to ensure global semantic alignment. Extensive experiments on several standard cross-modal retrieval datasets demonstrate that HINT outperforms current state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a critical limitation of existing unsupervised cross-modal hashing methods—their reliance on \"flat\" and sparse image-text pair signals, which ignores the hierarchical semantic structures prevalent in real-world data. Introducing hierarchical modeling into this domain is an intuitive and valuable direction, offering inspirational value.\n\n2. The proposed method is methodologically sound. The framework, which integrates hierarchical structure discovery, proxy sample generation, progressive alignment, and global consistency constraints, forms a logically coherent pipeline. Each component is well-defined and works synergistically toward the final objective."}, "weaknesses": {"value": "1. A core limitation is that the hierarchical encoding tree is constructed only once at the beginning of training and remains static. This process is highly dependent on the quality of the initial features. Sub-optimal or biased features could lead to an erroneous tree, irreversibly compromising the entire subsequent learning process.\n\n2. It is mentioned that The \"Merge\" and \"Compress\" operations are based on “if they can decrease the structural entropy”。Since optimizing structural entropy is an NP-hard problem, this greedy strategy is likely to converge to a local optimum, potentially limiting the quality of the learned hierarchy.\n\n3. In line 183, the description of Eq. (4) mentions terms T_α- and T_α, but these symbols do not appear in the equation itself. The authors should revise this for clarity and consistency.\n\n4.  The generation of proxy samples (Eq. 6) via simple neighbor averaging assumes that local neighborhoods are semantically clean and coherent. However, in real-world data, especially near class boundaries, neighbors may come from different fine-grained categories (e.g., a \"Shepherd dog\" neighboring a \"wolf\"). Averaging these features could lead to \"semantic drift,\" introducing noise rather than robust signals. The paper lacks an analysis of this neighborhood noise.\n\n5. HINT smooths the learning signals by building communities. However, this might also have a side effect: it might \"average out\" those very valuable difficult negative samples (i.e., samples that are semantically close but belong to different classes). For example, in a neighborhood community of an \"Alaskan Husky\" image, there might be an image of an \"Alaskan Malamute\". If they are averaged into a proxy sample, will this weaken the model's ability to learn fine-grained distinctions?\n\n6. Real-world datasets often exhibit significant class imbalance. For tail classes with few samples, the KNN-based neighborhoods can be sparse, unreliable, or even incorrectly connected to head classes. It is unclear whether HINT's tree construction and proxy generation mechanisms would exacerbate or mitigate this problem. The paper does not report on the model's retrieval performance on such tail classes.\n\n7. The paper's readability could be improved by clarifying several points:\n    - The term \"curriculum-based mixup\" is introduced without sufficient explanation or citation on its first appearance.\n    - In line 98, the meaning of \"common\" in \"common visual and text encoders\" is ambiguous and should be specified."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "geyBX81vnt", "forum": "Gq7mjFEoDm", "replyto": "Gq7mjFEoDm", "signatures": ["ICLR.cc/2026/Conference/Submission16096/Reviewer_LKH2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16096/Reviewer_LKH2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448100160, "cdate": 1761448100160, "tmdate": 1762926276785, "mdate": 1762926276785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses unsupervised cross-modal hashing retrieval by proposing HINT, which constructs a hierarchical encoding tree guided by structural entropy to mine local semantic communities and overcome the limitations of flat sparse connections in existing methods. The approach consists of three main components: hierarchical encoding tree construction based on structural entropy, curriculum-based modality mixup strategy, and proxy-based consistency learning. Experiments on benchmarks demonstrate that HINT achieves optimal performance. The work connects encoding trees with cross-modal hashing problems, and alleviates the difficulty of direct heterogeneous modality alignment through progressive alignment strategy."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper introduces structural entropy and encoding tree concepts into cross-modal hashing, providing a fresh perspective on understanding cross-modal relationships from a graph structure viewpoint, which is relatively uncommon in this field.\n2.\tThe curriculum-based modality mixup mechanism is well-designed, dynamically adjusting weights between same-modal and cross-modal features via MMD, embodying an easy-to-hard learning strategy that aligns with cross-modal learning characteristics.\n3.\tThe method achieves consistent performance improvements across three mainstream datasets, demonstrating reasonable generalization capability, particularly strong performance on the more challenging Text-to-Image direction.\n4.\tThe theoretical analysis section proves hash loss convergence to triplet loss, providing theoretical support for the method's effectiveness. This combination of theory and practice is commendable."}, "weaknesses": {"value": "1.\tThe encoding tree construction relies on initial feature representation quality. The paper uses features extracted from pre-trained models but does not discuss whether structural entropy optimization can still effectively recover hierarchical relationships when semantic structure in the initial feature space is unclear.\n2.\tThe proxy sample construction uses simple feature averaging, assuming neighbors have similar semantics, but semantic similarity among neighbors may vary significantly across different levels of the encoding tree. Have you considered weighted aggregation based on hierarchy or distance?\n3.\tThe encoding tree is constructed statically. While the appendix mentions dynamic update experiments, the explanation for why a static tree suffices is relatively simple.\n4.\tThe paper limits the method to unsupervised scenarios, but partial annotations often exist in practical applications. How HINT could be extended to semi-supervised settings, or how to leverage limited annotation information to improve encoding tree construction, deserves further consideration."}, "questions": {"value": "1.\tDuring the modality mixup stage, both m same and m cross proxies are generated. Could you explain why the combination of these two proxies is needed, rather than directly using cross-modal neighbors from the encoding tree to generate target hash codes?\n2.\tFor the Text-to-Image retrieval task, the performance improvement is more pronounced compared to Image-to-Text. Could you analyze the reasons for this asymmetry from the method design perspective? Is it related to certain characteristics of text features compared to image features?\n3.\tThe proxy-based design is reminiscent of prototype learning. Could you discuss the similarities and differences between proxies in HINT and prototypes in prototype learning, both conceptually and functionally?\n4.\tRegarding future work, the paper mentions extending to more modalities such as audio and video. What new challenges would encoding tree construction face in multi-modal scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QYXHWuSIOQ", "forum": "Gq7mjFEoDm", "replyto": "Gq7mjFEoDm", "signatures": ["ICLR.cc/2026/Conference/Submission16096/Reviewer_Y8zT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16096/Reviewer_Y8zT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799158435, "cdate": 1761799158435, "tmdate": 1762926276267, "mdate": 1762926276267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an unsupervised cross-modal hashing method, HINT, for efficient retrieval. It addresses the problem that existing methods often overlook the inherent hierarchical semantic structure of data and face difficulties in directly aligning different modalities. HINT constructs a hierarchical encoding tree guided by structural entropy to capture local semantic communities. It introduces a curriculum-based modality mixup mechanism using proxy samples generated from the tree to achieve progressive modal alignment. It employs a consistency learning objective to align the global semantic distributions between modalities. Experiments on three benchmark datasets demonstrate that HINT outperforms state-of-the-art methods."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The motivation is clear. The paper accurately identifies a key problem in unsupervised cross-modal hashing: the lack of hierarchical semantic modeling and the difficulty of direct modal alignment.\n2. Using a hierarchical encoding tree to mine the semantic structure of cross-modal data is an insightful contribution.\n3. The use of structural entropy to guide the tree construction is a principled and technically sound approach.\n4. The introduction of proxy samples is a smart way to provide more robust signals for hash learning by smoothing out potential noise from individual samples through leveraging local semantic communities.\n5. The paper is well-structured and easy to follow. The narrative flows logically from problem definition to methodology and experimental validation. The figures are helpful."}, "weaknesses": {"value": "1. The construction of the hierarchical encoding tree relies on an initial KNN graph, which could be sensitive to noise or data sparsity.\n2. Proxy samples are constructed by averaging neighbors. It might be interesting to discuss whether other aggregation strategies, such as weighted averaging or an attention mechanism, could yield further improvements.\n3. The encoding tree is constructed once before training. While efficient, a discussion on the potential limitations of this static structure when dealing with dynamic or streaming data would be beneficial.\n4. The evolution of the $\\lambda$ value in modality mixup is interesting. A deeper analysis of what factors drive this specific convergence pattern would offer more profound insights."}, "questions": {"value": "1. How is the number of neighbors determined for proxy sample construction? \n2. The structural entropy minimization process is greedy. Is the greedy approach practically sufficient to get close to a global optimum?\n3. The curriculum learning schedule seems to be determined automatically. Do you think, for certain tasks, it would be possible to introduce some form of manual control to guide this learning process?\n4. For future work, do you think integrating knowledge from pre-trained vlm into the construction of the hierarchical encoding tree would be a promising direction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "o9BrLd5iWu", "forum": "Gq7mjFEoDm", "replyto": "Gq7mjFEoDm", "signatures": ["ICLR.cc/2026/Conference/Submission16096/Reviewer_kUAR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16096/Reviewer_kUAR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836811096, "cdate": 1761836811096, "tmdate": 1762926275795, "mdate": 1762926275795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel unsupervised cross-modal retrieval framework named HINT (Hierarchical Encoding Tree with Modality Mixup). Specifically, HINT constructs a cross-modal encoding tree guided by hierarchical structural entropy, which organizes visual and textual representations into hierarchical communities. Each node in the encoding tree captures local semantic relations, while the overall tree structure preserves global semantic hierarchy. Based on this tree, the method synthesizes proxy samples for both modalities through a modality mixup strategy, enabling progressive alignment via curriculum learning. Experimental results show the proposed method can achieve good results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1) The manuscript is well organized and easy to follow. The motivation, methodological design, and experimental setup are clearly presented, making the overall contribution understandable and coherent.\n\n2) The proposed HINT framework achieves good and consistent results across multiple cross-modal retrieval benchmarks. \n\n3) The appendix provides detailed additional analyses, including implementation details and supplementary experiments."}, "weaknesses": {"value": "1) The contribution section uses overly strong and promotional language (e.g., “New Perspective,” “Coherent Framework,” “Outstanding Performance”), which is not fully justified by the presented methodology or experimental evidence. The proposed approach is conceptually sound, but the degree of novelty and improvement appears incremental rather than fundamentally transformative. The authors are encouraged to adopt a more objective tone and support such claims with stronger quantitative and qualitative evidence.\n\n2)  The paper states that connecting the encoding tree with cross-modal hashing offers a new perspective. However, the idea of representing cross-modal relations in a hierarchical structure is not entirely new and has been discussed in previous studies (e.g., [ref1]) on hierarchical representation learning and cross-modal graph encoding. The proposed method mainly extends existing hierarchical modeling techniques rather than introducing a fundamentally different formulation or conceptual insight.\n\n3) The paper only provides a brief time cost analysis in Table 4, 5 , without comparing the retrieval efficiency with existing cross-modal hashing methods (only 3). Moreover, as hashing-based models are typically valued for their efficiency, the absence of parameter-scale or computational complexity comparisons (e.g., model size, FLOPs, or training cost) weakens the empirical completeness of the work. \n\n4) The comparative methods used in the experiments appear to be outdated, with most baselines coming from earlier studies (mainly up to 2023 or before). Recent advances in cross-modal hashing and retrieval from 2025 are not included.\n\n5) The proposed modality mixup simply performs a linear interpolation between same-modality and cross-modality proxy samples. This operation lacks theoretical grounding on why such a linear combination can lead to better cross-modal alignment. Moreover, the parameter λ is only heuristically adjusted during training, and the method does not ensure semantic consistency when mixing features from heterogeneous modalities. As a result, the mixup process may blur modality-specific information rather than truly enhancing cross-modal representation learning.\n\n6) While the paper presents a well-structured framework, its overall novelty appears limited. Most components—such as hierarchical encoding, proxy construction, and mixup-based alignment—are adaptations or recombinations of existing techniques.\n\n\n[ref1]:  Jin W, Zhao Z, Zhang P, et al. Hierarchical cross-modal graph consistency learning for video-text retrieval[C]//Proceedings of the 44th International ACM SIGIR Conference on research and development in information retrieval. 2021: 1114-1124."}, "questions": {"value": "See the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OI0YoSQy4A", "forum": "Gq7mjFEoDm", "replyto": "Gq7mjFEoDm", "signatures": ["ICLR.cc/2026/Conference/Submission16096/Reviewer_qurP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16096/Reviewer_qurP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16096/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970964491, "cdate": 1761970964491, "tmdate": 1762926275232, "mdate": 1762926275232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}