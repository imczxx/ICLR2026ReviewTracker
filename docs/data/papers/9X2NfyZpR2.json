{"id": "9X2NfyZpR2", "number": 19797, "cdate": 1758299416983, "mdate": 1759897018868, "content": {"title": "Long-Term Action Anticipation via Transcript-based Supervision", "abstract": "Long-Term Anticipation (LTA) from video is a crucial task in computer vision, with significant implications for human-machine interaction, robotics,  and beyond. However, to date, it has been tackled exclusively in a fully supervised manner, by relying on dense frame-level annotations that hinder scalability and limit real-world applicability. To address this limitation, we introduce TbLTA (Transcript-based LTA), the first weakly-supervised approach for LTA, which relies solely on video transcripts during training. This high-level semantic supervision provides the narrative temporal structure that can guide the model toward understanding the relationships between events over time. Our model is built on an encoder-decoder architecture, which is trained using dense pseudo-labels generated by a temporal alignment module to supervise the predictions of both the segmentation head and the anticipation decoder. In addition, the video transcript itself is also used for 1) enhancing video features by contextually grounding them through cross-modal attention, 2) supplying a more global supervision to the model action segmentation predictions over the full video, which in turn helps to provide a better contextualized representation to the anticipation decoder. Through experiments on the Breakfast, 50Salads, and EGTEA benchmarks, we demonstrate that transcript-based supervision offers a very robust and less costly alternative to its fully supervised counterpart for the LTA task.", "tldr": "Long-Term Action Anticipation via Transcript-based Supervision", "keywords": ["Weakly-Supervised Learning", "Long-term Action Anticipation", "Language-based supervision", "Video Understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f2db035450ede89ca26d7233732ebd4fb77fd2d6.pdf", "supplementary_material": "/attachment/9e17502bc92afa44a3622ad89a38457bd806cbe8.pdf"}, "replies": [{"content": {"summary": {"value": "This paper presents TbLTA, a transcript-based weakly supervised framework for long-term action anticipation (LTA). The method eliminates dense frame-level annotations by aligning ordered transcripts with video frames using a temporal alignment module (ATBA) to generate pseudo-labels. It combines multiple objectives—alignment, segmentation (CTC), and anticipation (CRF and duration losses)—and leverages cross-modal attention between transcript embeddings and video features for contextual grounding. TbLTA achieves good performance on Breakfast, 50Salads, and EGTEA benchmarks, approaching supervised baselines despite using action-order supervision only."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "S1. First weakly supervised LTA framework: The paper introduces the first transcript-based approach for dense long-term anticipation, addressing scalability issues of fully supervised LTA.\n\nS2. Competitive results: TbLTA achieves strong performance without frame-level annotations, particularly on Breakfast, comparable to fully supervised methods."}, "weaknesses": {"value": "W1. Use of transcripts at inference: The method assumes transcript availability during inference (Sec. 3.1, L196–200), which raises concerns about practicality since ground-truth transcripts are typically unavailable at inference time in real-world settings. This assumption’s realism is questionable and should be clearly justified or relaxed.\n\nW2. Limited technical novelty: Most components (ATBA, CTC, CRF, duration loss) are adopted from prior works and combined as weakly supervised LTA rather than introducing fundamentally new modeling ideas.\n\nW3. Overly complex design: The framework introduces eight loss terms (3, 1, 4 for alignment, TAS, and LTA), multiple heuristics (e.g., affinity prior in duration loss), and multi-stage training, raising concerns about reproducibility, stability, and generalization. Sensitivity analysis on hyperparameters (λ₁–₃) is missing.\n\nW4. Missing validation of multi-stage training: The paper does not empirically verify whether the progressive training schedule contributes to the final performance."}, "questions": {"value": "Q1. The term “transcript” may be confused with ASR transcripts. Would a different term improve clarity?\n\nQ2. Regarding W1, How is cross-attention with transcript embeddings computed during inference, when no ground-truth transcript embedding is available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MdBMcuiVzF", "forum": "9X2NfyZpR2", "replyto": "9X2NfyZpR2", "signatures": ["ICLR.cc/2026/Conference/Submission19797/Reviewer_i3xn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19797/Reviewer_i3xn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567812336, "cdate": 1761567812336, "tmdate": 1762931646405, "mdate": 1762931646405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "• The author proposed TbLTA as the first weakly-supervised approach for dense Long-Term Action Anticipation (LTA), utilizing only video transcripts that ordered action lists without timing to overcome the scalability issues associated with costly dense frame-level annotations.\n\n• The model uses an encoder-decoder architecture that leverages a weakly-supervised temporal alignment module to generate dense pseudo-labels, providing crucial frame-level supervision for training the segmentation head and anticipation decoder.\n\n• Transcripts are further exploited as semantic context to enrich video features via a local cross-modal attention mechanism and supply robust global supervision to the action segmentation predictions using a Connectionist Temporal Classification (CTC) loss.\n\n• TbLTA establishes the pioneer transcript-only supervision baseline for LTA on benchmarks like Breakfast and 50Salads, achieving performance competitive with fully supervised methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The work introduces a weakly-supervised framework for the dense LTA task, training exclusively on ordered action lists without temporal information. This eliminates the need for expensive frame-level boundary annotations for LTA benchmarks.\n\n\n- The weakly supervised model achieves performance that is competitive with fully supervised methods on the Breakfast dataset, showing its ability to capture procedural regularities.\n\n\n- The architecture incorporates transcripts for various supervision: \n(1) Temporal alignment module generates frame-level pseudo-labels for dense supervision.\n(2) CTC loss enforces sequence consistency, which is crucial for stabilizing pseudo-labels and preventing error propagation.\n(3) Cross-modal attention layer leverages transcript semantics to contextually enrich video features."}, "weaknesses": {"value": "1. **Justification of Weak Supervision:** The paper claims to address a major limitation, stating that Long-Term Action Anticipation (LTA) has been tackled exclusively in a fully supervised manner and that highly granular labeling is costly and difficult to scale. However, the relevance of this weakly-supervised setting is also tackled by relevant contemporary work, PALM [1], which already addresses action anticipation without time annotation, using a different form of weak supervision, a few-shot setting. The paper's claim that its approach is the \"first fully weakly-supervised framework for dense LTA\" must be rigorously justified against such contemporary models, especially if they operate in a shot setting without explicit time boundaries, which aligns to reduce the annotation burden.\n\n\n2. **Exclusion of LTA Benchmarks:** The evaluation is limited to established procedural activity datasets: Breakfast, 50Salads, and EGTEA. The paper does not include experiments on the Ego4D dataset, which is one of the most well-known LTA datasets. Since Ego4D often involves complex, first-person (egocentric) interactions and activities, excluding it raises questions about the framework's scalability and generalizability to diverse, modern LTA settings, especially those relying on egocentric video where action boundaries might be less distinct than in procedural cooking videos.\n\n3. **Lack of Conceptual Advancement and Incremental Methodology:** The methodology appears to be a robust assembly of existing techniques, potentially lacking fundamental new conceptual insights:\n\n    (a) Architecture: The model is a transformer-based encoder-decoder architecture, similar to paradigms found in prior supervised LTA work like FUTR and ANTICIPATR.\n\n    (b) Alignment: The core weakly-supervised component, the temporal alignment module, adopts the ATBA module proposed in Xu & Zheng (2024).\n\n    (c) Losses: It utilizes well-established sequence-to-sequence learning objectives like the Connectionist Temporal Classification (CTC) loss (pioneered for segmentation by Huang et al. (2016)) and a Conditional Random Field (CRF) loss (adapted from Maté & Dimiccoli (2024)).\n\n\n\n[1] Kim, Sanghwan, et al. \"Palm: Predicting actions through language models.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024."}, "questions": {"value": "1. Please address the questions raised in the weakness section.\n\n\n2. The paper explicitly identifies that an accurate prediction of action durations is still a challenge, especially for unseen actions. Given that the affinity-based duration loss **($L_{dur}$)** is trained without temporal ground truth and relies on momentum-based class-wise prior estimates derived from predicted labels, how robust is this self-supervised mechanism when the actual duration of an anticipated action significantly deviates from the implicitly learned temporal statistics captured by the buffer?\n\n\n3. TbLTA’s success relies on generating frame-level pseudo-labels via a temporal alignment module. The CTC loss is critical as it stabilizes these pseudo-labels and prevents error accumulation. Since performance on datasets like 50Salads suffers because denser action distributions and frequent transitions amplify the impact of imprecise temporal alignment, how effectively does the CTC loss mitigate the noise and instability inherent in the generated pseudo-labels near action boundaries in dense video sequences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZWwDKECZQH", "forum": "9X2NfyZpR2", "replyto": "9X2NfyZpR2", "signatures": ["ICLR.cc/2026/Conference/Submission19797/Reviewer_6poY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19797/Reviewer_6poY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797840118, "cdate": 1761797840118, "tmdate": 1762931645579, "mdate": 1762931645579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles dense long-term action anticipation (LTA) without frame-level annotations by introducing TbLTA, a transcript-only, weakly supervised framework. To leverage the information from the transcript, the model introduces different loss objectives to align the provided transcript and visual features, which further improves the temporal action segmentation and future action anticipation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The performance is better than existing work in 50Salads and Breakfast datasets. \n2. The ablation study proves the effectiveness of the introduced loss and modules."}, "weaknesses": {"value": "1. The drawn figure is bad. It is not clear for review.  \n2. Though the performance is better, it is not well motivated to have additional transcript information. The purpose of the WS-TAL/LTA task is mainly solving the insufficient label for action localization/anticipation. The introduction of transcript will take more annotation efforts and go against the original purpose.  \n3. The novelty of the proposed method seems very limited; most modules or methods are mainly from previous work, and this paper is just introducing transcript-based supervision for the LTA task, with no additional interesting module proposed.  \n4. It is a conflict between Fig. 2 and the content in line 196; the figure shows there is no transcript for inference, while it has one in the content."}, "questions": {"value": "1. Make the figure clearer and readable.  \n2. Show strong motivation for the introduced transcript.  \n3. The novelty of the proposed method is very limited.  \n4. Consistent between the figure and text content."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "juLa65lBKg", "forum": "9X2NfyZpR2", "replyto": "9X2NfyZpR2", "signatures": ["ICLR.cc/2026/Conference/Submission19797/Reviewer_kytL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19797/Reviewer_kytL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977881381, "cdate": 1761977881381, "tmdate": 1762931644829, "mdate": 1762931644829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TbLTA, a novel weakly supervised framework for long-term action anticipation (LTA) that relies solely on video transcripts instead of dense frame-level annotations. The method uses a temporal alignment module (ATBA) to generate frame-level pseudo-labels from transcripts, which are then used to train an encoder-decoder model with a segmentation head and an anticipation decoder. The model also incorporates cross-modal attention between textual and visual features. Experimental results demonstrate that TbLTA achieves competitive performance to fully supervised baselines on Breakfast, and partially comparable results on 50Salads and EGTEA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear motivation:** This is the first work to address long-term action anticipation in a weakly supervised setting, which can avoid labor-intensive frame-level annotations.\n2. **Tailored objective design:** The integration of multiple loss components (alignment, CTC, CRF, and duration loss) is tailored for weakly supervised scenario. The ablation studies confirm the benefit of each component, particularly the CRF loss, which contributes notable improvements (5.3%p in 50Salads, 4.1%p in Breakfast).\n3. **Reasonable empirical validation**: Despite relying only on transcript-level ground truths, it achieves comparable performance with supervised fine-tuning methods (overall in Breakfast, partly in 50Salads and EGTEA)."}, "weaknesses": {"value": "1. **Strong reliance on ATBA pseudo-labels**: The most critical limitation of this paper is that the overall framework heavily depends on the quality of pseudo-labels generated by the ATBA alignment module. Since this component originates from prior work on weakly supervised action segmentation, much of the supervision signal in TbLTA is inherited from ATBA. This dependency somewhat limits the originality and isolates less of the contribution to the proposed anticipation framework itself.\n2. **Limited exploration of supervision quality**: The paper could better analyze the effect of pseudo-label noise or compare against an oracle using ground-truth labels for training. Such experiments would help contextualize the achievable upper bound and clarify how much performance is lost due to weak supervision. Without this, it is difficult to assess whether the method’s gains stem from the framework design or from the pseudo-label quality."}, "questions": {"value": "1. Dependence on ATBA: The method relies heavily on ATBA to generate pseudo-labels. Could the authors clarify how sensitive TbLTA’s performance is to errors in ATBA? For example, what happens if the alignment quality degrades? is the model robust to noisy pseudo-labels?\n2. Alternative alignment strategies: Have the authors explored alternative pseudo label generation methods borrowed from weakly-supervised action segmentation methods other than (Xu & Zheng, 2024)?\n3. Oracle comparison: Could the authors provide an oracle experiment where the same model is trained with ground-truth frame labels? This would help quantify the performance gap between weakly and fully supervised setups and reveal how much of the loss stems from label noise versus model design.\n4. Generalization to other weak signals: Could the proposed framework be extended to other weak supervision sources (e.g., narrations or subtitles) beyond transcripts? This would make the approach more generally applicable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Mht56ND1S5", "forum": "9X2NfyZpR2", "replyto": "9X2NfyZpR2", "signatures": ["ICLR.cc/2026/Conference/Submission19797/Reviewer_24ux"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19797/Reviewer_24ux"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995777730, "cdate": 1761995777730, "tmdate": 1762931644257, "mdate": 1762931644257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}