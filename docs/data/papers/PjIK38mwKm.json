{"id": "PjIK38mwKm", "number": 8387, "cdate": 1758081007161, "mdate": 1759897788376, "content": {"title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization", "abstract": "Large Language Model (LLM)-based web agents demonstrate strong performance on knowledge-intensive tasks but are hindered by context window limitations in paradigms like ReAct. Complex queries involving multiple entities, intertwined relationships, and high uncertainty demand extensive search cycles that rapidly exhaust context budgets before reaching solutions. To overcome this challenge, we introduce ReSum, a novel paradigm that enables indefinite exploration through periodic context summarization. ReSum converts growing interaction histories into compact reasoning states, maintaining awareness of prior discoveries while bypassing context constraints. For paradigm adaptation, we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and advantage broadcasting to familiarize agents with summary-conditioned reasoning. Extensive experiments on web agents across three benchmarks demonstrate that ReSum delivers an average absolute improvement of 4.5% over ReAct, with further gains of 8.2% following ReSum-GRPO training. Notably, with only 1K training samples, the ReSum-GRPO-trained 30B model achieves 33.3% Pass@1 on BrowseComp-zh and 18.3% on BrowseComp-en, surpassing most open-source web agents.", "tldr": "We propose the ReSum paradigm to overcome context limitations in web agents, along with a tailored ReSum-GRPO algorithm for paradigm adaptation.", "keywords": ["web agent", "search agent", "reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8388ef6397f296a5b98b1def7485be7cdd04a4b6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work focuses on web agents for information-seeking tasks and introduces the ReSum framework to address the context constraint problem that inevitably arises in long-horizon explorations. The framework periodically summarizes ongoing conversations into structured summaries. The summarization process aims to extract key clues and evidence from lengthy interactions, identify information gaps, and highlight next-step directions.\nTo enable this effectively, this work presents ReSumTool-30B, a summarization model designed for web-context interactions, and ReSum-GRPO, an algorithm that allows the agent to operate more effectively based on the summarized context."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- **Potential for small-scale models:** The proposed ReSum framework demonstrates the possibility of building effective web agents even with relatively small base models that have limited context windows, through the mechanism of context compression.\n\n- **Introduction of ReSumTool-30B:** This paper proposes a web-context summarization model that performs comparably well with a relatively modest parameter size (30B), suggesting its efficiency and practicality.\n\n- **Empirical findings from ReSum-GRPO:** The experiments related to ReSum-GRPO show that 1) agents perform significantly better when they learn how to utilize summarized contexts, and 2) ReSum-GRPO serves as a more effective reinforcement learning strategy compared to standard RL approaches."}, "weaknesses": {"value": "While the proposed ReSum framework forms the core of this paper, it raises several conceptual and methodological concerns:\n\n- **Lack of discussion on the distinction from world models:** This paper defines the goal of summarization as extracting key clues and evidence, identifying information gaps, and highlighting next-step directions (lines 79–80). Conceptually, this process 1) converts the interaction history into a structured format more interpretable by LLMs and 2) provides cues for the agent’s subsequent actions. In this sense, **I think ReSum’s summarization mechanism is highly similar to the World Model [1] specialized for information-seeking tasks.** Similarly, world models simulate the next observation from previous trajectories, thereby **offering structured summary and next-step guidance to the LLM.** The primary difference lies in the output format adapted to the specific task, while the underlying methodology remains fundamentally aligned. Consequently, I believe the paper would benefit from **a clearer articulation of the similarities and differences between ReSum and existing world-model approaches,** which is essential to substantiate the claimed novelty of the ReSum framework.\n\n- **Inference cost concerns:** As shown in Figure 8 of Appendix H.2, the ReSum framework incurs an increased inference cost. However, this overhead may undermine the claimed efficiency of achieving strong performance with smaller models. If a larger model (e.g., GPT-OSS in Table 1) achieves higher performance at a lower overall inference cost than the ReSum-enhanced smaller model, **the advantage of ReSum diminishes considerably.** To address this concern, the paper **should include comparative inference-cost analyses across different base models,** highlighting whether the performance gains justify the additional computational expense."}, "questions": {"value": "- Do you believe that the ReSum framework would remain effective for agents built upon base models with sufficiently large context windows? While I acknowledge that ReSum’s primary objective is context compression, prior works [2][3] have shown that converting context into structured representations can improve reasoning and planning even in models with larger context capacity. It would therefore be valuable to examine whether ReSum yields similar benefits under such settings.\n\n\n- In Table 1, what are the specific differences between the ReAct and Recent History baselines in terms of setup? A more detailed explanation of these configurations would clarify how the comparison was conducted.\n\n\n- Could you provide additional results comparing inference cost and performance across different base models (e.g., GPT-OSS and others)? \n\n[1] Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation; Chae et al.; ICLR 2025\n\n[2] Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models ; Yu et al.; EMNLP 2024\n\n[3] Improving Retrieval Augmented Language Model with Self-Reasoning; Xia et al.; AAAI 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Max38zldgp", "forum": "PjIK38mwKm", "replyto": "PjIK38mwKm", "signatures": ["ICLR.cc/2026/Conference/Submission8387/Reviewer_Je2p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8387/Reviewer_Je2p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723754124, "cdate": 1761723754124, "tmdate": 1762920290913, "mdate": 1762920290913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ReSum, a simple extension of ReAct for reasoning-then-act agents on long-horizon agentic tasks. In ReSum, the history of agent reasoning, action, and observations in previous iterations are periodically summarized into key takeaways and information gaps to recombine with the original query prompt, to avoid the agent history from running out of the context window of typical LLMs.\n\nThe paper also presents ReSumTool-30B, a smaller, specialized summarization model distilled from larger models that helps ReSum achieve good performance without the inference cost that comes with larger models. Furthermore, when GRPO is applied to ReSum trajectories (ReSum-GRPO), empirical results show further gains in agent performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a relatively simple yet effective solution to the problem of ReAct agent context overflow by summerizing past agent reasoning, actions, and observations in the agent history. The solution is well-motivated and clearly explained.\n2. The paper presents extensive experimentation with empirical results that demonstrates ReSum achieving gains over ReAct, as well as simpler context-fitting adaptations of ReAct such as agents that truncate distant history to only retain the most recent reasoning, actions, and observations (\"Recent History\")."}, "weaknesses": {"value": "1. The paper makes unsubstantiated or inaccurate claims about the proposed approach in a few places. For instance,\n\n    1. On L261, \"Unlike most agentic RL approaches (Liu et al., 2025; Dong et al., 2025) that impose format rewards, ours relies solely on answer correctness to provide a more result-oriented signal. Additionally, we perform format checks at each generation step: if the agent fails to adhere to specific tokens such as <think> </think>, the entire trajectory is terminated and assigned a zero reward as a penalty. This implicitly guides the agent to follow the required format effectively.\" What the stated approach is doing is precisely a form of format rewards, just instead of an additive reward, the format reward imposes a nullifying penalty to prevent any partial credits unless formatting requirements are met.\n    2. On L291, one of the stated benefits of ReSum-GRPO is \"strategic information gathering to collect evidence that yields high-quality summaries\". This has not been empirically verified by experiments.\n    3. On L366, the paper states \"ReSum paradigm consistently outperforms ReAct due to extended exploration opportunities.\" This is not substantiated by comparing success rate of long trajectories vs shorter ones, for instance.\n\n2. Questionable evaluation and comparison.\n\n    1. The main evaluation performed in the paper uses LLM as a judge (L310: \"We consistently use Qwen2.5-72B-Instruct as the scoring model to assess whether the predicted answer aligns with the ground truth.\"), but there has never been any evaluation of this evaluation method itself to make sure it is correct. Some benchmarks, take GAIA, for instance, come with official evaluation functions. It is unclear why the paper adopts an ad hoc evaluation metric when official ones are available for better comparability across the literature.\n    2. The only comparison to prior approaches of intelligent agent context compaction is in Appendix F, and only one prior work (MEM1) is compared. For a better evaluation of the contributions of the proposed approach, this should have been more comprehensive and be part of the main body of the paper.\n    3. In GRPO experiments, the paper leaves out \"Recent History\" as a potential compaction approach.\n    4. The paper shows ReAct performance of many leading closed- and open-weights models, but never apply ReSum to these agents. Is it because ReSum is not effective when the base model is strong enough? (which would also be useful to know)\n\n3. Contradicting statements and misrepresented results.\n\n    1. On Line 725, the paper states \"This design (ReSum) preserves the simplicity and efficiency of ReAct while ensuring seamless compatibility with existing agents.\", while on Line 236 it concedes \"This pattern (of summarized history) is out-of-distribution for standard agents, as they have not encountered summary-conditioned reasoning during training.\" While empirically we see that LLMs are able to make use of the summarized history in most cases, one of these statements is less accurate than the other.\n    2. On Line 399, the paper states \"ReSum integration effectively narrows the performance gap to leading pre-trained models.\" This conveniently leaves out the significantly superior performance of OpenAI o3 and DeepSeek v3.1, and also does not answer the question of where the performance gap might be.\n    3. In Table 1, the authors use bold fonts to indicate results from the proposed approach rather than the conventional best results (even if within the same model size class), which is potentially misleading to the casual reader."}, "questions": {"value": "1. What's the objective of the summarization, is there any evaluation of summary quality aside from prompt specification of the behavior of the teacher model itself?\n2. L253 states \"Each segment H(i) forms an individual training episode with input q(i−1) and output ($\\tau_{t-1}$, ...$a_{t_i}$).\" How is the loss calculated, is the model also updated to predict the $o_t$'s?\n3. What is the variance of the performance of the agent across independent reruns? Are the differences between the systems statistically significant / meaningful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RGzIZDIVKy", "forum": "PjIK38mwKm", "replyto": "PjIK38mwKm", "signatures": ["ICLR.cc/2026/Conference/Submission8387/Reviewer_FmG8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8387/Reviewer_FmG8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892637782, "cdate": 1761892637782, "tmdate": 1762920290455, "mdate": 1762920290455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors are trying to address a critical challenge of LLM-based agents with paradigms like ReAct, i.e., the fixed context window. The growing history of thoughts, actions, and observations quickly exhausts the available context, forcing premature termination before a solution is found. To solve this problem, in this paper, a method called ReSum is proposed, which enables indefinite exploration through periodic context summarization. A summarization tool ReSumTool-30B is proposed to extract key evidence and identify critical information gaps from lengthy interactions. The paper further presents the ReSum-GRPO reinforcement learning algorithm to adapt agents to summary-conditioned reasoning and demonstrate, across three established benchmarks, superior performance over ReAct."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written with good structure and is easy to follow. It presents a clear, practical, and effective solution to a significant and practical challenge in agentic AI.\n\n2. The ReSum-GRPO algorithm is a novel adaptation of RL. The concepts of trajectory segmentation and advantage broadcasting are well-suited for this specific problem and avoid the high cost of expert-annotated SFT data. addresses the distribution shift of summary-conditioned inputs by segmenting rollouts and broadcasting trajectory-level advantages, a reasonable and well-motivated modification that preserves GRPO’s structure while handling long trajectories.\n\n3. The proposed framework achieves good performance over other baselines. The proposed method is evaluated across multiple agent scales, benchmarks, and both training-free and RL settings. The authors also provide resource/cost vs. performance analyses and case studies showing qualitative behaviour. This breadth strengthens the empirical claim."}, "weaknesses": {"value": "1. In my opinion, the triggering strategy is rule-based and limited by the \"naive\" trigger, e.g., approaching the token budget. This is simple but may not be optimal, i.e., may encounter the case that is in the middle of a complex chain?\n\n2. As the proposed method follows a periodic context summarization and the entire paradigm depends on the quality of this summary, I wonder if this will include the risk of error propagation. For example, some crucial information can be lost and making the task really hard to solve.\n\n3. Although the paper effectively analyzes total token consumption and training time, it does not fully address the inference-time latency of the summarization step itself. Each call to ReSumTool-30B is an additional LLM call that pauses the agent's reasoning. For a long task requiring many summaries, this could add significant latency. A more detailed analysis of this latency would be valuable.\n\n4. The paper provides a high performance summary model, ReSumTool-30B. This seems like a big model for this task. However, only 1K training samples can make it effective. I wonder if the balance of these setups could be further studied."}, "questions": {"value": "Please check the questions in the Weaknesses, and also address the following questions.\n\n1. I wonder if more triggering strategies can be included for testing. Also, if the trigger frequency changes, will the performance also change a lot?\n\n2. Since summarization can abstract away details, how does the agent mitigate the risk of introducing inaccuracies or hallucinations in ongoing reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KXBwwG1GXj", "forum": "PjIK38mwKm", "replyto": "PjIK38mwKm", "signatures": ["ICLR.cc/2026/Conference/Submission8387/Reviewer_oXox"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8387/Reviewer_oXox"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939942205, "cdate": 1761939942205, "tmdate": 1762920290088, "mdate": 1762920290088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose ReSum; a framework that summarizes interaction history; proposing to train their own summarizer, then train an LLM using a modified variant of GRPO to be use to this new format of summary and then further interaction history, as opposed to standard ReACT. Experiments on three web search benchmarks (GAIA, BrowseComp-zh, BrowseComp-en) using WebSailor models (3B–30B) show that ReSum improves Pass@1 by ~4.5% (absolute) over standard ReAct, and applying ReSum-GRPO training yields a further ~8% gain."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper tackles an important problem that is highly relevant to the ML community, of how to leverage summaries when executing long-horizon LLM agent tasks.\n* Empirical performance of ReSum suggests such an approach is superior compared to ReAct; however the results lack error bars to say anything conclusive.\n* Paper is clearly written and easy to follow.\n* Plug-and-play: the change to ReAct is minimal (periodic summarization and resume), which makes it easy to adopt across agents.\n* Results are reported on three relevant web-agent benchmarks with consistent tooling; inclusion of a MEM1 reproduction is useful context."}, "weaknesses": {"value": "* Improvement of 4.5% over ReAct, could still be within the error bars. I encourage the authors to re-run their experiments for multiple random seeds and quote their error bars.\n* Figure 4; statement of “ReSum-GRPO demonstrates higher initial rewards and faster convergence compared to standard GRPO.”, appears to be an overclaim as sub figure b, WebSailor-30B, shows that the final reward and results for GRPO vs. ReSum-GRPO consistently overlap. It would be informative to produce this plots with error bars running this experiment over multiple random seeds to better quantify these claims, otherwise such claims should be weakened.\n* Limited novelty / missing citations: The paper’s core idea of compressing conversation context via summarization is an intuitive solution that has been explored by others (, Mem1 (Zhou et al., 2025b) and MemAgent (Yu et al., 2025a)). The authors do cite these, but portray ReSum as “lightweight” in comparison. In truth, the difference is incremental: ReSum outsources summarization to an external model instead of learning memory tokens, but the goal and even the use of RL adaptation are similar. Overall, the contribution feels more like a specific implementation of a growing trend (agentic summarization for long contexts) rather than a pioneering breakthrough. The authors should clarify explicitly what is novel here relative to prior context-management approaches (e.g. is the main novelty just the use of a fixed summary tool and minor RL tweak?). Currently, the work comes across as incremental.\n* The “surpassing most open-source web agents” statement in the abstract is too strong without broader, up-to-date comparisons (and with CIs). Please either temper this claim or add head-to-head results versus the strongest recent open agents under matched settings.\n* Evaluation relies on an LLM-as-judge signal (also used for RL reward). Please report calibration/human spot-checks or agreement rates, and quantify sensitivity to the chosen judge model.\n* Summarizer quality is only evaluated indirectly via downstream Pass@k. Include an intrinsic summary evaluation (coverage of key facts, omission rate) or at least failure analyses where a dropped detail changes the final answer."}, "questions": {"value": "* “Triggers for summarization can be systematic, e.g., exceeding a token budget or reaching a round limit, or agent-initiated, where the policy model decides to summarize for effective context management.”, can the authors provide an ablation on agent-initiated summarization?\n* How do you ensure when training the summarizer it will capture the correct long-distance information; and not just learn to compress to keep information/facts helpful for the final evaluated task reward? That is learning to attend and keep information, a form of long horizon credit assignment, information that is vital to a later state, however not immediately necessary for the subsequent states, for example a password to unlock a door at the end of a maze.\n* Novelty vs prior art: Can you clarify precisely what is novel in ReSum compared to earlier long-horizon agent frameworks? For example, Mem1 and MemAgent already enable agents to condense context (albeit via learned internal memory) , and concurrent works use similar summarization or memory editing strategies . Which aspect of ReSum’s paradigm or implementation do you consider unique? It would help to explicitly acknowledge overlapping ideas and highlight any distinctive contribution (if any) beyond a “plug-and-play” implementation.\n* Please provide error bars (or bootstrap CIs) for all key metrics; and report the number of seeds per configuration. Without this, several improvements could plausibly be within variance.\n* How sensitive is performance to the summarization frequency/threshold? An ablation sweeping the trigger threshold (or a learned trigger) would help understand when summarization helps vs. hurts.\n* For RL: what fails if one trains with standard GRPO directly under the ReSum prompt format (no “advantage broadcasting”)? A small ablation would justify the need for ReSum-GRPO beyond the inference paradigm itself."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Mn6NPFMGvh", "forum": "PjIK38mwKm", "replyto": "PjIK38mwKm", "signatures": ["ICLR.cc/2026/Conference/Submission8387/Reviewer_SSow"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8387/Reviewer_SSow"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969709529, "cdate": 1761969709529, "tmdate": 1762920288916, "mdate": 1762920288916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}