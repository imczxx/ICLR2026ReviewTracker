{"id": "dVlkUtsyg7", "number": 21735, "cdate": 1758321093128, "mdate": 1759896906072, "content": {"title": "Improving Feasibility via Fast Autoencoder-Based Projections", "abstract": "Enforcing complex (e.g., nonconvex) operational constraints is a critical challenge in real-world learning and control systems. However, existing methods struggle to efficiently and reliably enforce general classes of constraints. To address this, we propose a novel data-driven amortized approach that uses a trained autoencoder as an approximate projector to provide fast corrections to infeasible predictions. Specifically, we train an autoencoder using an adversarial objective to learn a structured, convex latent representation of the feasible set, enabling rapid correction of neural network outputs by projecting them onto a simple convex shape before decoding into the original feasible set. We test our approach on a diverse suite of constrained optimization and reinforcement learning problems with challenging nonconvex constraints. Results show that our method effectively improves constraint satisfaction at a low computational cost, offering a practical alternative to expensive feasibility correction techniques based on traditional solvers.", "tldr": "We propose a data-driven amortized approach that uses a trained autoencoder as an approximate projector to provide fast corrections to infeasible predictions.", "keywords": ["amortized optimization", "nonconvex optimization", "surrogate models", "feasibility"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87dbc1d6357884621d9f69be93ac57a0082d81a7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a FAB projection method, which encodes a prediction and context into a latent space, projects onto a simple convex set ùëÜ, and decodes back to the original space. A two-phase training is used. Phase 1 reconstructs the feasible set from feasible samples, while Phase 2 structures the latent via a discriminator with a hinge loss, plus a latent loss and a Jacobian regularizer. FAB is then attached as a plug-and-play mapping ùúô around a base network."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The main contribution is an amortized, non-iterative feasibility mapper that is fast at inference and empirically effective on diverse nonconvex toy sets. \n2. Its plug-and-play design is attractive for deployment, since ùúô can be mounted onto arbitrary predictors without rewriting training loops.\n3. The Safe RL experiment demonstrates the safety and latency advantages."}, "weaknesses": {"value": "1. FAB explicitly does not provide hard feasibility guarantees, making it unsuitable for safety-critical regimes, and no robustness evidence is given for distribution shift or dataset coverage failures that the authors acknowledge as limitations. \n2. Comparisons emphasize feasibility, but optimality gaps vary and many classical baselines (e.g., Projected Gradient, FSNet) can reach 100% feasibility as well.\n3. The experiments focus on toy nonconvex sets rather than realistic, parameter-dependent instances, so the path to real deployments remains unclear."}, "questions": {"value": "Two biggest concerns with L2O methods remain (1) the expensive training cost, and (2) the lack of feasibility guarantees. Can the authors discuss more about these two aspects?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TTEmU7LBUp", "forum": "dVlkUtsyg7", "replyto": "dVlkUtsyg7", "signatures": ["ICLR.cc/2026/Conference/Submission21735/Reviewer_WGVG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21735/Reviewer_WGVG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923720322, "cdate": 1761923720322, "tmdate": 1762941910944, "mdate": 1762941910944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FAB, a fast ‚Äúapproximate projector‚Äù built from a conditional autoencoder. Training has two phases: (1) reconstruct feasible points; (2) use a discriminator + simple penalties to make latent samples decode to feasible outputs. Experiments on several synthetic constraint families and one safe-RL task look promising."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Easy to attach after an existing predictor to improve feasibility with negligible latency.\n\n2. The two-phase training architecture is clear and easy to follow\n\n3. Tables report near-perfect feasibility and big time gains vs. homeomorphic projection and other baselines"}, "weaknesses": {"value": "It would further strengthen the paper if some form of theoretical guarantee‚Äîeven a weak or probabilistic feasibility bound‚Äîcould be established."}, "questions": {"value": "1. Could you report throughput vs. dimension and memory for different decoder widths and number of decoders, and a cost breakdown?\n\n2. Does a FAB trained on one hazard layout transfer to unseen layouts? Any results under domain shift?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8jw3OQJstg", "forum": "dVlkUtsyg7", "replyto": "dVlkUtsyg7", "signatures": ["ICLR.cc/2026/Conference/Submission21735/Reviewer_nF3p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21735/Reviewer_nF3p"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056023853, "cdate": 1762056023853, "tmdate": 1762941910179, "mdate": 1762941910179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Feasibility Autoencoder-Based (FAB) method, which uses an offline-trained autoencoder to project infeasible DNN outputs into a structured latent space for fast feasibility enforcement in constrained optimization. Key contributions include a two-phase training algorithm for the autoencoder, empirical speedups, and demonstrations of high feasibility with near-optimal performance on benchmarks like portfolio optimization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The method's originality lies in its pretrained encoder-decoder as a reusable feasibility projector, combining autoencoders with adversarial structuring for non-convex constraints and showing potential for generalization across tasks.\n\n2. Quality is good, with well-designed algorithms, and good presentation.\n\n3. The studied problem is important, as FAB enables real-time constraint handling in practical ML applications, potentially impacting fields like operations research and safe AI."}, "weaknesses": {"value": "1. The lack of theoretical guarantees, especially on optimality preservation post-projection, weakens reliability, as the method relies on empirical data without bounds on gaps or failure modes. The feasibility and optimality guarantee is not presented and the method itself seems only rely on the performance of the decoder\n\n2. Contributions are vague compared to baselines like homeomorphic projections, which offer low-complexity feasibility guarantees; clearer differentiation (e.g., quantitative edges in non-convex settings) is needed.\n\n3. The rationale of optimality consideration is questionable, please see below."}, "questions": {"value": "1. How does the autoencoder ensure optimality conservation during projection, and can you provide bounds or discuss failure modes?\n\n2. How does training handle multi-lable T_feas (could have multiple y per x)? Will different y harms the model performance as it could be confused on the targeted y.\n\n3. What are FAB's specific advantages over homeomorphic projections (e.g., in non-convexity or generalization), and how could the pretrained encoder-decoder be extended to new domains?\n\n4. The rationale of optimality consideration is questionable, like the decoder only see feaible/infeasible points, given an infeasible but close to optimal point, how could the model reconstruct a good solution with good optimality performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "VaxC74AM0Q", "forum": "dVlkUtsyg7", "replyto": "dVlkUtsyg7", "signatures": ["ICLR.cc/2026/Conference/Submission21735/Reviewer_3AaQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21735/Reviewer_3AaQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762494465148, "cdate": 1762494465148, "tmdate": 1762941909867, "mdate": 1762941909867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}