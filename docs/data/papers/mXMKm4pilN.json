{"id": "mXMKm4pilN", "number": 20768, "cdate": 1758309890222, "mdate": 1763729516344, "content": {"title": "VALM: Variational Autoencoder Language Models for Highly Parallel Text Generation", "abstract": "Autoregressive language models have shown impressive abilities across domains. However, their token-by-token decoding limits inference speed. We introduce Variational Autoencoder Language Models (VALM), a non-autoregressive architecture that predicts entire sequences in parallel from a single global latent, with no denoising or diffusion losses. VALM uses a bidirectional transformer encoder and decoder with an ELBO objective, reducing sequential depth from $\\mathcal{O}(LT)$ to $\\mathcal{O}(L)$ for an $L$-layer network generating $T$ tokens. We train VALM-1, which generates 32 tokens in a single forward pass, demonstrating the applicability of pure VAEs to discrete text and presenting a novel approach to high-throughput language modeling on standard GPUs.", "tldr": "Variational Autoencoders can be fast language models, generating tens of tokens simultaneously", "keywords": ["VAE", "Variational Autoencoder", "LLM", "Single-pass", "text", "parallel decoding", "ELBO", "discrete"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/920eff6af9a109f561a27c59314f2ab9f5ed32e1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This is basically asking - can we making a VAE-like model which instead of the classical token-by-token transformer setup, predicts every token in parallel at once ? Such attempts have been made in general, the paper notes diffusion models but the idea of using a latent to make > 1 prediction steps is not new. The usage of a VAE is somewhat unusual in the context.\n\nTo be honest, I am very skeptical of the idea, but in any case, the idea is not well executed (in my opinion). I detail on this below."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The idea of a fully parallel (for all tokens) generation process with a VAE is somewhat novel, and I like that there are some attempts to look at the scaling law part of it (although it seems to be very randomly inserted)"}, "weaknesses": {"value": "Unfortunately, the paper, in my opinion, suffers from multiple flaws.\n\n1 - On a theoretical level, fully parallel generation may not even be efficient. One can think of infinite sentences that convey the same meaning.  Therefore, latent space that only maps a latent to an idea, and then autoregressively and non-deterministically generates the words to convey that meaning, requires less work that mapping separate latents to all sentences that mean exactly the same thing. Fully parallel generation may stumble in these scenarios, e.g. consider conveying the meaning of \"A is shorter than B\" or \"B is taller than A\". Assuming that these are the only two representations for simplicity, first token + latent that maps to meaning can auto-regressively generate it, but the two statements must have different latents in the fully parallel world.\n\n2 - Beyond the theoretical problems, there is virtually no empirical work. There is no good empirical study on a lot of benchmarks against meaningful models, use of pre-trained encoders or decoders to see if things make sense, no study of latent space characteristics beyond some simple generated sentences..."}, "questions": {"value": "I simply ask for any one summary comparison table against peer methods (you have cited quite a few - and yet - where is the comparison table ?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dveF5EHHBa", "forum": "mXMKm4pilN", "replyto": "mXMKm4pilN", "signatures": ["ICLR.cc/2026/Conference/Submission20768/Reviewer_CNzG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20768/Reviewer_CNzG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632623782, "cdate": 1761632623782, "tmdate": 1762934201529, "mdate": 1762934201529, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We are withdrawing this submission due to insufficient experimental validation and comparison to prior methods. We sincerely thank the reviewers for their time and feedback."}}, "id": "4v0627tdvn", "forum": "mXMKm4pilN", "replyto": "mXMKm4pilN", "signatures": ["ICLR.cc/2026/Conference/Submission20768/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20768/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763729515747, "cdate": 1763729515747, "tmdate": 1763729515747, "mdate": 1763729515747, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Autoregressive language models (LMs) have demonstrated impressive capabilities across various domains. However, their token-by-token decoding inherently limits inference speed. This paper investigates the integration of Variational Autoencoders (VAEs) into a non-autoregressive architecture that predicts entire sentences in parallel from a single global latent variabe. Experimental results provide preliminary evidence of the effectiveness of the proposed approach."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Research on introducing VAEs into encoder-decoder architectures for fast inference (i.e., a single forward pass) has the potential to benefit the community."}, "weaknesses": {"value": "### Methodology:\n\n- Introducing VAEs into encoder-decoder architectures for fast inference is not particularly novel. As mentioned in the related work, VAE-based non-autoregressive models are highly relevant and widely used in machine translation tasks (conditional generation). Unfortunately, the paper does not clearly highlight the differences between the proposed approach and the existing VAE-based non-autoregressive models. From my understanding, the main difference appears to be limited to architectural modifications.\n\n---\n\n### Experiments:\n\n- The paper does not include any baselines, such as standard VAE-based autoregressive LMs, VAE-based non-autoregressive models, or diffusion LMs. Without a comparison to the most relevant VAE-based non-autoregressive models, the evaluation is not convincing. It seems reasonable that they could be adapted for unconditional generation tasks as well. At a minimum, the authors should attempt to include such a baseline [1] and present corresponding results.\n\n- No evaluation metrics commonly used in language modeling and generation [2] are reported. The paper only presents generated sentences, making it difficult to quantitatively assess the performance or effectiveness of the proposed approach.\n\n> **Reproducibility Statement:** We release code, configuration files, and experimental scripts to reproduce training and evaluation as an anonymous repository. The configuration files contain all hyperparameters and random seeds.\n\n- However, I was unable to locate the anonymous repository referenced in the Reproducibility Statement.\n\n[1] Latent-Variable Non-Autoregressive Neural Machine Translation with Deterministic Inference Using a Delta Posterior.\n\n[2] LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces.\n\n---\n\n### Writing:\n\n- The paper is not well written in its current form. While it describes what the authors did, the presentation reads more like a project report than a scientific paper suitable for conference submission. The writing lacks a clear narrative, logical flow, and proper academic tone. I recommend that the authors thoroughly revise the paper to improve its organization, clarity, and coherence.\n\n- Sections 1 and 2 currently lack citations. The authors should incorporate relevant references to prior work to properly situate their study within the broader context of existing literature."}, "questions": {"value": "- Figure 1: Why is a bidirectional decoder used instead of a standard decoder? During training, the task is reconstruction, but it is unclear whether a bidirectional decoder provides any advantage during inference. Could the authors clarify this design choice?\n\n- Equations (4) and others: It would be clearer to use $\\mathbf{x}$ to represent a complete sentence in the loss functions, rather than the scalar $x$."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MhketAkQHk", "forum": "mXMKm4pilN", "replyto": "mXMKm4pilN", "signatures": ["ICLR.cc/2026/Conference/Submission20768/Reviewer_xHKq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20768/Reviewer_xHKq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926221755, "cdate": 1761926221755, "tmdate": 1762934200377, "mdate": 1762934200377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a VAE-based language model named ​VALM (Variational Autoencoder Language Model)​, designed to address the ​high sequential decoding issue​ inherent in traditional Autoregressive Language Models (ARLMs) during inference, which arises from their token-by-token generation process. The core idea of VALM is to apply the ​Variational Autoencoder (VAE)​​ generative paradigm to discrete text, enabling ​parallel, single-pass text generation. \n\nSpecifically, VALM employs a bidirectional Transformer encoder to compress an input sequence into a ​global latent variable $z$. A bidirectional Transformer decoder then uses this single variable to predict logits for all token positions in parallel within a single forward pass.This design also prevents the common \"posterior collapse\" issue in text VAEs.\n\nPaper developed a prototype, ​VALM-1, which demonstrates the ability to generate coherent 32-token spans in a single pass. Preliminary scaling law studies on datasets like TinyStories and WikiText-103 indicate that the model's performance (ELBO loss) improves predictably with increases in both model parameters and data volume, all without exhibiting KL collapse."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The instruction and method part are easy to follow.\n2. The experiment clearly shows that VALM can maintain single-pass coherence within a short span."}, "weaknesses": {"value": "1. The model's architecture is very traditional and lacks innovation.\n2. VALM-1 can only generate fixed-length text of 32 tokens, and its results may not generalize to larger vocabularies, longer contexts, or more diverse domains.\n3. The paper does not systematically compare VALM with autoregressive models of comparable scale, lacking performance benchmarks under the same dataset and evaluation metrics.\n4. The experiments shown in the paper are very preliminary, lacking evidences to show this paradigm is promising on some  on some real-world applications."}, "questions": {"value": "1. The model relies entirely on the latent variable $z$ to establish dependencies between all tokens. How does this design impact its performance in modeling long-range dependencies, and does it imply a fundamental ceiling on the effectiveness of VAEs for language modeling?\n2. The paper primarily relies on qualitative examples to demonstrate coherence, lacking quantitative comparisons against baseline models on standard metrics. Does this omission suggest that the VALM framework faces challenges in being fairly evaluated within established benchmarking paradigms?\n3. Table 3 indicates a relatively weak scaling exponent with respect to token count for WikiText-103 (0.09). Could this suggest that the VALM architecture has inherent limitations in leveraging increased data to improve performance on more complex datasets?\n4. Table 2 and the main text highlight specific errors and limitations in the generated text. What is the hypothesized root cause of these errors? Would increasing the dimensionality of the latent variable $z$ be a viable strategy to mitigate these issues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SEovSjgJZO", "forum": "mXMKm4pilN", "replyto": "mXMKm4pilN", "signatures": ["ICLR.cc/2026/Conference/Submission20768/Reviewer_3Yuy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20768/Reviewer_3Yuy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926839562, "cdate": 1761926839562, "tmdate": 1762934199400, "mdate": 1762934199400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VALM, a variational autoencoder language model that generates text non-autoregressively in a single forward pass using a global latent vector. This design aims to overcome the sequential bottleneck of autoregressive models and demonstrates a prototype implementation (VALM-1) capable of producing short, coherent text sequences."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a novel and conceptually interesting approach, a non-autoregressive VAE for text generation, that challenges the dominant autoregressive paradigm.\n- The prototype implementation demonstrates the feasibility of one-pass text generation and shows that variational methods can be applied effectively to discrete sequences."}, "weaknesses": {"value": "- The evaluation is very limited, relying on small-scale qualitative examples (e.g., short passages from bAbI and TinyStories) without quantitative or comparative benchmarks.\n- The work appears to be at an early, exploratory stage, with limited empirical validation and scope. As such, it might be more appropriate as a position or concept paper rather than a full research contribution."}, "questions": {"value": "See my comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2391DJPo5Y", "forum": "mXMKm4pilN", "replyto": "mXMKm4pilN", "signatures": ["ICLR.cc/2026/Conference/Submission20768/Reviewer_qsMG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20768/Reviewer_qsMG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20768/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927555108, "cdate": 1761927555108, "tmdate": 1762934198066, "mdate": 1762934198066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}