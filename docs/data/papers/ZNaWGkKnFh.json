{"id": "ZNaWGkKnFh", "number": 10670, "cdate": 1758179191864, "mdate": 1759897636364, "content": {"title": "Self-supervised Sparse Vision Concepts for Image Understanding and Reconstruction", "abstract": "Self-supervised vision encoders have become critical components of modern machine learning systems. Despite remarkable advances in image understanding, generation, and multimodal alignment, the underlying representation of visual features has remained largely unchanged, constrained by historical architectures and benchmarks. This reliance on dense feature grids introduces redundancy and limits the integration of understanding and generation. We propose a novel framework that represents images with a small number of sparse tokens via low-rank matrix factorization. While mathematically simple, this formulation effectively disentangles semantic and spatial information. We demonstrate that vision-only self-supervised learning under this framework yields sparse token representations that simultaneously support high-quality image understanding, detailed pixel-level reconstruction, and fine-grained semantic understanding. Together, these results highlight sparse tokens as a promising alternative to dense grids for efficient and versatile visual representation learning.", "tldr": "", "keywords": ["Self-supervised Learning", "Representation Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4958699f30a6e8c5a5bfee6095c3dc01b67fd2e1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a self-supervised visual representation learning framework called STELLAR, which uses low-rank matrix factorization to compress images into a small number of sparse tokens. This approach supports both high-quality image understanding and pixel-by-pixel reconstruction without manual annotation. This approach learns representations by decoupling the semantics of \"what\" and the spatial location of \"where.\" It then uses self-supervised clustering and optimal transport to align concepts from different perspectives. This approach achieves superior reconstruction quality and semantic expressiveness with only eight sparse tokens, surpassing existing sparse modeling methods on multiple downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-motivated, that low-rank matrix factorization is used to generate small but informative sparse visual tokens, decoupling the semantic concepts and spatial localization.\n\n2. The method provides a relatively complete theoretical basis.\n\n3. The experiments are conducted across various tasks, including segmentation, classification, and linear probing, showing great generalization."}, "weaknesses": {"value": "1. In line 728, the paper claim that the model is initialized from MAE checkpoint. However, the MAE pre-trained model already has strong generalization properties and cannot demonstrate the effectiveness of the proposed STELLAR framework. Especially for self-supervised learning, it is particularly difficult to initialize the model from scratch. It is strongly recommended that authors provide pre-training results from scratch.\n\n2. Several self-supervised methods need to be discussed, especially for efficiency: MoCo v3[1], SiameseIM [2], and OCL[3].\n\n3. I wonder the number of learnable latent queries is fixed (8-24) or self-adapted? It is suggested to add ablation experiments about learnable latent queries. \n\n[1] Chen, Xinlei, Saining Xie, and Kaiming He. \"An empirical study of training self-supervised vision transformers.\" ICCV. 2021.\n\n[2] Tao, Chenxin, et al. \"Siamese image modeling for self-supervised vision representation learning.\" CVPR. 2023.\n\n[3] Yang, Xiaoyu, et al. \"One Leaf Reveals the Season: Occlusion-Based Contrastive Learning with Semantic-Aware Views for Efficient Visual Representation.\" ICML 2025."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fj5maORPAq", "forum": "ZNaWGkKnFh", "replyto": "ZNaWGkKnFh", "signatures": ["ICLR.cc/2026/Conference/Submission10670/Reviewer_nvJG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10670/Reviewer_nvJG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761418027350, "cdate": 1761418027350, "tmdate": 1762921921585, "mdate": 1762921921585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a self-supervised framework for sparse visual representations. Specifically, the authors represent images with a small number of sparse tokens via low-rank matrix factorization that disentangles semantic and spatial information. Several additional losses, including clustering loss and set alignment loss, are also introduced. Experimental results on image reconstruction and understanding tasks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe proposed method is well motivated. Sparse tokens are indeed a promising way for unifying efficiency, interpretability, and semantic richness in visual representations.\n-\tThe paper is generally well-written and easy to follow.\n-\tThe experiments are extensive and the results seem promising, while some parts need to be improved (see weaknesses)."}, "weaknesses": {"value": "-\tSome results are missing exact descriptions. For example, Figure 3 is a bit confusing for me. What do the three colors represent? Besides, what specific number of tokens are used in Table 1 and Table 2? It seems that the authors do not ablate the effect of the number of tokens for understanding tasks.\n-\tAccording to Table 3, it seems that adding clustering decreases the performance significantly (row 2 vs. row 3 in `Ablation model versions`). Could the authors provide a justification for this?\n-\tThe proposed method uses optimal transport matching and Sinkhorn-Knopp algorithms, which could be potentially computationally expensive. It would be better to provide a computational cost comparison (e.g., training time, memory cost) with previous sparse representation learning methods.\n-\tApart from linear probing that serves as a feature extractor, it would be better to provide full fine-tuning results as well, considering fine-tuning is also one of the important benchmarks to evaluate whether the learned representations can serve as a good initialization for downstream tasks."}, "questions": {"value": "I am concerned about the questions mentioned above. Given the current status of the paper, I am leaning towards borderline reject and hope the authors could address my concerns during the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ORbNxDrVdW", "forum": "ZNaWGkKnFh", "replyto": "ZNaWGkKnFh", "signatures": ["ICLR.cc/2026/Conference/Submission10670/Reviewer_ypJX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10670/Reviewer_ypJX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859306151, "cdate": 1761859306151, "tmdate": 1762921921133, "mdate": 1762921921133, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a self-supervised learning recipe in the vision domain. The authors introduce a self-supervised framework named STELLAR that replaces dense tokens with a small set of sparse tokens. STELLAR  Specifically, STELLAR learns a tiny set of sparse concept tokens and per-patch localization weights, then approximate a feature of the given input data using them, letting the model reconstruct images and transfer semantics with just a few tokens The authors employ transport (e.g., sinkhorn) to cluster the sparse visual concepts from the dataset into prototypes. Experimental results including reconstruction, linear probing, segmentation validates the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Introduce an idea of learning sparse visual concept\n* Provided an ablation study on each component"}, "weaknesses": {"value": "* Concern on technical novelty\n    * Isn't it highly dependent on the MAE initialization?\n\n* Concern on its motivation and intuition\n    * From my perspective, the motivation of the proposed method seems similar to that of SemMAE [1] since SemMAE also tried to learn the find-grained information of the semantics (e.g., information of the objects' part) of the. Could the authors clarify the difference between STELLAR and SemMAE in terms of the motivation and intuition?\n\n* The comparison in Table 2 is not reliable. Some baselines are reported far below than the results in their original papers\n    * e.g., according to Table 1 in the iBOT paper, linear probing accuracies for DINO and iBOT exceed 80.0%, surpassing all linear probing results in the Table 1 in the authors' paper\n    * Note that the epochs reported in the iBOT paper are effective epochs that account for multi-crop, not the actual pre-training epochs used by iBOT or DINO. They actually pre-trained only for 300 or 400 epochs.\n    * Moreover, the proposed method employ MAE model parameters for initialization, which is not fair with other baselines. Also, the total epoch should be regarded as 1600 (MAE pre-training epochs) + 150/100/50 (STELLA post-training epochs) = 1750 / 1700 / 1650.\n\n* A lot of recent self-supervised learning methods are missing.\n    * e.g., the references below [1-22]\n    * The proposed method should also be compared with these methods\n\n* Some core evaluation tasks are missing\n    * e.g., Detection and segmentation on COCO\n\n* Important comparison results are missing\n    * Fine-tuning performance comparison is very important in self-supervised learning area on visual data. However, \n        * I'm also suspecting that the proposed design may improve only linear-probing performance rather than fine-tuning performance. This concern is amplified by the utilization of the MAE initialization since MAE is well-known to show strong fine-tuning performance and weak linear-probing performance.\n\n\n\n[1] Li et al., SemMAE, NeurIPS 2022\n\n[2] Mishra et al., CAN, arXiv 2022\n\n[3] Baevski et al., data2vec, ICML 2022\n\n[4] Chen et al., SdAE, ECCV 2022\n\n[5] Assran et al., MSN, ECCV 2022\n\n[6] Dong et al., BootMAE, ECCV 2022\n\n[7] Baevski et al., data2vec2.0, ICML 2023\n\n[8] Wang et al., AdPE, arXiv 2023\n\n[9] Wu et al., ExtreMa, TMLR 2023\n\n[10] Huang et al., CMAE, TPAMI 2023\n\n[11] Yi et al., ConMIM, ICLR 2023\n\n[12] Yi et al., RC-MAE, ICLR 2023\n\n[13] Chen et al., MixedAE, CVPR 2023\n\n[14] Tao et al., SIM, CVPR 2023\n\n[15] Wang et al., HPM, CVPR 2023\n\n[16] Huang et al., MIRL, NeurIPS 2023\n\n[17] Fu et al., CrossMAE, arXiv 2024\n\n[18] Kim et al., LUT, ECCV 2024\n\n[19] Liu et al., dBOT, ICLR 2024"}, "questions": {"value": "What happens if STELLAR does not use the MAE initialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "B4I7ayGkJ8", "forum": "ZNaWGkKnFh", "replyto": "ZNaWGkKnFh", "signatures": ["ICLR.cc/2026/Conference/Submission10670/Reviewer_A18a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10670/Reviewer_A18a"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964388129, "cdate": 1761964388129, "tmdate": 1762921920647, "mdate": 1762921920647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes STELLAR, a self-supervised learning framework designed to learn sparse visual representations from images alone. The core idea of STELLAR is based on a low-rank matrix factorization, $V = LS$. The STELLAR framework is trained using a joint objective function that includes a reconstruction loss, a sparse concept clustering loss, a set alignment loss, and a KoLeo regularization term.The authors claim that this method, using as few as 8 latent tokens, can produce a single representation that simultaneously support high-quality image understanding, detailed pixel-level reconstruction, and fine-grained semantic understanding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tries to address the important problem of redundancy in dense visual representations. The motivation to learn a single, unified representation that excels at both high-level semantic understanding and low-level reconstruction is a valuable."}, "weaknesses": {"value": "1. The paper's claims of novelty are further undermined by a profound misrepresentation of its core mechanism. The authors claim to \"parameterize both S and L as learnable variables\" 1 as part of a \"low-rank matrix factorization\".1These two statements are mutually exclusive. $L$ cannot simultaneously be a set of learnable parameters and a computed output. This formulation is not matrix factorization in the algebraic sense (like NMF or SVD).The paper's actual mechanism is a standard attention operation, cloaked in the language of classical optimization. $S \\in \\mathbb{R}^{r \\times d}$ is a set of $r$ learnable \"latent query vectors\". $U \\in \\mathbb{R}^{n \\times d}$ is the dense patch-level feature map from the ViT encoder. Equation 5 is a standard cross-attention operation, where $S$ acts as the query  and $U$ acts as the key. $L$ is simply the resulting $n \\times r$ attention map, normalized via softmax.The \"reconstruction\" $V=LS$ 1 is then just an attention-pooled representation, where $S$ (the semantic concepts) are the values (V).Therefore, calling this \"low-rank convex semi-nonnegative matrix factorization\" is a profound misrepresentation of a standard attention mechanism. This attempts to invent novelty where none exists.\n2.  The method proposed in this paper is actually very similar to TokenLearner [1], only with different presentation. At the same time, the paper keeps claiming to learn a sparse visual representation, but in practice, it still **relies on a standard visual encoder to extract dense visual feature**. A truly sparse architecture should employ a flexible backbone that can adaptively extract visual features.\n\n[1] TokenLearner: What Can 8 Learned Tokens Do for Images and Videos? NeurIPS 2021"}, "questions": {"value": "1. In Appendix A.4, the manuscript states: \"For efficient training, we initialized the model from public MAE checkpoint\". This is a fatal confounder. The paper is presented as a novel self-supervised learning method that learns representations from scratch. But it is a fine-tuning procedure for a different existing model (MAE) actually."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Z14GK41n3l", "forum": "ZNaWGkKnFh", "replyto": "ZNaWGkKnFh", "signatures": ["ICLR.cc/2026/Conference/Submission10670/Reviewer_VRTX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10670/Reviewer_VRTX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762646024154, "cdate": 1762646024154, "tmdate": 1762921920159, "mdate": 1762921920159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}