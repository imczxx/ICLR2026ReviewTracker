{"id": "LqvvgdvKOc", "number": 826, "cdate": 1756819968170, "mdate": 1763081902396, "content": {"title": "ImplicitQA: Going beyond frames towards Implicit Video Reasoning", "abstract": "Video Question Answering (VideoQA) has made significant strides by leveraging multimodal learning to align visual and textual modalities. However, current benchmarks overwhelmingly focus on questions answerable through explicit visual content - actions, objects, and events directly observable within individual frames or short clips. In contrast, creative and cinematic videos - such as movies, TV shows, and narrative-driven content - employ storytelling techniques that deliberately omit certain depictions, requiring viewers to infer motives, relationships across discontinuous frames with disjoint visual contexts. Humans naturally excel at such implicit reasoning, seamlessly integrating information across time and context to construct coherent narratives. Yet current benchmarks fail to capture this essential dimension of human-like understanding. To bridge this gap, we present ImplicitQA, a novel benchmark specifically designed to test VideoQA models on human-like implicit reasoning. ImplicitQA comprises 1K meticulously annotated QA pairs drawn from 1K high-quality creative video clips covering 15 genres across 7 decades of content. Questions are systematically categorized into\nnine key reasoning dimensions: lateral and vertical spatial reasoning, depth and proximity, viewpoint and visibility, motion and trajectory, causal and motivational reasoning, social interactions, physical context, and inferred counting. These annotations are deliberately challenging, crafted by authors, validated through multiple annotators, and benchmarked against human performance to ensure high quality. Our extensive evaluations on 11 leading VideoQA models reveals consistent and significant performance degradation, underscoring their reliance on surface-level visual cues and highlighting the difficulty of implicit reasoning. Even the best model substantially underperforms human baselines with only 64% accuracy, and no open-source model exceeds 50% accuracy. Performance variations across models further illustrate the complexity and diversity of the challenges presented by mplicitQA. Our analysis highlights the unique challenges of implicit reasoning, ncluding limited gains from scaling frames or parameters. By releasing both the dataset and data collection framework, ImplicitQA establishes a rigorous, diverse, and reproducible testbed for advancing VideoQA.", "tldr": "", "keywords": ["Video understanding", "Implicit Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/a2182c0cbafee44a9b2e025ccd3e55d080b504ab.pdf", "supplementary_material": "/attachment/d5febe41063bfe2d5957cfeeb6d5b469f7038a79.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents ImplicitQA, a benchmark designed to evaluate Video Question Answering (VideoQA) models on implicit reasoning within narrative video content, such as movies. Unlike previous benchmarks that focus on explicit, surface-level visual cues, ImplicitQA challenges models to infer information such as motivational reasoning, spatial relationships, and inferred counting that is suggested but not directly depicted across discontinuous frames. The authors manually curated 1,000 question-answer pairs across 15 genres and 7 decades of film to create the testbed. Evaluations on numerous state-of-the-art VLMs show a significant performance gap compared to human baselines, with no open-source model exceeding 50% accuracy, highlighting the limitations of current architectures in achieving deeper narrative understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- New dataset focusing on implicit reasoning\n- The authors meticulously and manually annotate 1000 QAs\n- The dataset is quite diverse with 15 genres across 7 decades of content\n- The benchmarking is robust with 7 open-source models and 4 proprietary ones plus human-level performance \n- The paper is well-written and easy to follow"}, "weaknesses": {"value": "- I think novelty is an issue here as several datasets have already explored the claims made by the authors: CRIPP-VQA [1] and PhysBench [2] explore physics-based properties and directions covering Lateral and Vertical Reasoning, Depth, Viewpoint, Trajectory, Inferred counting and Physical and Environmental Context. Additionally, VCR [3] is a large-scale dataset that covers Social Interaction and Relationships as well as motivational reasoning, among other interesting implicit QA types.\n- I understand the challenge of manually annotating videos, yet I feel the dataset size is too small and tries to cover too much for its size. It would have been of more value to the community if the dataset had focused on a niche topic, then the size would not have been an issue to me.\n- Stemming from the previous weakness, about 50% of the data is about two classes (Fig. 5). The dataset aims to be diverse but does not really achieve that with the less represented class having 10 times less QAs than the most represented one.\n- The abstract claims 1K meticulously annotated QA pairs drawn from 1K high-quality creative video clips. In Figure 3 (c), I see that some videos have as many as 17 questions. Does that mean some videos do not have any QAs? If so, why? \n- A lot of QA are (arguably) direct (not implicit). For instance: How many individuals are in the scene?, Where did the group of girls go?...\n- [Minor] An ethics statement would be nice, since the authors employ humans for annotation and use videos from the internet.\n- [Minor] Line 912: Expertize (instead of expertise)\n\n[1] Patel, Maitreya, et al. \"Cripp-vqa: Counterfactual reasoning about implicit physical properties via video question answering.\", EMNLP’22\n\n[2] Chow, Wei, et al. \"Physbench: Benchmarking and enhancing vision-language models for physical world understanding.\", ICLR’25\n\n[3] Zellers, Rowan, et al. \"From recognition to cognition: Visual commonsense reasoning.\", CVPR’19"}, "questions": {"value": "Weaknesses + \n- Could the authors highlight the differences between ImplicitQA and the existing benchmarks that focus on implicit video QAs (see weakness #1)?\n- Why would vision-only be considered as a feature? Would having subtitles or audio alter the results? If so, the community would benefit from having these options."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kiq75SukLJ", "forum": "LqvvgdvKOc", "replyto": "LqvvgdvKOc", "signatures": ["ICLR.cc/2026/Conference/Submission826/Reviewer_Y1QG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission826/Reviewer_Y1QG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761041069554, "cdate": 1761041069554, "tmdate": 1762915619978, "mdate": 1762915619978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "37p6Sm9r5z", "forum": "LqvvgdvKOc", "replyto": "LqvvgdvKOc", "signatures": ["ICLR.cc/2026/Conference/Submission826/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission826/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763081901408, "cdate": 1763081901408, "tmdate": 1763081901408, "mdate": 1763081901408, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ImplicitQA, a video question answering benchmark specifically targeting implicit visual reasoning—i.e., questions that cannot be answered by cues visible in a single frame or short clip. The dataset contains 1,000 multiple-choice QA pairs from 1,000 movie clips, annotated by experts and organized into nine reasoning categories (e.g., lateral/vertical spatial reasoning, relative depth, motion/trajectory, viewpoint/visibility, causal/motivational, social interactions, physical context, inferred counting). Extensive evaluations show a large human–model gap: human baseline ≈83%, while the best model (GPT-o3) reaches ~64% overall accuracy, and scaling frames/parameters yields only modest gains. The authors plan to release the dataset and tooling."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Originality. First benchmark laser-focused on implicit, cross-frame visual inference with a clear taxonomy and visual-only constraint (no audio/subtitles), which isolates vision-centric reasoning. \nQuality. Careful expert curation and validation; an end-to-end pipeline for authoring questions and distractors; broad, category-wise analysis across many models; sensible probes on reasoning prompts and frame scaling.  \nClarity. The paper clearly states contributions, shows motivating examples, dataset stats, and a transparent curation workflow. \nSignificance. Reveals a substantial human–model gap on implicit reasoning and demonstrates that simply adding frames/parameters is insufficient, likely spurring architectural/training innovations."}, "weaknesses": {"value": "Scale & headroom. At 1k items, the dataset is relatively small for today’s regime; conclusions about scaling might change with a larger, more diverse corpus. Consider expanding to strengthen statistical power and allow train/val/test splits for learning-on-ImplicitQA.  \nMultiple-choice format. The benchmark uses MCQ with author-crafted distractors, which can introduce guessing artifacts and distractor bias; adding open-ended or rationale tasks would reduce format shortcuts.  \nDomain coverage. Clips come from movies; cinematic conventions differ from other long-video domains (egocentric, sports, instructional). A cross-domain extension would clarify generalization. \nOperational definition of “implicit.” The notion is central yet difficult to formalize. The paper mentions expert relabeling, but more IAA (inter-annotator agreement) and rubric details would bolster reliability.  \nModal ablations. The visual-only choice is principled, but many cinematic inferences hinge on dialogue/sound design. A controlled ablation (visual-only vs. +audio/+subs) would quantify the incremental difficulty.  \nBaselines & training. Results are mostly zero-shot evaluations. It would be informative to train (or adapt) at least one open model on a train split to establish a learnability baseline and prevent over-interpreting zero-shot gaps."}, "questions": {"value": "1.\tDefinition & QA policy. What exact rubric distinguishes implicit from explicit items (with examples that were excluded)? Do you report IAA for this labeling decision?  \n2.\tDistractor quality. How did you validate distractors to avoid annotation artifacts or unintended textual shortcuts (e.g., option length/frequency)? Any adversarial filtering beyond expert design?  \n3.\tSampling bias. What is the distribution across genres/decades and how does accuracy vary by them? Would you consider adding non-movie domains to assess transfer?  \n4.\tModal ablations. Since the benchmark is visual-only, can you report a small-scale ablation adding subtitles/audio to quantify the gap attributable to non-visual cues?  \n5.\tLearning curves. If you create a public train split, how do models improve with supervised fine-tuning or preference/RL signals? Do reasoning prompts remain beneficial once models are adapted?  \n6.\tTemporal sensitivity. How sensitive are results to frame sampling rate/stride and to clip length per category (e.g., counting vs. spatial)? Any per-category scaling curves?  \n7.\tLicensing & release. For movie clips, what usage and redistribution policy will accompany the public release (links + timestamps vs. video snippets)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Tf8rIsu6MW", "forum": "LqvvgdvKOc", "replyto": "LqvvgdvKOc", "signatures": ["ICLR.cc/2026/Conference/Submission826/Reviewer_Srmt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission826/Reviewer_Srmt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704564258, "cdate": 1761704564258, "tmdate": 1762915619831, "mdate": 1762915619831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work introduces a video dataset as a benchmark aimed evaluating implicit reasoning in Video Question Answering task.\nThe authors collect 1000 annotated QA pairs from 1000 creative video clips across 15 genres and 7 decades\nQuestions are categorized into  testing one of nine reasoning dimensions: lateral and vertical spatial reasoning, depth and proximity, viewpoint and visibility, motion and trajectory, causal and motivational reasoning, social interactions, physical context, and inferred counting. \nEvaluations of 11 models (7 open and 4 closed source) reveal consistent under-performance even top models like GPT-O3 reach only 64.1% average accuracy, while human baseline achieve 83.0%.\n\nWhile the introduce benchmark is small in size, with a clear motivation and high quality annotation, it can provide substantial diagnostic insights to pinpoints where and why current systems fail at human-like reasoning. With the strong capabilities of current VideoQA model, i can see the value this benchmark could provide by adding a new dimension to the problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The gap between explicit and implicit reasoning in video understanding is an interesting problem to tackle especially with the growing capabilities of modern systems. The focus on narrative inference and off-screen causality is novel and impactful.\n\nComprehensive evaluation across 30 model configurations including number of frames, impact of reasoning prompts mitigates performance degradation of models from other sources and lends credibility and relevance to the datasets proposed purpose of implicit reasoning.\n\nDiverse dataset that includes animation and live-action, temporal and stylistic diversity strengthens the dataset's generalizability."}, "weaknesses": {"value": "It is not abundantly clear on if there were any inter-annotator agreements on answer correctness and relevance of video segments. It would have been helpful if there was a quantitative agreement scores.\n\nLimited theoretical framing on how the nine distinct reasoning categories were decided. While each category is well defined within the paper, the work would benefit from connecting implicit reasoning to broader theories in cognitive science or narrative comprehension currently the framing is largely empirical.\n\nAlthough high quality, 1K samples are modest for a modern benchmark. The limited size could potentially restrict robust statistical comparisons but as stated in the summary this does not significantly affect my rating."}, "questions": {"value": "How did you define “implicit” to the annotators and how to ensure the definition was closely followed, was there a checklist or rule-based criterion during annotation?\n\nWhat were the number of expert and non-expert annotators?\n\nWhy are Lateral Spatial Reasoning and Vertical Spatial Reasoning considered separately, what is the distinction in the challenge provided between the two? Intuitively, I see no difference in the between the two and fall under the same umbrella of spatial reasoning.\n\nWere the distractor options validated for plausibility?\n\nDo all questions require multi-frame reasoning, or can some be inferred within a single frame?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "d0k3m9YxBh", "forum": "LqvvgdvKOc", "replyto": "LqvvgdvKOc", "signatures": ["ICLR.cc/2026/Conference/Submission826/Reviewer_HMjT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission826/Reviewer_HMjT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735139144, "cdate": 1761735139144, "tmdate": 1762915619530, "mdate": 1762915619530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ImplicitQA, a benchmark designed to test human-like implicit reasoning where viewers are required to infer spatial information, causal relationships, etc. from multiple video frames where the answer to the question is not explicitly shown in a single frame. The authors conduct experiments to show performance differences between human baseline and models, and additional studies examining (1) impact of reasoning prompt on GPT models, (2) effect of number of frames, and (3) uneven performance across reasoning categories."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is clearly written and easy to follow.\n2. The authors and annotators manually curated 1k QA pairs across 1k videos, and defined 9 reasoning categories to ensure diversity in the QA types.\n3. The authors highlight the performance gap between human and SOTA models demonstrating current models’ deficiency in reasoning over cinematic content where answers could only be inferred from across disjoint visual contexts."}, "weaknesses": {"value": "1. One of my main concerns is to what extent the performance gap reflects insufficient “implicit reasoning” as opposed to differences in how models allocate visual attention, such as focusing sparsely versus in detail.\\\n    (i). In the 6 examples shown in the paper, I personally was unable to answer correctly (a) Lateral Spatial Reasoning from Figure 1, and (a) Physical and Environment Context from Figure 6. This difficulty appears partially due to the high level of visual attention to detail required from viewers. No additional analysis has examined the effect of this factor. (See more in Question 1)\\\n    (ii). The authors have also observed “Uneven performance across reasoning categories” and reported relatively better performance on Social Interaction and Motivational Reasoning where attention and perceptual granularity are less detail-oriented.\n2. Only one human baseline is provided. As the non-expert annotators were only tested with the video setting but not frame setting like what models receive, it is unclear how much of the performance gap between humans and models arises from their differences in processing visual inputs vs. limitations in “implicit reasoning”. (See more in Question 3)\n3. ImplicitQA is largely composed of spatial reasoning QAs according to Figure 5. Since the authors have not provided qualitative or quantitative definitions of “implicit reasoning”, it remains unclear at the moment how this work could be differentiated from the vast amount of literature on spatial/-temporal reasoning benchmarks such as [1], [2].\n4. The claims that “no open-source model exceeds 50% accuracy” and “Open-source models generally performed worse than proprietary models” are not sufficiently supported by experiments when open-source model results reported in the main table are all 7B and the biggest open-source model tested in the paper being 32B Qwen2.5-VL. Considering [3] has reported slightly better results achieved by Qwen2-VL-72B compared to GPT-4o on temporal reasoning, the authors would need more experiments to make such claims.\\\n\\\n[1] Wu, Bo, et al. \"Star: A benchmark for situated reasoning in real-world videos.\" arXiv preprint arXiv:2405.09711 (2024).\\\n[2] Mao, Jiayuan, et al. \"Clevrer-humans: Describing physical and causal events the human way.\" Advances in Neural Information Processing Systems 35 (2022): 7755-7768.\\\n[3] Shangguan, Ziyao, et al. \"Tomato: Assessing visual temporal reasoning capabilities in multimodal foundation models.\" arXiv preprint arXiv:2410.23266 (2024)."}, "questions": {"value": "1. Following up on Weakness 1(i)., have the authors considered conducting experiments where captions are first generated for each frame, and models then reason only in a “text-only” setting to see if the details required to answer questions (such as (a) Lateral Spatial Reasoning from Figure 1, and (a) Physical and Environment Context from Figure 6) are already left out of caption generation step. This two-pass experiment could help clarify the source of the poor performance: sparse attention or “implicit reasoning” limitations. \n2. The 6 example videos all appear to be animated videos. What is the percentage of natural videos compared to animated videos presented in this benchmark? Have the authors considered analyzing the effect of such features on model performances?\n3. Have the authors considered including more baselines such as where non-expert annotators are only given the same frames as the models receive?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The benchmark includes clips from well-known copyrighted works such as Toy Story, Super Mario, Kung Fu Panda, etc., whose studios such as Disney/Pixar, Nintendo/Universal, and DreamWorks Animation are known to be very strict about video redistribution, even for academic use. However, the paper does not address any licensing, copyright, or redistribution compliance considerations associated with these materials."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sAJiqIfzZn", "forum": "LqvvgdvKOc", "replyto": "LqvvgdvKOc", "signatures": ["ICLR.cc/2026/Conference/Submission826/Reviewer_V3sQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission826/Reviewer_V3sQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957533379, "cdate": 1761957533379, "tmdate": 1762915619377, "mdate": 1762915619377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}