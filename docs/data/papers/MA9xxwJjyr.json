{"id": "MA9xxwJjyr", "number": 8596, "cdate": 1758092147729, "mdate": 1763654099220, "content": {"title": "HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application", "abstract": "Effective deep search agents must not only access open-domain and domain-specific knowledge but also apply complex rules—such as legal clauses, medical manuals and tariff rules. These rules often feature vague boundaries and implicit logic relationships, making precise application challenging for agents. However, this critical capability is largely overlooked by current agent benchmarks. To fill this gap, we introduce \\textsc{HSCodeComp}, the first realistic, expert-level e-commerce benchmark designed to evaluate deep search agents in hierarchical rule application. In this task, the deep reasoning process of agents is guided by these rules to predict 10-digit Harmonized System Code (HSCode) of products with noisy but realistic descriptions. These codes, established by the World Customs Organization, are vital for global supply chain efficiency. Built from real-world data collected from large-scale e-commerce platforms, our proposed \\textsc{HSCodeComp} comprises 632 product entries spanning diverse product categories, with these HSCodes annotated by several human experts. Extensive experimental results on several state-of-the-art LLMs, open-source, and closed-source agents reveal a huge performance gap: best agent achieves only 46.8\\% 10-digit accuracy, far below human experts at 95.0\\%. \nBesides, detailed analysis demonstrates the challenges of hierarchical rule application, and test-time scaling fails to improve performance further. Codes and the benchmark will be publicly released.", "tldr": "We introduce HSCODECOMP, the first realistic and expert-level benchmark designed to evaluate deep search agents in hierarchical rule application of e-commerce domain.", "keywords": ["HSCode Benchmark", "Deep Search Agent", "Precise Multi-Turn Deep Reasoning", "Hierarchical Rule Application"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea3db39a248ecdfdb4a46903058cd92d79c1386a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces HSCodeComp, a benchmark of 632 expert-labeled products for assigning 10-digit customs tariff codes. The authors formulate the problem as high stakes for trade compliance and demonstrate a noticeable gap for LLMs, the best model/agent is ~46.8% exact match at 10 digits versus ~95% for human experts. The paper also reports accuracy at coarser HS levels (2/4/6/8/10 digits), shows common failure modes, and argues that automated tariff classification is still far from solved. The paper argues this gap demonstrates that current LLM agents are not yet reliable for trade classification."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Practical task with real world impact. Tariff classification is a hierarchical categorization problem very similar to other industry classification specifications. The paper justifies the problem on how the classification directly affects duties, compliance risk, and audits.\n2. Furthermore, the current gap between humans and model/agents is demonstrated: top LLM agents still fail ~50% of the time at the final 10-digit code. \n3. Clear Expert-created workflow for ground truth: annotators gather product attributes (materials, intended use, etc.), consult official rulings/decision rules, resolve disagreements, and escalate difficult items. This data is not easily made via synthetic generation. \n4. Layered evaluation is reported at 2/4/6/8/10 digits that’s useful for probing heretical failures in the classification. \n5. Error analysis names specific failure modes (“valid but not chosen,” “outdated code,” material confusion like silicone vs. rubber) which is useful in grounding the nuances of the task in relation to alignment with language."}, "weaknesses": {"value": "1. The paper evaluates hierarchical levels, but it’s not explicit whether HSCodeComp is meant to be a flat 10-digit prediction task, or a hierarchical decision process / constrained decoding task. Would this be simpler if the workflow commits to a 2-digit chapter, then refine constrained of the child nodes? That needs to be made explicit for reproducibility and fairness in future comparisons.\n2. Metrics could be improved. Currently the paper uses exact-match, even if the model picked a code in the correct branch and only got the last two digits wrong. Furthermore, top n accuracy could be provided as an appendix. It would be useful to quantify near misses, sibling confusion, or a hierarchical distance for the analysis. The qualitative examples hint that these near misses are common. This weakens the interpretability of the 46.8% number.\n3. Lack of a constrained-decoding/structured prediction  baseline. From my understanding, the baseline agents are allowed to output nonexistent or structurally invalid 10-digit codes, or codes that don’t match their own predicted parent. Would a trivial hierarchical decoder (predict chapter, restrict to valid children, backtrack if invalid) would cut out a whole class of hallucination errors? Not including that baseline makes it harder to tell how much of the gap is deep legal reasoning vs. just lack of structural constraints. Furthermore could this be a language vs token misalignment, and if the numerical codes were replaced with text labels, would the gap still persist?\n4. Temporal stability is unclear, HTS codes change over time: some codes get split/retired, and new product categories appear. The paper discusses an outdated code failure mode. But it does not clearly state which HS/HTS revision date is considered authoritative for HSCodeComp, nor how future updates will be versioned. Without explicit versioning or a way to update future categorizations, long-term benchmarking and replication will be shaky. We see this in GICS classifications \n5. Representativeness and coverage is not clear, the dataset spans 27 HS chapters and 32 e-commerce categories, but in the dataset, how many unique 10-digit leaves are represented, how many examples per leaf, and whether this mix reflects everyday brokerage volume vs. being intentionally enriched for tricky, dispute-prone items. This matters for how generalizable the reported agent accuracy is on this set versus actual in production distributions. \n6. Rules hurting the agent needs one concrete example. The paper claims that giving agents human-written tariff “decision rules” sometimes degrades performance. That’s interesting and believable given similar nuances in legal and finance, but the paper doesn’t walk through a single case. One worked example would make that claim much more convincing."}, "questions": {"value": "1. Is the intended task definition a flat 10-digit classification or hierarchical code selection step by step? Did the authors evaluate these two modes of classification?\n2. Can you quantify “near misses”? How often is the model correct through 6 or 8 digits but off at the final branch? That seems essential for interpreting the ~46.8% 10-digit score. Can you quantify some distance metric in branches/leaves?\n3. Have you tried a constrained or hierarchical decoder baseline, or a structured outputs approach that only allows valid descendants and blocks nonexistent or outdated codes? Did you experiment by replacing numerical codes with enums or hard string labels for language grounding? How much could this be a side effect of tokenization?\n4. Which HTS revision (with date) are your gold labels tied to? And do you plan to release HSCodeComp as versioned snapshots over time to track tariff changes? How are you planning to update the benchmark when codes change in the future?\n5. How many distinct 10-digit codes are in HSCodeComp, and what does the per-code sample count look like? Is this mostly one-shot per code, or do some leaves recur?\n6. Could you include one concrete example where agent performance degraded when given a human tariff rule, and why? This is an interesting failure mode related to the task and can help future work understand where the gap stems from with regards to instruction following."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zGtXfAIJ3X", "forum": "MA9xxwJjyr", "replyto": "MA9xxwJjyr", "signatures": ["ICLR.cc/2026/Conference/Submission8596/Reviewer_4L3J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8596/Reviewer_4L3J"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672611492, "cdate": 1761672611492, "tmdate": 1762920441496, "mdate": 1762920441496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Common Questions"}, "comment": {"value": "Dear Reviewers and ACs,\n\nWe sincerely thank you for your time and insightful feedback, which have been invaluable in improving our manuscript. We have carefully considered all comments and have provided detailed responses to each reviewer individually.\n\nBelow, we summarize our main revisions addressing the common points of Reviewers #Gvkx and #4L3J for their thoughtful questions regarding model performance at intermediate hierarchical levels. \n\n**As requested, we now provide the conditional accuracy of partial models on Chapter, Heading, Sub-heading, and country-specific sub-category code classification tasks.**\n\n| Model                       | Type | Chapter Acc. (0-2 digit) | Heading Acc. (2-4 digit) | Sub-heading Acc. (4-6 digit) | Country-specific Acc. (6-10 digit) |\n|-----------------------------|------|--------------------------|--------------------------|------------------------------|------------------------------------|\n| **LLM/VLM-Only**            |      |                          |                          |                              |                                    |\n| GPT-5                       | VLM  | 82.12                    | 86.32                    | 84.60                        | 48.81                              |\n| Gemini-2.5-PRO              | VLM  | 82.28                    | 86.34                    | 83.08                        | 41.02                              |\n| Claude Sonnet 4             | VLM  | 78.80                    | 81.32                    | 70.61                        | 24.82                              |\n| Kimi-K2                     | LLM  | 78.01                    | 79.52                    | 71.18                        | 27.59                              |\n| DeepSeek-V3                 | LLM  | 77.06                    | 70.63                    | 59.31                        | 20.11                              |\n| O3-mini                     | LLM  | 77.22                    | 72.74                    | 43.67                        | 5.18                               |\n| **Open-source Agent Systems (GPT-5 Backbone)** |      |                          |                          |                              |                                    |\n| SmolAgents                  | VLM  | 82.06                    | 87.81                    | 86.57                        | 75.07                              |\n| Aworld                      | LLM  | 82.28                    | 85.57                    | 84.05                        | 69.79                              |\n| Agentorchestra              | LLM  | 82.12                    | 86.13                    | 85.45                        | 68.33                              |\n| OWL                         | LLM  | 72.63                    | 85.19                    | 83.37                        | 72.39                              |\n| WebSailor                   | LLM  | 81.64                    | 86.43                    | 81.16                        | 61.88                              |\n| Cognitive Kernel            | LLM  | 80.06                    | 86.37                    | 78.94                        | 48.40                              |\n| **Average**      |-  | 73.80                    | 75.16                    | 64.78                        | 35.32                              |\n\nThis performance pattern reveals three key insights:\n1. The 2-4 digit heading level shows highest average accuracy (75.16), since its tariff rules are most clearly defined\n2. Intermediate performance drops at the 4-6 digit subheading level due to increased the number of  rules and their vague descriptions in tariff rules\n3. The steepest decline occurs at the country-specific 6-10 digit levels, where rules become highly contextual and country-specific\n\nThis hierarchical performance pattern directly supports our main finding: \n* **current agents struggle with the complex, multi-step reasoning required for precise rule application at deeper hierarchical levels.** \n* **The significant accuracy drop from 64.8% (6-digit) to 35.3% (10-digit) demonstrates that correctly navigating the initial hierarchical steps does not guarantee success in the final classification, highlighting the cumulative reasoning challenges in hierarchical rule application.**\n\nThanks for the constructive responses of two reviewers, and we have added this analysis to Appendix D.1 in the revised manuscript to better highlight this important aspect of the challenge.\n\n\nBest regards,\n\nThe Authors"}}, "id": "adYyGFcIkJ", "forum": "MA9xxwJjyr", "replyto": "MA9xxwJjyr", "signatures": ["ICLR.cc/2026/Conference/Submission8596/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8596/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8596/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763652765750, "cdate": 1763652765750, "tmdate": 1763652765750, "mdate": 1763652765750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper releases \\textit{HSCODECOMP}, a benchmark of 632 expert‑annotated e‑commerce products (27 HS chapters, 32 first‑level categories) to test agents’ ability to apply hierarchical tariff rules and predict 10‑digit HS/HTS codes from noisy, multi‑modal inputs (title, attributes, image, price, category, URL). Baselines cover 14 LLM/VLMs and six open‑source agents (plus three closed‑source systems on a 49‑item subset); the best system (SmolAgents + GPT‑5 (VLM)) attains 46.83% 10‑digit accuracy vs. 95.0% for human experts (Table 1, p.6; Fig. 1, p.2). The paper analyzes “overthinking,” test‑time scaling, image utility, and failure modes, and promises to release code and data. Claimed contributions: (i) a realistic, expert‑level benchmark for rule application (their “Level‑3” knowledge); (ii) a multi‑expert annotation/validation pipeline; (iii) broad LLM/agent baselines with analyses of think‑depth, images, and test‑time scaling"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper tackles a timely and important challenge: applying rules for HS code classification rather than relying on open-ended retrieval. The motivation and problem space are clearly illustrated in Figure 1 (left side, page 2). \nThe dataset and setup are realistic—inputs combine noisy product listings, structured attributes, images, and URLs—and ablation studies show that including images improves accuracy in several scenarios (Table 4, page 7; Table 10, page 36).\nThe data labeling process is carefully designed: two experts annotate each item, a senior adjudicator resolves disagreements, and a 10% spot check shows only 2% disagreement (Figure 3, page 5).\nThe authors compare a wide range of baseline models and report results consistently at all HS code levels (2-, 4-, 6-, 8-, and 10-digit). The benchmark includes strong models such as GPT-4o and Qwen variants (Table 1, page 6).\nFinally, the paper provides thoughtful analysis, including (i) a study of “overthinking” behavior versus tool use (Table 5, page 7); (ii) an investigation of why scaling model size at test time brings limited gains (Figure 4, page 8); and (iii) a useful taxonomy of common failure types that highlights cases where predictions are “error-but-valid” (Figure 5, page 9)."}, "weaknesses": {"value": "First, the current evaluation metric is too narrow. It only counts exact 10-digit matches as correct, even when the model predicts a valid but slightly different code. The authors themselves note that many predictions are “Error-but-Valid.” This shows a need for more flexible metrics—such as hierarchical distance, agreement at higher HS code levels (2, 4, 6, or 8 digits), and a rule-consistency score. As it stands, many reasonable answers are unfairly marked wrong (Section 4.2, p. 5; Figure 5, p. 9).\n\nSecond, there is a factual mistake in the HS taxonomy explanation. Section 3 (“Output,” p. 4) incorrectly claims that “the last four digits (from 6 to 10) are country-specific,” when in fact only digits 7–10 vary by country, while the 6-digit level is globally standardized. This should be corrected for accuracy.\n\nThird, the source and authority of the “rules” used are ambiguous. The paper relies heavily on eWTP tariff rules (Figures 11–12; Section 4.1) but doesn’t explain how they align with official WCO or HTS legal notes or with U.S. CROSS rulings. Depending on a commercial taxonomy may lead to discrepancies with authoritative references.\n\nForth, the evaluation has fairness issues. Closed-source models were tested on only a 49-item subset (Table 2, p. 6), making their results not directly comparable to the 632-item open benchmark. The authors also disabled webpage retrieval because it slightly reduced accuracy, yet this choice makes the task less realistic for genuine research workflows."}, "questions": {"value": "$\\textbf{Correct HS Hierarchy Definition and Terminology (Sec. 3 “Output,” p. 4)}$. The authors should fix the inaccurate description of the HS hierarchy. Clearly distinguish between the global levels (2-, 4-, 6-digit, standardized by the WCO) and the national extensions (8-, 10-digit, defined by each country’s tariff schedule). Ensure consistent use of “HS” versus “HTS” throughout the text."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TvtUXS8oKZ", "forum": "MA9xxwJjyr", "replyto": "MA9xxwJjyr", "signatures": ["ICLR.cc/2026/Conference/Submission8596/Reviewer_rzk4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8596/Reviewer_rzk4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892354847, "cdate": 1761892354847, "tmdate": 1762920441129, "mdate": 1762920441129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Updates to Revised Manuscript"}, "comment": {"value": "Dear Reviewers and ACs,\n\nThank you for your time and valuable feedback. We have uploaded a revised version of our paper. We believe these updates fundamentally strengthen our work and address key concerns of reviewers.\n\nHere are the highlights of the revision:\n\n---\n\n1. **Clarification on the alignment among rules and knowledge (Section 3, lines 157, 186-187; Appendix F, lines 964-968):** In response to Weakness 3 raised by reviewer #rzk4, we have added clarifications and explanations demonstrating the alignment between HSCode tariff rules, the CROSS ruling database, and human expert decision rules. All these knowledge sources are specifically tailored to US tariff classification scenarios.\n\n2. **Typo correction (Section 3, line 190):** In response to Weakness 2 raised by reviewer #rzk4, we have corrected the typographical error.\n\n3. **Dataset Distribution Statistics Supplement (Appendix B):** In response to reviewer #4L3J's Weakness 5 requesting detailed statistical information about our dataset, we have provided comprehensive statistics demonstrating that our data distribution aligns with real-world e-commerce scenarios. As a result, our evaluation results accurately reflect model capabilities in practical applications.\n\n4. **Intermediate Step Performance (Appendix D.1):** In response to reviewer #Gvkx and #4L3J's request for intermediate step performance analysis, we have conducted additional experiments. Our analysis reveals that task difficulty and complexity increase progressively as we move deeper into the hierarchical rule structure. These findings further validate the paper's claim regarding the significant challenges agents face in hierarchical rule application.\n\n5. **Component effectiveness assessment (Appendix D.2):** Addressing Weakness 2 identified by reviewer #GvkX, we have added detailed ablation study results. Our experiments show that tariff rules provide the most significant performance improvement for agents. In contrast, hierarchical knowledge utilization proves most challenging for current agents and actually harms performance when incorporated. CROSS rulings provide only marginal and statistically insignificant performance gains.\n\n6. **Temporal specification (Appendix F, lines 969-971):** In response to reviewer #4L3J's Weakness Q4, we have clarified specific details of our evaluation experiments, explicitly noting they are based on tariff rules and knowledge from a specific time period. This clarification demonstrates the high reproducibility of our dataset.\n\n7. **Failure mode Case Study (Appendix G):** Addressing reviewer #4L3J's Weakness Q6, we have added specific case studies illustrating failure modes where models incorrectly apply hierarchical rules, along with detailed analysis of these failures.\n\n---\n\nGiven these fundamental improvements, we believe the paper is in a much stronger position. We respectfully ask you to re-evaluate our work based on this new version.\n\nWe will now proceed to address your individual comments and look forward to a constructive discussion.\n\nBest regards,\n\nThe Authors"}}, "id": "dkCHlKM74k", "forum": "MA9xxwJjyr", "replyto": "MA9xxwJjyr", "signatures": ["ICLR.cc/2026/Conference/Submission8596/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8596/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8596/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763652799611, "cdate": 1763652799611, "tmdate": 1763654173303, "mdate": 1763654173303, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HSCodeComp, a benchmark for evaluating deep search agents on hierarchical rule reasoning. The task is to predict 10-digit Harmonized System (HS) codes for e-commerce products based on noisy text descriptions, while following multi-layer tariff rules. The motivation is that most existing benchmarks only test structured or open-domain reasoning, but none evaluate agents’ ability to apply complex, human-written hierarchical rules, which is important for real-world expert systems like legal or trade automation.\n\nThe benchmark is built from real e-commerce data with expert annotations and covers 632 products across 27 chapters. The authors test various LLMs and agent systems, finding that even the best combination eaches only 46.8% accuracy, far behind humans (95%). Ablation shows that vague and interdependent rules are the main difficulty."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation is clear. he paper clearly identifies a missing evaluation angle: hierarchical rule following, which is indeed a challenging and realistic reasoning task.\n\n2. The dataset is comprehensive. The dataset is built with expert validation and seems to capture realistic product diversity and textual noise.\n\n3. The experiments compare many models and agent frameworks, giving a broad and fair view of the task difficulty."}, "weaknesses": {"value": "I am not an expert in search agent. My concerns are only raised from the research perspective not specific to this certain domian.\n\n1. Only 632 samples might be too few to show robust performance differences.\n\n2. Since rules come from different sources (tariff codes, human rulings, etc.), it would be useful to test which part contributes most to performance.\n\n3. I wonder how well models perform at intermediate steps (like predicting subcategories).\n\n4. Maybe models tuned for other structured domains (finance, medicine) could generalize better. A small cross-domain test would strengthen the claim that the challenge truly lies in hierarchical rule reasoning."}, "questions": {"value": "Please see the questions raised in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fNINU0hoHR", "forum": "MA9xxwJjyr", "replyto": "MA9xxwJjyr", "signatures": ["ICLR.cc/2026/Conference/Submission8596/Reviewer_GvkX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8596/Reviewer_GvkX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925266334, "cdate": 1761925266334, "tmdate": 1762920440581, "mdate": 1762920440581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "I think this is more like a technical report than a research paper, way below ICLR standard."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "It has lots of LLM used"}, "weaknesses": {"value": "Quality of the paper is very poor."}, "questions": {"value": "I would suggest the authors read more good papers and see how a good paper is written."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9hWUzT08F7", "forum": "MA9xxwJjyr", "replyto": "MA9xxwJjyr", "signatures": ["ICLR.cc/2026/Conference/Submission8596/Reviewer_T48D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8596/Reviewer_T48D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963206987, "cdate": 1761963206987, "tmdate": 1762920440078, "mdate": 1762920440078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "I think this is more like a technical report than a research paper, way below ICLR standard."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "It has lots of LLM used"}, "weaknesses": {"value": "Quality of the paper is very poor."}, "questions": {"value": "I would suggest the authors read more good papers and see how a good paper is written."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9hWUzT08F7", "forum": "MA9xxwJjyr", "replyto": "MA9xxwJjyr", "signatures": ["ICLR.cc/2026/Conference/Submission8596/Reviewer_T48D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8596/Reviewer_T48D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963206987, "cdate": 1761963206987, "tmdate": 1763655328586, "mdate": 1763655328586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}