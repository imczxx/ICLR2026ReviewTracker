{"id": "QSxnB2PXJb", "number": 6067, "cdate": 1757952033129, "mdate": 1763048163795, "content": {"title": "Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning", "abstract": "Current unlearning and safety training methods consistently fail to remove dangerous knowledge from language models. We identify the root cause – unlearning targets representations which are too general – and develop a highly selective technique that unlearns robustly while preserving general performance.\n\nOur method performs PCA on activations and module-output gradients to identify subspaces containing common representations, then collapses these subspaces before computing unlearning updates, a technique we term Collapse of Irrelevant Representations (CIR). This avoids unlearning general knowledge and targets only representations specific to the facts being unlearned.\n\nWhen unlearning bio- and cyber-hazardous facts from Llama-3.1-8B, we achieve over 30× greater reduction in post-attack accuracy than the best baseline (Circuit Breakers), while disrupting general performance 30× less, and using less than 3 GPU-seconds per fact.\n\nThus, by disentangling harmful and benign capabilities at the level of representations, CIR enables robust and non-disruptive unlearning.", "tldr": "When we collapse general representations before computing unlearning updates, we prevent the disruption of general performance and make unlearning more robust.", "keywords": ["unlearning", "representation-engineering", "language-models", "biosecurity", "cybersecurity", "fine-tuning", "robustness", "adversarial-attacks", "WMDP", "AI-safety", "selective-unlearning", "neural-representations", "evaluation-robustness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/a0edd77660813d0e3a7d3708c352abfc56c77efe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a novel unlearning method called Collapse of Irrelevant Representations (CIR), which collapses subspaces containing common representations, thus making the unlearning more effective and robust."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Analysis is comprehensive. The authors did a systematic analysis on the problems with current unlearning strategies, especially the study on the disruption of superficially similar facts, offering a new perspective in understanding the pitfalls of unlearning.\n2. The method is novel. The authors propose CIR approach, which collapses irrelevant representations, and demonstrate promising results in terms of the unlearning quality and robustness."}, "weaknesses": {"value": "1. Writing is poor. Many concepts are vague and many figures are hard to understand. Examples are listed below:\n\n   1. In Figure 2, I don't understand why in many cases, the WMDP accuracy during fine-tuning attack never reaches the WMDP accuracy at 0.1% Disruption threshold, and how these \"WMDP accuracy at 0.1% disruption threshold\" are computed. Also, I don't understand where we can infer \"only unlearning that happened after the disruption threshold can be reverted, and unlearning that happened without disruption remains robust,\" based on Figure 2, since there are no indicators on which traces represent the revert process, and how we can know which one is reversible. Overall, this figure is messy and hard to read.\n    2.  In Figure 3, I don't understand the meaning of the columns inside the activations and gradients plot. What is the meaning of \"a slide of activations\"? Is it a row/column of the activation? I just feel the dimension is incorrect, since for Llama3.1-8B, the hiddenlayer feature dimension should be 4096. I also don't understand the meaning of red and green in this figure.\n    3. In Figure 4, I don't understand the meaning of \"an update successfully unlearns a paraphrased fact\". How do we define a successful unlearning? I also don't understand the meaning of the scale of the color. Similarly, the shape of the matrix seems erroneous since the hidden matrix shape of Llama3.1-8B MLP layer should be (4096, 14336) (mlp.gate, mlp.up) (14336, 4096)(mlp.down)\n    4. In Figure 5 and Figure 1(b), I don't understand why the curve is not injective -- sometimes one wikitext loss corresponds to multiple WMDP accuracy in the unlearning plot.\n2. Experiments are oversimplified. Only evaluate on the LLama3.1 model, which is a small, outdated model. What's more, loss on wikitext is not a good metric of utility. We have a bunch of benchmarks that evaluate the model's utility from different perspectives (MCQ and open-ended). The authors should at least choose some of them (say 3 benchmarks) to strengthen their argument."}, "questions": {"value": "I have listed my questions in Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cocje6EC0S", "forum": "QSxnB2PXJb", "replyto": "QSxnB2PXJb", "signatures": ["ICLR.cc/2026/Conference/Submission6067/Reviewer_1ufv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6067/Reviewer_1ufv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761330436372, "cdate": 1761330436372, "tmdate": 1762918442178, "mdate": 1762918442178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "From the reviews it is clear to us that the paper will require major rework, so we decided to withdraw from ICLR.\n\nWe thank the reviewers for their time, and will work to expand the experiments and improve presentation, based on their suggestions."}}, "id": "rpNrZocDTu", "forum": "QSxnB2PXJb", "replyto": "QSxnB2PXJb", "signatures": ["ICLR.cc/2026/Conference/Submission6067/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6067/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763048162933, "cdate": 1763048162933, "tmdate": 1763048162933, "mdate": 1763048162933, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work identifies a root cause leading to the failure of unlearning or safety training methods in targeting too general representations to be unlearned. Based on their analysis, the authors propose a new approach, Collapse of Irrelevant Representations (CIR), that collapses activation subspaces by eliminating principal components to allow for more targeted unlearning and prevent the unlearning of general knowledge. The CIR method is evaluated on Llama-3.1 on bio-/cyber-hazardous facts datasets against the Circuit Breakers baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- S1. This work identifies and studies the important problem of unlearning/safety methods failing to achieve the promised target performance.\n- S2. The authors conduct an analysis of what is causing unlearning methods to fail and investigate what conditions lead to such results.\n- S3. The CIR method is grounded in the initial analysis and is effective, according to the authors’ evaluation."}, "weaknesses": {"value": "- W1. The analysis is very empirical and appears to lack statistical significance or theoretical insight.\n- W2. The computational cost and scalability of CIR are unclear. The authors report <3 GPU-seconds per fact on an 8B model, but there is no discussion on how this scales with model size or dataset size. Without such discussion, it's hard to judge if CIR would be practical for larger models or real-world use.\n- W3. The experimental evaluation is limited to a single model (LLaMA-3.1, 8B) and one dataset of hazardous facts. This narrow scope raises questions about the generality of the findings. Evaluating on additional models or datasets (or providing justification why it wasn’t done) would make the results more convincing."}, "questions": {"value": "- Q1. Section 3’s analysis could be more rigorous and explicitly structured. Can the authors summarize it by explicitly mentioning: (a) what the hypotheses involved are; (b) what the properties of the setting they are investigating are; and (c) what experimentation, metrics, and analysis they performed to validate their hypotheses?\n- Q2. Can the authors provide a more theoretical overview that explains the intuition behind their analysis and method, with sufficient references to the relevant literature?\n- Q3. Can the authors elaborate on why they chose to base their entire evaluation on a single dataset and a single model? Would it be possible to extend the evaluation further to make it more convincing?\n- Q4. What are the computational requirements of CIR for larger models or more data? Can the authors comment on how the PCA and collapse steps scale with model size, and whether there are any optimization tricks to keep it efficient?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oLM2Lu3dQw", "forum": "QSxnB2PXJb", "replyto": "QSxnB2PXJb", "signatures": ["ICLR.cc/2026/Conference/Submission6067/Reviewer_MM7y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6067/Reviewer_MM7y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930945706, "cdate": 1761930945706, "tmdate": 1762918441710, "mdate": 1762918441710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Collapse of Irrelevant Representations (CIR), a selective framework for Large Language Model (LLM) unlearning that is robust to fine-tuning attacks.\n\nThe authors first identify critical weaknesses with existing unlearning methods, noting that they indiscriminately disrupt general representations shared between harmful and benign knowledge. This, they argue, creates superficial unlearning: while models appear to forget on target data, they remain highly vulnerable to relearning attacks that easily repair these shared representations.\n\nTo address this, the paper introduces CIR, which performs representation-level purification of unlearning updates to make them highly specific. The framework contains two main components:\n\n(1) Activation–Gradient PCA, which identifies common subspaces in MLP layers that encode irrelevant or general-purpose representations;\n(2) Subspace Collapse, which removes these components from activations and gradients before weight updates, ensuring that unlearning targets only fact-specific directions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a significant contribution by moving beyond the general goal of minimizing disruption in unlearning and instead empirically identifying the precise point at which unlearning becomes unstable. Through extensive experiments, the authors show that once general performance degradation exceeds roughly 0.1%, the unlearned knowledge can be easily recovered through fine-tuning. This analysis, based on 50 unlearning runs, provides clear evidence that only unlearning achieved within the non-disruptive regime is robust, while unlearning in the disruptive phase is largely reversible. This insight explains the failure of previous methods that tolerated higher disruption levels and establishes a concrete design criterion for future work: robust unlearning requires maintaining near-zero disruption."}, "weaknesses": {"value": "**1. From the perspective of experimental datasets**\n\nThe paper only evaluates its method on a single dataset (WMDP) and a single model, which greatly limits the reliability and generalizability of its conclusions. The authors should include experiments on additional models and datasets to demonstrate the robustness and consistency of their approach.\n\n**2. From the baseline comparison perspective:**\n\nClaiming that Circuit Breakers represents the best-performing baseline is not entirely fair (L23–24: “greater reduction in post-attack accuracy than the best baseline (Circuit Breakers)”). At a minimum, the authors should compare CIR against more recent unlearning methods, such as the RMU method introduced in the WMDP paper itself. The two baselines reported in this work perform rather poorly, making it unclear whether the observed recovery under fine-tuning attacks results from under-unlearning rather than attack robustness. Consequently, the current comparisons are insufficient to convincingly establish CIR’s superiority against relearning attacks."}, "questions": {"value": "This paper provides a solid analysis of the disruption and relearning problem. If the authors could include more experiments and stronger baseline comparisons to further validate their method, I would be glad to raise my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GgsgzpJQz1", "forum": "QSxnB2PXJb", "replyto": "QSxnB2PXJb", "signatures": ["ICLR.cc/2026/Conference/Submission6067/Reviewer_JtCL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6067/Reviewer_JtCL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955445845, "cdate": 1761955445845, "tmdate": 1762918441351, "mdate": 1762918441351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper claims that unlearning is irreversible even under finetuning attacks till a certain threshold of retain degradation. \nThen the paper introduces a method \"Collapse of irrelavant representations\", where the main idea is removal of directions in the activation and gradient space during updates, which relevant to non-forget information.\\\nThe authors perform this by first averaging these quantities over some corpus (for example the unlearning corpus), then finding the principal components of this, which are subtracted from the original activations."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper seems to have interesting ideas about ablating out common representations between the unlearning information and the retain information.\\\nHowever, it is not ready for publication due to lack of clarity and experimental evidence."}, "weaknesses": {"value": "Several points are unclear in the paper\n- In Figure 1, the claim that finetuning attacks only restore WMDP accuracy till the disruption threshold seems inaccurate to me. This is because I see several points where it crosses the y=x line. Kindly explain or provide more evidence for this claim. \n- A more direct evidence of this claim will be  performing finetuning attacks directly on those checkpoints where the disruption threshold is still below 0.1%. Is it possible to show that there is no relearning for these models even across multiple runs ?\n- Disruption or disruption threshold is never defined. When it is said , 0.1% retain set disruption or 5% disruption of the retain loss, what does that mean ?\n- Later on, in Figure 3, disruption is defined as : \"cosine similarity between the model’s update on the ”Paris” fact and the other evaluated fact\". Is this the cosine similarity of the gradients between two different inputs ?\n- What is the loss that is being used when computing the gradient in Figure 3 ?\n- Please specify which layer, which model and detailed experimental setting for Fig 3. \n- In Section 3.4, the terms control updates and retaining updates are not defined. This makes it extremely hard to follow. \n- The statement - \"most representations are not specific to the fact we are trying to unlearn, but more general.\" seems to be formed from Figure 3.  This seems premature because this is obtained from a heatmap of just few activations. It is not surprising that activations of similar prompts i.e. \"The capital of {country} is\" are similar.\n- Please include utility measures. Currently the only thing measured is WIkitext loss, which is not enough to capture the retaining of utility. \n- In Figure 1 and 5, it seems Gradient Difference is unable to unlearn properly. This is surprising considering that it is an aggressive method Please clarify this discrepancy. \n\n\nI believe the overall idea is valuable, specifically from a difference-in-means [1] lens.\\\nI strongly urge the authors to improve on the current version with the goal of improving clarity. \n\n\n[1] Arditi, Andy, et al. \"Refusal in language models is mediated by a single direction.\" Advances in Neural Information Processing Systems 37 (2024): 136037-136083."}, "questions": {"value": "Please see Weaknesses Section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "u2pi0Uz69R", "forum": "QSxnB2PXJb", "replyto": "QSxnB2PXJb", "signatures": ["ICLR.cc/2026/Conference/Submission6067/Reviewer_Pq1a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6067/Reviewer_Pq1a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762037706304, "cdate": 1762037706304, "tmdate": 1762918440934, "mdate": 1762918440934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}