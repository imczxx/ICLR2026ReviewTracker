{"id": "BAskirM6hM", "number": 339, "cdate": 1756735943175, "mdate": 1759898267076, "content": {"title": "BachVid: Training-Free Video Generation with Consistent Background and Character", "abstract": "Diffusion Transformers (DiTs) have recently driven significant progress in text-to-video (T2V) generation. However, generating multiple videos with consistent characters and backgrounds remains a significant challenge. Existing methods typically rely on reference images or extensive training, and often only address character consistency, leaving background consistency to image-to-video models. We introduce BachVid, the first training-free method that achieves consistent video generation without needing any reference images. Our approach is based on a systematic analysis of DiT’s attention mechanism and intermediate features, revealing its ability to extract foreground masks and identify matching points during the denoising process. Our method leverages this finding by first generating an identity video and caching the intermediate variables, and then inject these cached variables into corresponding positions in newly generated videos, ensuring both foreground and background consistency across multiple videos. Experimental results demonstrate that BachVid achieves robust consistency in generated videos without requiring additional training, offering a novel and efficient solution for consistent video generation without relying on reference images or additional training.", "tldr": "BachVid", "keywords": ["video generation", "training-free", "consistent background and character"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2cc0055b71e7193f3a42fd734a1d31924f27249a.pdf", "supplementary_material": "/attachment/bccf471799f0f8cddc710b049001a19b650d4d7d.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents BachVid, a novel method for generating multiple videos from text prompts while maintaining consistency in both the character and the background. The authors claim this is the first training-free method to achieve this dual consistency without requiring any reference images. Experiments show the method improves background consistency and text alignment compared to baseline models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an unsolved challenge in video generation: maintaining dual (character and background) consistency across multiple videos.\n2. Quantitative results show that BachVid achieves a state-of-the-art PSNR-BG and CLIP-BG, demonstrating its effectiveness.\n3. The paper rightly identifies the OOM risk of caching all intermediate features. The vital layers determination step is a practical and necessary."}, "weaknesses": {"value": "1. The paper compares BachVid (which targets dual consistency) against ConsisID and TPIGE, which are methods designed only for character consistency. It is therefore unsurprising that BachVid wins on background metrics. A more convincing evaluation would require comparing against a stronger baseline, perhaps by adapting a dual-consistency image method (like CharaConsist, which the paper cites) to the video domain, or at least providing a thorough analysis of why such an adaptation is intractable.\n2. The results in Table 1 show that BachVid performs worse on identity consistency (Face-Arc) than the SOTA baselines (36.93 vs. 41.70 for TPIGE). The authors dismiss this as not contradictory and comparable, but a nearly 12% drop is significant. This suggests a critical trade-off: the method may be achieving background consistency at the expense of character consistency, which undermines the dual consistency claim. This trade-off is not adequately analyzed.\n3. The method is evaluated on a small, custom dataset of 40 prompt groups generated by an LLM (DeepSeek). There is no evaluation on a standardized, public benchmark. This makes it difficult to assess the generalizability of the results. The method may be implicitly overfitted to the paper's specific prompt structure: [Background], [Character], [Action].\n4. The paper frames its reference-free nature as a strength. However, for most practical applications, a user wants to provide a reference image of a specific person. By not supporting reference images, the method's practical utility is severely limited. This should be presented as a clear limitation, not just a design choice.\n5. The method relies on a set of magic numbers (specific layers and specific timesteps) derived from analyzing one model (CogVideoX). The paper claims the analysis applies across DiT-based video generation models but provides no evidence to support this. It is highly likely these optimal layers/timesteps are model-specific."}, "questions": {"value": "The paper requires significant revision before it can be considered for publication:\n1. A much more robust evaluation, ideally against a baseline adapted to perform dual consistency, or at minimum, a thorough ablation showing why existing methods fail so catastrophically at background consistency.\n2. Results on a more diverse and ideally standardized benchmark.\n3. A dedicated analysis section on the (Face-Arc) performance drop.\n4. An analysis of how the chosen vital layers and timesteps generalize (or don't) to at least one other DiT-based video model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E2n493vaGq", "forum": "BAskirM6hM", "replyto": "BAskirM6hM", "signatures": ["ICLR.cc/2026/Conference/Submission339/Reviewer_fmny"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission339/Reviewer_fmny"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761359198309, "cdate": 1761359198309, "tmdate": 1762915496826, "mdate": 1762915496826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BachVid, the first method capable of generating videos with consistent backgrounds and characters without requiring any training or reference images. By analyzing the Video Diffusion Transformer (DiT), the approach reveals its intrinsic ability to spontaneously extract foreground masks and identify matching points during the denoising process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is entirely training-free and does not rely on any reference images. While ensuring high efficiency, it simultaneously addresses the dual-consistency challenge of both characters and backgrounds."}, "weaknesses": {"value": "1. In quantitative experiments, BachVid does not surpass existing identity-preserving methods (such as TFIGE and ConsisID) on the identity consistency metric (Face-Arc). Can BachVid's background consistency mechanism be combined with the character consistency mechanisms of these methods, and would the resulting performance be better than that of the proposed approach?\n\n2. Although BachVid requires no training, it relies on a static \"identity video\" as a template. When the actions in subsequent videos differ significantly from those in the identity video, simple pixel-level matching and injection may fail.\n\n3. Please elaborate on the advantages of the proposed method in the paper compared to the following approach. This baseline method leverages DiffTrack's temporal correspondences to dynamically warp the selectively cached Key-Value pairs from CharaConsist, thereby maintaining character appearance consistency throughout motion sequences. This approach effectively transitions from static identity pasting to dynamic identity fitting."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xozzpNoRDI", "forum": "BAskirM6hM", "replyto": "BAskirM6hM", "signatures": ["ICLR.cc/2026/Conference/Submission339/Reviewer_64db"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission339/Reviewer_64db"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736217686, "cdate": 1761736217686, "tmdate": 1762915496664, "mdate": 1762915496664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BachVid, a training-free method that enforces both background and character consistency across multiple text-to-video generations from a Video Diffusion model. Key ideas: (1) extract foreground masks and temporal correspondences from DiT attention/outputs at select layers/timesteps, (2) generate an identity video and cache selected intermediate key/value/attention outputs, (3) inject (mapped & RoPE-shifted) key-values into subsequent generation runs at matched token positions while masking attention to preserve foreground/background separation. The paper presents layer/timestep analyses, a memory–vital-layer selection strategy, qualitative results, and metrics on a synthetic DeepSeek prompt suite. Overall, the work claims the first training-free, reference-image-free approach to obtain both background and character consistency for DiT-based T2V."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a concrete and useful gap: multi-video consistency (both background and character) without training or reference images.\n2. Systematic analysis of which DiT layers / timesteps encode masks and correspondences is useful and could inform other work.\n3. The vital layer selection to bound cached KV storage is practical."}, "weaknesses": {"value": "1. The core mechanism (cache keys/values from an identity and inject later) closely follows prior image methods (e.g., CharaConsist), the novelty for video is primarily empirical and in heuristics (layer, timestep selection, mapping). The paper should more clearly state what is fundamentally new versus prior work and justify why the video extension is non-trivial.\n2. Choices such as the first 15 layers and \\tau_{mask}, \\tau_{match} appear ad hoc. The paper lacks principled selection criteria, sensitivity analysis, or automated selection procedures. Please provide examples showing how performance varies with these hyperparameters.\n3. Caching and injecting KV for video-size latents can be expensive. The paper does not report runtime, GPU memory, or latency comparisons vs. baselines (and vs. vanilla generation). This is critical given the training-free claim, users need to know cost tradeoffs.\n4. The evaluation uses DeepSeek-generated prompts (synthetic). There is limited evidence this reflects real-world prompts or diverse scenes. Moreover, there may be a risk of overfitting the mode ([Background], [Character], [Action]). No experiments on external benchmarks or human-created prompts. Provide tests on the external, human-authored prompt set.\n5. Baselines (ConsisID, TPIGE) are identity-preserving and require a reference image; the authors feed them crops from the identity video. This is reasonable but not fully comparable because those methods are optimized for identity. The paper lacks comparison to other possible baselines: image-based key-value injection (CharaConsist) extended frame-by-frame.\n6. Is the reference-free property really an advantage? In fact, reference-based designs offer greater customizability."}, "questions": {"value": "The main concerns have been provided in the weaknesses section, here are some more minor issues:\n1. No discussion or experiment addresses ambiguous matches, occlusions, one-to-many mappings, or when the identity and frame differ significantly in pose/occlusion. These are common in video, robustness tests are needed.\n2. Missing failure cases."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a3jqPtoxCd", "forum": "BAskirM6hM", "replyto": "BAskirM6hM", "signatures": ["ICLR.cc/2026/Conference/Submission339/Reviewer_2fRy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission339/Reviewer_2fRy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925024188, "cdate": 1761925024188, "tmdate": 1762915496452, "mdate": 1762915496452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BachVid, a training-free method to generate multiple videos with a consistent character and background. The core idea is to generate one \"identity\" video and then reuse its internal features (keys and values) from specific layers of the diffusion model during the generation of new videos to enforce consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper tackles an practical problem: generating videos with consistency in both the character and the background\n- The systematic analysis of the video DiT internal mechanisms is a nice contribution. Pinpointing which layers and timesteps are most effective for foreground mask extraction, point matching, and key-value injection provides valuable insights that could be useful for other related video generation and editing tasks."}, "weaknesses": {"value": "- The qualitative results, particularly the video quality shown in the supplementary material, appear quite low and suffer from artifacts. Since the method is presented as a general, training-free technique, it is surprising that it wasn't demonstrated on more powerful, state-of-the-art open models (e.g., Wan 2.2). Testing only on CogVideoX-5B makes it hard to judge if the method is truly generalizable or if its effectiveness is limited to a specific model architecture.\n- The motivation for such a complex, training-free approach is not well-justified against simpler, well-established alternatives. A more straightforward way to achieve character and background consistency is to use efficient finetuning techniques like LoRA. Finetuning a LoRA on a custom character and background is very effective, and collecting the necessary data (a few images or a short video) is often cheap and easy. The paper needs a stronger argument for why this complex feature-injection method is preferable to a more direct finetuning approach.\n- The quantitative results for identity consistency are a significant concern. The paper's own results in Table 1 show that BachVid scores lower on the Face-Arc metric (36.93%) than both baseline methods it compares against (39.54% and 41.70%). While the paper aims for both background and character consistency, underperforming on a key metric for character identity weakens the overall claim of success and questions the trade-offs made."}, "questions": {"value": "The process for determining the \"vital layers\" for KV injection is based on an aesthetic score. This process needs more detail. Could you specify how general this set of vital layers is?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Biwm92DBxp", "forum": "BAskirM6hM", "replyto": "BAskirM6hM", "signatures": ["ICLR.cc/2026/Conference/Submission339/Reviewer_yqi6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission339/Reviewer_yqi6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission339/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959282612, "cdate": 1761959282612, "tmdate": 1762915496314, "mdate": 1762915496314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}