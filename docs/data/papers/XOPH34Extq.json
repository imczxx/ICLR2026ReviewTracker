{"id": "XOPH34Extq", "number": 2769, "cdate": 1757244153724, "mdate": 1763584051486, "content": {"title": "TabStruct: Measuring Structural Fidelity of Tabular Data", "abstract": "Evaluating tabular generators remains a challenging problem, as the unique causal structural prior of heterogeneous tabular data does not lend itself to intuitive human inspection. Recent work has introduced structural fidelity as a tabular-specific evaluation dimension to assess whether synthetic data complies with the causal structures of real data. However, existing benchmarks often neglect the interplay between structural fidelity and conventional evaluation dimensions, thus failing to provide a holistic understanding of model performance. Moreover, they are typically limited to toy datasets, as quantifying existing structural fidelity metrics requires access to ground-truth causal structures, which are rarely available for real-world datasets. In this paper, we propose a novel evaluation framework that jointly considers structural fidelity and conventional evaluation dimensions. We introduce a new evaluation metric, global utility, which enables the assessment of structural fidelity even in the absence of ground-truth causal structures. In addition, we present TabStruct, a comprehensive evaluation benchmark offering large-scale quantitative analysis on 13 tabular generators from nine distinct categories, across 29 datasets. Our results demonstrate that global utility provides a task-independent, domain-agnostic lens for tabular generator performance. We release the TabStruct benchmark suite, including all datasets, evaluation pipelines, and raw results.", "tldr": "We propose TabStruct, a comprehensive benchmark, along with a novel metric, global utility, for evaluating the structural fidelity of tabular data without requiring access to ground-truth causal structures.", "keywords": ["Tabular data", "Tabular data structure", "Synthetic data generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cc45dab6a95c3b871307ea40d81fce64ff75e32.pdf", "supplementary_material": "/attachment/82132363f399cab25d1ec5789955e73b41c734d4.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a benchmarking framework including a novel metric for tabular data generators. The benchmarking framework is applied to 29 tabular SCM-based or real-world datasets and 13 generators.\n\nThe work has several contributions:\n* A good motivation for introducing yet another benchmark for tabular data generators.\n* A novel way of assessing the performance of data generators, based on structural fidelity, inspired by a causal perspective.\n* A benchmarking framework with datasets, generators, and the evaluation pipeline. \n* Many insights into existing failure modes and strengths of tabular data generators. \n* An extreme commitment to reproducibility and scientific rigor in code and all parts of the paper. \n* A detailed and extended overview of related work for all parts of the work, from motivation to benchmark to insights."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The authors have executed all the above-mentioned contributions with an extremely high quality. The motivation and reasons for the benchmark are novel and original, and well related to prior work. The work seems highly significant, as it improves upon the benchmarking landscape for tabular data generators and may finally deliver actionable insights that allow us to identify good, usable generators for practical applications. The paper is written very clearly, and all information is easily accessible to the reader."}, "weaknesses": {"value": "There are no notable weaknesses as far as I can tell. Some of my questions might allude to potential weaknesses, but nothing concrete enough to mention here."}, "questions": {"value": "* Using SCM-based datasets to benchmark tabular data generators is well motivated. Yet, it is hard to tell if datasets, for example, about physical laws, are representative of real-world tabular data. Real tabular data is often noisier than datasets we have / can get from data with a ground-truth SCM. Thus, it is hard to predict how useful and representative the subset of SCM-based data will be for TabStruct in the future to guide generator development. Have the authors considered ways to get more such data or more realistic data? For example, priors of tabular foundation models such as TabPFN often use SCM-based data for pretraining that might be more realistic. Likewise, they usually add noise to the SCMs' data. \n* A core use of SCM-based data could be, as mentioned, to check for extrapolation instead of interpolation. This would also point to potentially benchmarking an entirely new kind of generators, that is, generators that can generate non-IID data. This would be akin to model benchmarks for non-IID data, e.g., https://arxiv.org/abs/2406.19380, rather than purely IID data, e.g., https://arxiv.org/abs/2506.16791. Given this comparison, TabStruct and the SCM-based data could enable benchmarks for non-IID data generators, and the Global CI would be appropriate for this. At the same time, Global utility might need to use non-IID models/validation splits. Was this intended by the distinction of interpolation vs extrapolation in Figure 1? And how does this framing position the current benchmark? \n* The current formulation of the Global utility metric utilizes a binary output of the CI test. While theoretically well motivated, I was wondering if a more continuous output of the CI test might not be better suited. For example, one could return the alpha value at which the test fails, rather than 0/1. Then, the \"decision boundary\" of the Global utility would be smoother. Moreover, it would be more similar to the smoothness of global utility (which averages over continuous values) and maybe provide a higher correlation. Or, one could use a similar significance test in global utility, such as a Wilcoxon test at alpha 0.01 to test if Perf(D_ref) is equal to Perf(D) over samples or bootstraps. \n* Why was balanced accuracy chosen as a metric for classification tasks? It is well established in model benchmarks that metrics such as ROC AUC for binary and log loss for multiclass classification are more appropriate for comparing models (e.g., see https://arxiv.org/abs/2506.16791 and its cited/related benchmarks and https://pages.cs.wisc.edu/~dpage/cs760/roc.pdf). I think here, a non-threshold-based metric would also be more appropriate. The impact of this choice might be less severe because AutoGluon was used, which, if optimized for balanced accuracy, should have employed threshold tuning by default (as required for evaluation for balanced accuracy). Do you know if threshold tuning was used in your experiments? On that note, the current formulation could also create some numerical problems for outliers where RMSE approaches 0. It might be worth improving the implementation with safeguards against numerical issues when computing relative metrics. \n* The description of nested validation in Line 314 is very hard to parse, in my opinion. The appendix and text make it clear enough later. But the work could benefit from shortly introducing the validation method in the first sentence of the paragraph. I think in this case, a nested repeated shuffle split would be one way to describe it (?).  Furthermore, is stratification used for splits for classification datasets? If not, this might be important for imbalanced classification datasets in the future. \n* How does the framework control randomness and seeds of the methods, data splits, and tuning?\n* The correlation analysis in Section 4.1 is nice and insightful, but it remains unclear if the correlation is high enough to be representative enough for a good benchmark. There might be ways to test or verify that the correlation is sufficiently high by checking if model rankings are also correlated well enough -- especially for generators that have a very high, similar performance. Have you looked into such experiments next to the results in the appendix (Figure 6 etc)? The answer might depend on how valuable the small model difference might be, or if preserving the ranking of models is required. The danger is that developers might start to optimize for a potentially noisy metric when comparing only the best models or marginal improvements over the best models. \n* The preprocessing as described in Appendix D.2 might be very suboptimal for some of the ML models from AutoGluon (as they expect to do their own model-specific preprocessing). But this make-everything-continuous preprocessing is needed for many of the generators, correct?  \n* As a final question / side note, a public, easy-to-share leaderboard of the methods might be cool, such as a Hugging Face leaderboard."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KWyO8ZLbnP", "forum": "XOPH34Extq", "replyto": "XOPH34Extq", "signatures": ["ICLR.cc/2026/Conference/Submission2769/Reviewer_AASQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2769/Reviewer_AASQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916510364, "cdate": 1761916510364, "tmdate": 1762916370575, "mdate": 1762916370575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response to all reviewers"}, "comment": {"value": "We thank the reviewers for the very constructive feedback!\n\n### **(1) Summary of positive things**\n\n- **Strong motivation and potential for high impact**\n    - `njCP`: *“a very relevant task for a broader audience at ICLR”; “much-needed research direction”*\n    - `7rse`: *“identifies a key gap/blind spot”; “addresses the very important (and difficult) problem”;*\n    - `mx31`: *“problem is highly relevant”; “identified a significant gap”*\n    - `AASQ`: *\"novel and original, and well related to prior work”; “highly significant”; “deliver actionable insights“*\n- **Novel SCM-free evaluation of structural fidelity**\n    - `njCP`: *“a new metric”;“enables the study of structural fidelity for real-world data”*\n    - `7rse`: *“a new evaluation dimension … and a new metric”;*\n    - `mx31`: *“the proposed metric is novel (global utility) and interesting”*\n    - `AASQ`: *“A novel way of assessing the performance of data generators”*\n- **Comprehensive and rigorous empirical study**\n    - `7rse`: *“a comprehensive benchmark”; “extensive evaluations”; \"experimental setup is as rigorous as it gets”*\n    - `mx31`: *“A comprehensive benchmark”; “rigorous and comprehensive”; “commendable … chose some challenging datasets”*\n- **Insightful analyses supported by strong evidence**\n    - `7rse`: *“corroboration to many observations in previous studies”; “new and valuable insights”*\n    - `mx31`: *“the paper provides strong empirical evidence”*\n    - `AASQ`: *“Many insights into existing failure modes and strengths”*\n- **Clear and well-organised presentation**\n    - `njCP`: *“enjoyed reading the paper”*\n    - `7rse`: *“very careful and detailed analyses”; “very thorough discussion”*\n    - `AASQ`: *“A detailed and extended overview of related work”; “written very clearly”; “all information is easily accessible”*\n- **High standards of reproducibility**\n    - `7rse`: *“easily plug in new datasets, generators, and metrics”; ”great resource to the community.”*\n    - `mx31`: *“a valuable open-source contribution”*\n    - `AASQ`: *“An extreme commitment to reproducibility and scientific rigor in code and all parts of the paper.”*\n\n### **(2) Summary of our responses and new experiments**\n\nWe replied to **all** questions and concerns raised by the reviewers. Specifically, we provide **7 new experiments**. The new results are consistent with the main text and further support the validity of the proposed TabStruct and global utility.\n\n*Table R0*. Number of our responses to the reviewers’ comments (`#Raised`/`#Replied`).\n\n| Reviewer | # Weaknesses  | # Questions | # New experiments |\n| --- | --- | --- | --- |\n| `njCP` | 2/2 | 2/2 | 2 |\n| `7rse` | 1/1 | 3/3 | 2 |\n| `mx31` | 4/4 | 3/3 | 1 |\n| `AASQ` | 0/0 | 9/9 | 2 |\n\n### **(3) Summary of updates to the manuscript**\n\nWe have revised the manuscript according to the feedback. In the revised manuscript, all changes are highlighted in blue. Below, we summarise the updates:\n\n**Main text**\n\n- `[njCP]` (throughout the main text): Minor edits to ensure consistency in text styles.\n- `[mx31]` (Section 3.3, Section 4.2, and Section 4.4): Additional emphases on the empirical relationship between global CI and the proposed global utility.\n- `[mx31]` (Section 3.2): Expanded explanation of the rationales behind CPDAG-level evaluation.\n- `[AASQ]` (Section 4): Clarifications on the data-splitting strategy employed in TabStruct.\n\n**Appendix**\n\n- `[njCP]` (Appendix E.3): A fine-grained analysis of utility score distributions across different features.\n- `[njCP]` (Appendix E.2): An ablation study examining the stability of the proposed global utility under varying synthetic sample sizes.\n- `[7rse]` (Appendix E.1): An extended discussion on detection score C2ST and structural fidelity.\n- `[7rse]` (Appendix E.6): Additional discussion and preliminary experiments exploring the concept of separate independent sets.\n- `[mx31]` (Appendix E.4): An ablation study investigating the robustness of the proposed global utility across varying levels of data availability, with a particular focus on low-data scenarios.\n- `[AASQ]` (Appendix E.4): An extended discussion on the variants of global CI and the proposed global utility.\n- `[AASQ]` (Appendix D.2): Clarifications on the data preprocessing protocols used by TabStruct.\n- `[AASQ]` (Appendix E.2): An extended analysis of the correlation between generator rankings induced by global CI and the proposed global utility."}}, "id": "nAfExfzMM0", "forum": "XOPH34Extq", "replyto": "XOPH34Extq", "signatures": ["ICLR.cc/2026/Conference/Submission2769/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2769/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2769/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763583416941, "cdate": 1763583416941, "tmdate": 1763583416941, "mdate": 1763583416941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of evaluating tabular data generators. The paper argues that the existing benchmarks are insufficient because they either neglect the structural properties of the tabular data, or are biased towards downstream task performances, and are often limited to toy examples where the causal graph is known.\n\nThe paper proposes to incorporate structural fidelity as a core evaluation dimension, and introduces a metric called, \"global utility\", which is an SCM-free metric for assessing structural fidelity. A comprehensive benchamark \" TabStruct\" is proposed to evaluate 13 generators across 29 datasets including real-world and SCM-based data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ the proposed problem is highly relevant. The paper identified a significant gap in the evaluation of tabular generative models and moves beyond simple ML efficacy (performance on downstream tasks) and density estimation\n+ the proposed metric is novel (global utility) and interesting. \n+ the empirical validation is rigorous and comprehensive with 13 generators from 9 categories and 29 datasets. It is also commendable that the authors chose some challenging datasets where the models don't easily achieve perfect scores.\n+ Tabstruct library is a valuable open-source contribution"}, "weaknesses": {"value": "- The global utility metric is a heuristic proxy, not a formal measure. Although the paper provides strong empirical evidence it is important to acknowledge that it is a proxy. Although it is acknowledged in the paper, it should probably be emphasized more. \n- The metric measures \"predictability\" rather than the \"causal structure\". A generative model could learn powerful but spurious correlations that allow for excellent cross-prediction of variables. This would make it score high on global utility, but be structurally unfaithful.\n- The \"full-tuned\" configuration of the global utility metric requires training an ensemble of predcitors for each of the columns in the dataset. That is computationally expensive, especially for high-dimension data. Although the \"tiny-default\" is much faster and seems to be stable ranking wise, the fundamental approach is very expensive and this could lead to scalablity issues in practice.\n- The finding that the BAyesian networks and GOGGLE perform poorly on structural fidelity metrics is counter-intuitive. Although there is some explanation in the paper, this should probably be probed more deeply. Where is the failure happening?"}, "questions": {"value": "1. Could you elaborate on the theoretical gap between high global utility and the preservation of the Markov equivalence class? Are you aware of any hypothetical data-generating processes where a model could achieve high global utility while  violating key conditional independencies? How likely is this to happen in practice?\n2. The high correlation between the global utility and the global_CI is taken as the validation of the global utility; but, the global_CI score is an imperfect ground truth because  the CI test have limitations themselves. So how does this inherent noise and potential inaccuracies of the CI-based ground truth affect your confidence in the correlation values and the conclusions you draw from it?\n3. global utility is normalized by the performance on the reference data. How does this metric behave in low-data regimes? What is the relationship between the reliability of the global utility and the size and quality of the reference dataset?\n. How does the computational cost of global utility scale with the number of features?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yzBFxScRfc", "forum": "XOPH34Extq", "replyto": "XOPH34Extq", "signatures": ["ICLR.cc/2026/Conference/Submission2769/Reviewer_mx31"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2769/Reviewer_mx31"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916766472, "cdate": 1761916766472, "tmdate": 1762916370359, "mdate": 1762916370359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a comprehensive benchmark for evaluating tabular generative models. It identifies a key gap in existing evaluation measures and proposes new metrics to address this issue. While most evaluation metrics focus on dimensions such as density estimation, privacy preservation, or machine learning efficiency, these metrics are not tailored to assess whether synthetic data preserves the underlying structural (causal) relationships among variables. To address this, the paper proposes a new evaluation dimension called structural fidelity, and a new metric, global CI, to measure this new dimension.  Because, this metric requires knowledge about the underlying structural causal model (SCM), which is almost always unknown for real datasets, the paper also introduces a SCM free heuristic metric, global utility, which correlates strongly with global CI and can be used as a proxy to measure structural fidelity.  \n\nThe paper performs extensive evaluations using 13 tabular generators across 29 datasets (including both expert-validated causal and real-world datasets) and provides many interesting insights about the performance of popular tabular generators and of the evaluation metrics."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper addresses the very important (and difficult) problem of evaluating the performance of tabular data generators.\n\nIt identifies a blind spot (structural fidelity) in the current evaluation metrics and proposes new metrics to measure it.\n\nThe experimental setup is as rigorous as it gets.\n\nThe very careful and detailed analyses of both the different generators and the different metrics provide corroboration to many observations in previous studies, as well as several new and valuable insights. \n\nThe paper provides a very thorough discussion of the limitations of the proposed metrics. \n\nThe framework’s modular design allows researchers to easily plug in new datasets, generators, and metrics, making it a great resource to the synthetic tabular data generation community."}, "weaknesses": {"value": "The one point I think is missing from the paper is a comparison against the detection test metric (C2ST), which measures data fidelity by evaluating the discriminating ability of classifiers trained to discriminate between synthetic and real data. C2ST is a very popular metric, widely used in the field, which also aims to measure fidelity at a full dataset scale.  \n\nGiven the large scale of the experiments presented on the paper, I understand that it would likely be difficult to include extensive comparisons during the short time frame of the discussion window. However, I think the paper would benefit from providing at least some preliminary comparisons of C2ST against the proposed global utility metric."}, "questions": {"value": "Some of the benchmark datasets are very large (e.g., Higgs, and SCM datasets). How did the paper handle the TabPFN model in situations where the evaluation dataset had more than 10,000 rows? Was TabPFN removed from the predictor’s ensemble in these cases? (My understanding is that the current version of TabPFN can only handle datasets with up to 10,000 rows.)\n\nThis is only a suggestion, that the paper might find useful for future work. In Table 2, in addition to the 13 generators, the paper includes the reference (training) data, $D_{ref}$, for direct evaluation. This helps better ground the interpretation of the scores achieved by the generators, showing that the metrics can distinguish between high- and low-quality data.  But, perhaps, another interesting comparison would be to include direct comparisons not only to the reference set but also to a separate independent set (of the same size as the reference set) from each dataset (which is not touched during training). Since the independent set, $D_{indep}$, is by construction independent and identically distributed to the reference set, it could be used to estimate the performance of an ideal generator, truly capable of generating independent samples from the same distribution as the reference data. \n\nHaving these comparisons would potentially provide complementary information for the $D_{ref}$ direct evaluations. For instance, in the case of privacy metric such as the DCR, while $D_{ref}$ will produce a score equal to zero, $D_{indep}$ would estimate the median DCR score we would expect to see in an ideal generator. This would represent a baseline value and sort of “ground truth” value the generators should be aiming to. (While low DCR values indicate low privacy, high DCR scores are not necessarily a good thing either, since they usually indicate low data fidelity.) Additionally, the direct evaluation of $D_{indep}$ could also be potentially used to check when models are generating data that is too close to the training set (e.g., when a model is generating data $D_{syn}$ with better fidelity scores than $D_{indep}$). \n\nTo generate this independent set the paper would need to change its current data split and re-split the full dataset of each benchmark into a test set, a validation set, a training (reference) set, and a independent set of the same size as the training set. While this would require re-running all the analyses in that paper (what is certainly not feasible during the short discussion phase) perhaps the paper might want to consider implementing these comparisons in the future, as they might provide additional insights (and the paper aims for TabStruct to be an ongoing effort aiming to continue to evolve). \n\nMinor suggestions:\n\nLine 1515: “TarStruct” -> “TabStruct”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1FyFqaqft1", "forum": "XOPH34Extq", "replyto": "XOPH34Extq", "signatures": ["ICLR.cc/2026/Conference/Submission2769/Reviewer_7rse"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2769/Reviewer_7rse"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973667833, "cdate": 1761973667833, "tmdate": 1762916369184, "mdate": 1762916369184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a benchmark framework to assess methods for generating synthetic data based on existing datasets. The main novelty is that it introduces a metric to measure structural fidelity based on a global utility score without the need for ground-truth causal graphs. This enables the study of structural fidelity for real-world data where such graphs are typically unavailable. In empirical experiments, they first demonstrate a high correlation between the new metric and existing metrics on synthetic tasks (where the ground truth is available) and then assess the performance of different generators on real-world tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Novelty** The paper introduces a new metric that complements the assessment of data generators on real-world tasks\n\n**Clarity** I enjoyed reading the paper as it is straightforward, with a good structure\n\n**Impact** Studying tabular generators is a very relevant task for a broader audience at ICLR and a much-needed research direction directly related to developing better foundation models for tabular data"}, "weaknesses": {"value": "**Aggregation of Metrics** I understand the importance of aggregating metrics to provide concise and interpretable results. However, a considerable amount of information may be lost in the aggregation process. It would be valuable to include some indication of the variability or distribution of utility scores across features. For instance, are there notable outliers where the generator fails to capture feature usefulness, or are all features consistently represented? Such an analysis could strengthen the empirical insights.\n\n**Readability** (This did not influence my rating, as it is straightforward to address.)\nThe paper employs multiple text styles (boldface, italics, color) without a clear or consistent rationale. For example, in lines 117-131, it is unclear why certain words are boldfaced. Similarly, the colored text and structure of (the huge) Table 2 do sufficiently aid visual comprehension or highlight the key messages. For example, consistent colors for the Top-3 and Bottom-3 results would help visualize similarities across columns. Moreover, showing real-world and SCM results for each metric side-by-side would support following the content of Section 4.2.\n\n**Ethics Statement** This is not an ethics statement but a summary/conclusion."}, "questions": {"value": "[clarification] Would reaching a utility score >1 be possible?\n\n[clarification] What is the size of the generated dataset D_synth with respect to D_ref? I assume they are the same size, but it might also be interesting to analyze how the proposed metrics behave for larger or smaller synthetic datasets relative to the reference data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ADpvGKWJQY", "forum": "XOPH34Extq", "replyto": "XOPH34Extq", "signatures": ["ICLR.cc/2026/Conference/Submission2769/Reviewer_njCP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2769/Reviewer_njCP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2769/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994132610, "cdate": 1761994132610, "tmdate": 1762916366570, "mdate": 1762916366570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}