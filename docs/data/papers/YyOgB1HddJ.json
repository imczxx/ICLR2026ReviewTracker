{"id": "YyOgB1HddJ", "number": 10114, "cdate": 1758161040032, "mdate": 1759897672878, "content": {"title": "Reversible Watermark-Guided Data Obfuscation to Prevent Exploitation by Unauthorized Models", "abstract": "The widespread availability of ``free'' data on the Internet has been key to recent advances in deep learning. However, it also raises serious concerns about the unauthorized use of public data to train commercial models. This paper investigates a critical issue in the domain of data privacy and deep learning: \\textit{Can data be made unexploitable to unauthorized models while being recoverable for authorized models?} To address this issue, we propose Reversible Data Obfuscation, the first method that leverages watermark in a reversible manner to control data usability for target models. Specifically, we employ a gradient-based search (GBS) combined with constraint-guided obfuscation (CGO) strategy to select watermarkable tokens, generating watermarked obfuscated data that remains imperceptible to humans while preserving normal data utility. For unauthorized models, watermarked obfuscated data increases the discrepancy between predicted and ground-truth labels, thereby effectively degrading model performance. In contrast, authorized models can extract the watermark to recover the original high-quality data for effective training. Experimental results on classification tasks show that models fine-tuned on the watermarked obfuscated data experience severe performance degradation, with accuracy dropping below $50\\%$. Moreover, the performance of unauthorized models degrades progressively with repeated embedding. The obfuscated data preserve high semantic similarity with the original data, and the embedded watermarks can be reliably extracted for ownership verification. Our work establishes an important step towards making data unexploitable to unauthorized models while remaining exploitable to authorized models.", "tldr": "", "keywords": ["Privacy", "natural language watermarking", "data obfuscation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/348ca372d22ee71d7ac321aa3c4bed6d15f7bde5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Reversible Data Obfuscation (RDO) for text: embed a reversible watermark via gradient-based search (GBS) to pick influential tokens and a constraint-guided obfuscation (CGO) rule to substitute them with top-k alternatives, so unauthorized models trained on the obfuscated corpus perform worse, while authorized users can extract the watermark and recover the original text. Claims include being the first to leverage reversible watermarking to control data usability, the two-stage GBS+CGO selection, and iterative embedding that increases capacity and progressively degrades downstream fine-tuning accuracy. Experiments on IMDB and AGNews with BERT/Qwen show sizable drops (the paper even reports sub-50% AUC/F1/accuracy in some settings), high semantic similarity by SBERT, “100%” recoverability without attack, and 92% bit-recovery under single-token tampering."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The GBS (importance via ∂ℓ/∂e) + CGO (accept only if loss increases and label flips) is simple, reproducible, and tied to reversibility through top-2 substitution. Algorithms are explicit. \n\n- The extraction/recovery routine is spelled out; under no attack the authors claim full recovery.\n\n- Capacity (bpw) rises with iterations and correlates with downstream degradation; the comparison to KSPEE is a useful touchstone. \n\n- Obfuscations crafted with (fine-tuned) DistilBERT still hurt BERT/Qwen, which suggests some transfer."}, "weaknesses": {"value": "- Novelty positioning / missing citation. The paper repeatedly frames itself as “first” to use watermark-guided reversible hiding for usability control in text, but hiding-based unlearnable examples already exist in vision (e.g., Semantic Deep Hiding for Robust Unlearnable Examples, TIFS 2024). This related line is not cited or contrasted; the “first” claim should be scoped and toned down.\nRequested point: Prior work using hiding to generate unlearnable examples in the image domain (TIFS 2024) exists and is not referenced here. \n\n- CGO accepts edits only when they increase loss and flip the predicted label; reversibility assumes the original token is in the top-2 at both embed and extract time. Paraphrasing, back-translation, grammar/style filters, or a different tokenizer can easily move the original out of top-2, undermining both robustness and recovery. The paper evaluates only single-token substitution attacks (92% bit recognition) — much weaker than paraphrase-level transformations that real crawlers or data cleaners apply.\n\n- Two datasets, mainly classification; no tests on summarization, QA, or instruction-tuning pipelines that re-tokenize and normalize text. Human perceptual studies are absent; imperceptibility rests solely on SBERT similarity. \n\n- The paper cites text unlearnables (e.g., Li & Liu ’23) but doesn’t compare against strong text-native UE/poisoning baselines under the same fine-tuning protocols. The KSPEE comparison is about watermark capacity, not downstream learnability."}, "questions": {"value": "See weakness as above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E9bJLEp8nX", "forum": "YyOgB1HddJ", "replyto": "YyOgB1HddJ", "signatures": ["ICLR.cc/2026/Conference/Submission10114/Reviewer_2HP9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10114/Reviewer_2HP9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632154392, "cdate": 1761632154392, "tmdate": 1762921491543, "mdate": 1762921491543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel reversible watermark-guided data obfuscation (DO) method that combines gradient-based search (GBS) and constraint-guided obfuscation (CGO) to prevent unauthorized model training while allowing data recovery for authorized users. The method significantly degrades unauthorized model performance, but only uses two datasets (IMDB, AG News), which limits its generalizability. It also lacks comparison with existing techniques in data poisoning and adversarial attacks. Further comparisons and more diverse datasets are needed to validate its effectiveness and practicality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper presents an innovative reversible watermark-guided data obfuscation (DO) method, which ensures ownership tracking and privacy protection without exposing the data content and prevents unauthorized model use.\n2. The background of this paper involves data privacy protection and intellectual property protection, which is of significant practical importance in the context of the growing reliance on large-scale datasets in deep learning models, particularly in preventing data misuse.\n3. The method is applicable to not only the IMDB and AG News datasets, but also performs effectively across various models such as Qwen and BERT, maintaining stable results across multiple classification tasks."}, "weaknesses": {"value": "1. The paper only uses the IMDB and AG News datasets, limiting the generalizability of the method. While these datasets are representative, to further demonstrate the broad applicability of the method, it should be extended to more datasets, such as multilingual tasks and cross-domain datasets.\n2. The paper mentions that watermark obfuscated data effectively prevents unauthorized training, but does not thoroughly explore the impact of adversarial attacks, especially after the data is tampered with or disturbed.\n3. The method presented in the paper is innovative, but it lacks sufficient experimental data and analysis when compared with existing data obfuscation techniques (such as data poisoning and unlearnable samples)."}, "questions": {"value": "1. The paper only uses IMDB and AG News datasets. Is there any plan to extend the method to more datasets, especially multilingual tasks and cross-domain validation, such as tasks beyond sentiment analysis?\n2. Have you considered the robustness of recovery ability across different tasks or datasets, especially when data is tampered or noisy data is involved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fSeizrkBYY", "forum": "YyOgB1HddJ", "replyto": "YyOgB1HddJ", "signatures": ["ICLR.cc/2026/Conference/Submission10114/Reviewer_KdRA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10114/Reviewer_KdRA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816991396, "cdate": 1761816991396, "tmdate": 1762921491121, "mdate": 1762921491121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the problem of unauthorized data use for training commercial deep learning models. It proposes a novel method called Reversible Data Obfuscation , which aims to make data unexploitable for unauthorized parties while remaining fully recoverable and usable for authorized parties. The core of the method is a reversible watermarking scheme that embeds \"model-sensitive perturbations\" into text. For unauthorized users, training on this obfuscated data leads to severe performance degradation, with accuracy on classification tasks dropping below 50% after iterative embedding. For authorized users, who possess the key, they can perfectly reverse the process, extract the watermark, and recover the original high-quality data for effective training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's primary strength is its novel problem formulation and solution. It bridges the gap between two previously separate fields: data poisoning/unlearnable examples and watermarking. The idea of a conditionally destructive dataset (unlearnable for unauthorized users but recoverable for authorized ones) is a original and valuable contribution.\n- The paper is exceptionally well-written and easy to follow. The high-level concept is perfectly illustrated in Figure 1, and the complex embedding mechanism is demystified in Figure 2. The methodology is further clarified with well-defined algorithms.\n- This work provides a practical, \"lock-and-key\" tool for data owners (e.g., artists, writers, researchers) to protect their public data from unauthorized scraping and exploitation by large model developers. This moves beyond passive protection (ownership verification) and destructive protection (unlearnable examples) to a more flexible and powerful form of data control."}, "weaknesses": {"value": "- The extraction process requires the authorized user to have the set of embedded positions. This \"key\" is a significant secret. The paper does not discuss how the set of embedded positions is managed, stored, or securely shared. If the set of embedded positions is simply \"the list of all indices that were successfully modified,\" an attacker with knowledge of the method might try to identify these locations (e.g., by finding tokens with low probability under a standard LM). This aspect of the threat model needs to be elaborated.\n- The robustness analysis in Appendix A.2 is a good start but is limited to a very weak attack: \"substitutes one token per sample\". A more realistic and powerful attack would be for an unauthorized user to paraphrase the entire dataset using a powerful LLM. This would almost certainly destroy the fragile token-level candidate relationships at the embedded positions, rendering extraction impossible and likely mitigating the obfuscation's effect. This is a major, unaddressed threat."}, "questions": {"value": "I have the following questions for the authors, which would help clarify the limitations and practical applicability of the work:\n- The extraction algorithm requires the set of embedded positions. How do you propose this \"key\" is managed in a real-world scenario? Is it a static list, or is it (or its starting seed) a secret shared between the owner and authorized users? If it's a static list, what prevents an attacker from trying to discover these positions?\n- A key threat not addressed is a paraphrasing attack, where an unauthorized user re-writes the dataset with an LLM before training. Have you tested the method's robustness against this? Would this not break both the watermark recovery and the obfuscation effect?\n- Could you please provide qualitative examples (like those in Table 3 ) of text samples after 5, 7, and 9 iterations of embedding? I am interested to see if the text remains \"imperceptible to humans\"  after so many cumulative modifications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jUw3mtq9V1", "forum": "YyOgB1HddJ", "replyto": "YyOgB1HddJ", "signatures": ["ICLR.cc/2026/Conference/Submission10114/Reviewer_9Dx5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10114/Reviewer_9Dx5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825469605, "cdate": 1761825469605, "tmdate": 1762921490665, "mdate": 1762921490665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}