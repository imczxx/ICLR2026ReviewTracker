{"id": "ESzbS71g3D", "number": 16477, "cdate": 1758264957594, "mdate": 1759897238229, "content": {"title": "Quantizing Space and Time: Fusing Time Series and Images for Earth Observation", "abstract": "We propose a task-agnostic framework for multimodal fusion of time series and single timestamp images, enabling cross-modal generation and robust downstream performance. Our approach explores deterministic and learned strategies for time series quantization and then leverages a masked correlation learning objective, aligning discrete image and time series tokens in a unified representation space. Instantiated in the Earth observation domain, the pretrained model generates consistent global temperature profiles from satellite imagery and is validated through counterfactual experiments. Across downstream tasks, our task-agnostic pretraining outperforms task-specific fusion by 6\\% in R$^2$ and 2\\% in RMSE on average, and exceeds baseline methods by 50\\% in R$^2$ and 12\\% in RMSE. Finally, we analyze gradient sensitivity across modalities, providing insights into model robustness. Code, data, and weights will be released under a permissive license.", "tldr": "", "keywords": ["Time-series quantization", "Cross-modal correlation learning", "Earth observation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/99ca4e5b1566baa7135b0dfd522fcfac40f04b29.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework for fusing time series data with single timestamp images that relies on quantization of the time series and image representations. In particular, the authors propose exploring a finite scalar quantization method for the time series.  The authors address the following limitations in existing approaches for fusing time series and images: they are supervised and task-specific. Instead, the correlation between time series and images is learned through encoding both modalities in a shared encoder and  learning to generate one modality from the other. The model is pre-trained on a dataset of EO imagery, TerraMesh alongside meteorological data from the NOAA Global Forecast System.\nThe pre-trained model is fine tuned on four downstream tasks of crop production"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is generally well written and clear.\nThe authors explore various methods for quantizing time.  provides some interesting analyses of the method, in particular the assessment of geo-location sensitivity. \nThe problem of fusing time series information with single timestamp image data is interesting and tackling it can be useful for many applications across agriculture, climate and biodiversity."}, "weaknesses": {"value": "My main concern is about the claim that this model is task-agnostic. \nThe authors try the method on 4 downstream tasks but they all come from the same dataset, so it's essentially one task of crop yield prediction. \nAlso there is not necessarily a big gain of performance of the model trained from scratch vs with the pre-trained weights. Therefore, I am wondering whether this claim that the model is task agnostic might be misleading. In the sense that it is an architecture that can be used for different problems , but this is probably also the case for the methods that the authors quality as task-specific. \nHave the authors tried transfer learning instead of fine-tuning? it might make more of a compelling case for the task-agnosticity of the model if the authors do not try their method on another evaluation dataset. \n\nAs stated by the authors in C.2 for daily temperature generation: \"We also acknowledge that this approach is not designed or intended to be used as a forecast, as the model more generates averages rather than it could infer the weather ten days ahead just from an optical satellite image\" \nSo if the pretext task is not of any practical relevance, it would have been good to showcase the potential of the method on another dataset on top of CropNet. If no other dataset is available to try this method, can the authors provide more concrete applications of their model?"}, "questions": {"value": "Besides answering to the point brought up in the weaknesses, I have the following questions: \n- Can you specify the computational cost of FSQ vs the other methods? \n- Can you specify details on how you split the dataset for pre-training and the evaluation of 5.2? \n- In L239-240, you specify that the model is pre-trained with different EO modalities and geolocation tokens. Is the geolocation pixel wise or there is only one location associated with each image and in that case, how are the geo-location tokens  integrated with the raster data? \n- Do you have an idea of how consistent the times at which the satellite imagery was taken? What is the distribution of timestamps of the image data? \n- Fig. 7 : Could you put the error map in celsius as well? \n- 5.4: why is the test set in the past (2020)? It is usually more common with crop yield prediction tasks to do forecasting. Also in  MMST-ViT, the task that is considered is one year ahead prediction. Would it make more sense to train on 2020, validate on 2021, evaluate on 2022? The setup that you choose is actually not the one proposed by CropNet originally. Could you clarify why you made this choice for evaluation? \n- Have the authors tried other designs for the hourly and daily datasets ? What was the rationale behind the choice of time ranges?  \n\nI am open to revising my score if the authors address my concerns. Thank you."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RTeHNZHVCN", "forum": "ESzbS71g3D", "replyto": "ESzbS71g3D", "signatures": ["ICLR.cc/2026/Conference/Submission16477/Reviewer_Saqg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16477/Reviewer_Saqg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761528004145, "cdate": 1761528004145, "tmdate": 1762926582033, "mdate": 1762926582033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a task-agnostic method for multimodal fusion between time series data and single-timestamp images with applications in teh domain of satellite images. It employs quantization strategies for time series and uses a masked correlation learning objective to align discrete tokens from both modalities within a shared representation space. Applied to Earth observation, the pretrained model generates coherent global temperature profiles during pretraining and enhances crop yield prediction downstream task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Fusing static images with timeseries is important in the domain of satellite images as it combines high-resolution spatial context with time series encode evolving dynamics.\n- Literature and methods around feature quantization are explored in detail.\n- Code and Data will be provided by the authors."}, "weaknesses": {"value": "- The modeling architecture is not exposed in detail, in particular the architecture and size, how the model understands to separate images and time series inputs are separated and how/if position is explicitly encoded. The proposed method is a pretraining method but it is compared againsta existing architectures. The baseline performance of the model in table 2 should be added to the baselines and results should be gathered for competing models as well to ensure a clear comparison.\n- The motivation for employing quantization is not clear (see questions). Alternatives are not explored.\n- The method is presented as agnostic but performance is evaluated in a single task (yield prediction).\n- Establishing model performance across several pretraining sets (ideally combinations) and downstream tasks is important to assess the generalization of the method."}, "questions": {"value": "- What is the motivation behind the choice of quantizing model inputs? Does it act as a means of corrupting and reconstructing inputs? Have you considered alternative ways of achieveing this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "435Gbt9nDS", "forum": "ESzbS71g3D", "replyto": "ESzbS71g3D", "signatures": ["ICLR.cc/2026/Conference/Submission16477/Reviewer_85Xn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16477/Reviewer_85Xn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818976597, "cdate": 1761818976597, "tmdate": 1762926581695, "mdate": 1762926581695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a task-agnostic framework for multimodal fusion between time series and single-timestamp images. The approach explores deterministic and learned quantization strategies for time series and employs a masked correlation learning objective to align discrete image and time series tokens within a shared representation space. Applied to the Earth observation domain, the model is pretrained to generate global temperature profiles from satellite imagery and evaluated through counterfactual experiments. The pretrained model reportedly outperforms task-specific fusion and baseline methods across downstream tasks such as crop yield prediction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of unifying time series and imagery into a shared latent space is interesting and potentially useful for Earth observation tasks.\n\n- The technical components, such as masked correlation learning and modality alignment, are reasonable choices."}, "weaknesses": {"value": "- The **motivation** for generating global temperature profiles is weak — “why we need global temperature profiles?” is not convincingly explained. Can't we see the date and location information of satellite imagery and check the temperature?\n\n- The role of **quantization** is unclear; the paper does not adequately justify why time series need to be quantized for cross-modal alignment.\n\n- The “quantizing time series” section is confusing and lacks logical flow between different methods.\n\n- In Section 3.2, the inclusion of a conditional diffusion decoder (Dφ) is not well motivated — it’s unclear why generation is needed in this setup.\n\n- The loss design (cross-entropy) in Section 3.3 seems inappropriate for the problem; an MSE-based objective between Y and Y_target may be more natural.\n\n- The paper evaluates only one model (TerraMesh), which is insufficient to demonstrate generality.\n\n- Downstream evaluation relies solely on the CropNet dataset, limiting the evidence for broad applicability.\n\n- The comment in Section 5.3 about the asymmetric confidence interval (“larger positive bound”) is confusing and even suggests degraded performance.\n\n- Overall, the paper reads more like a technical report than a mature ICLR submission — the writing and motivation need substantial refinement."}, "questions": {"value": "Please refer to Weaknesses.\n\nWhile the concept of fusing temporal and spatial modalities is promising, the paper lacks clarity, strong motivation, and sufficient empirical validation. The scope of experiments and explanations do not meet the standard expected at ICLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cFRMMh8YXY", "forum": "ESzbS71g3D", "replyto": "ESzbS71g3D", "signatures": ["ICLR.cc/2026/Conference/Submission16477/Reviewer_suZX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16477/Reviewer_suZX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997283435, "cdate": 1761997283435, "tmdate": 1762926581331, "mdate": 1762926581331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a task-agnostic framework for multimodal fusion of time series and single-timestamp images, enabling time-series generation from images. The approach quantizes both modalities into discrete tokens using deterministic quantile-based and learned Finite Scalar Quantization methods and then learning masked correlation objectives to align their latent spaces. This unified token-based representation allows bidirectional modality translation and self-supervised pretraining. The technique is applied to Earth Observation by aligning optical satellite imagery (TerraMesh dataset) with meteorological time series (NOAA GFS) and demonstrates that it can generate consistent global temperature profiles from satellite images without explicit geolocation data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The central idea of the paper is discretizing both time series and images into a shared token space is impactful. The approach establishes a task-agnostic, generative, and interpretable framework. Introducing Finite Scalar Quantization (FSQ) for time series is a good technical contribution. FSQ provides a stable, computationally efficient way to discretize long-tailed time series distributions. Treating tokens from different modalities as mutual predictive targets lead task-agnostic learning principle which makes the framework scalable to diverse downstream tasks.\n\nBy experiments, the authors confirm that the model learns visual-temporal correlations rather than relying on positional cues. In downstream tasks (e.g., crop yield prediction using CropNet), the pretrained model outperforms task-specific fusion and baseline models, achieving better performance compared to conventional baselines. The paper also includes gradient-based sensitivity analyses, showing that modality-specific gradients reveal robustness and identify spatial regions with weak predictive signals.\n\nThe concept of discrete multimodal alignment is not entirely unprecedented, many generative models use token alignment, though the innovation here lies in extending it to temporal signals. Hence, the novelty is incremental but well-adapted to EO and time-series domains."}, "weaknesses": {"value": "The masked correlation objective lacks a formal justification or ablation contrasting it with contrastive or mutual information-based alternatives. \nWhile FSQ is good to use, the paper does not thoroughly analyze token efficiency versus representation quality. This limits understanding of scalability.\nThe autoregressive generation may conflate spatial priors with temporal correlations. No explicit temporal grounding or causal validation is included.\nThere is no computational efficiency analysis e.g., inference cost, quantization overhead, scaling limits.\nWhile quantization improves cross-modal alignment but it may lose fine-grained temporal information which may be useful in analyzing high-frequency dynamics."}, "questions": {"value": "As in limitations"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "38btpnYHom", "forum": "ESzbS71g3D", "replyto": "ESzbS71g3D", "signatures": ["ICLR.cc/2026/Conference/Submission16477/Reviewer_eT7M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16477/Reviewer_eT7M"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762107555969, "cdate": 1762107555969, "tmdate": 1762926580936, "mdate": 1762926580936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}