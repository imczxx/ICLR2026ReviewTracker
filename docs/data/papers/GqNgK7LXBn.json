{"id": "GqNgK7LXBn", "number": 13000, "cdate": 1758212611033, "mdate": 1763704744194, "content": {"title": "Private Data Synthesis for Preference Alignment of Large Language Models", "abstract": "Preference alignment has become a crucial technique for aligning large language models (LLMs) with human values. However, training on real human preference data raises privacy concerns, as these datasets often contain sensitive user prompts and human judgments. To address this, we propose **DPPrefSyn**, a novel algorithm for generating differentially private (DP) synthetic preference data to enable privacy-preserving preference alignment. DPPrefSyn addresses three key challenges: modeling diverse human preferences via DP clustering and per-cluster DP scoring models; reducing dimensionality with DP-PCA to improve efficiency; and conserving privacy budget by leveraging public prompts. We conduct extensive experiments on three standard benchmarks and compare our method with DP fine-tuning on real data. Our results show that our framework achieves competitive performance under strong privacy guarantees. These results open up new possibilities for preference alignment with privacy protection for a broad range of applications. To the best of our knowledge, this is the first work to generate DP synthetic preference data for LLM alignment.", "tldr": "The first framework that generates differentially private synthetic preference data, enabling privacy-preserving preference alignment of large language models.", "keywords": ["Preference Alignment", "Differential Privacy", "Large Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7901961697294dd0a112f446cf34093ae1efdcb.pdf", "supplementary_material": "/attachment/8e7757e9febd95bb3f584aa87d55170369e754d2.zip"}, "replies": [{"content": {"summary": {"value": "The paper studied the privacy issue of the dataset in LLM alignment. They consider protecting the information of prompts and preference data. In order to achieve it, the authors proposed an algorithm called DPPrefSyn based on DP-PCA, DP-clustering, and DP-SGD to generate DP synthetic data. Their methods have advantages in data diversity, reducing data dimension, and saving privacy budget by using public data. The authors provided a privacy guarantee for their method and implemented their method on three benchmarks: OpenAssistan,  Anthropic-HH, and the TL;DR summarization task to show that their method also guarantees some competitive utility empirically."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  The writing of the paper is clear.\n2. The literature review is comprehensive. Most of the related work is discussed.\n3. The problem of private synthetic data to protect prompt and preference information in the paper is interesting.\n4. The empirical results are comprehensive to implement the methods on different tasks and datasets."}, "weaknesses": {"value": "1. The authors provided a privacy guarantee for their method. However, the privacy and utility trade-off is important in DP, and there is no utility theoretical guarantee.\n2. The key methods of DP-PCA, DP-KMeans, and DP-SGD are proposed in previous work. That means the novelty of the paper is limited."}, "questions": {"value": "1. Lack some important related work, e.g. [1]. And discuss whether the method in [1] can be used for your task?\n2. The paper claims DPPrefSyn “outperforms the utility of fine-tuning without DP constraints” (DP-FT with ε = ∞) while maintaining privacy. Could the authors clarify how this is theoretically possible, given that DP typically introduces noise that reduces utility?\n3. The paper relies on the assumption of a linear model for rewards. But in practice, it is mostly non-linear. Discuss how the reward model satisfied this linear assumption in your task.\n4. What are the computational costs of DP clustering and DP-PCA for large-scale preference datasets？\n5. How to deal with the distribution shift problem for the prompts? That is, if the distribution of prompts in the public set is different from the private set, how can you guarantee the method still works?\n\n[1]. Liu, Xiyang, et al. \"DP-PCA: Statistically optimal and differentially private PCA.\" Advances in neural information processing systems 35 (2022): 29929-29943."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kdsc8DyFpd", "forum": "GqNgK7LXBn", "replyto": "GqNgK7LXBn", "signatures": ["ICLR.cc/2026/Conference/Submission13000/Reviewer_dKzk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13000/Reviewer_dKzk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761403389091, "cdate": 1761403389091, "tmdate": 1762923746801, "mdate": 1762923746801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a novel method for differentially-private (DP) alignment. In the first stage, authors essentially learn a DP reward model. This is done by 1) extracting prompt-response pair features using an embedding model, 2) DP-PCA to reduce dimensionality, and then DP-K means to cluster the data, 3) learn linear reward model for each cluster. Then, on public data, authors use this DP reward model to rank responses to generate a new preference data. This new preference data can now be used for different alignment methods such as SFT, DPO, and PPO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Quality: Authors propose a method based on multiple well-established components in the literature, namely DP-PCA, DP-KMeans, and DP-SGD. This composition allows the authors to develop a principled method with a strong foundation in prior work, giving readers greater confidence in the practical utility of the proposed approach.\n\nOriginality, Significance: The proposed method for preference data synthesis departs significantly from previously proposed DP alignment approaches and, as the authors claim, has the merit of being applicable to any preference-based alignment algorithm. This versatility could promote wider adoption of the method and enhance its significance in shaping future research.\n\nClarity: The writing is easy to follow, and the main ideas are clearly conveyed. The limitations of the proposed method could be discussed more explicitly, however. More on this in the Weaknesses section."}, "weaknesses": {"value": "My primary concern is that the authors do not control for key differences between the private preference data and the synthetically generated data: (1) the quality of prompts and (2) the quality of candidate responses. Even the dataset volume is not controlled; for instance, the Alpaca dataset used as public data is larger than the portion of OpenAssistant used as private data. Since responses in the synthetic dataset are generated from LLaMA-7B-Chat, they may also be closer in distribution to the Pythia-2.8B policy being fine-tuned. These factors could account for DPPrefSyn’s superiority over DP-FT, making it difficult to conclude that DPPrefSyn is intrinsically a better DP method. In settings where “better” prompts and “better” response generators are unavailable, DPPrefSyn may not outperform DP-FT. In this work, across all three benchmarks, DPPrefSyn with no privacy control ($\\epsilon=\\infty$) already performs substantially better than DP-FT. Hence, it remains unclear whether DPPrefSyn’s advantage under private settings arises from better algorithmic design or from the use of higher-quality data. I suggest the following to strengthen the empirical claims:\n\n1. Prompt-set flip: swap public and private prompt sets to estimate the impact of prompt quality.\n2. Enhanced DP-FT baseline: in the DP-FT setting, use the same LLaMA-7B-Chat generator to produce responses and rank them with public reward models, ensuring the baseline is sufficiently strong for a fair comparison.\n\nFinally, the method assumes that the reward model is a linear function of extracted features. This assumption is uncommon in the preference-alignment literature and deserves clearer discussion. Although the authors partially address this by clustering the data and training cluster-specific linear models, it remains unclear to what extent this bridges the gap between standard practice (fine-tuning a pretrained LLM reward model) and the proposed simplified reward formulation."}, "questions": {"value": "Could the authors avoid the DP-PCA step by instead:\n\n1. performing non-private PCA on the private data to obtain the projection matrix,\n2. applying DP-KMeans on the PCA-projected representations, and\n3. using these PCA features (rather than the original high-dimensional embeddings) for DP-SGD reward modeling?\n\nSince the downstream stages (DP-KMeans and DP-SGD) already ensure differential privacy, the overall procedure would remain DP-compliant. While using reduced-dimensional features might modestly affect the quality of the reward model, this alternative would preserve more of the privacy budget for later stages. Hence, it is unclear whether employing DP-PCA provides any tangible benefit compared to simply using non-private PCA for dimensionality reduction before applying the DP algorithms."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GoJfpjjgKW", "forum": "GqNgK7LXBn", "replyto": "GqNgK7LXBn", "signatures": ["ICLR.cc/2026/Conference/Submission13000/Reviewer_WLzn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13000/Reviewer_WLzn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807057670, "cdate": 1761807057670, "tmdate": 1762923746541, "mdate": 1762923746541, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses privacy of human-annotated preference data used to fine-tune LLMs for preference alignment. The paper propses a novel method to generate privacy-preserving synthetic preference data as an intermediate on which to apply non-private SFT/DPO.  The method first performs a DP clustering of private preference data, and then trains a scoring model for each cluster to capture preference patterns. The method uses public prompts for the synthetic datasets so that the privacy budget can be allocated for the clustering."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper gives a well-presented intuition of the proposed method.\n- The experimental results show impressive successes on on the reference tasks.\n- The reusability of the extracted synthetic preference data is a neat trick."}, "weaknesses": {"value": "- The paper presents results for larger privacy budgets ($\\epsilon > 1$) only. It would be helpful to see evaluation vs the totally non-private baseline as well as smaller (<1) $\\epsilon$ values.\n- There is some concern about using GPT-4o as a proxy for human preference in determining the win rates in the experiments. While I understand that human studies may not be feasible, it would help to provide more explanation of how the win rate is calculated and why the win rate is a good measure."}, "questions": {"value": "- Is the grid search in the DP-FT a privacy leak?\n- Have you experimented with different ways to divide the privacy budget across the method steps rather than equal $epsilon$ values for each? Does this have any impact on the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0YTWnNykK5", "forum": "GqNgK7LXBn", "replyto": "GqNgK7LXBn", "signatures": ["ICLR.cc/2026/Conference/Submission13000/Reviewer_qLaC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13000/Reviewer_qLaC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964060580, "cdate": 1761964060580, "tmdate": 1762923746238, "mdate": 1762923746238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DPPrefSyn, a method to generate differentially private synthetic preference data for LLM preference alignment. The method uses DP-PCA, DP-KMeans clustering of embedding-difference features, and DP-SGD-trained linear scoring models on each cluster. The public prompts plus model-generated candidates are then filtered via the private scoring functions to produce synthetic preference pairs. The resulting synthetic dataset can be used to align models (e.g., via SFT + DPO) without direct access to private preference data. The authors present performance on various datasets and also report empirical membership-inference robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The pipeline combining DP-PCA, DP-KMeans, and DP-SGD with careful privacy accounting is interesting. \n\n- Empirical results are interesting and show promise. DP synthetic data can be used across models and preference-optimization algorithms.\n\n-  This work includes MIA evaluation suggesting improved privacy robustness vs DP-SGD baselines and also shown some ablation studies."}, "weaknesses": {"value": "- The key privacy claim that current preference-alignment pipelines meaningfully leak user privacy  is stated but not demonstrated. The paper does not show privacy leakage of existing RLHF datasets or methods. Without concrete evidence, the privacy motivation feels assumed rather than established in this context. This weakens the necessity argument.\n\n- There is no demonstration of harm or attack feasibility on real preference data. The work evaluates MIA only on their synthetic pipeline. It does not show privacy leakage from non-DP RLHF training, privacy risk in publicly-released human-preference datasets. \n\n- Synthetic responses are sampled from a strong LLM, private data may reflect lower-quality annotation distributions. This makes utility results hard to interpret as purely privacy-driven improvements.\n\n- Although justified by DP constraints, the choice limits expressiveness. A brief discussion on missed nuance (e.g., contextual preference factors) would help.\n\n- The public prompts may differ from private preference distributions, this work does not provide an analysis of the effect on preference fidelity."}, "questions": {"value": "Please refer to discussion in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NDBhhVAgjD", "forum": "GqNgK7LXBn", "replyto": "GqNgK7LXBn", "signatures": ["ICLR.cc/2026/Conference/Submission13000/Reviewer_uhN9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13000/Reviewer_uhN9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030709619, "cdate": 1762030709619, "tmdate": 1762923745942, "mdate": 1762923745942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}