{"id": "jcVAa9C75T", "number": 22936, "cdate": 1758337276845, "mdate": 1759896839555, "content": {"title": "Locally adaptive conformal inference for operator models", "abstract": "Operator models are regression algorithms between Banach spaces of functions. They have become an increasingly critical tool for spatiotemporal forecasting and physics emulation, especially in high-stakes scenarios where robust, calibrated uncertainty quantification is required. We introduce Local Sliced Conformal Inference (LSCI), a distribution-free framework for generating function-valued, locally adaptive prediction sets for operator models. We prove finite-sample validity and derive a data-dependent upper bound on the coverage gap under local exchangeability. On synthetic Gaussian-process tasks and real applications (air quality monitoring, energy demand forecasting, and weather prediction), LSCI yields tighter sets with stronger adaptivity compared to conformal baselines. We also empirically demonstrate robustness against biased predictions and certain out-of-distribution noise regimes.", "tldr": "Adaptive, distrbution free uncertainty quantification for operator models", "keywords": ["Operator learning", "conformal prediction", "uncertainty quantification"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0358eda1999f6f2140d5383f3ac0da26a08a8db8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Local Sliced Conformal Inference (LSCI), a CP algorithm for neural operator models that constructing locally adaptive, function-valued prediction sets. The method cleverly uses $\\Phi$-depth functions (inf of a family of linear maps) for functional conformity scores and similarity-localized calibration to generate prediction sets that adapt to heterogeneous residual distributions. The authors provide finite-sample validity guarantees under local exchangeability and demonstrate improvements over existing conformal baselines on synthetic and real-world datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "I have to admit that the operator model exposition is a little beyond me, so I'll be commenting mostly from a conformal prediction perspective. \n\n- It is a nice contribution to extend CP to operator learning and translate both the algorithm and guarantees to functional space. The authors have showed that this extension is nontrivial and yields better intervals than other (learning based) uncertainty quantification methods.\n\n- Other than good direct experiment results, I find the ablation study / robustness analysis in Figures 1 and 2 and Tables 2,4,5 to be thoughtful and convincing. The authors demonstrated that coverage is stable across various choices of localizers, feature maps, projection families, and depth functions (since CP should be model-agnostic), and show meaningful advantages over baselines in biased prediction / distribution shift scenarios.\n\n- On theory, the authors drew the connection between the localized calibration bound (Eq 13) to the tradeoff between localization and calibration data availability (Eq 14). Although more clarity can be desired (for example through experiments), the guidance is helpful for readers and practitioners."}, "weaknesses": {"value": "I found the paper to be a bit difficult to follow due to my lack of background in operator learning. (might not be a weakness). \n\nFor example, I didn't understand the significance of the statistical knockoff, why the Tukey (half-space) depth was selected, how exactly $\\lambda$ is tuned to balance the trade-off, and how FPCA sampling recreates the conformal interval.  Although the authors did try to explain these choices/algorithms, the current explanations are either rushed or a little hand-wavy and could benefit from more principled explanations, through equations and examples and plots, utilizing space in the appendix. \n\nAnother question that I had after reading this paper is, how is this UQ useful for Operator learning? The authors introduced 4 metrics, but in my experience did not explain what are the implications of each thoroughly. (I think FC and IS are intuitive for me, but how should I interpret the other two in the context of operator learning?) Maybe it is obvious for the operator learning community, but more likely it's the case that neither community knows what to do with this nice method you created. Some discussion on the properties and usefulness of the prediction sets, that needs to be created from rejection sampling and then empirical quantiles."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HwyMUnPzsD", "forum": "jcVAa9C75T", "replyto": "jcVAa9C75T", "signatures": ["ICLR.cc/2026/Conference/Submission22936/Reviewer_NpHH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22936/Reviewer_NpHH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708916262, "cdate": 1761708916262, "tmdate": 1762942446151, "mdate": 1762942446151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a method for constructing prediction sets for functional data. Using the tool of $\\Phi$-depths, the authors establish local $\\Phi$-scores, which act as localized conformity measures on residuals. Furthermore, by employing this score along with conformal prediction methods, the authors construct prediction sets for functional data. The authors also provide a more intuitive form of the constructed prediction sets through sampling. Finally, the authors present relevant theoretical properties and experimental results. The experimental results demonstrate that the proposed method outperforms baseline methods in terms of both coverage rate and the size of the prediction sets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors employ the tool of $\\Phi$-depths to establish conformity measures suitable for functional data. Additionally, the authors utilize local information to construct the score function, thereby endowing the proposed method with enhanced robustness."}, "weaknesses": {"value": "1. For methodology, in the literature on localized prediction sets (e.g., Guan, 2023[1]; Hore & Barber, 2023[2]; Barber, 2023[3]), the threshold $q_{\\alpha}(f_{n+1})$ is typically defined as the $1-\\alpha$ quantile of a weighted distribution $\\sum_{i=1}^{n+1}w_i\\delta_i$, where the weight $w_i$ assigned to each data point reflects its contribution to the construction of the prediction set. However, in line 201, the authors adopt a different quantile definition. Constructing prediction sets in this way may substantially undermine the validity of the proposed approach.\n\n2. Theoretically, in line 223, the theoretical result presented by the authors is not supported by the literature Barber (2023)[3], as the underlying methodologies differ. Moreover, the authors do not provide a detailed proof for it."}, "questions": {"value": "1 In this paper, local information is used to construct the $\\Phi$-scores, yet no local information is utilized when calculating threshold $q_{\\alpha}(f_{n+1})$. This differs from all existing conformal prediction frameworks (e.g., Guan, 2023[1]; Hore & Barber, 2023[2]; Barber, 2023[3]). Why not use existing localized conformal prediction methods to compute threshold $q_{\\alpha}(f_{n+1})$? Is the currently used threshold a reasonable one?\n\n2 The theoretical results presented in Line 223 are referenced to Barber (2023) [3]. However, there is a methodological discrepancy between the two works, particularly in the calculation of the threshold. Given this difference, the results from Barber (2023) [3] cannot directly support the authors' claim. It is hoped that the authors can provide a corresponding explanation or present the specific proof process.\n\n3 Depth-based prediction sets are defined implicitly as subsets of the function space. Therefore, I am curious about how the metrics in the experimental section were calculated, particularly the band width (BW)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KeVbmsHv0l", "forum": "jcVAa9C75T", "replyto": "jcVAa9C75T", "signatures": ["ICLR.cc/2026/Conference/Submission22936/Reviewer_8P52"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22936/Reviewer_8P52"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830846368, "cdate": 1761830846368, "tmdate": 1762942445889, "mdate": 1762942445889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a locally adaptive conformal score for models that map between function spaces. The present a depth-based conformal score function using a localized empirical cumulative distribution function with weights determined by a similarity kernel. They provide provable upper bounds on the coverage gap obtained from breaking the global exchangeability assumption. They validate their method on synthetic data and real-world data (Air Quality, Energy Demand, and Weather data)."}, "soundness": {"value": 4}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors validate their method with an intuitive upper bound for the coverage gap, which makes it clear how coverage suffers when local exchangeability is weakened. The experiments are quite robust and further strengthen their proposed method."}, "weaknesses": {"value": "Main Weaknesses\n* The proposed method isn’t well-motivated. The paper didn’t cite any examples where global exchangeability might break or local exchangeability might hold with functional data. \n* It’s not easy to see why depth-based scores are important to obtain local marginal coverage or tight prediction sets. In experiments, it’s clear that LSCI outperforms all the conformal baselines in the Interval Score metric, but there is no intuition behind why depth-based score can reduce Interval scores.  \n* There doesn’t seem to be any experiments validating the coverage gap bound in Proposition 3, which appears to be key result.\n\nWriting-related weaknesses\n* The conformal prediction background section needs to cover local exchangeability and local adaptive conformal inference in the finite-dimensional setting. \n* The introduction has a lot of unnecessary background on neural operators and fails to motivate the problem well."}, "questions": {"value": "* Can a depth-based score in a finite-dimensional setting outperform conformal baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ral4M7fwXL", "forum": "jcVAa9C75T", "replyto": "jcVAa9C75T", "signatures": ["ICLR.cc/2026/Conference/Submission22936/Reviewer_ngnT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22936/Reviewer_ngnT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945179902, "cdate": 1761945179902, "tmdate": 1762942445680, "mdate": 1762942445680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce LSCI, a novel, distribution-free UQ framework for operator models, such as NOs. LSCI provides statistically rigorous, function-valued prediction sets that are locally adaptive. The authors propose local Phi scores based on Phi-depth, allowing the method to measure the centrality or typicality of a residual function relative to a local distribution of residuals, rather than just using a single scalar value. LSCI computes a local, test-specific quantile by weighting calibration samples based on their feature-space similarity to the test input."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is technically sound\n2. The paper has strong empirical evidence and is convincing. The experiments show it produces prediction bands that are appropriately tight in low-variance regions and wider in high-variance regions, leading to more informative and useful uncertainty estimates.\n3. The theory relies on local exchangeability, a far more realistic assumption for complex, non-stationary data than the standard global exchangeability required by standard CP methods.\n4. The authors provide a practical algorithm"}, "weaknesses": {"value": "1. The method's adaptivity hinges on the choice of the localization kernel H, the bandwidth lambda, and potentially a feature map phi. While the paper ablates these (in fig 1) and suggests tuning lambda, it offers little guidance on how to choose H or other hyperparameters and analyzes how it affects the resulting efficiency. A discussion on how to choose these parameters would be beneficial.\n2. While weaker than global exchangeability, the assumption that residual distributions vary smoothly could be violated in scenarios with abrupt shifts or phase transitions. The paper does not test the method's robustness to such sharp breaks in the data-generating process. A discussion on the failure modes would be beneficial."}, "questions": {"value": "1. Table 4 shows that coverage is robust to the choice of projection Phi. But how does this choice affect the tightness and shape of the prediction bands?\n2. Can you provide more intuition or formal guidance on how to select the similarity kernel H and feature map for a new problem?\n3. Why were Bayesian Neural Operators (BNOs) or other probabilistic operator models not included as baselines? While they are not distribution-free, they are a primary competing approach for UQ in this domain.\n4. How does the method perform if the calibration set is very large? Does the need to compute n local scores for each test point become a practical bottleneck?\n5. How does LSCI's coverage and tightness behave if the model is significantly mis-specified or poorly trained (i.e., the residuals are very large and structured)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LSfC5KtPib", "forum": "jcVAa9C75T", "replyto": "jcVAa9C75T", "signatures": ["ICLR.cc/2026/Conference/Submission22936/Reviewer_gyDG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22936/Reviewer_gyDG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969021438, "cdate": 1761969021438, "tmdate": 1762942445438, "mdate": 1762942445438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}