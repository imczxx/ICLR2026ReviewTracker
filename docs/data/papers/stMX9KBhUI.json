{"id": "stMX9KBhUI", "number": 18979, "cdate": 1758292519785, "mdate": 1763726203593, "content": {"title": "Towards Improved Sentence Representations using Token Graphs", "abstract": "Obtaining a single-vector representation from a Large Language Model's (LLM) token-level outputs is a critical step for nearly all sentence-level tasks. However, standard pooling methods like mean or max aggregation treat tokens as an independent set, discarding the rich relational structure captured by the model's self-attention layers and making them susceptible to signal dilution. To address this, we introduce GLOT, a lightweight, structure-aware pooling module that reframes pooling as relational learning followed by aggregation. Operating on the outputs of a frozen LLM, GLOT first constructs a latent token-similarity graph, then refines token representations with a graph neural network, and finally aggregates them using a readout layer. Experimentally, our approach is remarkably robust and efficient: on a diagnostic stress test where 90% of tokens are random distractors, GLOT maintains over 97% accuracy while baseline methods collapse. Furthermore, it competitive with state-of-the-art techniques on benchmarks like GLUE and MTEB with 20x fewer trainable parameters and speeds up the training time by over 100x compared with parameter-efficient fine-tuning methods. Supported by a theoretical analysis of its expressive power, our work shows that learning over token graphs is a powerful paradigm for the efficient adaptation of frozen LLMs.", "tldr": "A lightweight relational learning based pooling module for sentence representations from frozen LLMs. It attains SOTA on GLUE, IMDB, and MTEB, and remains highly robust to random noise.", "keywords": ["Graph-based token pooling; Sentence embeddings"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/820020010a891afa93374203cefed1f7c1443eba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes GLOT which is a novel pooling methodology that builds graph out of latent representations and then pass through them multiple layers of GNN and lastly using the recent learnable pooling technique called AdaPool to capture sentence level representations. It operates on output tokens which comes from a frozen backbone so it has very low both computational and memory overhead. It shows significant improvements over static or learnable pooling across benchmarks like GLUE and other downstream tasks from MTEB, and IMDB long text classification dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper introduces an interesting approach to token pooling by using GNN-based graph construction to preserve token relationships that standard pooling methods discard.\n\n* The proposed architecture is lightweight, requiring only ~9M trainable parameters and 0.42GB GPU memory compared to fine-tuning approaches.\n\n* The method shows consistent improvements over frozen baseline pooling methods across multiple model architectures from BERT to Mistral-7B.\n\n* The diagnostic stress test provides useful insights into robustness under noisy conditions, where the method maintains performance while baselines degrade.\n\n* The paper includes reasonable ablation studies on graph sparsity and component contributions with sufficient implementation details for reproducibility.\n\n* Operating on frozen backbones makes the approach practically accessible for resource-constrained settings without expensive fine-tuning infrastructure."}, "weaknesses": {"value": "*  The evaluation approach may not fully align with the paper's focus on sentence representations, as it relies heavily on GLUE tasks rather than comprehensive MTEB evaluation, which is typically considered the standard benchmark for assessing sentence embedding quality.\n\n* The MTEB results are limited to 7 out of 58 tasks, which makes it difficult to assess the method's performance across the full range of sentence embedding applications and may not fully represent its general capabilities.\n\n* Comparisons to established sentence embedding methods like contrastive fine-tuning approaches (Sentence-BERT, SimCSE) or recent lightweight adaptation methods (LLM2Vec) would strengthen the evaluation and provide better context for the contributions.\n\n* The baseline comparisons involve different parameter counts (~9M for GLOT vs. minimal parameters for simple pooling), which makes it somewhat unclear whether improvements stem primarily from the graph-based design or from having additional learnable capacity.\n\n* The efficiency analysis in Table 5 focuses on CoLA, which emphasizes grammatical acceptability rather than semantic similarity, and the surprisingly strong performance relative to full fine-tuning raises questions about baseline tuning that could be addressed with more details.\n\n* Information about inference-time computational costs, particularly for graph construction and GNN processing, would help readers better understand the practical deployment trade-offs of the approach.\n\n* Including parameter-matched baselines such as deeper MLP-based pooling would help better isolate the specific contribution of the graph-based relational learning component.\n\n* A unified comparison showing both efficiency and performance metrics for all methods in a single analysis would provide clearer insights into the overall trade-offs and help readers make more informed assessments."}, "questions": {"value": "1. Could you provide complete MTEB results across all 58 tasks to give a comprehensive picture of the method's performance on standard sentence embedding benchmarks?\n\n2. How does GLOT compare to established sentence embedding methods like contrastive fine-tuning (Sentence-BERT, SimCSE) and recent lightweight approaches (LLM2Vec) in terms of both performance and computational cost?\n\n3. What is the performance of parameter-matched baselines, such as multi-layer MLP pooling with similar capacity (~9M parameters), to isolate whether gains come from graph-based relational learning or simply from having more learnable parameters?\n\n4. Could you clarify the training procedures for the full fine-tuning and LoRA baselines in Table 5, given that GLOT unexpectedly outperforms these much larger capacity methods?\n\n5. What are the inference-time computational costs including graph construction and GNN forward pass, and how do these compare to simpler pooling methods?\n\n6. How does GLOT perform when the backbone is fine-tuned rather than frozen, and does it provide complementary benefits to fine-tuning or does fine-tuning alone achieve comparable results?\n\n7. Could you provide a unified comparison table showing both efficiency metrics and performance across multiple representative tasks for all methods to enable clearer assessment of trade-offs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vvB9Rw9f1O", "forum": "stMX9KBhUI", "replyto": "stMX9KBhUI", "signatures": ["ICLR.cc/2026/Conference/Submission18979/Reviewer_aPBo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18979/Reviewer_aPBo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727571469, "cdate": 1761727571469, "tmdate": 1762931032802, "mdate": 1762931032802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GLOT, a new method for deriving sentence embeddings. Compared to naive mean or max pooling, GLOT explicitly models the semantic relationships between tokens by incorporating a Graph Neural Network (GNN) structure before the final aggregation step. Experimental results demonstrate that GLOT can achieve effective performance improvements with minimal computational resources while keeping the backbone model frozen."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Sentence embedding is a core topic in representation learning, and pooling is a crucial step in converting token-level embeddings into sequence-level embeddings. Therefore, the chosen track is highly relevant to the conference's scope and holds significant practical value.\n2. The paper is well-written. The methodology is presented clearly and intuitively. The authors claim that GLOT is the first work to learn sentence representations via GNNs on top of a frozen Large Language Model (LLM).\n3. The experiments are conducted on both discriminative PLMs (BERT, RoBERTa) and generative PLMs (Llama, Mistral), validating the method's effectiveness across different model architectures."}, "weaknesses": {"value": "1. The MTEB is a massive benchmark for embedding evaluation. The authors show that their method surpasses baselines on only a selected subset of tasks. This seems to be insufficient to support the claim of \"state-of-the-art performance\" in the abstract.\n2. The chosen baselines are rather conventional. I am curious about the comparison between this proposed method and the prompting-based methods that have emerged in the past two years. Prompting methods can have an even smaller memory footprint than this work, as they require no parameter updates at all. Some relevant references include:\n-\t“PromptBERT: Improving BERT Sentence Embeddings with Prompts”\n-\t“Scaling Sentence Embeddings with Large Language Models”\n-\t“Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models”\n3. The authors construct the graph based on the similarity between token embeddings. This approach is relatively straightforward for discriminative PLMs with bidirectional attention. However, for generative PLMs with unidirectional attention, tokens that appear later in a sequence have a larger attention scope. Shouldn't the similarity calculation between their token embeddings account for this disparity?"}, "questions": {"value": "Authors should give their response and explanation with respect to the weaknesses mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "APvcdiHOq1", "forum": "stMX9KBhUI", "replyto": "stMX9KBhUI", "signatures": ["ICLR.cc/2026/Conference/Submission18979/Reviewer_gK63"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18979/Reviewer_gK63"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744954206, "cdate": 1761744954206, "tmdate": 1762931032346, "mdate": 1762931032346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a lightweight structure-aware pooling module that reframes pooling as relational learning followed by aggregation, dubbed GLOT. The method is remarkably robust and efficient, as demonstrated by extensive experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper has a clear motivation.\n2) The method sounds technical.\n3) GLOT has been evaluated through extensive experiments."}, "weaknesses": {"value": "1) Some technical details of the method need to be presented.\n2) The paper lacks essential theoretical explanations to ensure the effectiveness of the method.\n3) The experimental analysis needs more powerful explanations.\n\nI have significant doubts about the rationale behind the design of the proposed GLOT method and about why using a GNN-based approach alone can substantially improve the problem. These issues should be clearly explained in the method description, theoretical analysis, and experimental validation, but I have not found such clarification in the current version. I consider this to be the core of the paper."}, "questions": {"value": "1) GLOT seems to only use an additional GNN to refine the representation. Why does it achieve such significant improvements in the results?\n2) GLOT is easy to implement, and its effectiveness seems to come entirely from the GNN. Would using more advanced variants of GNN further improve the robustness of the model?\n3) GLOT obviously has a more complex graph-based structure. What explains its lower number of parameters and faster training process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aR2hXNW3dX", "forum": "stMX9KBhUI", "replyto": "stMX9KBhUI", "signatures": ["ICLR.cc/2026/Conference/Submission18979/Reviewer_BcGt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18979/Reviewer_BcGt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803575148, "cdate": 1761803575148, "tmdate": 1762931031729, "mdate": 1762931031729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight pooling module, called GLOT, which transforms token-level outputs of frozen LLMs into sentence embeddings through graph-based relational learning. Instead of treating tokens independently, GLOT builds a token-similarity graph, refines representations using a graph neural network, and aggregates them via a learnable readout. Across GLUE, IMDB, and MTEB, GLOT outperforms standard and learnable pooling methods while being 20× more parameter-efficient and 100× faster than fine-tuning approaches. It also shows strong robustness to noise, maintaining over 97% accuracy with 90% distractor tokens."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Reframing sentence pooling as token-relation modeling.\nThe paper introduces GLOT, a new framework that replaces traditional pooling (e.g., mean or [CLS]) with a graph-based aggregation process. GLOT unifies prior pooling schemes as special cases (when the graph is empty or uniform) and explicitly addresses the long-standing signal dilution problem in sentence embeddings. Demonstrated by strong improvements in the signal dilution test (≈97% accuracy under 90% distractors; Table 7, Sec. 5.4).\n\n2. Strong consistency across diverse settings.\nGLOT shows consistent, significant improvements over both static and learnable pooling baselines on major benchmarks: GLUE, IMDB, and MTEB. It outperforms methods such as mean pooling, weighted pooling, [CLS] pooling, across both encoder-based and decoder-based LLMs. This shows the method’s robust generalization and practical applicability across models and tasks. (Sec. 5.1–5.3, Tables 2–4, Fig. 3.)\n\n3. High efficiency with minimal training cost.\nThe proposed module is extremely lightweight, only ≈ 8.9 M trainable parameters and ~ 0.42 GB GPU memory, compared to full fine-tuning or LoRA. Despite this small footprint, it delivers comparable or better accuracy and over 100× training-speed improvement. This efficiency makes GLOT highly practical for large-scale or resource-constrained deployments, supporting the paper’s claim of being a drop-in, low-cost alternative to fine-tuning. (Sec. 5.5, Table 5.)"}, "weaknesses": {"value": "1. While GLOT introduces a graph-based pooling paradigm, the graph construction step is heuristic; edges are formed by thresholding pairwise cosine similarities between token embeddings.\nThe paper does not explore learnable or adaptive graph formation, nor analyze how different thresholds quantitatively affect representation quality beyond a small ablation (τ = 0.4–0.6 works best). As a result, the approach lacks a deeper theoretical justification on how graph topology influences performance. (Sec. 3.2, Sec. 5.5, Table 6.)\n\n2. Although Table 5 reports large efficiency gains (≈100× faster, 0.42 GB GPU), this measurement is only for one dataset (CoLA) and one backbone (Mistral-7B). The paper does not provide cross-task or cross-model runtime, throughput, or latency benchmarks, and omits the cost of graph construction itself. Therefore, the claimed computational advantages, while promising, are not fully validated across broader conditions.\n\n3. The related-work section briefly mentions prior pooling and token-interaction models but does not clearly position GLOT relative to recent work. This leaves ambiguity about whether the proposed approach is a substantial conceptual advance or an effective adaptation of known ideas.\n\n4. The first step of GLOT involves computing a pairwise cosine similarity matrix between all L tokens in the sequence to determine the edges. This is an O(L^2) operation with respect to the sequence length L. While the paper tests on sequences up to 512 tokens (IMDB)9, this O(L^2) step could become a significant computational bottleneck for applications involving very long contexts (e.g., thousands of tokens), a limitation which is not discussed."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QqRJcMbCWy", "forum": "stMX9KBhUI", "replyto": "stMX9KBhUI", "signatures": ["ICLR.cc/2026/Conference/Submission18979/Reviewer_56SJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18979/Reviewer_56SJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975430566, "cdate": 1761975430566, "tmdate": 1762931030823, "mdate": 1762931030823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for their thoughtful and constructive feedback. We are encouraged that the reviewers recognized the **novelty** of reframing pooling as relational learning (**gK63**, **BcGt**), the **practical value** of operating on frozen backbones (**56SJ**, **aPBo**) and the **robustness** of GLOT to signal dilution (**56SJ**, **aPBo**). We are also pleased that **Reviewer 56SJ** found our method \"highly practical\" for resource-constrained deployments and **Reviewer gK63** noted the methodology is presented \"clearly and intuitively.\" \n\nIn response to your constructive and thoughtful feedback, we have revised our paper to strengthen its positioning and empirical and validation. Below is a summary of the major updates and additional experiments included in our revision:\n\n1. **Expanded MTEB Evaluation:** To address the coverage concerns raised by  Reviewers **gk63** and **aPBo**, we significantly expanded our evaluation on the MTEB benchmark using the Mistral-7B backbone. The results further support the strong performance of GLOT. We see that in many cases GLOT achieves competitive or better performance than state-of-the-art techniques with frozen-backbone baselines on tasks requiring explicit dependency modeling and signal preservation, significantly outperforming Mean Pooling on **Summarization** (+41.2% relative improvement), **Retrieval** (+18.3%), and **Pair Classification** (+8.2%). \n\n2. **Comparison to prompting-based sentence embedding methods:** Following Reviewer **gK63**’s suggestion, we compared GLOT against prompting-based methods (PromptBERT, PromptEOL, Pretrained CoT and Knowledge Enhancement). GLOT consistently outperforms these parameter-free approaches, for example, by +18.8% on BERT and +5.0% on Mistral-7B on the STS-B task. These results further confirm that learning from token interactions is more effective than current prompt based methods for our tasks.\n\n3. **Comparison to contrastive fine-tuning based sentence embedding methods:** Addressing Reviewer **aPBo**, we benchmarked GLOT against fully fine-tuned contrastive models (SBERT, SimCSE). GLOT achieves superior performance on complex GLUE tasks (e.g., +22.5 points on CoLA vs. SBERT) and competitive performance on semantic similarity, all while using ~92x fewer parameters and a frozen backbone.\n\n4. **Benchmarking the cost of graph construction:** We profiled the exact cost of the graph construction step (Reviewers **56SJ** and **aPBo**). We found that this step is relatively small, taking only ~0.04ms for standard lengths and constituting **less than 1.3% of total inference latency even at 32K context length**. Furthermore, we extended our efficiency analysis to include encoder-only models (BERT), confirming that GLOT delivers **$>100\\times$** speedups across different architectures.\n\n5. **Theoretical clarifications and additional ablations:** We clarify that the performance improvement of GLOT is attributed to the fact that standard pooling methods for frozen LLM backbones treat tokens as an independent set of vectors, analogous to the DeepSets framework. GLOT, on the other hand, treats tokens as semantic graph in the latent embedding space of the frozen LLM, and  learning a pooled representation from token interactions leads to robust sentence embeddings (Reviewer **BcGt**). Additionally, we attribute the performance of GLOT to the token-graph by ablating against a parameter-matched deeper MLP (Reviewer **aPBo**). Overall, with a take-home message that the **paradigm shift of token pooling as relational learning task** leads to efficient adaptation of billion-scale LLMs and superior performance. \n\nWe have incorporated these results into the revised manuscript, with all changes marked in blue. We believe these additions comprehensively address your comments, and we feel that your constructive feedback helped us to improve the quality of the paper. We hope that you find our responses satisfactory, and that you will consider revising your scores."}}, "id": "s43BwrbOIG", "forum": "stMX9KBhUI", "replyto": "stMX9KBhUI", "signatures": ["ICLR.cc/2026/Conference/Submission18979/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18979/Authors"], "number": 15, "invitations": ["ICLR.cc/2026/Conference/Submission18979/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763727348724, "cdate": 1763727348724, "tmdate": 1763727348724, "mdate": 1763727348724, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}