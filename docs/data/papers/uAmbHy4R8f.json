{"id": "uAmbHy4R8f", "number": 7244, "cdate": 1758012881178, "mdate": 1763303814966, "content": {"title": "MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring", "abstract": "The Mamba architecture has emerged as a promising alternative to CNNs and Transformers for image deblurring. However, its flatten-and-scan strategy often results in local pixel forgetting and channel redundancy, limiting its ability to effectively aggregate 2D spatial information. Although existing methods mitigate this by modifying the scan strategy or incorporating local feature modules, it increase computational complexity and hinder real-time performance. In this paper, we propose a structure-aware image deblurring network  without changing the original Mamba architecture. Specifically, we design a memory buffer mechanism to preserve historical information for later fusion, enabling reliable modeling of relevance between adjacent features. Additionally, we introduce an Ising-inspired regularization loss that simulates the energy minimization of the physical system's \"mutual attraction\" between pixels, helping to maintain image structure and coherence. Building on this, we develop MBMamba. Experimental results show that our method outperforms state-of-the-art approaches.", "tldr": "", "keywords": ["Image restoration", "Image deblurring", "Mamba"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aea2419687a8eab97916ee02e0d79b5e35b65c47.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper “MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring” aims to enhance the spatial structure modeling capability of Mamba-based image deblurring frameworks. The authors point out that the traditional Mamba’s “flatten-and-scan” strategy leads to local pixel forgetting and channel redundancy, thereby weakening spatial feature aggregation. To address this issue, they propose the MBMamba framework, whose core component is the Memory Buffering Mechanism (MemVSSM). This module preserves historical feature information through a memory buffer and fuses it via a Feature Cross-Attention Module (FCAM), strengthening local context modeling. In addition, an Ising-inspired regularization loss is designed to simulate pixel-level “mutual attraction” energy minimization, thereby maintaining structural consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The MemVSSM module is designed to enhance local context preservation via feature partitioning and temporal buffering. The physically inspired Ising Loss is incorporated into the deblurring loss function, enforcing spatial coherence through energy minimization. Notably, this module extends the spatial-awareness capability of the Mamba backbone without altering its main architecture, achieving both efficiency and accuracy.\n\nThe proposed method provides a new perspective on memory-based state-space modeling in visual tasks. It achieves superior results across four major benchmarks — GoPro, HIDE, RealBlur-R, and RealBlur-J — while maintaining significantly lower FLOPs and inference latency compared to Transformer-based methods."}, "weaknesses": {"value": "The title and abstract emphasize “Structure-Aware”, yet the manuscript does not present any structural-awareness visualizations or heatmaps (No direct evidence found in the manuscript).\n\nThe paper does not discuss how the depth (K) of MemVSSM or the number of channel partitions (N) affects performance. Similarly, no comparison is provided against a non-memory Mamba baseline trained in a single step.\n\nSections 3.2 and 3.3 describe MemVSSM and the Ising Loss separately but do not explain their training interaction or coupling mechanism. The hierarchical integration of SFEM/FCAM is also unclear — Figure 3 shows only a schematic representation without specifying whether the connection is serial or parallel.\n\nFinally, the authors omit details about hardware configurations, such as GPU type or training time. The hyperparameters λ and δ are adopted from prior work without justification or tuning analysis, leaving their choice insufficiently explained."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "isC13JB8tM", "forum": "uAmbHy4R8f", "replyto": "uAmbHy4R8f", "signatures": ["ICLR.cc/2026/Conference/Submission7244/Reviewer_jJFq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7244/Reviewer_jJFq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761479731204, "cdate": 1761479731204, "tmdate": 1762919381960, "mdate": 1762919381960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present MBMamba, a structure-aware image deblurring network that enhances existing methods by effectively integrating local and global features while maintaining computational efficiency. Local features are stored in a memory bank and subsequently aggregated with current representations to enrich local details, thereby advancing Mamba-based image restoration without incurring additional computational cost. Experimental results demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and easy to follow. The motivation for introducing a memory buffer is clearly presented, and its effectiveness is well supported by extensive experimental results."}, "weaknesses": {"value": "1. Task specificity\nThe proposed method is not specifically tailored to the image deblurring task. To demonstrate its generalization capability, the model should also be evaluated on other common image restoration tasks, such as image denoising and super-resolution.\n\n2. Memory mechanism design\nThe memory management strategy appears overly simplistic. The FIFO-based memory update may lead to the accumulation of redundant features in the memory buffer, which could eventually result in performance degradation.\n\n3. Novelty and contribution\nThe use of a memory buffer in state space models is not novel; for example, [Ref1] already explored this concept. Given that the proposed memory mechanism is relatively simple, its technical contribution seems limited. The work may be better characterized as a new application of a memory buffer for image restoration rather than a methodological advancement, which may not meet the technical threshold for acceptance at ICLR.\n[Ref1] Memory Augmented State Space Model for Time Series Forecasting, IJCAI 2022.\n\n4. Analysis and comparison\nA more in-depth analysis and broader comparison with other Mamba-based approaches are necessary. For instance, the activation patterns shown in Fig. 5 should be compared not only with the original SSM but also with other channel-attention-enhanced Mamba variants."}, "questions": {"value": "1. How well does the proposed method generalize to other image restoration tasks, such as denoising or super-resolution?\n\n2. Could the authors elaborate on the rationale for adopting a simple FIFO-based memory update? As memory management does not significantly affect overall computational complexity, it would be helpful to understand why a more advanced strategy was not considered.\n\n3. The use of a memory buffer in state space models has been previously studied. What distinguishes the proposed approach from prior memory-augmented SSMs in terms of architectural design or learning dynamics?\n\n4. Can the authors provide additional comparisons with other Mamba variants, such as those incorporating channel attention, to better contextualize the reported improvements?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XXv59ZJqIq", "forum": "uAmbHy4R8f", "replyto": "uAmbHy4R8f", "signatures": ["ICLR.cc/2026/Conference/Submission7244/Reviewer_7Rxy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7244/Reviewer_7Rxy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630922848, "cdate": 1761630922848, "tmdate": 1762919381644, "mdate": 1762919381644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MBMamba, a structure-aware image deblurring network that augments the Mamba state-space model with a memory buffering mechanism (MemVSSM) and an Ising-inspired regularization loss.\nThe memory buffer preserves historical context to mitigate pixel forgetting and channel redundancy inherent in Mamba’s flatten-and-scan strategy, while the Ising loss enforces spatial coherence by mimicking energy minimization between neighboring pixels.\nThe model achieves state-of-the-art PSNR/SSIM on both synthetic (GoPro, HIDE) and real-world (RealBlur) datasets, with notably reduced computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tElegant structural innovation.\nThe proposed memory buffer mechanism cleverly leverages temporal-like dependency modeling to retain local context within the Mamba framework, without modifying the core architecture. This is a nontrivial conceptual advancement, positioning MBMamba as a “plug-in” enhancement rather than a structural overhaul.\n2.\tIsing-inspired loss bridging physics and perception.\nThe introduction of the Ising regularizer is an intriguing idea that connects statistical physics with image restoration. Its simplicity and interpretability make it both effective and theoretically meaningful for enforcing spatial smoothness."}, "weaknesses": {"value": "1.\tLack of theoretical or mathematical grounding of the memory buffer mechanism\nThe design is described algorithmically but not theoretically. How does the memory depth K influence the state propagation in Mamba’s continuous SSM formulation? Is the memory fusion equivalent to modifying the recurrent transition kernel? Does this break the linearity and stability guarantees of Mamba’s state evolution? A deeper analysis (e.g., spectrum or stability study) would elevate the contribution from empirical to theoretical.\n2.\tPotential redundancy between Mamba’s state memory and the added buffer\nMamba inherently encodes long-range dependencies through selective state updates. The proposed buffer essentially adds a short-term memory atop it. The authors should clarify whether the memory buffer complements or duplicates the SSM’s internal recurrence. Ablations varying K or removing cross-attention could clarify this synergy."}, "questions": {"value": "The FIFO memory and cross-attention introduce potential non-stationarity in gradient propagation. Does the memory cause gradient explosion or saturation in deeper decoders? How is the memory reset between sub-decoders? These engineering choices critically affect convergence yet are not reported.\n\nProvide a mathematical derivation or theoretical interpretation of how memory fusion modifies the Mamba update rule. Include sensitivity studies for memory depth K, chunk size N, and Ising loss weight.\n\nCompare Ising loss with TV or Laplacian smoothness regularizers to establish distinct advantages."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F5GuhJY2HS", "forum": "uAmbHy4R8f", "replyto": "uAmbHy4R8f", "signatures": ["ICLR.cc/2026/Conference/Submission7244/Reviewer_NRXa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7244/Reviewer_NRXa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7244/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001958944, "cdate": 1762001958944, "tmdate": 1762919381312, "mdate": 1762919381312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}