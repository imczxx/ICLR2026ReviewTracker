{"id": "Tf4Lfw85lS", "number": 22707, "cdate": 1758334586679, "mdate": 1763695478539, "content": {"title": "TVTSyn: Content-Synchronous Time-Varying Timbre for Streaming Voice Conversion and Anonymization", "abstract": "Real-time voice conversion and speaker anonymization require causal, low-latency synthesis without sacrificing intelligibility or naturalness. Current systems have a core representational mismatch: content is time-varying, while speaker identity is injected as a static global embedding. We introduce a streamable speech synthesizer that aligns the temporal granularity of identity and content via a content-synchronous, time-varying timbre (TVT) representation. A Global Timbre Memory expands a global timbre instance into multiple compact facets; frame-level content attends to this memory, a gate regulates variation, and spherical interpolation preserves identity geometry while enabling smooth local changes. In addition, a factorized vector-quantized bottleneck regularizes content to reduce residual speaker leakage. The resulting system is streamable end-to-end, with $<$80 ms GPU latency. Experiments show improvements in naturalness, speaker transfer, and anonymization  compared to SOTA streaming baselines, establishing TVT as a scalable approach for privacy-preserving and expressive speech synthesis under strict latency budgets.", "tldr": "A streamable voice conversion/anonymization system that synchronizes time-varying timbre with content via a Global Timbre Memory, improving naturalness and privacy under strict low-latency constraints.", "keywords": ["Time-varying timbre", "Streaming voice conversion", "Content-synchronous speaker conditioning", "Speech anonymization", "Vector-quantized bottleneck"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/25fcee81d4769861cf1bab2a6486edf07cc3c040.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper state the static-dynamic mismatch between fixed speaker embeddings and time-varying linguistic content. The proposed TVTSyn model introduces a content-synchronized time-varying timbre representation, complemented by a factorized VQ bottleneck, to balance low latency, naturalness, and privacy."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper is well-structured and provide analysis on the effcient of the proposed system."}, "weaknesses": {"value": "- The paper’s central claim resolving the static-dynamic mismatch via time-varying timbre is not sufficiently novel. Prior work has already explored dynamic speaker conditioning for speech synthesis.\n- The dataset employed in experiments is not convince and popular, which make me confuse the correctness of the conclusion.\n- Incomplete baseline comparisons with voice privacy challenge baseline systems or other popular speaker anonymization systems."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RauX47wDCj", "forum": "Tf4Lfw85lS", "replyto": "Tf4Lfw85lS", "signatures": ["ICLR.cc/2026/Conference/Submission22707/Reviewer_1kxu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22707/Reviewer_1kxu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925926121, "cdate": 1761925926121, "tmdate": 1762942349804, "mdate": 1762942349804, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposed a streamable speech synthesizer, TVTSyn, for voice conversion and anonymization. \nThe model resolved a mismtach in prior work that linguistic content is time-varying but speaker identity is a static vector. The proposed model aligns the temporal granularity of identity and content relying on TVT, time-varying timbre representation. It also uses a speech content encoder encoder to extract feature that removes residual speaker information and regularize the content space with a factorized vector-quantized bottleneck.\nThe experiment shows that the model behaves strong compared to multiple baselines and the system is causal end-to-end with <80ms on GPU and <132ms on CPU, which is considered within real-time bounds. A comprehensive study on content and TVT representation and ablation study is also provided. \nOverall, the work has good vision and provide moderate novelty on model architecture and representations to resolve the mismatch between static speaker identity embedding and time-varying timbre representation. The strong empirical results show the effectiveness on model performance including speech naturalness, anonymization and system latency. The limitation of the work is that dataset is english only, evaluation samples size(N=20 for MOS) are limited and the theoretical analysis of the TVT representation could be better discussed."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem is well framed and it shows good intuition on the mismatch between dynamic input and static speaker embedding. The solution is intuitive by introducing a timbre representation that contains better temporal information.\n2. The overall system is well designed and end-to-end streamable. The proposed content encoder introduces a learnable bottleneck with factorized vector-quantization(VQ) that learns discrete, speaker-independent units while preserving linguistic fidelity. Also the time varying timbre representation is consists of a global timbre memory (GTM) that allowing content embedding to attend over the keys to retrieve weighted component using attention, and a combination of gating, interpolation to balance stability and flexibility.\n3. The author provided a comprehensive evaluation of the Voice Conversion and Speaker Anonimyzation, and latency analysis.  Also it shows the system is 79 ms latency on GPU and around 132 ms on CPU, achieving a real-time.\n4. Good analysis on content representation with tSNE visualization, showing the effectiveness of VQ bottleneck. Good representation on ablation study showing the removal of TVT and VQ causes degradation."}, "weaknesses": {"value": "1.The discussion for the design of gating/interpolation are mostly based on intuition and empirical results. It would be good to include more theoretical analysis. These innovations on TVT representation and the usage of gating/slerp interpolation are more at a level of improving on top of existing architectures. Yet they are proved effective from experiment results.\n2. The MOS tests are based on 20 samples which is limited and may cause bias"}, "questions": {"value": "Could you provide more insights on the design of Factorized VQ and gating/interpolation?\n\nFor example, in Section 3.2, you mention that slerp interpolation “respects the hyper-spherical geometry of the embedding space, ensuring smooth trajectories and preserving angular distances” which helps maintain “identity geometry”. Could you clarify or provide theoretical evidence for this claim? \nAlso in section 5.2 (c) it shows the difference of before and after applying slerp. How sensitive is the output timbre to the choice of interpolation method (Slerp vs. linear)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yZMwfTvtIN", "forum": "Tf4Lfw85lS", "replyto": "Tf4Lfw85lS", "signatures": ["ICLR.cc/2026/Conference/Submission22707/Reviewer_VGqB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22707/Reviewer_VGqB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956668512, "cdate": 1761956668512, "tmdate": 1762942349309, "mdate": 1762942349309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TVTSyn, a low-latency voice conversion and anonymization system that replaces the traditional static speaker embedding (x-vectors) with a content-synchronous, time-varying timbre (TVT) representation. The method uses a Global Timbre Memory (GTM), gated interpolation (Slerp), and a factorized VQ bottleneck to align the temporal granularity of speaker identity with linguistic content that are fed into a streamable speech synthesizer for generating anonymised audio. The system achieves <80 ms latency on GPU, ~132ms on CPU, shows improved naturalness, and provides a better privacy-utility trade-off than prior streaming baselines (SLT24, DarkStream, GenVC). Experiments follow the VoicePrivacy Challenge 2024 protocol, reporting reasonable performance in source-target speaker similarity, EER, WER, and perceptual quality metrics. EER in semi-informed case is significantly lower than DarkStream which is not elaborated further. It is potentially due to the content embeddings leaking speaker information. I doubt the claim that content embeddings are speaker-independent and needs to be qualified properly."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Authors identify a fundamental weakness (static speaker embedding) in the current speaker anonymisation techniques and proposes a well-justified fix via content-synchronous conditioning of the speaker embeddings\n- Well-founded experiments on VPC 2024 protocol and ablations confirm benefits across privacy, quality, and latency\n- Deployment conditions are kept in mind by demonstrating real-time performance on CPU/GPU under tight latency budgets (<80 ms), relevant for interactive applications\n- Clear figures, well-written text, and reproducible evaluation settings\n- Concrete future directions are presented that extend the technique significantly"}, "weaknesses": {"value": "- The performance of B1 baseline is mentioned during the analysis but not added to Table 2 for clear comparison\n- The gating and Slerp mechanisms are intuitively motivated but not analyzed quantitatively (e.g., contribution to expressivity or privacy).\n- Listening tests use a small Mechanical Turk sample (N = 20) without statistical significance analysis or demographic breakdown. A larger cohort of listeners must be recruited (>100) and carefully selected to include demographic variations (age, gender, native/non-native, etc.)\n- Authors claim that the content embeddings are speaker-independent and show it through t-SNE plots but do not quantify it through metrics. The claim of speaker-independence needs to be properly verified. One option is to classify speakers directly through content embeddings which might reveal how much speaker information is leaking through them as performed in this paper: https://petsymposium.org/popets/2023/popets-2023-0007.php"}, "questions": {"value": "1. Could the authors provide a quantitative ablation showing how the gating parameter $\\alpha_t$ or the number of timbre facets $K$ affects privacy (EER) and quality (NISQA) ?\n2. Is the Global Timbre Memory fixed per speaker or updated during fine-tuning ? Would dynamic adaptation hurt anonymity ?\n3. How robust is this technique to noisy or reverberant inputs ? does the causal encoder maintain intelligibility under such conditions ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fsQgG5rdwE", "forum": "Tf4Lfw85lS", "replyto": "Tf4Lfw85lS", "signatures": ["ICLR.cc/2026/Conference/Submission22707/Reviewer_he7Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22707/Reviewer_he7Q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22707/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762548119009, "cdate": 1762548119009, "tmdate": 1762942349028, "mdate": 1762942349028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}