{"id": "X7lrOUfNKa", "number": 18808, "cdate": 1758291022607, "mdate": 1759897080307, "content": {"title": "GraphFractalNet: A Fractal-Inspired Sparse Transformer for Ultra-Scalable Graph Representation Learning", "abstract": "Graph neural networks (GNNs) and Transformer-based architectures have achieved strong performance in graph representation learning, yet they often struggle with scalability, over-smoothing, and limited expressiveness on complex topological patterns. We propose GraphFractalNet, a novel framework that integrates spectral embeddings, dynamic graph rewiring, and a fractal attention mechanism to capture both global and hierarchical self-similar structures in graphs. By leveraging truncated spectral bases, GraphFractalNet provides topology-aware node embeddings, while the rewiring module adaptively optimizes edge connectivity to improve information flow and sparsity. The fractal attention layer further constrains attention to recursively clustered subgraphs, enabling sub-quadratic complexity of $O(N \\log \\log N)$ per layer while retaining expressive power beyond $k$-Weisfeiler–Lehman tests. Theoretically, we establish generalization bounds under spectral Rademacher complexity and prove that GraphFractalNet is strictly more expressive than standard message-passing GNNs. Empirical results show that GraphFractalNet delivers state-of-the-art performance on both molecular property prediction and large-scale node classification tasks, consistently improving accuracy and scalability. Comprehensive ablation studies underscore the critical roles of spectral embeddings, dynamic rewiring, and fractal attention, each contributing to the model’s effectiveness and efficiency. Overall, GraphFractalNet emerges as a principled and scalable architecture that seamlessly integrates spectral techniques with Transformer-inspired designs for graph learning.", "tldr": "", "keywords": ["Graph Neural Networks (GNNs)", "Graph Transformers", "Spectral Graph Methods", "Graph Representation Learning"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7bfc524a77d2bcd7a034fba54de277a3ee83dc58.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces \\textit{GraphFractalNet}, a graph neural network that integrates spectral graph representations, hierarchical clustering, and dynamic edge rewiring through a proposed fractal attention mechanism. \nEach layer operates in a Laplacian spectral space and selectively rewires edges using a distance-aware score based on shortest-path distances. \nThe authors claim a per-layer complexity of $\\mathcal{O}(N \\log\\log N)$ and provide theoretical arguments that the model is strictly more expressive than the $k$-Weisfeiler-Lehman (k-WL) test for $k \\ge 2$, supported by a generalization bound. \nEmpirical results on a wide range of standard benchmarks (ZINC, OGBN, etc.) show state-of-the-art performance and improved scalability compared to prior graph Transformer architectures.\n\nWhile the empirical resutls are impressive, the theoretical claims in expressiveness and complexity seem deeply flawed and, in my reading, do not hold."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I like the conceptual ideas, the fractal attention masking is a neat idea that could very well have merit, as is the combination with a rewiring phase in a transformer architecture. I have not encountered these ideas in this combination so far and it seems like an approach with merit overall.\n\nThe main strength of the paper are the very strong empirical results. The experimental evaluation is expansive both in terms of comparison architectures and benchmarks. The submission reports sota or near-sota on most standard graph learning benchmarks. However, there are some question marks about experimental setup, see questions."}, "weaknesses": {"value": "Review GraphFractalNet\n\n\n## Expressiveness Claims\n\nRepeatedly the submission claims high expressiveness as a key benefit of this approach. This is based on Theorem 1 presented in Appendix B, which states\nGraphFractalNet is strictly more expressive than the k-Weisfeiler Lehman test for $k \\geq 2$.\n\nThe statement does not even pass an initial sanity check as this would imply GraphFractalNet and decide graph isomorphism on finite graphs (take k to be the order of the graph). \n\nWhat is shown is that for every k, there exist pairs of graphs that are indistinguishable by k-WL, but have different spectra. Different spectra the rather straightforwardly implies distinguishability by GraphFractalNet. There are 2 major issues with this and I do not see that they are fixable:\n1. “strictly more expressive” in the literature means that the equivalence class is finer. You are missing the direction that **for any two graphs are equivalent to GraphFractalNet, you must show that they are also equivalent under k-WL**. Without this direction you are not showing \"strictly more expressive\", and the observation that there are graphs distinguishable by GraphFractalNet is not interesting without this direction.\n2. Moreover, the critical point of the proof is that there exist pairs of graphs that are equivalent under k-WL but have different spectra. This is not obvious at all and certainly requires a proof or a citation. In fact [1] may actually refute this assumption entirely (I did not have time to connect the details, but they show that the expressive power of the laplacian, and Fürer’s spectral invariant, are subsumed by 2WL). This at least seems like strong evidence to the contrary of what is assumed in this submissions.\n\n\n## Complexity Analysis\n\nThe complexity analysis is puzzling. The graph rewiring step seems quite obviously quadratic as you compute relevance for every pair of nodes, of which there are famously quadratically many. Moreover, because of the dynamic nature you have to do this at every layer.  Additionally, on which graph is SPD computed? The rewired graph or the original one? Either way this is not even possible in quadratic time in N.\n\nThe complexity analysis of the fractal clustering is also not discussed in sufficient detail (although here the log log n bound at least seems to pass a superficial check). In fact, despite the complexity being such a core point of this submission the analysis is a single paragraph in the appendix with no sufficient details or citations for any of the bounds stated there.\n\nSo by my rough accounting **a layer requires between O(N^2) and O(N^2 log N) time**, depending on whether SPD is computed on the rewired graphs. The stated $\\log \\log N$ bound only applies to the attention mask, not overall effort per layer.\n\nOn a more minor note, please already clarify in the introduction that N is the number of nodes.\n\n[1] Rattan and Seppelt. \"Weisfeiler-leman and graph spectra.\" SODA 2023."}, "questions": {"value": "Q1: You state in line 367 that every experiment is conducted 10 times. In your tables you state that you report mean/stddev over 5 runs. Can you please elaborate on that. How do you pick the 5 runs and why do you then run 10 runs in the first place?\n\nQ2: Regarding point 2 of the expressiveness part, the assumption that there exist k-WL equivalent pairs G,H such that G,H have different spectra. Could you elaborate on this statement and why it holds?\n\nQ3: Please comment on the issues raised regarding the complexity analysis. Most importantly, I do not understand how the relevance score $R_{ij}^{(\\ell)}$ can be computed in subquadratic time in the nodes when it scores every pair of nodes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kSa0h3mM5x", "forum": "X7lrOUfNKa", "replyto": "X7lrOUfNKa", "signatures": ["ICLR.cc/2026/Conference/Submission18808/Reviewer_zgCF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18808/Reviewer_zgCF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761473591046, "cdate": 1761473591046, "tmdate": 1762999990832, "mdate": 1762999990832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This works proposed GraphFractalNet, a dynamic graph re-wiring method for efficient graph representation learning. The re-wiring is guided by self-attention plus shortest-path distance (SPD) between nodes."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Baseline comparison contains great amount of existing GNNs and Graph Transformers."}, "weaknesses": {"value": "1. The method presentation is hard to follow. There is no workflow or pipeline of the proposed model, resulting in components that are separate from each other or aren't even used at all, e.g., the relevance score $R$ in Eq 3. How is the graph re-wiring implemented? \n\n2. GraphFractalNet is announced by the inspiration of fractal geometry, which is barely discussed in the paper.\n\n3. The provable bounds on the efficiency of GraphFractalNet are claimed as one of the contributions, while the theoretical results are absent in the main text.\n\n4. Experimental results (Tables 1 and 3) contain a notable number of missing results, which were not explained. \n\n5. The empirical evaluations on expressiveness and efficiency are insufficient. Although Table 4 shows a speed comparison, the baseline is picked from relatively weak models (out off the top-3 in Table 3), and node classification models weren't discussed regarding the efficiency. Furthermore, although Appendix B demonstrates that GraphFractalNet is strictly more expressive than the k-Weisfeiler–Lehman (k-WL). The isomorphic test as empirical evidence is also missing.\n\n6. I went through the proof of expressiveness, and it appears imprecise with conflict statements, *\"for $k\\geq 2$\"* in the Theorem but *\"for some $k$\"* in the proof. Furthermore, this proof is not rigorous with an undefined operator, i.e., spec(), since the strategy authors used here is entirely based on Eq 13. To me, implement the purpose of spec(), i.e., *\"let their Laplacian spectra differ\"*, is basically cycle proving the Theorem.\n\n7. Graph re-wiring baselines were not discussed nor compared."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DP95SW7Gtt", "forum": "X7lrOUfNKa", "replyto": "X7lrOUfNKa", "signatures": ["ICLR.cc/2026/Conference/Submission18808/Reviewer_pNfs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18808/Reviewer_pNfs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771565449, "cdate": 1761771565449, "tmdate": 1762999990855, "mdate": 1762999990855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GraphFractalNet, a novel framework that integrates spectral embeddings, dynamic graph rewiring, and a fractal attention mechanism to capture both global and hierarchical self-similar structures in graphs. The authors argue these jointly mitigate three key challenges in graph learning: scalability, over-smoothing, and limited expressiveness. They provide theoretical analyses (spectral Rademacher bounds, expressiveness beyond k-WL) and experiments across small, medium, and billion-scale datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The design achieves sub-quadratic complexity through hierarchical sparse attention and dynamic sparsification.\n2. Empirical results on OGB-papers100M show training feasible where other transformers hit OOM.\n3. The fractal attention concept is original and theoretically motivated by hierarchical self-similarity. Integrating spectral encodings and rewiring in a unified block is conceptually elegant.\n4. Benchmarked against many baselines (GraphGPS, Exphormer, SGFormer, etc.) with state-of-the-art or close results on multiple tasks.\n5. The paper derives Rademacher-based generalization bounds and a proof of expressiveness beyond k-WL, suggesting a thoughtful theoretical grounding."}, "weaknesses": {"value": "1. My biggest concern is: the claim that fractal attention and spectral message passing “mitigate over-smoothing” is only qualitative; no layer-depth ablation or node-embedding similarity analysis is reported. No explicit comparison to anti-smoothing methods (e.g., APPNP, DropEdge).\n2. The “strictly more expressive than k-WL” theorem relies on Laplacian spectral differences; this condition is not always sufficient to prove general superiority. Empirical evidence of expressivity (e.g., distinguishing isomorphic graphs) is absent.\n3. Although complexity is theoretically $O(N \\text{log} \\text{log} N)$, actual runtime vs $N$ scaling curves are not provided—only epoch times on select datasets.\n4. The ablation is mentioned but not detailed in the main text. Quantitative impact of each module (spectral encoder, rewiring, fractal attention) on scalability and accuracy should be clearer.\n5. The paper is dense; critical mechanisms (e.g., recursive clustering in fractal attention) could use clearer visualization or pseudo-code.\n6. The author did not provide reproducible code."}, "questions": {"value": "1. Can you provide empirical evidence that over-smoothing is mitigated—e.g., plotting node embedding similarity across layers? Does the performance of the testing model change by gradually increasing the network layer?\n2. How sensitive is performance to the number of spectral components and clustering levels ($k$)?\n3. Is the dynamic rewiring stable during training, or does it cause oscillation in graph topology?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7uwxTaPhwr", "forum": "X7lrOUfNKa", "replyto": "X7lrOUfNKa", "signatures": ["ICLR.cc/2026/Conference/Submission18808/Reviewer_rjkp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18808/Reviewer_rjkp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878493798, "cdate": 1761878493798, "tmdate": 1762999991158, "mdate": 1762999991158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GraphFractalNet, a new Transformer based framework for graph representation learning designed to achieve both high expressiveness and scalability. Existing Graph Neural Networks (GNNs) and Graph Transformers often face challenges such as over smoothing, limited global context modeling, and quadratic computational cost. GraphFractalNet addresses these issues by introducing a fractal inspired sparse architecture that combines spectral embeddings, dynamic graph rewiring, and a hierarchical fractal attention mechanism.\n\nThe model begins with a spectral encoder that uses truncated Laplacian eigenvectors to provide topology aware node and edge embeddings, capturing both local and global structural information. A dynamic rewiring module then adaptively updates the graph connectivity at each layer by selecting the most structurally relevant edges, promoting sparsity while preserving essential topology. On top of this, a fractal attention layer organizes attention hierarchically across recursively clustered subgraphs, allowing the model to capture multi scale dependencies efficiently. This design yields sub quadratic complexity of approximately O(N log log N) per layer while maintaining strong representational power.\n\nThe authors provide theoretical guarantees showing that GraphFractalNet is more expressive than standard message passing GNNs and can distinguish graphs beyond the k Weisfeiler Lehman hierarchy under mild spectral conditions. They also derive generalization bounds based on spectral Rademacher complexity, supporting the model’s robustness.\n\nEmpirically, GraphFractalNet demonstrates state of the art or superior performance on a wide range of benchmarks, including molecular property prediction, graph classification, and large scale node classification on datasets with up to 100 million nodes. It consistently outperforms strong baselines such as GRIT, GraphGPS, Exphormer, and SGFormer while being more efficient in training.\n\nIn summary, GraphFractalNet contributes a principled and scalable architecture that unifies spectral graph theory and sparse Transformer design, enabling efficient and expressive graph learning across diverse scales and structures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality:\nThe paper introduces a conceptually novel and elegant approach to scaling Graph Transformers by drawing inspiration from fractal geometry. The fractal attention mechanism, which recursively structures attention at multiple resolutions, represents an innovative step beyond existing sparse attention or hierarchical pooling methods. The combination of spectral embeddings, dynamic graph rewiring, and fractal attention into a unified Transformer architecture is both creative and technically coherent. Rather than proposing an incremental modification of existing models, the paper advances a genuinely new design principle that integrates structural sparsity and expressiveness within one framework. The theoretical connection between fractal self similarity and graph multi scale structure is particularly original and provides strong conceptual motivation.\n\nQuality:\nThe technical quality of the paper is strong. The authors provide both theoretical and empirical evidence supporting the soundness of the model. The expressiveness proofs, showing that GraphFractalNet can distinguish graph structures beyond the k Weisfeiler Lehman hierarchy, are rigorous and well situated within the graph learning literature. The proposed sub quadratic complexity analysis O(N log log N) is clearly derived and validated through runtime measurements. Experimental design is comprehensive, covering multiple types of graph learning tasks including molecular, social, and large scale node classification using diverse datasets up to 100 million nodes. Ablation studies further validate the contributions of each module such as spectral embedding, rewiring, and fractal attention, ensuring that the improvements are not artifacts of tuning or dataset bias.\n\nClarity:\nThe paper is well written and organized, balancing theoretical and practical insights effectively. The introduction clearly motivates the need for scalable yet expressive Graph Transformers and positions GraphFractalNet within that context. Diagrams and visualizations of the fractal attention hierarchy and dynamic rewiring steps make complex ideas intuitive. Mathematical derivations are detailed yet readable, and algorithmic procedures are summarized clearly. The presentation of empirical results, with consistent baselines and clear comparisons, demonstrates strong attention to reproducibility. Minor improvements could include a slightly more concise discussion of spectral properties, but overall the paper maintains clarity throughout.\n\nSignificance:\nThe significance of the work is high. GraphFractalNet addresses one of the central challenges in modern graph learning, scalability without sacrificing expressive power. The framework provides a path toward training Graph Transformers on truly large graphs without severe memory or runtime constraints, which could have major implications for molecular discovery, social network analysis, and knowledge graph modeling. By unifying spectral graph theory with fractal sparsity principles, the paper contributes both a new architectural paradigm and a practical solution to large scale graph representation learning. The demonstrated scalability to graphs with hundreds of millions of nodes, coupled with improved accuracy, suggests that the approach could become a foundation for future graph Transformer research."}, "weaknesses": {"value": "Weaknesses\n\nLimited interpretability and theoretical intuition of fractal attention:\nWhile the fractal attention mechanism is conceptually intriguing, its behavior is not deeply analyzed beyond performance metrics. The paper would benefit from a clearer explanation of why the fractal structure leads to better expressiveness and sparsity compared to existing sparse attention methods such as GraphGPS or Exphormer. For example, an empirical study showing how attention patterns evolve across fractal levels, or how node dependencies propagate differently under fractal recursion, would make the model more interpretable and theoretically grounded. Without this, the fractal analogy risks appearing more as a design metaphor than as a rigorously justified mechanism.\n\nScalability evaluation and resource reporting could be more detailed:\nAlthough the paper claims sub quadratic complexity and scalability to graphs with 100 million nodes, the experiments do not provide enough concrete runtime and memory benchmarks to fully verify these claims. For instance, the paper could include comparisons of training throughput, GPU memory usage, or wall clock time against existing efficient graph Transformers such as Exphormer, SGFormer, and GRIT. Clarifying the hardware setup and reporting scaling behavior with respect to both node and edge counts would make the scalability evidence more convincing.\n\nDependency on spectral preprocessing:\nGraphFractalNet relies on truncated Laplacian eigenvectors to generate spectral embeddings, which can be expensive for very large graphs. While the paper mentions using approximate spectral methods, it does not quantify their cost or error. A discussion or experiment analyzing the tradeoff between approximation quality and model performance would strengthen the practical feasibility argument. Moreover, the need for spectral preprocessing may limit applicability to dynamically evolving graphs where recomputation would be costly.\n\nAblation coverage and module interactions:\nThe ablation studies primarily test the effect of removing individual modules, but they do not fully explore interactions between them. For instance, it remains unclear how much of the gain comes from the spectral embedding versus the fractal attention, or whether dynamic rewiring contributes significantly when fractal sparsity is already applied. A more systematic factorial ablation could clarify these relationships and help isolate which components are most critical for performance.\n\nLimited diversity in downstream benchmarks:\nThe paper’s experiments span molecular, social, and large scale node classification datasets, but the evaluation could be broadened to include tasks that stress different aspects of graph reasoning such as temporal graphs, knowledge graphs, or relational reasoning benchmarks. Since the paper claims that GraphFractalNet captures multi scale structure efficiently, demonstrating its generalization to temporal or heterogeneous graphs would strengthen the impact and generality of the framework.\n\nClarity of empirical presentation:\nAlthough the paper is well structured overall, several figures and tables are densely packed and require cross referencing multiple sections to interpret. Providing more self contained captions, clearer axis labels for scaling plots, and a summarized comparison table of complexity and accuracy across baselines would improve readability and make the empirical claims easier to verify."}, "questions": {"value": "Clarification on the theoretical role of fractal attention:\nThe fractal attention mechanism is the conceptual centerpiece of GraphFractalNet, yet the paper does not clearly explain why fractal recursion specifically leads to improved representation power or sparsity compared to other hierarchical or sparse attention formulations. Could the authors provide additional theoretical intuition or empirical evidence (for example, visualization of attention distributions across fractal levels) to illustrate what structural dependencies fractal attention captures that standard local-global attention cannot?\n\nDetails on scalability and computational efficiency:\nThe paper claims sub quadratic complexity and scalability to graphs with up to 100 million nodes, but the experimental section provides limited concrete data on resource consumption. Could the authors include or describe runtime and memory usage comparisons against baselines such as Exphormer, GRIT, and SGFormer, ideally under a consistent hardware setup? Quantitative scaling curves or wall clock time measurements would help validate the efficiency claim.\n\nSpectral preprocessing overhead and approximation quality:\nSince the model relies on truncated Laplacian eigenvectors for spectral embedding, what is the computational cost of this step for large graphs, and how does approximation error in the eigen decomposition affect performance? Could the authors discuss whether the framework supports streaming or dynamically evolving graphs where recomputation of eigenvectors would be infeasible? This clarification is important for assessing practical deployment.\n\nInteraction between model components:\nThe paper presents three major innovations: spectral embeddings, dynamic rewiring, and fractal attention. Could the authors elaborate on how these components interact in practice? For instance, does dynamic rewiring meaningfully influence the spectral embedding or the fractal attention patterns, or are the improvements mostly additive? A more detailed ablation or dependency analysis would help clarify how each module contributes to the final performance.\n\nGeneralization to different graph domains:\nThe experiments focus on static molecular and social graphs. Could the authors discuss how GraphFractalNet might generalize to other graph types, such as temporal, heterogeneous, or knowledge graphs? Since fractal hierarchies imply multi scale representation, it would be interesting to know whether the architecture can handle evolving or multi relational structures without major modification.\n\nVisualization and interpretability of learned representations:\nGiven the hierarchical nature of fractal attention, visualization of node or subgraph embeddings at different fractal depths could provide insight into what the model learns at each scale. Would the authors consider including qualitative examples or embedding similarity maps to demonstrate that the model captures meaningful structural patterns across scales?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ePlqfbZGQ9", "forum": "X7lrOUfNKa", "replyto": "X7lrOUfNKa", "signatures": ["ICLR.cc/2026/Conference/Submission18808/Reviewer_VRHB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18808/Reviewer_VRHB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933552245, "cdate": 1761933552245, "tmdate": 1762999990967, "mdate": 1762999990967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}