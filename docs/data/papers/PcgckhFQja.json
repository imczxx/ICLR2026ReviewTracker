{"id": "PcgckhFQja", "number": 20618, "cdate": 1758308248853, "mdate": 1759896967740, "content": {"title": "STR-Bamba: Multimodal Molecular Textual Representation Encoder-Decoder Foundation Model", "abstract": "Most large-scale chemical language models are trained on a single textual molecular representation using self-supervised learning over large unlabeled corpora. These models excel in tasks such as property prediction and molecule generation by learning contextualized representations of input tokens. However, relying solely on one representation may result in the loss of structural or semantic information captured by alternative formats and may limit the model's ability to generalize across diverse molecular encodings. To address this limitation, we incorporate multiple textual molecular representations—including SMILES, SELFIES, molecular formula, IUPAC name, International Chemical Identifier (InChI), serialized polymer graph (SPG), and electrolyte formulations in an unified vocabulary to harness the unique strengths of each format. Here, we introduce a large encoder-decoder chemical foundation model based on the Bamba architecture, a hybrid of Transformers and Mamba-2 layers, designed to support multi-representational inputs. The model is pre-trained in a BERT-style on 588 million samples, resulting in a corpus of approximately 29 billion molecular tokens. These models serve as a foundation for language chemical research in supporting different complex tasks, including molecular properties prediction, classification, and molecular translation. Furthermore, extensive studies of the multimodal molecular latent space indicate cross-representation alignment and reveal how different textual encodings of the same molecule can converge toward a unified semantic representation. This shared space may facilitate deeper insights into molecular structure, enhance generalization, and support a broad range of downstream applications.", "tldr": "", "keywords": ["Foundation Model", "Transformer", "Mamba-2", "SMILES", "SELFIES", "InChI", "IUPAC Name", "Molecular Formula", "Polymer SMILES", "Electrolyte Formulation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/241ac7894e7d4d00db51e2776b4905bb7b35cf23.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present STR‑Bamba, a 426M‑parameter encoder–decoder chemical foundation model that combines Transformer and Mamba‑2 layers in a hybrid architecture, trained on 588M samples spanning seven textual molecular modalities (SMILES, SELFIES, formula, IUPAC, InChI, serialized polymer graph, electrolyte formulations). A unified tokenizer with modality tags is used. Pre‑training proceeds in two phases: encoder pretraining (masked token + sentence-level equivalence to align representations across modalities), then decoder training conditioned on encoder embeddings. The paper reports analyses of the latent space (t‑SNE + K‑means across modalities) and downstream property prediction across 29 datasets as well as translation between representations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Comprehensive modality coverage in a single vocabulary with a practical tokenizer.\n* Large‑scale pretraining and efficient architecture likely to benefit long‑context chemistry tasks.\n* Latent alignment evidence: 7‑cluster structure matches the number of modalities; quantitative clustering scores reported. (Fig. 2 & Table 2 in the results section)"}, "weaknesses": {"value": "* Limited baselines/ablations: Need apples‑to‑apples comparisons vs. similarly sized Transformer‑only and Mamba‑only models under controlled token budgets. Architectural choices should be empirically justified.\n* Attribution of gains: Separate the effects of multimodality from scale and architecture via controlled experiments.\n* Task breadth: Polymer/electrolyte tasks are highlighted in pretraining; ensure strong downstream validations for those domains, not only small molecules."}, "questions": {"value": "* How does STR‑Bamba perform vs. a Transformer‑only encoder–decoder with the same parameter count and token budget? Can you provide a controlled ablation?\n* What is the contribution of each modality? Can you report leave‑one‑modality‑out ablations on downstream tasks?\n* For translation tasks, can you report exact‑match and structure‑match rates with CIs across held‑out molecules, and failure analyses (e.g., stereochemistry)?\n* How are the representations aligned? If I understand correctly, Figure 2(a) shows that each modality is clustered separately than other modalities, even for the sam molecule. What exactly do you mean by \"alignment of modalities\"?\n\nComment:\n* Line 224: \"We used 396 and 8 NVIDIA A100 (40GB) GPUs to train\nphase 1 and phase 2, respectively.\": What does 'We used 396 and 8 NVIDIA A100' mean? A typo?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hr4sf9QSTq", "forum": "PcgckhFQja", "replyto": "PcgckhFQja", "signatures": ["ICLR.cc/2026/Conference/Submission20618/Reviewer_ugbg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20618/Reviewer_ugbg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603291852, "cdate": 1761603291852, "tmdate": 1762934020982, "mdate": 1762934020982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to integrate the multiple molecular notations into one pretrained model, to complement the important molecular information across different molecular notations. Based on this, the authors pretrain a hybrid model that utilizing both Transformer and Mamba architectures. In experiments part, STR-Bamba generally outperform the existing baselines under various tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The presentation is clear and the paper is easy to follow.\n\n2. The experiments across 29 benchmark datasets are comprehensive to validate the effectiveness of the proposed method.\n\n3. The motivation is clear to combine all the molecular notations to complement all molecular information."}, "weaknesses": {"value": "1. Although the motivation sounds reasonable, can authors provide a specific example to demonstrate the importance of incorporating each kind of molecular notation? It will be of great help to demonstrate which notation includes which kinds of information. And I am also wondering if it is necessary to incorporate all kinds of notation? Is it possible the information from one notation has been supplemented by the other notations?\n\n2. The authors claim the proposed method is inference-effienct. However, it seems no such experiment has been conducted to support such claim.\n\n3. The ablation study or analysis of introducing the multiple molecular notations is missing. While the authors have conducted some experiments (latent space and notation translation), the necessity of introducing the comprehensive molecular notations is still not convincing.\n\n4. The technical novelty is relatively weak. The model is basically adapted from existing works."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CkFNoapLcH", "forum": "PcgckhFQja", "replyto": "PcgckhFQja", "signatures": ["ICLR.cc/2026/Conference/Submission20618/Reviewer_vsgh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20618/Reviewer_vsgh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752911974, "cdate": 1761752911974, "tmdate": 1762934020495, "mdate": 1762934020495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes STR-Bamba426M, a hybrid encoder-decoder model for molecular language modeling that combines Transformer layers with Mamba-2 state-space layers. The model is designed to handle multiple molecular textual representations (SMILES, SELFIES, SPG, molecular formula, InChI, IUPAC names, and electrolyte formulations) within a unified vocabulary. The authors implement a custom tokenizer with modality-specific preprocessing and train the model on a large dataset comprising 117M small molecules, 2M polymer structures, and 258 electrolyte formulations. The model leverages shared token and sentence embeddings to align different representations of the same molecule, and employs cross-attention layers in the decoder to generate valid molecular sequences. Pre-training is performed in two stages: first, the encoder learns embeddings and aligns molecular formats; second, the decoder is trained to reconstruct molecular sequences conditioned on the encoder embeddings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Multimodal molecular representation: The model can handle multiple types of molecular textual inputs simultaneously, which may allow richer information capture compared to single-representation models.\n\n* Shared embeddings and alignment strategy: The use of token + sentence embeddings and alignment across molecular formats is a thoughtful design for improving the encoder’s representation quality.\n\n* Large-scale dataset: The dataset is extensive, including both small molecules and polymers, which can help in testing generalization across chemical domains."}, "weaknesses": {"value": "* Questionable need for long-range modeling: The justification for using Mamba-2 state-space layers to model “longer context lengths” is weak, because most molecular sequences (even when concatenating multiple representations) are relatively short compared to genomic sequences or long text, which state-space models were originally designed for. The added complexity may not be necessary and could likely be replaced with a simpler Transformer-only architecture.\n\n* Data quality issues: Synthetic polymer structures may include non-viable molecules; the impact of this on model performance is not well analyzed.\n\n* Tokenizer and vocabulary generalization: It is unclear how well the custom tokenizer handles rare molecules or new unseen formats. There is also no discussion on potential overfitting to frequent substructures."}, "questions": {"value": "* The authors mention that InChI tokens are tokenized by extracting atom-associated numbers. Given that InChI tokens are highly structured and interdependent, could the authors clarify how this tokenization preserves the chemical and structural semantics inherent in InChI?\n\n* What is the inference speed and memory requirement for STR-Bamba426M compared to simpler Transformer models for typical molecular inputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jC04VlnE5F", "forum": "PcgckhFQja", "replyto": "PcgckhFQja", "signatures": ["ICLR.cc/2026/Conference/Submission20618/Reviewer_WHBW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20618/Reviewer_WHBW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968033006, "cdate": 1761968033006, "tmdate": 1762934019779, "mdate": 1762934019779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces STR-Bamba426M, a 426-million-parameter multimodal encoder–decoder foundation model for molecular data. Unlike prior chemical LLMs that rely on a single representation (e.g., SMILES or SELFIES), STR-Bamba unifies seven textual modalities particularly SMILES, SELFIES, IUPAC, InChI, molecular formula, serialized polymer graphs (SPG), and electrolyte formulations, within a shared vocabulary and hybrid Transformer + Mamba-2 architecture. The model is pre-trained on 588 million molecular samples (≈29 billion tokens), using PubChem, synthetic polymers, and electrolyte data, then evaluated on MoleculeNet, polymer property prediction, and electrolyte formulation benchmarks. The authors claim near- or better-than-SOTA results on 9 of 11 MoleculeNet tasks, strong performance across 26 polymer tasks, and effective representation translation among molecular notations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "There is a comprehensive pre-training corpus (588 M samples; 29 B tokens) across small molecules, polymers, and electrolytes.\n\nThe hybrid architecture combines both long-context efficiency (Mamba-2) and attention flexibility.\n\nThere is a unified tokenizer for multiple modalities, which is a significant engineering contribution.\n\nExperiment results are thorough with extensive benchmarking across small molecules, polymers, and electrolytes.\n\nThere is demonstrated cross-representation latent alignment like t-SNE + K-means."}, "weaknesses": {"value": "This work lacks empirical rigor. Specifically, there is no ablation isolating modality impact or architecture components.\n\nMoreover, the comparisons are lacking evaluation against state of the art models. Specifically, head-to-head with MolX (KDD 2024), MolXPT, or Uni-Mol 3D under identical tuning comparisons are omitted.\n\nThis work is also not reproducible. Specifically, it is missing training FLOPs, parameter scaling laws, or energy footprint. Code/weights unavailable, tokenization details are also incomplete.\n\nThe writing can also be improved. It is too long, verbose, and sometimes repetitive for a 9-page main paper format.\n\nSome critical works are also missing for citation. These include the below: \n\n- Le, Khiem, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, Ting Hua & Nitesh V. Chawla. MolX: Enhancing Large Language Models for Molecular Understanding With A Multi-Modal Extension. Proceedings of the 2025 ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (MLoG-GenAI@KDD ’25), ACM, 2025. \n\n- Liu, Yong, et al. MolX: Enhancing Large Language Models for Molecular Understanding with a Multi-Modal Extension. KDD Proceedings, 2024. \n\n- Soares, Eduardo A., et al. “An Open-Source Family of Large Encoder–Decoder Foundation Models for Chemistry.” Communications Chemistry, vol. 8, no. 1, 2025."}, "questions": {"value": "How do you prevent data leakage between PubChem-derived pre-training and MoleculeNet test sets?\n\nWhat is the relative contribution of Mamba-2 layers vs. Transformers? Please include ablations.\n\nHow sensitive are results to tokenizer vocabulary size (5 k) and modality balance?\n\nCan the model generalize to unseen representations (e.g., CXSMILES or graphical forms)?\n\nWhat is the computational cost (GPU-hours, energy) of pre-training vs. SSM-only baselines?\n\nHow does translation fidelity compare with Chemformer or MolXPT using BLEU or Tanimoto metrics?\n\nWill you release the code and tokenizer to the community?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "This work uses open PubChem and literature data but no explicit license verification.\n\nThe environment cost of running this model should also be taken into consideration. There are 400+ GPUs over two phases (396 A100s + 8 A100s) suggests > 1 MWh consumption; no disclosure."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WiFPn1XCoL", "forum": "PcgckhFQja", "replyto": "PcgckhFQja", "signatures": ["ICLR.cc/2026/Conference/Submission20618/Reviewer_VYHY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20618/Reviewer_VYHY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20618/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762292253700, "cdate": 1762292253700, "tmdate": 1762934019175, "mdate": 1762934019175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}