{"id": "twbMFL0DMp", "number": 10833, "cdate": 1758182880057, "mdate": 1763653394509, "content": {"title": "Muon Outperforms Adam in Tail-End Associative Memory Learning", "abstract": "The Muon optimizer is consistently faster than Adam in training Large Language Models (LLMs), yet the mechanism underlying its success remains unclear. This paper demystifies this mechanism through the lens of associative memory. By ablating the transformer components optimized by Muon, we reveal that the associative memory parameters of LLMs, namely the Value and Output (VO) attention weights and Feed-Forward Networks (FFNs), are the primary contributors to Muon’s superiority. Motivated by this associative memory view, we then explain Muon’s superiority on real-world corpora, which are intrinsically heavy-tailed: a few 'head' classes are extremely frequent, while a vast number of 'tail' classes are individually rare. The superiority is explained through two key properties: (i) its update rule consistently yields a more isotropic singular spectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes tail classes more effectively than Adam. Beyond empirical evidence, we theoretically confirm these findings by analyzing a one-layer associative memory model under class-imbalanced data. We prove that Muon consistently achieves balanced learning across classes regardless of feature embeddings, whereas Adam can induce large disparities in learning errors depending on embedding properties. In summary, our empirical observations and theoretical analyses reveal Muon’s core advantage: its update rule aligns with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions than Adam.", "tldr": "", "keywords": ["Transformers", "Muon", "Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6c4d9ea856d2d7d75ae3022bd9db1a6f40048ced.pdf", "supplementary_material": "/attachment/66b5e75d9bd1c8c1bb3757c483517b146b4b1ebc.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies why Muon often outperforms Adam in LLM-pretraining. The central claim is that Muon’s advantage concentrates in associative-memory components, specifically the Value/Output (VO) attention matrices and FFNs. Muon's update rule yields a more isotropic singular spectrum, which in turn improves learning on tail classes in heavy-tailed distributions. The authors support this with \n1. component-wise optimizer ablations in Transformers,showing that VO+FFN accounts for most of the validation-loss gains for Muon.\n2. measurements of more isotropic singular-value spectra under Muon than under Adam. \n3. a knowledge-intensive QA task reflecting heavy-tailed data where Muon shows larger gains on tail examples.\n4. a theoretical analysis of a one-layer linear associative memory under class imbalance showing Muon’s superiority."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Focused component ablations.** The blockwise ablation is informative and directly isolates VO/FFN as primary beneficiaries, supporting the associative-memory lens.\n\n2. **Heavy-tail perspective with tail-class gains**. The paper emphasizes tail performance and links it to spectral isotropy, providing a plausible causal chain: Muon → more balanced singular spectrum → better tail learning.\n\n3. **Theory that matches the story.** The one-layer associative-memory model with class imbalance theoretically explains why Muon outperforms Adam under heavy-tailed class distributions, through the model is simple.\n\n4. **Muon wins over Adam, and community does not know why!**\n5. **LLM settings are diverse.** Covers both gated and non-gated 160M/700M LLM."}, "weaknesses": {"value": "1. **Multiple kinds of heavy-tailness** The paper presents servral kinds of heavy-tailness: data, knowledge, parameter and update. There are evidence in the paper to connect the heavy-tailness of data with the heavy-tailness of knowledge, there are also evidence to connect the heavy-tailness of parameter with the heavy-tailness of update. However, it remains unclear how to connect the heavy-tailness of data and knowledge with the heavy-tailness of parameter and update in the LLM.\n2. **$W_V$ as associative memory** Prior work rarely frames $W_V$ as an associative-memory parameter. Although the paper argues $W_V$ and $W_O$ play symmetric roles, that symmetry breaks under MQA/GQA. Treating VO jointly is understandable as a number of literatures[2][3] say that VO(Value-Output) are similar and can be considered together in practice even under the context of MQA/GQA, but a restatement is needed. \n3. **Observation 2 seems to be known in literature** The figure 4 in [3] already points out Observation 2 that Muon consistently yields more isotropic weight matrices with broadly distributed spectral energy than Adam.\n4. **FFN over QK may be the result of parameter counting** FFN over QK may be the result of parameter counting, not because of associative memory.\n5. **Reproducibility** The `model` are missing from the released code, making the work not fully reproducible.\n\n[1] Lin, C. H., Gao, S., Smith, J. S., Patel, A., Tuli, S., Shen, Y., ... & Hsu, Y. C. MoDeGPT: Modular Decomposition for Large Language Model Compression. In The Thirteenth International Conference on Learning Representations. \\\n[2] Wang, J., Wang, M., Zhou, Z., Yan, J., & Wu, L. The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training. In Forty-second International Conference on Machine Learning. \\\n[3] Liu, J., Su, J., Yao, X., Jiang, Z., Lai, G., Du, Y., ... & Yang, Z. (2025). Muon is scalable for LLM training. arXiv preprint arXiv:2502.16982."}, "questions": {"value": "1. **Capacity control** If you equalize parameter counts (e.g., only optimize $W_{in}$ parameter in some depth by Adam), do Muon-over-Adam gaps persist in QK vs. FFN?\n2. **Multiple kinds of heavy-tailness** Can you connect the heavy-tailness of data and knowledge with the heavy-tailness of parameter and update in the context of LLM?\n3. **Logic gap on associative-memory mechanism** The paper does not yet provide strong causal evidence that Muon surpasses Adam *specifically* because of associative-memory effects. The superiority on the knowledge-intensive QA task might not be attributable to associative memory per se. VO+FFN may contribute more ('may' is due to weakness 2 and 4), but the mechanistic link to associative memory remains suggestive rather than conclusive.\n\nOverall, the paper discusses a vital problem in the optimizer community. However, currentlymy tentative recommendation is borderline Weak Accept/Accept. I’m open to revising my score if these concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k3g3FOPFeQ", "forum": "twbMFL0DMp", "replyto": "twbMFL0DMp", "signatures": ["ICLR.cc/2026/Conference/Submission10833/Reviewer_sugx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10833/Reviewer_sugx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760539855176, "cdate": 1760539855176, "tmdate": 1762922037241, "mdate": 1762922037241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies why Muon often outperforms Adam in LLM-pretraining. The central claim is that Muon’s advantage concentrates in associative-memory components, specifically the Value/Output (VO) attention matrices and FFNs. Muon's update rule yields a more isotropic singular spectrum, which in turn improves learning on tail classes in heavy-tailed distributions. The authors support this with \n1. component-wise optimizer ablations in Transformers,showing that VO+FFN accounts for most of the validation-loss gains for Muon.\n2. measurements of more isotropic singular-value spectra under Muon than under Adam. \n3. a knowledge-intensive QA task reflecting heavy-tailed data where Muon shows larger gains on tail examples.\n4. a theoretical analysis of a one-layer linear associative memory under class imbalance showing Muon’s superiority."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Focused component ablations.** The blockwise ablation is informative and directly isolates VO/FFN as primary beneficiaries, supporting the associative-memory lens.\n\n2. **Heavy-tail perspective with tail-class gains**. The paper emphasizes tail performance and links it to spectral isotropy, providing a plausible causal chain: Muon → more balanced singular spectrum → better tail learning.\n\n3. **Theory that matches the story.** The one-layer associative-memory model with class imbalance theoretically explains why Muon outperforms Adam under heavy-tailed class distributions, through the model is simple.\n\n4. **Muon wins over Adam, and community does not know why!**\n5. **LLM settings are diverse.** Covers both gated and non-gated 160M/700M LLM."}, "weaknesses": {"value": "1. **Multiple kinds of heavy-tailness** The paper presents servral kinds of heavy-tailness: data, knowledge, parameter and update. There are evidence in the paper to connect the heavy-tailness of data with the heavy-tailness of knowledge, there are also evidence to connect the heavy-tailness of parameter with the heavy-tailness of update. However, it remains unclear how to connect the heavy-tailness of data and knowledge with the heavy-tailness of parameter and update in the LLM.\n2. **$W_V$ as associative memory** Prior work rarely frames $W_V$ as an associative-memory parameter. Although the paper argues $W_V$ and $W_O$ play symmetric roles, that symmetry breaks under MQA/GQA. Treating VO jointly is understandable as a number of literatures[1][2] say that VO(Value-Output) are similar and can be considered together in practice even under the context of MQA/GQA, but a restatement is needed. \n3. **Observation 2 seems to be known in literature** The figure 4 in [3] already points out Observation 2 that Muon consistently yields more isotropic weight matrices with broadly distributed spectral energy than Adam.\n4. **FFN over QK may be the result of parameter counting** FFN over QK may be the result of parameter counting, not because of associative memory.\n5. **Reproducibility** The `model` are missing from the released code, making the work not fully reproducible.\n\n[1] Lin, C. H., Gao, S., Smith, J. S., Patel, A., Tuli, S., Shen, Y., ... & Hsu, Y. C. MoDeGPT: Modular Decomposition for Large Language Model Compression. In The Thirteenth International Conference on Learning Representations. \\\n[2] Wang, J., Wang, M., Zhou, Z., Yan, J., & Wu, L. The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training. In Forty-second International Conference on Machine Learning. \\\n[3] Liu, J., Su, J., Yao, X., Jiang, Z., Lai, G., Du, Y., ... & Yang, Z. (2025). Muon is scalable for LLM training. arXiv preprint arXiv:2502.16982."}, "questions": {"value": "1. **Capacity control** If you equalize parameter counts (e.g., only optimize $W_{in}$ parameter in some depth by Adam), do Muon-over-Adam gaps persist in QK vs. FFN?\n2. **Multiple kinds of heavy-tailness** Can you connect the heavy-tailness of data and knowledge with the heavy-tailness of parameter and update in the context of LLM?\n3. **Logic gap on associative-memory mechanism** The paper does not yet provide strong causal evidence that Muon surpasses Adam *specifically* because of associative-memory effects. The superiority on the knowledge-intensive QA task might not be attributable to associative memory per se. VO+FFN may contribute more ('may' is due to weakness 2 and 4), but the mechanistic link to associative memory remains suggestive rather than conclusive.\n\nOverall, the paper discusses a vital problem in the optimizer community. However, currentlymy tentative recommendation is borderline Weak Accept/Accept. I’m open to revising my score if these concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k3g3FOPFeQ", "forum": "twbMFL0DMp", "replyto": "twbMFL0DMp", "signatures": ["ICLR.cc/2026/Conference/Submission10833/Reviewer_sugx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10833/Reviewer_sugx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760539855176, "cdate": 1760539855176, "tmdate": 1763348112988, "mdate": 1763348112988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors mainly did an abalation study of Muon. They showed two major and novel findings: 1. applying muon only on VO+FFN can recover the full muon training. 2. Muon narrows the head-tail performance gap when learning highly imbalance datasets. They also provided a preliminary theory to explain their findings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The ablation study of this work is sound to me. Muon only works for VO and FFN. This finding is somewhat interesting and might inspire following works to promote Muon's ability over QK.\n2. The heavy-tail findings is also interesting, which promotes the appliablity of Muon in practice."}, "weaknesses": {"value": "**The novelty and contributions of this paper could be further improved.**\n- Three observations are provided. Among these, the second observation is already presented in Liu et al. (2025). Though the differences between them are noted by the authors, such as Liu et al. (2025) conduced experiments on MoE models whereas this paper focues on Dense Models, these reasons cannot convince me about the novelty. \n- The first observations are good, yet I expect more analysis. A central question is \"Why Muon is more effective on VO+FNN than QK\", yet this paper didn't address this question. They attribute this phenomenon to the associative memory property of VO and FFN, but the causal relationship between these is not clear. Is the heavy-tail task ability of Muon relate to the effectiveness of Muon on VO+FNN?\n- In addition, via the third observation, the authors further highlighted the connection between Muon and the associative memory. However, I also doubt that if the third observation can fully supports that Muon acquires knowledge more evenly than AdamW. Muon indeed prompts the performance on heavy-tail tasks, but it might restrict to knowledge acquring tasks. I expect more experiments across various tasks.\n\n**The toy theory seems to be intuitive**\n- The theory seems to be intuitive. Muon produces updates evenly spread over all directions compared to GD and SignGD. Then Muon must give even/uniform weight matrices, and thereby in this simple associative memory modeling, mitigating the unbalance learning problem."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gaEnrFarVx", "forum": "twbMFL0DMp", "replyto": "twbMFL0DMp", "signatures": ["ICLR.cc/2026/Conference/Submission10833/Reviewer_Yhie"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10833/Reviewer_Yhie"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815045060, "cdate": 1761815045060, "tmdate": 1762922036779, "mdate": 1762922036779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies what transformer components benefit most from Muon as compared to Adam. The authors find that the main benefit comes from the VO and FFN blocks, which correspond to the associative memory stores. The authors use a heavy-tailed knowledge task to compare Adam and Muon and provide theoretical insights on one-layer models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The imbalanced knowledge experiment is a nice set up and the ablations over which layers most benefit from Muon are interesting\n- Provides an interesting connection between optimizer performance and prior work on associative memory \n- The writing and research motivation are clear"}, "weaknesses": {"value": "- Experiments are limited to a synthetic heavy-tailed knowledge task. It would be interesting to see in addition a task that relies heavily on QK to support the claim that Muon is not as important to apply to QK.\n- Language modeling results are limited to 160M parameter transformers. It would be interesting to see the main ablation (Muon only on VO+FFN) at a larger scale and performance measured on downstream tasks."}, "questions": {"value": "- Do you have any intuition on why benefits from Muon seem to diminish with scale (/if this pertains to associative memory)?\n- It seems like the Newton-Schulz application could be an interesting knob to turn here -- have you tried varying the Newton Schulz coefficients/iterations to vary how much of the spectrum Muon is capturing and test how this affects associative memory?\n- What LR schedule did you use for Adam and Muon respectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LSAZsoZet1", "forum": "twbMFL0DMp", "replyto": "twbMFL0DMp", "signatures": ["ICLR.cc/2026/Conference/Submission10833/Reviewer_W4iz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10833/Reviewer_W4iz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836072586, "cdate": 1761836072586, "tmdate": 1762922036452, "mdate": 1762922036452, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission investigates the reason for the improved performance of Muon over Adam on transformers. It establishes empirically that the weights of a transformer architecture that benefit most from Muon-style updates are the non-key/query parameters of the attention modules and the parameters of the feedforward networks. To explain this behaviour, the authors propose an explanation based on different frequencies of \"fact\" representations in an associative memory module, and show theoretically that Muon-style updates lead to more uniform convergence across different frequencies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "To my knowledge, this is the submission that attempts to provide empirical evidence for the source of the benefit of Muon updates over Adam on language models. The identification of specific weights that benefit most from Muon and the corresponding explanations help us understand the difficulties of optimization in transformer-based architecture, and hopefully design better methods. The experiment on the synthetic dataset with imbalanced facts is well-designed an provides good evidence for the claim that Muon helps most on associative memories. \n\nI am generally positive about the submission. My initial score for the review is low due to the issues listed below. Those should be addressable with some revisions to the presentation and an additional experiment. I am expecting to increase my score if that can be added."}, "weaknesses": {"value": "My main issue is with the presentation of the results, specifically in Figure 1, 2, 4 and §4.1.Those are dense and require a lot of care on the reader's part to understand what is being shown. This complexity does not appear necessary. A pass on the issues are detailed below could improve the clarity of the presentation significantly.\n\nOn the evidence, the main argument of the submission, that Muon helps with imbalance in \"facts\" stored in an associative memory. It appears difficult to distinguish between this explanation and the more general imbalance in token frequencies in text data for the success of Adam. See below of a possible additional experiment that would strengthen the argument of the submission.\n\nThe submission would be much stronger with those changes, and I would be happy to increase my score if they are addressed.\n\n## Distinguishing between frequency imbalance in \"facts\" vs. general imbalance in text\n\nThe argument presented by the submission is that the benefit of Muon over Adam stems from an imbalance in the frequencies of different \"facts\" stored in an associative memory.\nThe controlled experiment with a synthetic dataset in §4.1 is a very nice illustration of this point. However, we could also expect this imbalance to stem simply from the distribution of tokens in text data, which follow Zipf's law. Those have been shown to lead to a better performance of Adam over SGD (Kunstner et al., 2024) primarily due to the update in the first and last embedding layers (Zhao et al., 2024; Zhang et al., 2024). One hypothesis could be that Muon helps on additional layers for the same issue of imbalanced tokens, not specifically facts in associate memories. Adam cannot correct these imbalances beyond the embedding layers because they are no longer axis-aligned, while Muon could. \n\nThe main difference I am drawing between those two hypotheses is the whether the main difficulty is due to an imbalance in token frequencies or an imbalance in the frequencies of \"facts\" to be stored in associate memories. As facts are also represented as tokens, there is some overlap between those ideas, but there is a way to disentangle these two explanations. If the arguments of the submission is correct, that Muon target imbalances in \"facts\" and associate memories beyond mere imbalance in token frequencies, I would expect the performance gap between Muon and Adam to decrease as the fact imbalance is reduced. The synthetic dataset prepared by the authors could be modified to exhibit various levels of imbalance, varying the imbalance continuously between the approximate power-law distribution in Figure 3 to a uniform distribution. The imbalance in token frequencies should still follow a power-law (assuming the different templates are rich enough to approximate natural text, which should be checked), but the imbalance in \"facts\" would be controlled. If the performance gap between Muon and Adam shrinks as the \"fact\" imbalance becomes more uniform, this would give a lot more strength to the proposed fact-based explanation.\n\n\n\n## Presentation of the results\n\nThe specific recommendations below are only one possible way to improve the presentation, meant to help communicate the difficulties I encountered in understanding the results. The authors know their work best and might have other ideas on how to improve the presentation that are more appropriate. \n\n\n**Figure 1**:  \nThere are too many lines to be able to easily tell which combination of architecture/optimizer is performing best.\nThis detailed plot could be useful to readers wanting to see the behaviour over time, \nbut the main point of Figure 1 would be better illustrated by a simple bar plot of the final performance, \nusing more legible distinctions (colors/makers) or having the description of the setup directly above the bar. \n\nThe figure also combines gated and non-gated variants of the architecture, \nbut it is not clear why this distinction is important for the point being made.\nThe plot could be made much simpler by focusing on one of the two variants, \nand pointing the reader to \"the same happens on the gated version, see Appendix X\".\n\n**Figure 2**  \nsuffers from a similar issue.\nThe plot shows 4 metrics measuring the spread of the eigenvalues, \nsome which should be small while other should be large, \nfor 2 weights (VO, $W_{\\mathrm{out}}$) on 2 architectures (gated, non-gated).\nThis makes it hard to extract the main point of the figure at a glance.\nA simpler variant of the figure could focus on one architecture and one metric,\nrelegating the others to the appendix with the comment that a similar behaviour is observed,\nor use this space to show the full distribution of the spectrum over time.\n\nAn interesting addition to Figure 2 could be to show the QK weights. The Muon update is apparently less helpful for those weights; does the spectrum give a hint as to why?\n\n**Figure 4 (b, c)**:  \nI had a hard time understanding those figures, and what those metrics tell us about the update. \nI am not sure which is the best way to improve the presentation, but here are things I found confusing: \n\nThe figure 4b is implicitly parameterized by a step-size which is being varied. This can only be understood from the main text (not the caption), which should be fixed. Even with this information, it is not easy to see the \"step-size\" on the plot. This might be because the lines \"start\" at the bottom right ($\\eta = 0$ is at $(10^{.8}, 0)$) and go to the top left as $\\eta$ increases, going against the standard left-to-right/top-to-bottom reading direction. Flipping the x and y axes might help. Another option could be to use a plot with 2 y-axes (or two subplots on top of each other) that would show the $\\Delta$ vs. step-size and the corresponding loss vs. step-size. The same treatment could be applied to Figure 4c (with number of updates as the x-axis). Currently, 4b and 4c have a different experimental setting (varying step-size vs. varying time) but it is not clear from the figures/caption that they differ in this way.\n\n**Theoretical results in §4.2**  \nAssumption 4.2 seems overly complex for its uses in Theorem 4.3. The parameters $\\alpha$ and $\\beta$ appear to impact the results only through the ratio $r(\\alpha,\\beta)$. Since it happens to be $\\frac{\\min_i p_i}{\\max_i p_i}$, wouldn't it be easier to just define the degree of imbalance as $r = \\min_i p_i / \\max_i p_i$ directly, and avoid the extra notation of $\\alpha, \\beta$? If not, writing the overall loss after Assumption 4.2 as a function of $\\alpha, \\beta$ might be helpful.\n\nThe only place where I see their use is in the proof for Adam, but the range of behavior observed under the $(\\alpha,\\beta)$ assumption would also be observed on a model with imbalance ratio $r$ since it's strictly less restrictive. If I'm missing something and the $\\alpha, \\beta$ are important for the proof, they should still be mentioned, but an explanation of what $r(\\alpha,\\beta)$ is should be given around Assumption 4.2 to help the reader.\n\nCould some of the notation be simplified by assuming that the frequencies are sorted? For example $\\min_{k\\in[K]}$ and $\\max_{k\\in[K]}$ in Eq. 4.1 could be replaced by $p_K$ and $p_1$ if we assume the probabilities are decreasing.\n\nThe description of the stopping condition would also benefit from a slightly more wordy explanation. The start of §4.2 takes a while to parse. Something like \"For each optimizer, we take one step with step-sizes $\\eta$, selected such that the accuracy of the most frequent class reaches $1-\\epsilon$. We then look at the accuracy achieved on the least frequent class.\"\n\n\n## Additional references \n\nThe submission should cite and discuss the following works which have done similar ablation studies on Adam to justify its performance on language models and identify that they help most with the first and last embedding layers. This does not detract from the contributions of the submission, but those works should be acknowledge.$\n\n- Deconstructing What Makes a Good Optimizer for Language Models\n  Rosie Zhao, Depen Morwani, David Brandfonbrener, Nikhil Vyas, Sham Kakade\n  https://arxiv.org/abs/2407.07972\n- Adam-mini: Use Fewer Learning Rates To Gain More\n  Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Diederik P. Kingma, Yinyu Ye, Zhi-Quan Luo, Ruoyu Sun\n  https://arxiv.org/abs/2406.16793\n\n\n## Minor/Typos\n\n- \"intrinsically heavy-tailed: a few classes (tail classes) appear far less frequently than others\"\n  Is this a typo? Presumably if only a few classes are rare, their impact on the overall loss is small and Muon would not help much.\n- It is not clear where Theorem 4.3 ends. Could the paragraph breaks \"For GD, ... For Muon, ... For Adam, ...\" be placed in an itemize block to help that?\n- \"highly sensitive\" as a description of Adam's behavior in Figure 2 seems exaggerated. \"More sensitive\" would be appropriate."}, "questions": {"value": "**Why does Muon help more on the VO/FFN than on the QK weights?**  \nThis is the main distinction found in the first set of experiments. \nThe theory tries to explain why the VO and FFN weights are helped by Muon by drawing on the connection to associative memories, \nbut I have some reservations about this explanation.\n\nThe QK weights are also part of the attention mechanism and form an outer-product based on the representation of the tokens. Why would the associative-memory explanation not apply to them as well? The FFN weights do not seem to follow this outer-product structure, so why is the associate-memory explanation relevant for them?\n\nIt might be that the associative-memory explanation is most relevant for the VO weights and do not explain the benefits on FFN weights or the reduced benefits on QK weights.\nBut if that is the case, this should be acknowledged more explicitly in the submission.\n\nBeyond this theoretical explanations, it would be helpful for future work if the authors could look at the experiments and data they have collected to see if there is some empirical difference between the QK weights and the VO/FFN weights that could explain the different behavior, for example a different spread in the spectrum of the singular values of the inputs of each layer? This is not strictly necessary but even a speculative explanation (based on some empirical observation) might be helpful for future work,\n\n**Why the focus on Gated vs. Ungated FFN?**  \nIs there a-priori a reason to believe that Muon would behave differently? \n\n**How is the step-size set in Figure 1?**  \nIt is not clear to me whether the results presented in Figure 1 use the same step-size for both Adam and Muon, \nor use a tuned step-size for each optimizer. Could the authors clarify this point?\n\nThe later would seem more appropriate as it is not clear that the same step-size is optimal for both optimizers, especially when used at the same time for different weights.\nAlthough it would then require a comparison against using Adam for both subset of weights with a different step-size for each subset to account for the possibility that the improvement comes from using different step-sizes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aOUTky0JC7", "forum": "twbMFL0DMp", "replyto": "twbMFL0DMp", "signatures": ["ICLR.cc/2026/Conference/Submission10833/Reviewer_E1d8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10833/Reviewer_E1d8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10833/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762362192152, "cdate": 1762362192152, "tmdate": 1762922035817, "mdate": 1762922035817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of our main revisions"}, "comment": {"value": "Dear AC and Reviewers,\n\nWe sincerely thank you for the time and effort you have devoted to reviewing our work. We are very grateful for your insightful comments, which have helped us further improve the quality and clarity of the paper. In response to the reviewers’ constructive feedback, we have made the following main revisions:\n\n- **In response to Reviewer E1d8**, all experiments for the gated FFN variant have been moved to a dedicated section in Appendix G.3.  \n\n- **In response to Reviewer E1d8 Weakness 1**, Appendix G.8 now evaluates Muon on datasets that contain the same bio-information under different degrees of heavy-tailedness, by explicitly varying association frequencies while keeping single-token frequencies distributed according to Zipf’s law. These results indicate that the heavy-tailedness of facts can be **approximately independent of the tokens’ Zipf marginals** and instead depends on associations, i.e., co-occurrences, of tokens.\n  \n- **In response to Reviewer W4iz Weakness 1**, Appendix G.10 of the revised version now includes experiments where we train LLMs with Muon and Adam on an in-context linear regression task that primarily depends on learning the QK parameters well. Their similar performance on this task indicates that **they behave similarly when optimizing QK parameters**, consistent with our conclusion.\n\n- **In response to Reviewer Yhie Weakness 3**, we have added a new experiment on the WikiText-103 language modeling benchmark in Appendix G.9, whose token frequencies follow a heavy-tailed distribution. The benefit of Muon over Adam on the tail classes in this task further verifies that our conclusions hold beyond the knowledge-acquisition setting and **extend to general text**.\n\n- **In response to Reviewer sugx Weakness 4 and Question 1**, we have added a new analysis in Appendix G.2 that explicitly controls for the number of parameters when comparing the influence of FFN and QK. The gain of Muon over Adam, normalized by parameter count, indicates that the greater importance of FFN over QK does not stem from their larger parameter size **but from their inherent structure**.\n\nWe greatly value all of the reviewers’ comments and sincerely appreciate how they have helped us strengthen the paper. We hope that these revisions satisfactorily address the concerns raised.\n\nSincerely,  \nThe Authors"}}, "id": "RoeH7FGTPM", "forum": "twbMFL0DMp", "replyto": "twbMFL0DMp", "signatures": ["ICLR.cc/2026/Conference/Submission10833/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10833/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission10833/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763658184601, "cdate": 1763658184601, "tmdate": 1763658184601, "mdate": 1763658184601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of our main revisions"}, "comment": {"value": "Dear AC and Reviewers,\n\nWe sincerely thank you for the time and effort you have devoted to reviewing our work. We are very grateful for your insightful comments, which have helped us further improve the quality and clarity of the paper. In response to the reviewers’ constructive feedback, we have made the following main revisions:\n\n- **In response to Reviewer E1d8**'s suggestion about writing, all experiments for the gated FFN variant have been moved to a dedicated section in Appendix G.3.  \n\n- **In response to Reviewer E1d8 Weakness 1** concerning the distinction between frequency imbalance in \"facts\" and general imbalance in text. Appendix G.8 now evaluates Muon on datasets that contain the same bio-information under different degrees of heavy-tailedness, by explicitly varying association frequencies while keeping single-token frequencies distributed according to Zipf’s law. These results indicate that the heavy-tailedness of facts can be **approximately independent of the tokens’ Zipf marginals** and instead depends on associations, i.e., co-occurrences, of tokens.\n  \n- **In response to Reviewer W4iz Weakness 1** which raises the concern that the task relies heavily on the QK parameters. Appendix G.10 of the revised version now includes experiments where we train LLMs with Muon and Adam on an in-context linear regression task that primarily depends on learning the QK parameters well. Their similar performance on this task indicates that **they behave similarly when optimizing QK parameters**, consistent with our conclusion.\n\n- **In response to Reviewer Yhie Weakness 3** regarding the point that our heavy-tail tasks focus primarily on knowledge-acquisition tasks. We have added a new experiment on the WikiText-103 language modeling benchmark in Appendix G.9, whose token frequencies follow a heavy-tailed distribution. The benefit of Muon over Adam on the tail classes in this task further verifies that our conclusions hold beyond the knowledge-acquisition setting and **extend to general text**.\n\n- **In response to Reviewer sugx Weakness 4 and Question 1** concerning FFN over QK may be the result of parameter counting. We have added a new analysis in Appendix G.2 that explicitly controls for the number of parameters when comparing the influence of FFN and QK. The gain of Muon over Adam, normalized by parameter count, indicates that the greater importance of FFN over QK does not stem from their larger parameter size **but from their inherent structure**.\n\nWe greatly value all of the reviewers’ comments and sincerely appreciate how they have helped us strengthen the paper. We hope that these revisions satisfactorily address the concerns raised.\n\nSincerely,  \nThe Authors"}}, "id": "RoeH7FGTPM", "forum": "twbMFL0DMp", "replyto": "twbMFL0DMp", "signatures": ["ICLR.cc/2026/Conference/Submission10833/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10833/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission10833/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763658184601, "cdate": 1763658184601, "tmdate": 1763703064845, "mdate": 1763703064845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}