{"id": "rFLuaG9Yq6", "number": 24658, "cdate": 1758359068877, "mdate": 1759896756696, "content": {"title": "Use the Online Network If You Can: Towards Fast and Stable Reinforcement Learning", "abstract": "The use of target networks is a popular approach for estimating value functions in deep Reinforcement Learning (RL). While effective, the target network remains a compromise solution that preserves stability at the cost of slowly moving targets, thus delaying learning. Conversely, using the online network as a bootstrapped target is intuitively appealing, albeit well-known to lead to unstable learning. In this work, we aim to obtain the best out of both worlds by introducing a novel update rule that computes the target using the **MIN**imum estimate between the **T**arget and **O**nline network, giving rise to our method, **MINTO**. Through this simple, yet effective modification, we show that MINTO enables faster and stable value function learning, by mitigating the potential overestimation bias of using the online network for bootstrapping. Notably, MINTO can be seamlessly integrated into a wide range of value-based and actor-critic algorithms with a negligible cost. We evaluate MINTO extensively across diverse benchmarks, spanning online and offline RL, as well as discrete and continuous action spaces. Across all benchmarks, MINTO consistently improves performance, demonstrating its broad applicability and effectiveness.", "tldr": "MINTO is a simple, yet effective target bootstrapping method for off-policy RL that enables faster, more stable learning and consistently improves performance across algorithms and benchmarks.", "keywords": ["deep reinforcement learning", "q-learning", "actor-critic", "function approximation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1047499f819135e93daf7b03ea454eb2af4f3cf1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MINTO, a novel update rule for deep reinforcement learning that computes regression targets as the minimum estimate between the online and target networks. MINTO is designed to address the stability-efficiency trade-off inherent in standard RL, where target networks help stabilize learning but slow down convergence, while online networks foster efficiency but risk instability. Experiments on online, offline, discrete, and continuous RL benchmarks such as Atari games and robotic control tasks demonstrate consistent and significant improvements in sample efficiency and final performance over various baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated and the minimum-operator is easy to integrate into existing RL algorithms.\n\n2. The framework is validated on a wide set of benchmarks (Atari, MuJoCo, offline and online RL) and compared to several baselines, demonstrating consistent gains in both stability and sample efficiency.\n\n3. The modifications require minimal changes to underlying algorithms and introduce only a tiny computational cost, making practical adoption straightforward."}, "weaknesses": {"value": "1. The main evaluation is conducted against SimbaV1 and SimbaV2, which appear to be less strong baselines. This certainly raises the question whether this method can generalize to strong baselines such as Rainbow.\n\n2. The exclusive reliance on the minimum operator can be too conservative, possibly causing underestimation and inhibiting exploration in low-noise or optimistic environments.\n\n3. The code is not attached and will be released upon acceptance. This raises certain concerns."}, "questions": {"value": "1. Have you studied the impact of potential underestimation? While you claim that it is slight, is there any theoretical bound to it?\n\n2. The paper briefly mentions possible unexplored interactions between MINTO and exploration strategies. Can the authors clarify how MINTO affects exploratory behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KEMc0uvXQ2", "forum": "rFLuaG9Yq6", "replyto": "rFLuaG9Yq6", "signatures": ["ICLR.cc/2026/Conference/Submission24658/Reviewer_t6g5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24658/Reviewer_t6g5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760524820601, "cdate": 1760524820601, "tmdate": 1762943149876, "mdate": 1762943149876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method for using the target network in RL. \nSpecifically, the authors modify the target computation by taking the minimum of the target Q-network and the online Q-network. \nAlbeit simple, it shows substantial improvements across several RL settings, including online RL (DQN, SAC (Simba-V1, Simba-V2)), distributional RL (IQN), and offline RL (CQL)."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**S1. Simplicity of the method**\n* The proposed method is conceptually simple and easy to implement; it only modifies the target calculation by taking the minimum between the target and online Q-values. \n* This adjustment can be integrated easily into existing RL frameworks, as demonstrated in Algorithms 1–4. \nSuch simplicity makes the method practical and broadly applicable.\n\n**S2. Substantial improvement across diverse RL domains**\n* Despite its simplicity, the method demonstrates strong empirical performance. \n* It improves results across various RL domains --- online, offline, and distributional settings --- and across both continuous and discrete action spaces. \n* The reported gains over baseline methods in the DQN setting, suggest that the proposed approach effectively mitigates overestimation bias than other methods.\n\n**S3. Clear organization and presentation**\n* The paper is well structured and easy to read.\n* Experimental results are clearly presented."}, "weaknesses": {"value": "**W1. Comparison with other regularization methods limited in DQN setting**\n* Currently, the comparison with other bias-reduction or regularization methods is limited to the DQN (discrete online RL) setting. \nThis restricts the strength of the empirical claims, especially since the method is positioned as a general improvement applicable across multiple RL domains.\n* Including comparisons with well-established bias-reduction methods in other settings (e.g., SAC, IQN, and CQL) will clarify whether the observed improvements generalize beyond DQN."}, "questions": {"value": "**Q1. Missing comparison with Clipped Q-learning and related baselines**\n* To fully support the claim, the proposed method should be experimentally compared with other bias reduction methods (e.g., methods introduced in DQN experiment). \n* Among those, I think at least **comparison with Clipped Q-learning [1] is necessary**, which is equivalent to MaxMin DQN with N=2 --- the strongest baseline reported for DQN in the paper, and also widely used.\n\nI would like to emphasize that I believe this paper is strong and has substantial potential. However, this point is a critical concern for me. I currently lean toward a weak reject, but if the authors can provide the requested experimental comparisons (e.g., with Clipped Q-learning), I would be inclined to raise my recommendation, even significantly if the results meet the expectations."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YVtGXQg7N4", "forum": "rFLuaG9Yq6", "replyto": "rFLuaG9Yq6", "signatures": ["ICLR.cc/2026/Conference/Submission24658/Reviewer_Fczs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24658/Reviewer_Fczs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761465578363, "cdate": 1761465578363, "tmdate": 1762943149584, "mdate": 1762943149584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a technique (MINTO) to make better use of the online network when also using a target network for learning the value function. The technique simply changes the update target value to be the minimum between the target network and online network's value estimates.\nExperiments on a variety of benchmarks show that this technique is beneficial and can outperform common alternatives such as double Q-learning or using the minimum betweeen two target networks."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper was easy to follow and the proposed technique is very simple to understand and implement.\nThe experiments are comprehensive and show that method works in a variety of settings (Q-learning algorithms, actor-critic, distributional RL, continuous-control, atari).\nI appreciated testing the variants that would be most relevant such as mean or max of the online and target network estimates. \n\nIn summary, I think the proposed technique would make a nice addition to the deep RL toolbox."}, "weaknesses": {"value": "I did not identify any major weaknesses.\n\nIt could have been nice to include a more detailed analysis of why MINTO may be effective. In particular, thinking through the MINTO updates and comparing to using the target net as usual, the online network value replaces the target net value when the online network's value is lower. This means we can only use the most recent information (online network) if it is lowering the bootstrap target. \nI wonder if you could study the accuracy of the value estimates in a policy evaluation setting where we could accurately estimate true value functions. Perhaps MINTO produces better estimates overall."}, "questions": {"value": "Suggestions and comments(not directly impacting the score):\n- In the ablations, it is reported that taking the mean between the online and target network estimates was not helpful. Do you have any hypotheses why this woudl be the case? Using the mean between two target critics has been found to sometimes an effective alternative to the min between two critics [1] and sometimes underestimation bias has been found to be harmful.\n\n\n- The proposed method reminds me of the online network trust region method proposed in [2] since they both attempt to make use of the online network safely. It could be beneficial to include it as a baseline or provide some comparisons.\n\n- The algorithm labelled  \"Maxmin DQN\" in Fig.3. seems to be very similar to the clipped Double-Q learning update proposed in the TD3 paper [3] and I had not seen any mention of this.  \n\n\n[1] \"Scaling for compute and sample-efficient continous control\" Nauman et al.\n\n[2] \"Human-level Atari 200x faster\" Kapturowski et al.\n\n[3] \"Addressing function approximation error in actor-critic methods\" Fujimoto et al."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bEnUZS1heP", "forum": "rFLuaG9Yq6", "replyto": "rFLuaG9Yq6", "signatures": ["ICLR.cc/2026/Conference/Submission24658/Reviewer_vd9n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24658/Reviewer_vd9n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951096198, "cdate": 1761951096198, "tmdate": 1762943149332, "mdate": 1762943149332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "for the bootstrapping target in deep Q-learning, uses the minimum of the online net and target net value estimates, instead of using just the target net value estimate\n\nthey call their algorithm \"MINTO\""}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "applies MINTO on many base algorithms (DQN, IQN, CQL, SAC) and architectures (CNN, IMPALA, SimbaV1, SimbaV2), and fairly consistently shows some improvements in returns\n\nno additional hyperparameters\n\nsimple\n\na good number of fair baseline algorithms -- for ex, includes mean, max, random, and min of {online, target} nets in Fig 1, and later also includes Double DQN, FR-DQN, ScDQN, and Maxmin DQN\n\nnovel as far as I am aware\n\nwritten clearly"}, "weaknesses": {"value": "I think it would be great if the paper were even more clear about how the hyperparameters were set. while Appendix C.3 says \"All methods are run with their default hyperparameters\", it's not immediately clear to me how the values in Table 1 were set. are they the default hyperparameters from any existing codebase? if not, why not, and how were they set?\n\n5 seeds is not many seeds. (but the experiments cover so many base algorithms, architectures, codebases, and environment suites that I suspect this is not a big issue)\n\nno tests on toy tasks, along the lines of Baird's counterexample. I think there is a non-negligible chance that MINTO will get \"stuck\" on some such problems, where both TD and DQN sometimes empirically require their loss to increase before it will decrease. see for ex Fig 1e here: https://openreview.net/pdf?id=j3bKnEidtT. MINTO empirically appears to work on a wide variety of other benchmarks though, so this whole bullet might be irrelevant (even including the possible case that MINTO fails on some toy tasks).\n\na bit verbose, for ex the paper calls MINTO \"simple yet effective\" 4 times (once with a comma after simple)"}, "questions": {"value": "\"$Q$-learning offers a recursive update to approximate the state-action value function $Q$\"\n\nShould that last $Q$ be $Q^*$ instead of $Q$?\n\n&nbsp;\n\ndoes the \"preliminaries\" section allow for stochastic rewards? if not, is that intentional?\n\n&nbsp;\n\n\"The foundation of this success lies primarily in Deep RL, initiated by the introduction of the Deep Q-Network (DQN) (Mnih et al., 2013), which marked the first successful application of deep neural networks in RL.\"\n\nNeural Fitted Q Iteration was also a success, even though DQN could maybe be considered an even bigger success\n\n&nbsp;\n\n\"A problem that presents a challenge and obstacle\"\n\nwhy both?\n\n&nbsp;\n\n\"In deep RL, the problem of moving targets is especially evident due to the use of neural networks and the resulting uncontrolled fluctuations in the values of unseen states.\"\n\nwhat is this contrasting against? linear RL?\n\n&nbsp;\n\n\"For example, Gallici et al. (2025) demonstrated that cleverly using parallel environments eliminates the need for a target network\"\n\nsome people might consider \"eliminates the need for a target network\" as too strong of a claim\n\n&nbsp;\n\n\"This makes MellowMax orthogonal to our approach.\"\n\nyou might say \"somewhat orthogonal\" to help avoid confusion\n\n&nbsp;\n\n\"making this method orthogonal to our approach\"\n\nlikewise\n\n&nbsp;\n\n\"known as $Q$-function\"\n\ntypo, should be \"known as _a_ $Q$-function\" (or \"_the_ $Q$-function\")\n\n&nbsp;\n\n\"(Mnih et al., 2013) introduce a series of algorithmic components, more importantly, is the introduction of the target networks.\"\n\ntypos\n\n&nbsp;\n\n\"Despite the success, this results in a slow learning of value function as well as the policy due to relying on out-dated estimates\"\n\ntypo, should be \"learning of _the_ value function\"\n\n&nbsp;\n\n\"bellman\"\n\ntypo, should be \"Bellman\"\n\n&nbsp;\n\n\"can we find a practical bellman update rule that results in a stable and fast learning?\"\n\nwhat does \"practical\" specify here?\n\n&nbsp;\n\n\"When implemented in an efficient deep learning framework such as JAX (Bradbury et al., 2018), this overhead is negligible.\"\n\nI don't understand. does JAX automatically parallelize the additional forward pass with the target network's forward pass?\n\n&nbsp;\n\n\"addressing Q1.As\"\n\ntypo\n\n&nbsp;\n\nwhy is Fig 1 with 10 Atari games, but Fig 2 with 15 games?\n\n&nbsp;\n\nFig 2 shows CNN and IMPALA results on 100 vs 50 million frames. why not both 100 or both 50?\n\n&nbsp;\n\n\"into both value-based and actor–critic methods\"\n\nthis is maybe slightly confusing wording because actor-critic methods are partially value-based. maybe \"both actor-critic and purely value-based methods\" would be slightly clearer.\n\n&nbsp;\n\n\"Offline RL aims to learn an optimal policy from a large and static dataset\"\n\noffline RL does not need a \"large\" dataset to be offline RL\n\n&nbsp;\n\n\"A central challenge in this paradigm is the distributional shift problem: the learned policy may query the Q-function on state–action pairs absent from the dataset\"\n\noffline policy evaluation does not necessarily involve a learned policy. offline policy evaluation is also offline RL, and also may query the value function on state or state-action inputs not present in the dataset"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dOK4YjiJjY", "forum": "rFLuaG9Yq6", "replyto": "rFLuaG9Yq6", "signatures": ["ICLR.cc/2026/Conference/Submission24658/Reviewer_HyDL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24658/Reviewer_HyDL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24658/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762079073147, "cdate": 1762079073147, "tmdate": 1762943148943, "mdate": 1762943148943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}