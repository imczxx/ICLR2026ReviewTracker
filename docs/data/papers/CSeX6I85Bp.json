{"id": "CSeX6I85Bp", "number": 7405, "cdate": 1758020183963, "mdate": 1759897854702, "content": {"title": "Can Models Learn From Arbitrary Pairs?", "abstract": "Representation learning traditionally follows a simple principle: pull semantically similar samples together and push dissimilar ones apart. \nThis principle underlies most existing approaches, including supervised classification, self-supervised learning, and contrastive methods, and it has been central to their success. Yet it overlooks an important source of information: Even when classes appear unrelated, their samples often share latent visual attributes such as shapes, textures, or structural patterns. For example, cats, dogs and cattle have fur and four limbs etc. These overlooked commonalities raise a fundamental question: *can models learn from arbitrary pairs without explicit guidance?*\n\n\nWe show that the answer is yes. The primary challenge lies in learning from dissimilar samples while preserving the notion of semantic distance. We resolve this by proving that for any pair of classes, there exists a subspace where their shared features are discriminative to other classes. \nTo uncover these subspaces we propose **SimLAP**, a **Sim**ple framework to **L**earn from **A**rbitrary **P**air. SimLAP uses a lightweight feature filter to adaptively activate shared attributes for any given pair.\nThrough extensive experiments we show that models trained via SimLAP can indeed learn effectively from arbitrary pairs. \nRemarkably, models learned from arbitrary pairs are more transferable than those learned from traditional representation learning methods and exhibit greater resistance to representation collapse. \nOur findings suggest that arbitrary pairs, often dismissed as irrelevant, are in fact a rich, complementary and untapped source of supervision. By learning from them we move beyond rigid notions of similarity. Hopefully, SimLAP will open an additional pathway toward more general and robust representation learning.", "tldr": "SimLAP can learn from arbitrary pairs of classes robustly and promote distinct pairs close in subspaces while preserving class separability in global space", "keywords": ["representation learning", "contrastive learning", "supervised learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f05d4d240da270538bd24dfb97f100bb44c8a088.pdf", "supplementary_material": "/attachment/ecf449686d90207d1eb809907fe90247f381bbd3.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a way of training models which results in representations which allow for telling different classes apart, \nbut which also identifies the similarities between each distinct pair of classes.  \n\nThe paper achieves this by using the InfoNCE loss to only maximize the similarity of representations of different elements in a\nsubspace dependent on the classes the elements are from.  \n\nThe paper finds that it is possible to train models in this way and finds comparable performance between an instance of their \nsuggested model (SimLAP) and a ResNet50 and a supervised contrastive learning (SupCon) model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "**S1:** Focusing on which similarities can be found between all pairs of classes is an interesting idea.  \n\n**S2:** The paper does show that it is feasible to train a model in this way."}, "weaknesses": {"value": "**W1:** One of the main claims is not supported by the results in the article. \nIn line 103-104, the fourth main contribution is stated as \n\"the models exhibit better transfer learning performance than one-vs-all-approaches\". \nA similar claim is made in the discussion \"Our study demonstrates that learning from arbitrary pairs is not only feasible but also beneficial.\"(line 467).\nHowever, SimLAP does not in general show better performance than the two models used as baselines as seen in for example table 1. \n\n\n**W2:** Experiments are not sufficient to conclude anything. It seems only one seed was used for each model (see question **Q3**).\nIt varies a lot which model has the best performance in the various tasks and differences in performance are mostly too small to \nconclude anything."}, "questions": {"value": "Since the paper has unsupported claims (**W1**) and weak evaluation (**W2**), I recommend rejection. \n\nIf the paper had only claimed their method was a possible way to train new models instead of a better way, and if they had run \nseveral seeds such that one might get a better idea of the actual performance, then my recommendation would have been different.\n\n\n**Q1:** Figure 1 and Observation line 165-167: Since the motivation for the new training objective seems to be to make \nsubspaces wherein pairs of models are similar, it seems strange to emphasize that this already happens in a model trained with the \nusual supervised learning objective. Do you assume that it is possible to make the similarity in this subspace much higher? \nIn figure 5 it also seems that the supervised learning model has less overlap than SimLAP. How do you interpret this?  \n\n\n**Q2:** Feature filter 242-255: Do you enforce in any way that if two images are from the same class, then the gate vector will \nchoose a larger subspace? \n\n\n**Q3:** Section 4, Experiments: Did you only compare one seed of each model?  \n\n\n**Q4:** Line 350: It says: \"As shown in Table 4, SimLAP has higher mIoU scores\". However, this is not true. \n\n\n**Q5:** Figure 6: I don't understand this figure. What is the semantic distance? And which things are you measuring semantic distance between? \n\n\n**Q6:** Table 4: \"Semantic Segmentation with frozen features via FCN.\" What is FCN? \n\n\n**Additional Feedback:**\n\n**F1:** Line 129: Remember to introduce abbriviations before you use them. Does CL mean Continuous Learning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KcWukQfNgW", "forum": "CSeX6I85Bp", "replyto": "CSeX6I85Bp", "signatures": ["ICLR.cc/2026/Conference/Submission7405/Reviewer_SXrr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7405/Reviewer_SXrr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761306926135, "cdate": 1761306926135, "tmdate": 1762919527321, "mdate": 1762919527321, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SimLAP, a novel method that uses a contrastive loss in subspaces to learn representations from arbitrary pairs. It builds on the idea that even very distinct classes share some underlying features. Therefore, the representations should be similar when projected to a subspace selected specifically for that class pair. To achieve this, the authors introduced a feature filter that uses the class labels to gate the representation into a pair-specific subspace. A InfoNCE loss then enforces similarity between samples of these classes (positive pairs) while making representations of all other classes dissimilar in this subspace (negative pairs). Experiments on ImageNet and downstream tasks show that the learned representations transfer well, improve retrieval and segmentation in some cases, are complementary to other pretraining losses, and show greater robustness against dimensional collapse. The method is empirically promising, though formal guarantees are limited."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a well-motivated and intuitively appealing approach to contrastive representation learning. By using class pair-specific subspaces, SimLAP extends conventional contrastive setups in a meaningful way rather than replacing them. The idea of using a lightweight feature filter to gate representations into subspaces where dissimilar classes share latent attributes is creative and technically sound. The paper is clearly written, with accessible examples and sufficient methodological detail to support reproducibility. The experimental section is extensive, covering classification, retrieval, segmentation, and domain-shift benchmarks, and includes insightful analyses such as embedding visualizations and collapse-prevention studies. Overall, the work introduces an interesting direction that could stimulate further research in subspace-based and relational representation learning."}, "weaknesses": {"value": "- Some of the claims in the paper are overstated. For example, the assertion that all representation learning methods follow the principle of pulling semantically similar samples together and pushing dissimilar ones apart overlooks important exceptions such as (Masked) Autoencoders, which are widely used for representation learning without contrastive objectives. Similarly, the description of SimLAP as learning “without explicit guidance” is misleading, since it still relies on supervised class labels to construct pairs and generate subspaces. Certain figures and statements also appear inconsistent: for instance, Figure 5 claims that SimLAP achieves minimal overlap between inter- and intra-class similarity distributions, yet the plot suggests that standard supervised learning shows smaller overlap. Also, Table 7 Caption mentions SimLAP is trained for 100 epochs, but the table says 200 epochs.\n- While the feature filter is central to the contribution, its design choices are not well justified. It is unclear why a channel-wise gating mechanism is sufficient when learned representations often exhibit superposition across features. Likewise, averaging the two label embeddings to define a pair subspace seems ad hoc and might not optimally capture shared attributes between classes.\n- Experimental comparisons could be more consistent. The baselines vary across experiments, and including the same set of references—Supervised, SimCLR, and SupCon—throughout would make the results easier to interpret. Moreover, some key results, such as Table 1, lack confidence intervals or variance estimates. In that table, supervised learning outperforms SimLAP in five of eight benchmarks; discussing why SimLAP is better in the remaining three would help clarify when and why the proposed method is advantageous."}, "questions": {"value": "- How does the output of the feature filter actually look in practice? Is it sparse—selecting a subset of dimensions—or does it mainly act as a smooth scaling of all features? It would be helpful to include a distribution or histogram plot of the gate vector g. Relatedly, is the resulting subspace larger for semantically similar classes (e.g., kingsnake vs. garter snake) than for more distinct ones (e.g., kingsnake vs. Golden Retriever)? Figure 4 only reports similarity (cosine?) between gates but not their structure, and the axis scale (40–120) in that figure is also unclear.\n- Why is an approach without a filter better for similar class labels (Figure 6)? Would these mean that ranking by CLIP is a better approach than using your proposed learned class embedding?\n- The method defines similarity solely through class labels, but ignores low-level visual similarity (e.g., two images both containing a green background). Wouldn’t this cause the model to overlook shared visual attributes that are unrelated to class identity but still informative for representation learning? How does SimLAP handle or compensate for such appearance-based correlations that methods like SimCLR might capture? These features might be especially important for downstream tasks from different domains.\n- Why does Section 6 use MoCov3 for the joint loss (and not Sup, SupCon or SimCLR?)? Also why is for the Mid-Training switch a MAE model used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k4hNAVFhnn", "forum": "CSeX6I85Bp", "replyto": "CSeX6I85Bp", "signatures": ["ICLR.cc/2026/Conference/Submission7405/Reviewer_75Yh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7405/Reviewer_75Yh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726407578, "cdate": 1761726407578, "tmdate": 1762919526679, "mdate": 1762919526679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new training procedure for representation learning, positioned within the family of supervised contrastive learning methods. Unlike standard contrastive formulations that treat only same-class (or same-instance) pairs as positives and all others as negatives, the method aims to exploit similarities between instances coming from different pairs of classes. This is enabled by a gating module that is trained jointly with the embedding network and is used to restrict a standard InfoNCE computation to class-pair-specific subspaces in which contrastive differences can be maximized. The entire system is trained from scratch, without pretrained components.  Applicative results on commons setups (like classification and semantic segmentation, mostly under the transfer learning setup) demonstrate that the method is able to learn good representations. Several controlled experiments are provided in order to anaylze some of the differnet properties of the method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The idea of exploiting the very rich relations between different image classes, instead of simply pushing them away by treating them as negatives is a very good idea. One reason is that some classes clearly share significant similarities (especially if they are close under some natural class hierarchy). Another reason is that the vast majority of image pairs (in a batch) are of different classes (an order of n^2, compared to an order of n) and therefore it makes sense to exploit the information it contains, even if it is slightly more delicate to handle.\n2) The choice of doing so by alowing the model to identify *subspaces* in which particular class pairs can be seperated from others is a novel and interesting idea. It has the flexibility of treating all class pairs equally and avoids the need to design the full embedding space to incorporate all of such complex pairwise relations. \n3) I appreciate the decision to build a simple and clean implementation of the idea (using standard infoNCE, standard and simple architectures, no pretrained-modules, no language-based information, no advanced training schemes, etc') - making it easier to analyze and compare the essence of the main idea with respect to prior work. Clealy, this comes at the cost of not being able to compare directly to state-of-the-art results, per application.\n4) The method itself and the motivation are presented in a very accesible manner. The main ideas and observations are well highlighted and manage to convey the main properties of the method."}, "weaknesses": {"value": "1) Although it is built on contrastive learning principles, the method strictly relies on *supervised* training data. Therefore, I find it surprising that all of the experimentation is focused on transfer learning setups and not on demonstrating the qualities of the embedding for downstream tasks on the domain on which it was trained. That's where I would expect to see the most significant contribution. In addition, it would have beeen valuable to discuss whether the method could be adapted or extended to unsupervised or self-supervised settings, which are typically more common (and more effective) for transfer learning scenarios.\n2) The idea and usage of the gating function seems to be a good combination of simplicity and practicality. However, there is very limited justification of this particular choice, which effectively narrows the subspaces to be simple coordinate based projections. \n3) Empirical results are mostly on-par with the baselines. In the 'Dense prediction' section it is claimed to have higher mIOU compared to the baselines, but checking the numbers in Table 4 shows that it is not so. In any case, the results in general give the impression that the idea works reasonably well, but it doesn't provide very strong enough evidence as to when would it really be worthwile to adopt this approach. \n4) The theoretical analysis at the beginning of the Appendix is quite limited in scope and clarity, and it does not clearly convey how it supports or justifies the proposed approach. Why is it relevant to discuss random Gaussians? How does a proof that discusses the means have an implication on existence? Same goes for the simulation: The 'gap' defined in Eq. (5) does not have a clear connection to the result on the means and the plot in Fig. 9 is not very meaningful (in contrast to other empirical results which certainly do support many of the claims). \n5) The paper feels somewhat incomplete, especially in the Experimental and Analysis sections (4 and 5). These sections are far from being self-contained and I needed to extensively use the (very detailed) appendix, in order to understand even some of the very basics of the setup of each simulation and experiment. Some examples (out of many):\n- line 184: \"micro-class similarity\" - unclear and not defined anywhere\n- line 318: \"we use the KNN protocol\" - Which protocol? \n- line 334: Figure 4 is not sufficiently explained. What are synsets?\n- line 340: What are \"Medium\" and \"Hard\"?\n- line 357: \"MSimLAP exhibits competitive performance with joint loss of ICL and CCL..\" What does this mean? What are these?\n- line 475: \"The second point\""}, "questions": {"value": "1) Relating to W1: Can you justify the choice to focus the experiments entirely on transfer-learning setups? Why is the embedding itself analyzed only statistically and visually in the source domain, without showing performance of standard downstream tasks on imagenet itself (e.g. classification, in linear-probing or fine-tuning settings), or training on different data-sets with have other standard downstream tasks (such as segmentation and depth estimation).\n2) Relating to W2: While the choice of the gating is simple and elegant, have you tried other alternatives, such as a (class-pair dependent) linear projection to some fixed dimension?\n3) Relating to W3: What would be the main benefit of using the suggested training scheme? Is it mainly a way to achieve better stability in training, or in later fine-tuning? Or are the features more discriminative for downstream tasks? I guess that it depends on whether you compare to supervised or unsupervised approaches. In general, I would try to make such comparisons more separated, since in some cases it is not emphasized sufficiently (For example: In Table 7, SimLAP is claimed to avoid over adaptation, but such an improvement can be also due to access to labels (compared to MAE) and not only due to the pairwise-contrastive approach.\n4) The snake-lamp pair is given as a guiding example throught the paper, but in the limitations it is claimed that it might not be meaningful, lacking obvious commonalities. So do you predict that many (or most) class-pairs are not usefull, or even counter-productive to the learning scheme?\n5) I would suggest removing the ChatGPT correspondence at the end of the appendix. I don't think it is helpful to the reviewer/reader in any way."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "x8spnCHb6W", "forum": "CSeX6I85Bp", "replyto": "CSeX6I85Bp", "signatures": ["ICLR.cc/2026/Conference/Submission7405/Reviewer_tAo1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7405/Reviewer_tAo1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903503440, "cdate": 1761903503440, "tmdate": 1762919526241, "mdate": 1762919526241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an alternative to standard contrastive learning. The method aligns images of two classes $c_1$ and $c_2$ in a subspace determined by the labels corresponding to $c_1$ and $c_2$, while pushing away other images. The method is compared to Supcon, and at times SimCLR."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method is novel and interesting. The idea of using the labels to determine a relevant subspace is nice, and might especially be useful in conjunction with modern text-image models (though arguably, those are often trained constructively in the first place)."}, "weaknesses": {"value": "It is interesting that the feature filter is initialized as a random network and trained end to end. Do the authors have any intuition/insight regarding what aspects about the data might end up breaking the symmetry between different classes, drawing some classes closer together than others? There doesnt seem to be anywhere for any inductive bias to enter this picture, since the image and text stacks are not pretrained at all. The lack of intuition (not at all necessarily a rigorous justification) is a weakness."}, "questions": {"value": "Why is Section 3.1 surprising? For any pair of vectors a, b and any set of other vectors S, it is natural that there is a vector c such that c^Ta and c^Tb are closer to each other than they are to c^s for s\\in S. For instance, take c = a+b. A random other vector will almost be orthogonal to it in high dimensions.\n\nWhy is the ability of SimLAP to bring Chihuahua and Garter snack close to each other desirable?\n\nIf the label names are removed of any semantic meaning at all (so replaced by something like “Class 1”, “Class 15”, etc instead of “Chihuahua” and “Garter Snake”), would the filtering idea still work?\n\nThe motivation mentions finding general subspaces for each pair of classes, effectively suggesting a matrix $A$ such that $Ax$ is compared to $Ax^{+}$ and $Ax^{-}$ in SimCLR, while the implementation restricts this subspace as coming from an element-wise product (in other words restricting $A$ to be diagonal). Is this motivated by sparsity/computational constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PVaaGmRFd9", "forum": "CSeX6I85Bp", "replyto": "CSeX6I85Bp", "signatures": ["ICLR.cc/2026/Conference/Submission7405/Reviewer_9VN7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7405/Reviewer_9VN7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065023746, "cdate": 1762065023746, "tmdate": 1762919525908, "mdate": 1762919525908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}