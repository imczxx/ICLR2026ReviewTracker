{"id": "AnrDJaY39L", "number": 13218, "cdate": 1758215201264, "mdate": 1763732547633, "content": {"title": "Self-Improving Model Steering", "abstract": "Model steering represents a powerful technique that dynamically aligns large language models (LLMs) with human values and intentions during inference. However, conventional model-steering methods rely heavily on externally annotated data, not only limiting their adaptability to varying contexts but also tethering their effectiveness to annotation quality. In this paper, we present SIMS, the first self-improving model-steering framework that operates without relying on external supervision. At its core, SIMS autonomously generates and refines contrastive samples through iterative self-improvement cycles, enabling adaptive, context-specific steering. Additionally, SIMS employs novel strategies, including prompt ranking and contrast sampling, to further enhance steering efficacy. Extensive evaluation across diverse LLMs and benchmarks demonstrates that SIMS substantially outperforms existing methods in steering effectiveness and adaptability, highlighting self-improving model steering as a promising direction for future research on inference-time LLM alignment. The code for replicating SIMS is available at https://anonymous.4open.science/r/SIMS.", "tldr": "We present SIMS, the first self-improving model-steering framework that operates without relying on external supervision.", "keywords": ["Self-improvement", "Model steering"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f3300dc46f8cf38bb43e423908aba51eea28459c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SIMS (Self-Improving Model Steering), the first model-steering framework that operates without external supervision. Unlike conventional steering methods that rely on human-annotated contrastive QA pairs to learn activation-level interventions, SIMS autonomously generates and refines its own contrastive samples through iterative self-improvement cycles. At each iteration, the current steered policy generates multiple candidate responses per prompt; a preference oracle (e.g., PairRM or the model itself) ranks these responses to construct positive/negative sample sets. These are used to update layer-wise steering functions (e.g., via HPR), which are then composed with the base LLM to form an improved policy for the next round. The authors further introduce two enhancements: Prompt Ranking (SIMS-PR), which uses the LLM’s own judgment to rank responses without any external oracle, and Contrast Sampling (SIMS-CS), which maintains a memory bank of past samples and selects only the most informative (high-margin) examples for steering updates. Experiments on Llama3-8B and Mistral-7B show that SIMS significantly outperforms supervised steering baselines on Alpaca-Eval (LC/WR) and Arena-Hard after just a few iterations, with SIMS-CS achieving the strongest results."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel paradigm: SIMS is the first work to achieve model steering without external human annotations, shifting the field toward truly adaptive, inference-time alignment.\n- Elegant self-improvement loop: The iterative cycle of generation → self-evaluation → contrastive activation learning → policy refinement is conceptually clean and empirically effective.\n- Strong empirical results: Consistent gains across multiple metrics (LC, WR, Arena-Hard) and models (Llama3, Mistral), with performance improving monotonically over iterations.\n- Practical enhancements: SIMS-PR enables fully oracle-free steering, while SIMS-CS dramatically improves sample efficiency—especially valuable under limited compute or data budgets.\n- Broad validation: Includes ablation studies on iteration count, response diversity, dataset size, context length, model scale, and generalization to 8 diverse NLP tasks."}, "weaknesses": {"value": "- Self-ranking (SIMS-PR) lacks reliability analysis: The paper provides no evidence that the model’s self-judgments correlate with human preferences. Poor self-ranking could lead to error propagation or preference collapse, especially in early iterations.\n- Evaluation heavily dependent on GPT-4: Both Alpaca-Eval and Arena-Hard use GPT-4o as the judge. Since PairRM may implicitly align with GPT-style outputs, SIMS could be overfitting to the evaluator rather than learning general alignment.\n- Missing key baselines: No comparison against strong alternatives like Best-of-N sampling, rejection sampling, or DPO fine-tuning using the same self-generated data. It’s unclear whether steering is necessary at all.\n- No analysis of safety or harmful outputs: Self-generated negative samples may contain toxic, biased, or hallucinated content. If mislabeled by the oracle, such samples could corrupt the steering direction.\n- Limited out-of-distribution evaluation: All prompts come from UltraFeedback. Generalization to novel domains, adversarial prompts, or real-world user queries remains unverified."}, "questions": {"value": "1. Contrast sampling robustness: The margin reward in SIMS-CS assumes the oracle reliably identifies the “worst” response. But reward models are known to be unreliable on low-quality generations. How sensitive is SIMS-CS to oracle noise on negative samples?\n\n2. Steering function generalization: Do the learned steering functions generalize to unseen prompts outside the UltraFeedback distribution? Or are they overfitting to the training prompt set?\n\n3. Computational cost: What is the actual GPU-hour or FLOP cost per iteration? Generating K=10 responses per prompt and collecting full-layer activations is expensive—how does this compare to a single DPO epoch?\n\n4. Failure modes: Can you provide examples where SIMS degrades performance (e.g., on TruthfulQA or ToxiGen)? Under what conditions does self-improvement fail or diverge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ryD0WJiKd7", "forum": "AnrDJaY39L", "replyto": "AnrDJaY39L", "signatures": ["ICLR.cc/2026/Conference/Submission13218/Reviewer_QRfN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13218/Reviewer_QRfN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760581923075, "cdate": 1760581923075, "tmdate": 1762923907954, "mdate": 1762923907954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper *“Self-Improving Model Steering”* addresses limitations in standard model-steering frameworks, particularly those arising from data inefficiency. The authors propose an iterative algorithm that allows the model to **self-improve its steering behavior** by using LLMs to guide responses and by modifying parts of the underlying neural architecture during the process. The resulting method aims to dynamically refine steering directions over time rather than relying on fixed data-driven mappings. Empirical results indicate that the proposed approach generally outperforms existing model-steering baselines across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Extensive experiments:** The evaluation is broad, covering multiple datasets and tasks, with comparisons against standard model-steering approaches.  \n- **Empirical gains:** The proposed self-improving algorithm generally improves upon common model-steering baselines, demonstrating the potential of iterative self-adjustment in steering models."}, "weaknesses": {"value": "- **Inference-time feasibility:** The iterative algorithm may not be practically permissible or efficient at inference time, raising concerns about its real-world applicability. The paper does not clearly specify the additional computational overhead or time cost associated with the self-improvement loop.  \n- **Marginal improvements on some tasks:** In Figure 9, the gains on the NLP benchmark tasks appear minor, and in some cases, performance seems comparable to or slightly worse than standard model steering."}, "questions": {"value": "1. What is the compute budget of the proposed self-improving model steering compared to regular model steering, and is such overhead acceptable during inference?  \n2. Figure 9 shows results that appear comparable between self-improving and standard steering. Can you clarify whether these differences are statistically significant?  \n3. How does the data–compute trade-off compare between standard and self-improving steering? Also, does the increased compute justify the performance gains observed?  \n4. Are there safeguards to prevent overfitting or instability in the iterative self-improvement process during steering updates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rWQQIueuKp", "forum": "AnrDJaY39L", "replyto": "AnrDJaY39L", "signatures": ["ICLR.cc/2026/Conference/Submission13218/Reviewer_4U4i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13218/Reviewer_4U4i"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854200572, "cdate": 1761854200572, "tmdate": 1762923907510, "mdate": 1762923907510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the model steering problem. Due to the lack of adaptability to varying contexts, this paper propose a self-improving model steering framework that does not rely on external supervision. Instead, it autonomously generates and refines contrastive samples, enabling adaptive and context-specific steering. The experimental results demonstrate the effectiveness of the proposed methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper is well-written.\n\nThis paper proposes a new adaptive model steering framework, leveraging iterative self-improvement cycles that do not rely on external supervision. The experimental results are compelling."}, "weaknesses": {"value": "Regarding the self-improving model steering, the main operation involves considering K candidate responses and judging them through preferred and disfavored outputs. Then, contrastive training samples are generated. However, this approach lacks sufficient novelty and resembles a technical report. A comparison with previous methods should be included to highlight the differences between your method and the existing approaches.\n\nThe method utilizes informative question-response pairs to improve sample quality. However, the replay-based method may be limited by the memory bank (B), including both its size and quality, potentially restricting the adaptability of the proposed framework.\n\nEquation (7) should define y more clearly.\n\nThe dataset used is insufficient, as this paper focuses only on the UltraFeedback corpus 2023."}, "questions": {"value": "see the above the weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6mlsWsjsXA", "forum": "AnrDJaY39L", "replyto": "AnrDJaY39L", "signatures": ["ICLR.cc/2026/Conference/Submission13218/Reviewer_sPAZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13218/Reviewer_sPAZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972038842, "cdate": 1761972038842, "tmdate": 1762923907272, "mdate": 1762923907272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors claim that SIMS (Self-Improving Model Steering) is the first framework that enables large language models to align themselves at inference time without any external supervision. They argue that, unlike prior steering methods that depend on labeled datasets or human-annotated preference signals, SIMS allows a model to generate, evaluate, and refine its own contrastive examples through iterative self-improvement cycles. By continuously updating steering functions based on internal activation rather than retraining model weight, the method achieves adaptive, context-specific alignment that improves over iterations. The authors assert that SIMS and its variants (Prompt Ranking and Contrast Sampling) significantly outperform existing supervised steering approaches in steering effectiveness, efficiency, and generalization across multiple models and benchmarks, establishing self-improving model steering as a promising new direction for inference-time alignment research."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a new perspective on inference-time alignment by combining ideas from activation steering and self-improvement loops.\n2. The results show consistent improvement, across the iterations"}, "weaknesses": {"value": "1. The paper frames SIMS as an inference-time alignment method, yet Eq. (5) optimizes the policy $\\pi$, implying a parameter update process rather than a pure activation-level intervention.\nThe method for solving the arg min / arg max over π is never described, leaving the optimization step undefined.\n2. Eq. (5) includes a normalization constant  $Z_{\\pi_t}$ with no analytical form or computational approximation.\nThe absence of this detail makes the main optimization equation incomplete and unreproducible.\n3. The steering transforms f and f′ are central to SIMS but their formulation remains unspecified.\nThe paper does not explain how these functions are initialized, updated, or constrained across iterations.\n4. The authors claim SIMS requires no external supervision, yet Algorithm 1 (Line 226) queries an oracle for pairwise preferences each iteration. This oracle effectively provides external feedback, contradicting the stated independence from supervision.\n5. Authors claim that the proposed approach substantially outperform the existing method. However. There is no comparision against the other methods in the Figure 3 and 4"}, "questions": {"value": "1. How is the optimization in Eq. (5) actually performed in practice?  \n   Since the equation involves an arg min / arg max over $\\pi$, what parameters of the policy are being updated, and how does this remain inference-time only?\n\n2. What is the analytical form or computational treatment of the normalization constant $Z_{\\pi_t}$ in Eq. (5)?  \n   Is it estimated, ignored, or approximated during optimization?\n\n3. What is the specific formulation of the steering functions $f$ and $f′$ used in SIMS?  \n   Are they linear projections, learned transformations, or fixed parameter mappings?\n\n4. If Algorithm 1 queries an oracle for pairwise preferences each iteration, how does the method remain “free from external supervision”? \n5. What is MS in the results Figure 3 and 4"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6qxbR6JHaF", "forum": "AnrDJaY39L", "replyto": "AnrDJaY39L", "signatures": ["ICLR.cc/2026/Conference/Submission13218/Reviewer_9kQ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13218/Reviewer_9kQ5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976171950, "cdate": 1761976171950, "tmdate": 1762923906823, "mdate": 1762923906823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}