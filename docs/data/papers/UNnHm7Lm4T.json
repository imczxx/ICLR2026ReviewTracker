{"id": "UNnHm7Lm4T", "number": 15457, "cdate": 1758251570089, "mdate": 1759897305835, "content": {"title": "Trust the model when it is confounded: Model-based Reinforcement learning for Confounded POMDPs", "abstract": "We consider model‑based reinforcement learning (MBRL) in confounded partially observable Markov decision processes (POMDPs), where unobserved confounders in the environment will introduce bias into the learned dynamics model. Existing studies either rely on a kernel function to model the environment, or tabular settings where the observation spaces are discrete. To address these limitations, we propose a deep proximal causal MBRL (DPC-MBRL) method. Specifically, we first establish a consistent identification result for the policy value in confounded POMDPs through proximal causal inference. Based on this identification result, we then employ neural networks to model the environment dynamics, which enables a more flexible function approximation than existing studies. Through experiments on an advanced physics simulation benchmark MuJoCo and a real-world medical dataset, we demonstrate that DPC-MBRL mitigates the bias induced by unobserved confounders and yields more accurate dynamics model estimates than standard MBRL approaches.", "tldr": "", "keywords": ["Reinforcement Learning; Causal Inference; POMDP; Unobserved Confounder;"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2038082386ebc46b09efaf2de97f886c866aca5a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of learning dynamics models for model based RL in environments with unobserved confounders. The authors propose a method which reformulates the dynamics using proximal causal inference. The authors use neural networks to model bridge functions a Maximum Moment Restriction framework to train them. The authors validate their method on two benchmarks: a mujoco environment with synthetically added confounders and the real-world SUPPORT dataset. The results show that the method leads to more accurate dynamics models and superior policy value estimation compared to standard model based and model-free methods that do not account for confounding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper bridges the gap between the proximal causal inference literature and model-based RL. Using the bridge function $h_D$ as a de-biased dynamics model is an original idea.\n2. The method in the paper scales to continuous and high-dimensional spaces, overcoming the limitations of previous tabular and kernel-based methods.\n3. The authors do an excellent job in clearly formalizing the problem of confounding bias inthe dynamics, contrasting the observational expectation with the required interventional expectation.\n4. The experiments demonstrate a benefit of the proposed approach. They also show that if the proximal assumptions are met or assumed to be met, the method effectively reduces bias and improves performance."}, "weaknesses": {"value": "1. Related to the practical implications of this work, the work on \"Delphic Offline Reinforcement Learning\" (Pace et al. 2023) addresses a very similar problem (i.e., offline RL with hidden confounders) in a similar application domain (medical EHRs). I find it critical for this work to position itself relative to that. I would consider increasing my score if the authors add empirical comparisons.\n2. The entire method's validity rests on unverifiable assumptions of proximal causal inference: the existence of \"sufficiently rich\" proxies ($W_t, Z_t$) that precisely match the causal DAG in Figure 2. The paper itself notes this is \"non-trivial\" and a limitation. This emphasizes again the importance I find to comparing to the practical method in weakness 1, as it would demonstrate the significance of these assumptions. \\\n\\\nThis reliance on strong assumptions is is also evident by the experimental design. The mujoco experiment is a \"best-case\" tautology, as the proxies are synthetically generated to perfectly fit the model's requirements, which does not test robustness. The SUPPORT experiments, which is the most compelling, omit the most critical detail: the selection and justification of the proxies. The authors should not defer this to an external citation (i.e., Shen & Cui, 2023).\n\n3. The presentation of Section 3.2 (\"Estimation of Bridge Functions\") implies a novel methodological development. In fact, it is a direct and faithful application of the NMMR method (Kompa et al., 2022)  to the dynamics/reward functions. The novelty is in the application, not the method."}, "questions": {"value": "1. In the SUPPORT experiment, which of the covariates were selected as $Z_t$ and $W_t$? What is the domain-knowledge justification that these variables satisfy the causal assumptions? (e.g., $Z_t$ only affects $A_t$ via $U_t$; $W_t$ is not affected by $A_t$; etc.). Why should we believe these proxies are \"sufficiently rich\" to satisfy the completeness assumption? The real-world results are unconvincing without this justification.\n2. How does your method perform when the proximal assumptions are violated? For example, in the mujoco setting, what happens to the model's MSE if you introduce a weak causal link from $Z_t$ directly to $S_{t+1}$ or from $A_t$ directly to $W_t$?\n3. How was the \"ground truth\" policy value determined in Figure 3 for the SUPPORT dataset? This is a real-world offline dataset, so there is no access to a true environment or ground truth. Please clarify what this (log-mse) metric is being computed against."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wVgZTGnlvv", "forum": "UNnHm7Lm4T", "replyto": "UNnHm7Lm4T", "signatures": ["ICLR.cc/2026/Conference/Submission15457/Reviewer_ago2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15457/Reviewer_ago2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760717087857, "cdate": 1760717087857, "tmdate": 1762925744251, "mdate": 1762925744251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work focuses on learning a dynamics model (and subsequently a policy) in POMDPs with unobservable confounders. They assume two sets of proxy variables, $Z_t, W_t$, along with corresponding bridge functions that connect the effects of the unobserved confounders $U_t$ to the observed variables (e.g., states) and the proxies. The bridge functions are learned using deep neural networks, and the empirical advantages of the proposed method are demonstrated on MuJoCo simulators and a clinical dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clarity & Quality: The challenges of model learning in confounded POMDPs, arising from unobserved confounders that cause bias in the estimates, are clearly explained in Section 2.2. The assumptions required to enable the estimation of bridge functions are also well described. Algorithm 1 clearly illustrates how the bridge functions can be estimated from rollouts in the real environment and how a policy can be learned by rolling out imagined trajectories under the learned bridge functions. \n- Significance: Overall, this work tackles a realistic and relevant problem in RL, particularly for high-dimensional states, since most data collection and learning occur in POMDPs rather than MDPs, and it is infeasible to assume that the states fully encode all information about the environment. \n- Originality: This work adopts the bridge functions used in model-free POMDPs (e.g., Tchetgen Tchetgen et al., 2024) for model-based RL. While a two-stage estimation procedure is standard in MBRL, this approach improves on existing MBRL methods for POMDPs by removing the assumption by Grasse et al., 20223 that the underlying states are tabular and the dependence on kernel functions by Hong et al., 2024."}, "weaknesses": {"value": "- MuJoCo domains, where the unobserved confounder is nonlinearly transformed to create $Z_t, W_t$ (described in H.2) are too simple to make convincing claims about the empirical advantages of this algorithm.\n- Algorithm 1 describes a two-step process for policy learning. However, the experiments either report the error of the learned dynamics model or the OPE estimation errors, and do not present the overall performance of the learned policy resulting from completing the full procedure in Algorithm 1. While theoretically one might expect that reducing dynamics or OPE estimation errors could lead to improved policy learning, it would strengthen the work to compare the obtained policy’s value against that of the baselines or the oracle (if the oracle optimal policy is known).\n- Another key limitation of this work, as the authors also acknowledge, is its assumption about the existence of valid proxy variables that satisfy Assumption 3.2. In realistic scenarios, obtaining proxy variables $W_t, Z_t$ may not be feasible. This also speaks to my concern raised earlier that the MuJoCo environments and the data-generating process (i.e., transforming $U_t$ to $Z_t$ and $W_t$ via a tanh function and adding a very small Gaussian noise) are too simplistic to effectively model real-world systems."}, "questions": {"value": "For both experiments (Table 1 and the box plot in Figure 3), it would be beneficial to present the overall performance of the learned policy obtained after completing the full procedure described in Algorithm 1. This would also allow the authors to compare their results with model-free baselines in POMDPs with unobservable confounders (e.g., Tchetgen Tchetgen et al., 2024; Miao et al., 2024), as the current Table 1 only compares against the confounded model-based RL setting (\"Confounded MBPO\") and lacks comparisons with other baselines that would help contextualize the strength of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rKxjohbg0G", "forum": "UNnHm7Lm4T", "replyto": "UNnHm7Lm4T", "signatures": ["ICLR.cc/2026/Conference/Submission15457/Reviewer_dwrX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15457/Reviewer_dwrX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761588484493, "cdate": 1761588484493, "tmdate": 1762925743344, "mdate": 1762925743344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies model-based RL for confounded POMDPs by leveraging proximal causal inference with dynamic- and reward-emission bridge functions. Building on an identification result for the policy value and conditional moment restrictions for the bridge functions, the authors propose learning the bridges with neural networks by minimizing a maximum moment restriction (MMR) objective. Numerical experiments in simulated control tasks and on a real-world clinical dataset demonstrate the method’s effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Motivation is clear: The problem of confounded POMDPs is challenging, and there is limited practical work on model-based algorithms for this setting. This paper presents a practical algorithm with general function approximation using deep neural networks.\n\n2. Empirical evidence is supportive: The proposed method outperforms baselines in confounded POMDP settings, and its application to a real-world clinical dataset is promising. \n\n3. Presentation is well organized."}, "weaknesses": {"value": "In general, the contributions feel limited, and several major concerns remain.\n\n1. Originality of the theoretical result: The theoretical guarantee (presumably Theorem 3.1) is highlighted as a primary contribution. However, a similar result appears in Theorem 3.5 of [1]. The paper does not clearly articulate the novelty—either in the statement or in the proof techniques—relative to [1]. I suggest to add more discussions on the technical contributions.  \n\n2. Alignment between identification and estimation: The bridge functions used for identification (Theorem 3.1) and those used for estimation (Assumption 3.2; Section 3.2) appear misaligned. In particular, the identification bridges involve the immediate reward and the next state, whereas the estimation bridges do not. Based on lines 928–931 in the appendix, it seems you may need conditional moment restrictions that hold pointwise in the immediate reward and next state rather than only in expectation. I suggest the authors to carefully revisit this point.\n\n3. How estimated bridges are used in policy evaluation: Given the above point#2, it is unclear how the estimated bridge functions are actually used to perform policy evaluation under Theorem 3.1. Adding step-by-step details (or pseudocode) in Algorithm 1 would help make the pipeline explicit.\n\n\n\n\n[1] Model-based Reinforcement Learning for Confounded POMDPs. Hong et. al."}, "questions": {"value": "1. My understanding is that the main results are stated for the finite-horizon setting, while Section 2.1 also introduces a discounted infinite-horizon formulation. Could you clarify it?\n\n2. Methodological trade-offs: Could the authors provide more insight into the trade-offs between model-based approaches and existing model-free methods for confounded POMDPs [1][2]?\n\n[1] Off-policy evaluation for episodic partially observable markov decision processes under non-parametric models. Miao et. al.\n\n[2] Proximal reinforcement learning: Efficient off-policy evaluation in partially observed markov decision processes. Bennett et. al."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Pqa6EHWLev", "forum": "UNnHm7Lm4T", "replyto": "UNnHm7Lm4T", "signatures": ["ICLR.cc/2026/Conference/Submission15457/Reviewer_Uo35"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15457/Reviewer_Uo35"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948963905, "cdate": 1761948963905, "tmdate": 1762925742862, "mdate": 1762925742862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates model-based reinforcement learning (MBRL) in environments where unobserved confounders affect both actions and outcomes, a setting termed confounded partially observable Markov decision processes (confounded POMDPs). Existing methods either rely on tabular formulations or kernel-based nonparametric models, both of which are impractical for continuous and high-dimensional state spaces. To address these limitations, the authors propose a Deep Proximal Causal MBRL (DPC-MBRL) method that integrates proximal causal inference with neural network–based dynamics modeling. The paper first provides a theoretical identification result showing how the policy value can be consistently estimated under confounding using bridge functions with proxy variables. Then, it proposes a neural-network-based estimation of these bridge functions, offering a flexible and scalable approach. Empirically, DPC-MBRL is evaluated on MuJoCo control tasks and a real-world medical dataset. The experiments show that DPC-MBRL mitigates confounding bias more effectively and yields better model estimation accuracy than existing MBRL and model-free deconfounding methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The theoretical development is relatively rigorous and grounded in established causal inference literature (e.g., Miao et al., 2018; Tchetgen Tchetgen et al., 2024). The proofs (Appendix E) clearly demonstrate the unbiasedness of the identified policy value. \n2. This work contributes toward causally robust model-based RL—an underexplored area compared to model-free causal RL. Given the growing relevance of confounded observational data (robotics, healthcare, offline RL), this framework could have broad implications for safe and reliable deployment of RL systems."}, "weaknesses": {"value": "1. The approach assumes access to valid action-inducing and outcome-inducing proxies satisfying the completeness condition. While the paper provides conceptual examples and guidance (Appendix D), in real environments identifying or constructing such proxies remains challenging.\n2. The overall theoretical novalty of the proposed method seems limited, given existing works on proximal causal inference assisted RL and corresponding theoretical results, including model-based methods, e.g., Hong et al, 2024.\n3. From the perspective of RL, the presentation of the paper seems confusing. The initial development of the theoretical setup seems to be describing the observational data (offline data) generating process which involves confounding. But the algorithm framework (Algorithm 1) seems to be describing an online learning algorithm where the learned policy can be directly deployed and thus there should be no confounding issue. How should I understand that?"}, "questions": {"value": "1. Can the authors propose or test methods to *learn* proxy variables directly from raw observations, perhaps via auxiliary networks or latent representation learning?\n2. How sensitive is the approach if the selected proxies violate the completeness assumption? Would errors propagate to the learned policy?\n3. What is the empirical runtime overhead of DPC-MBRL compared to vanilla MBPO or model-free methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LjAUPC9oRo", "forum": "UNnHm7Lm4T", "replyto": "UNnHm7Lm4T", "signatures": ["ICLR.cc/2026/Conference/Submission15457/Reviewer_qSzC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15457/Reviewer_qSzC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15457/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762891920840, "cdate": 1762891920840, "tmdate": 1762925741782, "mdate": 1762925741782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}