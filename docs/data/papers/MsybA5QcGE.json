{"id": "MsybA5QcGE", "number": 5173, "cdate": 1757860500864, "mdate": 1759897990574, "content": {"title": "Temporal Motif Message Passing Network for Dynamic Graph Representation Learning", "abstract": "Although exsisting continuous-time dynamic graph representation learning methods have achieved great success by constructing extra features based on temporal motifs, they fail to fully leverage the spatial and temporal structures of temporal motifs to model the high-order correlations among nodes. An intuitive and innovative approach is to incorporate temporal motifs into message passing networks, which encounters the following challenges: (1) diverse semantics exhibit in a temporal motif; (2) complex high-order correlations exist among the constituent edges of temporal motifs. To this end, we propose a Temporal Motif Message Passing network (TMMP) for dynamic graph representation learning. Specifically, a temporal motif extension module is proposed to obtain extended temporal motifs based on different time window lengths. Then, an attentive temporal motif encoder is proposed to capture the diverse semantics of temporal motifs. In addition, a dual hypergraph temporal motif message passing mechanism is proposed to comprehensively model the complex relationships among the constituent edges. Extensive experiments on four real-world datasets demonstrate that TMMP achieves the state-of-the-art performance, surpassing the best baseline methods in both AP and AUC metrics on almost all datasets.", "tldr": "", "keywords": ["Dynamic Graph Representation Learning; Graph Neural Networks; Temporal Motifs"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/351d37962649506e26709edebbbcdcd344cb7332.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "To effectively capture high-order and complex relationships among nodes, this paper proposes a motif-based message passing extension for transformer-based dynamic graph learning methods. For each target node u and its historical edges, the method first constructs affinity matrices that record the frequency of each edge appearing within a set of predefined motifs over multiple time intervals. These matrices are then leveraged to update the representations of u’s historical edges using a hypergraph temporal message passing mechanism. Finally, transformer models are applied to the updated edge representations to generate the final embedding for node u."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The observation about the diverse semantics of temporal motifs make senses, and the ability to effectively handle such diversity could be crucial for applications such as fraud detection.\n- Tackling the diverse semantics of temporal motifs by varying their time intervals is a natural and valid solution.\n- The proposed method achieves better performance than the compared baselines."}, "weaknesses": {"value": "- The complexity of the proposed method is high, which might limit its application to small temporal graphs. Besides, such high complexity (in both concepts and computation) is also doubtful. This method builds upon already complicated sequence-based Transformers by adding motif affinity matrix construction, additional attention mechanisms, and a hypergraph neural network. However, as shown in Table 2, the performance improvement is marginal, while the computational cost—especially from motif matching—is extremely high. The authors should reconsider whether such design complexity is truly necessary.\n- The choice of datasets is limited and somewhat inappropriate. Most of the selected datasets exhibit weak higher-order patterns, making them unsuitable for evaluating a method designed to capture complex high-order dependencies. For example, the Wikipedia dataset is dominated by repetitive behaviors rather than intricate structural motifs. It would be more convincing to include datasets with richer higher-order patterns, such as MOOC, LastFM and some financial fraud detection datasets."}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A5eDBd1b1b", "forum": "MsybA5QcGE", "replyto": "MsybA5QcGE", "signatures": ["ICLR.cc/2026/Conference/Submission5173/Reviewer_LFzg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5173/Reviewer_LFzg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761386634125, "cdate": 1761386634125, "tmdate": 1762917929914, "mdate": 1762917929914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework that incorporates temporal motifs into message passing networks for dynamic graph representation learning. The approach includes: (1) a temporal motif extension module to capture motifs at different time window lengths, (2) an attentive temporal motif encoder to capture diverse semantics, and (3) a dual hypergraph message passing mechanism (explicit and implicit hypergraphs) to model high-order correlations among constituent edges. Experiments on four datasets demonstrate state-of-the-art performance in link prediction tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents the first work to incorporate temporal motifs into message passing networks rather than just using them as auxiliary features, which is a meaningful contribution to modeling high-order correlations.\n- The combination of explicit hypergraph construction and implicit hypergraph construction is innovative and comprehensive for capturing both obvious and hidden relationships among edges.\n- The paper includes comprehensive ablation studies, hyperparameter sensitivity analysis, and computational cost comparisons, providing good insights into model behavior."}, "weaknesses": {"value": "- Evaluation limited to single task type: this paper only evaluates on link prediction, although it contains transductive and inductive variants.  However, many of the baselines compared in the experiments have been evaluated on different tasks. For example, JODIE uses the user state change prediction, and TGAT chooses the dynamic node classification task.\n- The experimental evaluation uses relatively small-scale datasets where UCI has only 1,899 nodes and 59,835 edges, Enron has only 184 nodes and 125,235 edges, and even the larger datasets like Wikipedia and Reddit contain only ~10k nodes. This raises concerns about scalability to real-world applications. In comparison, TGAT has been tested on Industrial data, which has 170k nodes and 2m edges.\n- The performance gains over the best baselines are often very small shown in Table 1. This may raise the cost-benefit trade-off corncern, especially considering the complexity of the designed components such as motif searching and extension.\n- The ablation study shows that different components have no (?) much impact on the performance. More investigations should be conducted to explain where the improvement comes from."}, "questions": {"value": "- More diverse evaluation tasks\n- Experiments on larger-scale graphs\n- Explanations on ablation studies\n- Discussions on cost-benefit trade-off"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BoN4aWFORz", "forum": "MsybA5QcGE", "replyto": "MsybA5QcGE", "signatures": ["ICLR.cc/2026/Conference/Submission5173/Reviewer_3cQC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5173/Reviewer_3cQC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761502965854, "cdate": 1761502965854, "tmdate": 1762917928752, "mdate": 1762917928752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TMMP, a Temporal Motif Message Passing network designed to improve continuous-time dynamic graph representation learning by incorporating temporal motifs into message passing. It employs a temporal-motif extension module, an attentive motif encoder, and a dual-hypergraph message-passing mechanism to capture diverse temporal semantics and high-order correlations among motif edges."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a formulation and modular framework.\n\n- Experimental results are extensive.\n\n- The method captures high-order temporal dependencies in a principled manner."}, "weaknesses": {"value": "- The computational complexity of motif searching and dual-hypergraph construction is very high, which may limit scalability to large-scale dynamic networks.\n﻿\n﻿\n- The notation in the methodology section is heavy and sometimes inconsistent, reducing readability.\n﻿\n- Baseline coverage omits several recent 2024–2025 CTDG models and contrastive-learning approaches, which could affect fairness of comparison.\n﻿\n- The practical efficiency analysis reports per-epoch times but does not include end-to-end runtime or memory usage on larger datasets, making it difficult to assess deployment feasibility.\n﻿\n﻿\n- TMMP introduces a dual hypergraph message passing mechanism to model high-order correlations. Could the authors clarify whether this mechanism captures new relational patterns that cannot be represented by existing hypergraph-based or attention-based models?\n﻿\n- The proposed framework stacks multiple attention layers (in motif encoding and final aggregation). Has the impact of attention depth or head number been systematically analyzed, and how does this multi-level attention influence representation interpretability or model stability?"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a3YmVYAWfE", "forum": "MsybA5QcGE", "replyto": "MsybA5QcGE", "signatures": ["ICLR.cc/2026/Conference/Submission5173/Reviewer_AJii"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5173/Reviewer_AJii"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837535753, "cdate": 1761837535753, "tmdate": 1762917928310, "mdate": 1762917928310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new model for dynamic graph representation learning, called the Temporal Motif Message Passing Network (TMMP). The method constructs high-order dependencies through temporal motifs, transforming dynamic graphs into explicit and implicit hypergraphs, followed by multi-stage message passing. The authors validate the model on several public datasets, demonstrating performance superior to existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The design from temporal motif construction to the message propagation module follows a consistent logic and can be seamlessly integrated with existing dynamic graph representation learning frameworks."}, "weaknesses": {"value": "1. TMMP introduces multiple modules (such as motif expansion, dual hypergraph construction, and attention-based aggregation), which make the model structure complex and significantly increase the number of parameters. However, the resulting performance improvement is limited. As shown in the experiments in Section 5.6, TMMP has more than ten the number of parameters of CNEN and both its training and inference times are significantly higher. Moreover, the comparison in that table is not entirely fair. To my knowledge, model such as RepeatMixer also has far fewer parameters and run much faster,, while achieving very similar performance. Yet its result is not included.\n\n2. One of the core components of the model is the learning of the implicit hypergraph. However, the paper does not demonstrate whether the learned implicit hyperedges capture meaningful high-order structures, nor does it provide any visualization or statistical analysis. The learning mechanism of this component lacks intuitive explanation.\n\n3. The proposed model is primarily validated through empirical experiments, with limited theoretical justification or analysis of computational complexity. Motif enumeration and expansion are theoretically exponential in complexity. While the authors claim preprocessing is feasible, no large-scale experiments are provided. For million-edge graphs, the computational cost may be prohibitive. Furthermore, the paper does not provide a clear rationale for why the specific combination of explicit and implicit hypergraph propagation should be inherently superior to alternative forms of high-order modeling in the context of temporal motifs.\n\n4. The ablation/hyper-parameter studies lack cross-dataset consistency analysis and is limited to a single dataset (UCI). Moreover, the paper does not evaluate the model’s robustness, such as its sensitivity to time window selection or motif size.\n\n5. The main experiments do not align with established protocols in the literature (e.g., DyGFormer). Only a single negative sampling strategy is adopted, whereas recent works typically report results under random，historical，inductive negative sampling settings. Moreover, several standard continuous-time dynamic graph benchmarks—such as MOOC, Social Evo, Flights, UNtrade, and Contact—are omitted, limiting the comparability of the results."}, "questions": {"value": "please see the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "21cyE7Ni0p", "forum": "MsybA5QcGE", "replyto": "MsybA5QcGE", "signatures": ["ICLR.cc/2026/Conference/Submission5173/Reviewer_unnx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5173/Reviewer_unnx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5173/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877269825, "cdate": 1761877269825, "tmdate": 1762917927796, "mdate": 1762917927796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}