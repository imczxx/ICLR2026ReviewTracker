{"id": "Fz3JWBVLG8", "number": 24477, "cdate": 1758357225586, "mdate": 1759896763862, "content": {"title": "COALA: Convex Optimization for Alignment and Preference Learning on a Single GPU", "abstract": "Fine-tuning large language models (LLMs) to align with human preferences has driven the success of systems like ChatGPT and Gemini. \nHowever, methods like Reinforcement Learning from Human Feedback (RLHF) remain computationally expensive and complex. \nDirect Preference Optimization (DPO) offers a simpler alternative but has limitations such as inconsistent ranking accuracy, reliance on a frozen reference model, and high dependence on expensive GPU resources. \nWe introduce COALA, a novel lightweight algorithm that leverages the convex optimization reformulation of neural networks (cvxNN).\nBy exploiting the expressiveness of cvxNN, COALA eliminates the need for a reference model and obtains significant reduction in both training time and VRAM consumption, thus enabling efficient training on a single GPU. \nExperiments across three datasets—including a 23,228-sample synthetic educational feedback dataset—and five models (including LLaMA-8B) demonstrate that COALA outperforms traditional preference alignment methods such as DPO and ORPO.\nCOALA exhibits stable, monotonically increasing rewards and reaches peak margins in significantly shorter time in comparison to competitors. \nTo the best of our knowledge, this is the first time convex optimization has been successfully applied to preference fine-tuning of LLMs.", "tldr": "Using convex reformulation of neural networks for preference fine-tuning of LLMs on a single GPU.", "keywords": ["preference learning", "fine-tuning LLMs", "single GPU", "resource-constrained", "convex neural networks"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3bd364cd312ca94798736c32d534cdc3de68c40e.pdf", "supplementary_material": "/attachment/71cd8aa20583906758cfc9f5cbd24f4b22b168c1.zip"}, "replies": [{"content": {"summary": {"value": "This paper focuses on the efficiency of large language model (LLM) preference alignment, and proposes COALA, which is a convex framework for efficient preference alignment using less computational resources. Experiments demonstrate that COALA achieves comparable performance using a single RTX 4090 GPU."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The topic of efficiency of LLM alignment methods is meaningful and valuable.\n- The idea of considering convex optimization and providing theoretical guarantee for alignment methods is very interesting."}, "weaknesses": {"value": "- There are many formatting and layout errors in this paper, such as the pseudo codes in Algorithm 1 and Algorithm 2, Table 9, and \"Appendix !!\" appearing in Section 5.1.\n- The writing quality needs improvement, as the current manuscript is hard to follow. For example, there is no figure or clear description illustrating the overall pipeline or workflow of the proposed method.\n- The experiments are not solid enough. For example, SimPO is also a reference-free alignment method, which is mentioned in the paper, but the authors do not include it as a baseline for comparison in their experiments. \n- The evaluations are not convincing. The trends in Figure 1 seem unusual, and additional clarification or supporting evidence would help verify these observations. Moreover, there is only experimental result of one model Dolphin-7B and one benchmark AlpacaEval2 presented in the main text, which is a little bit poor.\n- The experimental results are not reported in a consistent and rigorous manner. For instance, in Tables 10, Table 11 and Table 12, the same metric is reported with varying decimal precision, sometimes in one decimal place, and sometimes in two decimal places.\n- The paper wants to demonstrate its efficiency contribution in reducing the computational burden of alignment method. However, in the main text, there are no experimental results to evaluate or discuss the performance of computational burden and resources."}, "questions": {"value": "- How do you handle scenarios where the model exceeds 24 GB VRAM? In particular, when you state \"one RTX 4090 24 GB GPU,\" what is the maximum model size in parameters this claim covers?\n- I am not sure whether you perform full-parameter tuning? I doubt that such training can be implemented on a single RTX 4090 24 GB GPU for a 7B- or 8B-scale model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OwgMTjoru2", "forum": "Fz3JWBVLG8", "replyto": "Fz3JWBVLG8", "signatures": ["ICLR.cc/2026/Conference/Submission24477/Reviewer_xyme"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24477/Reviewer_xyme"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761210509331, "cdate": 1761210509331, "tmdate": 1762943093047, "mdate": 1762943093047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "COALA reframes preference alignment as a convex optimization problem by attaching a two-layer convex neural network (cvxNN) head on top of a frozen LLM. Phase I trains the cvxNN with a specialized ADMM solver (CRONOS). Phase II freezes the first layer and optimizes only the last layer via a convex logistic objective, eliminating the need for a reference model and enabling single-GPU training with theoretical convergence guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Theoretical–practical bridge: convex guarantees (ADMM O(1/k); convex last-layer optimization) translate into stable training and predictable convergence. \nEngineering simplicity: no reference model, fewer brittle hyperparameters, single-GPU practicality."}, "weaknesses": {"value": "Though TFLOPs are reported, head-to-head compute-matched comparisons vs SimPO/ORPO at their best hyperparams would clarify pure algorithmic gains.\n\nMain datasets are UltraFeedback, IMDb, and a synthetic EduFeedback; broader multilingual/OOD sets and more granular human slices would strengthen generality.\n\nFreezing the backbone + optimizing a convex head may limit capacity on hard preference shifts (e.g., nuanced safety constraints, multilingual pragmatics). (Authors hint at future multilingual/safety.)"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tNwbYr0tmh", "forum": "Fz3JWBVLG8", "replyto": "Fz3JWBVLG8", "signatures": ["ICLR.cc/2026/Conference/Submission24477/Reviewer_yCRe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24477/Reviewer_yCRe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761383375012, "cdate": 1761383375012, "tmdate": 1762943092535, "mdate": 1762943092535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new framework for fine-tuning large language models to align with human preferences through convex optimization. Implemented in JAX, COALA achieves efficient single-GPU training while maintaining competitive or superior alignment performance compared to DPO and ORPO on benchmarks like AlpacaEval2, ArenaHard, and MT-Bench. The work also contributes a new preference dataset, EduFeedback, and an open-source JAX implementation."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The primary strength of this work lies in its highly original approach of reformulating LLM preference alignment as a convex optimization problem. This is a significant conceptual leap that shifts the paradigm from heuristic-driven, unstable training processes (like DPO) to a principled, theoretically-grounded one. The proof that the COALA loss is convex (Proposition 1) and the associated convergence guarantees (Theorems 1 & 2) provide a strong theoretical foundation that is often lacking in this empirical field. This is a fundamental contribution.\n\n2. By eliminating the need for a reference model and leveraging efficient solvers like CRONOS for its convex objective, COALA dramatically lowers the computational barrier for preference alignment. The ability to fine-tune models as large as LLaMA-8B on a single RTX 4090 is a game-changer. This not only makes alignment research more accessible and sustainable but also holds significant promise for practical, on-premise deployments. The reported TFLOPS measurements quantitatively confirm this remarkable efficiency gain (e.g., requiring only ~17.6% of DPO's TFLOPS for LLaMA-8B)\n\n3.  The authors have conducted a thorough and convincing empirical evaluation. The experiments span five different models, three datasets, and multiple established benchmarks. Crucially, the inclusion of a **107-participant human evaluation study** adds significant credibility to the automated metrics and directly validates that COALA's outputs are indeed preferred by humans. The consistent, monotonically increasing reward margins shown in Figure 1(a) are a powerful visual testament to the stability afforded by the convex formulation, in stark contrast to the noisy signals from DPO and ORPO."}, "weaknesses": {"value": "1.  The paper's clarity significantly hinders its impact. The overall narrative is difficult to follow, with key concepts and notations (e.g., the structure of the cvxNN and its inputs) often being used before they are formally defined.\n   * The decision to relegate almost all primary quantitative results (e.g., Tables 9-13 in the original document) to the appendix is a major flaw. This forces the reader to constantly switch between the main text and the appendix to understand the core findings, disrupting the flow and making a proper assessment difficult. Key results justifying the paper's claims must be in the main body.\n    * The figures, especially Figure 1, are of poor quality. The font size is illegibly small, and the labels are unreadable, which undermines their purpose. They need to be remade with accessibility in mind (larger fonts, clearer lines).\n\n2.  The core design of COALA involves freezing the base LLM and only training the final layer of a small, appended network. While this is key to achieving convexity, it raises a critical question about the **trade-off in expressiveness**. Is such a lightweight, \"external\" adapter powerful enough to instill complex, nuanced preferences, or is it limited to learning more superficial alignment signals? The paper would be much stronger if it included a discussion or an ablation study on the limitations of this approach compared to methods that update the full model weights (or a larger set of parameters via LoRA).\n\n3.  The proposed method for creating the `EduFeedback-Alternate` dataset is novel but rests on a strong assumption: that an agent's immediate response (`i`) is preferable to its subsequent one (`i+2`). This may not always hold true; a later response could be a clarification or a more thoughtful, refined answer. This data creation heuristic needs more justification or an analysis of its potential biases. How sensitive are the results to this specific strategy?"}, "questions": {"value": "1. Could you elaborate on the limits of freezing the base LLM? Have you considered a hybrid approach where COALA's convex objective is used to guide a more traditional (but perhaps more lightly regularized) fine-tuning of the base model's weights? Could COALA provide a \"stable reward signal\" to regularize a full DPO-style update?\n\n2. Could you clarify the exact architecture of the cvxNN and how the features from the base LLM $f_{\\theta_{pre}}(x)$ are fed into it? Section 4.1 is quite dense, and a clearer, step-by-step walkthrough would be beneficial. For instance, what is the dimensionality of the features extracted from the base model?\n\n3. Regarding the EduFeedback-Alternate dataset creation: Could you provide some qualitative examples or statistics to support the assumption that immediate responses are generally better than later ones in your conversational data? Have you experimented with other heuristics for creating (chosen, rejected) pairs from multi-turn dialogues?\n\n**Minor Comments & Typos**\n\n1. Citation style: incorrect at Lines 118–120, 189, 192, 254. \n\n2. Algorithm 1 formatting broken. \n\n3. Line 405 formatting failure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RBT5Z0LMy5", "forum": "Fz3JWBVLG8", "replyto": "Fz3JWBVLG8", "signatures": ["ICLR.cc/2026/Conference/Submission24477/Reviewer_GXL1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24477/Reviewer_GXL1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24477/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917063538, "cdate": 1761917063538, "tmdate": 1762943092266, "mdate": 1762943092266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}