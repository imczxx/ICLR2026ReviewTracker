{"id": "hYZ03uKVO7", "number": 15865, "cdate": 1758256315328, "mdate": 1759897276856, "content": {"title": "Jailbreaking in the Haystack", "abstract": "Recent advances in long-context language models (LMs) have enabled million-token inputs, expanding their capabilities across complex tasks like computer-use agents. Yet, the safety implications of these extended contexts remain unclear. To bridge this gap, we introduce NINJA (short for Needle-in-haystack jailbreak attack), a method that jailbreaks aligned LMs by appending benign, model-generated content to harmful user goals, leveraging the critical observation that the positioning of harmful goals plays a significant role in safety. Experiments show that NINJA significantly increases attack success rates across multiple small-to-mid-sized models, including LLaMA-3, Qwen-2.5 and Gemini Flash, achieving strong performance on HarmBench; we further validate positional effects in the BrowserART web-browsing agent framework. Unlike prior jailbreaking methods, our approach is low-resource, transferable, and less detectable. Moreover, we show that NINJA is compute-efficient: under a fixed compute budget, increasing context length can outperform increasing the number of trials in best-of-N jailbreak. These findings reveal that even benign long contexts—when crafted with careful goal positioning—introduce fundamental vulnerabilities in modern LMs.", "tldr": "We present Ninja, a prompt-based jailbreak attack on long-context LLMs that hides harmful goals in benign input. Simply extending context length can degrade alignment, enabling stealthy, compute-efficient attacks.", "keywords": ["long-context language models", "long-context attack", "jailbreak attacks", "alignment robustness", "adversarial prompting", "model safety", "prompt injection", "contextual vulnerabilities", "LLM evaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3aaf59653d3b52d863744b0e8f9c5e66396096b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces NINJA, a novel and highly effective jailbreak attack that exploits long-context language models by embedding harmful goals at the beginning of benign, model-generated content. Extensive experiments show that NINJA is stealthy, transferable, and compute-efficient, revealing a fundamental vulnerability in current LLM alignment strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces NINJA, a simple yet powerful jailbreak method that uses entirely benign, semantically relevant content instead of traditional adversarial prompts, revealing a new class of stealthy attacks.\n2. This paper provides compelling empirical evidence that the position of harmful goals dramatically affects jailbreak success, uncovering a previously underexplored safety weakness in long-context LMs.\n3. The authors evaluate their method across diverse models and agent settings, showing that NINJA is robust and transferable without requiring stronger attacker models or visible adversarial cues."}, "weaknesses": {"value": "1. While the empirical findings on goal positioning are strong, the paper does not offer a clear theoretical framework or model-level analysis (e.g., attention distribution) to explain why early-positioned goals are more effective.\n2. The experiments focus primarily on base instruct models and do not extensively evaluate NINJA against recent or state-of-the-art defense techniques.\n3. The comparison is limited to only two prior jailbreak methods and more diverse baselines such as GCG and AutoDAN should be included."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fHXjSzcII6", "forum": "hYZ03uKVO7", "replyto": "hYZ03uKVO7", "signatures": ["ICLR.cc/2026/Conference/Submission15865/Reviewer_EriM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15865/Reviewer_EriM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652675774, "cdate": 1761652675774, "tmdate": 1762926084345, "mdate": 1762926084345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NINJA (Needle-in-haystack jailbreak attack), a novel method that exposes a critical safety vulnerability in long-context language models. The attack operates by placing a harmful user goal (the \"needle\") at the beginning of a long, semantically relevant, but otherwise benign context (the \"haystack\"). The authors demonstrate that this technique significantly increases Attack Success Rates (ASR) on models like LLaMA-3.1 and Qwen2.5. The paper's core contributions are: (1) identifying that the position of the harmful goal is a critical variable, with attacks being far more successful at the start of the context than the end, and (2) demonstrating that this long-context attack is more compute-efficient than standard best-of-N sampling, especially as an attacker's compute budget increases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a clear identification and empirical validation of goal positioning as a critical safety vulnerability. This reframes positional bias from a simple capability quirk to a fundamental, exploitable flaw in safety alignment.\n2. The experiment in Section 5.3 / Figure 5, which compares relevant vs. irrelevant context, is a cool and unique contribution . It proves that the attack is not merely \"confusing\" the model with noise, but actively \"distracting\" its attention with semantically related, benign content.\n3. The compute-optimality analysis in Section 5.4 / Figure 6 is a significant strength."}, "weaknesses": {"value": "The paper clearly distinguishes NINJA (relevant context, goal at start) from Cognitive Overload (distracting context, goal at end) . However, it doesn't complete the \"cross-over\" experiment. The authors' own findings show NINJA fails if the goal is at the end (Figure 3). To fully prove that relevance is the key differentiator, they should have also tested a \"Cognitive Overload at Start\" (i.e., goal at start + irrelevant context). This would isolate whether the \"goal-at-start\" phenomenon is universal or one that is uniquely enabled by the relevant context."}, "questions": {"value": "Can you answer the Comments I made in the \"Weakness section\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Not Applicable"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5BmPazUjSg", "forum": "hYZ03uKVO7", "replyto": "hYZ03uKVO7", "signatures": ["ICLR.cc/2026/Conference/Submission15865/Reviewer_KXbt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15865/Reviewer_KXbt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797901525, "cdate": 1761797901525, "tmdate": 1762926083786, "mdate": 1762926083786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"NINJA\" (Needle-in-haystack jailbreak attack), a method that jailbreaks aligned LLMs by embedding a harmful goal at the beginning of a long, benign, and semantically relevant context. The authors claim that increasing context length itself significantly degrades model safety, with goal positioning being a critical factor. Experiments on HarmBench and agentic frameworks demonstrate NINJA's effectiveness over PAIR and Many-shot baselines, and its compute-efficiency under fixed budgets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper demonstrates that long, benign contexts can be an effective attack vector, which is practical and stealthy due to the use of non-malicious content.\n\n2. The study provides a clear empirical analysis of how goal positioning within the context affects attack success."}, "weaknesses": {"value": "1. The paper fails to adequately distinguish its core contribution from existing long-context attacks, particularly Many-shot Jailbreaking [1]. While the authors note that NINJA uses \"entirely innocuous context,\" this distinction is superficial. Both methods exploit long contexts to dilute safety alignment; the difference between \"explicitly harmful\" and \"benign\" examples is a matter of degree rather than a fundamental mechanistic difference. A deeper discussion of the underlying failure mode (e.g., attention dilution, task confusion) shared by both approaches is needed to establish a clear boundary for the claimed novelty.\n\n2. The reported performance gains do not robustly support the claim of \"significant\" safety degradation. While the improvement for Llama-3.1-8B is notable (23.7% to 58.8%), the results for Qwen2.5-7B (23.7% to 42.5%) are modest, and the gain for Gemini 2.0 Flash (23% to 29%) is minimal—a mere 6 percentage points. This weak performance on a widely-used model severely undermines the paper's argument that this is a universal and critical vulnerability. The claim would be better supported by a more nuanced interpretation that acknowledges significant model-dependent variation.\n\n3. The experimental comparison is limited to only two baseline methods (PAIR and Many-shot). This narrow scope overlooks several other relevant and strong baselines, such as ReNeLLM  or CodeAttack. The absence of these comparisons makes it difficult for the reader to gauge NINJA's true standing in the current landscape of jailbreaking techniques and to assess whether the observed performance is state-of-the-art or simply an incremental improvement over a weak set of baselines. \n\n4.The paper briefly suggests placing user goals at the end of the prompt as a mitigation strategy but provides no empirical validation of this defense's efficacy or its potential impact on model capability for legitimate long-context tasks. A convincing safety analysis should at a minimum test this proposed defense against the NINJA attack and discuss its limitations and potential side effects. The absence of any defensive evaluation makes the contribution less actionable for practitioners seeking to secure their systems.\n\nReferences:\n\n[1] Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, et al. Many-shot jailbreaking. arXiv preprint arXiv:2304.XXX, 2024b. Anthropic Technical Report."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xvt96w2NHe", "forum": "hYZ03uKVO7", "replyto": "hYZ03uKVO7", "signatures": ["ICLR.cc/2026/Conference/Submission15865/Reviewer_1kog"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15865/Reviewer_1kog"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15865/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920477800, "cdate": 1761920477800, "tmdate": 1762926083348, "mdate": 1762926083348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}