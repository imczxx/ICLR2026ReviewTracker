{"id": "1n7yRC65E4", "number": 4475, "cdate": 1757686538298, "mdate": 1759898030665, "content": {"title": "Semantically Guided Adversarial Testing of Vision Models Using Language Models", "abstract": "In targeted adversarial attacks on vision models, the selection of the target label is a critical yet often overlooked determinant of attack success. This target label corresponds to the class that the attacker aims to force the model to predict. Now, existing strategies typically rely on randomness, model predictions, or static semantic resources, limiting interpretability, reproducibility, or flexibility. This paper proposes a semantics-guided framework for adversarial target selection using the cross-modal knowledge transfer from pretrained language and vision-language models. We evaluate several state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity sources to select the most and least semantically related labels with respect to the ground truth, forming best- and worst-case adversarial scenarios. Our experiments on three vision models and five attack methods reveal that these models consistently render practical adversarial targets and surpass static lexical databases, such as WordNet, particularly for distant class relationships. We also observe that static testing of target labels offers a preliminary assessment of the effectiveness of similarity sources, a priori testing. Our results corroborate the suitability of pretrained models for constructing interpretable, standardized, and scalable adversarial benchmarks across architectures and datasets.", "tldr": "We turn target selection in adversarial attacks from random guessing into a semantics-guided, interpretable, and scalable benchmark powered by pretrained language and vision-language models.", "keywords": ["Adversarial Attacks", "Target Label Selection", "Computer Vision", "Semantic Similarity", "Explainability"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d197e181191e0d1c126eba793c1d85937799f7a3.pdf", "supplementary_material": "/attachment/85b10c96a2c8c1af7b8cbc3e5ed8eac61b716a7e.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a semantics-guided protocol for selecting target labels in targeted adversarial attacks by computing “Most Similar” and “Least Similar” targets using embeddings derived from class names with text or vision–language models; WordNet WUP serves as a classical baseline. The method precomputes lookup tables, aiming for interpretability and reproducibility, and introduces a static dissimilarity metric (DM) intended to preview attack difficulty a priori. Experiments on three ImageNet models and five attacks suggest CLIP/WUP align well for local similarity (MS), whereas text/VLM embeddings better capture global dissimilarity (LS). The results, framed by RQ-structured analyses, indicate that static DM trends often mirror post-attack behavior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem of under-specified target selection in targeted attacks is articulated clearly, motivating semantics-based standardization and interpretability.\n- The approach is compact and reproducible through precomputed lookup tables using cosine similarity in embedding space, avoiding per-image heuristics.\n- The evaluation covers three classifiers, five attack methods, and both MS/LS cases, with FR/TSR and a static DM variant to study predictive compatibility.\n- The empirical trends are concrete: text-domain models tend to yield harder LS targets; CLIP/WUP perform best for MS; static DM often tracks post-attack DM.\n- The paper’s organization is clear with RQ-driven sections and informative figures/tables that communicate the main patterns."}, "weaknesses": {"value": "-  Attack configurations ($\\epsilon$, step size, steps, norms) are not fully specified in the main text; very high TSR for some attacks raises concerns about calibration that may mask MS/LS differences.\n- The study omits robustly trained or certified models, which would be most informative for demonstrating discriminative value of MS vs LS selection.\n- Similarities are computed from raw class names only; there is no analysis of prompt phrasing, synonyms, or multi-label settings.\n- The code link is placeholder-like at this time, with the paper not enumerating the exact implementation details to reproduce the results.\n- Statistical reporting is light: confidence intervals, multi-seed variance, and formal significance testing are absent.\n- The benchmark scope is narrow, relying on the NIPS 2017 dev set rather than modern robustness suites (e.g., ImageNet-A/R/Sketch).\n- The evaluation lacks a standardized, masking-resistant baseline; AutoAttack (untargeted and targeted modes) is absent, making it difficult to disentangle semantic target selection effects from potential attack misconfiguration or gradient masking.\n- Targeted evaluation uses PGD/MIM/C&W without targeted AutoAttack (APGD-t, FAB-t) constrained to the proposed MS/LS targets, preventing a direct comparison for the semantics-guided target selection."}, "questions": {"value": "- It would be better to report $\\epsilon$, step sizes, number of steps, and norm constraints for FGSM/PGD/MIM/SPSA/C&W per model, and ensure calibration across models.\n- Can you include adversarially trained or certified models to verify that MS vs LS remains discriminative under strong defenses?\n- How sensitive are similarity choices to prompt wording for labels (e.g., “a photo of a {label}”, synonyms)? Can a simple prompt ensemble improve stability?\n- Could you provide evaluations on other modern robustness dataset (ImageNet-A/R/Sketch or CIFAR-100) to assess generalization beyond NIPS 2017 dev, if possible?\n- Could you provide and test a selection algorithm that uses static DM to pre-choose the K most diagnostic targets, quantifying evaluation efficiency against random or WordNet baselines?\n- Please add untargeted AutoAttack at the same ε/‖·‖ as a calibration/sanity check, reporting robust accuracy with multi-seed confidence intervals; confirm that AA’s error rate is at least as high as your strongest untargeted attack.\n- For the central semantics claim, run targeted AutoAttack (e.g., APGD-t and FAB-t) restricted to the MS/LS targets for each class, holding ε/steps/step size/loss (e.g., DLR) fixed across methods, and report TSR/FR and pre- vs post-attack DM."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "z5j3RSaSKx", "forum": "1n7yRC65E4", "replyto": "1n7yRC65E4", "signatures": ["ICLR.cc/2026/Conference/Submission4475/Reviewer_z7LT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4475/Reviewer_z7LT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760939254711, "cdate": 1760939254711, "tmdate": 1762917389001, "mdate": 1762917389001, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "We withdraw the submission"}, "comment": {"value": "Dear Reviewers and AC,\n\nAs the majority of the reviews are technically/scientifically unacceptable, we decided to withdraw our paper, as we do not expect much engagement from the reviewers, given the current content of the reviews. These reviews contain demonstrably false claims, generic templates and empty phrasing, and clear signs of automated generation (evidently far beyond editing and language improvement) rather than expert assessment. The reviewers showed little intention to engage with the paper. For example, we received a comment alleging a possible violation of anonymity due to a clearly fake placeholder URL (\"https://github.com/AUTHOR/REPO-ICLR\"). The requested experiments are also not clearly justified or motivated, since we have already tested multiple source and target models, as well as attacks. While we fully support additional experimentation when it helps test better hypotheses or reveal new insights, simply adding experiments because other models and methods exist should not be demanded. After all, while we now have many different models than those used in the paper, they do not behave much differently when exposed to various attacks. This is not a matter of disagreement with criticism; it is a matter of reviewers not reading, not analysing, and not trying to understand the paper. Given the lack of rigour in this evaluation, we will not proceed further. Nevertheless, this letter expresses our disagreement with such practices. \n\nKind regards, \n\n---\nAuthors"}}, "id": "NmbOKKBl86", "forum": "1n7yRC65E4", "replyto": "1n7yRC65E4", "signatures": ["ICLR.cc/2026/Conference/Submission4475/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4475/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4475/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763558587039, "cdate": 1763558587039, "tmdate": 1763558610128, "mdate": 1763558610128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper tackles targeted adversarial attacks where target selection is often ad hoc, proposing a semantics-guided protocol that derives Most Similar (MS) and Least Similar (LS) targets from class-name embeddings (text models, CLIP, VLMs) with WordNet WUP as a classical baseline, and adds a static dissimilarity metric (DM) to estimate attack difficulty a priori.\n- The empirical study spans three ImageNet classifiers and five attack algorithms, reporting that CLIP/WUP better reflect local similarity useful for MS, whereas text/VLM embeddings capture global dissimilarity more relevant for LS, and that static DM usually trends with post-attack outcomes such as Target Success Rate (TSR) and Fooling Rate (FR)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper standardizes target selection for targeted adversarial attacks via semantics-guided MS/LS choices with precomputed lookup tables, yielding a training-free, interpretable, and easily adoptable protocol\n- The dissimilarity metric (DM) serves as a practical pre-attack predictor and triage tool; observed trends between DM and TSR/FR support its potential value, while clarifying local and global semantics\n- The study spans three ImageNet models and five attacks, providing multi-axis coverage that improves comparability and reporting hygiene across methods\n- The framework attempts to well separate semantics (targeted) from optimization (attack budgets)\n- The approach can reduce compute by DM-guided top-K target selection while retaining high fidelity to full sweeps, suggesting practical benchmarking efficiency"}, "weaknesses": {"value": "- Evidence for transferability robustness is incomplete, lacking thorough cross-model, cross-attack (under matched budgets), and cross-dataset targeted transfer analyses\n- Budget configurations (ε, steps, step size, etc.) are lack details, potentially inflating and confounding of semantic effects and attack performance\n- Baselines such as well-known targeted AutoAttack are missing, potentially weakening claims\n- Statistical rigor seems insufficient without random-seed confidence intervals, paired tests/effect sizes, or calibration plots, etc. leaving the significance of the method uncertain\n- Reliance on class-name text introduces polysemy/taxonomy confounds. Lack of hierarchy-aware or hybrid text+visual distances and absence of adversarially trained/certified models could potentially limit generality and framework validity"}, "questions": {"value": "- Can you fully specify the attack budgets for all (model, attack) pair experiments to isolate semantic effects from optimizer strength?\n- Can you add targeted AutoAttack baselines restricted to MS/LS at matched budgets?\n- What are the per-model, per-attack Spearman/Pearson correlations (with 95% CIs) and DM-decile calibration plots demonstrating monotonicity between DM and TSR/FR?\n- Do MS/LS trends persist under cross-model, cross-attack, and cross-dataset targeted transfer, and on adversarially trained and certified models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o86FibQUIv", "forum": "1n7yRC65E4", "replyto": "1n7yRC65E4", "signatures": ["ICLR.cc/2026/Conference/Submission4475/Reviewer_ga7X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4475/Reviewer_ga7X"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656561507, "cdate": 1761656561507, "tmdate": 1762917388461, "mdate": 1762917388461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitation of existing target label selection strategies in targeted adversarial attacks on vision models. It proposes a semantics-guided framework that leverages cross-modal knowledge from pretrained language and vision-language models to select the most and least semantically similar target labels relative to the ground truth."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well written.\n- The proposed method is interesting."}, "weaknesses": {"value": "- This paper merely replaces WordNet/model weights with pretrained language/VL models but fails to justify why this is a paradigm shift rather than incremental improvement.\n- The NIPS 2017 dataset is outdated (2017) and small-scale, lacking the complexity of modern datasets with more diverse classes and realistic perturbations.\n- Only 3 vision models are tested, all of which are relatively shallow. The framework's performance on state-of-the-art VLMs remains unproven. \n- A GitHub link (https://github.com/AUTHOR/REPO-ICLR) appears in line 259 of this paper, and it is unclear whether this violates the double-blind review principle."}, "questions": {"value": "- Please see \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tuJojvby7U", "forum": "1n7yRC65E4", "replyto": "1n7yRC65E4", "signatures": ["ICLR.cc/2026/Conference/Submission4475/Reviewer_RfKq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4475/Reviewer_RfKq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789504111, "cdate": 1761789504111, "tmdate": 1762917388151, "mdate": 1762917388151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes selecting the most effective target labels for targeted adversarial attacks using pretrained language and vision–language models. Experiments show that the proposed method outperforms baseline approaches. However, there remain several issues."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses an important topic, i.e., evaluating model robustness under adversarial attacks."}, "weaknesses": {"value": "1. The motivation of the work is not convincing. Targeted attacks aim to mislead the victim model into predicting a specific target class. The target class should not be chosen adaptively as the most effective one, as this undermines the purpose of evaluating model vulnerability.\n2. The compared baselines are outdated and limited to basic attack methods. Recent approaches should be discussed and compared.\n3. Only a few networks are evaluated (MobileNetV2, EfficientNetV2B0, and ResNet50V2). The study should include more diverse architectures, such as Transformers.\n4. The proposed method appears to be designed for white-box scenarios, which limits its applicability.\n5. The performance improvement is minor."}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UiMiencPCa", "forum": "1n7yRC65E4", "replyto": "1n7yRC65E4", "signatures": ["ICLR.cc/2026/Conference/Submission4475/Reviewer_7x1L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4475/Reviewer_7x1L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4475/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877088848, "cdate": 1761877088848, "tmdate": 1762917387841, "mdate": 1762917387841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}