{"id": "SL3GBvdwKw", "number": 22779, "cdate": 1758335313524, "mdate": 1762952892463, "content": {"title": "VidEEG-Gen: A Dataset and Diffusion Framework for Video-Conditioned Privacy-Preserving EEG Generation", "abstract": "Recent advancements in multimodal learning have revolutionized text, video, and audio processing, yet Electroencephalography (EEG) research lags due to data scarcity from specialized equipment and privacy risks in personal signal sharing. These limitations, coupled with the shortcomings of prior generative models that produce signals lacking spatiotemporal coherence, biological plausibility, and stimulus-response alignment, hinder the development of EEG-based applications, such as emotion analysis and brain-computer interfaces, by restricting access to diverse, high-quality data.\nThe absence of a dedicated task for modeling the mapping from naturalistic video stimuli to personalized EEG responses has impeded progress in privacy-preserving EEG synthesis.\nTo advance the field, we propose the task of stimulus-/subject-conditional EEG generation under naturalistic stimulation, which is crucial for enabling low-cost, scalable data generation while addressing ethical concerns.\nTo support this task, we introduce Video stimulus/individual-conditioned EEG generation dataset (VidEEG-Gen), a unified dataset and generation framework for video-conditioned privacy-preserving EEG synthesis. VidEEG-Gen features 1007 aligned video-EEG generation samples that synchronize natural video stimuli with synthetic EEG dynamics. At its core, VidEEG-Gen employs a Self-Play Graph Network (SPGN), a graph-enhanced diffusion model specifically designed for modeling spatiotemporal EEG patterns conditioned on visual input. This integrated approach provides a foundation for emotion analysis, data augmentation, and brain-computer interfaces.\nWe further establish a dedicated evaluation system to assess EEG generation quality in dynamic visual perception tasks. In benchmark visual stimulus experiments, the SPGN model within VidEEG-Gen achieved a signal stability index of 0.9363 and a comprehensive performance index of 0.9373. The source code and dataset will be made publicly available upon acceptance.", "tldr": "This paper proposes a novel stimulus-/subject-conditional EEG generation task, introduces VidEEG-Gen (with a SPGN model), and enables privacy-preserving, scalable EEG synthesis for emotion analysis and brain-computer interfaces.", "keywords": ["Video stimulus/individual-conditioned EEG generation dataset (VidEEG-Gen)", "Self-Play Graph Network (SPGN)", "Graph-Enhanced Diffusion", "Denoising Diffusion Probabilistic Model(DDPM)"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/16e20cc9dc8ec5489a143740c63a84b2d9470406.pdf", "supplementary_material": "/attachment/dbb664324f6a768474bacc355930d28675ae8ec5.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces VidEEG-Gen, a framework addressing the scarcity and privacy concerns associated with EEG data. The authors propose a new task: generating personalized, synthetic EEG signals conditioned on naturalistic video stimuli. To support this, they present a synthetic dataset derived from the SEED-DV video stimuli and a novel generative model called Self-Play Graph Network (SPGN). SPGN is described as a graph-enhanced diffusion model designed to capture the spatiotemporal dependencies in EEG signals while being conditioned on video features and subject metadata (e.g., demographics). The goal is to produce biologically plausible, stimulus-aligned EEG data that preserves privacy by avoiding the use of real subject recordings. The authors evaluate SPGN against several other EEG generation models, claiming superior performance in signal fidelity, stability, and spectral characteristics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles the critical issues of data scarcity and privacy in EEG research, which are significant barriers in the field.\nProposing the specific task of video-conditioned, personalized EEG generation is potentially valuable for advancing stimulus-response modeling.\nThe SPGN model attempts to explicitly model both spatial (graph) and temporal (diffusion) aspects of EEG, which is methodologically relevant."}, "weaknesses": {"value": "The core contribution, the VidEEG-Gen dataset, is generated by the proposed model (SPGN) itself. The entire evaluation framework appears to operate within this synthetic domain, lacking grounding in real EEG data distributions.\nThere is no evidence presented showing that the synthetic EEG generated by SPGN accurately reflects the characteristics (dynamics, spectral properties, spatial patterns) of real EEG recorded in response to the SEED-DV videos. Claims of \"biological plausibility\" are entirely unsubstantiated.\nThe paper does not clearly explain how the initial \"ground truth\" synthetic EEG signals (used for training SPGN and evaluating all models) were created or validated."}, "questions": {"value": "Please clarify precisely how the \"ground truth\" synthetic EEG signals used for training the SPGN model and for evaluating all methods in Table 1 were generated. What ensures their fidelity to real EEG responses elicited by the SEED-DV videos?\n\nWhy was the evaluation not performed by training models on real SEED-DV EEG data (or another suitable real dataset) and evaluating their ability to generate plausible signals conditioned on video, perhaps assessing quality via downstream tasks or established distribution metrics (like FID adapted for EEG)?\n\nWhat specific mechanism allows SPGN to generate personalized EEG based on metadata? How was this personalization capability validated?\n\nWhat is the definition and validation for the \"signal stability index\" and \"comprehensive performance index\" metrics used to claim SOTA performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lBYRUYipWR", "forum": "SL3GBvdwKw", "replyto": "SL3GBvdwKw", "signatures": ["ICLR.cc/2026/Conference/Submission22779/Reviewer_Ga3Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22779/Reviewer_Ga3Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760961738057, "cdate": 1760961738057, "tmdate": 1762942383841, "mdate": 1762942383841, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Dear ICLR 2026 Program Chairs and OpenReview Team,\n\nWe, the authors of submission #22779 titled “VidEEG-Gen: A Dataset and Diffusion Framework for Video-Conditioned Privacy-Preserving EEG Generation”, hereby formally request to withdraw our manuscript from consideration for ICLR 2026.\n\nThis decision has been made unanimously by all co-authors after carefully reviewing the feedback provided by the reviewers during the review process. We appreciate the time and thoughtful comments from the reviewers and acknowledge the concerns raised regarding the synthetic nature of the dataset, validation against real EEG data, and methodological clarity. While we believe the proposed task and framework hold potential, we agree that substantial revisions and additional empirical validation are necessary before resubmission.\n\nIn accordance with the OpenReview withdrawal policy, we confirm that:\n\nAll authors consent to this withdrawal.\nThe withdrawal is voluntary and not due to a double-submission violation or ethical breach.\nWe understand that once withdrawn, the submission will no longer be considered for ICLR 2026, and its status will be marked as “Withdrawn” on OpenReview.\nThank you for your support and for facilitating a transparent and constructive review process.\n\nSincerely,\n\nYunfei Guo, Tao Zhang, Wu Huang, Yao Song\n\nOn behalf of all authors"}}, "id": "xm4cp3UIG7", "forum": "SL3GBvdwKw", "replyto": "SL3GBvdwKw", "signatures": ["ICLR.cc/2026/Conference/Submission22779/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22779/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762952891338, "cdate": 1762952891338, "tmdate": 1762952891338, "mdate": 1762952891338, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper defines a new task of stimulus- and subject-conditioned EEG generation under naturalistic video stimulation. The paper also introduces VidEEG-Gen, a unified dataset and framework to study it. VidEEG-Gen contains 1,007 samples that align video clips (drawn from the SEED-DV corpus for semantic diversity) with synthetic EEG trajectories. The method centres on SPGN, a graph-enhanced diffusion model that fuses video features, subject metadata and optional EEG priors via a dedicated alignment and fusion pipeline. The approach then models inter-electrode dependencies with electrode and signal graphs while using diffusion for temporally coherent synthesis. The paper also establishes an evaluation protocol (including spectral band similarity, correlation, stability and composite scores) and reports that SPGN outperforms several recent EEG generative baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces the task of stimulus- and subject-conditioned EEG generation under naturalistic video stimulation, whic is an interesting contribution. This redefines EEG synthesis as a multimodal mapping from visual input to biologically plausible neural dynamics, rather than data-driven signal reconstruction. The accompanying VidEEG-Gen dataset establishes the first benchmark for this setting.\n\n- The SPGN architecture represents a synthesis of graph neural networks (for electrode-level spatial structure) and diffusion models (for temporal consistency). This integration addresses the limitations of earlier GAN- or VAE-based EEG generators due to the lack of spatiotemporal coherence and stimulus-response alignment.\n\n- The paper presents the architecture and preprocessing pipeline in a structured and detailed way, including the temporal alignment, multimodal fusion and spatial graph construction processes. It is clear how EEG, video and text features interact across modules, to make reproducibility and easy understanding possible."}, "weaknesses": {"value": "- While the proposed task and dataset are conceptually novel, the work’s significance remains constrained by the narrow empirical scope. All experiments are based on video stimuli from SEED-DV, which covers only 15 participants and 40 concepts. The authors acknowledge that cross-subject generalisation degrades by 12% in mean squared error and that downstream utility (e.g., whether the generated EEG improves classifier performance) is untested. \n\n- The same SPGN model both defines and evaluates the dataset’s structure. The biological plausibility and realism of the synthetic signals are assessed through internal metrics (e.g., frequency-band similarity, stability index) but not through expert or empirical comparison with real EEG traces. This limits confidence in the dataset’s physiological credibility.\n\n- The paper presents multiple interacting modules, including CLIP-based video encoders, text embeddings, graph convolutions, adversarial self-play and diffusion steps, but the ablation study explores only a small subset of these components (spatial attention and diffusion step count). As a result, it is unclear which design elements contribute most to the observed improvements."}, "questions": {"value": "- The paper mentions that the EEG signals in VidEEG-Gen are generated entirely by the SPGN model and not derived from real EEG recordings. Could the authors clarify how they prevent circularity between dataset construction and model evaluation? Specifically, is the same trained SPGN model used both to create and to benchmark the dataset?\n\n- In Section 4.1, the paper describes a fusion pipeline that aligns CLIP-extracted video features, demographic text embeddings and optional EEG priors using cross-attention. Could the authors clarify the role of the EEG prior in this process? For instance, is the prior always available during training, or is it optional to simulate unseen-subject conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "08waAIe1B8", "forum": "SL3GBvdwKw", "replyto": "SL3GBvdwKw", "signatures": ["ICLR.cc/2026/Conference/Submission22779/Reviewer_gfqH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22779/Reviewer_gfqH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761509081628, "cdate": 1761509081628, "tmdate": 1762942383481, "mdate": 1762942383481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a dataset for privacy-preserving EEG synthesis. \nSynthetic data generation may be important for increasing augmented data available for applications, but I do not understand why we need a dataset for this task. Normally, we’d like to augment data from a particular recording setup and task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- None"}, "weaknesses": {"value": "-There is no description of data or recording from subjects\n- This is not dataset contrbution\n- Concept controllable EEG data is not considered to be valid. There is some evidence of neural correlates that differ across semantic stimuli, but generally this mapping does not exist as EEG is very noisy and encodes attention, but not semantics. \n- The paper claims to generate diverese synthetic responses, but I think this is overclaiming\n- If the approach is syntethic data generation, then why is the paper written as dataset contribution?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h6GaRYPu27", "forum": "SL3GBvdwKw", "replyto": "SL3GBvdwKw", "signatures": ["ICLR.cc/2026/Conference/Submission22779/Reviewer_cDo3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22779/Reviewer_cDo3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849767665, "cdate": 1761849767665, "tmdate": 1762942383254, "mdate": 1762942383254, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VidEEG-Gen, a dataset and framework for video-conditioned, privacy-preserving EEG generation. The authors define a new task—stimulus- and subject-conditioned EEG synthesis using naturalistic video inputs—and propose the Self-Play Graph Network (SPGN), a graph-based diffusion model designed to more faithfully capture the spatial, temporal, and semantic relationships in EEG. The work includes the release of a 1007-sample synthetic EEG dataset aligned to video stimuli, reference implementations, and comparative/ablation studies benchmarking the approach against recent alternatives."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles the important issue of data scarcity and privacy in EEG research, proposing synthetic generation as a practical solution for applications in brain-computer interfaces and emotion analysis.\nThe SPGN framework judiciously combines graph neural networks (for capturing inter-electrode spatial dependencies) with denoising diffusion probabilistic models and cross-modal alignment/fusion mechanisms.\nThe work includes quantitative comparisons across multiple recent generative baselines, highlighting SPGN's solid performance in terms of signal fidelity and computational efficiency."}, "weaknesses": {"value": "Explorations of cross-modal conditioning or alternative fusion architectures are not exhaustively presented—raising questions about generality.\nThe proposed dataset, while meticulously constructed, is generated using only SEED-DV video stimuli and is of modest size. \nThe generalizability of the approach to more diverse or non-Chinese video/subject populations, or other brain recording scenarios, is acknowledged as a limitation, but no quantitative cross-dataset evidence is provided."}, "questions": {"value": "Could you detail how \"spatial-graph attention\" in the SPGN is formulated and how it interacts with the denoising diffusion process at each step? Is it applied as a preprocessing step, or jointly with diffusion iterations?\nWhat is the practical impact of using solely synthetic EEG for both training and evaluation? Are there risks of model feedback loops or degraded transfer performance to real data scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KHC0IAkzL6", "forum": "SL3GBvdwKw", "replyto": "SL3GBvdwKw", "signatures": ["ICLR.cc/2026/Conference/Submission22779/Reviewer_iUcf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22779/Reviewer_iUcf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762332581646, "cdate": 1762332581646, "tmdate": 1762942382781, "mdate": 1762942382781, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}