{"id": "c3yt5VSZPQ", "number": 14970, "cdate": 1758246329497, "mdate": 1759897338547, "content": {"title": "Understanding the Robustness of Distributed Self-Supervised Learning Frameworks Against Non-IID Data", "abstract": "Recent research has introduced distributed self-supervised learning (D-SSL) approaches to leverage vast amounts of unlabeled decentralized data. However, D-SSL faces the critical challenge of data heterogeneity, and there is limited theoretical understanding of how different D-SSL frameworks respond to this challenge. To fill this gap, we present a rigorous theoretical analysis of the robustness of D-SSL frameworks under non-IID (non-independent and identically distributed) settings. Our results show that pre-training with Masked Image Modeling (MIM) is inherently more robust to heterogeneous data than Contrastive Learning (CL), and that the robustness of decentralized SSL increases with average network connectivity, implying that federated learning (FL) is no less robust than decentralized learning (DecL). These findings provide a solid theoretical foundation for guiding the design of future D-SSL algorithms. To further illustrate the practical implications of our theory, we introduce MAR loss, a refinement of the MIM objective with local-to-global alignment regularization. Extensive experiments across model architectures and distributed settings validate our theoretical insights, and additionally confirm the effectiveness of MAR loss as an application of our analysis.", "tldr": "We theoretically characterize the robustness of different distributed self-supervised learning frameworks under non-IID unlabeled data, and validate the derived insights through extensive experiments.", "keywords": ["Distributed Learning", "Heterogeneous and Unlabeled Data", "Self-Supervised Learning", "Federated Learning", "Decentralized Learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/08fb3863a135288f359c975f32e871b4aa9e7978.pdf", "supplementary_material": "/attachment/348fef235ab5c30ffbd3cbd2ec87b5474e583478.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies how different distributed self-supervised learning (D-SSL) choices handle non-IID client data. It analyzes a simplified label-skew model (each client i mostly has two classes plus a rare third) and defines a Representability Vector (RV)—the squared projection of standard basis vectors onto the learned feature subspace—to quantify “robustness” of different methods. The main theoretical claims are: (i) MIM-based D-SSL is asymptotically less sensitive to non-IIDness than CL-based D-SSL (Theorem 4.4), and (ii) robustness improves with the average client connectivity; FL is no less robust than DecL (Corollary 4.5, Theorem 4.6). Building on this, the authors propose MAR and report empirical trends on Mini-ImageNet pretraining with CIFAR-10/100 and ImageNet fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses a central D-SSL design question and provides actionable hypotheses . The empirical section illustrates these points across backbones and topologies. \n- The RV/sensitivity notion is intuitive for comparing learned subspaces across frameworks."}, "weaknesses": {"value": "Below are the most critical technical issues I found while reading the proofs and setup.\n\n1) Incorrect “equivalence” of the MIM objective (Eq. (3) → Eq. (4)). \nEq. (3) is an $\\ell_2$ reconstruction loss $\\|W(x\\odot m)-(x\\odot (1-m))\\|^2$. Eq. (4) replaces it by a negative squared inner product $-\\|(W(x\\odot m))^\\top(x\\odot(1-m))\\|^2 + \\tfrac12\\|W^\\top W\\|_F^2$, stated as \"equivalent\". This drops the $\\|W(x\\odot m)\\|^2$ and $\\|x\\odot(1-m)\\|^2$ terms that do affect the optimizer (the latter is data-dependent), and it adds a data-independent Frobenius penalty as a surrogate. No derivation is provided for this equality, only a citation to qualitative connections between MIM and contrastive alignment (which do **not** imply equality of objectives). The subsequent analysis treats (4) as the exact objective. This is a fundamental mismatch that invalidates the later first-order conditions and eigen-structure arguments built on top of (4). \n\n2) First-order optimality conditions are derived with wrong shapes and unjustified rearrangements.\nIn Appendix A.7, the gradient of the (modified) losses is set to zero and directly rearranged to $W^\\top W = \\mathbb{E}[\\cdots]$ (Eqs. (9)–(10) for MIM; Eqs. (33)–(35) for CL). However:\n  - For MIM, the objective involves an expectation of a *squared scalar* $\\|(W a)^\\top b\\|^2$. Its gradient has the form $-2A W$ with $A$ built from $\\mathbb{E}[aa^\\top b b^\\top]$, not $-2 W A$. The step \"$-2W\\,\\mathbb{E}[\\cdots]+2WW^\\top W=0 \\Rightarrow W^\\top W=\\mathbb{E}[\\cdots]$\" reverses the multiplication order and implicitly assumes invertibility/commutativity that does not hold. (pp. 19–20)\n  - For CL, Eq. (33) expands $\\|(W(x+\\xi))^\\top(W(x+\\xi'))\\|^2$ and then differentiates as if $\\mathbb{E}[x^\\top W^\\top W x]$ behaved like $\\mathbb{E}[x^\\top x]$, ultimately yielding Eq. (35): $W^\\top W = \\mathbb{E}[x^\\top x + x^\\top \\xi' + \\xi^\\top x + \\xi^\\top \\xi']$. This is **dimensionally inconsistent**: the RHS is a scalar, while $W^\\top W$ is a $d\\times d$ matrix. The derivation implicitly \"pulls out\" $W^\\top W$ from inside a squared scalar and replaces it by a scalar expectation. The CL proof with dissimilar augmentations (Eqs. (45)–(47)) suffers from the same issue.\n   \nThese steps are central: they enable the replacement of each loss by $\\|X - W^\\top W\\|_F^2\\$ and the invocation of Eckart–Young to claim the row-span of \\(W\\) equals the top-eigenspace of \\(X\\). Because the stationary conditions are not validly derived, the RV bounds that follow are on shaky ground. (Appendix A.7) \n\n3) Ambiguities and likely typos in the non-IID data model. \n- In the rare class definition, samples for client $i$ are given as $x^{(h_i)} = e_i + \\mu \\xi$ (p. 4). This reuses $e_i$ rather than $e_{h_i}$ (where $h_i\\in[2N]\\setminus\\{2i-1,2i\\}$), effectively collapsing the \"rare\" class onto the client's own axis instead of its designated class. This undermines the intended label heterogeneity.\n- The offset term $\\sum_{k\\neq i} q\\,\\tau e_k$ uses a single Bernoulli $q$ inside a sum, not $q_k$; thus the sum is either 0 or $\\tau\\!\\sum_{k\\neq i} e_k$, which is likely unintended.\n\n4) Definition and use of the Representability Vector (RV) are inconsistent.  \nDef. 4.1 defines $r=[\\|\\Pi_R(e_1)\\|^2,\\dots,\\|\\Pi_R(e_c)\\|^2]$ where $R\\subset\\mathbb{R}^d$ is the row-span and $c=\\text{rows}(W)\\le d$. Projecting only the first $c$ basis vectors is ad-hoc; later theorems index $k$ in ways that mix client and feature indices (e.g., \"where $i\\in[N]\\setminus k$\" in Theorem 4.2.1). The definition of \"sensitivity\" $s=\\lceil\\bar r\\rceil-\\lfloor\\bar r\\rfloor$ also lacks a precise norm (max–min over which coordinates?), yet it drives the main claims.\n\n5) Assumptions needed for concentration are not spelled out. \nThe proofs use matrix concentration to bound $\\|X - \\mathbb{E}X\\|$ \"with probability $\\ge 1-\\frac12 e^{-d^{1/10}}$\" (p. 20) but the number of samples per client, independence across samples/classes, and dependence on $d$ are unspecified beyond \"polynomial in $d$\" / $O(d^\\alpha)$. Without explicit sample sizes, the rates in $d$ (e.g., the $O(d^{-2/5})$ and $\\Theta(d^{7/20})$ terms) are hard to justify and compare. (Appendix A.7)\n\n6) Handling of the augmentation transform $H$.\nFor the \"dissimilar\" CL case, bounds scale with $\\operatorname{tr}(H)$ (Theorem 4.3). But $\\operatorname{tr}(H)$ can be zero or negative (e.g., certain rotations or permutations), which can flip signs in denominators and invalidate inequalities used to argue robustness ordering. The required conditions on $H$ (PSD? orthogonal? non-negative trace?) are not stated.\n\n7) Modeling choices weaken the external validity of the theory. \nThe linearization of encoders/decoders and of augmentations, and the unusual scaling $N=\\Theta(d^{1/20})$, $\\tau=d^{1/5}$, $\\mu=d^{-1/5}$, seem chosen to make algebra convenient, but the conclusions then rely on delicate powers of $d$ (e.g., $d^{2/5}$ vs. $d^{7/20}$). It is unclear how stable the ordering (MIM < CL sensitivity; FL ≥ DecL) is beyond this stylized regime (e.g., different $p$ schedules, non-linear encoders)."}, "questions": {"value": "Answers to the following questions may change my assessment:\n\n1. Please provide a rigorous derivation (or inequality bounds) showing that the reconstruction loss in Eq. (3) is equal to (or tightly bounded by) the alignment-only objective in Eq. (4) plus a data-independent Frobenius regularizer. If it is merely a lower/upper bound, how do the subsequent minimizer and eigenspace arguments change? \n\n2. In Appendix A.7, how do you get from the true gradients of $-\\mathbb{E}\\|(W a)^\\top b\\|^2$ (and the CL analog) to the identities $W^\\top W = \\mathbb{E}[\\cdots]$ (Eqs. (9)–(10), (35), (47))? Could you rewrite the proofs using the form $-\\mathrm{tr}(W^\\top A W)$ with $A=\\mathbb{E}[aa^\\top bb^\\top]$, leading to $A W = W W^\\top W$, and then justify the Eckart–Young step from there?\n\n3. Should the rare-class mean be $e_{h_i}$ (not $e_i$) and should the sum over offsets use $q_k$ (independent Bernoullis) rather than a single $q$? If yes, how do these corrections affect the covariance calculations and bounds?\n\n4. Please formalize $s=\\lceil\\bar r\\rceil-\\lfloor\\bar r\\rfloor$: which coordinates of $\\bar r$ are considered, and is it a max–min over all $d$ or only the first $c$? How sensitive are Theorems 4.4 and 4.6 to this choice?\n\n5. What properties of $H$ are required (e.g., PSD, orthogonal, $\\operatorname{tr}(H)\\ge 0$) to ensure the denominators in Theorem 4.3 and Eq. (54) are positive and the inequalities hold?\n\n6. Please specify the per-client sample sizes (as functions of $d$) and the independence assumptions used for concentration. Can you restate the bounds explicitly in terms of $n_i$ (client-level sample sizes) rather than only in powers of $d$? (Appendix A.7)\n\n7. Do the key inequalities (MIM < CL sensitivity; FL ≥ DecL) persist under non-linear encoders/decoders (e.g., Lipschitz networks) and realistic augmentations? Any partial evidence or counter-examples? (Secs. 3–4)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M8ZUWaOKxE", "forum": "c3yt5VSZPQ", "replyto": "c3yt5VSZPQ", "signatures": ["ICLR.cc/2026/Conference/Submission14970/Reviewer_MVSv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14970/Reviewer_MVSv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746682424, "cdate": 1761746682424, "tmdate": 1762925303871, "mdate": 1762925303871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretical analysis of the robustness of Distributed Self-Supervised Learning (D-SSL) frameworks against non-IID data. The authors' primary contributions are twofold: (1) A theoretical argument, based on a simplified linear model, that Masked Image Modeling (MIM) is inherently more robust to data heterogeneity than Contrastive Learning (CL). (2) A theoretical claim that the robustness of D-SSL improves with network connectivity, implying that Federated Learning (FL) is no less robust than Decentralized Learning (DecL). To illustrate the practical implications of their theory, the paper introduces MAR loss, a refinement of the MIM objective that adds a local-to-global representation alignment regularizer using Maximum Mean Discrepancy (MMD). The authors conduct extensive experiments to validate their theoretical claims and the effectiveness of MAR loss."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a critical and underexplored question in the D-SSL literature: the fundamental robustness of different frameworks (MIM vs. CL, FL vs. DecL) to non-IID data. Attempting to build a theoretical foundation for this problem is an ambitious and valuable goal. The questions asked are highly relevant to the community.\n\n2. The experimental evaluation is comprehensive. The authors validate their claims across multiple model architectures (ResNet and ViT), SSL paradigms (SimSiam and MAE), distributed settings (FL and DecL), and benchmark datasets. The inclusion of experiments on feature space visualization and ablation studies on MAR loss strengthens the empirical part of the paper."}, "weaknesses": {"value": "1. The entire theoretical analysis (Theorems 4.2-4.6) is built upon a set of highly simplified and potentially unrealistic assumptions. The analysis is restricted to a linear embedding function $f_W(x) = Wx$. Modern SSL methods, especially those using ViT and ResNet backbones, rely on deep, highly non-linear transformations. It is not at all obvious why insights derived from a linear model would generalize to these complex settings. The paper fails to provide any justification for this leap of faith, other than \"mathematical tractability.\" This severely limits the generality and impact of the theoretical claims. The non-IID data generation process described in Section 3.2 is artificial. Data points are constructed from orthogonal basis vectors ($e_i$), and each client $i$ is assigned data from two specific, unique classes ($2i-1$ and $2i$). This is a toy problem that does not reflect the complexity of real-world label heterogeneity, where classes can have semantic overlap and distributions are far more nuanced. There is a significant disconnect, as the theory is derived for this synthetic setting, while experiments use the more standard (and complex) Dirichlet distribution.\n\n2. The paper presents MAR loss as an \"illustrative case study,\" but it is also listed as a main contribution. The idea of using a distributional alignment penalty like MMD to enforce consistency between local and global models in FL is not new (e.g., work on FedMD, FedProx, etc., which use similar ideas for knowledge distillation or regularization). The contributions of MAR loss—using an adaptive MMD kernel and a cosine decay schedule for the weight—are incremental engineering refinements rather than a fundamental algorithmic innovation. Framing this as a key contribution overstates its novelty and distracts from the paper's main focus on theoretical understanding."}, "questions": {"value": "1. Could you elaborate on why the insights from a linear model $f_W(x) = Wx$ are expected to hold for highly non-linear models like ViT and ResNet? Is there any specific property of SSL objectives (MIM/CL) that makes this simplification more valid than it would be for, say, standard supervised learning?\n\n2. The analysis relies on a specific data generation process where each client has nearly orthogonal data classes. How does this simplified model capture the complexities of real-world non-IID data, such as the Dirichlet distribution used in your experiments, where clients share overlapping classes with different priors? Could the theory be extended to more realistic data distributions?\n\n3. The use of MMD to align distributions in FL is an established technique. Could you please clarify the core novelty of MAR loss beyond the combination of A-MMD and a decay schedule? How is the design of MAR loss a direct and unique consequence of your theoretical findings (e.g., Theorem 4.2), rather than a general-purpose regularizer that could be applied to many FL scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cl9Huohxod", "forum": "c3yt5VSZPQ", "replyto": "c3yt5VSZPQ", "signatures": ["ICLR.cc/2026/Conference/Submission14970/Reviewer_PoK1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14970/Reviewer_PoK1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926550868, "cdate": 1761926550868, "tmdate": 1762925303578, "mdate": 1762925303578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes robustness in distributed self-supervised learning under non-IID client data, comparing contrastive learning (CL) and masked image modeling (MIM) in both decentralized (peer-to-peer) and federated (server-aggregated) settings. The main theoretical result is that MIM is inherently less sensitive to client heterogeneity than CL. The analysis also shows that robustness in decentralized SSL improves with higher average graph connectivity, and that federated SSL is at least as robust as decentralized SSL. Motivated by the theory, the authors introduce MAR: a MIM objective augmented with an adaptive MMD alignment term whose weight decays over training to encourage early consensus. Experiments on standard vision benchmarks indicate that MIM and MAR degrade less from IID to non-IID splits and that MAR provides consistent gains with modest communication overhead."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "*  Strong theoretical analysis. Comparative guarantees showing MIM’s robustness advantages over CL; useful system-level insight about connectivity and federated vs. decentralized training\n* The paper establishes the relationship between network connectivity and robustness. This provides useful system-level guidance: increased connectivity improves decentralized robustness."}, "weaknesses": {"value": "* Restrictive weighting model: only uniform neighbor averaging; no analysis or experiments with alternative consensus matrices (e.g., doubly-stochastic, push-sum), adaptive/learned weights, or time-varying/intermittent connectivity.\n* Topology realism is limited: results are tied to average degree rather than spectral gap/mixing rate; experiments rely on simple random graphs, not clustered or heavy-tailed real networks.\n* Sensitivity to MAR hyperparameters (mask ratio, batch size affecting MMD, decay schedule) is underexplored.\n* Limited empirical evaluation breadth: few baselines/datasets and sparse ablations"}, "questions": {"value": "1) Can you state the graph/weighting assumptions explicitly in the main text (not just appendix) as a numbered assumption set\n2) Do the robustness results still hold under alternative consensus matrices (doubly-stochastic, row/column-stochastic, push-sum for directed graphs)? If not, what changes in the bounds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ErHoMGdGdN", "forum": "c3yt5VSZPQ", "replyto": "c3yt5VSZPQ", "signatures": ["ICLR.cc/2026/Conference/Submission14970/Reviewer_gLn9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14970/Reviewer_gLn9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976395777, "cdate": 1761976395777, "tmdate": 1762925303119, "mdate": 1762925303119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}