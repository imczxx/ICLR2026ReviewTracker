{"id": "4HGIIekCx3", "number": 22322, "cdate": 1758329633027, "mdate": 1759896872435, "content": {"title": "DenseMixer: Improving MoE Post-Training with Precise Router Gradient", "abstract": "Mixture-of-Experts (MoE) models are notoriously harder to train compared with dense models. Existing approaches either rely on imprecise router gradient or freeze router parameters entirely, limiting training effectiveness. We introduce DenseMixer, a novel MoE post-training technique that trades one extra forward pass on inactive experts for a more precise router gradient estimation. Our method consistently outperforms conventional methods across different MoE scales (7B, 14B, 16B, 30B), architectures (with/without shared experts), pre-training methods (from scratch/up-cycling), and post-training data types (instruction/long CoT data). It is universally applicable to any MoE using Top-K routing and can be used in a plug-and-play manner, compatible with existing training libraries and parameter-efficient methods like LoRA, introducing no changes to inference. We provide comprehensive empirical validation showing DenseMixer's effectiveness in improving MoE post-training quality while maintaining practical computational overhead.", "tldr": "We propose DenseMixer, a plug-and-play MoE post-training method that improves router gradients and training quality.", "keywords": ["MoE", "Post-Training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b4aebe633e7adf8cda9ca1fdc15fb70e26652f72.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper is introducing a post-training method for MoE models to improve the estimation of the gradient of the router which conventionally uses a non-differentiable top-k operation for sparsity in the MoE layer. The core idea of the paper is simply to introduce the STE technique for the gradient of the router logits w.r.t to the top-k selection function. This requires all experts to be activated (no sparsity). As a result, all experts are used and gradients for all router logits are more precise. Furthermore, they also provide a methodology to apply the STE for renormalized MoE models (where the router scores are normalized again after the top-k procedure)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "I believe the main strength of the paper is that it provides a practical solution to making pretrained MoE models more performant as this is a post-training technique. Their results on several MoE models (OlMoe, QWen, DeepSeek) definitely show improvement over benchmark tasks (even though for many tasks the improvement is quite marginal). Therefore, I believe that as far as pushing the performance of existing models goes, this is certainly a useful technique that I believe could be incorporated into standard post-training pipelines for sparse MoE models. The paper was written clearly, I did not find any mathematical or typographical errors and the efficiency / overhead analysis explains the tradeoffs in performance vs cost quite well."}, "weaknesses": {"value": "While I certainly believe the paper will be a useful method for pushing up the performance of MoE models, my main issue is with novelty in this area. The issue of incorrect gradients in MoE models for the router has been explored a lot in the literature. As noted in the related work section and other papers, many novel techniques have been developed to get a more precise gradient. Compared to those papers, the improved estimation technique is really basic as it is a simple use of the well known STE estimator technique. I believe the only novelty in this paper is that they discovered it is good enough to only do this in post-training. If not for post-training, this technique would be very prohibitive and not very likely to be used. Regardless, the STE is the motivation for most of the 'more accurate router gradient paper' as the goal is to achieve that performance without the cost of activating all experts."}, "questions": {"value": "The paper was written quite well and therefore I don’t really have any serious confusions or questions. My only question is how well this technique would work if incorporated during the pre-training stage as compared to in the post-training stage. Of course it would be much more expensive but I wonder if the authors did any experiments regarding this. I do not expect them to do such an experiment but would want to know if they did or what their thoughts are."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n2M5C0txIa", "forum": "4HGIIekCx3", "replyto": "4HGIIekCx3", "signatures": ["ICLR.cc/2026/Conference/Submission22322/Reviewer_4vjf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22322/Reviewer_4vjf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22322/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761165828955, "cdate": 1761165828955, "tmdate": 1762942169829, "mdate": 1762942169829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors address the non-differentiability of Top-K routing in Mixture-of-Experts models. The paper proposes DenseMixer, using a straight-through estimator. DenseMixer computes hard TopK selections in the forward pass, then overrides the TopK backward pass so that the gradients flow as if the TopK was replaced with the identity function. This enables gradients to propagate through all expert outputs. The paper shows consistent empirical gains across several MoE scales, datasets, and fine-tuning methods (full and LoRA), and the improvements justify the FLOP overhead and run-time costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method is practical and simple to implement, and it preserves model behavior at inference.\n\nThe empirical evidence is very strong. The DenseMixer approach outperforms standard MoE across multiple model scales and datasets.\n\nThe authors provide an efficiency analysis that justifies the increase in computational overhead relative to performance improvements.\n\nThe method is compatible with other fine-tuning tricks such as normalized TopK and LoRA for parameter efficient training."}, "weaknesses": {"value": "The experiments lack various settings of different K and N to demonstrate how the method is affected by sparsity.\n\nIt would be convincing to see how the memory and computational overhead scales as a function of model size, e.g. to confirm that the overhead is not increasing as the model becomes larger."}, "questions": {"value": "Can you provide a small-scale experiment where you compute a numerical finite-difference estimate (or autograd) of the router gradient (or an alternative high-fidelity gradient proxy) and compare the bias/variance of standard MoE vs. STE/DenseMixer vs. dense model? This would make the claim about improved gradient concrete.\n\nHow does the performance of DenseMixer depend on the sparsity (K)?\n\nHow sensitive are the gains to the amount of post-training data e.g. do benefits saturate quickly?\n\nAre there any training stability issues when enabling a dense forward pass, provided the model is pretrained with sparse forward passes and the new activations are out of distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QLaZDbfrll", "forum": "4HGIIekCx3", "replyto": "4HGIIekCx3", "signatures": ["ICLR.cc/2026/Conference/Submission22322/Reviewer_VXEj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22322/Reviewer_VXEj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22322/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869799915, "cdate": 1761869799915, "tmdate": 1762942169562, "mdate": 1762942169562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Rebuttal by the Authors"}, "comment": {"value": "We thank all reviewers for their constructive feedback. Several comments across reviewers concern (1) **comparisons with additional baselines** and (2) **hyperparameter search rigor**. We address these common questions here.\n\n---\n\n**1. Additional Baseline Comparisons**\n\nMultiple reviewers suggested baselines like **DefaultMoE**, **SparseMixer**, and **ReMoE**. Given time limit, we compare them using Qwen1.5-MoE (14B) and DeepSeek-v2-Lite (16B).\n\n- **`Results on Qwen1.5-MoE`**\n    |Method|GSM|MBPP|HumanEval|Intent|Law|Sum.|Trans.|avg|\n|---|---|---|---|---|---|---|---|---|\n|Base|38.69|38.84|32.31|16.83|18.20|28.29|16.53|27.10|\n|Frozen|53.37|35.20|37.10|82.20|33.01|38.29|32.75|44.56|\n|Conv.|53.42|34.60|36.43|81.80|29.25|37.80|33.02|43.76|\n|**DenseMixer**|**55.16**|**35.40**|**39.68**|**83.40**|**33.83**|**40.56**|**33.90**|**45.99**|\n|DefaultMoE*|51.00|33.20|35.33|80.60|31.20|38.40|24.00|41.96|\n|SparseMixer*|1.30|0.00|3.90|3.80|3.40|2.10|3.50|2.57|\n|ReMoE*|46.30|33.00|36.80|60.80|25.50|25.80|16.99|35.03|\n\n    DenseMixer consistently outperforms these baselines due to two key factors: \n\n    (1) **Precision (vs. DefaultMoE):** It relies on historical approximations for inactive experts, while DenseMixer computes exact outputs. \n\n    (2) **Compatibility (vs. ReMoE/SparseMixer):** These methods require model to be pretrained in the same way. Applying them *post-hoc* to standard TopK models creates structural misalignment, whereas DenseMixer is compatible with existing TopK mechanisms.\n\n- **`Results on MiniCPM-8x2B`**\n\n    As noted in our paper (L438-441), SparseMixer is designed for small K and specific pre-training regimes. To validate our implementation, we test with **MiniCPM-8x2B** (K=2) and SparseMixer showed gains in this low-$K$ setting.\n\n    |Method|GSM|MBPP|HumanEval|Intent|Law|Sum.|Trans.|avg|\n    |---|---|---|---|---|---|---|---|---|\n    |Base|60.10|39.20|52.30|17.60|19.00|26.10|18.10|33.20|\n    |Conv.|53.40|40.20|43.20|70.80|24.20|39.20|17.20|41.17|\n    |DenseMixer|56.10|40.60|44.50|72.40|29.70|41.80|17.40|43.21|\n    |SparseMixer*|42.20|34.68|40.82|63.40|23.30|29.70|16.80|35.08|\n\n\n\n- **`Results on Deepseek v2 lite (16B)`**\n\n    Due to time limit, we are only able to get the following results on **Deepseek v2 lite**.\n\n    |Method|GSM|MBPP|HumanEval|Intent|Law|Sum.|Trans.|avg|\n    |---|---|---|---|---|---|---|---|---|\n    |Base|19.00|43.00|27.44|3.00|14.90|16.50|16.20|20.01|\n    |Conv.|48.50|46.00|31.71|81.80|29.70|43.00|24.30|43.57|\n    |DenseMixer|51.50|47.00|32.32|82.40|32.10|45.80|25.36|45.21|\n    |DefaultMoE*|38.64|39.60|30.10|61.40|31.80|40.90|18.70|37.31|\n\n---\n\n**2. Hyperparameter Search Clarification**\n\nThe search ranges and selected optimal values (which fall inside the ranges) are detailed below:\n\n|Model|Method|BS|LR|\n|---|---|---|---|\n|Qwen1.5-MoE|full|{32,64,128}|{5e-7,1e-6,2e-6,5e-6,1e-5,2e-5,3e-5,4e-5}|\n| |lora|{32,64,128}|{5e-5,1e-4,2e-4,3e-4,4e-4,5e-4,6e-4,8e-4}|\n|OLMoE|full|{64,128,256,512}|{5e-7,1e-6,2e-6,5e-6,1e-5,2e-5,3e-5,4e-5}|\n| |lora|{128,256,512}|{5e-5,1e-4,2e-4,3e-4,4e-4,5e-4,6e-4,8e-4}|\n|DeepseekV2-Lite|full|{32,64,128}|{5e-7,1e-6,2e-6,5e-6,1e-5,2e-5,3e-5,4e-5}|\n| |lora|{32,64,128}|{5e-7,1e-6,2e-6,5e-6,1e-5,2e-5,3e-5,4e-5}|\n|Qwen3-30B-A3B|full|{8,16,32}|{5e-6,1e-5,2e-5,3e-5,4e-5,5e-5}|\n\n---\n\n**3. Efficiency Considerations**\n\n**Theoretical FLOPs**\n\nWhile the FFN forward cost scales linearly with $E$, the **total** training overhead is sub-linear. \nThe total training FLOPs can be decomposed as:\n\n$Total FLOPs=Forward_{Attn} + `Forward_{FFN}`+ Backward$\n\n- In standard training, $Backward \\approx 2 \\times Forward$. DenseMixer only increases `$Forward_{FFN}$`. So the total training FLOPs does not grow linearly with expert number.\n- Given smaller post-training datasets, the absolute overhead is marginal. For instance, on Qwen1.5-MoE, training time increased by only ~2 minutes (22-24 min), representing a highly favorable trade-off for the consistent performance gains.\n\n**Wall-clock Time**\n\nWe use Qwen1.5-MoE with LLaMA-factory library and 4xH200 GPUs. As expected, DefaultMoE is slightly faster due to reduced FLOPs. However, for post-training, DenseMixer's absolute overhead is marginal—often just a few minutes—which we believe is a worthy trade-off for the superior performance.\n\n|Dataset|Conv.|DenseMixer|DefaultMoE|\n|---|---|---|---|\n|Intent (7K)|22min|24min|22.7min|\n|Law (1K)|8.5min|9.5min|8.9min|\n|Summary (19K)|1.2h|1.4h|1.28h|\n|Translation (11K)|39min|45min|42min|\n\n---\n\n**4. Summary**\n\nAcross all added experiments and analyses, DenseMixer demonstrates:\n\n- **Superior accuracy** to DefaultMoE, ReMoE, and SparseMixer on models using mainstream Top-K routing,\n- **Compatibility** with existing MoE checkpoints (no pretraining modifications),\n- **Stable performance** under standard hyperparameter searches, and\n- **Acceptable efficiency trade-offs** for the post-training setting.\n\nWe thank the reviewers again and hope the new results clarify DenseMixer’s empirical and practical advantages."}}, "id": "IdlmTjr8tL", "forum": "4HGIIekCx3", "replyto": "4HGIIekCx3", "signatures": ["ICLR.cc/2026/Conference/Submission22322/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22322/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22322/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763723706248, "cdate": 1763723706248, "tmdate": 1763723706248, "mdate": 1763723706248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of post-training MoE models, where the non-differentiable nature of the top-k routing mechanism hinders effective gradient-based optimization of the router. The authors propose *DenseMixer*, a technique that applies the well-established Straight-Through Estimator (STE) to create a more precise, dense gradient signal for the router parameters during the backward pass. This is achieved by performing an additional forward pass on inactive experts during training, trading a modest increase in computation for improved performance. The method is evaluated on a diverse set of modern MoE models and consistently outperforms standard post-training baselines like conventional training  and freezing the router, all while maintaining inference-time efficiency"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clearly written and easy to follow. The main contributions are clear and backed up with experimental results.\n- The authors have considered relevant datasets and benchmarks, the proposed method is well evaluated. Also, the post-training setup is relevant to the practitioners.\n- The paper explicitly mentions the computational overhead introduced by DenseMixer and provides detailed wall-clock time measurements."}, "weaknesses": {"value": "**Limited Novelty:** The primary concern is that the core technical contribution is the application of the Straight-Through Estimator (STE), a well-known technique, to MoE routers. While the authors effectively demonstrate its utility for MoE post-training, the paper does not introduce a new fundamental algorithm. The authors should reframe their contribution to be more precise. Simply showing post training would benefit from the well-established straight-through router is a not a sufficient contribution.\n\n**Insufficient Comparison to Other Differentiable Routing Methods:** The paper’s experimental baselines are limited to simple heuristics (freezing the router) or the standard training method. It fails to compare against other sophisticated methods that have been explicitly designed to solve the same non-differentiable routing problem. The related work section mentions methods like SparseMixer and ReMoE, but these are not included as experimental baselines. The authors argue that these methods are not suitable for the post-training context or for large *K* values, but this claim is not empirically substantiated. A direct comparison is necessary to properly situate DenseMixer in the literature and validate its superiority\n\n**Hyperparameter Sensitivity and Tuning Overhead:** The paper states that learning rate and batch size were selected via grid search, and Appendix B only provides wide ranges for these hyperparameters. This undermines the \"plug-and-play\" claim, as it implies that significant tuning may be required to achieve the reported results, introducing a hidden computational cost. To substantiate the method's robustness, the authors should include a *sensitivity analysis* showing how performance varies with changes in key hyperparameters.\n\n**Justification of Cost-Benefit Trade-off:** While the overhead is well-documented, the performance gains, although consistent, are sometimes modest. For example, on several benchmarks, DenseMixer provides a 2-4% absolute improvement over conventional training in exchange for a 15-30% increase in training time"}, "questions": {"value": "- A few more implementation details could have been given. For example, what is the number of tokens used for answer generation? What exactly avg@N means in Table 4? I assume the accuracies were averaged over N runs but it would be nice to have it clearly mentioned in the text. Similarly, in Appendix B, some of hyperparameter values are given in a range (e.g., learning rate from 1x10^-6 to 3x10^-5). While these are minor details, it is very helpful to know all the hyperparameters. \n\n- From the paragraph \"Evaluation Setup\" at the end of Section 3, it follows that batch size and learning rate were chosen based on the performance of the method. How robust is the model's improvement under changing LR and BS values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rSvNYNrgEo", "forum": "4HGIIekCx3", "replyto": "4HGIIekCx3", "signatures": ["ICLR.cc/2026/Conference/Submission22322/Reviewer_7J9M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22322/Reviewer_7J9M"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22322/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939662982, "cdate": 1761939662982, "tmdate": 1762942169367, "mdate": 1762942169367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to improve the training of mixture-of-experts (MoE) language models by leveraging a dense forward pass to obtain the outputs of each expert while keeping the backward pass sparse. The key idea is to trade off computation during the “forward pass” for a dense router gradient during the backward pass. In their experiments, the authors evaluate their method against select baselines for supervised fine-tuning. They find that their approach yields consistent performance improvement across a range of benchmarks compared to the baselines they selected. They also measure computational and memory overhead, which is appreciated."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Practical Technique: The method is straightforward and is therefore much more likely to be adopted by practitioners and used for large-scale training.\n\n- Reasonable Model Scales and tasks: They tested on MoEs that are large enough to be realistic for practical use on practical datasets and benchmarks, so the results are relevant to real-world models.\n\n- Clear Writing: The paper is well-written and the authors explain their method and results clearly, making it easy to follow.\n\n- Hyperparameter search: The authors claim to have conducted a hyperparameter grid search on lines 308-309. This reassures me that the paper is likely to report the best possible performance of tested methods. \n\n- Reasonable evaluation of memory and computational overhead: In tables 6 and 7, the authors provide a reasonable evaluation of the computational and memory overhead of their method."}, "weaknesses": {"value": "- My main concern is that the authors do not compare to a clearly relevant and published baseline from the literature (e.g., DefaultMoE). As the authors mention on line 448\n\n> DefaultMoE [1] shares a similar philosophy by maintaining sparse training while providing dense gradients through substituting inactive expert outputs with exponential moving averages of previously computed expert values in the same batch. \n\nThe authors seem to provide two reasons for not comparing (Lines 450-456):\n\n    1. Equating the performance gains of DefaultMoE during pre-training with the performance gains of DenseMixer during post-training, although the two are not directly comparable.\n\n    2. Stating that DefaultMoE focuses on pre-training.\n\nI disagree with the authors as their claim (1) is made between quantities that cannot be compared: there are too many confounding factors between pre- and post-training, which could affect the claimed comparison. I also disagree that (2) is a good reason not to compare the methods since DefaultMoE can be trivially applied to post-training and will scale better in FLOP-overhead as the number of experts E is increased than DenseMixer. **Ideally, the authors should include a wall-clock and per-step comparison to DefaulMoE.**  Other relevant baselines addressing the same problem that are not compared to include [2,3], but I believe that the most relevant is [1].\n\n\n- Missing limitation: poor scaling with the number of experts (E). The proposed method scales linearly in FLOP-overhead relative to conventional or published techniques from the literature that tackle the same problem (DefaultMoE [1,2]) as the number of experts (E) is increased. This is essential for post-training the largest models (e.g., DeepSeekV3 256 total experts and KimiK2 384 total experts). Ideally, the scaling should be reported in the evaluation of computational overhead.\n\n- Misleading claim in the abstract. The following sentence makes me believe the authors evaluate their technique in a pre-training setting while this is not the case:\n\n>Extensive experiments demonstrate that DenseMixer consistently outperforms conventional approaches across MoE scales (7B–30B), architectures (with and without shared experts), pre-training regimes (from scratch and up-cycling), and post-training data types (instruction tuning and long chain-of-thought).\n\nI believe it would be clearer if the authors modified this to indicate that Dense Mixer can fine-tune models pre-trained in these ways.\n\n- Although the paper states that it performed a grid search over hyperparameters, this could be trivial if the authors were to select a grid of two values. Could the authors confirm that the hyperparameters selected were interior points of the set of values tried? \n\n\n[1][Dense Backpropagation Improves Training for Sparse Mixture-of-Experts; NeurIPS 2025]\n\n[2][Grinmoe: Gradient-informed mixture-of-experts.]\n\n[3][Dense training, sparse inference: Rethinking training of mixture-of-experts\nlanguage models.]\n\n\n**Adding a comparison to defaultMoE, confirming the validity of your hyperparameter search, and adding the overhead scaling of the method would cause me to raise my score.**"}, "questions": {"value": "**Typos found:**\n- Line 179 straightforward → straight-through"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zeci27mrvM", "forum": "4HGIIekCx3", "replyto": "4HGIIekCx3", "signatures": ["ICLR.cc/2026/Conference/Submission22322/Reviewer_WUwb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22322/Reviewer_WUwb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22322/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762359719086, "cdate": 1762359719086, "tmdate": 1762942169030, "mdate": 1762942169030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}