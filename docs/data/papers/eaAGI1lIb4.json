{"id": "eaAGI1lIb4", "number": 10066, "cdate": 1758159508914, "mdate": 1759897676761, "content": {"title": "XIL: Cross-Expanding Incremental Learning", "abstract": "Class-Incremental Learning (CIL) traditionally assumes that all tasks share a similar domain distribution, limiting its applicability in real-world scenarios where data arrive from evolving environments. \nWe introduce a new problem setting, Cross-Expanding Incremental Learning (XIL), which extends CIL by requiring models to handle class-incremental data across distinct domains and to expand class-domain associations bidirectionally.\nIn this setting, new classes should be integrated into previously seen domains, while earlier classes are extended to newly encountered ones, a capability we refer to as bidirectional domain transferability (BiDoT).\nTo address XIL, we present a new framework, Semantic Expansion through Evolving Domains (XEED), which leverages domain-specialized prompts, residual-guided representation modulation, and evolving prototype embeddings to expand class semantics across previously encountered domains.\nWe further introduce the BiDoT Score, a novel metric for quantifying the degree of BiDoT.\nExtensive experiments on benchmark datasets with significant domain shifts demonstrate that XEED outperforms existing CIL baselines by a large margin in both standard accuracy and BiDoT scores, establishing a strong foundation for realistic continual learning under domain-evolving conditions.", "tldr": "", "keywords": ["Class-incremental learning", "Continual learning", "Image Classification"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5015872f1f6527a8165c41e69738c9978586adc1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Cross-Expanding Incremental Learning (XIL)‚Äîa setting where both classes and domains evolve‚Äîand proposes XEED, a rehearsal-free framework that (a) learns domain-specialized prompts, (b) modulates class features with domain residuals using a diffusion model (IP-Adapter + SDXL), and (c) performs prototype-based classification with evolving, domain-aware prototypes. Also, define the BiDoT Score to measure bidirectional domain transferability. XEED substantially improves BiDoT and accuracy on PACS, Office-31, and DomainNet versus strong CIL/prompt baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a new continual learning setting, specifically,\n\n1) Clean formulation of XIL + dedicated BiDoT metric.\n\n2) Method components are modular and leverage frozen encoder + small learned parts.\n\n3) Privacy-friendlier than exemplar replay; aligns with synthetic replay trends."}, "weaknesses": {"value": "1) The paper admits that no directly comparable XIL baselines exist and hence reuses CIL prompt-based methods (S-Prompts, CODA-P, CPrompt, etc.) as proxies. Since XIL is a new setting, the paper adapts existing CIL methods as baselines rather than comparing with purpose-built domain-evolving or generative continual learning frameworks. This makes it difficult to fully assess XEED‚Äôs relative performance or to verify whether its improvements come from the new setting or the framework design itself.\n\n2) XEED‚Äôs strongest BiDoT improvements depend critically on synthetic image generation using IP-Adapter + SDXL. The ablation study shows that removing this component causes F-BiDoT to plummet (e.g., from 65.19 ‚Üí 20.91 on PACS and 33.63 ‚Üí 4.47 on DomainNet). This dependence implies that the system‚Äôs success hinges on access to a large diffusion model and high-quality generation. The method may fail or become impractical if the computing is limited or the diffusion model struggles with domain realism.\n\n3) Limited or no experiment in common continual learning benchmarks such as CIFAR100, TinyImagenet200, ImageNetsubset100, Food, Cars, ImageNet-R/A."}, "questions": {"value": "Please refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Z15NjnUauv", "forum": "eaAGI1lIb4", "replyto": "eaAGI1lIb4", "signatures": ["ICLR.cc/2026/Conference/Submission10066/Reviewer_EJAk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10066/Reviewer_EJAk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760878881067, "cdate": 1760878881067, "tmdate": 1762921460922, "mdate": 1762921460922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents cross-expanding incremental learning where not only class changes occur but also domain shift happens at the same time. To address this problem, authors proposes Semantic Expansion through Evolving Domains (XEED)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) although it intersects with existing setting, the XIL problem is new. \n2) the proposed solution, namely XEED, performs well."}, "weaknesses": {"value": "1) the following references also deal with domain adaptation and continual learning. I suggest to review them in the paper.\n\n[1] Towards Cross-Domain Continual Learning\n[2] Cross-Domain Continual Learning via CLAMP\n\n2) please kindly explain the difference of your works with cross-domain continual learning as proposed in [1] and [2]\n\n3) I suggest to elaborate more on the real-world context of your setting. This allows readers to appreciate more on your contributions. Also, it is suggested to link directly your problem with a concrete dataset that you use. Is there any dataset that can represent your problem?\n\n4) the prompt selection mechanism is non-parametric and perhaps over-simplified.\n\n5) prompt selection accuracy should be reported.\n\n6) the domain order should affect the result. it should be detailed in the paper."}, "questions": {"value": "1) the following references also deal with domain adaptation and continual learning. I suggest to review them in the paper.\n\n[1] Towards Cross-Domain Continual Learning\n[2] Cross-Domain Continual Learning via CLAMP\n\n2) please kindly explain the difference of your works with cross-domain continual learning as proposed in [1] and [2]\n\n3) I suggest to elaborate more on the real-world context of your setting. This allows readers to appreciate more on your contributions. Also, it is suggested to link directly your problem with a concrete dataset that you use. Is there any dataset that can represent your problem?\n\n4) the prompt selection mechanism is non-parametric and perhaps over-simplified.\n\n5) prompt selection accuracy should be reported.\n\n6) the domain order should affect the result. it should be detailed in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0DC6qYYmtJ", "forum": "eaAGI1lIb4", "replyto": "eaAGI1lIb4", "signatures": ["ICLR.cc/2026/Conference/Submission10066/Reviewer_w46G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10066/Reviewer_w46G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761614909917, "cdate": 1761614909917, "tmdate": 1762921460465, "mdate": 1762921460465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Cross-Expanding Incremental Learning (XIL), a new setting of continual learning. The XIL presents the problem of existing setting of class-incremental learning (CIL), where the model learns from the data of same domains. XIL emphasizes the  capability of transferring knowledge across different domains and proposes a new settiing. A corresponding method XEED is proposed to address the XIL. XEED is a generative replay-based method which leverages a pre-trained diffusion model and CLIP to generate samples that are transferable across different domains. Domain-specific prompts and prototypes are proposed to preserve knowledge from different tasks. Several experiments are conducted to validate the effectness of XEED."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper consider a new paradigm of continual learning, where the capability of transferring across different domains are considered.\n2. This paper is well writen with good presentation and clear motivation.\n3. The technique of generating exemplars with representation modulation is noval and sound."}, "weaknesses": {"value": "1. Using the pre-trained diffusion model and CLIP. Although this paper is overall good, I have several concerns about using the pre-trained diffusion model. Since a good exemplar or sample for transferring can be generated, the knowledge related to the domains and classes is already encoded in the pre-trained model. As such, do we still need continual learning of another model? Most existing generative replay-based methods train a generative model during the continual learning, which is incremental learning process parallel to the training model. However, if the model needs another model which already has the knowledge of future data, since we already have a good one, what is the meaning of incremental learning of that model? \n2. The proposed method XEED is simple and preserving prompts and prototypes specific to distinct domains or classes has been widey studied in the community of continual learning. \n3. The capability of transferring across domains, the major challenge defined in this paper, seems mainly benefit from the generative replay. It seems that the major problem defined in this paper is solved by introducing knowledge of other models while the other parts of this method have mere innovation. One can argue that with such a replay mechamism, existing CIL methods can be easily transferred to the setting of XIL.\n4. Results on dataset specially designed for the domain and class incremental learning, e.g., CoRE50 [1]. should be included. The datasets used in the experiments are not designed for incremental learning. The CoRE50, which is designed for both the CIL and Domain-incremental learning tasks, can be a good benchmark of XIL.\n\n[1] Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous object recognition. In Conference on Robot Learning, pages 17‚Äì26, 2017."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c8RW8xIxo7", "forum": "eaAGI1lIb4", "replyto": "eaAGI1lIb4", "signatures": ["ICLR.cc/2026/Conference/Submission10066/Reviewer_gcQm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10066/Reviewer_gcQm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746414511, "cdate": 1761746414511, "tmdate": 1762921459778, "mdate": 1762921459778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Cross-Expanding Incremental Learning (XIL), a new, more realistic problem setting that extends traditional Class-Incremental Learning (CIL) by requiring models to learn new classes that arrive from distinct, evolving domains. The core challenge is achieving bidirectional domain transferability (BiDoT), where the model must generalize previously learned classes to new domains and new classes to old domains, even for combinations never seen during training. To address this, the authors propose a novel framework called Semantic Expansion through Evolving Domains (XEED), which utilizes domain-specialized prompts to adapt to different domains, a generative model to synthesize images for unseen class-domain pairs, and evolving prototypes to continuously expand the classifier's semantic space. Accompanied by a new evaluation metric, the BiDoT Score, experiments show that XEED significantly outperforms existing CIL methods, which struggle in this dynamic setting, thereby establishing a strong foundation for continual learning in real-world, non-static environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's foremost strength is its introduction of the Cross-Expanding Incremental Learning (XIL) problem, which establishes a more realistic and challenging benchmark for continual learning by forcing models to simultaneously handle new classes and evolving data domains.\n\nThe proposed XEED presents a highly innovative solution, ingeniously using a generative model to synthesize data for unseen class-domain pairs, thereby directly addressing the core challenge of bidirectional knowledge transfer."}, "weaknesses": {"value": "The paper does not sufficiently explain how the baseline methods, originally designed for the standard CIL setting, were adapted to the new multi-domain XIL setting. This is particularly concerning for methods like S-Prompts, which show drastically lower performance, raising questions about whether the comparison is entirely fair\n\nAs a paper proposing a new problem setting and a novel evaluation metric, the absence of publicly available code is a major drawback. This lack of resources makes it extremely difficult for the research community to verify the reported results, adopt the XIL benchmark, and fairly compare future methods, thus hindering follow-up research.\n\nWhile the XEED is effective, its core components: domain-specific prompts, generative replay, and prototype-based classifiers, are existing techniques."}, "questions": {"value": "NO"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6wxoH1pJQG", "forum": "eaAGI1lIb4", "replyto": "eaAGI1lIb4", "signatures": ["ICLR.cc/2026/Conference/Submission10066/Reviewer_9JFF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10066/Reviewer_9JFF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811783796, "cdate": 1761811783796, "tmdate": 1762921459459, "mdate": 1762921459459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Cross-Expanding Incremental Learning (XIL), a new setting that extends class-incremental learning to handle data from distinct and evolving domains. XIL requires models to perform bidirectional domain transferability, integrating new classes into old domains and adapting old classes to new ones. To address this challenge, the authors propose XEED, a framework that combines domain-specialized prompts, residual-guided modulation, and evolving prototypes to expand class semantics across domains. They also introduce the BiDoT Score, a metric that measures how well models generalize across unseen class‚Äìdomain combinations. Experiments on benchmark datasets with significant domain shifts show that XEED outperforms existing methods in both accuracy and BiDoT scores."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and well-structured, allowing readers to easily follow the logical flow and understand the motivation and proposed method at both conceptual and technical levels.\n\n- The empirical results convincingly demonstrate the effectiveness of the proposed approach across multiple standard benchmark datasets."}, "weaknesses": {"value": "- The **quality and fidelity of synthetic exemplars** present a potential limitation. The proposed method assumes that diffusion-based generation can effectively represent unseen domain‚Äìclass combinations. However, such generative processes may produce low-fidelity or stylistically inconsistent samples, which could introduce noise into the evolving prototype updates and distort the learned feature space. A more detailed analysis of the generation quality‚Äîeither through visual inspection, quantitative metrics such as FID or LPIPS, or ablation studies‚Äîwould help validate whether the synthetic exemplars truly enhance domain transfer rather than acting as noisy augmentations.\n\n- Another concern lies in the **high computational cost and reliance on diffusion models**. The method depends on a pre-trained diffusion generator to synthesize cross-domain exemplars, which is computationally expensive due to multiple denoising steps, high GPU memory requirements, and significant I/O overhead. While diffusion models contribute to the quality of generated samples, their use raises questions about scalability and practicality in large-scale or real-time incremental learning scenarios. A discussion of the computational budget, inference latency, and trade-offs between efficiency and performance would strengthen the paper‚Äôs experimental transparency.\n\n- Finally, the framework‚Äôs **dependence on frozen backbones and domain prompts** may restrict adaptability under severe domain shifts. The feature extractor remains fixed during prompt tuning, assuming pre-trained features are sufficiently general for new domains. In cases where the domain gap is large (e.g., transferring from natural images to sketches or depth maps), this assumption may not hold, leading to suboptimal adaptation even with domain-specific prompts. Allowing partial backbone fine-tuning or incorporating adaptive normalization could improve flexibility and robustness in such scenarios."}, "questions": {"value": "- How do the authors ensure that the diffusion-generated exemplars are of sufficient visual and semantic quality to represent unseen domain‚Äìclass combinations?\n\n- Could noisy or low-fidelity exemplars negatively affect the evolving prototype updates, and if so, how is this mitigated?\n\n- How scalable is XEED when applied to larger datasets or a greater number of incremental tasks and domains?\n\n- Why was the backbone \nùëì\nùúô\nf\nœï\n\t‚Äã\n\n kept frozen during prompt tuning, and have the authors experimented with partial fine-tuning or adapter layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G2InBT10Hg", "forum": "eaAGI1lIb4", "replyto": "eaAGI1lIb4", "signatures": ["ICLR.cc/2026/Conference/Submission10066/Reviewer_RYCp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10066/Reviewer_RYCp"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932944899, "cdate": 1761932944899, "tmdate": 1762921459185, "mdate": 1762921459185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}