{"id": "4pivvEJiCl", "number": 8990, "cdate": 1758106246516, "mdate": 1759897749631, "content": {"title": "Reconstructing KV Caches with Cross-Layer Fusion for Enhanced Transformers", "abstract": "Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths.\nAlthough Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA.\nTo understand the root cause, we investigate the information flow of keys and values of the top-layers.\nOur preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers.\nBuilding upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. \nThis fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings.\nTo further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys.\nCompared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity.\nIn experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative. We have made our Triton implementation available.", "tldr": "This paper proposes FusedKV and FusedKV-Lite. Both methods reduce KV cache memory by up to 50% while achieving lower perplexity than standard Transformers, offering a memory-efficient, high-performance architectural alternative", "keywords": ["KV Cache", "Pretraining", "LLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f0c8183caa44e6adcd37311b1e478a72af891a7e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces FusedKV and FusedKV-Lite, two methods for cross-layer KV cache reconstruction in Transformers to reduce memory footprint and prefilling cost. The approach leverages an observed asymmetry between keys and values—where higher-layer keys draw from mid-layer information, while values rely more on lower layers. Both methods halve KV cache memory and achieve 2× acceleration in prefilling while maintaining comparable perplexity to standard Transformers. Experiments across model sizes show consistent performance gains and favorable scaling behavior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The evaluation is extensive, covering multiple model scales, diverse tasks, and scaling laws. The inclusion of a scaling experiment convincingly demonstrates the effectiveness and generality of the approach.\n2. FusedKV introduces only a small architectural modification, where cross-layer fusion through linear weighting, and achieves measurable efficiency gains. The method integrates smoothly with existing architectures and remains conceptually clear."}, "weaknesses": {"value": "1.\tThe paper focuses on achieving ~2× cache and prefilling acceleration, while prior methods like YOCO already explore two-stage attention designs (prefill and decode) achieving greater long-sequence efficiency. This limits the novelty in acceleration.\n2. Table 2 shows that FusedKV-Lite already provides strong accuracy and latency trade-offs. The heavier FusedKV variant adds learnable parameters and I/O overhead but yields only marginal improvement, making its necessity unclear.\n3. Since KV cache architecture primarily affects long-sequence efficiency, the evaluation mainly on perplexity and standard benchmarks doesn’t fully validate the benefit under ultra-long input settings."}, "questions": {"value": "1. Given that FusedKV-Lite already performs well, what is the intended use case for the heavier FusedKV model?\n2. How compatible is FusedKV with GQA methods? Can they be combined effectively, and what would be the impact on speedup and accuracy?\n3. The paper notes that KV sharing typically reduces parameters in MHA settings. Does the proposed design introduce additional parameters (e.g., in FFN layers or fusion weights), and if so, what is their quantitative effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ojyxqj7Bpz", "forum": "4pivvEJiCl", "replyto": "4pivvEJiCl", "signatures": ["ICLR.cc/2026/Conference/Submission8990/Reviewer_SbV3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8990/Reviewer_SbV3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760864153554, "cdate": 1760864153554, "tmdate": 1762920720574, "mdate": 1762920720574, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a training-based KV cache compression method that approximates top-layer caches by fusing caches from bottom layers. Two options are proposed: FusedKV, which creates a weighted sum of bottom and middle-layer caches to achieve better quality at the cost of some inference overhead, and FusedKV-Lite, which directly reuses a cache from a single, lower layer. The experiments are conducted on a range of models from 300M to 4B parameters, using training budgets on the order of 100B tokens."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed method reduces KV cache memory usage by 50% without any performance drop relative to the baseline.\n\n* The observation of KV cache asymmetry appears to be novel within the compression literature."}, "weaknesses": {"value": "* The overall approach lacks conceptual novelty. The idea of cache reuse has been explored in several prior works [1, 2] in a post-training setting, where cache similarity was used as the criterion for reuse. Approaches based on linear predictors [3] have achieved 4x compression with no performance drop and up to 8x compression with only minor degradation.\n\n* The idea of a weighted fusion of previous keys and values also appeared in [4] in the context of improving gradient flow. It is possible that the improvement in validation loss observed in this work stems from the same mechanism.\n\n* The proposed approach requires costly training from scratch, whereas numerous inexpensive post-hoc techniques already exist that achieve high KV cache compression rates [7, 8] with little to no performance decline (see 1st weakness).\n\n* The most practical application for KV compression techniques is in long-context tasks, where the KV cache incurs significant memory overhead and increases inference latency. To fully demonstrate the technique's efficacy, the authors should provide results on established long-context benchmarks, such as LongBench [5] or RULER [6].\n\n* *(minor)* For the smaller models, performance on benchmarks like MMLU (25% random guess) and ARC-C (25% random guess) is only slightly above chance. Given the high noise and potential for random results, along with the cost of estimating standard deviation, the authors might consider removing these tasks for the smaller models.\n\nReferences\n\n---\n\n[1] Yang Y. et al. Kvsharer: Efficient inference via layer-wise dissimilar kv cache sharing //arXiv preprint arXiv:2410.18517. – 2024.\n\n[2] Liu A. et al. Minicache: Kv cache compression in depth dimension for large language models //Advances in Neural Information Processing Systems. – 2024. – Т. 37. – С. 139997-140031.\n\n[3] Shutova, Alina, et al. \"Cache Me If You Must: Adaptive Key-Value Quantization for Large Language Models.\" arXiv preprint arXiv:2501.19392 (2025).\n\n[4] Pagliardini M. et al. Denseformer: Enhancing information flow in transformers via depth weighted averaging //Advances in neural information processing systems. – 2024. – Т. 37. – С. 136479-136508.\n\n[5] Bai Y. et al. Longbench: A bilingual, multitask benchmark for long context understanding //arXiv preprint arXiv:2308.14508. – 2023.\n\n[6] Hsieh, Cheng-Ping, et al. \"RULER: What's the Real Context Size of Your Long-Context Language Models?.\" arXiv preprint arXiv:2404.06654 (2024).\n\n[7] Liu, Zirui, et al. \"Kivi: A tuning-free asymmetric 2bit quantization for kv cache.\" arXiv preprint arXiv:2402.02750 (2024).\n\n[8] Hooper, Coleman, et al. \"Kvquant: Towards 10 million context length llm inference with kv cache quantization.\" Advances in Neural Information Processing Systems 37 (2024): 1270-1303."}, "questions": {"value": "* How well would the proposed method perform in a post-training setting? For example, could one take a pre-trained model and simply estimate the fusion coefficients for FusedKV (or directly reuse bottom-layer caches for FusedKV-Lite) without full retraining?\n\n* Do I understand correctly that the trained models use Grouped-Query Attention (GQA) with 16 query heads and 2 KV heads? Or is the number of query heads and KV heads the same in experiments?\n \n> For evaluation, we adopt GQA with 128 query heads and 2 key–value heads as the baseline \n\n* What is the head dimension? This configuration is unusual, as large models typically have 8 or more KV heads. A setup with only 2 KV heads is more common for smaller models (e.g., those with 16 or 32 query heads)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U150fMhzRo", "forum": "4pivvEJiCl", "replyto": "4pivvEJiCl", "signatures": ["ICLR.cc/2026/Conference/Submission8990/Reviewer_7x2j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8990/Reviewer_7x2j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761381252791, "cdate": 1761381252791, "tmdate": 1762920719554, "mdate": 1762920719554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new KV cache sharing algorithm, FusedKV. By leveraging their findings on key-value reconstructions, they suggested which layers should be connected for sharing kv pairs. This ends up sharing values from bottom layers and keys from middle layers. They tested two variants, with and without learnable weights."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Drawing insights from preliminary experiment (dense fusion) seems a good research direction to build up their own methods.\n- Measure various metrics to compare methods.\n- Validation at scale (larger model sizes or larger training token numbers) seems good."}, "weaknesses": {"value": "- I feel like FusedKV is not a good approach due to its lower throughput, which comes from its higher KV IO, and marginal quality differences over FusedKV-Lite.\n- Proposed method is too heuristic. Although I personally like heuristic, simple yet effective method, I'm not sure about the generalizability of this proposed method. Using the first and middle layers as the source layer can be not optimal for other architectures (or with other modality). It seems like well-tailored, specific sharing strategy for this scenario.\n- In Figure 9, why do you think dense fusion outperforms vanilla models? Don't they (vanilla) have larger degree of freedom (i.e., larger trainable parameters) than reconstructing caches?\n- In L.413-414, isn't this typo? Rev means K^i = K^1, V^i = V^8?\n- I believe overall presentation could be enhanced, like Figure captions or visualization of results)."}, "questions": {"value": "See above Weakness parts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cdHWT6eIjb", "forum": "4pivvEJiCl", "replyto": "4pivvEJiCl", "signatures": ["ICLR.cc/2026/Conference/Submission8990/Reviewer_K5c9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8990/Reviewer_K5c9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761616032167, "cdate": 1761616032167, "tmdate": 1762920719070, "mdate": 1762920719070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FusedKV and FusedKV-Lite as techniques to reduce key-value cache memory in transformer decoder-only models. The key contribution is identifying that top-layer key-value caches can be asymmetrically reconstructed: values are predominantly derived from the bottom layer, while keys benefit from information spanning both bottom and middle layers. FusedKV learns a dimension-wise weighted fusion of caches from these two source layers, operating on post-RoPE keys to preserve relative positional information without recomputation. FusedKV-Lite offers a simpler variant that directly reuses the middle-layer keys and bottom-layer values, eliminating fusion overhead. Across model scales from 332M to 4B parameters, both methods reduce cache memory by 50 percent while achieving lower validation perplexity than standard transformers, establishing an effective cross-layer cache sharing paradigm."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The asymmetric key-value sharing principle is a clear, well-motivated insight grounded in empirical analysis (Figure 2), distinguishing this work from prior indiscriminate cross-layer approaches.\n- Solid experimental validation across multiple model sizes (332M to 4B) with consistent gains over baselines. Validation loss curves (Figure 6) demonstrate stability across training.\n- Practical design: operating on post-RoPE keys elegantly preserves positional information without computational overhead.\n- Comprehensive ablation studies (Figure 8) clearly demonstrate the importance of asymmetry direction and learnable weights.\n- Efficient implementation details provided, including Triton kernels and complexity analysis (Table 1)."}, "weaknesses": {"value": "- Missing fundamental architectural comparison: This paper does not compare against MLA, which reduces cache dimensionality by design rather than reconstructing across layers. For practitioners optimizing cache size, adopting MLA is a more principled solution than reconstructing caches across standard multi-head attention layers. Direct comparison (accuracy, memory, inference speed) would clarify when cross-layer fusion is preferable to architectural redesign -- and particularly the potential of the combination (do the gains stack?). After all, shouldn't we start improving on top of the best known methods?\n- Limited scope of baselines: Missing comparisons to recent token-level or head-level reduction methods such as SpindleKV (2025), Value Residual Learning (2025), and Mixture-of-Recursions (2025), which operate on complementary dimensions.​\n- Evaluation restricted to standard language modeling and GLUE-like benchmarks. The most KV-cache intensive scenarios are missing from the evaluation: a) truly long-sequence tasks (>100k tokens); b) multi-modal or encoder-decoder architectures.\n- No analysis/discussion of training cost or convergence properties relative to vanilla training. Does learnable fusion increase training time or memory significantly?\n- Potential interaction with other efficiency techniques unclear. Can this paper's method be orthogonally combined with quantization, pruning, or token merging for further gains?"}, "questions": {"value": "- How does this paper's method compare quantitatively against MLA in terms of accuracy, cache memory footprint, and end-to-end inference speed? Since MLA reduces cache size structurally rather than through reconstruction, this comparison would establish whether cross-layer fusion offers advantages over fundamental architectural changes, or whether practitioners should adopt MLA instead. What are the performance-cost tradeoffs?\n- How does this paper's approach interact with token-level or head-level cache reduction? If combined with SpindleKV or head-importance pruning, could further memory savings be achieved without degradation? This clarifies orthogonality versus redundancy with complementary approaches.\n- Does performance degrade significantly on extremely long sequences (>100k tokens) or knowledge-intensive retrieval tasks? Stress testing on long-context QA would strengthen robustness claims across diverse inference scenarios.\n- What is the training overhead compared to vanilla training? How do learnable fusion weights affect gradient flow, training stability, or convergence speed? Reporting training time, memory, and loss curves would clarify practical costs.\n- Can the asymmetric sharing principle generalize to encoder-decoder, mixture-of-experts, or other architectural variants? Current evaluation is limited to dense decoder-only models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "crZahH43Ri", "forum": "4pivvEJiCl", "replyto": "4pivvEJiCl", "signatures": ["ICLR.cc/2026/Conference/Submission8990/Reviewer_j1dt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8990/Reviewer_j1dt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125603727, "cdate": 1762125603727, "tmdate": 1762920718641, "mdate": 1762920718641, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}