{"id": "2i27nl0VPc", "number": 8763, "cdate": 1758097433320, "mdate": 1762945612358, "content": {"title": "Occluded 3D Object Reconstruction via Masked Multi-view Volumetric Transformer", "abstract": "Recent advancements in image-to-3D reconstruction and generation have yielded remarkable progress. \nHowever, applying these methods to occluded objects in cluttered scenes remains challenging due to the incomplete information in occluded areas. \nTo tackle this issue, we present a feed-forward method for reconstructing 3D occluded shapes using a data-driven approach. \nOur method utilizes a large-scale dataset of cluttered scenes and incorporates multi-view occlusion-aware 3D reconstruction through a Transformer architecture that draws inspiration from masked autoencoders. \nOur model, \\textit{Masked Multi-view Volumetric Transformer}, utilizes global reasoning from an arbitrary number of multi-view 2D image information and cross-attention between 3D-lifted obstacle mask volumes and volumetric latents, enabling the model to predict information for occluded regions accurately.\nFurthermore, we have created a synthetic cluttered scene dataset comprising $\\sim$30,000 scenes with Objaverse objects, designed to illustrate various occlusion scenarios. \nOur approach surpasses previous methods in predicting complete shapes from occluded images of unseen objects, achieving completed mesh extraction in five seconds.", "tldr": "", "keywords": ["Occlusion", "3D Reconstruction", "Amodal Completion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/daa5ec5dc2c72ae4041d12781deb316017b30710.pdf", "supplementary_material": "/attachment/2717796760a3309e1a4a28951d2e759dd3bb4967.zip"}, "replies": [{"content": {"summary": {"value": "## Summary\n\nThis paper addresses the problem of multi-view amodal 3D reconstruction, which takes multi-view images with camera poses as input and outputs parameters of 2D Gaussians—including RGB, depth, and alpha maps. The proposed pipeline builds upon LaRa, first constructing 3D feature volumes by lifting 2D DINO features into a canonical volume, and then applying a volumetric transformer to reconstruct a Gaussian volume. The key contribution lies in handling incomplete input images: the authors introduce a MAE-inspired module for completing DINO features and integrating both the completed volumes and volumetric occlusion masks into the 3D generation process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "## Strengths\n\n1. This paper presents a novel pipeline for multi-view amodal completion, formulating the problem of the generation of Gaussian volumes and designing a MAE-like latent reconstruction method for feature completion. \n2. The authors also contribute a new large-scale training dataset comprising 29,853 occluded object scenes built from Objaverse, which provides a valuable foundation for future research. \n3. The quantitative results in Table 1 are highly promising and demonstrate the effectiveness of the proposed approach."}, "weaknesses": {"value": "## Weaknesses\n1. Fairness of Comparisons: To the best of my knowledge, methods such as Amodal3R and other image-based occluded object generation approaches typically operate on unposed images. Therefore, a direct comparison with these methods may not be entirely fair.\n\n2. Role of the DINO-MAE Module: Although the authors include ablation studies with and without the 2D reconstruction module, it remains unclear whether the model could directly infer 3D Gaussian volumes from incomplete volume features and volumetric masks alone—an approach that has been validated in works like Amodal3R. Further analysis on this point would help clarify the necessity and contribution of the 2D reconstruction component."}, "questions": {"value": "The author should provides a more fair comparisons with the baselines. I am open to adjust the rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "i5ElBX5ed0", "forum": "2i27nl0VPc", "replyto": "2i27nl0VPc", "signatures": ["ICLR.cc/2026/Conference/Submission8763/Reviewer_W1bq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8763/Reviewer_W1bq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477919512, "cdate": 1761477919512, "tmdate": 1762920546204, "mdate": 1762920546204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Thank you for your reviews. Based on the comments, we have decided to withdraw our submission. We truly appreciate the constructive suggestions, and we will incorporate them to improve the work for future publication."}}, "id": "dAEJ7vcEMz", "forum": "2i27nl0VPc", "replyto": "2i27nl0VPc", "signatures": ["ICLR.cc/2026/Conference/Submission8763/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8763/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762945609694, "cdate": 1762945609694, "tmdate": 1762945609694, "mdate": 1762945609694, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel method for reconstructing 3D objects from multi-view images under occlusion. The proposed approach, *Masked Multi-view Volumetric Transformer (MMVT)*, leverages a masked autoencoder-inspired architecture to reconstruct missing latent representations from occluded regions, ensuring multi-view consistency. The authors also contribute a large-scale synthetic dataset of cluttered scenes for training and evaluation. Experimental results demonstrate that the method outperforms existing baselines in both quantitative metrics and inference speed, completing 3D reconstructions in approximately five seconds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed MMVT model effectively handles occluded object reconstruction by integrating multi-view reasoning and volumetric obstacle masks, achieving state-of-the-art performance in both 3D reconstruction and novel view synthesis.\n2. The method is more efficient, with inference times faster than most competing approaches (e.g., inpainting-based or diffusion-based methods).\n3. The paper is overall well-written and should be easy to follow."}, "weaknesses": {"value": "1. While quantitative comparisons with methods like pix2gestalt and Amodal3R are provided, the lack of corresponding qualitative visualizations makes it difficult to fully assess the visual superiority of the proposed method. Including more visual comparisons, especially rotating videos of reconstructed objects, would strengthen the results.\n2. The real-world qualitative results in Fig. 6 appear less convincing, raising concerns about the model’s generalization to real-world scenes. Results on established real-world datasets such as ScanNet++ or Mip-NeRF 360 would help validate its practical applicability."}, "questions": {"value": "In the paper, the obstacle mask is defined as the union of all instance masks excluding the target object. However, this mask may include regions that do not actually occlude the target. Would it be more straightforward and effective to treat the entire image area outside the target object mask as the obstacle mask? This could simplify the input conditions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xpjcz7NrI0", "forum": "2i27nl0VPc", "replyto": "2i27nl0VPc", "signatures": ["ICLR.cc/2026/Conference/Submission8763/Reviewer_4j3T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8763/Reviewer_4j3T"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803861178, "cdate": 1761803861178, "tmdate": 1762920545710, "mdate": 1762920545710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose to solve a problem of reconstructing occluded 3D shapes using a novel masked multi-view volumetric transformer method which reconstructs missing image features token at masked patches in a feed-forward manner.\n\nSpecifically, the authors use DINOv3 image features + plucker ray modulation. Feedforward reconstruction has MAE-like architecture with global + frame-level self-attention over concatenated multi-view tokens."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Author's approach of patching final image tokens with unmasked tokens is, to my knowledge, novel\n- This method of getting image feature token, in combination of using 3D obstacle volume mask with group cross-attention is novel, to my knowledge\n- According to quantitative evaluation, the method appears to be producing SOTA results on author's test dataset"}, "weaknesses": {"value": "1. The main constribution appears to be in handling occluded images and incorporating occlusion masks in generating image tokens, while existing approach (pre-trained LaRa) is used afterwards for feed-forward reconstruction. I'm doubtful of the value of this contribution alone, yet willing to reconsider if other reviewers think otherwise.\n2. The choice of patching image tokens with unmasked tokens is not ablated.\n3. Albeit strong quantitative results (Table 1), visual presentation is very unconvincing (e.g. Fig 1, Fig 6) and is clearly behind state-of-the-art in 3D reconstruction\n4. The custom dataset that authors produce cannot be considered a solid contribution in it's current form. Additionally, authors make no promises to release the dataset for community\n5. Authors select weak method (LaRa) for testing their inpainting methods"}, "questions": {"value": "1. Please clarify what are the main contributions of the paper, apart from incorporating occlusion masks while getting image tokens for feed-forward reconstruction.\n2. Please clarify why LaRa was selected as 3D reconstruction pipeline with inpainting methods? I suggest using one of the most recent methods (Trellis / Step1X-3D / Hunyuan3D)\n3. Please provide more qualitative evaluations supporting statements in Table 1. Specifically, visual comparison b/w your method, inpainting methods with stronger baseline (see 1.), and Amodal3R. For Amodal3R specifically, the visuals in the paper are significantly stronger than the work in question, so I'd appreciate to hear more details to explain this discrepancy.\n4. Please provide ablation on patching unmasked image tokens (Fig. 2), it's not clear how helpful this decision is.\n5. L70 - which artifacts? Could you provide examples / elaborate?\n2. Please clarify whether you plan to release the code and dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jv9Ld6PsOB", "forum": "2i27nl0VPc", "replyto": "2i27nl0VPc", "signatures": ["ICLR.cc/2026/Conference/Submission8763/Reviewer_vprQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8763/Reviewer_vprQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870994188, "cdate": 1761870994188, "tmdate": 1762920545087, "mdate": 1762920545087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a transformer-based model for reconstructing partially occluded 3D shape. Goal is fast feedforward inference with multi-view consistency. It consists of two parts: a feature extractor and a shape decoder. The feature extractor fuses different views via attention, and each view is processed with the frozen DINOv3 model. Since instance segmentation is required as input, we know which patch/token is an occlude that we are not reconstructing, so the obstacle masks can be lifted into 3D just like the image features. The shape decoder then produces 3D shape from these feature and obstacle mask volumes.  Shape representation is gaussian splats.\n\nThe feature extractor is built on top of a MAE but made multi-view. View information is injected as pluecker ray directions via adaptive layer norm. The decoder is a pre-trained LaRa. The loss is standard re-rendering loss along with SSIM and perceptual metrics. Regularization is depth distortion and normal consistency."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Feedforward, multi-view-consistent 3D reconstruction is a very practical and important problem to solve. The paper tackles exactly this. Good topic.\n\nThe quality achieved on non-standard shape looks good, and quantitative results show that the model is strong and fast."}, "weaknesses": {"value": "The occlusion setup feels quite artificial to me – the procedurally generated scenes are nowhere close to what occlusions look like in real life. I understand how training can be hard without ground truth on real occluded scenes, but at least the trained model should be put to test on such scenes. However, the current test cases are similar-looking synthetic scenes (objects being scanned and real is not helping much here).\n\nSimilarly, requiring instance seg masks is another major drawback, limiting the practicality. Either the model can work with a quick scribble defining the object of interest, or we should be after a non-object centric model, if we are for practical approaches that can actually be useful. \n\nPresentation needs more work. Multiple places with broken contents or grammar. Examples: broken caption in L170-173. Maybe due to this, I found this paper hard to follow."}, "questions": {"value": "What do the results on real scenes with occlusion look like? \n\nCan the model work with a quick scribble instead of pixel-aligned ins seg masks?\n\nDoes the ability to complete occluded parts come *more* from geometry priors learned from data or multi-view attention design? A or B? Why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FY7UhS0yFC", "forum": "2i27nl0VPc", "replyto": "2i27nl0VPc", "signatures": ["ICLR.cc/2026/Conference/Submission8763/Reviewer_e5s5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8763/Reviewer_e5s5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066492513, "cdate": 1762066492513, "tmdate": 1762920544626, "mdate": 1762920544626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}