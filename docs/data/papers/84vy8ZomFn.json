{"id": "84vy8ZomFn", "number": 68, "cdate": 1756728387747, "mdate": 1763317424907, "content": {"title": "Breaking Scale Anchoring: Frequency Representation Learning for Accurate High-Resolution Inference from Low-Resolution Training", "abstract": "Zero-Shot Super-Resolution Spatiotemporal Forecasting requires a deep learning model to be trained on low-resolution data and deployed for inference on high-resolution. Existing studies consider **maintaining** similar error across different resolutions as indicative of successful multi-resolution generalization performance. However, deep learning models serving as alternatives to numerical solvers should **reduce** error as resolution increases. The fundamental limitation is, the upper bound of physical law frequencies that low-resolution data can represent is constrained by its Nyquist frequency, making it difficult for models to process signals containing unseen frequency components during high-resolution inference. *This results in errors being anchored at low resolution, incorrectly interpreted as successful generalization.* We define this fundamental phenomenon as a new problem distinct from existing issues: **Scale Anchoring**. Therefore, we propose architecture-agnostic Frequency Representation Learning. It alleviates Scale Anchoring through resolution-aligned frequency representations and spectral consistency training: on grids with higher Nyquist frequencies, the frequency response in high-frequency bands of FRL-enhanced variants is more stable. This allows errors to decrease with resolution and significantly outperform baselines within our task and resolution range, while incurring only modest computational overhead.", "tldr": "", "keywords": ["Scale Anchoring", "Zero-Shot Super-Resolution", "Spatiotemporal Forecasting", "Frequency Representation"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/779378f4f98c96fe9df884b49fe8b2a666f722c4.pdf", "supplementary_material": "/attachment/5c15803f970e08058eb5c6c9ec1fd16dadd86cb9.zip"}, "replies": [{"content": {"summary": {"value": "This paper involves downsampling the data to different resolutions, then using a special Nyquist-normalized frequency position encoding, along with a frequency consistency loss, to achieve super-resolution. Results on two tasks demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a new approach based on the frequency domain to address the limitations in current Zero-Shot Super-Resolution Spatiotemporal Forecasting methods. It explains the Nyquist frequency limitation of low-resolution data and offers a new perspective on why current methods struggle with high-quality high-resolution predictions.\n2. The paper highlights the flaws in traditional multi-resolution techniques, especially the problems of scale anchoring and spectral bias. It proposes Nyquist-normalized frequency representation and frequency-aware loss to solve these issues, which improves the model's performance.\n3.The proposed method demonstrates strong performance in weather forecasting and fluid simulation tasks, highlighting the effectiveness of the approach and its potential for practical applications.\n4. The proposed method is validated on multiple network architectures, including CNN, Transformer, Mamba, and GNN, demonstrating its versatility and effectiveness across different networks."}, "weaknesses": {"value": "1. The method shares similarities with ZSSR and other Deep Internal Learning approaches, making it unclear what is fundamentally new beyond the frequency-domain reinterpretation.\n2. The experiments are limited to spatiotemporal forecasting; applying the method to standard image super-resolution datasets (like Set5, Set14, BSD100) would provide more comprehensive comparisons.\n3. The frequency consistency loss seems similar to existing frequency-domain losses, and its contribution may be limited. Additional ablation studies comparing it with other methods could clarify its impact."}, "questions": {"value": "In the experiments, the method is tested at only a few discrete scales. Could you test the method over a wider range of scales by adjusting the scale in fixed steps (e.g., 2x, 3x, ... 64x) and report the results? Plotting a performance curve with these varying scales would help demonstrate the method's effectiveness and robustness across a broader range of resolutions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "This paper has no ethical concerns."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HgHvQaqgS0", "forum": "84vy8ZomFn", "replyto": "84vy8ZomFn", "signatures": ["ICLR.cc/2026/Conference/Submission68/Reviewer_Kovs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission68/Reviewer_Kovs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission68/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760975014730, "cdate": 1760975014730, "tmdate": 1762915446096, "mdate": 1762915446096, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors identify a problem when performing zero-shot super resolution, when a model is trained on low‐resolution data and then deployed on higher‐resolution data, the error does not decrease despite more fine‐grained input. They argue this is because low‐resolution training data limits the maximum representable frequency (via Nyquist), so the model never “learns” higher‐frequency components. They propose a method called Frequency Representation Learning (FRL) that (a) uses resolution‐aligned frequency representations (normalized to account for the Nyquist at each resolution) and (b) incorporates a spectral‐consistency training objective across multiple resolution levels. They show experiments on spatio‐temporal forecasting tasks (zero-shot super‐resolution forecasting) where their FRL approach results in error decreasing with increasing resolution (unlike baselines) and improved frequency response stability in higher‐frequency bands."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The identification of the “scale anchoring” problem is interesting and relevant in several fields, such as neural operators. \n\n2. The proposed method (FRL) is architecture‐agnostic in principle.\n\n3. The experiments show that the inference error can be even lower for zero-shot super resolution. Existing works usually consider the same error as a success."}, "weaknesses": {"value": "Some of these weaknesses below fall somewhere between questions and weaknesses, so I’ve included them here.\n\n1. A central conceptual concern is that the zero-shot high-resolution inference task itself appears ill-posed under the stated assumptions. If the model is trained purely on low-resolution data, its training distribution is inherently band-limited by the Nyquist frequency of that grid. Consequently, the high-frequency components present in the high-resolution domain are unobserved and unidentifiable during training.\nFrom a signal-theoretic standpoint, there exist infinitely many high-frequency realizations consistent with the same low-frequency field, so the inverse mapping from coarse to fine scales has no unique solution. Therefore, it is unclear in what precise sense the model can achieve lower error at higher resolution, may be to test model generalization in some sense? \n\n2. It seems that the testing datasets are fairly smooth, and the low-frequency modes dominate the dynamics. Experiments on data with high-frequency or multi-scale content can provide more insights. \n\n3. FRL seems to rely implicitly on the assumption that the underlying system exhibits scale-consistent spectral structure (as in turbulence or diffusion). Can the authors elaborate more on this?\n\n4. Zero-shot super resolution suffers from aliasing/discretization errors as noted in [1,2,3], there is no discussion on these works and there is no discussion on if and how FRL mitigates these issues. \n\n[1] Representation Equivalent Neural Operators: a Framework for Alias-free Operator Learning\n[2] Discretization-invariance? On the Discretization Mismatch Errors in Neural Operators\n[3] Discretization Error of Fourier Neural Operators"}, "questions": {"value": "1. Given that training data are band-limited by the Nyquist frequency, what information allows FRL to improve predictions beyond that spectral limit?\n\n2. Does FRL rely on the implicit assumption that the underlying physical process is scale-consistent or spectrally smooth?\n\n3. Since multiple high-resolution fields can correspond to the same low-resolution sample, how does FRL regularize or constrain the mapping to select a physically meaningful one?\n\n4. What is the theoretical justification for expecting a neural operator trained on low-resolution data to exhibit convergence behavior analogous to that of a numerical solver?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cm5MZMAe8O", "forum": "84vy8ZomFn", "replyto": "84vy8ZomFn", "signatures": ["ICLR.cc/2026/Conference/Submission68/Reviewer_7EgH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission68/Reviewer_7EgH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission68/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687883864, "cdate": 1761687883864, "tmdate": 1762915445824, "mdate": 1762915445824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel method called Frequency Representation Learning (FRL) to address the \"Scale Anchoring\" problem in Zero-Shot Super-Resolution Spatiotemporal Forecasting (ZS-SR STF). The authors point out that existing methods, when trained on low-resolution data, struggle to process high-frequency components beyond the Nyquist frequency of the training data during high-resolution inference, resulting in errors being \"anchored\" at the low-resolution level and mistakenly interpreted as successful generalization. FRL achieves resolution-invariant frequency embeddings through multi-resolution data construction, normalized frequency representation, and spectral consistency training, significantly reducing high-resolution inference errors. Its effectiveness is validated across multiple architectures and tasks (fluid simulation and weather forecasting)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1、Novel: The paper is the first to clearly identify the \"Scale Anchoring\" phenomenon and provides a theoretical analysis of its mechanism (frequency blindness and high-frequency error dominance), addressing an under-recognized limitation in existing zero-shot super-resolution research.\n2、Strong Generalizability: FRL is architecture-agnostic and can be seamlessly integrated into various mainstream models such as GNNs, Transformers, and CNNs. Experiments show that it significantly improves high-resolution inference accuracy across all tested architectures.\n3、Comprehensive Experimental Validation: Extensive tests on multiple tasks (2D/3D fluid simulation and weather forecasting) combined with error metrics and frequency response analysis thoroughly demonstrate the method’s effectiveness and generalization capability."}, "weaknesses": {"value": "1、Insufficient Computational Overhead Analysis: Although a training complexity increase of approximately 1.1–1.4× is mentioned, the paper lacks detailed discussions on actual training time and memory usage comparisons across different architectures, as well as the storage requirements for multi-resolution data construction.\n2、Limited Generalization in Extreme Physical Scenarios: The authors note in the appendix that FRL fails in high-Reynolds-number turbulence (e.g., Re=10^5), but they do not deeply analyze the reasons for failure or propose improvement directions, which limits the method’s applicability in complex physical systems."}, "questions": {"value": "1、Regarding computational overhead: FRL requires storing and processing multi-resolution data during training. How can the storage cost be balanced against performance gains in practical deployment? Are there compression or dynamic sampling strategies to mitigate storage pressure?\n2、Regarding generalization capability: The performance degradation of FRL in high-Reynolds-number turbulence suggests that the method relies on consistent frequency response patterns in physical systems. Is there a plan to incorporate adaptive mechanisms or physical constraints to enhance adaptability to highly nonlinear and multi-scale coupled systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j6q5nidED4", "forum": "84vy8ZomFn", "replyto": "84vy8ZomFn", "signatures": ["ICLR.cc/2026/Conference/Submission68/Reviewer_SPE1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission68/Reviewer_SPE1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission68/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862585490, "cdate": 1761862585490, "tmdate": 1762915445654, "mdate": 1762915445654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper defines and analyzes an overlooked phenomenon, called scale anchoring, in cross-resolution generalization for zero-shot super-resolution. To mitigate this generalization issue, this paper proposes the frequency representation learning method through three steps from data and optimization perspectives. The results have shown the superiority of the proposed method compared to existing methods on fluid simulation and weather forecasting datasets. \n\nContributions:\n\nThis observation is new and provides an analysis of why models trained at low resolution fail to improve accuracy at higher resolutions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper looks into spectral bias-related stuff, which is important in scientific machine learning.\n\n- This paper proposes a new concept, called scale anchoring, in zero-shot super-resolution. \n\n- This paper provided a theoretical and empirical analysis of this scale anchoring phenomenon. \n\n- This paper is generally well-written and well-presented."}, "weaknesses": {"value": "I have several major concerns listed below. \n\n- First, what is the difference between the defined scale anchoring and the prior concepts, such as spectral bias / discretization-invariance? To me, scale anchoring refers to a failure mode in which a model trained on low-resolution data fails to get good accuracy at finer resolutions, due to missing high-frequency information beyond the coarse data’s Nyquist limit. Spectral bias also refers to the same things that a NN model cannot learn high-frequency features from data. \n\n- Also, there is a pretty similar paper that discussed resolution generalization and discretization mismatch errors. What is the difference between this paper and the previous paper [1]? \n\n- The proposed three-step method covers some tricks from the data and optimization sides. It would be good to have an ablation study to test how much each step contributes to the performance improvement. Also, it seems the third step (frequency loss) has already been seen in many prior work [2,3] that uses spectral loss to mitigate the spectral bias issues. \n\n- I feel like it would strengthen the paper by providing a comparison of computational cost versus accuracy improvement. The computational cost of using low-resolution grids should be small, and with the proposed method, one can get better performance on finer grids. Then, people can have a better sense of the tradeoffs. \n\n ---\n\n**Refs:**\n\n[1] Gao, Wenhan, et al. \"Discretization-invariance? on the discretization mismatch errors in neural operators.\" The Thirteenth International Conference on Learning Representations. 2025.\n\n[2] Chattopadhyay, Ashesh, Y. Qiang Sun, and Pedram Hassanzadeh. \"Challenges of learning multi-scale dynamics with AI weather models: Implications for stability and one solution.\" arXiv e-prints (2023): arXiv-2304.\n\n[3] Saccardi, Carlo, et al. \"Assessing the Geographic Generalization and Physical Consistency of Generative Models for Climate Downscaling.\" arXiv preprint arXiv:2510.13722 (2025)."}, "questions": {"value": "- Is there any stability issue when you extrapolate beyond the training Nyquist range?\n\n- On page 6, line 321, do you need bold for “Notably”?\n\n- In Table 3, why do you have some uncommon grids like 43^3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7tihJD2Ngj", "forum": "84vy8ZomFn", "replyto": "84vy8ZomFn", "signatures": ["ICLR.cc/2026/Conference/Submission68/Reviewer_cSKw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission68/Reviewer_cSKw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission68/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866154241, "cdate": 1761866154241, "tmdate": 1762915445483, "mdate": 1762915445483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Based on the first-round reviews, we revised the paper to **Rebuttal Version 1**.  \nWe define the notation as follows:  \n- **[A]**: added new content  \n- **[R]**: revised existing content  \n- **[xx–xx]**: line numbers of the added/revised content  \n- **[xxxx Wn/Qm]**: reviewer xxxx’s Weakness n or Question m addressed by the change  \n\nWe made the following modifications to the original paper:\n\n---\n\n### **2 Related Work**\n\n- **[R] [88–91] [Kovs W1&2]**  \n  Revised the definitions of STF and ZS-SR STF to make them more self-contained.\n\n- **[R] [102–132] [cSKw W1–3, 7EgH W4, Kovs W3]**  \n  Consolidated all related issues into a single subsection.  \n  Clearly distinguished Scale Anchoring from existing problems.  \n  Discussed all relevant existing problems and mainstream solutions.  \n  Pointed out that Step 1 and Step 3 of FRL are variants of existing techniques.  \n  Analyzed differences between SA and existing problems from the perspectives of source, scope, and consequence.\n\n---\n\n### **5 Solution for Scale Anchoring**\n\n- **[A] [301–308] [cSKw W3, Kovs W3, 7EgH Q4]**  \n  Explicitly stated that Steps 1 and 3 of FRL resemble existing methods.  \n  Emphasized that Step 2 is the key to addressing SA, and cited the ablation study in Appendix H to support this claim.  \n  Cited Appendix D to indicate that FRL guarantees decreasing high-resolution inference error as resolution increases, though not power-law convergence like numerical solvers.\n\n- **[R] [309–319] [SPE1 W2&Q2, 7EgH W1–3, cSKw Q1]**  \n  Clarified the effective regime, limitations, and failure boundaries of FRL, and referenced Appendix I for detailed failure mode analysis.  \n  Retained hyperlinks to the pseudocode and complexity analysis appendices.\n\n---\n\n### **6 Empirical Evaluation**\n\n- **[R] [355–357] [cSKw Q2]**  \n  Removed the boldface on “notably.”\n\n- **[A] [327–344] [cSKw Q3, Kovs Q1]**  \n  Added explanations to each dataset on how low-resolution data were obtained and why the specific downsampling factors and maximum factors were chosen.\n\n- **[A] [503–512] [cSKw W4, SPE1 W1&Q1]**  \n  Added a standalone subsection at the end of the experimental results summarizing key computational cost measurements, with pointers to Appendix E for full complexity analysis.\n\n---\n\n### **Appendices**\n\n- **[R] [756–772]**  \n  Updated the appendix table of contents.\n\n- **[A] [1127–1241] [7EgH Q4]**  \n  Added a theoretical analysis appendix showing that FRL guarantees reduced high-resolution inference error (but not power-law convergence as in numerical solvers).\n\n- **[R] [1326–1392] [cSKw W4, SPE1 W1&Q1]**  \n  Replaced the previous coarse cost estimates with a complete computational complexity table.\n\n- **[R] [2040–2135] [SPE1 W2&Q2, 7EgH W1–3, cSKw Q1]**  \n  Expanded the FRL failure-mode analysis with concrete examples, clearly explaining FRL’s assumptions, boundaries of validity, failure modes, and possible improvements.\n\n---\n\n### **Abstract, 1 Introduction, and 7 Discussion**\n\n- **[R]**  \n  Revised relevant descriptions to reflect all modifications above.\n\n---\n\nWe thank all reviewers for their insightful comments and questions, which significantly improved the completeness and clarity of this work."}, "title": {"value": "Rebuttal Version 1"}}, "id": "jSDTTgroyr", "forum": "84vy8ZomFn", "replyto": "84vy8ZomFn", "signatures": ["ICLR.cc/2026/Conference/Submission68/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission68/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission68/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763317876167, "cdate": 1763317876167, "tmdate": 1763317904849, "mdate": 1763317904849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}