{"id": "VYQuICALXj", "number": 11140, "cdate": 1758190611167, "mdate": 1759897605174, "content": {"title": "Cross-Modal Redundancy and the Geometry of Vision–Language Embeddings", "abstract": "Vision–language models (VLMs) align images and text with remarkable success, yet the geometry of their shared embedding space remains poorly understood. \nTo probe this geometry, we begin from the Iso-Energy Assumption, which exploits cross-modal redundancy: a concept that is truly shared should exhibit the same average energy across modalities.\nWe operationalize this assumption with an Aligned Sparse Autoencoder (SAE) that encourages energy consistency during training while preserving reconstruction.\nWe find that this inductive bias changes the SAE solution without harming reconstruction, giving us a representation that serves as a tool for geometric analysis.\nSanity checks on controlled data with known ground truth confirm that alignment improves when Iso-Energy holds and remains neutral when it does not.\nApplied to foundational VLMs, our framework reveals a clear structure with practical consequences: \n**(*i*)** sparse *bimodal* atoms carry the entire *cross-modal* alignment signal; \n**(*ii*)** *unimodal* atoms act as *modality-specific* biases and fully explain the modality gap; \n**(*iii*)** removing unimodal atoms collapses the gap without harming performance; \n**(*iv*)** restricting vector arithmetic to the bimodal subspace yields in-distribution edits and improved retrieval. \nThese findings suggest that the right inductive bias can both preserve model fidelity and render the latent geometry interpretable and actionable.", "tldr": "Understanding the geometry of multimodality through a concept-based approach, leading to applications like semantic vector arithmetic and modality gap free embeddings.", "keywords": ["multimodal", "concepts", "sparse autoencoder", "modality gap", "applications of interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/86222acb2d6d8dad5303fe0c78cf9b01aa93cc98.pdf", "supplementary_material": "/attachment/3e244347137e83a9b4fcdf599eb3d6fdc43269e0.pdf"}, "replies": [{"content": {"summary": {"value": "The paper investigates the geometry of the embedding space of CLIP-like models using sparse autoencoders. The authors augment SAE training by an iso-energy regularization term that encourages SAE latents to have similar spreads (i.e., second moments) for both modalities (Def. 2). They show that only a small set of features explain the modality gap and the remaining features are sufficient for cross-modal alignment. Removing former features reduces modality gap while retaining performance and the latter allows for vector arithmetic."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* S1: The energy penalty (Def. 2/ Eq. 1) is interesting and a simple addition to the SAE loss.\n\n* S2: Synthetic & real experiments confirm that the proposed aligned SAE better matches the geometry of CLIP-like models. Particularly, if the SAE features can distinguish between shared or modality-gap-specific.\n\n* S3: The paper introduced four metrics to evaluate whether the SAE variants capture the geometrical or functional properties of the VLMs.\n\n* S4: The proposed SAE allows semantic vector arithmetic."}, "weaknesses": {"value": "* W1: 3 out of the 4 key findings have been reported in previous work (see bullet points below). While the findings are reached using a different, more complex approach, the current paper seems to re-report these findings.\n\t* Few (unimodal) features fully explain the modality gap (Fig. 2 left, 3) ~> see Fig. 4 in [3] or Fig. 3 in [4]\n\t* Bimodal features carry the entire cross-modal alignment signal (Fig. 2 right, Fig. 3) ~> cross-modality transferability experiments in [2], e.g., Tab. 2.\n\t* Removing those modality-gap features reduces the modality gap without loss of performance (Fig. 4) ~> again, see cross-modality experiments in [2], e.g., Tab. 2.\n\n* W2: The paper provides little to no experimental details in the main text, making it hard to understand the results without searching the supplemental.\n\n* W3: It is assumed that bimodal atoms are semantically aligned across modalities (“bimodal atoms encode the shared conceptual backbone” l. 345) and few qualitative examples are provided in Appendix G. However, there is no quantitative evaluation for this claim.\n\n* W4: Only contrastive models are evaluated. For example, the modality gap has been also observed in multimodal LLMs. It’d be important to include such results.\n\n## Comment\n\n* C1: I’d encourage the authors to include discussions on missing relevant literature [1-4].\n\n* C2: This work’s proposition 1 seems closely related to [2]’s proposition A.1. The only difference seems to be that the modality information can be adaptive here.\n\n* C3: The caption of Fig. 4 is partially occluded from Fig. 5.\n\n---\n\n[1] https://www.mlmi.eng.cam.ac.uk/files/2021-2022_dissertations/understanding_and_fixing_the_modality_gap_in_vision-language_models_reduced.pdf \n\n[2] https://openreview.net/forum?id=D-zfUK7BR6c \n\n[3] https://openreview.net/forum?id=uAFHCZRmXk\n\n[4] https://openreview.net/forum?id=QGUju9B68Z"}, "questions": {"value": "* Q1: Is the standard SAE (l. 176/177, 185) the MP-SAE or is it truly standard SAE?\n\n* Q2: How are unimodal or bimodal features separated?\n\n* Q3: Do the unimodal features approximate the modality gap vector? Related to that, does it explain why they all have such high cosine similarities (Fig. 16b)?\n\n* Q4: What is $\\mu$ in Fig. 2 left?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DuOtipv5xc", "forum": "VYQuICALXj", "replyto": "VYQuICALXj", "signatures": ["ICLR.cc/2026/Conference/Submission11140/Reviewer_XysU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11140/Reviewer_XysU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760891873397, "cdate": 1760891873397, "tmdate": 1762922310686, "mdate": 1762922310686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the geometry of VLM embedding spaces via the Iso‑Energy Assumption—shared concepts should have domain‑invariant average squared activation. The authors train an Aligned Matching‑Pursuit SAE with a small cross‑modal alignment regularizer, yielding a dictionary that separates bimodal atoms (which carry all cross‑modal alignment) from unimodal atoms (a few high‑energy modality‑specific biases explaining the modality gap). On synthetic data and CLIP/OpenCLIP/SigLIP variants, this preserves reconstruction while markedly improving multimodality metrics, and enables interventions such as removing unimodal atoms to close the modality gap without hurting retrieval and performing in‑distribution semantic arithmetic restricted to the bimodal subspace."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Clear Problem Formulation and Strong Motivation:** The paper articulates a pertinent and significant problem in VLM interpretability and manipulability. By focusing on the geometric underpinnings of cross-modal alignment and the \"modality gap,\" the work addresses a critical area for improving VLM transparency and control. \n2.  **Novel and Intuitive Hypothesis:** The \"Iso-Energy Hypothesis\" offers an elegant and interpretable statistical prior for identifying shared concepts within a sparse dictionary. This hypothesis provides a concrete, measurable criterion that transforms the abstract notion of \"cross-modal redundancy\" into an actionable constraint for dictionary learning.\n3.  **Demonstrated Practical Interventions:** The ability to close the modality gap by masking uni-modal atoms and to perform \"in-distribution\" semantic arithmetic within the bi-modal subspace represents a significant practical contribution. These interventions offer concrete pathways for improving the robustness and interpretability of VLM applications."}, "weaknesses": {"value": "1.  **Reliance on Paired Data for Alignment Regularization:** Although lines 158-160 allude to the potential of leveraging \"cross-modal redundancy alone,\" the current formulation of the alignment regularizer explicitly requires instance-level image-text pairs. The robustness of the method to noisy or imperfect pairings, or its applicability in settings with weak or no explicit pairings (e.g., using only domain labels), remains unexplored. This dependency may limit its generality and practical scope.\n2.  **Limited Assessment of Dictionary Stability and Generalizability:** While the paper aims to enhance SAE dictionary stability via the Iso-Energy Assumption and demonstrates improved recovery on synthetic data during \"Sanity check\", it lacks a systematic and multi-faceted analysis of this robustness on large-scale real-world VLM datasets. The reproducibility of the learned dictionary under varying conditions, such as different expansion ratios, sparsity targets, or subsets of training data, remains unexplored. Thus, the evaluation of this crucial aspect in practical scenarios is not yet comprehensive.\n3.  **Scope of Evaluation and Downstream Task Relevance:** While the paper demonstrates strong results on retrieval-oriented metrics and interventions, the generalizability to other VLM tasks (e.g., visual question answering, image generation, localization, counting, spatial reasoning) is not fully explored. The claim that \"masking uni-modal atoms does not hurt performance\" might hold for certain tasks, but could be detrimental for tasks that rely on more modality-specific information."}, "questions": {"value": "**External Validation of Atomic Concepts:** While visualizations are provided, the \"semantic stability\" of the atoms is largely qualitative. Is it possible to introduce quantitative measures for concept purity, namability, or alignment with human annotations to further validate the interpretability and meaningfulness of the identified bi-modal and uni-modal atoms? This would provide stronger evidence that the method is indeed recovering genuine, human-understandable concepts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vXdTMs94vx", "forum": "VYQuICALXj", "replyto": "VYQuICALXj", "signatures": ["ICLR.cc/2026/Conference/Submission11140/Reviewer_BMn2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11140/Reviewer_BMn2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836018147, "cdate": 1761836018147, "tmdate": 1762922310208, "mdate": 1762922310208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the geometry of vision–language embeddings through a proposed Iso-Energy assumption, which states that shared cross-modal concepts should have equal activation energy across modalities. To explore this, the authors introduce an aligned sparse autoencoder (SAE-A) that adds a cosine-similarity–based alignment loss to a standard sparse autoencoder. The numerical experiments on CLIP, OpenCLIP, and SigLIP embeddings show that the aligned SAE could improve cross-modal alignment metrics while maintaining reconstruction quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides an interesting perspective on the geometry of vision–language embeddings by introducing the Iso-Energy assumption.\n\n2. The numerical results are consistent, showing that the aligned SAE can improve cross-modal alignment metrics without damaging reconstruction quality."}, "weaknesses": {"value": "1. The connection between the Iso-Energy Assumption in Definition 2 and the implemented loss in Equation (1) is not that clear. Definition 2 describes a population-level equality of per-coordinate activation energies across modalities, whereas the alignment loss in (1) simply quantifies the batch-level sum of cosine similarity between sample codes. The paper does not provide a derivation or justification showing that this cosine similarity sum term directly enforces or meaningfully approximates the Iso-Energy property.\n\n2. The alignment loss in Equation (1) effectively reduces to a vanilla sum of cosine similarities between the latent codes from two modalities. This formulation looks too simple and somewhat ad hoc, lacking a clear connection to encourage equalized energy statistics as defined by the Iso-Energy assumption.\n\n3. The paper introduces the aligned sparse autoencoder without providing sufficient background on the baseline SAE formulation, its reconstruction, and sparsity terms. This makes the method less self-contained and more difficult for readers less familiar with the SAE framework to follow.\n\n4. Some of the mathematical definitions, particularly in Definition 2, are not presented rigorously. The conditional expectation is written as if conditioned on the specific sample $X$, which collapses the expectation to the outcome for that given value of $X$ in the conditional expectation of (1)."}, "questions": {"value": "1. Can the authors clarify the precise theoretical link between the Iso-Energy Assumption in Definition 2 and the cosine-similarity–based alignment loss in Equation (1)? \n\n2. As the alignment loss in (1) reduces to a simple sum of cosine similarities, did the authors experiment with other similar regularizers (e.g., the sum of the squared or absolute value of the inner products in (1)) or other regularizers that can more directly enforce the Iso-Energy property in Definition 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q6n5sx19o3", "forum": "VYQuICALXj", "replyto": "VYQuICALXj", "signatures": ["ICLR.cc/2026/Conference/Submission11140/Reviewer_Yfxj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11140/Reviewer_Yfxj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890351610, "cdate": 1761890351610, "tmdate": 1762922309746, "mdate": 1762922309746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an Iso-Energy prior for learning aligned sparse concept dictionaries on top of VLM embeddings. By mildly enforcing equal second-moment (“energy”) of a concept across image/text domains, the aligned SAE separates bimodal atoms (semantic carriers) from unimodal atoms (modality-specific bias). This yields two actionable interventions: (i) closing the modality gap by masking unimodal atoms without hurting retrieval, and (ii) performing robust semantic vector arithmetic within the bimodal subspace, reducing OOD drift."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear and effective framing of a testable modeling intuition.\nThe paper presents a well-motivated and conceptually coherent formulation. It articulates a precise inductive bias: that shared cross-modal concepts should exhibit similar activation statistics across modalities. This idea is not only intuitively appealing but also operationalized in a mathematically minimal way through second-moment constraints. The writing and structural clarity further reinforce this framing, making the contribution accessible and theoretically grounded.\n\n2. Methodologically grounded execution with dual functionality.\nThe proposed method delivers more than conceptual framing. It constructs a sparse, interpretable bimodal subspace that supports both analysis and intervention. The same subspace allows for attribution-style interpretation as well as semantically coherent editing, demonstrating that the learned structure is not only intelligible but also functionally controllable. This dual capacity is rarely achieved in the interpretability literature and gives the method both analytical and practical value."}, "weaknesses": {"value": "1. Sufficiency versus necessity of the Iso-Energy criterion.\nEqual second moments across modalities can indicate shared concepts, but they are not required. Without invariance to modality-specific anisotropy or rescaling, genuinely shared factors may be labeled unimodal. It would be better to add invariance controls such as per-modality whitening or variance normalization, and to compare with covariance-aware baselines such as CCA or CORAL to verify that the findings are not driven by marginal variance.\n\n2. Sensitivity to pairing noise and frequency imbalance.\nThe alignment term relies on paired image and text data, where long-tail frequencies and noisy matches are common. Energy equality can be confounded by corpus artifacts rather than semantics. It would be better to add two controls: a frequency-matched subsample that balances concept prevalence across modalities, and a shuffled-pairs stress test to quantify robustness to misalignment noise."}, "questions": {"value": "1. To what extent do the conclusions generalize to more complex tasks and architectures, such as VQA on LLaVA-series models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P2FGxaMJlL", "forum": "VYQuICALXj", "replyto": "VYQuICALXj", "signatures": ["ICLR.cc/2026/Conference/Submission11140/Reviewer_U7cA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11140/Reviewer_U7cA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898998744, "cdate": 1761898998744, "tmdate": 1762922309255, "mdate": 1762922309255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}