{"id": "sVBJ2Jib9E", "number": 20321, "cdate": 1758304721262, "mdate": 1759896983957, "content": {"title": "Operator Learning for Families of Finite-State Mean-Field Games", "abstract": "Finite-state mean-field games (MFGs) arise as limits of large interacting particle systems and are governed by an MFG system, a coupled forward–backward differential equation consisting of a forward Kolmogorov–Fokker–Planck (KFP) equation describing the population distribution and a backward Hamilton–Jacobi–Bellman (HJB) equation defining the value function. Solving MFG systems efficiently is challenging, with the structure of each system depending on an initial distribution of players and the terminal cost of the game. We propose an operator learning framework that solves parametric families of MFGs, enabling generalization without retraining for new initial distributions and terminal costs. We provide theoretical guarantees on the approximation error, parametric complexity, and generalization performance of our method, based on a novel regularity result for an appropriately defined flow map corresponding to an MFG system. We then demonstrate empirically that our framework achieves accurate approximation for two representative instances of MFGs: a cybersecurity example and a high-dimensional quadratic model commonly used as a benchmark for numerical methods for MFGs.", "tldr": "We provide an operator learning method for solving finite-state mean-field games, equipped with rigorous approximation and generalization guarantees.", "keywords": ["Operator learning", "mean-field games", "parametric complexity"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/258e2be19d573b341eafc56f5c9b72a2b3b7f194.pdf", "supplementary_material": "/attachment/98b0293ab5d2429b3b0a9673c837d486094f9bf6.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes an operator learning method to approximate Nash equilibrium (NE) of families of finite-state mean-field games (MFGs), which are represented using forward Kolmogorov-Fokker-Planck (KFP) equation. Unlike standard solution techniques which require re-solving the game for any change in the configuration, learning mapping between the distribution of configurations to the corresponding solution (here, NE) is greatly efficient and desired. Motivated by this, the authors frame MFG equilibria as outputs of an operator, dubbed flow map, which maps the initial distributions (of initial states) and cost functions to its corresponding NE."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written with necessary proofs to support the claims. Operator learning is a natural progression when it comes to problems of this nature (i.e., learning equilibrium control policies, solving PDEs), and there exists a relevant literature that proposes operator learning for games (although it is for differential games) [1]. This brings us to the weakness. \n\n[1] Zhang, L. et al., *Pontryagin Neural Operator for Solving General-Sum Differential Games with Parametric State Constraints*. L4DC 2024."}, "weaknesses": {"value": "I think the authors need to highlight the challenges in extending existing operator learning methods to solve MFGs; otherwise it risks reading as merely a new case study, which I don't believe is the case here."}, "questions": {"value": "Some questions and comments: \n\n1. It seems that the bottleneck is the picard solver, can the authors comment on the following queries:\n\n    - How well does the current picard iteration implementation handle high dimensional problems (i.e., how does the empirical complexity look like)?\n    - How do you obtain the initial guess and how sensitive is the current solver to the initial guess?\n    - In your experiments, do you notice any convergence issues?\n\n2. Do you have any insights on why ResNet results in better performance?\n\n3. While value function plots are informative, it would be better if the authors could show additional visualization showcasing the interaction (e.g., state evolution). This might help readers familiarize with the examples and make sense of the optimal actions.\n\n4. You mention that learning becomes unstable beyond d=10 in the case of high-dimensional quadratic models. Could you comment on which aspect of the algorithm leads to the instability? Is it the picard solver?\n\n5. Just curious if the authors tested out-of-distribution generalization of the learned flow operator."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fkZRMtIGL7", "forum": "sVBJ2Jib9E", "replyto": "sVBJ2Jib9E", "signatures": ["ICLR.cc/2026/Conference/Submission20321/Reviewer_Di3U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20321/Reviewer_Di3U"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761072961556, "cdate": 1761072961556, "tmdate": 1762933782550, "mdate": 1762933782550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studied the proposed operator learning method of numerically solving finite-state mean-field games (MFGs). The main contributions include:\n\n* A framework of solving MFGs by using neural networks to approximate the flow map, which is an operator that maps the initial distribution of players’ states $\\eta$ and the terminal cost function $g$ to the value function $u$. The framework generally follows the operator learning methods in the literature. Compared with previous methods, the advantage of the proposed method is that it can treat all the initial conditions without re-training.\n\n* The authors provide theoretical analysis on the generalization error and approximation error of the proposed method. Especially, the analysis provides guarantees on the errors in terms of the weights of the networks, the state space dimension, and the dimension of the space that is used to parameterize the terminal cost function.\n\n* Two numerical experiments are provided to show the efficiency of their methods: the cybersecurity model with the dimension of state space $4$, and quadratic models with 10 dimensions. In these experiments, the authors compare the output of their operator learning methods with the solutions produced by solving the ODE systems numerically."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The studied problem is well-motivated, and the relationship between the current work and related works is clearly stated. \n* The experimental settings are described in detail.\n* The authors have provided both theoretical analysis and experimental results on their proposed methods. \n* The proposed method has the potential to be used to solve MFGs efficiently, because it does not need to be re-trained to adapt to a specific choice of the MFG."}, "weaknesses": {"value": "**Theoretical side**: I think the theoretical bounds provided in this paper are loose, and I doubt whether it is possible to use them to gain practically relevant insights. Especially, my concerns mainly rely on the following points:\n\n* Both in Corollary 4.3 and 4.5, the dependency on the parameter $\\mathcal{K}$ is omitted in the big-O notation. Since the dimension of the parameterized space of terminal cost functions is an important block of the proposed methods, and its dimension can be large, I think it is important to have a clear understanding of how its dimension affects the performance of the proposed method.\n\n* As the authors said in Appendix E.2, Corollary 4.3 and 4.5 are almost directly taken from the corresponding results in (Jiao et al., 2023). However, the results of (Jiao et al., 2023) are very general results that hold for ReLU neural networks on regression problems. This means the bounds in Corollary 4.3 and 4.5 in the current paper do not use the special structures of MFGs. This makes me feel the provided bounds are too general and cannot provide special insights into the problem of solving MFGs with neural networks.\n\n**Experimental side**: My general feeling is that the scope of experiments in this paper is rather limited: only two examples are provided, and how the choices of architectures, optimizers, sampling method, and loss functions affect the performance is discussed very briefly.\n\nFor example, most experiments are done with fully connected ReLU networks, and it is only briefly mentioned that the ResNet architecture can benefit the performance when $d = 10$ in the quadratic models. It is hard to gain understanding from such a brief discussion, and it may lead to confusion: does the ResNet architecture only have better performance than fully connected ReLU networks when $d = 10$, or does such a phenomenon consistently appear in other settings?\n\nThe authors motivate their choice of using fully connected ReLU neural networks as being most aligned with their theoretical results (lines 680–682). However, since these experiments are not only aimed at verifying their theoretical results, I am not convinced that this is a good reason to limit their experiments to fully connected ReLU networks. Even for the purpose of verifying their theoretical bounds, I also did not see any discussion on the comparison between their theoretical results and experimental results.\n\n**Other problems**:  \n* In the first paragraph of Section 2.1, there is no clear definition of what the \"action\" of the players is.  \n* In line 153, the notation $\\mathcal{L}(\\cdot)$ is not defined.\n* In Figures 2 and 3, the trajectories of the proposed methods and the ODE solvers are too close. I suggest including figures that present their differences as well."}, "questions": {"value": "* Compare with the general bounds provided in (Jiao et al., 2023), what special insights can Corollary 4.3 and 4.5 provide for the problem of learning flow maps using neural networks?\n* I suggest that the authors add discussions on how the architectures, optimizers, sampling methods, and loss functions can affect the performance of the proposed methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HKUGnYn8Zg", "forum": "sVBJ2Jib9E", "replyto": "sVBJ2Jib9E", "signatures": ["ICLR.cc/2026/Conference/Submission20321/Reviewer_PKPa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20321/Reviewer_PKPa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841640411, "cdate": 1761841640411, "tmdate": 1762933782095, "mdate": 1762933782095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an operator learning framework to solve entire families of finite-state mean-field games (MFGs). Instead of solving each MFG instance individually, the authors aim to learn a neural network that approximates the \"flow map\" as an operator that maps the game's parameters (initial player distribution and a terminal cost parameter) to the corresponding value function. The methodology involves generating training data by solving specific MFG instances using fixed point iteration on the coupled forward-backward HJB-KFP system. The paper provides theoretical guarantees on the approximation and generalization error of this approach, and the framework is is empirically validated on a cybersecurity model and a high-dimensional quadratic MFG benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The primary strength of this work is its framing of the problem. Moving from solving single MFG instances to learning an operator for entire parameterized families is a significant and practical step forward. This approach enables rapid, retrain-free generalization to new initial conditions and cost functions, which is highly valuable for applicability of MFGs.\n\n- The paper provides rigorous theoretical guarantees for the approximation error and generalization error. The results require common MFG assumptions.\n\n- The paper is well-written, structured, and easy to follow. It begins with a clear introduction to finite-state MFGs, logically motivates the need for operator learning, defines the flow map, details the algorithm, and then presents the theory and experiments. The connection to prior work is clearly articulated.\n\n- The experiments are well-chosen and effectively demonstrate the method's capabilities. The use of a standard cybersecurity benchmark and a scalable quadratic model shows the method's accuracy and performance as the state-space and parameter dimensions increase. The inclusion of Figure 4, which analyzes the effect of network width on training and test loss, is a good ablation study that supports the theoretical claims."}, "weaknesses": {"value": "- The paper honestly notes that learning becomes \"increasingly unstable\" for dimensions beyond d=10. The success in d=20 is shown for a simplified setting with a fixed time discretization. This suggests that the primary method has practical scalability limits, since only 10 to 20 states could be too little in practice.\n\n-  The theoretical guarantees, while valuable, also exhibit a \"curse of dimensionality\" in their dependence on the state. These bounds suggest that the methodology in general does not scale well."}, "questions": {"value": "1.  The paper mentions instability for d > 10. Is this instability primarily a result of the data generation step (i.e., convergence issues with Picard iteration in high dimensions) or the neural network training process (e.g., optimization challenges)?\n\n2.  Does the methodology generalize to discrete-time problems beyond time discretizations of continuous-time problems, or do you expect additional difficulties?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1CKQMjClVR", "forum": "sVBJ2Jib9E", "replyto": "sVBJ2Jib9E", "signatures": ["ICLR.cc/2026/Conference/Submission20321/Reviewer_v1jZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20321/Reviewer_v1jZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971949192, "cdate": 1761971949192, "tmdate": 1762933781749, "mdate": 1762933781749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a supervised operator learning method for solving mean-field games (MFGs) with finite state spaces. The proposed method trains a neural operator using samples generated by Picard iteration. It then applies the neural operator to solve different MFGs with varying initial distributions and terminal costs. Theoretical guarantees are discussed, and numerical experiments are provided."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work is well-motivated, as learning MFGs is in general challenging and computation-heavy.\n- The paper is well-written and easy to follow.\n- Theoretical guarantees are discussed for the supervised learning method.\n- Various numerical experiments are conducted, with their settings and results clearly presented."}, "weaknesses": {"value": "- Any finite state space can be embedded into a continuous state space. It's thus not clear to me why addressing only finite state spaces is a contribution of this work over those addressing continuous state spaces.\n- The contribution of the work over Cohen et al. (2024) is the ability of handling different terminal cost functions, which seems marginal to me.\n- Separable running cost function (action and population are decoupled) is considered, which is a huge simplification that needs further clarification.\n- The excess risk misses two significant error sources: discretization error and sample label approximation error. The latter is critical: for each \"sample\", its reference equilibrium is only the approximate solution generated by finite steps of Picard iteration.\n- The approximation error of the neural network depends on $K$, the bound of the weights. How large is $K$? How do we know if it's bounded? To give an informative error bound, $K$ needs to be expressed using algorithm parameters.\n- Typos and format issues.\n  - Please correctly use in-text and end-of-text citations.\n  - Line 056: typo \"parame\"\n  - Line 061: has \"a\" generalization error\n  - Line 260: $u^\\eta$ is missing a superscript $g$.\n  - Lines 266-267: $b_j$ and $B_j$ are inconsistent.\n  - Line 347: the domain and codomain of $\\Phi$ is off."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oA2fl41N6m", "forum": "sVBJ2Jib9E", "replyto": "sVBJ2Jib9E", "signatures": ["ICLR.cc/2026/Conference/Submission20321/Reviewer_rSss"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20321/Reviewer_rSss"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20321/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986388209, "cdate": 1761986388209, "tmdate": 1762933781526, "mdate": 1762933781526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}