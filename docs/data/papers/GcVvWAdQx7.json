{"id": "GcVvWAdQx7", "number": 16038, "cdate": 1758258951805, "mdate": 1759897266145, "content": {"title": "Advanced Image Forensics: Detecting Tampered and AI-Generated Images with Adversarial Learning", "abstract": "Detecting image tampering and Artificial Intelligence Generated Images are vital challenges in the fields of computer vision. The primary difficulty in identifying tampered images lies in uncovering minute evidence of manipulation, while AIGC image detection struggles with the increasingly lifelike quality of generated images. Existing solutions often focus on either tampered or AIGC images, yet both can coexist in various contexts. To address this limitation, we propose an advanced framework for social media image forensics that utilizes adversarial learning to identify both tampered and AIGC images. This framework employs a tri-branch architecture that combines generative adversarial learning with deep neural networks, effectively identifying both tampered and AI-generated content. We validated our framework using public image tampering datasets, a specialized AIGC image dataset, and datasets containing both tampered and AI-generated images. Experimental outcomes demonstrate that our framework significantly enhances accuracy by approximately 20\\%, 30\\%, and 20\\% for tampered, AI-generated, and tampered AIGC image datasets, respectively. Code will be available on request.", "tldr": "", "keywords": ["Adversarial learning", "tri-branch", "image forensics", "AIGC detection"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/405567709f2b278bf8479ac02a1310cfa78dc811.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper claims to employ adversarial learning with a generator \\( G \\) and three discriminators \\( D \\), where \\( G \\) aims to produce an image \\( \\hat{I}_t \\) that appears \"real\" to deceive the discriminators."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "please refer to the \"Questions\" sections directly."}, "weaknesses": {"value": "please refer to the \"Questions\" sections directly."}, "questions": {"value": "- In image forensics—e.g., as in [1]—generators are typically used to simulate forgery processes, thereby enriching the training data for discriminators with diverse synthetic manipulations. In contrast, the generator here appears to \"repair\" tampered or AI-generated inputs by transforming them into realistic-looking images. This approach contradicts the core objective of forgery detection, which is to learn discriminative features of manipulation rather than to synthesize or restore realistic content. Consequently, the framework’s motivation is unclear, and its effectiveness for detection tasks is questionable.\n\n[1] Zhuo et al., Self-Adversarial Training Incorporating Forgery Attention for Image Forgery Localization, IEEE TIFS, 2022.\n\n- The paper proposes a unified framework for detecting both tampered and AI-generated images. While the goal is commendable, several key aspects require clarification:\n\na) Ambiguous Detection Target: The introduction highlights \"Tampered AIGC\" as a distinct category requiring detection. However, it's unclear whether the model's target for such images is to identify them as AI-generated, as tampered, or as a combined state. The output lacks explicit indicators for which specific task (tampering vs. AI-generation) the detection applies to, making the \"simultaneous detection\" objective ambiguous.\n\nb) Unclear Labeling for Tampered AIGC Dataset: Table 2 presents pixel-level metrics (ACC, F1) for the \"Tampered AIGC\" dataset. It is crucial to clarify whether the ground truth labels for this dataset are at the image level (indicating if the image is tampered/AI-generated) or pixel level (marking specific tampered regions). This directly impacts the interpretation of the reported results.\n\nc) Inconsistent Metric Interpretation in Table 1: Table 1 reports ACC and F1 scores across diverse datasets (tampered, GAN-generated, combined). Given that Section 4 specifies pixel-level evaluation for tampered regions and image-level for AI-generated content, it is ambiguous whether the F1/ACC values in Table 1 refer to pixel-level or image-level performance for each dataset type. This inconsistency hinders clear comparison.\n\nd) Lack of Visual Examples: The paper mentions applying \"adversarial tampering\" to create the Tampered AIGC dataset but provides no visual examples. Including sample images would significantly aid readers in understanding the nature and subtlety of these specific manipulations.\n\n- The paper aims to address the joint detection of both tampered and AI-generated images. However, the evaluation is limited to only two tampered datasets (CASIA V2 and IDM) and three GAN-based synthetic datasets (ProGAN, StyleGAN, CycleGAN). Given that AI-generated images now encompass diverse generation paradigms—especially diffusion models—the exclusive focus on GAN-generated content undermines the generalizability of the proposed method.  \nTo strengthen the evaluation, the authors may align their test sets with those used in recent state-of-the-art methods such as TruFor [2], which includes comprehensive tampered benchmarks (e.g., Columbia, Coverage, NIST, DSO) and modern AI-generated image datasets like GenImage [3], covering a broader spectrum of generative models.  \n\n[2] Guillaro et al., TruFor: Leveraging All-Round Clues for Trustworthy Image Forgery Detection and Localization, CVPR 2023.  \n\n[3] Zhu et al., GenImage: A Million-Scale Benchmark for Detecting AI-Generated Images, NeurIPS 2023.\n\n- Other issues:\n\n(a) Table 4 does not specify the evaluation metric used, making the reported values ambiguous.  \n\n(b) The decision threshold for computing F1-scores in Tables 1–3 is not stated.  \n\n(c) While Appendix A.7.1 and A.7.3 define the pixel-level F1-score, the image-level F1 and accuracy (ACC) metrics referenced in Section 4 are not formally described. Providing their definitions would improve reproducibility and clarity.\n \n(d) Table 1 is not referenced in the main text.  \n\n(e) The methods “TruFor” and “DFVT” listed in Table 1 are not discussed or cited anywhere in the manuscript, and “DFVT” corresponding references are missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FgNvoXdn3n", "forum": "GcVvWAdQx7", "replyto": "GcVvWAdQx7", "signatures": ["ICLR.cc/2026/Conference/Submission16038/Reviewer_3Ufx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16038/Reviewer_3Ufx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760930369194, "cdate": 1760930369194, "tmdate": 1762926237694, "mdate": 1762926237694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a joint image forensics task by integrating tamper detection and AIGC detection, and presents a simple model with two separate branches for tamper detection and AIGC detection respectively. The model incorporates adversarial training to improve detection accuracy. Finally, a small mixed dataset containing both tampered and AIGC images is constructed, and the model demonstrates certain performance advantages on this dataset."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Proposes a joint image forensics task by integrating tamper detection and AIGC detection.\n- Presents a joint detection model for tamper and AIGC based on adversarial training.\n- Some experimental results show improvements compared with existing works."}, "weaknesses": {"value": "- Unreasonable motivation explanation: The paper states that \"Existing solutions often focus on either tampered or AIGC images, yet both can coexist in various contexts\", but fails to explain why tamper detection and AIGC detection need to be addressed jointly. What is the correlation between them? If tampering mainly refers to manual Photoshop manipulation, its artifacts and the generative artifacts of AIGC belong to two different categories. Why is it beneficial to learn them simultaneously? It is suggested that the authors conduct a more in-depth analysis of the problem background. Additionally, simply combining the two categories is not sufficient to support it as a contribution.\n- Insufficient technical innovation in the method: First, the use of adversarial learning to improve robustness/generalization is not novel. Moreover, the network modules, loss functions, and other components covered in the method section are all based on existing works. What is the authors' core original contribution?\n- The constructed dataset (Tampered AIGC images Dataset) lacks novelty: It is simply created by applying copy-move, splicing, inpainting, and other tampering operations to 10,000 AIGC images. On the one hand, the workload involved is relatively simple. On the other hand, compared with existing datasets, it does not demonstrate any unique attributes.\n- Unclear experimental setup: It is impossible to determine which training set the authors used. Furthermore, in Table 1, comparative methods such as TruFor and CNNSpot were trained on different training sets, making it unfair to compare them directly."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BxWJk04Avc", "forum": "GcVvWAdQx7", "replyto": "GcVvWAdQx7", "signatures": ["ICLR.cc/2026/Conference/Submission16038/Reviewer_kvhH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16038/Reviewer_kvhH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552418237, "cdate": 1761552418237, "tmdate": 1762926237359, "mdate": 1762926237359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for unified image forensics, designed to simultaneously detect both tampered images and AI-generated content (AIGC). The core of the proposed method is an adversarial learning architecture featuring a generator and a tri-branch discriminator. The three discriminator branches are specialized to identify distinct forensic traces: one for edge-based tampering artifacts, one for noise-based inconsistencies, and a third for features characteristic of AI-generated images.\nThrough adversarial training, the generator learns to produce increasingly realistic forgeries that challenge the discriminator, which in turn enhances the discriminator's ability to detect subtle manipulations. To facilitate evaluation, the authors construct a new benchmark dataset named \"Tampered AIGC images Dataset,\" which contains AI-generated images that have been subsequently manipulated.\nThe experimental results demonstrate that the proposed framework significantly outperforms existing aigi detection and imdl methods across various datasets, including standard tampering benchmarks (CASIA V2, IDM), GAN-generated datasets (ProGAN, StyleGAN, CycleGAN), and their custom combined dataset."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The primary merit of this paper lies in its focus on a highly relevant and pressing issue. The authors correctly identify the need for a unified framework that can simultaneously detect traditional image tampering and AI-generated content, a scenario that is increasingly common in real-world applications like social media analysis."}, "weaknesses": {"value": "Severely Flawed and Incomprehensible Methodological Description: The paper's core methodology, particularly the formulation of its loss functions and the adversarial training procedure, is riddled with inconsistencies, ambiguities, and conceptual errors. This makes the proposed method impossible to understand or reproduce, and it casts serious doubt on the validity of the implementation used to generate the results. For example, the authors define the adversarial loss L_adv in two conflicting ways. Equation (2) presents the standard GAN minimax objective, while Equation (12) (and Algorithm 1) redefines L_adv as a weighted sum of the discriminators' Binary Cross-Entropy (BCE) losses. \n\nThe authors seem to have a severe disconnect between their architectural diagram, their mathematical formulation, and their actual experimental implementation. They present the AIGC branch as a segmentation/localization module but appear to have trained and evaluated it as a simple image classifier.\n\nOutdated and Incomplete Experimental Scope: The paper's evaluation of AIGC detection is exclusively focused on GAN-based models (ProGAN, StyleGAN, CycleGAN). For a submission targeting ICLR 2026, the complete absence of experiments on diffusion models (e.g., Stable Diffusion, Midjourney)—the dominant class of generative models for the past several years—is a critical oversight.\n\nUnreasonable experimental setup: As a method that integrates AIGI and IMDL detection, the paper retrains other AIGI and IMDL baselines on mixed datasets for comparison, causing those methods to lose their basic domain-specific capabilities. For example, on GAN-generated datasets, methods like CNNSpot should achieve 90%+ accuracy. This undermines fairness. In addition, the paper lacks a basic comparison that fuses two domain-specific detectors via a combined decision, which makes its fusion strategy unconvincing and raises doubts about whether the problem formulation is meaningful in the first place."}, "questions": {"value": "Please refer to \"Weaknesses\" for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yBiIIMQsLT", "forum": "GcVvWAdQx7", "replyto": "GcVvWAdQx7", "signatures": ["ICLR.cc/2026/Conference/Submission16038/Reviewer_xb9y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16038/Reviewer_xb9y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877937105, "cdate": 1761877937105, "tmdate": 1762926236961, "mdate": 1762926236961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a dual-branch network that jointly performs AIGC detection and tampering localization via cross-attention and feature fusion. A new Tampered AIGC Dataset is introduced, built by applying copy-move, splicing, and inpainting to ProGAN/StyleGAN/CycleGAN outputs. The method outperforms existing forensic baselines, achieving 79% accuracy and 0.76 F1-score on a mixed-domain test set."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The technical approach is well-motivated. The dual-branch architecture with cross-attention enables mutual learning between global authenticity cues (AIGC vs. real) and local inconsistency signals (tampering boundaries).\n- Paper is clearly written and easy to follow."}, "weaknesses": {"value": "- Dataset uses GANs (ProGAN/StyleGAN) but omits modern diffusion-based generators (e.g., Stable Diffusion), limiting practical relevance.\n-  Edits are traditional (copy-move, etc.), not reflecting current AI-assisted editing tools (e.g., Generative Fill and Inpainting).\n- Misses comparison to recent AIGC-aware forensics (e.g., UniversalForensics, GenImage).\n- Lacks localization metrics (e.g., IoU)\n\n**Excessive length: The paper exceeds the 9-page limit.**"}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BPAVOjM07M", "forum": "GcVvWAdQx7", "replyto": "GcVvWAdQx7", "signatures": ["ICLR.cc/2026/Conference/Submission16038/Reviewer_3RVG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16038/Reviewer_3RVG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16038/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996262964, "cdate": 1761996262964, "tmdate": 1762926236640, "mdate": 1762926236640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}