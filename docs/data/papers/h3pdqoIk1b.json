{"id": "h3pdqoIk1b", "number": 18973, "cdate": 1758292489760, "mdate": 1763312715034, "content": {"title": "With Great Backbones Comes Great Adversarial Transferability", "abstract": "Advancements in self-supervised learning (SSL) for machine vision have enhanced representation robustness and model performance, leading to the emergence of publicly shared pre-trained backbones, such as $\\text{\\emph{ResNet}}$ and $\\text{\\emph{ViT}}$ models tuned with SSL methods like $\\text{\\emph{SimCLR}}$. Due to the computational and data demands of pre-training, the utilisation of such backbones becomes a strenuous necessity. However, employing backbones may imply adhering to the existing vulnerabilities towards adversarial attacks. Prior research on adversarial robustness typically examines attacks with either full ($\\text{\\emph{white-box}}$) or no direct access ($\\text{\\emph{black-box}}$) to the target model, but the adversarial robustness of models tuned on known pre-trained backbones remains largely unexplored. Furthermore, it is unclear which tuning configuration is critical for mitigating exploitation risks. In this work, we systematically study the adversarial robustness of models that use such backbones, evaluating $20,000$ combinations of tuning configurations, including fine-tuning techniques, backbone families, datasets, and attack types. To uncover and exploit vulnerabilities, we propose using proxy models to transfer adversarial attacks, fine-tuning them with various configurations to simulate different levels of knowledge about the target. Our findings show that proxy-based attacks can outperform strong query-based $\\text{\\emph{black-box}}$ methods with sizable budgets approaching the effectiveness of $\\text{\\emph{white-box}}$ methods. Critically, we construct a naive $\\text{``backbone attack\"}$, leveraging only the shared backbone, and show that even it can achieve efficacy consistently surpassing $\\text{\\emph{black-box}}$ and and closing in towards $\\text{\\emph{white-box}}$ attacks, thus exposing critical risks in model-sharing practices. Finally, our ablations reveal how tuning configuration knowledge impacts attack transferability.", "tldr": "SSL pre-trained backbones like ResNet and ViT face overlooked adversarial risks. This study reveals minimal tuning knowledge enables near-white-box attacks, exposing vulnerabilities in model-sharing practices.", "keywords": ["adversarial", "attack", "security", "evaluation", "robustness", "transferability", "safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/355f13abb691b7ed3661c01049d07d227dd7ff92.pdf", "supplementary_material": "/attachment/496a6133484b176de3f5223ebc2ea5c15115105c.zip"}, "replies": [{"content": {"summary": {"value": "This paper provide a detailed evalution on the impact of different levels of pre-knowledge of victim models, which are all regarded as grey-box scenarios in the paper. Results are provided competely in graphic presentation and indicate an intermediate performance between white box and black box attacks. The author also proposed a backbone attack which essentially use the identical pre-trained backbone and validated its effectiveness."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper provides a systemactic evaluation on the model hyperparameters in a finer perspective, including weights, trainind method, datasets, etc. These works validate some established consensus regarding adversarial attacks in an experimental way."}, "weaknesses": {"value": "The paper suffers from several major weaknesses, which can be categorized into 3 aspects: **Contribution**, **Experiments**, and **Writing**.\n\n1. **Contribution**. The research contribution of this work is severely limited. Specifically, the method proposed in this paper, i.e., backbone attack, has no methodological innovation as it simply uses PGD to minimize the cosine similarity between adversarial and original examples. Apart from the method, the innovation brought by this paper comes from the finer-grained hyperparameters regarding attacker knowledge (training, dataset, layer). In this regard, this paper more resembles a technical report in exploration for grey-box settings with the optimal transferability rather than a scientific paper. Furthermore, the paper cited an unpbulished paper ( Katzir & Elovici, 2021), which questioned the practicality of transfer attack. While such a statement remains questionable, the paper also failed to provide practical evaluations and demonstrate its supriority/contribution regarding this limitation.\n\n2. **Experiments**. Experiments lack depth, consistency, and a significant number of details, making it difficult to convey the effectiveness of their proposed method and subsequently convince readers. *Depth-wise*, all experiments remain shallow as most of them, although presented as main results, are simple ablation studies for comparing different hyperparameters. Does the optimal design exhibit universality? Why do such settings prevail among others? Besides, only 4 classification datasets (CIFAR/Oxford) are considered in the paper, without using the predominant Imagenet family. *Consistency-wise*, the results repeatedly jump from ASR to TSR, entropy, and divergence, etc., without sticking to a main/crucial metric to demonstrate the transferability. For example, a source-target table that intuitively shows the white-box, grey-box, and black-box results simultaneously, using ASR, as is done in most papers on transferability. *Detail-wise*, many important details are missing or lack a specific explanation. For example, the number of steps, step-size, and epsilon for PGD, the model used as proxy/target model and why, the specific reason for choosing sqaure attack. These missing details make it difficult to understand the insight which authors try to convey.\n\n3. **Writing.** The writing also further hinders reading. Despite some bizarre wording and repetitive notation, such as 'machine vision' (should be computer vision) and $ z(\\cdot )$-$ f(\\cdot) $, the paper lacks a tracable and self-contained storyline. To begin with, the motivation for adopting such a grey-box setting is unclear and unjustified given that transfer-based attacks have achieved promising results. The paper further proposes a frame work with several tunable hyperparameters for grey-box evaluation. Despite the scalability of such framework, the information presented is too scattered and lacks in-depth analysis/innovation. Specifically, some conclusions are even contradictory to the other: In line 375, the paper states that `the depth of the tuning is the least important knowledge for\nobtaining a transferable attack`, while in line 419, the paper writes that `attackers do need to have access to ... model classification depth to craft adversarial samples`, which requires further clarification."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "THlF2EhnL5", "forum": "h3pdqoIk1b", "replyto": "h3pdqoIk1b", "signatures": ["ICLR.cc/2026/Conference/Submission18973/Reviewer_9B3r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18973/Reviewer_9B3r"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761471398632, "cdate": 1761471398632, "tmdate": 1762931024878, "mdate": 1762931024878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We are withdrawing the submission because we are certain that at least two, and possibly three, of our reviews have been generated by AI. This is a very sad day for science and professional integrity."}}, "id": "z2ISHJXYmd", "forum": "h3pdqoIk1b", "replyto": "h3pdqoIk1b", "signatures": ["ICLR.cc/2026/Conference/Submission18973/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18973/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763312714266, "cdate": 1763312714266, "tmdate": 1763312714266, "mdate": 1763312714266, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the adversarial robustness of models fine-tuned from publicly available pre-trained backbones (e.g., ResNet, ViT) under a “grey-box” setting, where attackers have partial knowledge of the target model (e.g., backbone weights, fine-tuning mode). The authors introduce a simple “backbone attack” method that leverages the shared feature extractor to generate transferable adversarial examples, and conduct an extensive empirical study (≈20,000 experiments) across multiple datasets and configurations. The paper concludes that access to pre-trained backbone weights is sufficient to achieve near white-box performance in transfer attacks.\n\nWhile the paper is well written and the empirical analysis is extensive, the methodological novelty is limited, and several core findings overlap with conclusions already reported in prior works on transferable adversarial attacks. The work is primarily an observational study without introducing a truly new algorithmic insight or robust defense/attack strategy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic is relevant and timely, especially given the prevalence of shared pre-trained models in modern vision pipelines.\n\n2. The authors provide a large-scale, systematic empirical analysis across numerous backbone configurations, datasets, and tuning modes.\n\n3. The experimental observations (e.g., backbone access ≈ full-knowledge access) are clearly presented and supported by data."}, "weaknesses": {"value": "1. The proposed backbone attack is essentially a simplified variant of standard PGD that maximizes cosine distance in the feature space. While the systematic evaluation is valuable, the technical contribution is modest—no new attack or defense mechanism is introduced.\nThe paper’s main strength lies in empirical observations rather than algorithmic innovation. To improve impact, the authors could propose a new attack/defense method motivated by the findings (e.g., a method exploiting or mitigating backbone vulnerability), rather than stopping at descriptive analysis.\n\n2. The central conclusion—that access to backbone weights suffices for strong transferability (“access to backbone ≈ white-box”)—has been noted in several earlier studies, including [1] [2]. These works have already reported that shared backbone or feature-space similarity drives high transfer success across tasks. The current paper does not sufficiently differentiate its contributions from these prior findings or discuss what new insights are obtained beyond broader benchmarking.\n\n3.  The study focuses entirely on classification tasks using standard datasets (CIFAR-10/100, Oxford Pets, Flowers). This significantly limits the generality of the conclusions. Since many modern applications of pre-trained backbones involve detection, segmentation, VQA, and captioning, extending the evaluation to these domains would make the results more impactful and practically relevant.\n\n4. While the large-scale empirical analysis is thorough, the paper does not propose actionable outcomes for the community, such as defense strategies or guidelines for safe backbone sharing. The study would be more valuable if the authors leveraged their observations to derive practical insights or quantitative mitigation recommendations.\n\n[1] Transferable Adversarial Attacks on SAM and Its Downstream Models, In Neurips 2024.\n[2] AdvCLIP: Downstream-Agnostic Adversarial Examples in Multimodal Contrastive Learning, In ACM MM 2023."}, "questions": {"value": "Please see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sdx9Z62fSX", "forum": "h3pdqoIk1b", "replyto": "h3pdqoIk1b", "signatures": ["ICLR.cc/2026/Conference/Submission18973/Reviewer_zN99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18973/Reviewer_zN99"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974388258, "cdate": 1761974388258, "tmdate": 1762931024016, "mdate": 1762931024016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates gray-box adversarial transferability in realistic settings where attackers know only the publicly released backbone weights. It conducts a large-scale study across datasets, architectures, and fine-tuning regimes, and introduces a simple Backbone Attack that perturbs representations in the backbone feature space. Key findings show that adversarial examples crafted on proxy models outperform strong query-based black-box methods and approach white-box performance and the proposed Backbone Attack likewise exceeds black-box baselines and nearly matches white-box results."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Realistic gray-box formulation tied to today’s model-sharing ecosystem.\n2. Large-scale, systematic study across many backbones/datasets with consistent metrics.\n3. Backbone Attack is simple, reproduces easily, and approaches white-box.\n4. Clear empirical insight: fine-tuning mode dominates transferability; backbone weights is equivalent in effectiveness to possessing all tuning configurations about the target model."}, "weaknesses": {"value": "1. Black-box baselines: main-text details on query budgets/early-stopping are sparse; broader black-box comparisons would help, eg, transfer-based attack.\n2. Limited to classification and small datasets; unclear if results hold for detection/segmentation or larger-scale datasets.\n3. Reduced transfer on domain-specific datasets is noted but under-analyzed."}, "questions": {"value": "1. What explains the performance drop on domain-specific datasets (class imbalance, background bias, scale)? Any mitigation (augmentation, cross-domain proxies)?\n2. Does “backbone ≈ full config” still hold for deeper heads (>3 layers) or other SSL families?\n3. Could you add practical guidance for safer backbone sharing (eg. minimum disclosure)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o57TGkQ7c7", "forum": "h3pdqoIk1b", "replyto": "h3pdqoIk1b", "signatures": ["ICLR.cc/2026/Conference/Submission18973/Reviewer_PVk1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18973/Reviewer_PVk1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18973/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981257171, "cdate": 1761981257171, "tmdate": 1762931023447, "mdate": 1762931023447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates a grey-box threat model where attackers have partial knowledge of the target model’s construction (e.g., backbone weights, fine-tuning mode, dataset, depth) for the adversarial vulnerabilities of machine vision models fine-tuned from publicly available self-supervised learning (SSL) backbones. They have simulated over 20,000 adversarial transferability comparisons across 352 models from 21 backbone families, using various fine-tuning configurations and datasets. A key contribution is the “backbone attack”, which uses only the shared backbone (without tuning configuration knowledge) to generate adversarial examples via PGD in representation space. The authors show that this naive attack can outperform strong black-box attacks and approach white-box effectiveness. They also analyze which tuning configurations most affect transferability and model decision shifts."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It presents a backbone-only attack that is simple yet surprisingly effective, highlighting risks in model-sharing practices.\n\n2. It covers a wide range of experiments, such as 352 models, 4 datasets (CIFAR-10/100, Oxford Pets, Flowers), multiple SSL methods (SimCLR, SwAV, DINO, PIRL, etc.), and attack types (PGD, FGSM, Square). \n\n3. The paper is well-structured, with clear definitions of tuning configurations, proxy models, and attack metrics (ASR, TSR)."}, "weaknesses": {"value": "> The backbone attack is essentially PGD in representation space, similar to prior work in self-supervised adversarial training (e.g., NPR).\n\n> The idea of transferability from shared components has been explored in surrogate-based attacks and meta-surrogates.\n\n> While the paper exposes vulnerabilities, it does not suggest any concrete defenses or guidelines for secure backbone sharing. \n\n> All experiments are on classification tasks. It’s unclear whether the findings generalize to other domains (e.g., segmentation, detection, multimodal models).\n\n> The claim that backbone knowledge is “equivalent” to full configuration knowledge may be dataset-dependent. Domain-specific datasets (e.g., Oxford Flowers) show reduced transferability."}, "questions": {"value": "1. How does the backbone attack compare to NPR and other representation-space attacks in terms of formulation and effectiveness?\n\n2. Can you propose defense mechanisms or best practices for backbone sharing to mitigate the risks you expose?\n\n3. Do your findings generalize to non-classification tasks (e.g., segmentation, detection)? Have you tested any?\n\n4. Is there a lighter benchmark or subset of your framework that can be used for reproducibility by researchers with limited compute?\n\n5. Can you clarify the failure modes of backbone attacks—when do they fail to transfer effectively, and why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CusMoI7Bxg", "forum": "h3pdqoIk1b", "replyto": "h3pdqoIk1b", "signatures": ["ICLR.cc/2026/Conference/Submission18973/Reviewer_1KqY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18973/Reviewer_1KqY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18973/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762947551709, "cdate": 1762947551709, "tmdate": 1762947551709, "mdate": 1762947551709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}