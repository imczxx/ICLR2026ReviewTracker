{"id": "sAzwmLa1Lw", "number": 8453, "cdate": 1758084523617, "mdate": 1759897782987, "content": {"title": "DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning", "abstract": "Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce \\textbf{DRIFT} (\\textbf{D}issatisfaction-\\textbf{R}efined \\textbf{I}terative pre\\textbf{F}erence \\textbf{T}raining), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world \\textit{WildFeedback} datasets and synthetic \\textit{UltraFeedback} datasets achieve up to +6.23\\% (7B) / +7.61\\% (14B) on WildBench Task Score and up to +8.95\\% (7B) / +12.29\\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal.", "tldr": "", "keywords": ["large language models", "preference learning", "user feedback", "post-training", "self-improvement"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a4933af3f40f4476dcc9a3fac84432a428f564d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces DRIFT (Dissatisfaction-Refined Iterative Preference Training), a method for aligning large language models using abundant real-world user dissatisfaction (DSAT) signals instead of scarce human-labeled or positive feedback. DRIFT anchors on real DSAT negatives and dynamically samples positives from the evolving model, maintaining effective learning signals.\n\nExperiments on WildFeedback and UltraFeedback show DRIFT outperforms SPIN and IterDPO, achieving up to +12% win-rate gains and even surpassing GPT-4o-mini at 14B scale. The approach preserves response diversity and avoids gradient collapse. Theoretically, it guarantees non-vanishing gradients and stable preference margins."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Novel Supervision Source: Effectively leverages abundant real-world user dissatisfaction (DSAT) signals—an underutilized yet rich feedback channel—for scalable preference learning.\n\n* Strong Empirical Performance: Demonstrates consistent and significant gains over SPIN and IterDPO on WildBench and AlpacaEval2, even surpassing GPT-4o-mini at 14B scale.\n\n* Theoretical and Practical Robustness: Provides clear theoretical guarantees (non-vanishing gradients, stable preference margins) and preserves exploration diversity, avoiding mode collapse common in self-improving methods."}, "weaknesses": {"value": "* Label Noise Risk: The approach depends on automatically inferred DSAT labels (e.g., via SPUR), which may introduce noise or misclassifications that affect training quality.\n\n* Limited Generalization Evidence: Experiments are confined to Qwen2.5 models; results on other architectures or tasks would strengthen claims of broad applicability.\n\n* Insufficient Bias and Ethics Discussion: The paper minimally addresses potential biases in user dissatisfaction data or ethical implications of learning from negative, possibly adversarial, feedback."}, "questions": {"value": "1. How robust is DRIFT to noisy or ambiguous DSAT signals, especially when user dissatisfaction stems from subjective or stylistic preferences rather than factual errors?\n\n2. Could the authors provide insight into how DRIFT scales or adapts when applied to different model families or domains beyond conversational tasks (e.g., coding, reasoning, or multimodal settings)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NUb9Beb3Qz", "forum": "sAzwmLa1Lw", "replyto": "sAzwmLa1Lw", "signatures": ["ICLR.cc/2026/Conference/Submission8453/Reviewer_zGYz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8453/Reviewer_zGYz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761428439363, "cdate": 1761428439363, "tmdate": 1762920338167, "mdate": 1762920338167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce DRIFT, a direct (preference) alignment approach that leverages abundant signals from dissatisfactory user-chatbot interactions. The method samples preferred responses from the policy and employs real-world DSAT responses as non-preferred responses, in an iterative, on-policy pipeline.\nThe authors provide experimental validation of their method, theoretical guarantees, as well as insightful analysis of the semantic and reward coverage of the induced policy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. The method aims to leverage real-world interactions, allowing the modelling of more nuanced aspects of human preference found in satisfaction / dissatisfaction data.\n\nS2. The authors provide sound theoretical guarantees of vital properties of the method such as avoiding gradient degeneration and maintaining preference margins."}, "weaknesses": {"value": "W1. The experiments section could benefit from more clarity and details, especially in the diversity-reward coverage analysis\n\nW2. Rather inconclusive results on wildbench and alpaca-eval2, specifically at lower scales or later iterations"}, "questions": {"value": "# Questions and comments\n\n- Eq2, L184: it is unclear what \"l(*)\" refers to since it does not appear in the equation\n- Algorithm 1 (L186) is never referenced in the text\n- Section 4.1, L211: Could you please provide more details about the curation process of this subset? Were annotators employed? if so, what were they expertise? How many annotations were gathered per prompt or interaction? what was the inter-annotator agreement? what were the rubrics for such annotation? This kind of information would help the community replicate or expand this subset, since it seems it is an important component of the method recipe.\n- L243. The actual HF ID name would be better mentioned as a footnote or in the Appendix. In the main text, a short explanation of what this reward model is would be better for the story.\n- L263. There seems to be different reported percentages for 14B on AlpacaEval2, wrt to numbers in the abstract and the introduction (e.g. 12.11 vs 12.29 %)\n- L267, Table2. It would be beneficial to have a more detailed, realistic discussion of the results, pointing our also the limitations of the method. For instance, on AlpacaEval2, only 7B shows slight improvement, while 14B shows degradation; on both setups (full, controlled)\n- L297, Table2. The statement \"DRIFT maintains steady improvements\" only holds for the full setup, and doesn't for AlpacaEval2 14b, where it shows degradation after the first iteration\n- Table 3. it seems that the wrong cell was bolded in the AlpacaEval2--14B-LC column and AlpacaEval2--7B-LC column. Please add an explanation in the caption.\n- Section 4.3, L323. Could you please elaborate on how the embeddings were calculated? and how is the response reward calculated?\n- Figure 3. Could you please elaborate on what the axes are, and their scales, of the UMAP plot on the left? Example responses from across the region would help us understand the semantic coverage of the represented region"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2SdvOlzExR", "forum": "sAzwmLa1Lw", "replyto": "sAzwmLa1Lw", "signatures": ["ICLR.cc/2026/Conference/Submission8453/Reviewer_aMHA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8453/Reviewer_aMHA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761737360360, "cdate": 1761737360360, "tmdate": 1762920337710, "mdate": 1762920337710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "My understanding of the key contribution of the paper, is that performing iterative DPO while using an off-policy sample with negative feedback as the rejected response, and an on-policy generation from the previous model iteration as the chosen response (called DRIFT hereafter), produces better results than: \n- The inverse i.e. iterative optimization where an off-policy sample with positive feedback is used as the chosen response, and an on-policy generation from the previous model iteration is used as the rejected response (called SPIN hereafter) \n- On-policy self-play, where in each iteration multiple on-policy responses are generated and the model itself is used to choose the best and worst response as the chosen resp. rejected response (called IterDPO hereafter) \n\nThe “goodness” is measured via datasets UltraFeedback and WildFeedback, using WildBench (Elo) and AlpacaEval2 (Win rate). \n\nThis finding is relevant because off-policy samples with negative feedback are more ubiquitous than samples with positive feedback, in a context of real-world preference learning, i.e. beyond learning from human-annotated datasets, which are costly to produce."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The rationale for the proposed approach and experiments is well explained, i.e., the need to move beyond annotation, followed by the observation that in a real-world setting negative feedback is more abundant than positive feedback. \n\n- The experiments are well set up, including reruns based on the papers this paper builds on, i.e. a setup for SPIN, and a setup for IterDPO. \n\n- The paper features valuable analyses, both empirical and theoretical."}, "weaknesses": {"value": "- In the results we see that SPIN lags behind IterDPO, and IterDPO lags behind DRIFT. However, when looking at how the training samples are created in section 4.1 (Training Recipe), we see that for both IterDPO and DRIFT, in order to create the positive response, the model is guided, i.e. it is given the negative response and an instruction to improve. There does not seem to be an equivalent for SPIN: there is no instruction to create a “worse” negative relative to a reference point. I’m wondering to what extent this has an impact on the results."}, "questions": {"value": "I have one main question, related to the weakness put forward above:  would it be feasible to rerun (part of) the experiment without giving InterDPO and DRIFT the reference point and an improvement instruction, so that we get an “apples to apples” comparison with SPIN, where the on-policy generation is not guided?   \nPlease provide further motivation for the experimental choices, and if my question is deemed justified, additional insights from a more direct comparison may convince me to upgrade my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KescU8lXgx", "forum": "sAzwmLa1Lw", "replyto": "sAzwmLa1Lw", "signatures": ["ICLR.cc/2026/Conference/Submission8453/Reviewer_ukXJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8453/Reviewer_ukXJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912919401, "cdate": 1761912919401, "tmdate": 1762920337372, "mdate": 1762920337372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DRIFT, a preference-optimization framework that leverages abundant real-world dissatisfaction (DSAT) signals rather than relying primarily on scarce explicit satisfaction (SAT) labels. DRIFT anchors learning on negative examples and dynamically samples positives from the model itself, promoting continued policy improvement. The authors provide theoretical guarantees showing non-vanishing gradients and expected utility improvements, and empirically demonstrate strong performance over IterDPO and SPIN across WildFeedback, WildBench, and AlpacaEval2 benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a clear and practical motivation for leveraging DSAT signals, which are abundant in real-world user logs and therefore highly scalable compared to SAT-based approaches.\n\n- The empirical results are strong, showing consistent improvements over iterative baselines across multiple benchmarks.\n\n- The theoretical analysis is meaningful, demonstrating that DRIFT maintains a non-vanishing learning signal and guarantees improvement under local correlation assumptions with small update steps.\n\n- The method is simple and scalable, integrating easily with existing preference-optimization pipelines without significant architectural changes."}, "weaknesses": {"value": "- Baseline behavior appears inconsistent with prior literature: IterDPO and SPIN degrade after iteration here, whereas their original works report improvement. This raises questions about reproduction fidelity and tuning fairness.\n\n- The method's reliance on self-generated positives may amplify noise or bias, potentially causing training instability, especially in iterative settings.\n\n- Limited discussion on long-horizon stability; only up to iter-2 results are shown, making it unclear whether performance remains stable or oscillates over extended iterative training."}, "questions": {"value": "- Theoretically, DRIFT maintains a gradient with a lower bound, but could this persistent gradient signal introduce instability?\n\n- Have you evaluated more than two iterations? What happens when DRIFT is run for longer iterative cycles?\n\n- I do not fully understand how Z_all (g) is computed in line 323. What exactly does \"all responses\" refer to in this context?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Dm5FW5k0gI", "forum": "sAzwmLa1Lw", "replyto": "sAzwmLa1Lw", "signatures": ["ICLR.cc/2026/Conference/Submission8453/Reviewer_i1Ue"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8453/Reviewer_i1Ue"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8453/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970352246, "cdate": 1761970352246, "tmdate": 1762920337016, "mdate": 1762920337016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}