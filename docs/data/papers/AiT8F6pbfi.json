{"id": "AiT8F6pbfi", "number": 18024, "cdate": 1758283014711, "mdate": 1763635775076, "content": {"title": "Tracing and Reversing Edits in LLMs:  A Study on Rank-One Model Edits", "abstract": "Knowledge editing methods (KEs) are a cost-effective way to update the factual content of large language models (LLMs), but they pose a dual-use risk. While KEs are beneficial for updating outdated or incorrect information, they can be exploited maliciously to implant misinformation or bias. In order to defend against these types of malicious manipulation, we need robust techniques that can reliably detect, interpret, and mitigate adversarial edits. To that end, we introduce the tasks of tracing and reversing edits. We propose a novel method to infer the edited object entity, solely based on the modified weights, without access to the editing prompt or any other semantically similar prompts, with up to 99\\% accuracy. Further, we propose an effective and training-free method for reversing edits. Our method recovers up to 93\\% of edits, and helps regain the original model's output distribution without access to any information about the edit. This method can further be used to distinguish between edited and unedited weights. Our findings highlight the feasibility of tracing and reversing edits based on the edited weights, opening a new research direction for safeguarding LLMs against adversarial manipulations.", "tldr": "We propose two novel methods to trace an reverse rank-one mode edits based solely on the edited parameters", "keywords": ["Model Editing", "Knowledge Editing", "Countermeasures to Malicious Knowledge Editing"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9ba03c741b1442c224f51976e3fc7adf0d8a4b18.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes countermeasures against the potential misuse of model editing, namely **tracing and reversing edits**. The paper proposes using fixed inputs to guide the edited model in producing the target output. To approximate the update matrix, it employs rank-one approximations based on the highest singular values from a Singular Value Decomposition (SVD) of the edited matrix."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents a clear motivation and addresses an important research problem.\n* It proposes corresponding solutions for both **tracing** and **reversing** edits.\n* Extensive experiments are conducted to validate the effectiveness of the proposed methods."}, "weaknesses": {"value": "* The writing of the paper needs improvement.\n* The proposed **tracing and reversing edits** methods are only validated on the ROME series of approaches, and their effectiveness on other model editing methods remains unknown, which limits the applicability of the proposed approach.\n* The paper only considers **rank-one updates** for single pieces of knowledge, without addressing multi-knowledge or sequential knowledge updates.\n* The results of **ANALYSIS OF RANK-ONE APPROXIMATIONS** and **REVERSAL** appear to be uncorrelated and may even contradict each other."}, "questions": {"value": "* Lines 130–131: Why can’t the *unedited model* be the *original model*?\n* The results in Figure 4 show that the **maximum cosine similarity** differs significantly between GPT-J and Llama, yet their **reversal accuracy** is similar. How can this be explained?\n* In Figure 4, when *k = 1*, GPT2-XL achieves the highest **maximum cosine similarity** but the lowest **reversal accuracy**. How can this discrepancy be interpreted?\n* Based on the results in Sections 6.2 and 6.3, there seems to be no clear correlation between **rank-one approximation** and **reversal edits**. How do the authors explain this? Figure 4 indicates that smaller *k* values make the rank-one approximation closer to the update matrix, while Table 2 shows that larger *k* values yield better **reversal and editing accuracy**, and Table 4 shows that larger *k* values make the reversed model closer to the original model. These findings appear counterintuitive, as one would expect that a closer rank-one approximation to the update matrix would lead to the reversed model being closer to the original model. How do the authors reconcile this?\n* Can the results for **Qwen** in Figure 4 be shown?\n* Do the conclusions in lines 309–313 imply that the assumption at the beginning of Section 6.2 does not hold?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "i3m2LJS3OJ", "forum": "AiT8F6pbfi", "replyto": "AiT8F6pbfi", "signatures": ["ICLR.cc/2026/Conference/Submission18024/Reviewer_aKZH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18024/Reviewer_aKZH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760974843626, "cdate": 1760974843626, "tmdate": 1762927815554, "mdate": 1762927815554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the tasks of tracing and reversing malicious rank-one knowledge edits, relying on the modified model weights without access to the editing prompt or original weights. For tracing, the authors propose a novel method that trains the unedited weights with a fixed random input to decode the edited MLP projection matrix and accurately infer the edited object. For reversing, they introduce an efficient, training-free method using bottom-rank approximations to neutralize the edit and recover the model's original output distribution. The methods achieved high accuracy across various LLMs and showed generalization, highlighting the feasibility of developing robust countermeasures against adversarial knowledge manipulation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed methods for both tracing and reversing are designed to operate solely on the edited weights, without requiring access to the editing prompt, unedited weights, or any other information about the edit. This makes the countermeasures more practical for real-world defense against malicious editing.\n- The tracing method achieved high accuracy in identifying the edited object and showed strong generalization to out-of-distribution data and different editing methods (ROME and r-ROME). Similarly, the reversal method recovers up to 93% of edits and significantly restores the original model's output distribution.\n- The edit reversal technique, based on bottom-rank approximations, is training-free, making it highly efficient. This same technique can be repurposed to distinguish between edited and unedited weights by observing the number of unique predictions on unrelated text, offering a robust detection mechanism."}, "weaknesses": {"value": "- The effectiveness of the reversal (and analysis of rank-one approximations) is shown to be model-dependent. For example, the optimal rank k for bottom-rank approximation varies significantly across models (e.g., k=11 for GPT2-XL vs. k=15 for llama3 to achieve highest reversal accuracy), and the similarity of the top rank-one approximation to the update matrix is much lower for LLAMA3 than for GPT models. This suggests a need for model-specific tuning of the reversal hyperparameter. \n- The core of the methods, especially the reversal technique, relies on the assumption that the malicious edit is a rank-one update (like ROME or r-ROME). The effectiveness for different types of model edits is not explored. If the proposed method only works on rank-one update methods, the usability is limited since there are many other types of model editing methods like memory-based and constrained-tuning-based methods.\n- While the reversal accuracy is high (up to 93%). A qualitative analysis showed that even when reversed, the outputs, while semantically similar, are sometimes not identical to the original model's unedited outputs. Also, the decrease in editing accuracy is higher than the increase in reversal accuracy, indicating the method is better at removing the edit than fully recovering the original output distribution."}, "questions": {"value": "- How does the required model-specific tuning of the optimal rank k for bottom-rank approximation impact the practical deployability of the edit reversal method?\n- How can the reversal method be made less model-dependent?\n- To what extent does the non-identical nature of the model's reversed outputs (despite being semantically similar) and the accuracy imbalance between edit removal and original output recovery limit the utility of the reversal method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q7NL7vdD58", "forum": "AiT8F6pbfi", "replyto": "AiT8F6pbfi", "signatures": ["ICLR.cc/2026/Conference/Submission18024/Reviewer_QZJb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18024/Reviewer_QZJb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916232712, "cdate": 1761916232712, "tmdate": 1762927815150, "mdate": 1762927815150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes methods to detect and undo malicious or unintended knowledge edits. These edits modify a model’s internal parameters to change factual outputs, which can be useful for updating information but also pose security risks. The authors introduce two tasks: tracing edits and reversing edits. Their tracing method infers the edited object solely from altered weights, achieving high accuracy. Their reversal method uses bottom-rank approximations from singular value decomposition to remove edits without retraining. The study demonstrates strong generalization across 2 datasets and 4 models, showing that both tracing and reversal are feasible using only model weights."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a training-free framework for detecting and reversing malicious edits directly from model parameters, a new defense direction for LLM safety.\n- Experimental results show high accuracy and generalization across different models and datasets, suggesting good robustness.\n- The methods are computationally efficient and require no access to original weights or editing prompts, enhancing practical applicability for security auditing."}, "weaknesses": {"value": "- The study focuses only on rank-one edits, limiting applicability to other editing methods and scenarios like MEMIT, MEND, SERAC.\n- The motivation\nThe evaluation scope is restricted to controlled datasets and synthetic edits, leaving real-world validation uncertain.\n- The interpretability of why bottom-rank approximations work well for reversal is not fully explored, reducing theoretical clarity of the mechanism.\n- The motivation for reversing edits is questionable, since model editing is primarily designed to update or add new knowledge rather than to be undone; thus, the practical need for edit reversal appears limited. The paper does not compare its reversal method against simply reapplying the inverse or original edit, which would be a more direct and intuitive baseline for restoring model behavior"}, "questions": {"value": "- Can you clarify whether the proposed tracing and reversal techniques would still work, or how they might adapt, when applied to higher-rank or alternative editing methods such as MEMIT, MEND, or SERAC?\n- Have you considered testing their approach on real or naturally occurring edits (beyond controlled datasets) to assess its reliability in more practical or adversarial settings?\n- Can you provide a clearer explanation or empirical evidence for why bottom-rank components capture “pre-edit” information and effectively reverse edits?\n- What is the practical advantage of using the proposed reversal method over simply reapplying the inverse or original edit, especially given that model editing is typically meant to add or update knowledge rather than undo it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "55vlNCSsmk", "forum": "AiT8F6pbfi", "replyto": "AiT8F6pbfi", "signatures": ["ICLR.cc/2026/Conference/Submission18024/Reviewer_AH7s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18024/Reviewer_AH7s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959323077, "cdate": 1761959323077, "tmdate": 1762927814813, "mdate": 1762927814813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how to trace and reverse malicious edits in large language models that were modified using rank-one editing methods like ROME or r-ROME. It introduces two complementary defenses: (1) Tracing, which identifies the specific edited fact (object) directly from the edited weight matrix without needing the original weights or prompts, and (2) Reversing, which removes the malicious change by replacing the edited matrix with its bottom-rank singular value decomposition (SVD) approximation, effectively removing the edit signal concentrated in the top singular modes. Experiments across multiple LLMs show that tracing achieves near-perfect accuracy and reversal restores the model’s original behavior with high fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The proposed defense is practical and lightweight, requiring only access to the edited weights and no training data or edit prompts, making it suitable for real-world forensic use.\n+ The reversal approach is simple yet effective, using an interpretable SVD-based method that efficiently removes the edit signal while maintaining model integrity.\n+ The experimental validation is comprehensive and convincing, demonstrating strong performance across multiple models and datasets with clear quantitative results and ablation studies."}, "weaknesses": {"value": "- The method’s generality is limited, as it is evaluated only on single-layer, rank-one edits and may not extend to more complex, multi-layer, or non-rank-one scenarios.\n- The evaluation scope is narrow, focusing mainly on object recovery and KL divergence without exploring broader behavioral or capability effects after reversal."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Md0qgiRhZe", "forum": "AiT8F6pbfi", "replyto": "AiT8F6pbfi", "signatures": ["ICLR.cc/2026/Conference/Submission18024/Reviewer_8BSp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18024/Reviewer_8BSp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977974347, "cdate": 1761977974347, "tmdate": 1762927814106, "mdate": 1762927814106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}