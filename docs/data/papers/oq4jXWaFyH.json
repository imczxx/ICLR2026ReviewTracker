{"id": "oq4jXWaFyH", "number": 22910, "cdate": 1758336995724, "mdate": 1759896840376, "content": {"title": "The Natural Geometry of Code: Hyperbolic Representation Learning for Program Reasoning", "abstract": "State-of-the-art models for code representation, such as GraphCodeBERT, embed the hierarchical structure of source code into Euclidean space. This approach can lead to significant representation distortion, especially when embedding deep or highly branched hierarchies,limiting the models' ability to capture deep program semantics. We argue that the natural geometry for code is hyperbolic, as its exponential volume growth perfectly matches the tree-like structure of a code's Abstract Syntax Tree (AST), enabling low-distortion hierarchical embeddings. We introduce {HypeCodeNet}, a geometric deep learning framework that operates natively in hyperbolic space. Formulated in the numerically stable Lorentz model, its manifold-aware components include a hyperbolic embedding layer, a tangent space message-passing mechanism, and a geodesic-based attention module. On code clone detection, code completion, and link prediction, HypeCodeNet significantly outperforms existing Euclidean models, especially on tasks requiring deep structural understanding. Our work suggests that hyperbolic geometry offers a geometrically sound foundation for code representation, establishing hyperbolic geometry as a key to unlocking the structured semantics of code.", "tldr": "This work demonstrates that representing source code in its natural hyperbolic geometry, rather than Euclidean space, significantly enhances program reasoning models.", "keywords": ["Geometric Deep Learning", "Hyperbolic Representation Learning", "Source Code", "Program Reasoning", "Graph Neural Networks", "Abstract Syntax Tree (AST)"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7bbc70a051a726e1ba48f38fff74ab8288d324f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper explores the hypothesis that the hierarchical structure of source code, typically represented by Abstract Syntax Trees (ASTs), can be more effectively modeled in hyperbolic space than in traditional Euclidean geometry.\nBased on this intuition, the authors introduce HypeCodeNet, a hyperbolic graph neural network constructed in the Lorentz model, integrating manifold-aware components such as hyperbolic embeddings, tangent-space message passing, and geodesic attention.\nThe model‚Äôs performance is evaluated across multiple code understanding benchmarks, including BigCloneBench and POJ-104 for code clone detection, as well as CodeXGLUE for code completion and GitHub Java Call Graphs for link prediction.\nIn these experiments, HypeCodeNet is compared against nine baseline models and results show that HypeCodeNet demonstrates performance comparable to or exceeding that of established baselines across multiple benchmarks, particularly on tasks requiring structural reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-formulated hyperbolic representation architecture\nThe proposed model presents a mathematically well-grounded extension of GNNs into hyperbolic space, specifically tailored to capture hierarchical tree structures.\nUnlike prior works that primarily relied on exponential mapping, this paper explicitly defines the logarithmic map and integrates it into a novel geodesic attention mechanism, providing both theoretical and empirical justification for its design.\n\n2. Strong empirical results across multiple benchmarks\nThe model is comprehensively evaluated against nine baselines, spanning sequence-based, graph-based, and hybrid architectures, across three tasks and four datasets.\nHypeCodeNet consistently surpasses or matches the strongest baselines, demonstrating its robustness and generality across code understanding scenarios.\n\n3. Rich analytical evaluation\nThrough extensive ablation studies, the authors show that the proposed structure performs effectively even with extremely low-dimensional embeddings (e.g., 32 dimensions), and can outperform GraphCodeBERT with significantly fewer parameters.\nThis suggests that hyperbolic geometry provides an efficient inductive bias for hierarchical data."}, "weaknesses": {"value": "1. Limited input flexibility due to reliance on AST parsing\nThe model requires input to be parsed into a valid AST graph.\nConsequently, it cannot directly process unstructured code snippets, natural language prompts, or free-form textual descriptions.\nThis constraint implies that the method may need additional components to handle real-world data where parsing is incomplete or ambiguous, limiting the model‚Äôs applicability in broader scenarios.\n\n2. Relatively slow training and inference\nAs reported in Appendix J, despite having fewer parameters, HypeCodeNet involves multiple nonlinear hyperbolic operations and iterative computations.\nThis results in $1.5-2\\times$ slower training and approximately 20% longer inference time compared to baselines.\nWhile this trade-off is understandable given the geometric complexity, it reduces throughput and may hinder deployment in latency-sensitive environments.\n\n3. Lack of theoretical discussion on gradient convergence\nThe paper empirically demonstrates stable training and overall convergence, supported by techniques such as Riemannian gradient clipping and curvature annealing.\nHowever, it does not provide a formal proof of convergence, nor does it include a theoretical analysis of convergence behavior. Also, the convergence bounds of the iterative Fr\\‚Äôechet mean computation remain unspecified.\nWhile the empirical results indicate reliable convergence in practice, the lack of theoretical guarantees leaves the model‚Äôs stability only experimentally supported and may limit its applicability in more rigorous settings."}, "questions": {"value": "**Points to justify**\n\nI do not consider those weaknesses to significantly diminish the overall contribution of the paper.\nHowever, I believe the following points require further justification and clarification.\nIf the authors can adequately address the questions raised below, I am willing to reconsider my evaluation. \n\n1) As ‚Äúreasoning‚Äù has taken on a different connotation in recent LLM literature, the term ‚ÄúProgram Reasoning‚Äù in the title may be somewhat misleading. It would be clearer to emphasize that the model focuses on encoder-based code comprehension, for example by adopting a title such as ‚ÄúHyperbolic Representation Learning for Encoder-based Code Comprehension.‚Äù\n\n2) While the proposed model makes noteworthy efforts to ensure fair comparison, the baseline models and target tasks used are largely outdated, originating from 2022 or earlier. Moreover, recent advances in code understanding have been increasingly driven by large language models (LLMs), most of which follow a decoder-only paradigm, fundamentally differing from the encoder-based architecture proposed in this paper. Therefore, a discussion on the methodological timeliness, empirical relevance, and potential extensibility of the proposed approach is necessary to position the work within the modern landscape.\n\n3) Incorporating CodeT5+ [1], the successor to CodeT5, as an additional baseline would strengthen the experimental comparison and provide a more up-to-date benchmark context.\n\n4) Regarding the Code Completion task, it appears that the evaluation in this paper functions more as a Cloze Test, since it involves predicting a few masked tokens rather than performing true line-level completion. In fact, CodeXGLUE also treats this as a Cloze Test when evaluating encoder-only models such as CodeBERT, while Code Completion is evaluated using decoder-based models like CodeGPT. If there is a specific reason for evaluating under the ‚ÄúCode Completion‚Äù split of CodeXGLUE despite this distinction, please provide further justification. Regardless of this reasoning, aligning the task naming with the corresponding dataset would improve clarity and consistency.\n\n5) The geodesic distance appears to be incorrectly defined. For a vector $u \\in \\mathcal{L}_c^d$, $d_c(u,u)=\\frac{1}{(-c)^{\\frac{1}{2}}}\\arcosh(-c<u,u>_L)=\\frac{1}{(-c)^{\\frac{1}{2}}}\\arcosh(-1)$. However, $\\arcosh$ is defined only for $x \\ge 1$. Also, $<p, log_p^c(h)>_{\\mathcal{L}} \\neq 0$. Thus, I suspect that $<x, x>_{\\mathcal{L}}$ should be $-\\frac{1}{c}$ in the definition of $\\mathcal{L}_c^d$.\n\n6) Including a proof, or at least an outline of the key steps, demonstrating that the exponential map is the inverse of the logarithmic map is necessary for mathematical completeness and clarity.\n\n7) Equation (40), which approximates $-c<u,v>_L to 1+\\frac{-c}{2} \\left\\| u_E - v_E \\right\\| ^2$, should be further justified.\n\n**Suggestions**\n\n1) As an upper bound reference, it would be valuable to additionally report the performance of a recent LLM-based baseline, even if only with a smaller model variant.\n\n2) More comprehensive experimentation would strengthen the paper. Reporting additional results on the Defect Detection task from CodeXGLUE and on Cloze Test experiments across languages beyond Java and Python would provide stronger empirical support. If such experiments are infeasible, clarifying the reasons would be appreciated.\n\n3) It would be helpful to include a diagram, perhaps in the appendix, illustrating how attention is computed within each layer.\n\n\n4) The term \"central node\" is used but not clearly defined. If it does not carry a specific meaning, consider replacing it with a clearer phrase such as \"node to update\" or explicitly defining it for clarity.\n\n5) It would be preferable to avoid the excessive use of bold text, as it may visually interfere with paragraph structure. Using italics or underlines for emphasis would improve readability and consistency.\n\n6) Since the initial vector $z_v^E \\in \\mathbb{R}^d$ and $T_{o_c}\\mathcal{L}_c^d \\in \\mathbb{R}^{d+1}$, the dimensionality increases. Therefore, the operation described should be referred to as an injection or extension rather than a projection.\n\n7) On p.15, if $q_v^{(k)}=0$, there appears to be no need to include the bias term or the matrices $W_{Q,k}^{(l)}$ and $W_{K,k}^{(l)}$ in the computation. In this case, it would be equivalent to performing a learnable weighted sum over $m_{u \\to v}^{(l)}$.\n\n8) The section formatting is inconsistent. For instance, Section 3.2 introduces points as (1), (2), and (3), but later paragraphs switch to different numbering styles or omit numbering altogether, while the description of the output layer appears separately in Section 3.4. The numbering and formatting should be standardized for consistency, maintaining a uniform scheme or using sub-subsections if hierarchical structure is intended.\n\n**Typo**\n- p.3, line 126: \"base point\" should be written as ``base point''.\n- Appendix A: The letter 'A' in the title should also be enclosed as `A'.\n- Markdown-style emphases such as *text* or **text** appear in the appendix and should be removed for formal consistency.\n\n[1] Wang, Y., Le, H., Gotmare, A., Bui, N., Li, J., & Hoi, S. (2023, December). CodeT5+: Open Code Large Language Models for Code Understanding and Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (pp. 1069-1088)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KJNffqydRw", "forum": "oq4jXWaFyH", "replyto": "oq4jXWaFyH", "signatures": ["ICLR.cc/2026/Conference/Submission22910/Reviewer_Huu9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22910/Reviewer_Huu9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717712223, "cdate": 1761717712223, "tmdate": 1762942433387, "mdate": 1762942433387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HypeCodeNet, a novel hyperbolic Graph Neural Network (GNN) that learns source code representations directly in hyperbolic space, rather than traditional Euclidean embeddings. Motivated by the hierarchical, tree-like nature of Abstract Syntax Trees (ASTs), the authors argue that hyperbolic geometry better captures program structure with low distortion.\nHypeCodeNet operates in the Lorentz model for stability and integrates manifold-aware message passing, curvature annealing, and Riemannian optimization. Across three tasks: code clone detection, code completion, and function call link prediction, the model achieves strong gains over GraphCodeBERT, UniXcoder, and CodeFORMER. Ablations further confirm that performance improvements stem mainly from the hyperbolic geometry rather than architectural tweaks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality (Novel geometric framing):\nThe paper introduces a foundational geometric shift in code representation, from Euclidean to hyperbolic space, supported by clear theoretical intuition. It‚Äôs the first to present an end-to-end hyperbolic framework for code reasoning, filling a notable gap in the literature.\n\nQuality (Sound formulation & strong empirical results):\nThe proposed model is mathematically rigorous, leveraging Lorentz manifolds, log/exp maps, Riemannian Adam, and curvature annealing for stability. Results across multiple benchmarks (BigCloneBench, CodeXGLUE, GitHub Java Corpus) show consistent SOTA performance, with up to 3‚Äì5% improvement over strong baselines. Ablations convincingly isolate geometry as the key performance driver.\n\nClarity & Significance:\nThe paper is well-organized, with detailed geometric explanations and intuitive visualizations (e.g., Figure 1). Its results suggest non-Euclidean geometry may be a next paradigm for program representation, opening a new research direction bridging geometric deep learning and code understanding."}, "weaknesses": {"value": "Reproducibility & implementation accessibility:\nThe paper does not mention public release of code or datasets, and implementation details (e.g., curvature annealing schedule, manifold dimensionality tuning) are deferred to appendices. Reproducibility may be difficult without explicit scripts or pretrained models.\n\nLimited scope of benchmarks:\nAlthough the model excels on line-level code completion and clone/link tasks, all evaluations remain static-structure-centric. The framework isn‚Äôt tested on dynamic or generation tasks (e.g., code repair, test generation), which would test whether hyperbolic embeddings generalize beyond AST reasoning."}, "questions": {"value": "Curvature tuning:\nHow sensitive is model performance to the final curvature value? Would a mixed-curvature or adaptive manifold (e.g., product manifolds combining Euclidean + hyperbolic subspaces) perform better?\n\nGeneralization beyond AST-based tasks:\nCan HypeCodeNet generalize to non-AST graph structures, such as interprocedural dependency graphs or text-conditioned code generation? Have you explored transfer learning between programming languages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kNoYH0tuEH", "forum": "oq4jXWaFyH", "replyto": "oq4jXWaFyH", "signatures": ["ICLR.cc/2026/Conference/Submission22910/Reviewer_rLfX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22910/Reviewer_rLfX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979933270, "cdate": 1761979933270, "tmdate": 1762942433152, "mdate": 1762942433152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HypeCodeNet, a geometric deep learning framework for code representation learning that operates natively in hyperbolic space, formulated under the Lorentz model.\nThe authors argue that source code‚Äôs Abstract Syntax Tree (AST) inherently possesses hierarchical and tree-like structures that are better represented in negatively curved manifolds than in Euclidean space.\nHypeCodeNet integrates manifold-aware components ‚Äî a hyperbolic embedding layer, a tangent-space message-passing mechanism, and a geodesic-based attention module ‚Äî trained with Riemannian optimization and curvature annealing.\nAcross three standard benchmarks (clone detection, code completion, link prediction), it consistently outperforms strong Euclidean baselines such as CodeBERT, GraphCodeBERT, and CodeFORMER, supporting the claim that hyperbolic geometry aligns more naturally with program hierarchies."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tStrong conceptual motivation grounded in geometric theory\nThe paper convincingly argues that the exponential volume growth of hyperbolic space matches the hierarchical expansion of ASTs. The ‚Äúdistortion‚Äù argument is well supported, referencing Bourgain‚Äôs theorem and previous work on low-distortion tree embeddings.\n2.\tTechnically principled formulation\nBy adopting the Lorentz model instead of the unstable Poincar√© ball, the authors ensure both numerical stability and Riemannian differentiability, enabling a deep stack of manifold layers with standard GPU parallelization.\n3.\tWell-designed architecture bridging geometry and semantics\nThe ‚Äúlog‚Äìaggregate‚Äìexp‚Äù message-passing paradigm and geodesic-aware attention are elegant and grounded in geometric consistency. The method preserves curvature constraints while incorporating multi-head attention and layer normalization ‚Äî non-trivial achievements in hyperbolic neural design."}, "weaknesses": {"value": "Theoretical insufficiency in proving ‚Äúnaturalness‚Äù of hyperbolic geometry.\nThe core claim ‚Äî that ‚Äúhyperbolic geometry is the natural geometry of code‚Äù ‚Äî remains conceptually persuasive but not theoretically rigorous. No formal quantification of distortion or curvature‚Äìhierarchy correlation (e.g., tree embedding distortion bounds). Missing mathematical analysis of curvature c ‚Üí embedding fidelity or proofs showing convergence of representations to low-distortion manifolds.\nAdding a distortion vs. curvature empirical curve or formal derivation would strengthen the argument substantially.\n\nLimited comparison with non-Euclidean or hybrid geometries.\nThe paper frames hyperbolic geometry as the only alternative, but mixed-curvature or spherical‚Äìhyperbolic hybrid embeddings could better capture cross-function semantics.\nA control experiment with mixed curvature or product manifolds (ùîº√ó‚Ñç) would clarify whether pure hyperbolic geometry is indeed optimal.\n\nOverlooked semantic‚Äìsyntactic decoupling.\nThe model tightly couples AST topology with embedding curvature but does not explicitly distinguish between semantic relations and syntactic nesting.\nThis may limit generalization to tasks involving cross-file or semantic code reasoning. Integrating semantic edges (DFG, CFG) or cross-function attention could make the representation more complete."}, "questions": {"value": "1.\tProvide a formal distortion analysis: derive or empirically approximate the embedding distortion as a function of curvature.\n2.\tInclude ablation across manifold types: compare Lorentz, Poincar√©, and product manifolds.\n3.\tAnalyze training dynamics of curvature annealing: curvature vs. epoch plot.\n4.\tExplore semantic augmentation: integrating CFG/DFG edges to test the universality claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tuA9097LF9", "forum": "oq4jXWaFyH", "replyto": "oq4jXWaFyH", "signatures": ["ICLR.cc/2026/Conference/Submission22910/Reviewer_enmz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22910/Reviewer_enmz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005189327, "cdate": 1762005189327, "tmdate": 1762942432871, "mdate": 1762942432871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "‚Äã‚ÄãThis paper argues that hyperbolic geometry is a better inductive bias for program representations than Euclidean space, because Abstract Syntax Trees (AST) are tree-like and expand exponentially. The authors introduce HypeCodeNet, a Lorentz-model hyperbolic GNN that introduces manifold aware operations to the embedding layer and message passing attention module.  Across code detection, line-level code completion, and link prediction on call graphs, HypeCodeNet achieves SOTA results and exceeds strong Transformer and graph baselines, demonstrating that hyperbolic geometry offers meaningful advantages for modeling hierarchical code structures."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper demonstrates careful engineering work to make hyperbolic deep learning stable and practical for code representation:\n\n1. **Consistent improvements across diverse tasks demonstrate general utility and significance.** The approach shows modest but consistent gains over CodeFORMER in semantic understanding (BigCloneBench: +1.2% F1) and code completion (~1% across metrics), while achieving substantial improvements in link prediction (+5.0% AUC, +5.2% Hits@10). This variation suggests the method particularly excels at structural reasoning tasks.\n\n2. **High-quality experimental evaluation.** The authors compare against 9 baselines spanning sequence-based, graph-based, and hybrid architectures across multiple datasets per task. The ablation studies (Section 4.5) provide clear empirical evidence that performance gains stem from hyperbolic geometry rather than other architectural choices.\n\n3. **Original technical contributions beyond existing hyperbolic models.** HypeCodeNet advances beyond prior work through: (i) use of the numerically stable Lorentz model, (ii) geodesic-based attention mechanism, and (iii) curvature annealing for stable training. These design choices address known challenges in hyperbolic deep learning.\n\n4. **Strong empirical validation of theoretical claims.** The distortion analysis (Figure 7, Appendix) provides compelling evidence that hyperbolic embeddings preserve AST hierarchical structure with significantly less distortion than Euclidean alternatives, supporting the paper's central thesis. \n\n5. **Well-written paper with clear presentation.** The overall thesis is compelling and the technical approach is explained systematically. The experimental section is particularly well-organized. (Though Figure 1 could be improved, as noted in the weaknesses section)"}, "weaknesses": {"value": "1. **Insufficient Geometric Analysis of Learned Representations.** While the paper provides compelling empirical evidence that hyperbolic geometry improves performance, it lacks rigorous geometric validation of the learned embeddings. The authors cite Yang et al. (2023) for hyperbolic representation learning, yet that work explicitly cautions that hyperbolic embeddings do not automatically guarantee hierarchical structure and demonstrates cases where geometric properties may not align with semantic hierarchies. While Figure 8 provides valuable qualitative intuition, it requires accompanying quantitative validation. The current visualization alone cannot confirm that the hierarchical structure is preserved beyond visual inspection.\n\n**Suggested quantitative analyses:**\n   - **Distance-to-Origin Analysis:** Provide quantitative measurements of node distances from the origin in the Lorentz model. Specifically, verify that root nodes consistently map near the origin (small $d(v, o_c)$ ) while leaf nodes map toward the boundary, with statistical significance tests across multiple ASTs.\n   - **Hierarchical Structure Validation:** Compute the correlation between graph depth and hyperbolic distance to origin. Following Nickel & Kiela (2018), report Spearman's rank correlation coefficient between tree depth and $\\|h_v\\|_L$.\n   - **Statistical Validation:** Consider adding histograms of distance distributions by tree depth and statistical tests confirming that parent-child pairs maintain consistently smaller distances than arbitrary node pairs at the same depth.\n\n2. **Figure 1 needs significant improvement to effectively convey the paper's core contributions.**\n   \n   **Part (a) - The geometric intuition is too generic and could represent any hierarchical structure.** \n   The authors might consider:\n   - Showing a concrete code snippet (5-10 lines) with its actual AST and node labels displaying real AST node types (e.g., IfStatement, ForLoop, Variable)\n   - Providing a side-by-side comparison with Euclidean embeddings to demonstrate distortion differences\n   \n   **Part (b) - The 'log-aggregate-exp' visualization feels cluttered and unfocused.** \n   To improve clarity, the authors could:\n   - Use progressive panels that track a specific node through each transformation step\n   - Add visual differentiation through color coding (e.g., parent nodes in blue, children in green, message flow as arrows)\n   - Emphasize the tangent plane with shading or perspective to help readers understand why operations must occur in this local Euclidean space\n   \n   While these are suggestions, addressing the abstract nature of the current visualization would significantly strengthen the paper's accessibility."}, "questions": {"value": "1. The visualization in Figures 7 and 8 is compelling.\n   - How many ASTs contributed to the aggregated statistics in Figure 7? (Figure 8 appears to show a single example)\n   - What is the size range of AST(s) in your evaluation (min/max/median nodes and depth)?\n   - Does the distortion advantage hold consistently across different code complexity levels?\n\n2. How does memory consumption scale compared to Euclidean models, especially for very large ASTs?\n\n3. The paper is missing a citation for CodeFORMER, which appears to be your primary baseline.  Liu et al. (2023) \"CodeFORMER: A GNN-Nested Transformer for Source Code Representation\" (MDPI Electronics 12(7):1722). Please confirm this is the correct reference and add it to your bibliography."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XbROOhkTbP", "forum": "oq4jXWaFyH", "replyto": "oq4jXWaFyH", "signatures": ["ICLR.cc/2026/Conference/Submission22910/Reviewer_4Dzo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22910/Reviewer_4Dzo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22910/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762467810534, "cdate": 1762467810534, "tmdate": 1762942432670, "mdate": 1762942432670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}