{"id": "IDJabw9ri3", "number": 12002, "cdate": 1758205139601, "mdate": 1759897540136, "content": {"title": "Gaussian Process Priors for Boundary Value Problems of Linear Partial Differential Equations", "abstract": "Working with systems of partial differential equations (PDEs) is a fundamental task in computational science. Well-posed systems are addressed by numerical solvers or neural operators, whereas systems described by data are often addressed by PINNs or Gaussian processes. In this work, we propose Boundary Ehrenpreis--Palamodov Gaussian Processes (B-EPGPs), a novel probabilistic framework for constructing GP priors that satisfy both general systems of linear PDEs with constant coefficients and linear boundary conditions and can be conditioned on a finite data set. We explicitly construct GP priors for representative PDE systems with practical boundary conditions. Formal proofs of correctness are provided and empirical results demonstrating significant accuracy and computational resource improvements over state-of-the-art approaches.", "tldr": "GP priors for general systems of linear PDEs with constant coefficients and linear boundary conditions.", "keywords": ["Gaussian Processes", "Partial Differential Equations", "Regression"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/85fad306e36bddbed78bc3b0eb0885ec8738f90c.pdf", "supplementary_material": "/attachment/ff71318d2c2a025e91a71d83a225839c86695e5b.zip"}, "replies": [{"content": {"summary": {"value": "The proposed methodology makes use of Ehrenpreis–Palamodov boundary-repecting basis functions to construct highly constrained Gaussian Process prior for PDE inference. The method is primarily used on the wave equation, showing its high accuracy. The method is compared to neural operators, 3D wave equation, and shown to be used with circular sectors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is written with good precision. \n\n- The proposed methodology is theoretically sound and mathematically grounded, answering one of the principle challenges in applying ML to problems in PDEs.\n\n- Great care is taken in proposing a principled approach.\n\n- Extensive exposition and testing is provided in the appendix."}, "weaknesses": {"value": "- Much attention is given to describing how to constrain basis functions to respect boundary conditions. Could some amount of space be dedicated to showing how this is incorporated into the GP? i.e. how do we assign variances to each basis term?\n\n- Arbitrary polygonal boundaries are discussed. These are of great importance in engineering applications. The construction of the basis seem to be carried out by hand. Could it be stated more clearly if there is an algorithm for computing these bases? Including pseudocode to this end would be helpful. If such an algorithm does not exist, how could one go about constructing an approximate automated method for this?\n\n- Handling boundary conditions is the principle objective of this paper, however, only simple boundaries and geometries are considered. For practical relevance, it would be important to include an example of non-trivial geometry.\n\n- How does this methods fair with the incorporation of observational data; it seem the method is only used for solving PDEs?"}, "questions": {"value": "- Can more clarity be given to (1) and explain the relevance of half spaces here and how this relates to the PDEs of interest? Perhaps some foreshadowing could help readers navigate this section.\n\n- As the PDE is assumed to be known as well as the boundary conditions, can this problem be rephrased as an inverse problem for recovering initial conditions? Is so, stating this explicitly would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8MwuyB2vEb", "forum": "IDJabw9ri3", "replyto": "IDJabw9ri3", "signatures": ["ICLR.cc/2026/Conference/Submission12002/Reviewer_2LpB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12002/Reviewer_2LpB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760960285289, "cdate": 1760960285289, "tmdate": 1762922992912, "mdate": 1762922992912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**(Nb: when I submitted this review, curly braces did not render. If they are not visible in the final review, please read the text as if there were braces around the sets. If they are visible, ignore this message.)**\n\n\nThe submission is the next iteration in the work of Gaussian process priors based on Ehrenpreis-Palamodov theory. More concretely, it extends the work by Härkönen et al. (2023), who develop Gaussian process priors that satisfy linear PDEs exactly (but no boundary conditions) to priors that satisfy linear PDEs and linear boundary conditions exactly. The approach goes (roughly) as follows:\n- Solutions of linear ODEs with constant coefficients can be expressed in terms of basis functions: e.g., for $y'' - 3y' + 2y = 0$, we have $y = w_1 e^t + w_2 e^{2t}$ for some coefficients $w_1$ and $w_2$. Letting $w_1$ and $w_2$ be Gaussian variables, this $y$ becomes a parametric Gaussian process (GP) constrained to the linear ODE.\n- For partial differential equations (PDEs), a similar setup is possible, but it is more involved in that the basis functions are an uncountably infinite set that needs truncation. For example, for the heat equation $u_t = u_{xx}$ in 1d, any solution can be approximated arbitrarily well by a (complex) linear combination of finitely many elements of $\\{e^{z^2 t + zx} \\mid ~ {z \\in \\mathbb{C}}\\}$, which is due to the Ehrenpreis--Paladomov theorem. Choosing a finite number of such $z \\in \\mathbb{C}$, and linearly combining them with Gaussian weights, we get a parametric GP constrained by the heat equation. The hyperparameters of this parametric GP can be optimised with the usual Gaussian process machinery. This has been done by Härkönen et al. (2023).\n- The submission now extends this to the case of linear PDEs with linear boundary conditions. This is based on the idea that for a finite selection of $z \\in \\mathbb{C}$, which leads to a finite number of functions, the weights can be constrained to automatically satisfy the boundary conditions. For the heat equation in 1d and boundary condition $x=0$, we get the collection $\\{e^{z^2t + zx} - e^{z^2t - zx} \\mid z \\in \\mathbb{C} \\}$. The submission also discusses wave equations, Neumann conditions, and higher-dimensional settings, and it is shown that the restricted candidate functions (eg $\\{e^{z^2t + zx} - e^{z^2t - zx} \\mid z \\in \\mathbb{C} \\}$) are dense in the space of solutions of each respective PDE of interest, similar to the original Ehrenpreis--Paladomov theorem. \n\nNumerical results show that this closed-form enforcement in the proposed method beats not enforcing the boundary conditions in closed form, which is necessary if one relies on previous work.\n\n\n**Summary of my recommendation:**\nI find this work promising, but the paper seems incomplete, by which I mean that the method and the experiments need a more careful explanation, and that the experiments miss some important questions. Concrete examples follow under \"Weaknesses\" below. In general, I believe that the algorithm has potential, but I find the current version not sufficiently accessible, not even to the subset of the machine-learning community that focuses on the intersection of differential equations and GPs (like myself). Therefore, I recommend rejection."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "In principle, I really like the line of work that derives parametric GPs so that they automatically satisfy PDEs, including the submission, but also prior work by Härkönen et al., Lange-Hegermann, and others. What I like especially about the submission and the closely related Härkönen et al. is that it applies to underspecified PDE problems. Enforcing essentially unique functions would not give much space for machine-learning algorithms to, loosely speaking, \"do their thing\". \nAs such, I believe the setup in this submission fits well into the literature on physics-informed machine learning.\nAnd even though I have strong enough concerns about the presentation to recommend rejecting this work, I expect a future version of this manuscript to be a valuable contribution to the machine learning literature."}, "weaknesses": {"value": "Even though I think that the general idea of the contribution has potential, I think that the submission has considerable gaps in presentation and numerical evaluation:\n\n**A lack of mathematical precision inhibits readability:**\nIt is clear from reading the manuscript that the authors are skilled mathematicians; however, the presentation lacks the technical precision that I would expect from this kind of work:\n\n- When writing \"linear span\" (eg in Theorems 2.1 or 3.6), do I assume correctly that we mean linear combinations with complex coefficients? Distinguishing real and complex coefficients is important because complex linear combinations would stand in contrast to the real-valued Gaussian coefficients used to define a parametric Gaussian process (line 106, line 168). In Line 144, it would be good to specify whether $w_j$ are some fixed real/complex coefficients or whether they are Gaussian random variables, and it would also be good to explicitly state that the number and location of the $z_j$ are chosen by users. After reading the full manuscript, I have answered these questions myself, but I would prefer if they were unambiguous at the first occurrence. In general, since Ehrenpreis--Paladomov theory is not trivial, I would recommend carefully defining each variable at each occurrence (eg the imaginary unit in Line 223 comes a bit out of nowhere), even if this makes the notation in this paper somewhat heavier.\n\n- I think it would help accessibility if the word \"basis\" were used a bit more carefully: For example, why would it be clear (at this point of the manuscript) that Equation 3 is indeed a basis, which line 164 states, but only Theorem 3.6 shows?\nSimilarly,  the manuscript regularly calls a set of functions \"basis\" before the theorem that shows this set of functions spans a space of interest (eg in Lines 223 right before Theorem 3.9; also, linear independence is not discussed but I think it should be). I recommend being more precise with which features are known about a set of functions at the time of introducing it, eg in line 164, line 215, line 233, but also on other occasions. \n\n- The submission contains three types of sets of functions that serve the purpose of function approximation. There are finite expansions (eg $f$ in line 144), uncountably infinite sets of functions (Theorem 3.6) and countably infinite sets of functions (Theorems 3.9 and 3.11). It is not stated explicitly that the finite expansion in Line 144 serves only as intuition for what follows after, determining what eventually leads to Equation 3. Furthermore, the step from uncountably infinite to countably infinite sets of functions in line 224 needs to be explained more thoroughly, both in the main paper and in Appendix J.1 (eg that this set is a basis is only determined later). It would be nice if a future version of this manuscript had a longer explanation at these links.\n\nI understand that the level of technical rigour in machine learning is a bit lower than in mathematics publications. However, making the technical derivations in Section 3 more precise would make the paper easier to understand.\nTo realise these suggestions, I would recommend showing fewer examples in the main paper but explaining each of them in more detail. For example, I found Appendix C to be required reading for understanding Section 3, and if Example 3.8 and Theorem 3.9 are explained thoroughly, Example 3.10 and Theorem 3.11 could almost be left to the reader.\n\n\n\n**Important numerical considerations are omitted:**\nIn my opinion, the paper does a good job of deriving those sets of functions that span the spaces of PDE solutions. However, computational considerations are not discussed adequately: the only mention of constructing a Gaussian process model is in Line 105, and this reference only mentions optimising $z_j$ and the hyperparameters of the prior over each $w_j$ with stochastic gradient descent. The submission does not explain which loss is used. The choice of loss matters because while hyperparameters for $p(w_j)$ may be straightforward to optimise with marginal likelihood calibration (typical for GPs), optimising $z_j$ is closer to inducing points respectively GP latent variable models, for which the state of the art uses more sophisticated techniques (typically, based on variational inference). Independent of what is actually used in the experiments, not discussing these considerations at all is a major shortcoming of the submission.\nThe accuracy of the regression tasks (and difficulty of the optimisation problem) also depends on the number and location of basis functions, and the effects of this approximation need to be studied, at least empirically. Regarding baselines, the present work only compares to Härkönen et al. (and neural operators), but other GP-PDE combinations are not benchmarked. For example, I would have expected physics-informed GPs to be a baseline, possibly in combination with inducing points; something like:\n\n> Hamelijnck, Oliver, Arno Solin, and Theodoros Damoulas. \"Physics-informed variational state-space Gaussian processes.\" Advances in Neural Information Processing Systems 37 (2024): 98505-98536.\n\nAs is, the submission does not embed as well into the literature on GPs and PDEs as is necessary for me to recommend publication at ICLR.\n\n\n\n**Related work:**\nFinally, I would like to discuss the choice of related work mentioned in the submission. I consider the following weakness significantly less important than the ones above, but I would like to mention it regardless.\nI appreciate the thorough related work section, but I think that in the current version, it optimises a bit too much for breadth and too little for depth. For example, other than also belonging to the class of PDE-ML algorithms, I am a bit surprised that neural operators and PINNs feature prominently in the related work. I understand the desire to make the work accessible to a broad audience at ICLR, but currently, this is to the detriment of discussing closely related work adequately. Instead of neural operators and PINNs, I would have appreciated more depth on the explanation of how the present work differs from Lange-Hegermann (2018 & 2021), Jidling et al. (2017), and the rest of the citations in Lines 042. I also think some additional references might deserve closer discussion: \nLatent force models (Alvarez et al.), especially in their SDE variation (Hartikainen et al.), also construct ODE/PDE-informed priors for GP regression. They're doing something slightly different from the proposed work, but I think they're more closely related than many of the works cited in Section 4. See also Bosch et al., who use these linear-ODE priors for probabilistic solvers for nonlinear ODEs:\n\n> Alvarez, Mauricio, David Luengo, and Neil D. Lawrence. \"Latent force models.\" Artificial intelligence and statistics. PMLR, 2009.\n\n> Hartikainen, Jouni, and Simo Särkkä. \"Sequential Inference for Latent Force Models.\" Proceedings of The 27th Conference on Uncertainty in Artificial Intelligence (UAI 2011), Barcelona, Spain, July 14-17, 2011. 2011.\n\n> Bosch, Nathanael, Philipp Hennig, and Filip Tronarp. \"Probabilistic exponential integrators.\" Advances in Neural Information Processing Systems 36 (2023): 40450-40467."}, "questions": {"value": "The following are minor questions and comments that do not really affect my score, but might be helpful to incorporate into a revised version of this manuscript.\n\n- Line 40: What is meant by \"bilinear covariance structure\" in this context?\n- Line 104: I appreciate that the main paper explains the concepts using a simplified setting of Ehrenpreis--Palamodov theory. It would be nice if the manuscript could discuss to what extent the simplifying conditions of Theorem 2.1 hold. For example, I find it non-obvious whether the characteristic equations of an ODE have a multiplicity (eg the example given in Line 090 does), but for all PDEs studied in this submission, multiplicities seem to be no issue. \n- Line 162: I understand that Gröbner bases were important for prior work in this space, but in the present submission, all bases were possible to derive with pen and paper (as is stated in Line 815). Since terminology like \"Gröbner basis\" or \"syzygy module\" is likely not common vocabulary at ICLR, why are lines 163 and Appendix B important for this submission? \n- The submission regularly states that traditional GP methods and traditional PDE solvers suffer from the curse of dimensionality (eg Line 127, Line 409) and that the proposed algorithm does not. I am wondering whether more nuance is required here, because while the proposed method does not need to place a grid to enforce boundary conditions, the number of terms in the basis functions grows with dimension. For example, compare Example 3.7 (2d) to Example 3.8 (1d). Also, the derivation of basis functions becomes more laborious in higher dimensions. Line 953 states:\n\n    > The calculations extend easily to arbitrary dimensions, but the formulas are cumbersome\n\n    which makes it sound like extension to arbitrary dimensions is not as easy in practice. I recommend being more careful with claims about avoiding the curse of dimensionality (I find the contribution of exact enforcement strong enough on its own)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cjToSBgukx", "forum": "IDJabw9ri3", "replyto": "IDJabw9ri3", "signatures": ["ICLR.cc/2026/Conference/Submission12002/Reviewer_r3kn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12002/Reviewer_r3kn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761822219412, "cdate": 1761822219412, "tmdate": 1762922992591, "mdate": 1762922992591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Gaussian process method for solving boundary value problems of linear partial differential equations with constant coefficients. The key contribution is a framework that encodes boundary conditions directly into basis elements, allowing the construction of GP priors whose domain automatically aligns with the PDE solution space. By imposing Gaussian priors on parameters of the solution's functional form (where the functional form is known but parameters are unknown), the authors obtain a Gaussian process that exploits the specific PDE structure. The paper departs from standard collocation approaches and includes proofs of correctness and convergence. The authors also propose a hybrid approach that combines their method with data-driven techniques for handling curved boundaries."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "## Strengths\n\n1. **Novel GP construction**: The approach cleverly constructs kernels (from basis expansion) so that the GP domain naturally overlaps with the PDE solution space, which is an elegant solution to the problem.\n\n2. **Strong theoretical contributions**: The paper provides rigorous derivations, theorems, and proofs (including proofs of correctness and convergence), which is commendable as theory often lags behind in this field. [however I didn't check the proof details]\n\n3. **Exploits problem structure**: Unlike PINNs and collocation-based GP methods, this approach makes significant and intelligent use of the linear PDE structure, which should lead to better performance for the target class of problems.\n\n4. **Clear exposition**: The examples (particularly at lines 90, 171-178) effectively communicate the intuition behind the method and help readers understand the approach.\n\n5. **Solid work within scope**: For the specific area of linear PDEs with constant coefficients and limited data, this is useful and well-executed research."}, "weaknesses": {"value": "## Weaknesses\n\n1. **Requires bespoke treatment**: The method requires tailored, hand-crafted basis functions for each different PDE, with most calculations done manually. This is a stark departure from methods like PINNs that can be applied more automatically. While this leads to better performance, it significantly limits ease of use and scalability.\n\n2. **Limited scope**: The method explicitly targets linear PDEs with constant coefficients, which is a restrictive problem class.\n\n3. **Hybrid approach undermines main contribution**: Section 3.3's hybrid approach (line 255) doesn't integrate well with the elegant exact solution framework. If data is needed for curved boundary pieces, the claim of exactness is compromised, which diminishes the value proposition.\n\n## Minor points\n4. **Computational cost unclear**: For the hybrid approach (Section 5.3, line 420), it's unclear whether the method still suffers from the curse of dimensionality mentioned in Remark 3.1. The computational cost analysis needs clarification.\n\n5. **Missing references**: Several relevant Gaussian process methods for PDEs should be cited (some suggestions have been provided by the reviewer). It will be better to position the work relative to existing Gaussian process techniques for PDEs (e.g., work by Andrew Stuart, Houman Owhadi, Mark Girolami, Philipp Hennig, Jon Cockayne). The novelty is unclear without reading deeper into the paper."}, "questions": {"value": "I have listed structured questions (with help of LLM) in the above weakness part. I am going to say here my honest thoughts when reading the paper as it presents, and hopefully this can help you understand how a new reader perceives your paper. These raw feelings are genuine and I hope they provide a more human-to-human communication and contexts for the structured question above.\n\n# Review of \"Gaussian Process Priors for Boundary Value Problems of Linear Partial Differential Equations\"\n\n\n## Initial Impressions (Abstract)\n\nI'm reviewing this paper on Gaussian process priors for boundary value problems of linear partial differential equations. From the name \"EP”, the approach seems to be specific to linear PDEs. \n\nThe abstract feels very short and doesn't touch upon existing Gaussian process techniques, of which I'm aware of quite a few. After reading the abstract, I'm curious to see how this differs from the Gaussian process work by Andrew Stuart and the Houman Owhadi group at Caltech, and also from the probabilistic numerics group in the UK (Mark Girolami, Philipp Henning, and Jon Cockayne). The novelty isn't very clear from the abstract alone.\n\n## Early Sections (Lines 31-48)\n\nAfter reading line 37, I see that the authors are moving away from the collocation approach, which is interesting. \n\nAt the end of line 48, I think citations are needed. Most Gaussian process methods for solving PDEs still rely on certain characterizations of the PDEs that are not exact—they are exact at the collocation points where the information operator is conditioned, but the notion of \"physics constraint\" needs clarification. Please provide references to specify exactly which approaches you're discussing.\n\nMy understanding is that for the linear case, collocation GP also works really well, and the problem is basically solved. So I don't see what's new from line 31 yet.\n\nLine 76 Point number three—the proof of correctness and convergence—is intriguing. In most of the work I've seen, the theory always lags a little behind. I'm curious what new contribution is introduced here.\n\nLine 78 I also wonder why this is framed as a regression model. Neural operators learn many instances, so I need to see whether you're doing single-instance inference or many-instance inference.\n\n## Problem Formulation (Line 90)\n\nI really like line 90. The example helped me understand the problem being solved here. This is interesting: the functional form of the solution is known, but some parameters of this functional form are unknown. By imposing a Gaussian prior on these parameters, you effectively get a Gaussian process. This feels like a very smart construction of the kernel so that the solution space automatically aligns with the domain of the Gaussian process. I wouldn't be surprised if this performs better than PINNs because it exploits the particular PDE structure. Essentially, you've structured a specific Gaussian process that is engineered to overlap with the PDE solution space.\n\n## Key Contribution (Lines 111-130)\n\nLine 111 is the real contribution and the key idea. Now it's very clear. Line 130 further explains the problem from line 111: the boundary conditions are encoded into the basis elements.\n\nAfter reading Equation 3, I understand the intuition. The boundary conditions can be encoded as basis functions, but this requires tailored or bespoke treatment for each PDE. You need to be clever in finding these bases.\n\nThe examples from lines 171 to 178 are very helpful in understanding the intuition. I wish there were a more automatic way to do this. I understand that right now, most of the calculation is done by hand. But this is a stark difference from PINNs, right? PINNs can be used very easily—they basically remove a lot of the necessity for smart calculations. There are pros and cons. I wouldn't be surprised if, after all this basis construction work, the performance is better. That's a no-brainer.\n\n## Section 3.2 and Beyond\n\nI'm being quick in Section 3.2 because I think I understand the gist of what's going on. I'm unable to check all the exact detailed mathematics, but at this point, I have enough intuition. You also claim there are many proofs in the appendix that I wasn't able to check.\n\nIn Section 3.3, I didn't understand in what sense the method is \"hybrid.\" Is it hybrid because you have both flat and curved pieces of boundary? If you're using data for the curved pieces, then you've somewhat lost your claim of things being exact. I think Sections 3.3 actually diminish the value proposition—you can plug in collocation points, but it doesn't integrate quite as well with the elegant solution you're proposing. This comment refers specifically to line 255.\n\n## Literature Review\n\nFor the literature search, since you mentioned PINNs and Gaussian processes, I think neural operators aren't that related to this work, but Gaussian process methods used in the PINN style should be referenced. I see you already have a few familiar names here: Philipp Hennig, Mark Girolami's group, Philipp Wenk, Andrew Stuart, Houman Owhadi, and some others. I'll suggest a few representative papers that I think you should cite as well.\n\nHowever, your approach is significantly different from these. Those are Gaussian process counterparts of PINNs. Yours is very different—it makes significant use of the linear problem structure.\n\n## Experiments\n\nFor the experiments section, I wouldn't be surprised by the results, given such heavy use of the problem structure and the mathematics to make it work. You do have experiments on the hybrid approach, but the hybrid approach doesn't quite align with your main storyline.\n\nIn Section 5.3, line 420: What's the computational cost of your hybrid approach? I would imagine your hybrid method still suffers from the curse of dimensionality mentioned in your Remark 3.1, unless I'm missing something.\n\n## Overall Assessment\n\nThis is good work—very solid. I appreciate that you have all the derivations, examples, and theorems. The scope is limited, as it explicitly targets linear partial differential equations with constant coefficients. For that specific area where you have some data, I think this is useful.\n\n\n## References \n\nCockayne, J., Oates, C., Sullivan, T. and Girolami, M., 2017, June. Probabilistic numerical methods for PDE-constrained Bayesian inverse problems. In AIP Conference Proceedings (Vol. 1853, No. 1, p. 060001). \n\nChen, Y., Hosseini, B., Owhadi, H. and Stuart, A.M., 2021. Solving and learning nonlinear PDEs with Gaussian processes. Journal of Computational Physics, 447, p.110668.\n\nWenk, P., Gotovos, A., Bauer, S., Gorbach, N.S., Krause, A. and Buhmann, J.M., 2019, April. Fast Gaussian process based gradient matching for parameter identification in systems of nonlinear ODEs. In The 22nd International Conference on Artificial Intelligence and Statistics (pp. 1351-1360). PMLR.\n\nLi, Z., Yang, S. and Wu, C.J., 2024. Parameter inference based on Gaussian processes informed by nonlinear partial differential equations. SIAM/ASA Journal on Uncertainty Quantification, 12(3), pp.964-1004.\n\nLong, D., Wang, Z., Krishnapriyan, A., Kirby, R., Zhe, S. and Mahoney, M., 2022, June. AutoIP: A united framework to integrate physics into Gaussian processes. In International Conference on Machine Learning (pp. 14210-14222). PMLR."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n02AfP2jNc", "forum": "IDJabw9ri3", "replyto": "IDJabw9ri3", "signatures": ["ICLR.cc/2026/Conference/Submission12002/Reviewer_yyWM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12002/Reviewer_yyWM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761849244608, "cdate": 1761849244608, "tmdate": 1762922991885, "mdate": 1762922991885, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new probabilistic framework, Boundary Ehrenpreis-Palamodov Gaussian Processes (B-EPGPs), for designing Gaussian Process (GP) priors that are constrained to satisfy a linear partial differential equations (PDEs) system with constant coefficients and a linear boundary condition system. The work builds upon existing \"physics-constrained\" methods like EPGP, which use the Ehrenpreis-Palamodov theorem to create priors whose realizations are exact solutions to the PDE system (represented as combinations of exponential-polynomial functions $e^{x \\cdot z}$). However, existing methods have a hard time handling boundary conditions in a clean, analytical way. They often rely on conditioning the model on a large number of boundary data points—a strategy that’s both inefficient and quickly runs into the curse of dimensionality. The key idea behind B-EPGP is to build a new Gaussian Process basis that automatically satisfies the boundary conditions. For a given boundary, the method identifies groups of related frequencies—called “fibers,” $S_{z’}$—from the PDE’s characteristic variety. It then builds special linear combinations of basis functions, $\\sum_{z \\in S_{z’}} w_z e^{x \\cdot z}$, that automatically satisfy the boundary conditions. The whole process is framed as a syzygy computation problem, which can be solved symbolically using Gröbner bases. It’s a neat and elegant way to handle what’s usually a messy part of the problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "It provides a general, \"physics-constrained\" framework for a broad and critical class of PDE problems. By ensuring both the PDE and its boundary conditions are met exactly, B-EPGP offers a model that is more reliable, data-efficient, and computationally faster than methods that treat boundaries as soft penalties or data-fitting problems."}, "weaknesses": {"value": "1. The algorithm for polygonal boundaries is based on an iterative process of the form \"Repeat until S stabilizes.\" The authors say, \"These steps in . . . do not generally terminate.\" While it is mentioned that for their examples it does terminate, the non-fulfillment of this important practical condition is not fully brought out. The conditions under which the process is guaranteed to terminate are not indicated.The infinite slab example is presented as a case of non-termination, but the solution is found via a \"shortcut\" (manually selecting a discrete Fourier basis $\\xi \\in \\mathbb{Z}$) rather than as a direct output of the described iterative algorithm. This makes the polygonal algorithm feel less general and more \"artisanal\" than the very robust halfspace method.\n\n2. The paper states that the Gröbner basis computations for finding fibers and syzygies have \"negligible complexity in practice\". This is justified by noting the computations are on the (small) operator equations, not the (large) data. While this holds for the single-equation PDEs (heat, wave) used in the examples, it is not clear if this optimism holds for large systems of coupled PDEs (e.g., linear elasticity or Maxwell's equations). The complexity of Gröbner basis algorithms can be severe, and the \"negligible\" claim should be better qualified by specifying the types of PDE systems for which it has been verified.\n\n3. The paper extends the framework to inhomogeneous systems in a straightforward way: the solution is written as $u = u_p + v_{homo}$, where B-EPGP handles the homogeneous part, $v_{homo}$. However, this approach depends on being able to find a particular solution, $u_p$. While the authors describe a spectral method for doing this, their main examples use very simple cases where $u_p$ is just the forcing function $f$, which makes the demonstration a bit less convincing. This part of the contribution feels underdeveloped. If finding $u_p$ is itself a complex numerical task, it could become the new bottleneck and re-introduce numerical errors, which could undermine the \"exact\" nature of the B-EPGP framework."}, "questions": {"value": "1. Could the authors be more precise about the termination conditions for the iterative algorithm for polygonal boundaries? Are there known classes of polygonal domains (e.g., convex polygons, or domains with angles that are rational multiples of $\\pi$) for which termination is guaranteed? For the non-terminating case (like the infinite slab example), is the manual selection of a discrete basis the intended workflow, or can the algorithm be modified to output this basis directly?\n\n2. Could the authors comment on the practical scalability of the symbolic pre-computation step (computing fibers and syzygies) when moving from single-equation PDEs to systems of coupled linear PDEs? For instance, have they explored the feasibility for systems like time-harmonic Maxwell's equations or linear elasticity?\n\n3. The demonstration for inhomogeneous systems uses a known, simple particular solution $u_p$. How does the framework perform when $u_p$ is non-trivial and must be approximated using the proposed spectral method? Is there a risk that a numerical approximation of $u_p$ introduces significant errors, thereby negating the benefit of B-EPGP's exactness on the homogeneous part?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VrjmfcVnqJ", "forum": "IDJabw9ri3", "replyto": "IDJabw9ri3", "signatures": ["ICLR.cc/2026/Conference/Submission12002/Reviewer_qz1s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12002/Reviewer_qz1s"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12002/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889916025, "cdate": 1761889916025, "tmdate": 1762922991516, "mdate": 1762922991516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}