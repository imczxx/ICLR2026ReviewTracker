{"id": "ziP9zetlLp", "number": 22317, "cdate": 1758329589427, "mdate": 1759896872694, "content": {"title": "Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment", "abstract": "Recent AI trends seek to align AI models to learned human-centric objectives, such as personal preferences, utility, or societal values. Using standard preference elicitation methods, researchers and practitioners build models of human decisions and judgments, to which AI models are aligned. However, standard elicitation methods often fail to capture the true cognitive processes behind human decision making, such as the use of heuristics or simplifying structured thought patterns. To address this limitation, we take an axiomatic approach to learning cognitively faithful decision processes from pairwise comparisons. Building on the vast literature characterizing cognitive processes that contribute to human decision-making and pairwise comparisons, we derive a class of models in which individual features are first processed with learned rules, then aggregated via a fixed rule, such as the Bradley-Terry rule, to produce a decision. This structured processing of information ensures that such models are realistic and feasible candidates to represent underlying human decision-making processes. We demonstrate the efficacy of this modeling approach by learning interpretable models of human decision making in a kidney allocation task, and show that our proposed models match or surpass the accuracy of prior models of human pairwise decision-making.", "tldr": "We propose a new way to align AI with human decision-making by modeling the cognitive processes behind choices, with an axiomatic approach: features are processed with learned rules, then aggregated with a fixed rule e.g., Bradley-Terry.", "keywords": ["Human-Centric AI", "Moral Preference Elicitation", "Axiomatic Analysis", "Interpretable Machine Learning"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/775ef87b95ccc3c625444e45f715e2748462eb46.pdf", "supplementary_material": "/attachment/b610908549d8d46e0e013acab59daf09c558645e.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces a principled framework for learning \"cognitively-faithful\" models of human decision-making from pairwise comparisons. The central thesis is that for AI alignment to be successful, particularly in high-stakes domains, the models learning human preferences must not only be predictively accurate but also reflect the underlying cognitive processes. The authors propose a two-stage model architecture: editing stage applies feature-level transformation rules to capture how humans simplify and process information (e.g., applying thresholds, ignoring features), and the dominance testing stage aggregates these processed features to produce a choice. The key contribution is a rigorous axiomatic framework from which this two-stage structure is formally derived. The authors prove that their general framework can be specialized to recover classic models like logistic and probit regression under stronger assumptions. The method is evaluated on real-world and synthetic data from the kidney allocation domain, where it demonstrates competitive accuracy against a range of baselines while providing highly interpretable, participant-level insights into decision heuristics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Principled Axiomatic Foundation:** The paper's greatest strength is its axiomatic approach. Rather than proposing an ad-hoc model architecture, the authors derive their two-stage framework from a set of simple axioms of choice. This provides a strong theoretical grounding for the model class and elegantly justifies its structure. It moves the field from simply using models that fit to reasoning about why certain model structures are appropriate.\n\n2.  **Cognitive Plausibility and High Interpretability:** The model architecture directly maps onto well-documented cognitive strategies like heuristic-based decision-making. The empirical results persuasively demonstrate this strength, revealing plausible and nuanced individual-level heuristics."}, "weaknesses": {"value": "1.  **Empirical Score:** Evaluation is limited to the kidney allocation setting and synthetic benchmarks. The cognitive heuristics and moral trade-offs in this domain are quite specific. The paper's broad claims about providing a general framework for cognitively-faithful modeling would be significantly more convincing if broadly validated in less moral or more perceptual decision domains.\n\n2.  **Unvalidated Interpretability:** The central claim of the paper is \"cognitive fidelity\" and the resulting interpretability. While the learned models and their visualizations are highly plausible, these are ultimately claims about human psychology. Without user studies to confirm that people find these explanations understandable, trustworthy, and more insightful than those from other interpretable models (e.g., partial dependence plots from a GAM), the \"interpretability\" remains a feature of the model rather than a proven benefit for human-AI collaboration.\n\n3.  **Idealized Axioms vs. Bounded Rationality:** The axiomatic framework describes an \"ideal\" decision-maker who is, for instance, perfectly transitive (Weak Transitivity). This is a useful and standard starting point for theoretical modeling. However, a vast literature in behavioral economics is dedicated to studying systematic *violations* of such axioms. A weakness of the current work is that it does not deeply engage with the implications of these violations. The framework is used to model choices that *fit* the axioms, but not to diagnose or characterize interesting deviations from them."}, "questions": {"value": "Primary questions already posed as weaknesses earlier. I was also curious about:\n1. **Quantifying Axiom Violations:** Can you report empirical rates of transitivity or complementarity violations in your human data, and how these affect model accuracy or interpretability?\n2. **Reverse Use for Diagnosis:** Could your framework be used inversely to *detect* when individuals systematically violate the axioms by analyzing poor model fit or structured residuals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Rj9ncQRUOZ", "forum": "ziP9zetlLp", "replyto": "ziP9zetlLp", "signatures": ["ICLR.cc/2026/Conference/Submission22317/Reviewer_xeec"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22317/Reviewer_xeec"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760593717970, "cdate": 1760593717970, "tmdate": 1762942166843, "mdate": 1762942166843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for *cognitively-faithful modeling of human decision processes* in pairwise comparisons. The goal is to address that current learning and AI alignment methods focus on outcomes of human decisions (such as pairwise preferences or rewards) but do not capture the *cognitive processes* that produce them.\n\nThe authors present an axiomatic approach to human pairwise decision-making based on five principles (complementarity, weak transitivity, codomain span, noninteractive, and conditional compositionality). From these axioms, they develop a *two-stage model*. First, they define a set of *editing rules* that modify or simplify feature information (such as thresholding and tallying). Then, they introduce a *dominance testing rule* that combines the edited features into a probabilistic choice. The resulting model framework unites some classical models as special cases and seeks to connect interpretability with cognitive realism.\n\nEmpirically, the authors evaluate a kidney allocation task using real human data and synthetic data simulating known heuristics. Their models achieve comparable or superior accuracy to baselines (logistic, GAM, MLP, decision trees, etc.) while offering interpretable insight into human heuristics. The paper concludes by discussing implications for AI alignment and emphasizing the importance of faithfulness to human reasoning in high-stakes moral decisions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper identifies (ttbomk) an under-explored problem: existing preference-learning frameworks rarely attempt cognitive fidelity, even though interpretability and trust depend on it. The argument is well contextualized in AI alignment and moral decision-making literature.\n\n* The axiomatic formulation is intuitive and well-organized. The derivation from axioms to model families (logistic, monotonic, conditional GAM) is clear. \n\n* The paper builds on the work of Gigerenzer, Kahneman, and others to formalize heuristics in a way that is compatible with ML, representing an ambitious and valuable interdisciplinary effort.\n\n* The kidney allocation domain is well-motivated, connects to prior moral-AI work (e.g., Sinnott-Armstrong et al. or Keswani et al.), and emphasizes the importance of process alignment beyond just accuracy.\n\n* The examples of individual decision-makers (e.g., participant P4, simulated DM1) are insightful. The figures clearly illustrate how the proposed model captures thresholding and conditional heuristics that logistic or tree-based models overlook.\n\n* The authors openly acknowledge limitations and the idealized nature of their axioms, which strengthens the credibility of the paper."}, "weaknesses": {"value": "*  Although the example is well motivated and exemplifies that the theoretical formulation works, the empirical validation focuses on a single decision dataset with a small sample size ($n=15$ in study I and $n=40$ in study II). It is unclear how the approach scales to richer, higher-dimensional, or continuous domains.\n\n* Although the paper claims \"cognitive faithfulness,\" it mainly assesses accuracy and qualitative interpretability. There is no quantitative measurement of *faithfulness* (such as user validation or alignment with verbal reports).\n\n* The paper's analysis of the literature is comprehensive and strong, but it could incorporate more discussion on computational cognitive modeling (e.g., modern differentiable cognitive architectures [[1](https://www.nature.com/articles/s41598-022-15679-5), [2](https://proceedings.mlr.press/v216/hamalainen23a/hamalainen23a.pdf)]) to better emphasize its contribution and even identify potential synergies.\n\n* Releasing the code would help the reader replicate the learning setup. There are anonymous ways of sharing it, such as web pages or adding it to OR as supplementary material. \n\n* Minor: Table 1 is quite dense and poorly organized (fonts are too small, column alignment is unclear)."}, "questions": {"value": "1. Could the authors provide quantitative evidence that the learned editing rules correspond to reported human heuristics (e.g., through previous studies showing that feature-importance aligns with qualitative interviews)?\n2. Is the approach computationally scalable for large-feature or neural representations (e.g., embedding-based preference learning)?\n3. Could cognitively faithful modeling inform fairness-sensitive decisions, where human heuristics themselves may be biased?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FHmSr0tz3E", "forum": "ziP9zetlLp", "replyto": "ziP9zetlLp", "signatures": ["ICLR.cc/2026/Conference/Submission22317/Reviewer_6AEu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22317/Reviewer_6AEu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761735252104, "cdate": 1761735252104, "tmdate": 1762942166627, "mdate": 1762942166627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a cognitively-faithful class of decision-making models derived from simple axioms over binary choices. A decision is modeled as a two-stage process: (i) feature-wise “editing rules” that threshold, transform, or ignore attributes (optionally conditioned on a small context set), and (ii) an aggregation rule such as Bradley–Terry or a sigmoid link to produce pairwise choice probabilities. The axioms (complementarity, weak transitivity, codomain span, and compositionality) yield factorization theorems connecting the class to logistic/probit special cases and to conditional GAM-like structures. Empirically, on kidney-allocation decisions with real participants and simulated decision-makers, the learned two-stage models match or surpass standard baselines while exposing interpretable per-feature rules."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The work offers a principled axiomatic foundation and shows that these axioms imply a two-stage, feature-edited factorization with a sigmoid link (σ-transitivity), recovering classical models (logistic/probit) under additional assumptions and yielding conditional, GAM-like structures when allowing limited interactions. This yields interpretable inner functions that visualize per-feature heuristics (e.g., thresholds, diminishing returns) on real and simulated subjects, while achieving accuracy on par with or better than logistic, GAM, trees, and other baselines in kidney-allocation tasks."}, "weaknesses": {"value": "First, the axioms and factorization lean on properties (weak transitivity, compositionality) that can be violated in human judgments; the paper acknowledges that axioms target an “ideal” process, not observed behavior under biases, so model fidelity in settings with known transitivity/complementarity violations remains uncertain without further user studies.\n\nSecond, expressiveness hinges on the chosen editing-rule families and the limited context set ω (one feature for real data). If key interactions require broader context or non-monotone transformations, the learned inner functions may underfit or attribute effects incorrectly; identifiability is also delicate because the factorization need not be unique, which can complicate interpretation claims.\n\nThird, empirical scope is narrow: pairwise kidney-allocation tasks with small participant counts (Study One: 15, Study Two: 40) and a limited feature set. Although the models visualize plausible heuristics, the paper needs real-world validation of interpretability and trust claims; broader domains (non-moral choices, higher-dimensional attributes, n-way decisions beyond pairwise) and robustness under distribution shift are largely untested."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uqTKuMzLwS", "forum": "ziP9zetlLp", "replyto": "ziP9zetlLp", "signatures": ["ICLR.cc/2026/Conference/Submission22317/Reviewer_E8xC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22317/Reviewer_E8xC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829649047, "cdate": 1761829649047, "tmdate": 1762942166408, "mdate": 1762942166408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an axiomatic approach to learning cognitively faithful models of human decision-making from pairwise comparisons. The approach is grounded in the heuristics literature and aims to learn individual-level decision rules that better reflect actual cognitive processes rather than idealized rational preferences. The proposed model uses a two-stage process: features are first transformed through learned rules (feature editing functions), then aggregated via a fixed rule such as Bradley-Terry to produce decisions. This framework can be viewed as extending prospect theory beyond decisions under risk, enabling the capture of non-linear feature transformations. The authors demonstrate their approach on a kidney allocation task, showing that their models achieve comparable or superior accuracy to existing methods while maintaining interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "•\t**Interpretability**: The proposed decision models maintain interpretability similar to conventional decision-making models, making the learned rules accessible and understandable to practitioners.\n\n•\t**Rigorous theoretical foundation**: The paper provides rigorous axiomatic derivation as the mathematical foundation for the modeling approach, grounding the method in established decision theory principles."}, "weaknesses": {"value": "•\t**Limited application scope**: The modeling approach remains purely descriptive, positioned between decision trees (with added constraints) and linear classifiers (with relaxed constraints). Without sufficient data, more theoretical models may be preferable; with abundant data, decision trees might perform better. This intermediate position may limit the method's practical applicability.\n\n•\t**Generalizability concerns**: Compared to models that infer internal motivations and goals, this data-driven approach may struggle to generalize to new contexts or features not present in training data. This limitation is inherent to heuristic-based approaches and has been a longstanding criticism in the human decision-making literature (e.g., by Gigerenzer's critics).\n\n•\t**Marginal performance gains and lack of validation**: The proposed model shows only marginal improvement over baseline methods (0.78 vs. 0.76 for logistic classifier). More critically, while the authors emphasize cognitive faithfulness, they provide no direct empirical evidence that their learned models better align with participants' self-reported beliefs or decision strategies.\n\n•\t**Questionable axiomatic assumptions**: The \"natural decision-making axioms\" employed (complementarity, weak transitivity, and noninteractive compositionality) were proposed in 1960s decision theories and have been repeatedly shown to be violated in human behavior. As the authors acknowledge in their discussion, humans notoriously violate such decision rules, undermining the cognitive faithfulness claim.\n\n•\t**Limited predictive validity of rule-based models**: Rule-based (heuristics-based) decision-making usually provides substantially worse predictions than value-oriented models in most contexts (see Erev et al., 2010, Journal of Behavioral Decision Making), raising questions about when the proposed approach would be preferred.\n\n•\t**Confounding cognitive limitations with preferences**: Bounded rationality suggests that human decision-making is constrained by limited cognitive resources. The measured feature editing functions may simply reflect these cognitive limitations rather than genuine preferences or decision rules, making it unclear what the model actually captures about decision-makers' underlying values or goals."}, "questions": {"value": "**Q1: Application scope and practical utility**\n- Can the authors provide clearer guidance on when their approach would be preferred over simpler methods (e.g., decision trees) or more theoretically grounded models? What are the specific data regimes or decision contexts where this method offers distinct advantages?\n- Suggestion: Include empirical analysis comparing performance across different dataset sizes and complexity levels to delineate the method's practical niche.\n\n**Q2: Generalization to new features and contexts**\n- How does the model handle scenarios where decision-makers encounter new features not present in the training data? Can the learned feature editing functions transfer or adapt to novel attributes?\n- Suggestion: Conduct experiments demonstrating the model's ability (or inability) to generalize beyond the training feature space, and discuss potential extensions that could improve transferability.\n\n**Q3: Validation of cognitive faithfulness**\n- While the model is motivated by cognitive faithfulness, no direct validation is provided. Can the authors demonstrate that the learned rules actually correspond to participants' self-reported decision strategies or mental processes?\n- Suggestion: Include qualitative validation through participant interviews, think-aloud protocols, or post-hoc surveys asking participants to confirm whether the learned rules reflect their actual reasoning. Alternatively, compare the learned feature editing functions against known psychological phenomena.\n\n**Q4: Handling axiom violations**\n- Given that the foundational axioms (complementarity, weak transitivity, noninteractive compositionality) are known to be violated in human behavior, how do the authors justify building models upon these assumptions? What happens when participant behavior systematically violates these axioms?\n- Suggestion: Empirically test how often the axioms are violated in your data, and discuss either: (a) model extensions that can accommodate violations, or (b) why the violations may be negligible in your specific application domain.\n\n**Q5: Distinguishing preferences from cognitive constraints**\n- How can the model distinguish between genuine preferences and artifacts of bounded rationality or cognitive limitations? Are the learned feature editing functions capturing what people *want* or merely what they *can* process?\n- Suggestion: Design experiments that vary cognitive load or time pressure to test whether the learned rules remain stable, or conduct comparative analysis with decisions made under different resource constraints."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vIk2z0oVVa", "forum": "ziP9zetlLp", "replyto": "ziP9zetlLp", "signatures": ["ICLR.cc/2026/Conference/Submission22317/Reviewer_QKXr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22317/Reviewer_QKXr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22317/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978627773, "cdate": 1761978627773, "tmdate": 1762942166198, "mdate": 1762942166198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}