{"id": "ihucMuRXcY", "number": 3997, "cdate": 1757581575374, "mdate": 1759898059270, "content": {"title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning", "abstract": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice’s soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.", "tldr": "", "keywords": ["LLM", "RL"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/df768e40be088df6bfdbdb32db8c453b3b09ccaa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Generative Adversarial Reasoner (GAR): a joint RL framework that co-trains a reasoner LLM with a slice-level LLM discriminator to provide dense, on-policy process rewards. Reasoning chains are partitioned into “logically complete” slices; the discriminator outputs an analysis → YES/NO → brief rationale per slice, and training combines exact-match and aggregated slice rewards. On seven math benchmarks,  GAR improves over strong DeepSeek-R1-Distill baselines; authors report efficiency tricks and partial-trace RL without final answers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. Stepwise tables show improvements from: (i) adding any critic, (ii) reframing to slice-level judgments, and (iii) joint on-policy updates with alignment + discriminator rewards—culminating in the strongest results.\n\nS2. The discriminator operates on logically complete slices with an analysis → YES/NO → brief rationale format, producing localized, checkable feedback that improves credit assignment without expensive human PRM labels."}, "weaknesses": {"value": "W1.  Novelty is overstated vs. existing PRM/LLM-judge and adversarial/self-play lines What is addressed as “co-evolving slice-level discriminator” is very close to (i) PRMs trained on stepwise feedback (human or LLM-as-judge) and (ii) self-play / adversarial critics already explored for reasoning. The paper doesn’t clearly isolate what’s genuinely new beyond (a) cutting long traces into “slices” and (b) mixing a GAN-style real/fake discriminator with an answer-aligned term. Without tighter theoretical or empirical differentiation from PRM-as-judge (e.g., Generative Verifiers, Math-Shepherd, debate/multi-agent critics), the claimed contribution reads incremental. Also, GAN objectives assume a meaningful data distribution for “real” vs. “generated.” Here, “real slices” are reference CoT fragments from existing models, not ground-truth human-verified reasoning. The discriminator can then learn stylistic artifacts of reference traces rather than logical soundness. This undermines the premise that Rd improves correctness (vs. style mimicry). No analysis disentangles style detection from error detection.\n\nW2. Slicing heuristic (L=320 tokens, delimiter rules) is arbitrary and unvalidated\nThe core claim—“slice-level evaluation improves reliability”—depends on how you segment traces. But the paper gives no sensitivity study over L, no human audit of “logical completeness,” and no robustness to alternate chunking strategies. If performance hinges on a specific, hand-tuned slicer, the method risks brittleness and poor reproducibility.\n\nW3. All evidence comes from seven mathematical benchmarks. Math affords crisp local checks (algebraic identity, parity, etc.), which makes slice-level “soundness” judgments unusually reliable. It’s unclear that the same discriminator will remain calibrated on domains without local verifiers (open-ended QA, multi-hop fact retrieval, safety reasoning, tool use, code with flaky unit tests, long-form writing).\n\nW4. The “analysis–score–rationale” prompt and binary slice labels fit math well (short, checkable steps). In natural language reasoning, law, or medical QA, slices can be ambiguous and correctness is non-binary. The paper doesn’t show how slice segmentation and binary scoring handle ambiguity, partial credit, or evidence attribution."}, "questions": {"value": "How do you detect and mitigate reward hacking (e.g., producing slice-looking “clean” text without real progress)? Any concrete failure cases? Why binary slice labels (YES/NO) instead of graded scores or token-level attributions? Did you try continuous rewards?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MCkqEpbnJG", "forum": "ihucMuRXcY", "replyto": "ihucMuRXcY", "signatures": ["ICLR.cc/2026/Conference/Submission3997/Reviewer_JCij"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3997/Reviewer_JCij"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761033034506, "cdate": 1761033034506, "tmdate": 1762917129860, "mdate": 1762917129860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Generative Adversarial Reasoner, a framework that co-trains an LLM reasoner and an LLM discriminator through adversarial reinforcement learning. \nThe discriminator evaluates \"slices\" of the reasoning chain, providing dense, step-level rewards that improve credit assignment and sample efficiency over sparse final-answer rewards. \nThis method enhances performance on mathematical benchmarks and offers a flexible, modular approach for enhancing reasoning quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a co-training approach where the reasoner and discriminator evolve together, providing dense, calibrated, step-level rewards that significantly improve credit assignment over sparse outcome-based methods.\n\nIt shows improvements across multiple mathematical reasoning benchmarks (e.g., +7.3 on AIME24), outperforming strong RL baselines while maintaining comparable training efficiency.\n\nIt implements compute-efficient innovations like slice-level evaluation and response length truncation (128 tokens) that maintain performance while substantially reducing training time and resource requirements."}, "weaknesses": {"value": "The adversarial training setup may lead to reward hacking, where the discriminator and reasoner adapt to each other's weaknesses rather than improving reasoning quality. How does this paper mitigate the risk of reward hacking?\n\nIt is recommended that the method proposed in this paper be validated on a broader range of tasks, such as coding and commonsense reasoning tasks."}, "questions": {"value": "See the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fgUGGjg0CJ", "forum": "ihucMuRXcY", "replyto": "ihucMuRXcY", "signatures": ["ICLR.cc/2026/Conference/Submission3997/Reviewer_gLeA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3997/Reviewer_gLeA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761411243852, "cdate": 1761411243852, "tmdate": 1762917129526, "mdate": 1762917129526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Generative Adversarial Reasoner (GAR), an on-policy framework that jointly trains an LLM reasoner and an LLM discriminator via adversarial reinforcement learning. The discriminator evaluates slices of reasoning, providing dense, on-policy, step-level rewards. This method significantly improves mathematical reasoning, showing strong gains on benchmarks like AIME24."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper's core idea and method hold research value. Judging from the experimental results, the proposed method indeed brings about a relatively clear improvement compared to standard reinforcement learning methods.\n\n2. The paper conducts thorough ablation studies, sufficiently demonstrating the effectiveness of each component within the designed framework. Furthermore, the discriminator's truncation experiment design gives consideration to its impact on training efficiency.\n\n3. The paper includes several interesting discussions. First, the discussion on the experimental conclusions regarding policy entropy collapse is quite thorough. Additionally, the discussion on \"RL without Full Chain-of-Thought or Verifiable Final Answers\" is, in my opinion, quite intriguing and holds potential reference value for the community's understanding of RL."}, "weaknesses": {"value": "1. First, I believe the paper aims to propose a denser reward strategy. However, the final method is still based on GRPO, which assigns a reward to the entire sequence. Therefore, although the proposed method can provide step-level rewards, it essentially still provides a denser Outcome Reward during training. I think the authors' description of their contribution requires careful consideration, as the PRM is not used in a step-level way. Admittedly, in Section 4.5, the authors mention the possibility of \"RL without Full Chain-of-Thought or Verifiable Final Answers.\" Detailing these experiments would clarify the paper's contribution.\n\n2. The comparison with other RL methods is insufficient. The authors only include standard RL as the sole baseline. However, there is a significant amount of other work on LLM Reasoning and RL. I believe the authors need to compare their method against some of the more influential works in this area to further demonstrate its effectiveness.\n\n3. The \"slice\" plays a crucial role in the proposed method. The paper defines it as \"logically coherent slices\" segmented by \"delimiters.\" This definition is somewhat heuristic. Is the model sensitive to the choice of $L$? Furthermore, what exactly are these \"delimiters\" (e.g., newlines, specific punctuation, or semantic boundaries)? A clearer description of the slicing algorithm is needed for reproducibility. Additionally, the text mentions mixing generated slices with an \"equal number of reference slices.\" Do these \"reference slices\" originate from the baseline model’s (DeepSeek-R1) COT? If so, this should be clarified in methodology (Section 3). This element seems to function more like a regularizer (to prevent the discriminator from deviating excessively from reasonable reasoning patterns) than a traditional GAN objective (imitating $p_{ref}$).\n\n4. The proposed GAR framework is quite complex, involving numerous training hyperparameter settings ($\\lambda_{1}=\\lambda_{2}=\\lambda_{3}=1, \\lambda_{4}=0.5$). The paper lacks an analysis of these weight choices. If performance is highly sensitive to minor variations in these weights, the framework will be relatively difficult to reproduce and apply."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Tn0VqtQpMq", "forum": "ihucMuRXcY", "replyto": "ihucMuRXcY", "signatures": ["ICLR.cc/2026/Conference/Submission3997/Reviewer_iNKw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3997/Reviewer_iNKw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761543800966, "cdate": 1761543800966, "tmdate": 1762917129322, "mdate": 1762917129322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Generative Adversarial Reasoner (GAR), a novel framework that jointly trains an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning to enhance mathematical reasoning capabilities. The key innovation is partitioning reasoning chains into logically complete \"slices\" that the discriminator evaluates for soundness, providing dense step-level rewards rather than relying solely on sparse final-answer feedback. The reasoner is rewarded for producing logically consistent steps leading to correct answers, while the discriminator receives rewards for correctly detecting errors and distinguishing between generated and reference reasoning. Experiments on mathematical benchmarks show consistent improvements over strong baselines, with notable gains on challenging datasets like AIME24 (+7.3 for DeepSeek-R1-Distill-Qwen-7B and +10.0 for DeepSeek-R1-Distill-Llama-8B)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a novel approach to LLM reasoning improvement by adapting GAN-style adversarial training to the reasoning domain. The slice-level evaluation mechanism is creative and addresses computational efficiency concerns while maintaining granular feedback. The joint training paradigm where both reasoner and discriminator co-evolve is innovative compared to static reward models.\n\n2. The experimental methodology is solid with comprehensive evaluation across seven mathematical benchmarks. The authors provide proper statistical analysis (averaged over 30 runs) and fair comparisons using unified evaluation protocols. The ablation studies systematically validate each component's contribution. The selective-entropy analysis provides valuable insights into how the method avoids common pitfalls of RL training.\n\n3. The paper is generally well-written with clear motivation and methodology. The slice-level feedback examples in Table 2 effectively demonstrate the discriminator's capabilities. The figures and experimental results are presented clearly."}, "weaknesses": {"value": "1. Limited Theoretical Foundation: While the empirical results are strong, the paper lacks theoretical analysis of the joint training dynamics. There's no convergence analysis or guarantees about the co-evolution process, which is concerning given the potential for reward hacking mentioned by the authors.\n\n2. Slice Segmentation Methodology: The slice partitioning approach appears somewhat ad-hoc. The paper doesn't provide sufficient justification for these choices or analysis of how sensitive the results are to different segmentation strategies.\n\n3. Computational Overhead Analysis: While the authors claim efficiency improvements, the analysis is incomplete. Table 3 shows training time comparisons, but there's no detailed breakdown of computational costs during inference or comprehensive analysis of the discriminator's computational burden across different problem complexities.\n\n4. Limited Baseline Comparisons: The paper primarily compares against variants of DeepSeek-R1-Distill models. Comparisons with other recent reasoning enhancement methods (beyond standard RL) would strengthen the evaluation. The related work section mentions several relevant approaches that aren't empirically compared."}, "questions": {"value": "1. Why use a smaller model (1.5B vs 7B) for the discriminator in the Qwen setup but the same size for Llama? How does discriminator capacity affect performance?\n\nOthers see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EI1XsWGYES", "forum": "ihucMuRXcY", "replyto": "ihucMuRXcY", "signatures": ["ICLR.cc/2026/Conference/Submission3997/Reviewer_mfoq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3997/Reviewer_mfoq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832847232, "cdate": 1761832847232, "tmdate": 1762917129085, "mdate": 1762917129085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}