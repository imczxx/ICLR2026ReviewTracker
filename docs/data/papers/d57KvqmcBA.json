{"id": "d57KvqmcBA", "number": 7489, "cdate": 1758024343860, "mdate": 1763721294182, "content": {"title": "CPQS-Tuning: A Model Self-Perception-Based Data Filtering Algorithm for Efficient Instruction Fine-Tuning", "abstract": "Instruction fine-tuning is a key technique for enhancing the performance of large language models (LLMs), but low-quality and redundant data often hinder its effectiveness. Recent studies suggest that filtering a small amount of high-quality data for instruction fine-tuning can achieve faster and more efficient training performance. However, existing data filtering approaches predominantly depend on predefined evaluation models or manually designed metrics, without leveraging information from the target LLM itself. This limitation may result in a mismatch between the filtering criteria and the actual requirements of the LLM being fine-tuned, thereby reducing the effectiveness of the fine-tuning process.  To address these issues, we propose a novel perspective: the hidden states of LLMs implicitly reflect the quality of the training data. Based on this insight, we propose a novel data filtering method that extracts the hidden states that reflect the target LLM’s perception of the data as representative features, and builds a data classification model upon them, which outputs the Contrastive Perception Quality Score (CPQS) for dataset filtering. Our experiments are conducted in both general and downstream domains.\n(1) In the general domain, our experiments show that training on under 10\\% of the data from both the Alpaca\\_GPT4 and DeepSeek-R1 synthesized reasoning datasets enables our method to outperform models trained on the complete datasets. Moreover, it surpasses the performance of current state-of-the-art data-selection techniques.\n(2) In downstream tasks, our approach delivers an average performance gain exceeding 3.6\\% over leading data-selection algorithms across multiple benchmarks, including GSM8K, HumanEval, and HumanEval-Plus.", "tldr": "", "keywords": ["Instruction Fine-tuning", "LLMs", "Data Filtering", "CPQS", "Hidden States"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/45d8f311dac8c7f55c33f9a662b643f2a759cb05.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel method for filtering instruction-tuning data for LLMs. The authors identify that many available instruction-fine-tuning datasets contain low quality or redundant examples, and that existing filtering approaches often rely on external evaluation models or manually designed metrics, which may mis‐align with the target LLM’s actual needs. \nTo address this gap, they posit that the hidden states of the target LLM implicitly encode its perception of data quality. They extract hidden‐state embeddings from the LLM when processing candidate instruction examples, train a contrastive classification model to distinguish high vs low quality samples, and define CPQS to rank/filter the dataset. \nEmpirically, they show that using under 10 % of the data from large synthetic instruction dataset, their method outperforms models trained on full dataset; in downstream tasks, the CPQS-filtered subset achieves on average +3.6 % improvement over leading selection algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Introduces an innovative strategy: leveraging the target LLM’s own hidden‐state representations to capture its perception of data quality — this internalizes the filter criterion to the fine-tune target rather than relying on external proxies.\n\nDemonstrates significant empirical efficiency: using under 10% of data and still achieving (or exceeding) full‐data performance is compelling for resource‐constrained tuning scenarios.\n\nThe pipeline is reasonably practical — hidden‐state extraction + classifier + ranking is implementable and does not require extensive external hardware (relative to training giant filter models).\n\nAddresses a timely problem: as LLM fine‐tuning scales up, data redundancy and dataset cost are real bottlenecks; effective filtering helps reduce compute/data cost."}, "weaknesses": {"value": "The training data for classifier (high/low quality) comes from synthetic sources (GPT-4 vs weaker models) rather than human annotated “instruction usefulness,” which may bias the filter toward model‐generated signal rather than actual instruction utility for users.\n\nThe evaluation, while covering several benchmarks, is limited in scope: primarily to specific datasets (Alpaca_GPT4, DeepSeek-R1) and certain LLM sizes; less evidence for very large scale (>30 B) or for domain‐specific instruction sets.\n\nThe cost/efficiency claims are somewhat under‐detailed: while “under 10% data” is quoted, actual compute/GPU‐hours, hyper‐param parity, training seeds, variation across runs are less fully described, which may reduce replicability confidence.\n\nThe method introduces potential bias in data selection: by relying on hidden-state classification, it may inadvertently favour samples that “look good” to the model’s internal representation (which may favour certain styles or tasks) and down-select others (e.g., rare domains, under‐represented instruction types). The paper does not deeply analyse coverage/diversity of the selected subset or risk of bias.\n\nWhile the hidden‐state classifier is shown to separate high vs low quality (AUC=1.00 in logistic regressor example), that may reflect a somewhat contrived two‐class setup (strong model vs weak model generation) rather than realistic open filtering scenarios; the real generalization beyond this setting may be less strong."}, "questions": {"value": "The paper defines “high‐quality” vs “low‐quality” data via outputs from stronger vs weaker models (e.g., GPT-4 vs Llama-3.2-1B-Instruct). Could the authors clarify how this quality definition correlates with user‐instruction usefulness or downstream task gain? Have you measured human‐annotated instruction usefulness to validate this proxy?\n\nHow robust is the CPQS classifier to architecture shifts? For example, if the target fine-tune model is of a significantly different architecture (or size) from the one used to extract hidden‐states, how does performance vary?\n\nCould the authors provide more detailed compute cost comparisons (selection + fine-tuning) versus baseline filtering algorithms (e.g., IFD, Superfiltering) under identical hardware settings, including GPU-hours, memory, latency?\n\nHave you analysed the selected subset in terms of domain/task coverage, instruction style diversity, difficulty distribution, or edge‐cases (e.g., dialogue vs code vs reasoning)? Specifically, is there a risk that CPQS selects only “safe/easy” instructions that align with the model’s hidden-state biases?\n\nIn downstream tasks (GSM8K, HumanEval, HumanEval-Plus), the average +3.6% improvement is promising — could you provide per‐benchmark breakdowns (wins vs losses) and variance across random seeds to help assess stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JhOdwczldn", "forum": "d57KvqmcBA", "replyto": "d57KvqmcBA", "signatures": ["ICLR.cc/2026/Conference/Submission7489/Reviewer_ZLxp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7489/Reviewer_ZLxp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761475777369, "cdate": 1761475777369, "tmdate": 1762919604085, "mdate": 1762919604085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "Thank you for sharing this fascinating paper—it has been quite enlightening. I've recently been exploring data filtering for fine-tuning large models. If you have some spare time, would you be willing to address my questions about this paper?\n\nQ1: I've read many papers on instruction data filtering, and they consider not only data quality but also factors like diversity. This paper doesn't seem to mention diversity—is it because diversity has a relatively minor impact on model performance?\n\nQ2: The experimental section shows that Llama-3.2-1B-Ins and Qwen-2.5-1.5B-Ins to generate low-quality samples. However, according to their respective official technical reports, these models outperform the Llama-2-7b used in the experiments. Would the low-quality data generated by the aforementioned two models also be classified as low-quality data by Llama-2-7b?\n\nI hope my questions aren't bothering you. Thank you for your time!"}}, "id": "0f3eJVaN3s", "forum": "d57KvqmcBA", "replyto": "d57KvqmcBA", "signatures": ["~Zihao_Chen10"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "~Zihao_Chen10"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7489/-/Public_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763097682019, "cdate": 1763097682019, "tmdate": 1763097682019, "mdate": 1763097682019, "parentInvitations": "ICLR.cc/2026/Conference/-/Public_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to select data that is most informative for a specific target model, rather than data that is \"high quality\" in a general sense. The approach uses the hidden states of LLMs to train a two-way classifier on a synthetic dataset labeled with quality scores, and this framework is validated through experimental analysis."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper’s core motivation and its corresponding experimental validation are internally consistent and logically presented."}, "weaknesses": {"value": "1. The paper proposes a pipeline with data synthesis, CNN training and data evaluation, while completely omitting an analysis of computational cost, which is a critical metric for any work in the data efficiency domain.\n\n2. The comparative study is weak. For example, but not limited to:\n- What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning\n- DataMan: Data Manager for Pre-training Large Language Models \n- IMPROVING DATA EFFICIENCY VIA CURATING LLM-DRIVEN RATING SYSTEMS\n- SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection\n- LESS: Selecting Influential Data for Targeted Instruction Tuning\n\nBesides, It notably lacks a comparison against random sampling, which is the simplest and one of the most essential baselines for data selection.\n\n3. The motivation is not novel. The idea of selecting `actual requirements of the LLM` is the fundamental premise behind all uncertainty-based and influence-based data selection methods.\n\n4. The proposed pipeline appears overly complex and is not clearly justified. See question."}, "questions": {"value": "The synthetic \"quality\" dataset (and its good/bad labels) is shared across all experiments, even though a separate CNN is trained for each target LLM. This design implies that \"quality\" is a pre-defined, model-agnostic concept, determined by the generative source. If all model-specific CNNs are being aligned to this single, shared quality space, why do the final results show that different LLMs have different judgments of \"quality\"? This seems contradictory."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZcNwViF66S", "forum": "d57KvqmcBA", "replyto": "d57KvqmcBA", "signatures": ["ICLR.cc/2026/Conference/Submission7489/Reviewer_jCwj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7489/Reviewer_jCwj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761504878083, "cdate": 1761504878083, "tmdate": 1762919603757, "mdate": 1762919603757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper trains a CNN to classify low-quality data and high-quality data with the hidden state of a model, before tuning the model on the selected high-quality data. It achieves state-of-art performance on multiple tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper has a novel idea\n2. The experiments can demonstrate the effectiveness of the proposed method\n3. The paper is written clearly"}, "weaknesses": {"value": "1. The paper lacks in-depth analysis of the method's improvement over baselines. For example, Alpagasus is the method to provide ground truth labels, but the paper's method performs even better than that. The paper provides no explanation to this.\n2. The paper lacks a justification of the CNN structure: It would be good to compare with a more natural transformer structure."}, "questions": {"value": "1. I see in the appendix A.4.2, you demonstrated that for similar sized models, it's better to use the model's own hidden states to do the classification. What if you use models of different size? For example, will a 13B model's hidden state be better than a 1B model's own hidden state?\n2. See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DrlOMXettI", "forum": "d57KvqmcBA", "replyto": "d57KvqmcBA", "signatures": ["ICLR.cc/2026/Conference/Submission7489/Reviewer_vfku"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7489/Reviewer_vfku"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761585784602, "cdate": 1761585784602, "tmdate": 1762919603294, "mdate": 1762919603294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CPQS-Tuning (Contrastive Perception Quality Score), a data filtering algorithm for instruction fine-tuning based on model self-perception. The core idea is to leverage the hidden states of large language models (LLMs) as implicit indicators of data quality. A CNN classifier is then trained on these hidden-state features to output a contrastive perception quality score (CPQS) for each training example, enabling the selection of high-quality samples. This work is particularly promising in the area of evaluating the quality of synthetic instruction data."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1.The paper takes an innovative perspective by using the model’s own hidden states as a signal of data quality, removing dependence on external evaluation models or manually crafted metrics. It would be interesting to see future work analyzing how these hidden-state signals correlate with human evaluation metrics to improve interpretability.\n\n2.Experimental results demonstrate that the proposed approach can achieve better performance with less than 10% of the data compared to full-dataset training, which substantially reduces training costs.\n\n3.The linear separability and layer-wise comparison analyses nicely support the claim that hidden states indeed encode discriminative semantic features of high- vs. low-quality data — this is a very interesting finding.\n\n4.The appendix includes additional experiments such as full-parameter fine-tuning, iterative filtering, inter-layer comparison, and cross-model preference studies, all of which strengthen the robustness of the method. I look forward to the authors releasing the project code after acceptance so that I can further explore this work in detail."}, "weaknesses": {"value": "1.The method is relatively complex, as it requires extracting multi-layer hidden states and training an external CNN model, which demands significant GPU memory and computation.\n\n2.The cross-model experiment shows that using “high-quality data” selected by Qwen2.5–7B-Instruct to fine-tune LLaMA2–7B–Chat actually led to degraded performance. This indicates that each LLM has its own internal definition of “high-quality data,” implying that CPQS must be trained separately for each target model."}, "questions": {"value": "1.It remains unclear whether CPQS can generalize well across different model architectures or parameter scales. Can the method be effectively applied to other open-source LLMs?\n\n2.Although the results show linear separability between high- and low-quality samples, the underlying mechanism of why hidden states reflect data quality is still largely empirical. I encourage the authors to explore this aspect more theoretically in future work.\n\n3.Since hidden layers contribute differently to performance (see Appendix A.4.1), it would be valuable to develop an automatic strategy for determining the optimal layer combination for CPQS extraction."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gXkEBPZYGR", "forum": "d57KvqmcBA", "replyto": "d57KvqmcBA", "signatures": ["ICLR.cc/2026/Conference/Submission7489/Reviewer_eg3w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7489/Reviewer_eg3w"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7489/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640981006, "cdate": 1761640981006, "tmdate": 1762919602906, "mdate": 1762919602906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}