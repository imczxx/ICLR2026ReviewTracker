{"id": "uMj2gwldEY", "number": 10338, "cdate": 1758167472852, "mdate": 1759897657513, "content": {"title": "Federated DRL-based Coordination of Multi-UAVs for Wildfire Tracking", "abstract": "Formation control continues to pose significant challenges in the field of multi-agent deep reinforcement learning (DRL). This paper presents a formation strategy for multiple UAVs engaged in large-scale wildfire tracking. The proposed approach leverages the Deep Deterministic Policy Gradient (DDPG) algorithm to enable individual UAVs to adapt their path planning and control policies in real time. Although effective for single-UAV scenarios, standard DDPG does not scale well to multi-UAV coordination. Moreover, wildfire fronts rarely evolve symmetrically, as environmental factors such as wind, terrain, and fuel conditions can accelerate fire spread in certain directions, producing irregular boundaries that complicate the maintenance of uniformly spaced formations. To address these limitations, the proposed framework integrates Federated Learning (FL) with DDPG to facilitate collaborative policy refinement without exchanging raw data, employing a distance- and performance-weighted federated averaging scheme. We apply FL in a novel way to the DDPG components governing linear velocity and its corresponding control gain, both of which are critical for acceleration control and inter-UAV spacing. The simulation results indicate that our method, FL-DDPG, yields significantly improved formation stability—with 2.5 m average spacing variance compared to 14 m for standard DDPG—and improves the average episode reward from –355.45 to –122.21. These results highlight the effectiveness of FL-DDPG in achieving robust, decentralized multi-UAV coordination in complex wildfire environments.", "tldr": "", "keywords": ["UAV Formation Planning and Control", "Fire Front Tracking", "Multi-Agent Systems", "Deep Reinforcement Learning", "Federated Learning", "Scalability"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6150fe36c130246cd0f3930bafea384edb19ead3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an algorithm for formation control of multiple UAVs tracking a wild fire using a federated learning approach to DDPG.\nA _weighted_ federated learning approach is proposed whereby the parameters that are accumulated and merged into an averaged global policy are done so weighted by the performance of the individual agents.\nExperiments are performed illustrating the advantages of this modified algorithm compared to DDPG in a simulated multi-UAV wild fire tracking simulation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a novel weighted federated learning algorithm that could potentially be applied to many other RL settings beyond the application explored in this paper. As far as I am aware this weighted averaging of parameters in an FL algorithm is novel - although I am not very familiar with all the relevant literature - and given that the paper does not cite similar instances of this algorithm I am assuming that the authors consider it novel also.\n2. The paper applies this novel algorithm to the important real world problem of wild fire tracking.\n3. The results show _some_ evidence of improved performance compared to DDPG."}, "weaknesses": {"value": "1. The paper is not written particularly clearly and fails to highlight the true novelty and purpose of the paper which is the _weighted_ FL algorithm. This is not mentioned in the abstract referring only to an FLDDPG approach, which has already been done before in Na et al. 2023. The abstract also highlights the reward difference between the two compared approaches; however, it is easy to construct a particular reward function such that a large reward difference doesn't necessarily reflect a large skill difference.\n2. The proposed weighted FLDDPG is not compared to a non-weighted FLDDPG in the experiments, this would have been the most appropriate comparison to illustrate the advantages of the propose novelty. It is therefore unclear whether the proposed novelty of the paper is the reason for outperforming DDPG or just the fact that it integrates an FL aspect, which has already been explored in Na et al 2023.\n3. The advantages of FLDDPG are not necessarily reflected in the results because both algorithms achieve the maximum reward of 1 during the simulation.\n4. It is unclear to me whether training is occurring online during the simulated 20 seconds, or whether training occurred before this simulation and this simulation is a rollout of the optimised policies. If the former, then point 3. is even more pertinent because one would just select the final policy as that which achieved the maximum reward, which occurred for both algorithms.\n5. As far as I can tell multiple experiments were not performed to highlight robustness.\n6. There should be a single plot comparing both algorithms, rather than providing the results in two separate plots.\n7. The images of the UAVs and the fire do not illustrate how to UAV swarm reacts to the fire evolving through time, which would have been informative.\n8. It is unclear to me why bother reporting the results of a 3 UAV simulation _and_ a 5 UAV simulation when the latter illustrates the same advantages as well as demonstrating scalability. The 3 UAV experiments therefore seem redundant.\n9. The mathematical notation is a little sloppy, in particular with relation to the t subscript in the equation in line 183 and the subscripts in equation 10. There are also multiple variables that are not adequately described making the maths a little hard to follow.\n10. The images are pixelated and should instead be vector images."}, "questions": {"value": "1. Is the learning performed during the simulated 20 seconds or is it performed prior to the simulation reported in the results section?\n2. Why did you not compare to a non-weighted FLDDPG approach?\n3. Did you perform multiple experiments or just the one per algorithm?\n4. Do you forsee this weighted FLDDPG algorithm as a more general RL algorithm with applications beyond wild fire tracking?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0YckSmAcOK", "forum": "uMj2gwldEY", "replyto": "uMj2gwldEY", "signatures": ["ICLR.cc/2026/Conference/Submission10338/Reviewer_xqkU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10338/Reviewer_xqkU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761491587873, "cdate": 1761491587873, "tmdate": 1762921670209, "mdate": 1762921670209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of formation control in multi-UAV systems for large-scale wildfire tracking using deep reinforcement learning. The authors propose FL-DDPG, a framework that combines Federated Learning (FL) with the Deep Deterministic Policy Gradient (DDPG) algorithm to enhance decentralized coordination among UAVs. Unlike standard DDPG, which struggles to scale across multiple agents and irregular wildfire dynamics, FL-DDPG enables collaborative policy updates without sharing raw data, using a distance- and performance-weighted averaging method. Simulation results demonstrate that FL-DDPG achieves far greater formation stability, reducing spacing variance from 14 m to 2.5 m."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Strong and complete backgound (\"Premininaries\") section\n- Great to see real-world fire data as part of the simulation"}, "weaknesses": {"value": "- Introduction could use additional references on MARL for Wildfires and communication / collaboration with human and machines: HIVEX: A High-Impact Environment Suite for Multi-Agent Research and LLM-Mediated Guidance of MARL Systems (Siedler et. al.)\n- Section naming could be more conventional e.g. \"Background\" instead of \"Preliminaries\"\n- Figure 1 could use more detailed but high level (non technical) description in the caption on how things work\n- Figure 3: The differences between the two plots is barely noticeable, maybe would be good to zoom in and or highlight the differences explicitly in the caption.\n- Experiment runs are statistically not satisfying. I would have expected 3-10 runs and resulting plots showing averages, and tables showing standard deviation etc.\n- There could be more discussion on why FL-DDPG is better than DDPG\n- Personally I believe it would be fruitful to also run UAV-count exepriments, how does the current solution scale from 3-9 UAVs? etc."}, "questions": {"value": "- \"The pair (Tx, Ty ) corresponds to a clockwise rotation; for consistency across the controller design,\nwe adopt the counterclockwise convention above throughout this work.\" - independent of counter or clockwise, there is a full update - the entire perimeter - for each step, correct?\n- The description for the rewards given are not enough. This is what I understand: There is a reward for keeping safe distance to neighbours. What I don't understand is \"UAV's heading with the direction of the spread\" does that mean the UAV's must align with direction of fire-spread? Are we tracking \"un-seen\" fire front? How does this work?\n- Generally we are looking at 2D coordinates for the UAV, however there is a mention that the \"FARSITE model\" includes terrain data, how does this relate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jvr60SmmN3", "forum": "uMj2gwldEY", "replyto": "uMj2gwldEY", "signatures": ["ICLR.cc/2026/Conference/Submission10338/Reviewer_nyaT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10338/Reviewer_nyaT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644806126, "cdate": 1761644806126, "tmdate": 1762921669830, "mdate": 1762921669830, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel framework called FL-DDPG to coordinate a team of UAVs tasked with tracking the perimeter of a wildfire. The primary challenges with existing methods include - (1) Standard RL and DRDL methods are often very inefficient and do not scale well (2) Firefronts evolve continuously which make it hard for achieving formation control.\n\nFederated Learning-based DDPG (FLDDPG) framework employs:- \n(a) A selective federation strategy where - each UAV transmits only the relevant subsets of its local model parameters—specifically those governing velocity and control gain\n(b) A weighted aggregation of above said parameters - agents that maintain tighter adherence to the desired formation spacing exert greater influence on the global policy thereby biasing learning toward stability under asymmetric fire-front dynamics\n\nThe authors present simulation results, based on a FARSITE-calibrated environment, claiming that FL-DDPG significantly outperforms a \"standard DDPG\" baseline in both formation stability and average episode reward"}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Originality - A creative combination of Federated Learning and Deep Reinforcement Learning has been applied to a challenging domain which is definitely a great idea. Secondly, the federated aggregation is clever in the fact that it directly links the agent's influence on the global model to a key term from it's own rewards function (formation-spacing error). The selective federation strategy to share just the velocity and gain parameters is also an original win.\n2. Quality - The quality of the paper lies majorly in its experimental setup where they have demonstrated results on a high fidelity simulation environment i.e the FARSITE simulator with calibration done on historical data. This points to a very non trivial problem being solved \n3. Clarity - For the most part, the clarity is great. The ideas are easy to follow and well articulated. The visual evidence also does provide a compelling story that FL-DDPG agents do maintain a coordinated formation compared to agents that use DDPG only.\n4. Significance - The significance of the paper is high because the problem being solved is non trivial and is a great real world application with societal benefits."}, "weaknesses": {"value": "1. Lack of ablation studies to prove \"performance-weighted\" aggregation  and the \"selective federation\" hypothesis actually work - because the paper includes no ablation studies, it's impossible to know if either of these novelties is actually necessary or beneficial.\n2. The only baseline being compared against is standard DDPG. The lack of comparison to any credible MARL baselines like MADDPG or even MAPPO or VDN is a very critical flaw.\n3. The reward function does not seem standardized - It is a complex, custom-engineered equation with multiple hand-tuned hyperparameters and no sensitivity analysis.\n4. The reward plots are normalized which is poor practice. You are not able to figure out if the agents are actually solving the tasks or just converging to a less bad policy than the baseline"}, "questions": {"value": "1. Did the authors consider or experiment with more modern on-policy MARL algorithms (like MAPPO), which are often state-of-the-art for cooperative tasks?\n2. Can the authors also provide an ablation study against MADDPG (an established MARL baseline) and/or VDN ? \n3. Can the authors provide ablation studies for proving the \"performance-weighted\" aggregation  and the \"selective federation\" hypothesis ? Like what would happen if a standard FedAvg algorithm is used ? \n4. What is the justification for the exact equal weighting of the fire-tracking penalty (value=20) and the formation-spacing penalty(value=20) ? Also can we get sensitivity analysis on both of these ? \n5. The paper claims that the main benefit of FL is reducing communication bandwidth. How is the O(N) state shared? Because Equation 7 is an O(N) state that probably requires O(N^2) communication cost and would it not negate scalability benefits ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TedV1CxP1U", "forum": "uMj2gwldEY", "replyto": "uMj2gwldEY", "signatures": ["ICLR.cc/2026/Conference/Submission10338/Reviewer_6EN6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10338/Reviewer_6EN6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10338/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971062437, "cdate": 1761971062437, "tmdate": 1762921669387, "mdate": 1762921669387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}