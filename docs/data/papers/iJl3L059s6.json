{"id": "iJl3L059s6", "number": 1906, "cdate": 1756963646468, "mdate": 1759898179496, "content": {"title": "Cutting the Skip: Training Residual-Free Transformers", "abstract": "Transformers have achieved remarkable success across a wide range of applications, a feat often attributed to their scalability. Yet training them without residual (skip) connections remains notoriously difficult. While skips stabilize optimization, they also disrupt the hierarchical structure of representations, raising the long-standing question of whether transformers can be trained efficiently without them. In this work, we address this problem by analyzing the Jacobian of a skipless transformer block, showing why residuals improve conditioning and revealing that their stabilization benefits can be recovered through a principled initialization strategy. Building on this insight, we introduce the first method that enables stable and efficient training of skipless transformers without altering the standard architecture. We validate our approach on Vision Transformers (ViTs) in both supervised and self-supervised settings, demonstrating that skipless ViTs trained with our initialization overcome the usual optimization barriers, learn richer hierarchical representations, and outperform strong residual baselines on dense prediction benchmarks. These results show that skip connections are not a fundamental requirement for training ViTs and open new avenues for hierarchical representation learning in vision models.", "tldr": "", "keywords": ["Vision Transformers", "Skip Connections", "Network Conditionin"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e07f41cd674f1febbd2723b89e7e8478b3e381d9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an initialization to train a transformer model without skip connection. The initialization strategy is developed based on a theoretical analysis of the transformer Jacobian, which reveals that skip connections stabilize optimization by improving Jacobian conditioning. To replicate this stabilization effect without skip connections, the paper initializes the product of the projection matrices to ensure the good conditioning of attention matrix.  Results on ViT show that, when combined with SOAP optimizer, skipless models achieve comparable convergence speed and accuracy to residual-based models in supervised image classification task, and outperform residual-based models in self-supervised dense prediction tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The overall paper is well-written and easy to follow.\n- The proposed initialization scheme requires no architectural modifications.\n- The experimental validation is comprehensive, covering both supervised and self-supervised learning settings on ViTs. And the result aligns with the analysis and explanation."}, "weaknesses": {"value": "- How to demonstrate \"learning richer, more semantically coherent internal representations\" is unclear. Additionally, the significance of training skipless transformers remains ambiguous.\n- Regarding experiments across different scales, I am curious whether this method is only effective for Vision Transformers (ViTs) or applicable to transformer models with general architectures. This is because the analysis in the paper has little relevance to vision-specific characteristics.\n- The paper only explores which type of initialization can lead to better conditioning of the network Jacobian, but it does not essentially guarantee better model performance.\n\nOverall, this work represents a valuable initial attempt. However, for different transformer architectures, including those with various subsequently proposed activation functions, normalization layers, and MoE structures, the applicability and generalizability of this initialization method still require further experimental validation."}, "questions": {"value": "- Why does an abrupt inflection point appear in Figure 1? The loss is obviously not smooth."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NGcO15w15s", "forum": "iJl3L059s6", "replyto": "iJl3L059s6", "signatures": ["ICLR.cc/2026/Conference/Submission1906/Reviewer_rJvD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1906/Reviewer_rJvD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761630455000, "cdate": 1761630455000, "tmdate": 1762915940201, "mdate": 1762915940201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates training ViTs without residual connections. The authors provide a Jacobian-based theoretical analysis showing that skip connections improve network conditioning, and propose a principled initialization strategy that enables stable training of skipless transformers without architectural modifications. The key contributions include: 1. theoretical analysis of transformer Jacobian conditioning; 2. a novel initialization scheme; and 3. empirical validation showing skipless ViTs can match residual models in supervised learning and outperform them in dense prediction tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Principled Theoretical Foundation: The authors provide some mathematical insights on the Jacobian analysis, which provides clear intuition for why skip connections help optimization.\n2. This approach requires no architectural modifications.\n3. The authors provide thoughtful experiments."}, "weaknesses": {"value": "1. The restriction to Vision Transformers is a significant limitation.\n2. The improvements are minor."}, "questions": {"value": "1. Can you provide more experiments on LLM?\n2. Can you ablate the specific properties of SOAP that enable skipless training?\n3. Do the benefits hold at larger scales?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "H31haQrpte", "forum": "iJl3L059s6", "replyto": "iJl3L059s6", "signatures": ["ICLR.cc/2026/Conference/Submission1906/Reviewer_RZmz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1906/Reviewer_RZmz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993683974, "cdate": 1761993683974, "tmdate": 1762915939614, "mdate": 1762915939614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reformulates the problem of stabilizing training in residual-free transformers as minimizing the condition number of the network Jacobian. It proposes a principled parameter initialization method that improves the conditioning of the self-attention layer. The analysis shows that the Jacobian’s conditioning is dominated by two key matrix products: $W^Q W^{K\\top}$ and $W^V W^O$ and that initializing $W^V W^O$ to be scaled-orthonormal and enforcing diagonal dominance in $W^Q W^{K\\top}$ can significantly improve conditioning of self-attention layer Jacobian and thus enhance training stability.\nExperiments on both supervised learning (ImageNet classification with ViT-Base) and self-supervised learning (DINO feature pretraining with ViT-Small) demonstrate that this initialization enables stable optimization of skipless transformers. The method is compatible well with SOAP optimizer and achieves comparable or better performance than residual-based counterparts. Visualization of the final-layer features  shows that skipless models trained with the proposed initialization yield clearer class boundaries."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper has clear writing and is easy to follow.\n2. This paper introduces a novel principled initialization method that is effective on alleviating  instability in residual-free transformer training.\n3. This paper conducted experiments on multiple tasks including image-classification and DINO feature pertaining."}, "weaknesses": {"value": "1. **Limited theoretical robustness beyond initialization**. While the principled initialization helps improve conditioning at the beginning of training, but the paper provides no guarantee that good conditioning persists as parameters evolve during training. In contrast, skip connections offer a structural and long-term stabilizer independent of parameter drift.\n2. **Simplified input distribution assumption**. The theoretical derivation(Appendix A4.1) assumes token embeddings are drawn from  a normal distribution with mean 0 and identity covariance, which deviates from the empirical distribution after LayerNorm or embedding layers where the tokens are correlated across dimensions and not centered at 0.\n3. **Indirect evidence for hierarchical representation**. While visualizations suggest clearer feature boundaries and clean feature maps, no direct qualitative and quantitative evidence is provided to show skipless ViT yields more hierarchical or compositional features than residual ones. Given it’s considered as one strong benefit of skipless ViT. More evidence is supposed to be provided.\n4. **Limited model scale diversity**. The experimental validation involves only ViT-Base (supervised) and ViT-Small (self-supervised). Without testing larger models (e.g., ViT-Large or ViT-Huge) and different model size on the same task, it remains unclear whether the initialization scales effectively with increasing parameter count.\n5. **Material Missing**. In 5.2, Appendix E is mentioned but is missing in provided materials."}, "questions": {"value": "1. **Optimizer inconsistency: SOAP > AdamW in supervised, but Adam > SOAP in self-supervised**. In the supervised ImageNet setting with ViT-Base, skipless+init trained with a second-order method (SOAP) reaches parity/slight gains vs residual baselines , whereas in DINO pretraining with ViT-Small the best results appear under Adam rather than SOAP (Fig. 2 in the paper). Please analyze why SOAP’s advantage does not carry over to DINO.\n2. **Limited theoretical robustness beyond initialization.** Could you track $\\kappa(J)$, per-layer $\\kappa(K_\\ell)$ over epochs for both skipless+init and residual baselines? \n3. **More evidence for hierarchical/compositional representations**. Author shows clearer class boundaries from late-layer features in skipless+init models and gains on dense probing/object discovery  , but direct, layer-wise evidence of hierarchy/compositionality is limited. Please provide more evidence for hierarchy/compositionality.\n4. **More model scale diversity**. Could you provide the results of ViT-Large and ViT-huge to evaluate scalability of proposed method. (Consider demands on computing resources, it's listed as optional. )\n5. **Is minimizing the Jacobian condition number sufficient for stability?** Skipless+init converges to accuracy similar to residual models, but residual training shows smoother loss curves(Figure 1), suggesting potential landscape differences not captured by condition-number alone. If so, will such difference cause instability? (Although this analysis would be valuable and necessary for complete understanding, it falls beyond the scope of this paper. Therefore, it is listed as an optional question.)\n\n Authors’ rebuttal adequately addressing the first three points is necessary for me to raise the final score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lzr60nWY5o", "forum": "iJl3L059s6", "replyto": "iJl3L059s6", "signatures": ["ICLR.cc/2026/Conference/Submission1906/Reviewer_WDvd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1906/Reviewer_WDvd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1906/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762412237694, "cdate": 1762412237694, "tmdate": 1762915939297, "mdate": 1762915939297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}