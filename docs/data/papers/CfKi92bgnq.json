{"id": "CfKi92bgnq", "number": 1387, "cdate": 1756878992686, "mdate": 1759898211247, "content": {"title": "Reasoning in Space via Grounding in the World", "abstract": "In this paper, we claim that 3D visual grounding is the cornerstone of spatial reasoning and introduce the $\\textit{Grounded-Spatial Reasoner (GS-Reasoner)}$ to explore the effective spatial representations that bridge the gap between them. Existing 3D LLMs suffer from the absence of a unified 3D representation capable of jointly capturing semantic and geometric information. This deficiency is manifested either in poor performance on grounding or in an excessive reliance on external modules, ultimately hindering the seamless integration of grounding and spatial reasoning. To address this, we propose a simple yet effective \\emph{dual-path pooling} mechanism that tightly aligns geometric features with both semantic and positional cues, constructing a unified image patch-based 3D representation that encapsulates all essential information without extra tokens. Leveraging this holistic representation, GS-Reasoner is the first 3D LLMs that achieves autoregressive grounding entirely without external modules while delivering performance comparable to state-of-the-art models, establishing a unified and self-contained framework for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we introduce the $\\textit{Grounded Chain-of-Thought (GCoT)}$ dataset. This dataset is meticulously curated to include both 3D bounding box annotations for objects referenced in reasoning questions and step-by-step reasoning paths that integrate grounding as a core component of the problem-solving process. Extensive experiments demonstrate that GS-Reasoner achieves impressive results on 3D visual grounding, which in turn significantly enhances its spatial reasoning capabilities, leading to state-of-the-art performance.", "tldr": "", "keywords": ["3d spatial reasoning", "3d visual grounding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07abd2173febdd3309a8f706fce1b0a5b930a496.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GS-Reasoner, a novel spatial reasoning framework that enhances vision-language models' spatial understanding by incorporating 3D visual grounding as an intermediate reasoning step. Its core contributions include: a semantic-geometric hybrid 3D scene representation that aligns and fuses semantic features, geometric features, and 3D positional information via a dual-path pooling mechanism; the GCoT dataset, which provides 3D bounding box annotations and chain-of-thought reasoning paths to integrate grounding into spatial reasoning; and an autoregressive grounding capability that, enables end-to-end 3D localization without external detectors or modules, achieving competitive performance across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper is well written and easy to follow\n2.The dual-path pooling mechanism effectively mitigates misalignment issues between semantic-geometric and position-geometric features.\n3.The GCoT dataset fills a critical gap in existing resources for integrated \"grounding-reasoning\" tasks, and extensive evaluations across 3D grounding, spatial reasoning, and general 3D tasks provide comprehensive validation."}, "weaknesses": {"value": "1.The training stage is mainly focus on 3d reasoning,  I'm curious about whether the performance of GS-Reasoner on general-purpose benchmarks would decline after being trained on such a large amount of 3D-related data, and if so, by how much.\n2.The construction pipeline of this representation method is somehow complex. It requires parallel execution of a 2D vision encoder, a 3D point cloud encoder, and an additional dual-path pooling fusion module, I wonder what is the. compute cost of these modules.\n3.The \"geometric\" component of the entire representation heavily relies on the quality of depth maps and point clouds generated by preceding steps. In real-world scenarios, depth estimation (e.g., via VGGT-SLAM or MoGe-2) is inherently imperfect, I think it will be better to show how these noise will affect the performance of model."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i6exx98UAQ", "forum": "CfKi92bgnq", "replyto": "CfKi92bgnq", "signatures": ["ICLR.cc/2026/Conference/Submission1387/Reviewer_TRwm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1387/Reviewer_TRwm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909639139, "cdate": 1761909639139, "tmdate": 1762915758269, "mdate": 1762915758269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GS-Reasoner, and a GCoT dataset, to tackle 3D visual grounding. Specifically, GS-Reasoner encodes image and 3D representations with a semantic geometric fusion model before feeding into a Video LLM for finetuning. GCoT dataset, on the other hand, construct QA pairs, and then augment them with CoT paths with GPT-4o based on the information of the bird’s eye view, object information, and Q&A. GS-Reasoner, when fine-tuned with GCoT, performed better on multiple visual grounding datasets, including ScanRefer, Multi3DRef, and spatial reasoning benchmarks like VSI-Bench, etc."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I think the architecture itself is a great contribution. Fusing depth and image seems like a straightforward but valid approach to improve spatial reasoning. I do wonder whether other information like normal maps could potentially give similar results.\n\nFor Table 2, I think showing the results of predicted and GT depth is a great addition, as it allows us to understand the upper bound of the current architecture and training.\n\nBaselines are strong, spanning across closed and open-source models, as well as expert VLMs, with recent state-of-the arts such as VLM-3R-7B."}, "weaknesses": {"value": "One of the main claims of this paper is that 3D visual grounding is the cornerstone of spatial reasoning. This suggests that improvement on spatial grounding would generally improve Spatial VQA, and from this perspective I find just the set of benchmarks tested is slightly lacking given the large variety of spatial reasoning benchmarks these days. To my understanding, SQA3D, Scan2Cap, and ScanQA questions are still majorly descriptors/grounding of 3D objects in a scene in similar scale. VSI-Bench is also heavily based on ScanNet and ScanNet++. The full story of spatial reasoning may not be told with these datasets alone. Some additional benchmarks that could be helpful (not asking for all evaluation but some additional ones with larger domain shift): SPAR-Bench, All-Angles Bench, MMSI-Bench, etc. This would give us insights on better visual grounding that translates to general 3D understanding. \n\nThe same questions also apply to training with/without GCoT and with/without CoT within the GCoT dataset. I believe understanding these would make the paper more comprehensive and solidify the claim that visual grounding is highly correlated with the other spatial reasoning tasks with larger domain change."}, "questions": {"value": "The main questions I have derived from the Weaknesses.\n\n1. Is GS-Reasoner set up purely for visual grounding? Or does it actually help with more generalized spatial reasoning to other scales and other types of questions?\n\n2. Does fine tuning on the GCoT dataset help with visual grounding? Does it help with other spatial reasoning benchmarks?\n\n3. Does CoT within the GCoT dataset help with visual grounding and other benchmarks?\n\nOverall, I think the current stage of the experiments are not comprehensive for me to accept just yet. I hope the authors can shed light on some of the questions listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2RgxHJvtyK", "forum": "CfKi92bgnq", "replyto": "CfKi92bgnq", "signatures": ["ICLR.cc/2026/Conference/Submission1387/Reviewer_L2UB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1387/Reviewer_L2UB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955834730, "cdate": 1761955834730, "tmdate": 1762915758147, "mdate": 1762915758147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unified representation of geometry and semantics, and proposes a method called Dual-Path Pooling to address misalignment issues in deriving per-patch representations. Furthermore, the authors present the GCoT dataset, which incorporates grounding as an intermediate step in spatial reasoning to enhance the spatial reasoning capabilities of MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The two identified misalignments are a valid concern, and the proposed Dual-Path Pooling offers a simple yet effective solution to mitigate them. In particular, the approach of directly sampling 3D points and subsequently interpolating their geometric features is both elegant and effective.\n\n2. I agree that grounding is essential for spatial reasoning, and the proposed GCoT appears to be of high quality.\n\n3. The experiments are sufficient to demonstrate the effectiveness of the proposed method and dataset."}, "weaknesses": {"value": "1. The paper claims that the lack of a unified 3D representation leads to reliance on external modules. However, as shown in Table 1, while the proposed method significantly improves the Acc@25 metric on ScanRefer, it does not outperform LLMs equipped with external grounding modules on Acc@50. This suggests that although the method enhances the model’s spatial reasoning capabilities, it does not improve its ability to perform precise object localization—and thus does not fully eliminate the need for additional localization modules. Accurate localization may still require such external modules. \n\n2. The efficiency of the proposed model is unclear; it would be beneficial to include an analysis or empirical results demonstrating its computational efficiency.\n\n3. Since GCoT is built upon GPT-4o, it is important to clarify how the authors ensure the correctness of the generated data. Providing a explanation of the validation or filtering mechanisms would help substantiate the quality of the proposed dataset and, in turn, strengthen its overall contribution."}, "questions": {"value": "The issues identified and the proposed solutions appear reasonable, and the introduced dataset makes a meaningful contribution. While a few minor points remain unclear, I believe they can be adequately addressed during the rebuttal phase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VUSqm7R2Pq", "forum": "CfKi92bgnq", "replyto": "CfKi92bgnq", "signatures": ["ICLR.cc/2026/Conference/Submission1387/Reviewer_2vrB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1387/Reviewer_2vrB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966269165, "cdate": 1761966269165, "tmdate": 1762915758030, "mdate": 1762915758030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a 3D large language model, GS-Reasoner, and introduces a new dataset, GCoT. The GS-Reasoner constructs a semantic–geometric hybrid representation of 3D scenes through a dual-path pooling mechanism. The model achieves strong performance on both 3D visual grounding and spatial reasoning tasks without relying on pretrained 3D detectors or external decoders."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation addressing key limitations of 3D LLMs. The work is both meaningful and timely, as it explores how to empower 3D LLMs with spatial reasoning and visual grounding capabilities without depending on pretrained 3D detectors or external decoders.\n2. Well-presented methodology and thorough validation. The paper provides detailed methodological descriptions and extensive experiments. The results are competitive on 3D visual grounding tasks and achieve state-of-the-art performance on spatial reasoning and general 3D benchmarks."}, "weaknesses": {"value": "Weaknesses: \n1. Performance gap with state-of-the-art baselines. The proposed GS-Reasoner still lags behind ROSS3D on several key benchmarks, including ScanRefer (Acc@50), Multi3DRef (F1@50), ScanQA, and SQA3D.\n2. Dependence on external modules for geometry estimation. The proposed GS-Reasoner still relies on VGGT-SLAM to estimate depth maps and camera parameters, which introduces additional dependencies and may limit the model’s end-to-end autonomy."}, "questions": {"value": "1. The proposed GS-Reasoner shows a relatively larger performance gap between Acc@25 and Acc@50 on ScanRefer, and between F1@25 and F1@50 on Multi3DRef, compared with other methods. Could the authors clarify the reason behind this discrepancy? Additionally, what strategies might help narrow this gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t6GMwkkQne", "forum": "CfKi92bgnq", "replyto": "CfKi92bgnq", "signatures": ["ICLR.cc/2026/Conference/Submission1387/Reviewer_ckhy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1387/Reviewer_ckhy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762502071599, "cdate": 1762502071599, "tmdate": 1762915757899, "mdate": 1762915757899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response for Common Concerns"}, "comment": {"value": "### **Relatively larger performance gap on Acc@25(F1@25) and Acc@50(F1@50) in visual grounding tasks**\n\nWe thank reviewers ckhy (Q1) and 2vrB (W1) for raising this question, and we provide a unified response here.\n\nIn fact, we have also noticed this issue during our research and have been trying to narrow this gap. One of the purposes of the dual-path pooling strategy proposed in the main paper is to reduce this performance gap. Dual-path pooling provides more accurate geometric features for image patches, enabling more precise localization. We have also validated the effectiveness of this strategy in our ablation studies(Tab. 5). It can be observed that the improvement in Acc@50 is significantly greater than that in Acc@25 when using this strategy. \n\nHowever, as the reviewers mentioned, the gap between GS-Reasoner and baseline methods in Acc@25(F1@25) and Acc@50(F1@50) remains relatively larger, which also results in GS-Reasoner performing worse than some baselines in Acc@50(F1@50). We conduct a comparative analysis of the differences between GS-Reasoner and baseline methods and believe that the key issues lie in the following two aspects:\n\n* **Mask supervision during visual grounding training**. Almost all baseline methods benefit from mask supervision, either directly (by using segmentation loss for training) or indirectly (by utilizing proposal priors, where proposals are extracted from mask3D trained with segmentation loss). Mask supervision is more conducive to precise object localization than bbox supervision, as also reported in Locate3D[1] and UniVLG[2].\n* **Inference with mesh point cloud input**. Most baseline methods use mesh point cloud input during visual grounding inference, while GS-Reasoner only uses sensor point cloud. Compared to sensor point cloud, mesh point cloud has less noise and higher point cloud density, providing more accurate geometric information that helps improve localization accuracy.\n\nIn the context of GS-Reasoner, we cannot leverage mask supervision to enhance grounding accuracy. Therefore, a direct approach is to increase the number of point cloud features contained within the target object, thereby compensating for the inherent limitations of sensor point clouds. We have implemented two methods to achieve this:\n* Increasing the number of input images, from 32 to 48 or 64;\n* Using a frame sampling algorithm that is relevant to the question context. Here, we adopt the algorithm proposed in cdViews[3] for sampling.\n\nBy employing the above two methods, we achieved the following results on ScanRefer and Multi3DRef:\n\n|  | ScanRefer Acc@25 | ScanRefer Acc@50 | Multi3DRef F1@25 | Multi3DRef F1@50 |\n| --- | --- | --- | --- | --- |\n| 32 frames uniform | 60.8 | 42.2 | 61.7 | 45.3 |\n| 48 frames uniform | 61.7 | 44.5 | 62.2 | 46.9 |\n| 64 frames uniform | 61.2 | 45.1 | 62.5 | 48.0 |\n| 32 frames cdViews sample | 61.1  | 43.4 | 61.9 | 46.2 |\n\nAs can be seen, uniformly sampling more frames leads to a **more pronounced improvement in Acc@50 (F1@50) compared to Acc@25 (F1@25)**, indicating that providing denser point clouds can indeed help enhance localization accuracy. Similarly, using a frame sampling algorithm relevant to the question context also yields comparable effects.\n\nWe believe that achieving further improvements under this setting is inherently challenging. In particular, surpassing the sota baselines in Acc@50 (F1@50) without any post-processing is difficult, as bounding-box supervision is fundamentally less informative than mask supervision in 3D visual grounding tasks.\n\n### **Computation efficiency of GS-Reasoner**\nGS-Reasoner's computational overhead compared to the backbone LLaVA-Next-Video primarily arises from the construction of visual tokens. Specifically, GS-Reasoner requires encoding point clouds using Point Transformer and integrating them into image patch features. Therefore, we conducted a comparison of computational overhead during the visual token construction phase, using a single 4090 GPU with 32 input images. The results are as follows (in milliseconds):\n\n|  | 2D vision encoder | 3D vision encoder | dual path pooling | total input preparation |\n| --- | --- | --- | --- | --- |\n| LLaVA-Next-Qwen | 429 | - | - | 470 |\n| GS-Reasoner | 430 | 204 | 41 | 737 |\n\n\nAs can be seen, the additional overhead introduced in the 3D vision encoder and dual-path pooling is acceptable. Regarding the subsequent VLM autoregressive inference phase, since the number of constructed visual tokens remains unchanged, the computational overhead is the same as that of the backbone.\n\n\n[1] McVay, Paul, et al. \"LOCATE 3D: Real-World Object Localization via Self-Supervised Learning in 3D.\" ICML 2025.\n\n[2] Jain, Ayush, et al. \"Unifying 2D and 3D Vision-Language Understanding.\" ICML 2025.\n\n[3] Huang, Jiangyong, et al. \"Unveiling the mist over 3d vision-language understanding: Object-centric evaluation with chain-of-analysis.\" CVPR 2025."}}, "id": "q8R0yQQXhf", "forum": "CfKi92bgnq", "replyto": "CfKi92bgnq", "signatures": ["ICLR.cc/2026/Conference/Submission1387/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1387/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission1387/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763723263901, "cdate": 1763723263901, "tmdate": 1763724387820, "mdate": 1763724387820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}