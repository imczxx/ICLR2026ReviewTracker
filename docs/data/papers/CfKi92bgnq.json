{"id": "CfKi92bgnq", "number": 1387, "cdate": 1756878992686, "mdate": 1759898211247, "content": {"title": "Reasoning in Space via Grounding in the World", "abstract": "In this paper, we claim that 3D visual grounding is the cornerstone of spatial reasoning and introduce the $\\textit{Grounded-Spatial Reasoner (GS-Reasoner)}$ to explore the effective spatial representations that bridge the gap between them. Existing 3D LLMs suffer from the absence of a unified 3D representation capable of jointly capturing semantic and geometric information. This deficiency is manifested either in poor performance on grounding or in an excessive reliance on external modules, ultimately hindering the seamless integration of grounding and spatial reasoning. To address this, we propose a simple yet effective \\emph{dual-path pooling} mechanism that tightly aligns geometric features with both semantic and positional cues, constructing a unified image patch-based 3D representation that encapsulates all essential information without extra tokens. Leveraging this holistic representation, GS-Reasoner is the first 3D LLMs that achieves autoregressive grounding entirely without external modules while delivering performance comparable to state-of-the-art models, establishing a unified and self-contained framework for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we introduce the $\\textit{Grounded Chain-of-Thought (GCoT)}$ dataset. This dataset is meticulously curated to include both 3D bounding box annotations for objects referenced in reasoning questions and step-by-step reasoning paths that integrate grounding as a core component of the problem-solving process. Extensive experiments demonstrate that GS-Reasoner achieves impressive results on 3D visual grounding, which in turn significantly enhances its spatial reasoning capabilities, leading to state-of-the-art performance.", "tldr": "", "keywords": ["3d spatial reasoning", "3d visual grounding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07abd2173febdd3309a8f706fce1b0a5b930a496.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GS-Reasoner, a novel spatial reasoning framework that enhances vision-language models' spatial understanding by incorporating 3D visual grounding as an intermediate reasoning step. Its core contributions include: a semantic-geometric hybrid 3D scene representation that aligns and fuses semantic features, geometric features, and 3D positional information via a dual-path pooling mechanism; the GCoT dataset, which provides 3D bounding box annotations and chain-of-thought reasoning paths to integrate grounding into spatial reasoning; and an autoregressive grounding capability that, enables end-to-end 3D localization without external detectors or modules, achieving competitive performance across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper is well written and easy to follow\n2.The dual-path pooling mechanism effectively mitigates misalignment issues between semantic-geometric and position-geometric features.\n3.The GCoT dataset fills a critical gap in existing resources for integrated \"grounding-reasoning\" tasks, and extensive evaluations across 3D grounding, spatial reasoning, and general 3D tasks provide comprehensive validation."}, "weaknesses": {"value": "1.The training stage is mainly focus on 3d reasoning,  I'm curious about whether the performance of GS-Reasoner on general-purpose benchmarks would decline after being trained on such a large amount of 3D-related data, and if so, by how much.\n2.The construction pipeline of this representation method is somehow complex. It requires parallel execution of a 2D vision encoder, a 3D point cloud encoder, and an additional dual-path pooling fusion module, I wonder what is the. compute cost of these modules.\n3.The \"geometric\" component of the entire representation heavily relies on the quality of depth maps and point clouds generated by preceding steps. In real-world scenarios, depth estimation (e.g., via VGGT-SLAM or MoGe-2) is inherently imperfect, I think it will be better to show how these noise will affect the performance of model."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i6exx98UAQ", "forum": "CfKi92bgnq", "replyto": "CfKi92bgnq", "signatures": ["ICLR.cc/2026/Conference/Submission1387/Reviewer_TRwm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1387/Reviewer_TRwm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909639139, "cdate": 1761909639139, "tmdate": 1762915758269, "mdate": 1762915758269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GS-Reasoner, and a GCoT dataset, to tackle 3D visual grounding. Specifically, GS-Reasoner encodes image and 3D representations with a semantic geometric fusion model before feeding into a Video LLM for finetuning. GCoT dataset, on the other hand, construct QA pairs, and then augment them with CoT paths with GPT-4o based on the information of the bird’s eye view, object information, and Q&A. GS-Reasoner, when fine-tuned with GCoT, performed better on multiple visual grounding datasets, including ScanRefer, Multi3DRef, and spatial reasoning benchmarks like VSI-Bench, etc."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I think the architecture itself is a great contribution. Fusing depth and image seems like a straightforward but valid approach to improve spatial reasoning. I do wonder whether other information like normal maps could potentially give similar results.\n\nFor Table 2, I think showing the results of predicted and GT depth is a great addition, as it allows us to understand the upper bound of the current architecture and training.\n\nBaselines are strong, spanning across closed and open-source models, as well as expert VLMs, with recent state-of-the arts such as VLM-3R-7B."}, "weaknesses": {"value": "One of the main claims of this paper is that 3D visual grounding is the cornerstone of spatial reasoning. This suggests that improvement on spatial grounding would generally improve Spatial VQA, and from this perspective I find just the set of benchmarks tested is slightly lacking given the large variety of spatial reasoning benchmarks these days. To my understanding, SQA3D, Scan2Cap, and ScanQA questions are still majorly descriptors/grounding of 3D objects in a scene in similar scale. VSI-Bench is also heavily based on ScanNet and ScanNet++. The full story of spatial reasoning may not be told with these datasets alone. Some additional benchmarks that could be helpful (not asking for all evaluation but some additional ones with larger domain shift): SPAR-Bench, All-Angles Bench, MMSI-Bench, etc. This would give us insights on better visual grounding that translates to general 3D understanding. \n\nThe same questions also apply to training with/without GCoT and with/without CoT within the GCoT dataset. I believe understanding these would make the paper more comprehensive and solidify the claim that visual grounding is highly correlated with the other spatial reasoning tasks with larger domain change."}, "questions": {"value": "The main questions I have derived from the Weaknesses.\n\n1. Is GS-Reasoner set up purely for visual grounding? Or does it actually help with more generalized spatial reasoning to other scales and other types of questions?\n\n2. Does fine tuning on the GCoT dataset help with visual grounding? Does it help with other spatial reasoning benchmarks?\n\n3. Does CoT within the GCoT dataset help with visual grounding and other benchmarks?\n\nOverall, I think the current stage of the experiments are not comprehensive for me to accept just yet. I hope the authors can shed light on some of the questions listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2RgxHJvtyK", "forum": "CfKi92bgnq", "replyto": "CfKi92bgnq", "signatures": ["ICLR.cc/2026/Conference/Submission1387/Reviewer_L2UB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1387/Reviewer_L2UB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955834730, "cdate": 1761955834730, "tmdate": 1762915758147, "mdate": 1762915758147, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unified representation of geometry and semantics, and proposes a method called Dual-Path Pooling to address misalignment issues in deriving per-patch representations. Furthermore, the authors present the GCoT dataset, which incorporates grounding as an intermediate step in spatial reasoning to enhance the spatial reasoning capabilities of MLLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The two identified misalignments are a valid concern, and the proposed Dual-Path Pooling offers a simple yet effective solution to mitigate them. In particular, the approach of directly sampling 3D points and subsequently interpolating their geometric features is both elegant and effective.\n\n2. I agree that grounding is essential for spatial reasoning, and the proposed GCoT appears to be of high quality.\n\n3. The experiments are sufficient to demonstrate the effectiveness of the proposed method and dataset."}, "weaknesses": {"value": "1. The paper claims that the lack of a unified 3D representation leads to reliance on external modules. However, as shown in Table 1, while the proposed method significantly improves the Acc@25 metric on ScanRefer, it does not outperform LLMs equipped with external grounding modules on Acc@50. This suggests that although the method enhances the model’s spatial reasoning capabilities, it does not improve its ability to perform precise object localization—and thus does not fully eliminate the need for additional localization modules. Accurate localization may still require such external modules. \n\n2. The efficiency of the proposed model is unclear; it would be beneficial to include an analysis or empirical results demonstrating its computational efficiency.\n\n3. Since GCoT is built upon GPT-4o, it is important to clarify how the authors ensure the correctness of the generated data. Providing a explanation of the validation or filtering mechanisms would help substantiate the quality of the proposed dataset and, in turn, strengthen its overall contribution."}, "questions": {"value": "The issues identified and the proposed solutions appear reasonable, and the introduced dataset makes a meaningful contribution. While a few minor points remain unclear, I believe they can be adequately addressed during the rebuttal phase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VUSqm7R2Pq", "forum": "CfKi92bgnq", "replyto": "CfKi92bgnq", "signatures": ["ICLR.cc/2026/Conference/Submission1387/Reviewer_2vrB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1387/Reviewer_2vrB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966269165, "cdate": 1761966269165, "tmdate": 1762915758030, "mdate": 1762915758030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a 3D large language model, GS-Reasoner, and introduces a new dataset, GCoT. The GS-Reasoner constructs a semantic–geometric hybrid representation of 3D scenes through a dual-path pooling mechanism. The model achieves strong performance on both 3D visual grounding and spatial reasoning tasks without relying on pretrained 3D detectors or external decoders."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation addressing key limitations of 3D LLMs. The work is both meaningful and timely, as it explores how to empower 3D LLMs with spatial reasoning and visual grounding capabilities without depending on pretrained 3D detectors or external decoders.\n2. Well-presented methodology and thorough validation. The paper provides detailed methodological descriptions and extensive experiments. The results are competitive on 3D visual grounding tasks and achieve state-of-the-art performance on spatial reasoning and general 3D benchmarks."}, "weaknesses": {"value": "Weaknesses: \n1. Performance gap with state-of-the-art baselines. The proposed GS-Reasoner still lags behind ROSS3D on several key benchmarks, including ScanRefer (Acc@50), Multi3DRef (F1@50), ScanQA, and SQA3D.\n2. Dependence on external modules for geometry estimation. The proposed GS-Reasoner still relies on VGGT-SLAM to estimate depth maps and camera parameters, which introduces additional dependencies and may limit the model’s end-to-end autonomy."}, "questions": {"value": "1. The proposed GS-Reasoner shows a relatively larger performance gap between Acc@25 and Acc@50 on ScanRefer, and between F1@25 and F1@50 on Multi3DRef, compared with other methods. Could the authors clarify the reason behind this discrepancy? Additionally, what strategies might help narrow this gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t6GMwkkQne", "forum": "CfKi92bgnq", "replyto": "CfKi92bgnq", "signatures": ["ICLR.cc/2026/Conference/Submission1387/Reviewer_ckhy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1387/Reviewer_ckhy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1387/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762502071599, "cdate": 1762502071599, "tmdate": 1762915757899, "mdate": 1762915757899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}