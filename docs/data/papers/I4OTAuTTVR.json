{"id": "I4OTAuTTVR", "number": 10123, "cdate": 1758161348508, "mdate": 1763638263083, "content": {"title": "In-Context Learning can Perform Continual Learning Like Humans", "abstract": "Large language models (LLMs) can adapt to new tasks via in-context learning (ICL) without parameter updates, making them powerful learning engines for fast adaptation. While extensive research has examined ICL as a few-shot learner, whether it can achieve long-term retention and cross-task knowledge accumulation when multitasks arrive sequentially remains underexplored. Motivated by human memory studies, we investigate the retention characteristics of ICL in multitask settings and extend it to in-context continual learning (ICCL), where continual learning ability emerges through task scheduling and prompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that, for specific large-language models, ICCL benefits from distributed practice (DP) in a manner analogous to humans, consistently revealing a spacing ``sweet spot'' for retention. Beyond retention performance, we propose a human-retention similarity metric to quantify how closely a continual-learning (CL) method aligns with human retention dynamics. Using this metric, we show that linear-attention models such as MAMBA and RWKV exhibit particularly human-like retention patterns, despite their retention performance lagging behind that of Transformer-based LLMs. Overall, our results establish ICCL as both cognitively plausible and practically effective, providing an inference-only CL paradigm that enables LLMs to maintain long-term retention of a target task under cross-task interference without parameter updates.", "tldr": "Human-inspired spacing turns LLMs into efficient continual learners that forget less.", "keywords": ["In-Context Learning; Continual Learning; Retention"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c1ab92037ad8dc73511ff9056553079e77e0e40d.pdf", "supplementary_material": "/attachment/3ae3e4d3ddef3d7884099c6528a60999919403d2.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes In-Context Continual Learning (ICCL), extending in-context learning (ICL) to handle sequential tasks without parameter updates. Drawing on human memory research, it studies how large language models retain information under different task schedules and finds that Distributed Practice yields a spacing “sweet spot” that enhances long-term retention, similar to human learning. A new Human Retention Similarity (HRS-MD) metric quantifies how closely model retention matches human memory patterns. Experiments on Markov-chain tasks show that while Transformer models achieve higher retention, linear-attention models (e.g., MAMBA, RWKV) exhibit more human-like forgetting curves. The study positions ICCL as a cognitively grounded, inference-only approach to continual learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces an innovative concept, posing an interesting question: Can in-context learning itself support continuous learning without gradient updates? It successfully bridges cognitive science and artificial intelligence.\n2. By introducing HRS-MD as a quantitative metric for “human-like similarity” in memory and linking ICCL with the interval effect and ACT-R memory model, this study establishes a robust, interpretable analogy between human memory retention and large language model behavior—a finding of significant value for understanding the emergent properties of ICL."}, "weaknesses": {"value": "1. While this paper empirically observes knowledge retention patterns, it fails to fully elucidate the ICCL mechanism, such as how it supports sustained retention. Without theoretical analysis, it remains unclear whether ICCL reflects genuine information memory or merely short-term contextual reuse. \n2. Based on the above, the lack of experimental comparisons with existing memory enhancement models makes it difficult to assess whether ICCL truly offers novel capabilities or merely replicates partial effects of these architectures under specific conditions.\n3. The title and abstract assert that ICL “performs continual learning like humans.” However, the ICCL setup involves rearranging demonstrations within a single prompt rather than true knowledge transfer or consolidation over long timescales. This is conceptually closer to contextual rehearsal than to true continual learning, where the model updates its internal state across episodes. A more cautious framing might be advisable."}, "questions": {"value": "1. Does ICCL's performance depend on the available context length? Specifically, how does its memory retention capability vary with context size? This is crucial for evaluating whether ICCL can achieve practical scalability beyond short sequence tasks. Additionally, how sensitive are the results in the paper to the selected hyperparameters?\n2. Can the conclusions regarding “human-like continuous learning” be extended to real-world language or reasoning tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vBLILPVvrr", "forum": "I4OTAuTTVR", "replyto": "I4OTAuTTVR", "signatures": ["ICLR.cc/2026/Conference/Submission10123/Reviewer_3FtD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10123/Reviewer_3FtD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724615012, "cdate": 1761724615012, "tmdate": 1762921495580, "mdate": 1762921495580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Overall Clarification and Summary of Revisions"}, "comment": {"value": "We thank all reviewers for their insightful feedback. Across the revision, we have clarified the scope and positioning of our work: ICCL is not a new parametric continual-learning method, but the first framework aimed at understanding inference-time retention and maintaining long-term retention of a target task under sequential task interference. We strengthened the motivation, refined the framing in Sections 1–3, and added clearer descriptions of our benchmarks, task construction, and experimental setup in Appendix A2, A4 and A6. We also expanded analyses to show that the retention patterns we report (such as spacing effects and interference-driven decay) are robust across both synthetic and language-style benchmarks. These revisions address the key questions raised by reviewers and make the contribution and empirical evidence of ICCL more precise and transparent."}}, "id": "YzDmEI3jkE", "forum": "I4OTAuTTVR", "replyto": "I4OTAuTTVR", "signatures": ["ICLR.cc/2026/Conference/Submission10123/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10123/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10123/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763626672021, "cdate": 1763626672021, "tmdate": 1763626672021, "mdate": 1763626672021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends in-context learning to an inference-only continual learning setting, termed ICCL, through task scheduling and prompt rearrangement. Experiments on Markov-chain benchmarks show that ICCL benefits from distributed practice, revealing a human-like spacing effect that enhances retention. The proposed HRS-MD metric further quantifies similarity to human memory, showing that linear-attention models such as Mamba and RWKV exhibit more human-like retention dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and easy to follow. The study explores an interesting and novel perspective by connecting in-context learning with human memory theories. The idea of modeling retention dynamics through prompt scheduling and drawing parallels to human spaced learning is creative and provides valuable insights into the cognitive plausibility of large language models."}, "weaknesses": {"value": "While the idea of connecting in-context learning with human memory is interesting, I’m not fully convinced that the proposed ICCL setup really fits the continual learning setting. Since the model doesn’t update its parameters and only relies on prompt scheduling or rearrangement, it feels more like a clever version of memory replay than real continual learning.\n\nI have several concerns:\n\n* From a CL standpoint, as illustrated in Fig. 1, when evaluating task i, the inclusion of demonstrations from other tasks seems unnecessary. I think the performance on task i should primarily depend on its own data, and adding unrelated task examples may introduce noise rather than benefit learning. If those experience of other tasks were removed, the setup would simply become standard ICL again — so it feels like the cross-task demonstrations are added mainly to make the setting look like CL. It’s not clear whether doing so is really necessary or meaningful.\n\n* In the “massed practice” setup, I think simply reordering the experiences of the target task and other tasks might actually lead to better results. This way, the most relevant examples are placed closer to the query, which could even outperform the proposed distributed practice (DP) setting. Did you try this?\n\n* Because there’s no training or parameter update, the model’s performance on new tasks depends entirely on a few in-context examples. This makes it hard to believe the method can generalize well to new or unseen tasks.\n\n* The results could also be quite sensitive to how the “experiences” (both from target and other tasks) are selected. This is essentially the same issue as demonstration selection in ICL, and comparing to existing retrieval-based methods would make the evaluation more convincing.\n\nOverall, the paper feels more like a cognitive-style study of LLM memory behavior than a real CL method. It’s interesting, but the framing as continual learning seems a bit forced."}, "questions": {"value": "Please see the Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9mDUlT8e1A", "forum": "I4OTAuTTVR", "replyto": "I4OTAuTTVR", "signatures": ["ICLR.cc/2026/Conference/Submission10123/Reviewer_f1EV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10123/Reviewer_f1EV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826419477, "cdate": 1761826419477, "tmdate": 1762921495207, "mdate": 1762921495207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the capability of large language models (LLMs) to perform continual learning through in-context mechanisms, introducing the concept of in-context continual learning (ICCL). Drawing inspiration from human memory studies, the authors analyze how LLMs retain and accumulate knowledge across sequential multitask settings without parameter updates. Using Markov-Chain-based benchmarks, they demonstrate that ICCL exhibits human-like retention behaviors. They further introduce a human-retention similarity metric to measure alignment between model and human retention patterns, revealing that linear-attention models show more human-like retention despite lower absolute performance than Transformers. The findings suggest ICCL offers a cognitively grounded and practical, inference-only approach to continual learning that alleviates catastrophic forgetting."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea of in-context continual learning (ICCL) is novel and conceptually intriguing. The paper's design draws inspiration from cognitive psychology, adding an interesting interdisciplinary angle. Table 1 provides a clear and helpful comparison among gradient-based continual learning, standard in-context learning, and the proposed ICCL approach."}, "weaknesses": {"value": "The motivation and design of ICCL need stronger justification. Essentially, ICCL remains a variant of in-context learning that includes demonstrations from multiple tasks rather than just the current one. While this may improve performance when the tasks are similar, it seems less effective when the tasks differ significantly. In such cases, using independent in-context learning for each task may yield better results, as it avoids the interference caused by unrelated demonstrations."}, "questions": {"value": "1. At the beginning of Section 2.1, in the definition of $D_{\\varphi}^{\\tau}$, only $y$ carries the superscript $\\tau$ (task index) while $x$ does not. Does this imply that different tasks share the same input distribution?  \n2. Section 2.3 is difficult to follow, particularly for readers without a background in cognitive psychology. A more accessible explanation or illustrative example would be helpful.  \n3. It is unclear why retention (or forgetting) should be a concern in ICCL. In gradient-based continual learning, parameter updates can cause forgetting, but in ICCL the parameters remain fixed. If one simply restores the demonstration samples for a previous task, its performance should, in principle, be fully recovered (implying zero forgetting). Please clarify this point.  \n4. The explanation of Eq. (6) is too brief. Since this equation is central to interpreting the experimental results, a more detailed discussion of its derivation and implications would strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hMciDdy8YV", "forum": "I4OTAuTTVR", "replyto": "I4OTAuTTVR", "signatures": ["ICLR.cc/2026/Conference/Submission10123/Reviewer_pMTu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10123/Reviewer_pMTu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941282973, "cdate": 1761941282973, "tmdate": 1762921494644, "mdate": 1762921494644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes In-Context Continual Learning (ICCL), an extension of ICL that lets LLMs retain knowledge across sequential tasks through task scheduling and prompt rearrangement, without updating parameters. Furthermore, a human-retention similarity metric (i.e., Human Retention Similarity via Mahalanobis Distance, HRS-MD) is proposed to evaluate alignment between human retention dynamics and continual learning methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The retention characteristics of ICL in multi-task settings are extended to ICCL.\n- The HRS-MD metric is proposed to quantify alignment with human memory dynamics."}, "weaknesses": {"value": "- Overall, the ICCL method appears overly simple and lacks novelty, as the ordering of demonstrations has already been extensively studied (e.g., for multi-task settings [1] , others as noted in Section 4). Most importantly, the paper does not discuss the distinction between multi-task and single-task settings in the context of the demonstration arrangement problem. For instance, will the results differ if $\\varphi_I$ is simply removed from DP? Furthermore, will the results be affected if different $\\varphi_I$ (i.e., other tasks) are rearranged?\n- To further validate the effectiveness of ICCL, more recent related GBCL baselines (e.g., EC-ACE [2], PGP [3], InsCL [4], etc.) should be discussed and compared. Some of these methods are applied to either Natural Language Processing or Computer Vision tasks, making them valuable for real-world applications.\n- This paper only focuses on a single synthetic task (Discrete Markov Chains) and lacks real-world benchmarks, which significantly reduces the generalizability of the proposed method.\n- The authors claim that, with larger $\\varphi_D$, ICCL models exhibit more stable outcomes (see Lines 308–311). However, in Table 2, ER and EWC demonstrate greater stability compared to SGD, showing a marginal gap compared with ICCL methods. Does this suggest that the GBCL methods exhibit greater variability, and that incorporating more recent baselines (see Weakness above) could lead to different conclusions?\n- Figure 3 is hard to understand, IMO. I was unable to distinguish between the results with and without task identifiers.\n- The benchmark task of Discrete Markov Chains (DMC) should be discussed in detail. Additionally, on what basis are parameters (e.g., $\\varphi, K, \\varphi_D$) set? For instance, this paper does not include an ablation study on $K$.\nMinor:\n- Line 105: \"a\" $\\rightarrow$ \"an\"\n- Line 258: \"$\\varphi_I \\in [...]$\" $\\rightarrow$ \"$\\varphi_I \\in \\{[...]\\}$\"\nReferences:\n[1] Xiong, Zheyang, et al. \"Everything everywhere all at once: Llms can in-context learn multiple tasks in superposition.\" arXiv preprint arXiv:2410.05603 (2024).\n[2] Caccia, Lucas, et al. \"New insights on reducing abrupt representation change in online continual learning.\" arXiv preprint arXiv:2104.05025 (2021).\n[3] Qiao, Jingyang, et al. \"Prompt gradient projection for continual learning.\" The Twelfth International Conference on Learning Representations. 2024.\n[4] Wang, Yifan, et al. \"Inscl: A data-efficient continual learning paradigm for fine-tuning large language models with instructions.\" arXiv preprint arXiv:2403.11435 (2024)."}, "questions": {"value": "please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ua8ZL5oe0D", "forum": "I4OTAuTTVR", "replyto": "I4OTAuTTVR", "signatures": ["ICLR.cc/2026/Conference/Submission10123/Reviewer_qTMj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10123/Reviewer_qTMj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10123/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762001519580, "cdate": 1762001519580, "tmdate": 1762921494033, "mdate": 1762921494033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}