{"id": "YvmR4vNai2", "number": 2523, "cdate": 1757132881492, "mdate": 1759898143199, "content": {"title": "Advancing Universal Deep Learning for Electronic-Structure Hamiltonian Prediction of Materials", "abstract": "Deep learning methods for electronic-structure Hamiltonian prediction has offered significant computational efficiency advantages over traditional density functional theory (DFT), yet the diversity of atomic types, structural patterns, and the high-dimensional complexity of Hamiltonians pose substantial challenges to the generalization performance. In this work, we contribute on both the  methodology and dataset sides to advance universal  deep learning paradigm for Hamiltonian prediction.  On the method side, we propose $NextHAM$, a neural E(3)-symmetry and expressive correction method for efficient and  generalizable materials electronic-structure Hamiltonian prediction. First, we  introduce the zeroth-step Hamiltonians, which can be efficiently constructed by the initial charge density of DFT, as informative descriptors of neural regression model in the input level and initial estimates of the target Hamiltonian in the output level, so that the regression model directly predicts the correction terms to the target ground truths, thereby significantly simplifying the input-output mapping and facilitating fine-grained predictions. Second, we present a neural Transformer architecture with strict E(3)-Symmetry and high non-linear expressiveness for Hamiltonian prediction. Third, we propose a novel training objective to ensure the accuracy performance of Hamiltonians in both real space and reciprocal space, preventing error amplification and the occurrence of \"ghost states\" caused by the large condition number of the overlap matrix. On the dataset side, we curate a broad-coverage large benchmark, namely Materials-HAM-SOC,  comprising $17,000$ material structures spanning $68$ elements from six rows of the periodic table and explicitly incorporating spin–orbit coupling (SOC) effects, providing  high-quality data resources for training and evaluation. Experimental results on Materials-HAM-SOC demonstrate that NextHAM achieves excellent accuracy in predicting Hamiltonians and band structures, with spin-off-diagonal block  reaching the accuracy of sub-$\\mu$eV scale. These results establish NextHAM as a universal and highly accurate deep learning model for electronic-structure prediction, delivering DFT-level precision with dramatically improved computational efficiency.", "tldr": "An advanced deep learning method and a comprehensive benchmark for universal Hamiltonian prediction across materials", "keywords": ["AI for Materials", "Deep Learning", "Electronic-Structure Hamiltonian Prediction"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/42108231fae2de430206f1af17699ea9b7646ca2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents NextHAM, a deep learning framework for predicting electronic structure Hamiltonians. Leveraging zeroth-step Hamiltonians as informative priors, NextHAM employs an E(3)-equivariant transformer for expressive modeling and incorporates a dual-space loss function to ensure both stability and accuracy. Additionally, the paper introduces a new dataset, Materials-HAM-SOC, which includes spin–orbit coupling effects, to facilitate benchmarking."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-structured and clearly written, offering an accessible and concise overview of both the problem and the proposed solution.\n* The appendix includes a thorough introduction to the relevant DFT background, making the work particularly approachable for readers less familiar with the field.\n* The authors open source their code. Moreover, the authors provide a new dataset of material Hamiltonians, which represents a valuable contribution to the open-source community in this domain."}, "weaknesses": {"value": "* The authors propose using the zeroth-step Hamiltonian as input and training the model to predict the correction term—a strategy that essentially follows the well-established delta-learning paradigm in machine learning. This approach has been previously employed in the context of machine-learned density functional theory (DFT), for instance in [1]. While the ablation study demonstrates that this technique effectively reduces prediction errors, it does not introduce significant novelty.\n* Although the related work section discusses equivariant approaches, it omits several key references. While this paper focuses on Hamiltonian prediction, the earliest and most influential applications of equivariance in materials science were in machine-learned force fields—foundational works that paved the way for modern equivariant networks. These should be cited; for example, [2-10]. In addition, several important studies [11, 12] on using machine learning to predict Hamiltonians are also missing.\n* The evaluation results are primarily demonstrated on the authors’ newly constructed dataset, Materials-HAM-SOC. However, to provide a more comprehensive and convincing assessment of the model’s true performance, comparisons with existing methods on established public benchmarks are essential.\n* I greatly appreciate the authors’ inclusion of a detailed introduction to DFT-related background in the appendix, which enhances accessibility. That said, since the proposed model heavily relies on TraceGrad, the current manuscript provides surprisingly little explanation of it. For the sake of completeness and self-containment, I recommend adding a more thorough description of TraceGrad in the appendix, so readers do not need to consult external sources to understand key technical details.\n\n[1] Chen Y, Zhang L, Wang H, et al. DeePKS: A comprehensive data-driven approach toward chemically accurate density functional theory[J]. Journal of Chemical Theory and Computation, 2020, 17(1): 170-181.\n\n[2] Schütt K T, Sauceda H E, Kindermans P J, et al. Schnet–a deep learning architecture for molecules and materials[J]. The Journal of chemical physics, 2018, 148(24).\n\n[3] Batzner S, Musaelian A, Sun L, et al. E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials[J]. Nature communications, 2022, 13(1): 2453.\n\n[4] Thomas N, Smidt T, Kearnes S, et al. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds[J]. arXiv preprint arXiv:1802.08219, 2018.\n\n[5] Musaelian A, Batzner S, Johansson A, et al. Learning local equivariant representations for large-scale atomistic dynamics[J]. Nature Communications, 2023, 14(1): 579.\n\n[6] Gasteiger J, Groß J, Günnemann S. Directional message passing for molecular graphs[J]. arXiv preprint arXiv:2003.03123, 2020.\n\n[7] Gasteiger J, Becker F, Günnemann S. Gemnet: Universal directional graph neural networks for molecules[J]. Advances in Neural Information Processing Systems, 2021, 34: 6790-6802.\n\n[8] Wang Y, Wang T, Li S, et al. Enhancing geometric representations for molecules with equivariant vector-scalar interactive message passing[J]. Nature Communications, 2024, 15(1): 313.\n\n[9] Wang Z, Liu G, Zhou Y, et al. Efficiently incorporating quintuple interactions into geometric deep learning force fields[J]. Advances in Neural Information Processing Systems, 2023, 36: 77043-77055.\n\n[10] Batatia I, Kovacs D P, Simm G, et al. MACE: Higher order equivariant message passing neural networks for fast and accurate force fields[J]. Advances in neural information processing systems, 2022, 35: 11423-11436.\n\n[11] Schütt K T, Gastegger M, Tkatchenko A, et al. Unifying machine learning and quantum chemistry with a deep neural network for molecular wavefunctions[J]. Nature communications, 2019, 10(1): 5024.\n\n[12] Unke O, Bogojeski M, Gastegger M, et al. SE (3)-equivariant prediction of molecular wavefunctions and electronic densities[J]. Advances in Neural Information Processing Systems, 2021, 34: 14434-14447."}, "questions": {"value": "* I greatly appreciate the authors’ effort in performing extensive DFT calculations to provide the open-source community with a new dataset of material Hamiltonians. The paper mentions that “17,000 material structures [were] sampled from the Materials Project.” I am curious about the selection criteria: many of these materials exhibit weak SOC effects. What is the rationale for performing the more computationally expensive SOC-inclusive calculations on such systems? What benefits does this bring, especially when SOC contributions are negligible?\n* NextHAM uses the zeroth-order Hamiltonian $H_0$ as model input. The paper states: “These two types of sub-matrices can naturally serve as the initial descriptors for nodes and edges, respectively, in the graph neural network.” While I agree this is a natural choice, I would like to understand more concretely how these equivariant matrices are transformed into node and edge features. Moreover, have the authors considered alternative approaches to constructing $H_0$-based features, such as those proposed in [1] and [2]?\n* In the introduction section, the authors note that prior works often “neglect spin–orbit coupling (SOC) effects.” However, to the best of my knowledge, DeepH-E3 [3] already incorporates a method to handle SOC. This makes the introductory claim somewhat inaccurate. More importantly, does NextHAM introduce any specific architectural or algorithmic modifications to better capture SOC effects?\n* The paper frames the use of $H_0$ as a way to inject prior knowledge into the model. In the Hamiltonian prediction literature, other forms of prior knowledge have also been explored—for instance, enforcing self-consistency constraints [2,4,5]. It would be valuable to include a discussion comparing different strategies for incorporating physical priors: what are their respective strengths, limitations, and trade-offs in terms of accuracy, computational cost, and generalizability?\n\n[1] Qiao Z, Christensen A S, Welborn M, et al. Informing geometric deep learning with electronic interactions to accelerate quantum chemistry[J]. Proceedings of the National Academy of Sciences, 2022, 119(31): e2205221119.\n\n[2] Wang Z, Liu C, Zou N, et al. Infusing self-consistency into density functional theory hamiltonian prediction via deep equilibrium models[J]. Advances in Neural Information Processing Systems, 2024, 37: 89652-89681.\n\n[3] Gong X, Li H, Zou N, et al. General framework for E (3)-equivariant neural network representation of density functional theory Hamiltonian[J]. Nature Communications, 2023, 14(1): 2848.\n\n[4] Zhang H, Liu C, Wang Z, et al. Self-consistency training for density-functional-theory hamiltonian prediction[J]. arXiv preprint arXiv:2403.09560, 2024.\n\n[5] Li Y, Tang Z, Chen Z, et al. Neural-network density functional theory based on variational energy minimization[J]. Physical Review Letters, 2024, 133(7): 076401."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "P3esh9EBeQ", "forum": "YvmR4vNai2", "replyto": "YvmR4vNai2", "signatures": ["ICLR.cc/2026/Conference/Submission2523/Reviewer_Yiie"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2523/Reviewer_Yiie"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760798430767, "cdate": 1760798430767, "tmdate": 1762916266120, "mdate": 1762916266120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on predicting the hamiltonian matrix of materials. There are four core contributions that I understood:\n\n- Using H0 as an initial guess and predicting the difference\n- Incorporating the TraceGrad architecture (although I'm not sure if this is intended to be a novelty, since it seems to be already published). \n- Incorporating the coupling between occupied and unoccupied states of the reciprocal Hamiltonian into the loss\n- A new dataset of material hamiltonians"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I think that predicting the difference between H0 and the final Hamiltonian is an interesting approach. However, as pointed out by the authors in the appendix, it comes at a formidable computational cost (55s on average for structures which seem to have between 5 and 20 atoms, going by Fig. 7).\n\nI also appreciate that this network accounts for SOC (the only other one which does so to my knowledge is DeepHE3)"}, "weaknesses": {"value": "My primary concerns are (1) I think there have been simpler ways to do what the authors are doing with an otherwise complex architecture and loss function, (2) without either performance comparisons or applications (eg, scf iteration reduction), there is no way to judge the utility of their model, and (3) I question the universality and utility of this dataset of 12,000 structures, seemingly containing fewer than 10 instances of several atomic elements. I elaborate on these points below:\n\n(1):\n\nA lot of the components of this model (use of the trace as a supervision signal, initializing the features with H0, predicting the refinement of H) seem to be ultimately reducing the numerical range of the data (as mentioned in the paper). Other works have tackled the same problem by simply scaling the trace out of the data [1, 2], similar to what is commonly done in MLIPs. I'm not yet convinced that the architectural complications introduced has a benefit over simply scaling the data and using a regular equivariant network like DeepHE3 (which also treats SOC) or QHNet.  \n\nOther overly complex sections of the architecture include the sub-models for various radii. I think this kind of hard-coding defeats the purpose of either learning the correct attention mechanism, or simply learning the coulomb decay from data. Does this improve generalizability, compared to a model with the same number of parameters?\n\nLoss function: I understand that the near-ill-conditioning of the overlap matrix can amplify small errors in matrix elements. However, I believe this is effectively treated by Ref [3] in the wavefunction alignment loss. Can the authors discuss the computational cost and scaling of their loss function, and compare to using the wavefunction alignment loss? Could the effective separation between occupied and unoccupied subspaces be recovered by using a simpler subspace loss? \n\n(2):\n\nI'm not sure how to evaluate the performance of this model. Matrix-element MAE errors are meaningless when introducing a new dataset, especially one which covers multiple atomic elements since heavier elements will have larger-valued elements. The authors mentioned that off-diagonal matrix element errors are \"sub-meV\" - but these matrix elements are typically an order of magnitude lower than diagonal matrix elements already, so this is expected. MAE matrix element errors are only useful for comparing on benchmarks with other models. \n\nExamining the provided bandstructures in the appendix, and comparing them to those computed by methods like Wannierization, I think the errors seem to be fairly high. In many cases, a rigid shift from the bandstructures computed from H0 looks like it could recover the learned solution.\n\n(3):\n\nSeveral key details about the dataset are missing, such as the basis set used, and the distribution of the number of atoms per structures (the authors mention it does not contain large systems, but this is not quantified). My current understanding based on Fig 7 is that there are 5-20 atoms per structures, and the corresponding Hamiltonian matrices have sizes on the order of 100s. \n\nIn general, Hamiltonian datasets can contain fewer structures due to the increased amount of information-per-structure. However, I am questioning the utility of 12,000 training structures distributed across 68 elements. For example, looking at Fig 6a in the appendix, there seems to be on the order of 1 atom of several noble gases in the training data. I don't know if this can really be considered a representative dataset of \"68 atomic elements\". I think it would ultimately be more useful to focus on a smaller range of elements with better sampling across them.\n\n[1] Equivariant Electronic Hamiltonian Prediction with Many-Body Message Passing, https://arxiv.org/html/2508.15108v1\n[2] GEARS H: Accurate machine-learned Hamiltonians for next-generation device-scale modeling, https://arxiv.org/abs/2506.10298\n[3] Enhancing the Scalability and Applicability of Kohn-Sham Hamiltonians for Molecular Systems, https://arxiv.org/abs/2502.19227"}, "questions": {"value": "What is the computational complexity of this loss function?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6vmK2ovPKY", "forum": "YvmR4vNai2", "replyto": "YvmR4vNai2", "signatures": ["ICLR.cc/2026/Conference/Submission2523/Reviewer_hRHV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2523/Reviewer_hRHV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760830170840, "cdate": 1760830170840, "tmdate": 1762916265935, "mdate": 1762916265935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NextHAM, a neural E(3)-equivariant model for predicting materials' electronic structure Hamiltonians. The method incorporates zeroth-step Hamiltonians from the initial DFT charge density as both informative input descriptors and initial estimates for the target. The architecture employs a Transformer designed for strict E(3)-equivariance and high non-linear expressiveness. A novel training objective combining real and reciprocal space losses is introduced to mitigate error amplification and \"ghost states\" caused by the large condition number of the overlap matrix. Evaluated on a newly curated benchmark, Materials-HAM-SOC, NextHAM demonstrates excellent accuracy in predicting Hamiltonians and band structures, with spin-off-diagonal blocks reaching sub-μeV scale."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed NextHAM method is technically sound and demonstrates state-of-the-art performance on the newly introduced benchmark.\n* The release of the Materials-HAM-SOC dataset is a valuable contribution that will facilitate future research in machine learning for materials science."}, "weaknesses": {"value": "* Overall, the technical novelty is limited since the core technical components build heavily upon established concepts:\n\n * The delta-learning strategy (predicting residuals to a zeroth-order Hamiltonian) is a well-established paradigm in ML-based Hamiltonian and potential energy surface prediction [1, 2].\n\n  * The TraceGrad branch for non-linear expressiveness is a direct adaptation of a prior architecture [3] to this task.\n\n  * The primary novel contribution appears to be the reciprocal space ($k$-space) loss. However, its performance gain over the strong baseline (which already uses the zeroth-step Hamiltonian as an output estimate) seems marginal (Table 4). Furthermore, the authors do not provide sufficient empirical evidence or analysis to demonstrate how this loss concretely alleviates the \"ghost states\" issue.\n\n* The paper lacks comparisons with several relevant recent methods in the field (e.g., those methods for material systems mentioned in section 2), making it difficult to assess NextHAM's relative advancement. A well-defined, chemically meaningful accuracy threshold is also absent. While the reported metrics (e.g., sub-μeV) are low, it remains unclear whether this precision is sufficient for reliable downstream property predictions (e.g., band gaps, densities of states) in practical computational materials science.\n\n\n[1] Δ-Machine Learned Potential Energy Surfaces and Force Fields. JCTC 2022\n\n[2] Efficient and Scalable Density Functional Theory Hamiltonian Prediction through Adaptive Sparsity. ICML 2025\n\n[3] Tracegrad: a Framework Learning Expressive SO(3)-equivariant Non-linear Representations for Electronic-Structure Hamiltonian Prediction. ICML 2025"}, "questions": {"value": "* Could the authors provide more details on how the sub-matrices of the zeroth-step Hamiltonian are transformed into initial node and edge descriptors? The feature engineering process is crucial for reproducibility.\n\n* The term \"universal\" in the title is a strong claim, implying robust generalization across diverse chemical and conformational spaces. However, the current experiments primarily evaluate on a static benchmark. Could the authors provide additional evidence, such as out-of-distribution generalization tests, to substantiate this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns identified."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gv9UjcH1gR", "forum": "YvmR4vNai2", "replyto": "YvmR4vNai2", "signatures": ["ICLR.cc/2026/Conference/Submission2523/Reviewer_Gv72"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2523/Reviewer_Gv72"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865128669, "cdate": 1761865128669, "tmdate": 1762916265730, "mdate": 1762916265730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NextHAM, a deep-learning framework for predicting electronic-structure Hamiltonians of materials with near-DFT accuracy.\n\nThe method introduces a physically informed prior, the zeroth-step Hamiltonian $H^{(0)}$, derived from the initial electron density (superposition of atomic charge densities). The model learns only the correction $\\Delta H = H^{(T)} - H^{(0)}$, effectively simplifying the regression space.\n\nNextHAM uses a Transformer-based E(3)-equivariant architecture extended with the TraceGrad mechanism for non-linear expressiveness under strict SO(3)-equivariance, and jointly optimizes both real-space and reciprocal-space Hamiltonians to reduce “ghost-state” artifacts.\nA new dataset, Materials-HAM-SOC, is also released, covering 17 000 crystalline materials across 68 elements, computed with ABACUS + PYATB including explicit spin–orbit coupling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong Physical–ML Integration: The introduction of the zeroth-step Hamiltonian as both input and output prior is conceptually sound and reduces the learning complexity.\n2. Extends TraceGrad to an equivariant Transformer with enhanced edge-centric attention\n3. Training target in both real and reciprocal spaces\n4. Large-Scale and Diverse Datasets: Materials-HAM-SOC is the first open benchmark with SOC-included Hamiltonians across nearly the full periodic table, offering a valuable resource to the community."}, "weaknesses": {"value": "1. Marginal Conceptual Novelty:\nThe introduction of the “zeroth-step Hamiltonian” is interesting, but it can be viewed as a variant of the superposition of atomic potentials or the initial DFT guess—concepts that have already appeared in physics-informed ML models. Hence, the main contribution seems to lie in the integration and refinement of known components rather than in introducing a fundamentally new idea.\n2. Venue Suitability:\nThe contribution of this work feels more aligned with a scientific journal in computational materials science than with the core themes of ICLR. The paper emphasizes methodical accuracy, dataset curation, and physical completeness, but it lacks the conceptual abstraction or general ML innovation typically expected at this venue.\n3. Limited Evidence for Claimed Efficiency:\nThe paper claims substantial computational speed-ups, yet it provides no quantitative scaling analyses (e.g., runtime versus system size) or benchmarks against established ML Hamiltonian predictors such as DeepH-2. The reported improvements remain qualitative. I understand that the focus here is on SOC-inclusive Hamiltonians—a domain rarely addressed in prior work—but even so, direct numerical comparisons with existing open-source models would considerably strengthen the experimental section.\n4. Unclear Dataset Generation Strategy:\nThe dataset construction choices appear somewhat arbitrary. For instance, all DFT calculations use a 6×6×6 k-point grid, but no justification is given for this selection. Was this chosen to maintain consistent k-space density, convergence accuracy, or computational feasibility? A systematic benchmark or discussion of how these parameters were determined—either within this work or relative to existing datasets—would improve the scientific rigor.\n5. Lack of Physically Meaningful Evaluation:\nThe reported metrics (e.g., MAE of Hamiltonian matrix elements or visual comparison of band structures) show numerical accuracy but do not necessarily reflect physically interpretable performance. While these are useful from a machine-learning perspective, it would be more convincing to assess the model’s impact on derived physical quantities such as effective masses, band velocities, or even carrier mobilities. These quantities would offer a clearer picture of how well the learned Hamiltonians reproduce observable electronic behavior."}, "questions": {"value": "One point is unclear to me. In NAO-based electronic structure calculations, the reciprocal-space Hamiltonian $H(\\mathbf{k})$ is obtained through the Fourier transform of the real-space Hamiltonian $H(\\mathbf{R})$. However, the authors mention applying an 8 Å cutoff in their model architecture. This implies that for interatomic distances beyond 8 Å, the predicted matrix elements vanish, i.e., $H(\\mathbf{R}) = H^{(0)}(\\mathbf{R})$. If so, the Fourier transform would only include short-range interactions, and the long-range components in H(\\mathbf{k}) would be incorrect. How, then, does the model reconstruct a physically meaningful $H(\\mathbf{k})$ and corresponding band structure? Did I misunderstand the implementation, or is there an additional correction or extrapolation beyond the cutoff?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jtGtg2NUUi", "forum": "YvmR4vNai2", "replyto": "YvmR4vNai2", "signatures": ["ICLR.cc/2026/Conference/Submission2523/Reviewer_ts7f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2523/Reviewer_ts7f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2523/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883948108, "cdate": 1761883948108, "tmdate": 1762916265566, "mdate": 1762916265566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}