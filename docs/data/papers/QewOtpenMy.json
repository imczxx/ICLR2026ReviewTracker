{"id": "QewOtpenMy", "number": 11214, "cdate": 1758193585559, "mdate": 1759897600796, "content": {"title": "One-Token Verification for Reasoning LLMs, Anytime, Anywhere", "abstract": "Reasoning large language models (LLMs) have recently achieved breakthrough performance on complex tasks such as mathematical problem solving. A widely used strategy to further improve their performance is parallel thinking, wherein multiple reasoning traces are generated and the final prediction is chosen using methods such as Best-of-N or majority voting. However, two limitations remain: most existing methods lack effective mechanisms for assessing the quality of reasoning traces, typically relying only on final logits or answers, and multi-sample decoding incurs substantial inference latency for long outputs. To address these challenges, we propose **One-Token Verification** (OTV), a lightweight framework for assessing reasoning quality via a single one-token forward during generation. The proposed OTV method introduces a learnable LoRA-based role vector that, without interfering with the primary reasoning process, enables the LLM to assume a verification role and probe the past KV cache for correctness confidence estimation. Unlike generic verifiers or external reward models, OTV is trained natively for each LLM directly leveraging internal computations for token-level scoring. As a result, OTV can provide confidence signals at any point in generation and for any token position, realizing ``anytime, anywhere'' verification. Experiments on math benchmarks show that OTV consistently outperforms state-of-the-art baselines in parallel thinking. Moreover, based on OTV, we introduce efficient variants that terminate most traces early and retain only a single complete reasoning path, reducing the token usage by up to 90\\%. In this setting, OTV maintains superior performance while favoring shorter, more reliable solutions.", "tldr": "", "keywords": ["LLM", "parallel thinking", "LoRA", "test-time scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c12f9ec925ff3e58784b684e5254a3ed83ffb0d1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an efficient way (using an adapter, based on LoRA) to train a verifier to assess intermediate results of reasoning traces. The experimental results demonstrate the superior performance of the proposed method on math benchmarks against SOTA baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a clever way to address the computational efficiency for the intermediate assessment, which could also be easily adapted to other scenarios such as actor-critic based PPO training.\n2. The experimental results show a clever gain against the baselines, especially for the weighted average scenario\n3. The paper is well-written and clearly presented"}, "weaknesses": {"value": "1. The second contribution in the Introduction part does not hold \n> We design a heuristic supervision and parallel training scheme that provides dense step-level signals, making the proposed OTV method efficient, scalable, and adaptable to metrics beyond answer correctness\n\n[1,2] have already pointed out the value/advantage function should be the correct training target for step-wise signals, it remains unclear to me why these heuristics are developed. There lacks the discussion and comparison to these prior works.\n\n2. Main evaluation is only conducted on Qwen models and AIME benchmarks. Given the evaluation issues raised by recent works [3,4], I think the evaluation is not comprehensive.\n\n3. The BoN results do not show a consistent improvement (Table 2). \n\n[1] Feng et al. Step-by-Step Reasoning for Math Problems via Twisted Sequential Monte Carlo. ICLR 2025.\n\n[2] Setlur et al. Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning. ICLR 2025\n\n[3] Shao et al. Spurious Rewards: Rethinking Training Signals in RLVR. \n\n[4] Hochlehnert et al. A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility. COLM 2025."}, "questions": {"value": "The novelty part of this work seems not so clear. Given my list of previous works, this paper only stands out in proposing a more efficient strategy to finetune/store the verifier rather than making any algorithmic innovations. Most importantly, there lacks a clear efficiency comparison with baselines.\n\nFor example, within the same time budget, could DeepConf have more sampled trajectories?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CGFwQLEJaf", "forum": "QewOtpenMy", "replyto": "QewOtpenMy", "signatures": ["ICLR.cc/2026/Conference/Submission11214/Reviewer_KSje"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11214/Reviewer_KSje"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761252898492, "cdate": 1761252898492, "tmdate": 1762922360901, "mdate": 1762922360901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To assess the quality of reasoning traces at any point in generation and for any token position, the authors introduce OTV, a lightweight framework for assessing reasoning quality via a single one-token forward during generation. They conduct extensive experiments across multiple reasoning LLMs and math benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper shows that OTV can surpass recent PRMs across various LLMs and settings (e.g., self-consistency, BoN).\n2. The authors visualizes the confidence trajectories of different scoring methods, highlighting that OTV more faithfully captures the\nunderlying reasoning quality."}, "weaknesses": {"value": "1. The authors miss an important reference: \"Token-Supervised Value Models for Enhancing Mathematical Problem-Solving Capabilities of Large Language Model, ICLR 2025\". The missing reference is totally critical due to the fact that this work already offers anytime, anywhere verification even without any special token like `[ToT]`. Moreover, this work already shows its effectiveness for step-level signals, but the authors do not compare OTV with this work at all. Finally, this work theoretically shows that its method can allow tree search algorithms to be value-guided, but OTV is based on heuristic confidence. In other words, there is no theoretical analysis why OTV can work well. In this sense, the novelty of OTV seems negligible.\n2. In addition to Figure 1, it would be instructive if the authors share the example of confidence decreasing for inference, whether this example really results in a wrong answer, and at which token the confidence decreases dramatically.\n3. It seems necessary to show that OTV can be helpful for MCTS or Step-by-Step Beam Search to show that OTV can be also applied to advanced search strategies."}, "questions": {"value": "Can OTV be extended to other tasks (e.g., code generation, neural theorem proving)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mrXWQogHMb", "forum": "QewOtpenMy", "replyto": "QewOtpenMy", "signatures": ["ICLR.cc/2026/Conference/Submission11214/Reviewer_NZWw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11214/Reviewer_NZWw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741132137, "cdate": 1761741132137, "tmdate": 1762922360374, "mdate": 1762922360374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose One-Token Verification (OTV), a very lightweight method to assess reasoning quality during the model reasoning process. OTV trains a LoRA for self-verification as a role vector and uses special token [ToT] to measure the verification confidence. The experiment shows promising result of OTV in parallel thinking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Paper is relatively well written and easy to follow.\n2. OTV is sound and well constructed in the methodology."}, "weaknesses": {"value": "1. The concept of OTV is not super new in that it uses (an adapted) model internal confidence to measure the certainty of a verifiable result.\n2. Evaluation is not comprehensive enough. The evaluation only shows result against math reasoning, and have not shown OTV can generalize beyond the standard math benchmarks. \n3. Related works lack the various works related to model confidence measure and early-stopping in LLM reasoning."}, "questions": {"value": "1. Why choosing the DAPO17K dataset as the dataset to train against?\n2. How will this method perform against non-mathematical datasets?\n3. The experiment & Appendix D shows the various traces of problem (with correct vs incorrect) and the change of the score. Statistically, do you observe the answer flipping phenomenon to happen, meaning the retained cases actually contains the right answer (in the trajectory) but then flipped to the wrong one in later stage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gOASxnlfs5", "forum": "QewOtpenMy", "replyto": "QewOtpenMy", "signatures": ["ICLR.cc/2026/Conference/Submission11214/Reviewer_PtHd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11214/Reviewer_PtHd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928762860, "cdate": 1761928762860, "tmdate": 1762922359706, "mdate": 1762922359706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript proposes One-Token Verification (OTV), which uses a LoRA to mode-switch the LLM to switch to the “role” of a verifier. By feedforwarding a special verification token, the model can probe the base model’s KV cache and output a token level confidence score. For training signals, heuristic pseudo-labels are used for confidence trajectories. The authors also propose a method to perform parallel verification, making it possible to score all positions in one pass. On AIME datasets, OTV improves majority-voting/Best-of-N and outperforms Reward Models, and shows best or near-best performance for efficient variants of BoN."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Does not require a fully separate verifier model, saving memory and compute.\n- LoRA instead of full finetuning for the “verifier role” can mitigate catastrophic forgetting on the verifier’s side.\n- Efficient parallel verification is possible."}, "weaknesses": {"value": "- Confidence labels are synthetic and heuristic, which can drift significantly from actual process correctness. Also, as far as I know, the heuristic label scheme cannot model situations where the model succeeds in a major self-correction during its trajectory.\n- Although the manuscript briefly claims to justify the fairness of its evaluation protocol, I believe it is unfair. OTV is tuned with DAPO17K for each base backbone model, but other baseline Reward Models are off-the-shelf. Thus, OTV is tailored for each idiosyncratic backbone, while other baselines are fixed to a single backbone. Also, the baseline RMs are built on backbones that are older than Qwen 3, which is the backbone for OTV.\n- Experiments are conducted on only a single model family(Qwen 3) and single benchmark family (AIME 24, 25), which is arguably a narrow coverage."}, "questions": {"value": "- How does OTV fare for the setting where the generator and the verifier are heterogeneous (e.g. OTV is built on Qwen 3 and the generator is built on Gemma 3)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IkrMcymBva", "forum": "QewOtpenMy", "replyto": "QewOtpenMy", "signatures": ["ICLR.cc/2026/Conference/Submission11214/Reviewer_DkwK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11214/Reviewer_DkwK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975987160, "cdate": 1761975987160, "tmdate": 1762922359185, "mdate": 1762922359185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}