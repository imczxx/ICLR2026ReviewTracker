{"id": "y8jS7mDurI", "number": 18089, "cdate": 1758283715859, "mdate": 1759897134134, "content": {"title": "Proactive Interference Reveals Working Memory Limits in LLMs Beyond Context Length", "abstract": "Information retrieval in Large Language Models (LLMs) is increasingly recognized as intertwined with generation capabilities rather than mere lookup. While longer contexts are often assumed to improve retrieval, the effects of intra-context interference remain understudied. To address this, we adapt the proactive interference (PI) paradigm from cognitive science, where earlier information disrupts recall of newer updates. In humans, susceptibility to such interference is inversely linked to working memory capacity. We introduce PI-LLM, an evaluation that sequentially streams co-referenced key–value updates and queries only the final values. Although these final values are clearly positioned just before the query, LLM retrieval accuracy declines log-linearly toward zero as co-referenced interference accumulates; errors arise from retrieving previously overwritten values. Attempts to mitigate interference via prompt engineering (e.g., instructing models to ignore earlier input) yield limited success. These findings reveal a fundamental constraint on LLMs’ ability to disentangle interference and flexibly manipulate information, suggesting a working memory bottleneck beyond mere context access. \n\nThis test advances Needle-in-a-Haystack/MRCR paradigms by eliminating the haystack altogether: By isolating and varying the number of co-referenced “needles,” it directly quantifies interference, revealing a robust log-linear decline in retrieval as interference grows across SOTA models. This calls for approaches that strengthen models’ ability to suppress co-referenced information during retrieval.", "tldr": "LLMs fail to retrieve recent updates when earlier input gets in the way, revealing working-memory-like limits beyond a model's context length.", "keywords": ["model evaluation", "long-context language models", "working memory limitations", "contextual interference", "In-context learning", "proactive interference", "robustness & reliability", "top‑down control", "cognitive‑science–inspired evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/971a5d6b749d3e27cea4af4a8c7408ced8ae1643.pdf", "supplementary_material": "/attachment/0ad9f653ff7235411ca6d94f98ffb00fb9c4a9ce.zip"}, "replies": [{"content": {"summary": {"value": "LLMs are tested with PI-LLM, a setup that streams many co-referenced key–value updates and then asks for the final value. Across diverse models, accuracy falls roughly log-linearly as interference (more updates/keys/longer values) grows—even when total prompt length is held constant—implicating a working-memory-like interference limit rather than context size. Prompting or CoT doesn’t fix it; a simple “reset” hack helps only partially. The paper proposes an Interference Endurance Score (IES) and observes larger dense models resist interference better than smaller/MoE ones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Clean, controlled benchmark that isolates proactive interference (not search or sheer length) via co-referenced updates and a fixed-length control.\n* Reveals a consistent, interpretable log-linear degradation pattern across many model scales, providing a quantitative handle on interference robustness.\n* Demonstrates that current prompting and reasoning strategies are insufficient"}, "weaknesses": {"value": "The main concern is that the setting is extremely limited. The experimental design is clean, but it’s also very artificial. Because the task uses simple key–value updates, it’s hard to know whether the same interference appears in real settings.\n* You could test a long article where an entity’s attributes change over time and see if the model retrieves the most recent one.\n* A multi-turn chat or agent session, where a small profile field keeps being updated, would also make the setup feel more realistic.\n* Showing that models with stronger interference in your test also make more “old value” mistakes in these realistic cases would make the results more convincing.\n\nMost of the interventions the paper tests are just prompt variations, which doesn’t really address the underlying issue. If the problem is interference between parts of the context, there should be at least one experiment that changes how the model or system handles memory.(e.g., KV-cache resets/segmenting, retrieval-augmented state, local attention, active-experts in MoE).\n\nThe formatting of the figures could be improved (e.g. in figure 1, the 1*, 2*, 3*, 4* is hard to read)."}, "questions": {"value": "How is the fixed-length control enforced at the token level? The same number of key–value pairs doesn’t guarantee equal token length—please specify the tokenizer, any padding/truncation, and whether the final query length is held constant."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DfM5TyKLJV", "forum": "y8jS7mDurI", "replyto": "y8jS7mDurI", "signatures": ["ICLR.cc/2026/Conference/Submission18089/Reviewer_Ue62"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18089/Reviewer_Ue62"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589851127, "cdate": 1761589851127, "tmdate": 1762927864480, "mdate": 1762927864480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates into a particular factor that impacts LLM performance: limited anti-interference capacity, where the paper shows that LLM performance degrades in a log-linear way as interferences increase. \n\nIn the experiments, the authors show that this increase is disentangled with length increase (Section 3), thus makes it an independent causal factor. The tests have been carried over numerous LLMs and they show similar log-linear decrease trend. Besides, the authors have investigated into ways to mitigate the issue including various prompt changes as well using CoT. The improvement was only minimum and the authors show that the problem persists well."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Understanding the LLM limitation is surely an important topic, particularly if novel insights are brought into the community. The paper identified the anti-inference capacity as an LLM performance limitation; most importantly, the paper has shown that the inference factor is independent of context length factor to impact LLM performance. \n\nThe paper has demonstrated the results over various LLMs to show that it is a general problem for current LLMs; besides, the paper has investigated into different prompt strategy including CoT to mitigate the issue and show that the problem persists."}, "weaknesses": {"value": "I don't doubt the novelty in this work, nevertheless, I would encourage the authors to include a Related Work to show the connections that the paper has with existing research.\n\nThe results demonstrated in the paper, well kind of novel, is not surprising: the LLM performance is not conditioned on the length of the context but also the problem difficulty; for the later case, the community has identified quite early on that the LLM memory can be one of such problem difficulty (in this sense, the current paper is well related to these lines of works). While prompt strategy has been investigated, we also notice that more involved architectures (LLM workflow, or agents) are not investigated in the paper; sure that it is not directly linked with the interference factor that limits the LLM performance, but it is important to show that these problems can/cannot be solved alternatively, for the current paper, it reads like it is not solvable which I believe is not true."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G7ETQaWInz", "forum": "y8jS7mDurI", "replyto": "y8jS7mDurI", "signatures": ["ICLR.cc/2026/Conference/Submission18089/Reviewer_eEkx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18089/Reviewer_eEkx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667126716, "cdate": 1761667126716, "tmdate": 1762927863719, "mdate": 1762927863719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how information retrieval in LLMs is affected by intra-context interference, adapting the proactive interference (PI) paradigm from cognitive science. The authors propose PI-LLM, an evaluation where models must recall the latest value among sequentially updated, co-referenced key–value pairs. Results show a log-linear decline in retrieval accuracy as interference accumulates, with models often recalling outdated values. Prompt-based mitigation fails to resolve the issue, indicating a working memory–like limitation in LLMs. By removing irrelevant context (“haystack”) and directly measuring interference, PI-LLM provides a principled framework to assess and improve LLMs’ information disentanglement and memory control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear and well-written: The paper is logically structured and easy to follow, with clear motivation and experimental design.\n\n2. Valuable and insightful finding: The discovery of log-linear interference effects offers deep insights into LLM working memory limits and provides meaningful guidance for future model development."}, "weaknesses": {"value": "1. No discussion with a very related work (motivation and discovery): https://arxiv.org/abs/2502.05252.  This paper also discusses how to insert noise in the context and find something very similar to log-linear degradation.\n\n2. Do not provide executable insights on how to improve LLM training in the discussed problem."}, "questions": {"value": "In the weakness.\n\n1. Are models making advancements in the discussed problem? For example, Llama 2, Llama 3, Llama 4/Qwen3/Deepseek —how is the trend? \n\n2. Does model architecture make a difference? For example, xLSTM and Mamba are compared to transformers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GfmPYs6wAY", "forum": "y8jS7mDurI", "replyto": "y8jS7mDurI", "signatures": ["ICLR.cc/2026/Conference/Submission18089/Reviewer_zn3J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18089/Reviewer_zn3J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896374011, "cdate": 1761896374011, "tmdate": 1762927863076, "mdate": 1762927863076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LLMs are known to struggle when conflicting evidence is present in the context. This has been extensively studied in the past in relation to RAG (retrieval augmented systems). This paper proposes a task to systematically evaluate the same. The context consists of key value pairs, with same key potentially having differing values in the same context. The LLM is supposed to find the value associated with the last occurrence of a key. The work shows that increasing such conflicts leads to a consistent drop in the accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- THe experiments are thorough. The authors clearly specify the prompts they used to ensure that the model follows the task as desired. \n- The control for various confounders like length. \n- The capacity analysis is nice."}, "weaknesses": {"value": "- It is a well-documented fact in the RAG literature that intra-context conflicts cause performance degradation (see https://arxiv.org/abs/2507.21544, https://arxiv.org/abs/2504.13079v2). In fact, prior works have shown that when conflicting evidence is present in context, models tend to rely on parametric knowledge biases rather than the retrieved evidence (https://arxiv.org/abs/2402.07867, https://aclanthology.org/2022.emnlp-main.146.pdf).\n\n- Given these studies, it is unclear what the proposed benchmark adds—whether it identifies a genuinely new failure mode or deepens understanding of existing, well-known interference phenomena.\n\n- Relation to real-world scenarios: In realistic settings, conflicting evidence typically co-occurs with confounders such as the confidence of retrieved snippets or the credibility of their sources. These factors often dominate which information an LLM uses in its final response. The proposed benchmark abstracts away such factors, making it difficult to extract actionable insights for practitioners. It largely remains a toy key–value benchmark with limited ecological validity.\n\n- Suggestions: Future versions of the work could incorporate more nuanced and realistic setups—e.g., multi-hop reasoning or information extraction under conflicting evidence—to better connect with real-world retrieval challenges.\n\n- Minor: The plots can be made much more cleaner in the main paper, with only key curves being shown and rest deferred to the appendix. In general, the manuscript needs significant more efforts in better presentation."}, "questions": {"value": "See the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2SVSuyDKBG", "forum": "y8jS7mDurI", "replyto": "y8jS7mDurI", "signatures": ["ICLR.cc/2026/Conference/Submission18089/Reviewer_LZv7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18089/Reviewer_LZv7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948199138, "cdate": 1761948199138, "tmdate": 1762927862684, "mdate": 1762927862684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}