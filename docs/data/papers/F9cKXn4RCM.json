{"id": "F9cKXn4RCM", "number": 23135, "cdate": 1758340046628, "mdate": 1759896830887, "content": {"title": "Membership Inference Attack via Soft Prompt Manipulation in Federated Fine-Tuning", "abstract": "Membership inference attack (MIA) poses a significant privacy threat in federated learning (FL) as it allows adversaries to determine whether a client’s private dataset contains a specific data sample. While defenses against membership inference attacks in standard federated learning have been well studied, the recent shift toward federated fine-tuning has introduced new, largely unexplored\nattack surfaces. To highlight this vulnerability in the emerging FL paradigm, we demonstrate that federated prompt-tuning, which adapts pre-trained models with small input prefixes to improve efficiency, also exposes a new vector for privacy attacks. We propose PROMPTMIA, a membership inference attack tailored to federated prompt-tuning, in which a malicious server can insert adversarially crafted prompts and monitor their updates during collaborative training to accurately determine whether a target data point is in a client’s private dataset. We formalize this threat as a security game and empirically show that PROMPTMIA consistently attains high advantage in this game across diverse benchmark datasets. Our theoretical analysis further establishes a lower bound on the attack’s advantage, which explains and supports the consistently high advantage observed in our empirical results. We also investigate the effectiveness of standard membership inference defenses originally developed for gradient or output-based attacks and analyze their interaction with the distinct threat landscape posed by PROMPTMIA. The results highlight non-trivial challenges for current defenses and offer insights into their limitations, underscoring the need for defense strategies that are specifically tailored to prompt-tuning in federated settings.", "tldr": "", "keywords": ["federated fine tuning", "membership inference attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/61c83acfd9eeeb50eb3c8cf72331a00700661c5b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Parameter-efficient-fine-tuning (PEFT) used in federated learning exposes new security concerns. This paper proposes a new membership inference attack, PromptMIA, which leverages the soft prompt vectors prepended to input embeddings to explore the privacy risk associated with federated prompt tuning. PromptMIA focuses on a different attack setting where only soft prompt vectors are required by exploiting the prompt selection and update method. Specifically, PromptMIA designs the diverse adversarial keys that achieve high cosine similarity with the target query. PromptMIA then works by injecting the adversarial keys through a shared prompt pool. When a client's data includes the target key, the adversarial key is selected and updated, achieving the goal of the attack."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The author has made an important observation, justifying the research motivation.\n2. A rigorous theoretical performance guarantee is provided for the proposed method. The paper provides carefully derived mathematical proof of the lower bounds on true positive rate(TPR) and false positive rate(FPR) of the approach. \n3. Extensive studies with experiment on the existing defense mechanism deployed on federated learning to prevent membership inference attack. \n4. The approach is evaluated on multiple datasets and multiple models, demonstrating the generalizability across different tasks."}, "weaknesses": {"value": "1. The method is specifically designed on soft prompts fine tuning. There’s also other PEFT variants which also expose similar privacy concerns. I would appreciate a more detailed discussion on how the proposed method might be generalized and extend to other alternative PEFT methods.\n2. In section 3.2.3, assumption 2 requires that the distribution of non-member queries q(x) belonging to a benign cluster is a spherical Gaussian distribution centered at that key. I am concerned that this is a rather strong assumption to hold outside of a controlled experimental setting, especially under non-IID client data. Sensitivity experiments or studies on how the attack and the bound response to violations of these assumptions would be worth performing."}, "questions": {"value": "Could you respond to each weakness point?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OVrW3pkq9b", "forum": "F9cKXn4RCM", "replyto": "F9cKXn4RCM", "signatures": ["ICLR.cc/2026/Conference/Submission23135/Reviewer_vEpM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23135/Reviewer_vEpM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761385950695, "cdate": 1761385950695, "tmdate": 1762942526276, "mdate": 1762942526276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies membership inference in federated prompt-tuning, arguing that soft-prompt updates create a new attack surface distinct from gradient or output-based channels. It proposes PROMPTMIA, where a malicious server injects adversarial keys/prompts into the global prompt pool so that, if a target sample is present on a client, those keys are deterministically selected and updated; the server then infers membership by monitoring which prompts were updated."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Fresh idea. The work focuses on prompt-tuning — a new and realistic attack surface that most existing MIAs ignore. The attack setup (malicious server inserting keys) is simple but clever.\n\n2. Theory matches intuition. The math nicely explains why the attack works better when prompts cluster tightly. It’s not just empirical; the logic behind it makes sense.\n\n3. Solid experiments. The attack performs impressively well across several backbones and datasets, and the paper clearly shows where existing defenses fail."}, "weaknesses": {"value": "1. Assumptions feel a bit unrealistic. The attack assumes the server can freely modify the global prompt pool and observe which prompts were updated. In many real FL systems, clients verify global models and use secure aggregation. It’d be good to discuss how the attack holds up if those protections are in place.\n\n2. Defense part is quite weak. The tested defenses (noise, anomaly detection) are pretty basic. It would help to check more relevant defenses, like randomizing prompt selection, key rotation, or aggregation hiding which prompts were changed.\n\n3. No comparison with recent attacks. The paper doesn’t compare against FedMIA (Zhu et al., CVPR 2025), which is a very recent and strong MIA baseline. Even a brief comparison or discussion would make the results more convincing.\n\n4. Limited scope. All experiments are on vision transformers. It’s unclear whether this method would still work for LLM-style prompt-tuning or other PEFT methods like LoRA or adapters.\n\n5. Unclear real-world applicability. The paper shows that PROMPTMIA works in a simulated setup, but it’s not very clear how often such server-side control or data knowledge exists in real deployments."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HqVospuxzC", "forum": "F9cKXn4RCM", "replyto": "F9cKXn4RCM", "signatures": ["ICLR.cc/2026/Conference/Submission23135/Reviewer_bo1w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23135/Reviewer_bo1w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714612990, "cdate": 1761714612990, "tmdate": 1762942525969, "mdate": 1762942525969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies membership inference in the emerging setting of federated soft prompt tuning. Instead of exploiting gradients or model parameters, the server performs a targeted prompt-key injection attack, adding adversarial keys aligned to a target sample and then inferring membership by observing which prompt keys clients select and update. The method requires no shadow models or gradient access and can succeed in a single communication round. The authors analyze detection challenges (naive injection collapses, so diversity and controlled similarity are imposed), provide a lower bound on attack advantage under a Gaussian clustering assumption, and empirically demonstrate high attack success across models and datasets. Experiments also show DP-SGD and anomaly detectors are ineffective while input noise reduces utility. The work highlights a new attack channel in federated prompt learning and argues for better defenses beyond gradient-level privacy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Explores federated soft-prompt tuning and uses prompt-selection metadata instead of gradients or model parameters, representing a meaningful shift from classical MIA.\n\n2. Works in a single round with no shadow models or gradient access and achieves consistently high ASR across models and datasets.\n\n3. Provides a lower-bound theoretical analysis for attack advantage and validates it on multiple ViT backbones with systematic experiments.\n\n4. Shows DP-SGD does not mitigate this threat, traditional anomaly detectors are unreliable, and input-noise defenses impose serious utility loss."}, "weaknesses": {"value": "1. Relies on the server being able to inject prompt keys and observe prompt updates, which may not hold under secure aggregation or metadata masking in real FL systems.\n\n2. Membership leakage under large-batch updates is not fully clarified; mapping prompt updates to individual samples may weaken in this setting.\n\n3. Does not test realistic system-level defenses such as index randomization, prompt-dropout, DP applied to selection events, or private/local prompt pools.\n\n4. Experiments focus on vision prompt tuning; generalization to text or multimodal prompt-tuning setups remains unclear."}, "questions": {"value": "1. Under secure aggregation or masked updates, can the server still infer prompt indices reliably?\n\n2. How does leakage scale with large batch sizes? Can you quantify sample-level vs batch-level leakage?\n\n3. Can you evaluate system-level defenses such as randomized key indices, selection masking, prompt-dropout, or DP on prompt-selection?\n\n4. Does the attack transfer to LoRA-based FL or text prompt-tuning (e.g., P-Tuning v2)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qD9oSq7QxI", "forum": "F9cKXn4RCM", "replyto": "F9cKXn4RCM", "signatures": ["ICLR.cc/2026/Conference/Submission23135/Reviewer_YLTA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23135/Reviewer_YLTA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801090258, "cdate": 1761801090258, "tmdate": 1762942525760, "mdate": 1762942525760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}