{"id": "2zQJHLbyqM", "number": 9505, "cdate": 1758125334058, "mdate": 1763570823961, "content": {"title": "CTFusion : A CTF-based Benchmark for LLM Agent Evaluation", "abstract": "Recent advances in Large Language Models (LLMs) have enabled agentic systems for complex, multi-step tasks; cybersecurity is emerging as a prominent application.  To evaluate such agents, researchers widely adopt Capture The Flag (CTF) benchmarks.  However, current CTF benchmarks reuse existing challenges, which exposes them to data contamination and potential cheating.  Notably, we confirmed these issues in practice by integrating web search tools into an existing agent.  To address these limitations, we present CTFusion, a streaming evaluation framework built on Live CTFs.  To achieve this, CTFusion employs virtualization and aggregation to ensure that agents interact with CTF tasks independently, while minimizing impact on real competitions.  Moreover, we implement CTFusion as a Model Context Protocol (MCP) server on the widely used CTFd platform, which offers broad applicability to diverse CTF events and agent types.  Through experiments with three LLMs, two agents, and five Live CTFs, we demonstrate that existing CTF benchmarks can be unreliable in assessing LLM-based agents, while CTFusion can serve as a robust solution for evaluating cybersecurity agents.  We release CTFusion as open source to foster future research in this area.", "tldr": "We present CTFusion, a streaming evaluation framework that uses live CTF competitions to assess LLM-based cybersecurity agents.", "keywords": ["LLM", "Cybersecurity Agents", "CTF Benchmarks", "Live Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/302d5572d7fd047f441616d9b848976d579d4a46.pdf", "supplementary_material": "/attachment/f917a3d307b8afcc03aff580dbc7a8092b3dcc33.zip"}, "replies": [{"content": {"summary": {"value": "Existing Capture The Flag (CTF) benchmarks for evaluating Large Language Model (LLM) agents in cybersecurity are unreliable due to widespread data contamination and potential cheating from publicly available solutions. To address this, the paper introduces CTFUSION, a novel streaming evaluation framework built on LIVE CTFS competitions, which uses virtualization and aggregation to ensure isolated agent interaction and minimize impact on real contests. Experiments show that static benchmarks significantly inflate agent performance (e.g., 14.4% success rate) compared to CTFUSION's live evaluation (6.3%), validating CTFUSION as a robust solution for assessing genuine LLM agent capabilities in cybersecurity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The design is well-articulated: it employs a two-tier gateway with an MCP server and proxy that enable per-agent view isolation (each agent sees all challenges as unsolved until it solves them), and aggregation (ensuring each challenge is credited only once to the contest).\n2. The authors implement the full system and plan to open-source it.\n3. The paper tackles an important issue at the intersection of LLM agents and cybersecurity.\n4. A particularly compelling part of the work is the analysis of the D-CIPHER-WEB agent. By enabling a simple web-search step, they show that the agent’s success rate on the static bench nearly doubles, and they back this up with logs: “71 cheating attempts” were logged, including 63 direct flag-copy events."}, "weaknesses": {"value": "1.  Several important claims are not fully substantiated. For example, the paper states that CTFUSION has “negligible impact on real competitions”, but provides no empirical data to support this. There is no measurement of server load, latency, or effect on other teams. The assertion of “minimal configuration changes” in applying CTFUSION is plausible but not demonstrated beyond five contests. Likewise, the claim that static benchmarks are unreliable rests on the observed performance gap, but the paper does not isolate the causes. The authors themselves note two factors—task difficulty and contamination—but do not quantify their contributions. It remains possible that the live CTFs tested were simply harder or less representative than the static tasks. Without a controlled ablation (e.g. using an LLM with an earlier knowledge cutoff, or filtering static tasks with known write-ups), it is hard to attribute the gap primarily to data leakage.\n2. The paper focuses on aggregate success rates, but offers little deeper insight. For instance, there is no breakdown of results by challenge category (crypto vs pwn vs web, etc.), nor analysis of which particular tasks were solved or failed.\n3. The use of a fixed pass@3 metric and $3 cost cap is reasonable, but details are sparse. For example, how random are the results? Did the authors run multiple trials to compute confidence intervals? None are reported, so it's unclear how much variance there is.\n4. Novelty is limited.  The system design reuses well-known components (MCP, Docker, API proxies). The paper does not introduce new learning techniques or benchmarks, but rather repurposes existing CTF infrastructure in an incremental way."}, "questions": {"value": "1. Can you provide evidence that tasks in the NYU CTF BENCH actually appeared in the LLMs’ training data or public write-ups? For example, did you check which static challenges have publicly available solutions and correlate that with agent success? This would clarify whether data leakage is the main cause of the performance gap (rather than inherent difficulty).\n2. You use pass@3 with a $3 cost cap for each attempt (Sec.5.1). How sensitive are your results to these choices? Did you try different k (e.g. pass@1 vs pass@5) or cost limits to see if the relative performance changes? Also, do agents terminate upon a correct flag or always use all k attempts? Some clarity on agent stopping criteria would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zlBkgXVpAO", "forum": "2zQJHLbyqM", "replyto": "2zQJHLbyqM", "signatures": ["ICLR.cc/2026/Conference/Submission9505/Reviewer_xRZ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9505/Reviewer_xRZ7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821854230, "cdate": 1761821854230, "tmdate": 1762921078457, "mdate": 1762921078457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper highlights the data contamination and evaluation bias risks in existing CTF-based benchmarks for assessing LLM cybersecurity agents. To verify these concerns, the authors conduct controlled experiments comparing model performance on static benchmarks versus unseen challenges, confirming significant contamination effects. To address this, the paper introduces CTFusion, a new framework that evaluates models through live CTF competitions, ensuring fairness and preventing leakage from pre-existing datasets or write-ups. Experiments on five real-world live CTFs show that model performance drops by nearly half compared to static benchmarks, revealing how current evaluations substantially overestimate model capabilities. The proposed CTFusion framework establishes a reproducible, isolation-based, and dynamic evaluation pipeline, setting a more reliable foundation for future LLM agent research in cybersecurity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong motivation: Identifies a previously overlooked issue—benchmark contamination and fairness in LLM CTF evaluation;\n2.  Robust engineering design: Virtualization, proxy aggregation, and live integration are elegantly executed;\n3. Strong empirical findings: Demonstrates clear performance overestimation in static benchmarks (e.g., GPT-4.1 drops from 19.4% → 9.7%);\n4. Practical impact: Encourages the community to move towards live and transparent benchmarking;\n5. Reproducibility: Open-sourced framework with standardized interfaces to CTFd and MCP APIs;"}, "weaknesses": {"value": "1. One major consequence of data contamination is the loss of fairness in evaluation. Could the authors report the correlation between model rankings on your benchmark (CTFusion) and those on prior CTF benchmarks? Such an analysis would clarify whether the contamination issue also affects relative ranking stability across benchmarks;\n2. The evaluation exclusively relies on three closed-source commercial models without including open-source alternatives such as Llama, Qwen, or DeepSeek. This omission limits the generalizability of findings and undermines the data contamination hypothesis, as open-source models with transparent training data could serve as crucial controls;\n3. The paper lacks detailed error analysis and failure case studies that would provide actionable insights for future research. While the authors report aggregate success rates across models and agents, they do not systematically analyze why agents fail on specific challenges, what types of errors are most common, or which vulnerability categories pose the greatest difficulties;"}, "questions": {"value": "See the Weaknesses part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fIuhQeQfHY", "forum": "2zQJHLbyqM", "replyto": "2zQJHLbyqM", "signatures": ["ICLR.cc/2026/Conference/Submission9505/Reviewer_NbCu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9505/Reviewer_NbCu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960742138, "cdate": 1761960742138, "tmdate": 1762921077361, "mdate": 1762921077361, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents\n\n1. A study on potential contamination/cheating in commonly used CTF benchmarks\n2. A new framework that allows to make easy to run on currently ongoing challenges on CTFd, the biggest site of CTF competitons\n\nFor 1, the authors modify D-ciper a CTF agent by forcing it to start every agent run with a google search. They then analyze how often the agent \"cheats\", i.e., retrieves the solution instead of solving the challenge themselves.\n\nFor 2, the authors create a new framework that allows to conveniently evaluate multiple agents on ongoing CTFd challenges without being disruptive to the CTFd leaderboards."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Tapping into live challenges to evaluate LMs on CTF skills without any possible contamination or cheat issues is a relevant contribution. Based on the details in the paper, the implementation of this novel evaluation framework seems to be well-engineered and thought-through in particular giving thought to avoid disrupting the CTFd leaderboards disproportionately.\n\nThe studies on cheating are an interesting and novel way to essentially determine upper limits of possible cheating. \n\nThe paper is overall well written, though some section might benefit from focusing on the big picture over implementation details."}, "weaknesses": {"value": "**Studies on models cheating**\n\nThe study essentially seems to be giving an \"upper limit\" to cheating , as agents are specifically told to \"google for anything that might be related\" and generally almost incentivizing the model to \"cheat\". I think this is an interesting idea, but it doesn't necessarily quantify how much cheating occurs in current benchmark evaluations. For example, benchmark evaluators might specifically ask models not to cheat by asking them to not read any writeups and only search for technical details (or not to use a search engine at all), thereby at the very least significantly reducing this form of cheating.\nI assume that D-cipher technically has the ability to search for things by using `wget` or similar, but just from the score difference with D-cipher-web, it definitely doesn't seem to look up solutions often.\n\nI also wonder: If you can separate a cheating and non-cheating component on the D-cipher web run, couldn't you do the same for the default D-cipher and show how much of its performance is due to cheating in Fig 2.?\n\n**Using CTFd for benchmarking**\n\nThe main weakness is taking this new datasource and obtaining interesting results about model behavior, ranking, etc. In other words: What can I learn from this new benchmark (or based on this new datasource)?\nClearly, existing benchmarks are somewhat contaminated by writeups and so it is not too surprising that scores differ (but scores are also vastly different between different CTF benchmarks anyway). Moreover, as the paper remarks itself, there are also other conflating factors other than just contamination/cheating. One possible direction the paper could explore is whether the model ranking based on LiveCTF is different than on other benchmarks, for example. One of the possible issues with the LiveCTF might be that there are only relatively few task instances available at any point, so thought would have to be given to statistical treatment.\nIf on the other hand, old task instances from past challenges are kept on being used, then the benchmark cannot add newly released models while staying completely contamination free, facing the same issues as other benchmarks.\nFocusing the paper on these questions and on concrete takeaway messages would significantly strengthen the paper.\n\n**Readability**\n\n* Fig 4 has numbers for the arrows, but they don't seem to be references by the text. \n* I found Fig 5 hard to read. At normal magnification, the legend for the color coding is very small and the hatching color patches barely visible. It would also be good to maybe show `n=` (number of instances in each of these datasets) in the title of each subplot. That would help to get a feeling of what uncertainties might be. One idea would be to use horizontal bar charts that way you can have the model/agent names spelled out on the y axis, much easier to read\n* Fig 6 left I found pretty confusing at first, the only thing I immediately understood is that live ctfs are harder than nyu ctf. Wouldn't it be much easier to present this in another bar chart?\n* Line 234: It took me quite some to understand statements like \"two-tier gateway architecture (enforces) isolated view for each agent\" means (this is mentioned again also in 273). Reducing jargon and implementation details and focusing on the bigger picture might make 4.1 + 4.2 an easier read, for example by starting with the reason why you only want to have one CTFd account and then explaining the basic principles of the implementation in simple language."}, "questions": {"value": "* D-Cipher vs D-cypher web. I assume D-cipher technically already had that capability by using wget etc.? So the main difference with D-cipher web is  that you forced it to start with a search?\n* Regarding the evaluation setup/implementation: Do certain challenges require a more complicated setup? E.g., having a specific docker container etc.? How is that handled automatically in this setup (since you download the problem using mcp it sounds like the docker container is always the same?)\n* Line 416: How is difficulty determined? I.e., how do you know that the performance gap is partially due to difference in difficulty (or is that a hypothesis?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YtSzxWiJeP", "forum": "2zQJHLbyqM", "replyto": "2zQJHLbyqM", "signatures": ["ICLR.cc/2026/Conference/Submission9505/Reviewer_bsbd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9505/Reviewer_bsbd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762222652807, "cdate": 1762222652807, "tmdate": 1762921077057, "mdate": 1762921077057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses critical flaws in current methods for evaluating Large Language Model (LLM) agents in cybersecurity tasks. It argues that existing benchmarks, which often reuse old Capture The Flag (CTF) challenges, are unreliable due to data contamination (where solutions are in the LLMs' training data) and potential cheating (where agents use web search to find public solutions).\n\nTo solve this, the authors introduce CTFUSION, a new streaming evaluation framework that uses LIVE CTFS—ongoing competitions with fresh, unreleased challenges. This approach prevents agents from finding existing solutions online. CTFUSION uses virtualization and aggregation to allow multiple agents to compete independently in live events without disrupting the real competition.\n\nExperiments showed that agents performed significantly better on a static benchmark (14.4% success) than on LIVE CTFS (6.3%), suggesting the static results are inflated. A custom agent with web search achieved a 24.07% success rate on the static benchmark, confirming that cheating is a significant issue. The paper concludes that CTFUSION is a more robust solution for evaluating cybersecurity agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Demonstrating Vulnerabilities: It provides evidence that existing static CTF benchmarks are vulnerable to both data contamination and potential cheating.\n\nProposing CTFUSION: It proposes and implements CTFUSION, a novel, real-time streaming benchmark system that evaluates agents using LIVE CTFS to ensure challenges are new.\n\nProviding Robust Evaluation: Through experiments, it shows that CTFUSION serves as a more robust and reliable solution for evaluating cybersecurity agents compared to static benchmarks.\n\nOpen-Sourcing: The authors are releasing the CTFUSION framework as open source to encourage future research in this area."}, "weaknesses": {"value": "The paper's primary evidence is the performance gap between the static NYU CTF BENCH (14.4% success) and LIVE CTFS (6.3% success). The authors attribute this gap to two factors: (1) data contamination on the static benchmark and (2) task difficulty. The authors suspect contamination is the primary driver, but they provide no direct evidence to disentangle it from task difficulty. The conclusion that static benchmarks \"inflate success rates\" is an interpretation, not a proven fact. It is equally plausible that the five LIVE CTFs were simply harder or contained different types of problems than the 2017-2023 challenges aggregated in the static benchmark.\n\nThe paper introduces \"D-CIPHER-WEB\" to demonstrate \"potential cheating.\" This experiment effectively proves that an agent with web access can find solutions that are publicly available. However, the paper creates a contradiction.\n\nThe paper evaluates agents on five LIVE CTFs. The resulting success rates are extremely low (averaging 5.11% to 7.10%). While the total number of problems across five CTFs (16+31+55+37+ScriptCTF) seems large, the number of successful solves—the key data point—is dangerously small. A 6.3% average success rate across ~170 problems means the entire conclusion about live performance is based on approximately 10-11 solved challenges per agent configuration. A difference of just one or two solved problems could dramatically swing the percentages (e.g., from 6.3% to 7.5%). The results are therefore highly sensitive to a small number of successes, making broad conclusions about the \"true\" capabilities of agents unreliable."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nVUHvdFgjh", "forum": "2zQJHLbyqM", "replyto": "2zQJHLbyqM", "signatures": ["ICLR.cc/2026/Conference/Submission9505/Reviewer_N8G9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9505/Reviewer_N8G9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9505/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762340356139, "cdate": 1762340356139, "tmdate": 1762921076766, "mdate": 1762921076766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}