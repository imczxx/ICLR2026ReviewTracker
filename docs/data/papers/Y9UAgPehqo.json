{"id": "Y9UAgPehqo", "number": 7396, "cdate": 1758019776847, "mdate": 1759897855390, "content": {"title": "AnyUp: Universal Feature Upsampling", "abstract": "We introduce AnyUp, a method for feature upsampling that can be applied to any vision feature at any resolution, without encoder-specific training. Existing learning-based upsamplers for features like DINO or CLIP need to be re-trained for every feature extractor and thus do not generalize to different feature types at inference time. In this work, we propose an *inference-time* feature-agnostic upsampling architecture to alleviate this limitation and improve upsampling quality. In our experiments, AnyUp sets a new state of the art for upsampled features, generalizes to different feature types, and  preserves feature semantics while being efficient and easy to apply to a wide range of downstream tasks.", "tldr": "A universal feature upsampling model that can be used to upsample any feature from any to any resolution and generalizes to features unseen during training.", "keywords": ["feature upsampling", "representation learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/17200e62d0f89cab2cf90694713032066a986141.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a feature upsampling method applicable to any visual feature at any resolution without encoder-specific training. The Feature-Agnostic Layer achieves applicability to feature dimensions. The results show that it achieves state-of-the-art performance on multiple datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. This method makes a great contribution to the community. It solves the problem that previous methods require re-training for specific encoders and achieves more universal upsampling.\n2. The article is well-written and easy to understand.\n3. Feature-Agnostic Layer is a novel, concise and effective module.\n4. It can be seen from the experimental results that AnyUp has achieved SOTA performance."}, "weaknesses": {"value": "Some details of the methods were not explained clearly. Please refer to the section Questions."}, "questions": {"value": "1. This LOCAL WINDOW ATTENTION cannot capture long-range dependencies like GLOBAL ATTENTION can. Will this lead to performance degradation?\n2. Are these artifacts introduced by the GLOBAL ATTENTION itself? Please analyze the reasons.\n3. What is the significance of sampling the image and then supervising the local part? Is it merely for being more lightweight and efficient?\n4. Table 7 shows that the model with data sampling has better performance. Logically, the unsampled supervision information should be more comprehensive and should yield better results. Could you explain why data sampling has better performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WyqBJ0biVP", "forum": "Y9UAgPehqo", "replyto": "Y9UAgPehqo", "signatures": ["ICLR.cc/2026/Conference/Submission7396/Reviewer_spdT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7396/Reviewer_spdT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713587957, "cdate": 1761713587957, "tmdate": 1762919519989, "mdate": 1762919519989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AnyUp, a universal upsampling framework that can upsample features from any encoder to any resolution. This paper contributes i) a feature-agnostic convolutional layer independent of feature dimensions, ii) a local window attention mechanism to avoid irrelevant matches introduced by global attention in space, improving the quality and efficiency of upsampling, and iii) a lightweight training strategy based on local image cropping to get rid of generating high-resolution features as supervision. Experimental results show that AnyUp achieves state-of-the-art performance on several segmentation, surface normal estimation and depth estimation benchmarks. Crucially, AnyUp can generalize effectively to vision encoders unseen during training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  Clear Motivation and Significant Value: The \"encoder-specific\" problem is a real pain point when applying recent model-agnostic upsamplers. A \"train once, use anywhere\" upsampler is of high practical value for the dense prediction community.\n2.  Comprehensive Experimental Validation: The paper conducts extensive comparisons against strong baselines on multiple benchmarks (COCO, PASCAL-VOC, ADE20k, NYUv2), consistently achieving SOTA or highly competitive performance. Generalization experiments also demonstrate the model’s transferable ability on different feature extractors.\n3.  Insightful Analysis of Feature Space Preservation: The feature space preservation experiment in Table 5 is an interesting supplementary analysis. Beyond performance gains, it reveals why AnyUp succeeds—by maximally preserving the semantic distribution of the original features during upsampling, which offers more insights than simple metrics."}, "weaknesses": {"value": "1.  Insufficient Analysis of the Feature-Agnostic Layer: How sensitive is the model to the hyperparameter M (the number of basis filters)? What kind of visual patterns do the learned basis filters (ψj) actually capture? How much performance improvement can feature-agnostic layers itself bring? What is the performance difference between the proposed feature-agnostic layer and the standard convolution? Experiments on these factors are expected for a comprehensive understanding.\n2.  Parameters and inference time should be analyzed. As an inherited version from JAFAR, the number of parameters of AnyUp and the inference time should be discussed, just like JAFAR did.\n3.  A key design choice is that the Value vector (V) in the attention mechanism directly uses \"unprocessed input features\" (line 197), justified as a way to preserve the feature space. This assertion lacks direct experimental evidence (e.g. What impact will inserting a convolution layer in the value-branch could have on performance?). \n4.  Ambiguous terms used in the motivation table: what is the meaning of “any task”? If I interpret correctly, most upsamplers (e.g., Dysample) can be used for different tasks. It is just a matter of improved or decreased performance. It seems that the paper would like to say whether an upsampler should be retrained on a new task. This is not what ‘any task’ means.\n5.\tTo strengthen the comparison of the encoder-agnostic property of AnyUp in Table 6, it is better to include results from other upsampling models when they operate on features from different encoders. Specifically, these models could be generalized to different encoders in a training-free manner (e.g., via channel-wise linear interpolation) for a supplementary comparison.\n6.\tSome typographical and formatting errors: line-090, “it is was not trained on”, line-242, “the query point (Ramachandran et al., 2019)., As a high” (This point is not considered for rating)"}, "questions": {"value": "As the paper's most central technical novelty, the Feature-Agnostic Layer (Section 4.1) is clearly explained in principle, but lacks a deep experimental analysis of its internal mechanism."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VeOq0V7Sz7", "forum": "Y9UAgPehqo", "replyto": "Y9UAgPehqo", "signatures": ["ICLR.cc/2026/Conference/Submission7396/Reviewer_9SrJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7396/Reviewer_9SrJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812910828, "cdate": 1761812910828, "tmdate": 1762919519010, "mdate": 1762919519010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces AnyUp, an algorithm for upsampling deep vision network representations while retaining their semantics. Most importantly, AnyUp expands beyond other works in the literature by introducing a layer that once-trained will work on any deep network's features. They achieve this with the introduction of a new windowed guided upsampling layer that operates on features of any dimensionality. They justify their work with a suite of experiments that test the ability of features to be used in downstream dense prediction tasks, the ability to be used as a drop-in replacement for existing tasks, and the ability to generalize across networks and resolutions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Simple and elegant idea that is of interest to the community\n- Good comparisons to prior work and nice panoply of experiments\n- Nice ablations and thorough investigations of transfer to other backbones and resultions\n- Good writing and presentation"}, "weaknesses": {"value": "- Nit: line 53: some prior methods like FeatUp dont require evaluating the backbone on higher resolution images but rather small pixel jitters\n- Your explanation of the feature agnostic upsampler is a little hard to understand from the small paragraph provided, consider expanding this to provide a bit more intuition behind the math here. Also see questions section for another possible way to provide intuition behind this core contributionm\n- FeatUp's implicit version, which performs better than its JBU version does not seem to be compared against in this work, this method is also allows for any resolution inference, though at a time cost to perform the implicit optimization\n- Consider adding flop / peak mem comparisons\n- Some prior works evaluate in the joint training context to demonstrate that upsamplers can be used as a replacement for resize convs in unets. One might consider that experimental setting here as well."}, "questions": {"value": "- How does AnyUp handle tokens that are non-spatial like those in DinoV2 before the paper Vision Transformers need registers\n- During training AnyUp uses crops of the original image to help it learn to reconstruct the intricacies of high resolution features, to what extent does this reliance on image parts change its semantics (i.e the features for an object can change when that object is small or large relative to the field of view)\n- What does AnyUp learn that enables such promising transfer? What do these filters learn that enable them to upsample? Perhaps adding a section in the appendix might help the curious get a sense for what the parameters you add learn."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bTKMiyGcLP", "forum": "Y9UAgPehqo", "replyto": "Y9UAgPehqo", "signatures": ["ICLR.cc/2026/Conference/Submission7396/Reviewer_MPXT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7396/Reviewer_MPXT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867162848, "cdate": 1761867162848, "tmdate": 1762919518515, "mdate": 1762919518515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Previous feature upsampling methods cannot generalize among different feature encoders, and need to be re-trained for different encoders. This paper proposes AnyUp, which addresses the limitation of existing upsamplers, which is feature-agnostic at inference time. Experiments validate the effectiveness of AnyUp."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The feature-agnostic inference is meaningful for feature upsampling\n- The paper is well-written, and the method is simple and easy to follow\n- The experiments are comprehensive demostrating the effectiveness. The visualizations are sufficient to explain the design and show the good visual appreance of the generated feature."}, "weaknesses": {"value": "- The techniques used in the method is not very new, for example, window attention-based upsampling has already been used in SAPA."}, "questions": {"value": "- Have you tried training AnyUp on a mixture of features from different encoders (e.g., DINOv2, CLIP, MAE) simultaneously? Would this \"universal\" training further improve its generalization to completely unseen encoders compared to training on just one?\n- You motivate window attention by noting that global attention can pull in \"vastly unrelated and distant\" features. Can you quantify the performance gain and efficiency (FLOPs/latency) improvement from using windowed attention versus the global attention used in the JAFAR baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kgsKbABSUg", "forum": "Y9UAgPehqo", "replyto": "Y9UAgPehqo", "signatures": ["ICLR.cc/2026/Conference/Submission7396/Reviewer_kTsf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7396/Reviewer_kTsf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973633183, "cdate": 1761973633183, "tmdate": 1762919517950, "mdate": 1762919517950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}