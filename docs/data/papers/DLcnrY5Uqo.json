{"id": "DLcnrY5Uqo", "number": 7369, "cdate": 1758018094002, "mdate": 1759897857127, "content": {"title": "PointRePar : SpatioTemporal Point Relation Parsing for Robust Category-Unified 3D Tracking", "abstract": "3D single object tracking (SOT) remains a highly challenging task due to the inherent crux in learning representations from point clouds to effectively capture both spatial shape features and temporal motion features. Most existing methods employ a category-specific optimization paradigm, training the tracking model individually for each object category to enhance tracking performance, albeit at the expense of generalizability across different categories. In this work, we propose a robust category-unified 3D SOT model, referred to as SpatioTemporal Point Relation Parsing model (*PointRePar*), which is capable of joint training across multiple categories while excelling in unified feature learning for both spatial shapes and temporal motions. Specifically, the proposed *PointRePar* captures and parses the latent point relations across both spatial and temporal domains to learn superior shape and motion characteristics for robust tracking. On the one hand, it models the multi-scale spatial point relations using a Mamba-based U-Net architecture with adaptive point-wise feature refinement. On the other hand, it captures both the point-level and box-level temporal relations to exploit the latent motion features. Extensive experiments across three benchmarks demonstrate that our *PointRePar* not only outperforms the existing category-unified 3D SOT methods significantly, but also compares favorably against the state-of-the-art category-specific methods. Codes will be released.", "tldr": "", "keywords": ["3D single object tracking", "category-unified", "point relation parsing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/658d178e45b30e61f282ffa67fe8e8bb55a1f5b0.pdf", "supplementary_material": "/attachment/24e6be1748f79fdfbc798bc5b15306c0cac6648f.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Point Relational Parsing (PointRePar) model, supporting multi-category joint training. Robust tracking is achieved through two core designs: First, the U-shaped Spatial Relational Parsing Module (USRPM) based on Mamba, combined with Dynamic Feature Aggregation (DFA), captures multi-scale spatial point relationships to enhance foreground - background feature separation. Second, it employs dual-level temporal relationship modeling at both point-level (Temporal Scan Mamba) and bounding-box-level (Long-term Motion Trajectory Rectification, LMTR), while introducing Conditional Gaussian Perturbation (CGP) to simulate prediction errors related to scene sparsity. Experiments validated on KITTI, NuScenes, and WOD benchmark datasets demonstrate that PointRePar not only significantly outperforms existing category-unified methods but also rivals current state-of-the-art category-specific approaches. This provides a balanced solution for 3D SOT that optimizes “generalizability, performance, and training efficiency”."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Existing methods predominantly employ “category-specific training,” requiring separate models for each category, resulting in poor generalization and low training efficiency. Conversely, few unified category methods (such as CUTrack) suffer from insufficient feature discriminative power and weak motion modeling, leading to performance significantly lagging behind category-specific approaches. PointRePar achieves “unified category training + category-specific SOTA performance” for the first time, resolving the core challenge of balancing “cross-category generalization” with “high-performance tracking.”\n\n2. Spatial modeling breaks traditional frameworks: Combining Mamba (high-efficiency long sequence modeling) with a U-shaped structure to design USRPM, paired with DFA for dynamically adjusting point feature receptive fields — — simultaneously resolving the low feature separability issue in PointNet++ and AdaFormer (validated by Figure 2 t-SNE plot) while outperforming Transformer-based methods in efficiency (36.6 FPS inference speed). This achieves a balance between “multi-scale spatial relationship analysis + low computational overhead.”\n\n3. Temporal Modeling Addresses Fine-Grained & Long-Range Requirements: Point-level modeling captures pixel-level motion features between frames, while bounding-box-level LMTR refines trajectories using historical bounding-box sequences. This dual-level design balances “fine-grained motion details” with “long-range trajectory consistency”; CGP dynamically adjusts noise based on scene sparsity, better matching real-world error patterns than traditional uniform noise (Figure 4 validates sparse scene error patterns), significantly enhancing robustness.\n\n4. Strong Module Synergy: The workflow design—“coarse prediction (lightweight tracker) → spatial feature encoding (USRPM) → temporal relationship analysis (TSM+LMTR) → fine decoding (cross-attention)”—forms a complete “spatio-temporal fusion → error robustness → precise localization” chain, with complementary yet non-redundant module functions.\n\n5. Comprehensive Benchmarking: Performance surpasses category-unified methods (MoCUT, TrackAny3D) by over 15%, while matching category-specific SOTA (SiamMo, MBPTrack), challenging the assumption that category-unified approaches inherently underperform category-specific ones.\n\n6. Thorough ablation studies: Validated the effectiveness of four core modules—DFA, USRPM, LMTR, and CGP (Table 3)—analyzed the impact of hyperparameters (DFA offset points, LMTR sequence length) on performance (Tables 4 and 5), and even supplemented efficiency comparisons between Mamba and self-attention (Table 13), yielding highly credible results."}, "weaknesses": {"value": "1. Insufficient analysis of extreme scenarios: While the paper mentions strong performance in sparse scenarios, it lacks dedicated analysis for extreme cases such as “heavy occlusion (target point cloud < 10 points)” or “rapid viewpoint changes (e.g., sharp vehicle turns).” It also fails to compare behavioral differences with SiamMo in such scenarios, making it difficult to fully support the conclusion that “robustness surpasses category-specific methods.”\n\n2. Lack of core module mechanism visualization: Designs like USRPM's Mamba bidirectional scanning and DFA's dynamic receptive field adjustment are validated solely through performance improvements. Without supplementary feature visualizations (e.g., attention weight maps for different-scale features) or error heatmaps, it remains difficult to intuitively demonstrate “how spatio-temporal relationship analysis enhances tracking accuracy.”"}, "questions": {"value": "1. Regarding Mamba's advantages over USRPM: Compared to Transformers or RNNs, is there quantitative evidence of Mamba's “long sequence modeling advantage” on unstructured point cloud data? For example, under identical FLOPs, how does Mamba's feature separation (e.g., foreground-background cosine distance) compare to Transformers?\n\n2. Regarding CGP's Generalizability: CGP is designed based on the “scene sparsity - error magnitude” correlation. If scene sparsity changes drastically over time (e.g., when an object is momentarily occluded by a building), can CGP's dynamic noise adjustment still function stably? Can qualitative tracking results for such dynamic scenes be provided?\n\n3. To enhance the literature context and highlight research relevance, it is suggested to add references to related work:\n[1] Instance-guided point cloud single object tracking with inception transformer, IEEE TIM 2023;\n[2] Revisiting Siamese-Based 3D Single Object Tracking With a Versatile Transformer, IEEE TPAMI 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No Ethics Concerns"}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nwe9kpXKc9", "forum": "DLcnrY5Uqo", "replyto": "DLcnrY5Uqo", "signatures": ["ICLR.cc/2026/Conference/Submission7369/Reviewer_pyGc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7369/Reviewer_pyGc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760669674620, "cdate": 1760669674620, "tmdate": 1762919500109, "mdate": 1762919500109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PointRePar, a category-unified 3D single object tracking (SOT) method that leverages spatiotemporal point relation parsing to achieve robust tracking across multiple object categories. The authors propose a Mamba-based U-Net architecture for multi-scale spatial relation modeling, a dynamic feature aggregation (DFA) mechanism, a long-term temporal relation parsing module, and a conditional Gaussian perturbation (CGP) scheme to enhance robustness. Extensive experiments on KITTI, NuScenes, and Waymo Open Dataset demonstrate that PointRePar outperforms existing category-unified methods and is competitive with state-of-the-art category-specific trackers."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a unified tracking framework that effectively combines spatial and temporal relation parsing using Mamba-based architectures, which is innovative.\n\n2. The paper focuses on the unified category 3D SOT tracking, which has solid contributions. Most 3D trackers are category-dependent, which is unnecessary and introduces significant redundancy into 3D SOT trackers.\n\n3. The results are comprehensive and good."}, "weaknesses": {"value": "1. The overall method is too complex, which includes a U-Net for feature fusion, motion-modeling, and dynamic feature aggregation.  \n\n2. The experimental results should clearly reveal the effects of each module and the hyperparameters design for the proposed module. \n\n3. The speed comparison and ablation studies should be clearly presented."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4wzK1Rqfsl", "forum": "DLcnrY5Uqo", "replyto": "DLcnrY5Uqo", "signatures": ["ICLR.cc/2026/Conference/Submission7369/Reviewer_ncKM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7369/Reviewer_ncKM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707151560, "cdate": 1761707151560, "tmdate": 1762919499564, "mdate": 1762919499564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles LiDAR‐based 3D Single Object Tracking (SOT) in a category-unified setting. It proposes PointRePar, a coarse-to-fine tracker that parses spatial and temporal point relations to improve robustness across categories. Experiments on KITTI, nuScenes, and WOD show large gains over prior category-unified trackers and competitive results vs category-specified SOTA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation for unified tracking and robust spatio-temporal relation parsing; the goal and assumptions are explicitly stated\n\n2. Well-structured architecture: a coherent coarse-to-fine pipeline with motion-based coarse prediction, spatial relation parsing (USRPM+DFA), temporal relation parsing (TSM+LMTR), and a cross-attention decoder.\n\n3. SoTA performance."}, "weaknesses": {"value": "1. Limited novelty, the paper’s novelty is mainly compositional/system-level, not a fundamentally new primitive."}, "questions": {"value": "1. Will CGP works for other methods?\n\n2. Could you clarify how each ablation in Table 3 is instantiated? For example, in the ‘w/o USRPM’ setting, is USRPM simply removed, or replaced with a capacity-matched alternative (e.g., a PointMamba-style block)?”\n\n3. How big is the gap between the coarse-stage predictions and the refined predictions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aYInyRwpgP", "forum": "DLcnrY5Uqo", "replyto": "DLcnrY5Uqo", "signatures": ["ICLR.cc/2026/Conference/Submission7369/Reviewer_jhyB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7369/Reviewer_jhyB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807344533, "cdate": 1761807344533, "tmdate": 1762919499162, "mdate": 1762919499162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PointRePar tackles the task of 3D single object tracking, which involves tracking a given 3D bounding box query throughout a sequence of sparse point clouds, ultimately predicting the center and yaw of the bounding box at each frame. Since this task relies solely on point representations, it requires robust shape modeling and effective utilization of temporal features.\nTo address these challenges, the paper introduces a Mamba-based spatial and temporal encoder. Moreover, because the inference process is autoregressive, the model must be resilient to errors propagated from previous predictions. To enhance robustness, the authors propose a dynamic Gaussian perturbation strategy during training, where the perturbation scale is adaptively adjusted according to the local point sparsity.\nFinally, the paper presents a unified model applicable across multiple object categories, demonstrating strong performance on benchmark datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong performance on the benchmark.\n\n2. An interesting result in the supplementary shows that the model achieves better efficiency and performance when Mamba is used instead of Attention for point cloud processing."}, "weaknesses": {"value": "1. Although category-unified and category-specific paradigms are listed separately in all tables, I believe a fully unified model could still be trained in the same way as the category-specific one. To ensure a fair comparison, PointRePar should be trained separately for each category and compared directly with the state-of-the-art category-specific models. I am not fully convinced that these paradigms are inherently different to the extent that they cannot be fairly compared in the same table.\n\n2. Although Conditional Gaussian Perturbation is only applied during training, Figure 3 may mislead readers into believing that it is also used during inference."}, "questions": {"value": "I am not very familiar with the field of 3D single object tracking, so my evaluation may not be fully conclusive. The paper is overall well-structured and clearly written. However, I am not entirely convinced that the proposed model is truly novel or provides substantial insights to the community.\n\nThe use of Mamba for point cloud encoding has already been demonstrated in prior work, such as Zhang et al., “Point Cloud Mamba: Point Cloud Learning via State Space Model,” 2024, as well as in several subsequent studies. While applying Mamba to motion trajectory rectification is an interesting direction, its significance seems limited given that Mamba is already well-known for its strong performance in sequential processing.\n\nOverall, my impression is that the introduction of Mamba forms a major part of this paper’s contribution, but the work does not clearly highlight why or how Mamba is particularly effective for this specific task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "A6FGGb68FK", "forum": "DLcnrY5Uqo", "replyto": "DLcnrY5Uqo", "signatures": ["ICLR.cc/2026/Conference/Submission7369/Reviewer_h3CE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7369/Reviewer_h3CE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762158388611, "cdate": 1762158388611, "tmdate": 1762919498788, "mdate": 1762919498788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}