{"id": "educGk5ykl", "number": 25102, "cdate": 1758364164950, "mdate": 1759896734103, "content": {"title": "Flow-Based Alignment of Uni-Modal Vision and Text Encoders for Few-Shot Image Classification", "abstract": "Few-shot classification with vision–language models remains challenging, particularly when relying on multi-modal encoders such as CLIP that are restricted to paired image–text data. We introduce FSF, a framework that leverages arbitrary uni-modal encoders—including vision or text models that were pretrained on broad or domain-specific corpora—and aligns them for cross-modal classification. FSF first applies a closed-form orthogonal Procrustes map to align image and text embeddings while preserving their geometry, and then trains a lightweight flow-matching prior that regularizes adaptation in the few-shot regime. At inference, images are classified by cosine similarity in the aligned feature space between query embeddings and mapped class prototypes. Experiments on standard benchmarks, ImageNet variants, and VinDr-CXR, a large-scale chest X-ray benchmark, show that FSF is able to leverage stronger or specialized encoders, achieving competitive or superior accuracy compared to recent adaptation methods.", "tldr": "Few-shot classification framework that aligns uni-modal image and text encoders with orthogonal Procrustes and flow matching, leveraging large-scale or domain-specialized models for adaptation.", "keywords": ["few-shot classification", "vision-language models", "CLIP adaptation", "alignment of uni-modal encoders", "flow matching"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/004838e193489ac39f4a8030bb47298b056ecc04.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes **FSF** (Few-shot-Flow) which is a framework that aligns arbitrary uni-modal vision and text encoders for cross-modal **FSC**, enabling the use of stronger or domain-specific models without join pretraining. **FSF** combines two components: **Orthogonal Procrustes (OP) Alignment** and **Flow-Matching Prior**. Unlike CLIP-dependent adapters, **FSF** bridges independent encoders without large paired datasets or heavy training. The proposed method demonstrates consistent improvements across 11 benchmarks, demonstrating robustness to distribution shifts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The proposed method achieved consistent performance via a novel combination of Orthogonal Procrustes (OP) alignment, a closed-form linear mapping that preserves geometric structure, and a lightweight flow-matching prior that models non-linear cross-modal transport in latent space. The study addresses a gap in adapting uni-modal models for few-shot classification without large paired datasets.\n\n2) The study provides a flexible alignment of uni-modal encoders, which can be extend to other multi-modal tasks, particularly in resource-constrained or specialized domains.\n\n3) **FSF** consistently achieves competitive accuracy compared to SOT few-shot classification methods on standard benchmarks even when using the standard multi-modal CLIP backbone."}, "weaknesses": {"value": "1) The paper claims the flow \"lightweight\", but lacking report of training time, inference latency compared to baselines.\n\n2) The paper is lacking a fair comparison with other non-linear mapping between feature spaces such as Optimal Transport (OT) and Gromov-Wasserstein (GW) methods."}, "questions": {"value": "1) Could you provide a report of training and inference time compared to baselines.\n\n2) Could you conduct experiments for comparing the proposed method with other non-linear mapping such as Optimal Transport or Gromov-Wasserstein?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dsBkp1tMIf", "forum": "educGk5ykl", "replyto": "educGk5ykl", "signatures": ["ICLR.cc/2026/Conference/Submission25102/Reviewer_Ws4H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25102/Reviewer_Ws4H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653980808, "cdate": 1761653980808, "tmdate": 1762943325943, "mdate": 1762943325943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FSF, a novel framework for few-shot image classification that enables flexible alignment of uni-modal (independently trained) vision and language encoders. The method first applies a closed-form Orthogonal Procrustes (OP) linear map to establish an initial feature space alignment. It then learns a lightweight flow-matching prior to model non-linear transformations between image features and text prototypes. Extensive experiments show that FSF outperforms baselines on standard few-shot benchmarks, excels under distribution shifts, and adapts effectively to domain-specific tasks like medical imaging, all while maintaining efficiency."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper is well-written and easy to follow.\n- The proposed orthogonal procrustes and lightweight flow-matching are novel and effective.\n- The experimental results demonstrate the effectiveness of the proposed methods."}, "weaknesses": {"value": "- Missing comparison with recent methods. The authors only compare FSF with early baselines and do not make a comparison with recent methods such as GDA-CLIP[1], TIMO[2]. Could the authors provide a justification?\n- Format error in line 442.\n\n\n\nReference:\n\n[1] A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation. ICLR24\n\n[2] Text and Image Are Mutually Beneficial: Enhancing Training-Free Few-Shot Classification with CLIP. AAAI25"}, "questions": {"value": "- Justification of baseline choices as mentioned in the weakness section.\n- I'm kind of surprised by the results of the flow matching training could be done with such limited data. Could the authors provide more insights into this phenomenon?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mzy5cBVx4P", "forum": "educGk5ykl", "replyto": "educGk5ykl", "signatures": ["ICLR.cc/2026/Conference/Submission25102/Reviewer_5BWx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25102/Reviewer_5BWx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752669677, "cdate": 1761752669677, "tmdate": 1762943325566, "mdate": 1762943325566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a flow-based model for multimodal few-shot classification. The approach is highly interesting, thoughtfully addresses various experimental settings, and is well-supported by comprehensive experiments that align closely with the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper innovatively employs a flow-based model to achieve multimodal alignment under few-shot settings. The lightweight flow-matching framework appears to incur minimal computational overhead during training, yet yields significant performance gains.\n2. The experimental evaluation is exceptionally thorough: the main text includes results on ImageNet and medical CXR datasets, while the appendix extends the analysis to multiple additional small-scale datasets. Moreover, the authors explore a wide variety of model combinations, offering robust evidence for the effectiveness of their method.\n3. The writing is clear and accessible, making the technical content easy to follow."}, "weaknesses": {"value": "**Major Weakness:**\n\nI do not identify any significant weaknesses in the paper.\n\n**Minor Weaknesses:**\n\n1. For independently trained unimodal models, what performance would be achieved if FSF were applied directly without OP? The paper does not report this ablation, although I can intuitively sense the function of this component from the author's description.\n2. The appendix includes a comparison between GOP and LOP , but it does not explain why each variant excels in different experimental settings. A deeper analysis of this behavior would be insightful.\n3. In Table 3 (right panel), the second row appears to correspond to LOP but is mistakenly labeled as “OP.”\n4. Line 250 contains a typo: “Sec.” should be formatted properly (e.g., “Section”).\n5. The heading of Section 4.3 suffers from formatting issues.\n6. Throughout many tables, CLIP model names (e.g., “RN50”) are used to refer specifically to their text encoders. While this is generally understandable, it can momentarily confuse readers who naturally associate these names with the full CLIP model."}, "questions": {"value": "1. I encourage the authors to address the minor weaknesses above, particularly by adding the suggested ablation studies and clarifying the GOP vs. LOP behavior.\n2. Section 4.3 presents additional experiments in the medical domain, which, while useful, seem less central than the GOP/LOP analysis and ablation studies in the appendix. Consider swapping their placement to prioritize the more fundamental methodological insights.\n3. Would applying OP to standard CLIP (without FSF) lead to performance improvements? This would help isolate the contribution of OP itself.\n4. Including t-SNE visualizations of feature embeddings before and after flow-based alignment would offer intuitive, qualitative evidence of the method’s impact.\n5. The paper focuses exclusively on few-shot classification. However, I am curious whether the proposed flow-based alignment could be scaled to large-scale multimodal datasets (e.g., LAION-400M). For instance, initializing with DINOv2 and CLIP’s text encoder, then training the flow model for cross-modal alignment. Could this yield representations that surpass standard CLIP?\n\nAlthough I have raised several questions, I consider this a high-quality paper. I am inclined to give it a score of 7–8. However, since the current scoring system cancels a 7, I am provisionally assigning a 6. If the authors can address a substantial portion of my concerns, I would be happy to raise my score to 8 or higher."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7wyjM08eAc", "forum": "educGk5ykl", "replyto": "educGk5ykl", "signatures": ["ICLR.cc/2026/Conference/Submission25102/Reviewer_dW7u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25102/Reviewer_dW7u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914290188, "cdate": 1761914290188, "tmdate": 1762943325271, "mdate": 1762943325271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework for few-shot image classification that can leverage arbitrary, independently pre-trained uni-modal vision and text encoders. The key idea is to bridge the gap between these separate encoders in a two-step process. First, it applies a closed-form Orthogonal Procrustes (OP) mapping to achieve a geometry-preserving linear alignment of the text and image embedding spaces. Second, it trains a lightweight, continuous-time flow model that learns a non-linear transport path between the aligned embeddings. Experiments across multiple benchmarks (ImageNet, OOD variants and VinDr-CXR) demonstrate that FSF is highly effective, often outperforming methods reliant on jointly trained multi-modal encoders."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The primary strength of FSF is its ability to decouple the choice of encoders from the alignment process. Unlike methods that are constrained to work with jointly trained vision-language models like CLIP, FSF can flexibly combine arbitrary uni-modal encoders. This is a significant advantage, as it allows ones to plug in state-of-the-art vision or text models that may be more powerful or better suited for a specific domain (e.g., vision encoder for chest X-rays), as demonstrated by the strong results with RAD-DINO."}, "weaknesses": {"value": "- While the proposed alignment method is interesting, the motivation of using it for few-shot image classification is unclear. Why do we need to align pretrained image and text encoders for few-shot classification? \n\n- The few-shot ImageNet experiment could be flawed. The classes or even images in the ImageNet dataset are likely to be covered by image encoder pretraining dataset. The authors are encouraged to consider the following few-shot learning benchmark.\n\nA Broader Study of Cross-Domain Few-Shot Learning. ECCV 2020\n\n- More recent baselines should be compared because LFA was published in 2023. \n\nFrozen Feature Augmentation for Few‑Shot Image Classification. CVPR 2024\n\nMulti-Label Few-Shot Image Classification via Pairwise Feature Augmentation and Flexible Prompt Learning. AAAI 2025"}, "questions": {"value": "Increased Hyperparameter Complexity: FSF introduces several new hyperparameters compared to simpler alignment methods, including the intermediate integration time τ and the mixing parameter α for inference. While the appendix shows these can be tuned for good performance, this adds a layer of complexity and potential tuning cost that might be a barrier for easy adoption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fgCUm4X133", "forum": "educGk5ykl", "replyto": "educGk5ykl", "signatures": ["ICLR.cc/2026/Conference/Submission25102/Reviewer_eWWC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25102/Reviewer_eWWC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134464938, "cdate": 1762134464938, "tmdate": 1762943324435, "mdate": 1762943324435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}