{"id": "xvQarojMGO", "number": 9753, "cdate": 1758138421603, "mdate": 1759897700441, "content": {"title": "Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling", "abstract": "Extending the context window support of large language models (LLMs) is crucial for tasks with long-distance dependencies. RoPE-based interpolation and extrapolation methods, such as linear scaling and frequency-aware schemes, enable longer input length support without retraining, while post-training quantization (PTQ) makes deployment practical. However, we show that combining RoPE position interpolation (PI) with PTQ degrades accuracy due to coupled effects including long-context aliasing, dynamic-range dilation, anisotropy from axis-aligned quantizers vs. rotated RoPE pairs, and outlier shifting that produces position-dependent logit noise. We provide, to the best of our knowledge, the first systematic analysis of the PI+PTQ approach and introduce two practical diagnostics: interpolation pressure (per-band sensitivity to phase scaling) and tail-inflation ratios (outlier shift from short to long contexts). Following the analysis results, we propose Q-ROAR (Quantization, RoPE-interpolation, and Outlier Aware Rescaling), a weight-only, interpolation-aware stabilization of PI for quantized LLMs. Q-ROAR groups RoPE dimensions into a small number of frequency bands and performs a lightweight search over per-band scales for Key and Query weights (with an optional symmetric variant to preserve logit scale). The search is guided by our diagnostics and uses a tiny long-context development dataset, requiring no fine-tuning to the model, no architecture or kernel changes, and no additional deployment overhead. Empirically, Q-ROAR reduces the model's perplexity on long-context workloads by more than 14%, while preserving short-context performance, inference throughput, and compatibility with existing inference stacks.", "tldr": "", "keywords": ["Rotary positional embedding", "Quantization", "Long-context LLM", "positional Interpolation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2d6ecf8b48ab0dca0585b45709202d362b4c01fa.pdf", "supplementary_material": "/attachment/1b3193a87c6a4508e3ebb65de9d1f4d29cd0919f.zip"}, "replies": [{"content": {"summary": {"value": "The authors identify a negative interaction between interpolating positional embeddings and post-train quantization, as well as introduce methods to mitigate this."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- They study lots of metrics about weights/activations related to quantization. \n- PTQ and long-context are indeed both important to the modern LLM community."}, "weaknesses": {"value": "- My overall gripe is that the paper reads as presenting very complicated interventions and new ideas for a very specific problem they identify, without spending much time ablating or checking carefully that the problem exists across a range of settings (architectures, algorithms, hyperparameters) in the first place. This means a lot of results may be artifacts of their (somewhat bizarre) training setup. \n- For instance, they choose to use Llama-2-7B on the GovReport dataset. This is an ancient model on an entirely nonstandard dataset, and it is the setting chosen for the main Figure in the paper! They also study FP16 reference, when in practice mixed precision always refers to BF16, which is known for being more stable and well behaved. The ablations are with Vicuna (!!!) models, which are *even older and more irrelevant*. Overall, the in which they identify this very specific \"problematic interaction\" is anything close to modern LLM training. You would need to replicate this on Llama3/4 and Qwen-3 as well as at least some standard datasets (GSM, MATH, Alpaca, HumanEval, etc) and it would need to show consistently the same results to be believable. \n- As another specific example of why I'm not convinced this problem is even real is the fact that much of this paper studies outliers that make quantization difficult. Work [1] has long shown a lot of the \"outliers\" in quantization are merely artifacts from not using enough weight decay during training, and that you can get weights/features without outliers if you just change your training to include stronger weight decay, with no effect on performance. This is exactly an artifact that isn't a real phenomenon. \n- The baselines are weak and out of date. The most recent used in Fig1(b) is AWQ from 2 years ago -- there has been a lot of work improving on it since then (QuIP, BitNet, QTIP, etc etc). \n- The prose and structure of the paper is bloated and unnecessary complicated. It especially introduces a lot of heavy jargon for what seems like an oddly specific phenomenon in the first place. This makes it very hard to read. This jargon includes \"phase error,\" \"interpolation pressure,\" \"energy of the noise\" and much more that is not clearly defined up front. Even if it were clearly defined, it's an overwhelming amount of needless complexity and jargon for an extremely specific phenomenon I'm not convinced is real in the first place. \n\n[1] Intriguing Properties of Quantization at Scale. Ahmadian et al, 2023. https://arxiv.org/pdf/2305.19268"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xe1lOGx7r2", "forum": "xvQarojMGO", "replyto": "xvQarojMGO", "signatures": ["ICLR.cc/2026/Conference/Submission9753/Reviewer_b8DP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9753/Reviewer_b8DP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622930518, "cdate": 1761622930518, "tmdate": 1762921247717, "mdate": 1762921247717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the significant performance degradation that occurs when combining Post-Training Quantization (PTQ) with Position Interpolation (PI) to extend the context window of RoPE-based LLMs. The paper provides a systematic analysis of why this combination fails. It attributes the performance drop to several coupled effects, including dynamic-range dilation, anisotropy shift, etc. To mitigate these issues, the authors propose Q-ROAR, which grouping RoPE dimensions into a small number of frequency bands, and applying per-band scaling factors to the weights. Empirically, Q-ROAR demonstrated a clear reduction in perplexity (PPL) on long-context benchmarks,  outperforming baseline PTQ methods like AWQ and RTN."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The strength of this paper lies in its interesting observation that quantized models experience severe performance degradation when using PI-like extrapolation. It also proposes a lightweight, fine-tuning-free method to address this issue."}, "weaknesses": {"value": "The paper's weeknesses are as follows: \n\n1.\tMissing Evaluation Metrics: The paper aims to optimize the performance of quantized models in long-context extrapolation scenarios, but this part of the evaluation relies heavily on Perplexity (PPL), which is an incomplete metric[1]. Although Table 3 tests performance on some datasets, their average lengths are relatively low and cannot effectively reflect long-context capabilities. I suggest the authors compare performance on LongBench-v2 and the \"Needle-in-a-Haystack\" test—two common long-context tasks—to effectively demonstrate Q-ROAR's performance improvement in long-text scenarios.\n2.\tMissing Baseline: An important baseline, SmoothQuant[2], is missing. SmoothQuant also performs a degree of scaling on the weights, which is very similar to the method in this paper, yet it was not included in the comparison.\n\n[1] WHAT IS WRONG WITH PERPLEXITY FOR LONGCONTEXT LANGUAGE MODELING?\n[2] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large"}, "questions": {"value": "1 The experiments in the paper are largely based on Llama-2, which only has a 4k context length. However, current models already have context lengths of 128k. It is uncertain whether the paper's method will still be effective when models extrapolate even further.\n2 In Figure 1(a), why do \"2k\" and \"4k\" appear twice on the x-axis in different positions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j6WHDwvQmX", "forum": "xvQarojMGO", "replyto": "xvQarojMGO", "signatures": ["ICLR.cc/2026/Conference/Submission9753/Reviewer_LGMn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9753/Reviewer_LGMn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747736765, "cdate": 1761747736765, "tmdate": 1762921247390, "mdate": 1762921247390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the sharp degradation of long-context performance when RoPE-based positional interpolation (PI) is applied to post-training-quantized (PTQ) LLMs, and proposes Q-ROAR (Quantization, RoPE-interpolation, Outlier-Aware Rescaling) as a mitigation strategy.\n\nAlthough prior work (e.g., YaRN, NTK-aware scaling) demonstrated that RoPE scaling can extend the context window without additional fine-tuning, the failure modes arising from its interaction with PTQ have not been systematically analyzed. The authors identify four coupled factors—(1) long-context aliasing, (2) dynamic-range dilation, (3) anisotropy mismatch, and (4) outlier shifting—that distort the statistical and geometric properties of Q/K representations under RoPE, ultimately inducing position-dependent logit noise and degrading performance.\n\nTo mitigate this issue, Q-ROAR partitions RoPE frequencies into a small number of bands and performs lightweight grid search to determine band-wise rescaling factors on W_Q and W_K. This weight-only, training-free approach requires no architectural or kernel modifications.\n\nExperiments on LLaMA-2-7B show that Q-ROAR yields ~14–21% perplexity improvements over AWQ/RTN baselines on long-context evaluations such as GovReport and Proof-Pile."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Presents a structured analysis of why RoPE-based position interpolation degrades under post-training quantization, clarifying a practical issue in modern long-context LLMs.\n* By disentangling four contributing effects—aliasing, dynamic-range dilation, anisotropy mismatch, and outlier shifting—the work provides a coherent and interpretable failure characterization.\n* Introduces Interpolation Pressure (IP) and Tail-Inflation Ratio (TIR) as quantitative diagnostics, offering actionable tools to assess frequency-band sensitivity under interpolation.\n* Shows that a lightweight band-wise rescaling of W_Q / W_K can effectively mitigate degradation without retraining, architecture changes, or kernel modifications, making the approach straightforward to adopt.\n* The method exhibits consistent long-context perplexity gains (≈14–21%) over AWQ and RTN on GovReport and Proof-Pile, while preserving short-context performance."}, "weaknesses": {"value": "* Band partitioning and scale selection rely on heuristic grid search, with limited discussion on more principled alternatives.\n* Comparison against more recent quantization approaches (e.g., FlexRound) is limited, making it difficult to fully assess competitiveness.\n* Evaluation focuses primarily on perplexity and zero-shot metrics; few-shot or reasoning benchmarks are not explored.\n* Experiments are centered on LLaMA-2-7B, with insufficient validation on larger models or diverse architectures.\n* It remains unclear how well the method generalizes beyond RoPE-based interpolation or to alternative positional encodings."}, "questions": {"value": "* Comparison with recent quantization techniques (e.g., FlexRound) appears limited. Would the authors expect Q-ROAR to exhibit similar gains relative to these methods?\n* The evaluation focuses on perplexity and zero-shot metrics. Have the authors considered examining few-shot or reasoning benchmarks to provide a broader performance picture?\n* Experiments are primarily conducted on LLaMA-2-7B. Do the authors anticipate similar behavior on larger models or alternative architectures?\n* It is not fully clear how the approach might transfer beyond RoPE-based interpolation. Could the authors comment on applicability to other positional encodings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EyNJFcWKbA", "forum": "xvQarojMGO", "replyto": "xvQarojMGO", "signatures": ["ICLR.cc/2026/Conference/Submission9753/Reviewer_4wZF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9753/Reviewer_4wZF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936459125, "cdate": 1761936459125, "tmdate": 1762921246531, "mdate": 1762921246531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates post-training quantization performance for LLMs when position interpolation is used to extend the context window. It identifies quantization difficulties (though not clearly enough) introduced by the YaRN technique, and then proposes rescaling the Query and Key weights. Experiments are conducted on Llama-7B and Vicuna-7B with 4-bit weight-only and weight-activation quantization. The evaluation uses GovReport and Proof-Pile with perplexity, and commonsense reasoning (Table 3) with accuracy. However, most tasks evaluated in Table 3 involve short contexts (much shorter than pre-training) and therefore do not demonstrate the effectiveness of the proposed method in the long-context regime."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The paper investigates quantization for long-context LLMs after applying position-interpolation techniques. This appears not to have been explored by others.\n\n* The paper analyzes how RoPE scaling affects quantization and finds that it leads to long-context aliasing, dynamic-range dilation, anisotropy, and outlier shifting/amplification.\n\n* Experiments cover both the perplexity and accuracy comparisons."}, "weaknesses": {"value": "* There are some problems with the experiments.\n  * Because the method rescales W_q and W_k, can it be used in models with grouped-query or multi-query attention? These are common techniques in Llama-3-8B and Gemma models.\n\n  * In Table 1, can the authors explain why the proposed method (W4) has even better perplexity than the FP16 baseline, especially at longer lengths? For Table 2, I found that the FP16 baseline of YaRN differs from Table 2 in the original YaRN paper. Since I understand they are evaluated on the same dataset, with the same policy and the same models, I wonder what causes the difference—for example, 5.386 for the FP16 baseline in this paper vs. 2.37 in the YaRN paper. \n  * For Table 3, I don’t think it demonstrates the method well. As far as I know, ARC-C, ARC-E, BoolQ, HellaSwag, and WinoGrande mainly consist of very short sequences (e.g., no more than 512 tokens), so these accuracy comparisons are not a good example for position interpolation and do not illustrate the effect of the proposed quantization method in the long-context regime. Moreover, as observed in Table 2, extending the context window to 32K makes the baseline perform much worse than the 4K window, which contradicts the aim of long-context extensions. To illustrate performance, I suggest accuracy comparisons on long-context benchmarks such as LongBench.\n\n  * In practice, researchers usually first adopt position-interpolation methods as a good starting point and then apply mid-training or fine-tuning to learn at the target length for long-context LLMs. Therefore, I wonder whether the benefits of the proposed method can still be retained for models produced in this more practical pipeline.\n\n* As I understand it, the paper proposes that RoPE rescaling (applied to the query and key) introduces more difficulties for activation quantization. However, to solve the activation problem, what are the benefits of rescaling the weights (W_q and W_k)? Also, how do you ensure that the proposed method does not introduce side effects on the weights?\n\n* Although the authors claim many difficulties—such as long-context aliasing, dynamic-range dilation, anisotropy, and outlier shifting/amplification—brought by position interpolation, the descriptions and demonstrations are not clear to me. For example, in the tail-ratio comparisons in Figure 21, they look similar among No-PI, NTK, and YaRN. In Figure 20, in the upper-left subplot, it is hard to compare No-PI and NTK; in the lower-left subplot, it seems that YaRN even has a smaller outlier count. Therefore, can the authors provide clearer and strong illustrations and elaborations to support the motivation?"}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XEVceLjL6Z", "forum": "xvQarojMGO", "replyto": "xvQarojMGO", "signatures": ["ICLR.cc/2026/Conference/Submission9753/Reviewer_F1UR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9753/Reviewer_F1UR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956068764, "cdate": 1761956068764, "tmdate": 1762921246175, "mdate": 1762921246175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}