{"id": "Pwnf1vsucu", "number": 23774, "cdate": 1758348251613, "mdate": 1759896798005, "content": {"title": "Implicit Models: Expressive Power Scales with Test-Time Compute", "abstract": "Implicit models, an emerging model class, compute outputs by iterating a single parameter block to a fixed point. This architecture realizes an infinite-depth, weight-tied network that trains with constant memory, significantly reducing memory needs for the same level of performance compared to explicit models. While it is empirically known that these compact models can often match or even exceed larger explicit networks by allocating more test-time compute, the underlying reasons are not yet well understood.\n\nWe study this gap through a non-parametric analysis of expressive power. We provide a strict mathematical characterization, showing that a simple and regular implicit operator can, through iteration, progressively express more complex mappings. We prove that for a broad class of implicit models, this process allows the model's expressive power to grow with test-time compute, ultimately matching a much richer function class. The theory is validated across three domains: image reconstruction, scientific computing, and operations research, demonstrating that as test-time iterations increase, the complexity of the learned mapping rises, while the solution quality simultaneously improves and stabilizes.", "tldr": "", "keywords": ["Implicit models", "Deep equilibrium models", "Expressive power"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c7fd22e1f3388642819e77827a84796cbc3003db.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work is concerned with trying to explain the viability of equilibrium models. It is shown using Banach's fixed point theorem that using a regular map can approximate a function class that is only locally Lipschitz, where regularity is defined by contractiveness and global Lipschitz property. The more the map is applied the closer the approximation, thus scaling with test-time compute. This is illustrated with a toy example and further validated by case studies in different domains as image reconstruction, scientific computing, and operational research. The result sheds light on how implicit models can outperform explicit models by finding that implicit models only need to approximate a more regular function, whereas an explicit network would need to be able to approximate the actual function."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The study of the expressivity of implicit models is well motivated. The work presents a clear advantage of equilibrium models in this context. The exposition of work is good with an illustrative toy example and various case studies, especially Fig1 helps to substantiate the theoretical claims made. The argument that we need to be able to approximate locally Lipschitz functions provides also a new generalization perspective."}, "weaknesses": {"value": "1) The theory is an application of a well known result: Banach's fixed point theorem. The theory could be strengthened by providing a rate of minimal/maximal possible growth for the Lipschitz constant. For example Figure 4a seems to suggest non-monotone behavior of the Lipschitz constant growth. Providing a bound could help with heuristics of how many iterations we have to apply in practice.\n\n2) Even tough the work validates their finding in multiple applications they are all from areas in applied mathematics. The work could be strengthened by showing results on a language task or a more open ended task.\n\n3) The illustrations of the implicit and non-implicit FNO in Figure 5 seem to be not very different can this be highlighted better? The only visual difference is the black region in the bottom right corner. In addition, the channel figures might not convey much information. Perhaps removing them and then highlighting the difference more could benefit the exposition."}, "questions": {"value": "See weaknesses as well.\n\nIs the comparison in Figure 5 fair, as the two models have the same amount of parameters, and the implicit model is allowed to reiterate effectively using more compute?\n\nIs the finding of the ability of training implicit GCNs with more overparameterization a new finding or is it more a confirmation of previous work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LOlvPfXezg", "forum": "Pwnf1vsucu", "replyto": "Pwnf1vsucu", "signatures": ["ICLR.cc/2026/Conference/Submission23774/Reviewer_KADN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23774/Reviewer_KADN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761475901761, "cdate": 1761475901761, "tmdate": 1762942799747, "mdate": 1762942799747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper defines a theory of expressivity for implicit models (also called deep equilibrium models or fixed point models). Doing so, the paper provides a theoretically sound contribution towards (and essentially a solution to) the open problem of understanding the richness of implicit neural networks. Implicit architectures have been the subject of much attention over the last several years, but a precise characterization of what can be represented by implicit models is absent in the literature.  The paper contains three case studies illustrating the new expressivity results and defining appropriate \"regular\" implicit models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The key result is that any locally Lipschitz map (on a bounded domain) can be represented as the fixed point of a \"regular\" implicit operator, and vice versa.  Here \"regular\" describes a map that is globally Lipschitz in both variables, but not uniformly so. This result means that a regular implicit model can express maps with singularities and unbounded locally-Lipschitz behavior.\n\nIn short, I find this result, stated in Theorems 2.4 (sufficiency) and 2.5 (necessity), to be remarkable and clearly deserving acceptance.\n\nThe presentation in Introduction and Main result (sections 1 and 2) is lucid.\n\nIn each of the case studies (image reconstruction, scientific computing, and operations research), (i) the target map is locally Lipschitz (ii) an implicit model is proposed that, after standard training, is verified to be regular, and (iii) indeed as the interation count increases the implicit model realized increasingly accurate maps."}, "weaknesses": {"value": "The abstract sats \"these compact models can match or even exceed larger explicit networks\" --- it is unclear what \"exceed\" means. In what sense? Accuracy? Memory or computational efficiency?  (the same confusing language at line 42)\n\nThe document should address the following topic: Do there exist explicit/feedforward networks  capable of expressing locally Lipschitz target maps?  I feel such a comparison should be prominently addressed in the introduction.\n\nCould sparse reconstruction problems (as in compressive sensing) be a special case of the image reconstruction approach in case study 1?"}, "questions": {"value": "Line 32: \"at inference time we apply\"--- but there might be better algorithms to apply than the simple Picard iteration, correct?\n\nLine 128: definition of regular implicit operator. Perhaps you could clarify whether the map has any joint Lipschitz properties.\n\nTheorem 2.5: \"must be locally Lipschitz\" -> \"is locally Lipschitz\".  (but I agree that this is simply a stilistic matter).\n\nLine 174: \"growth\" -> \"grows\"\n\nLine 304: \"Solving ... solves\" = typo\n\nLine 395: \"Programm\"\n\nLine 1157 \"postive\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BrPp1wS3hR", "forum": "Pwnf1vsucu", "replyto": "Pwnf1vsucu", "signatures": ["ICLR.cc/2026/Conference/Submission23774/Reviewer_xk5a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23774/Reviewer_xk5a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761676454965, "cdate": 1761676454965, "tmdate": 1762942799533, "mdate": 1762942799533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper adds to the understanding of how expressive implicit models can be. \n\nImplicit models are an interesting vein for adaptive test-time computation for adaptive model quality. Older related work in adaptive test time compute has focused on using more/less features (see e.g. Parrish et al. JMLR 2013) or more/less base models in an ensemble (see e.g. Wang 2021 Quit When You Can), this approach is a bit more like the strategy of only using as many base models as you have time for from an ensemble, but with the nice difference that you just need to re-use the same model over and over."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "This paper argues that implicit models are even more interesting because they can more easily express certain kinds of complexity than standard explicit models, especially certain kinds of nonlinearities where the iterative mapping G can be reasonably smooth but iterating on it leads to sharp behavior that may otherwise be difficult to model. \n\nThey conclude that their insights mean one popular strategy of implicit models is often over-regularized, and suggest using other regularization strategies instead."}, "weaknesses": {"value": "Modeling Novelty: Bit weird to act like this is a totally new idea guys - using iterative models is an ancient strategy in signal processing (and image processing) and controls. Kalman filtering, iterative filtering, etc,  the idea of repeatedly applying the same model, and that model having free parameters, is not at all a new idea, especially in areas like image deblurring. \n\nTheory Novelty: Theorems 2.4 & 2.5 really seem like something that mathematicians who study fixed-point stuff would already know and have studied in the 70’s.   Does the ML angle really bring a novelty to this problem that means mathematicians who study similar compositions wouldn’t have already studied this exact question? Sadly I’m not a fixed-point math expert, but I’d like to hear from one before being willing to believe the presented theorem is at all novel. \n\nExperiments: 3 case studies were interesting but small and lacked enough comparisons that I was able to be convinced of anything. \n\nReminder to use {} in your bib tex for capitalization (e.g. Euclidean) and make sure the same conference has the same title throughout your references. \n\nOverall Contribution: even ignoring my kvetching about novelty, I didn't the contributions significant enough to merit inclusion at NeurIPS, but I could be swayed if other reviewers helped me see why their insights are more unexpected and interesting than my read."}, "questions": {"value": "Have you talked to pure mathematicians about Theorems 2.4 and 2.5? Does the ML angle really bring a novelty to this problem that means mathematicians who study similar compositions wouldn’t have already studied this exact question and you could just be citing their work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RtoIILLApB", "forum": "Pwnf1vsucu", "replyto": "Pwnf1vsucu", "signatures": ["ICLR.cc/2026/Conference/Submission23774/Reviewer_La71"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23774/Reviewer_La71"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955469306, "cdate": 1761955469306, "tmdate": 1762942799261, "mdate": 1762942799261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the expressive power of implicit neural models, analyzing how iterative fixed-point dynamics affect model capacity both theoretically and empirically."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper provides a rigorous theoretical analysis of the expressive power of implicit neural models, offering how iterative fixed-point dynamics influence expressive capacity."}, "weaknesses": {"value": "- The paper’s framing of \"implicit models\" is confusing and deviates from the standard definition in the community, where implicit models such as Deep Equilibrium Models find an **equilibrium point** via a numerical solver like root-finding techniques rather than explicitly unrolling the recurrence; here, the authors effectively treat implicit models as **recurrent networks** and study the number of unrolled iterations, which misrepresents the core concept, and the term \"test-time computation\" is also confusing in this context.\n\n- The theoretical analysis relies on well-known tools such as function composition, contraction mappings, and ODE formulations, offering limited novelty or new insights beyond existing expressivity analyses.\n\n- The experiments focus on small toy problems and shallow MLP-like architectures without evaluation on modern large-scale tasks or established implicit models; moreover, there is no discussion of how the proposed analysis or conclusions scale with model size, data, or computation, which is central to understanding expressivity and scaling laws in contemporary deep learning.\n\n- The paper overlooks key studies on implicit and equilibrium models, including those addressing training convergence, generalization, and the theoretical connections between implicit models, explicit deep networks, and kernel methods, which weakens its positioning within the broader literature and limits its conceptual impact."}, "questions": {"value": "See the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VqXS92CLrn", "forum": "Pwnf1vsucu", "replyto": "Pwnf1vsucu", "signatures": ["ICLR.cc/2026/Conference/Submission23774/Reviewer_Ybb3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23774/Reviewer_Ybb3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980816779, "cdate": 1761980816779, "tmdate": 1762942798800, "mdate": 1762942798800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Prior works in deep equilibrium models have shown that these networks can outperform regular feedforward models with the same architecture as well as larger models by using more test-time iterations. This paper identifies a broad class of functions called regular functions for which implicit representations provide simple update operators while expressing complex fixed-point mappings. These regular implicit operators gain expressive power with more test-time iterations and can represent more complex class of locally Lipschitz functions. The key assumption is that the domain is bounded and the function is locally Lipschitz on this bounded domain. Then, the paper states that the first iterate is a simple globally Lipschitz map but the subsequent iterates make the map more complex resulting in locally Lipschitz function. This increase in complexity is measured via increase in Lipschitz constant. This theory has been validated on three case studies: image reconstruction, Navier-stokes PDE (scientific computing), and linear programming (operations research). All the three cases demonstrate that increase in test-time iterations result in a better solution and the composition of the regular operator results in a more complex and expressive function."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The key findings are insightful to the community. Prior works have provided proof of universal approximation of implicit models such as deep equilibrium models. The key finding of this paper is the identification of the particular class of functions for which the operator  gains expressive power through additional test-time compute and can match more complex function class the includes singularities and unbounded Lipschitz behavior. This has been proved both theoretically and empirically.\n2. The paper provides valid practical recommendations for implicit models which is to infuse the architecture for operator G with domain priors and constraints instead of constraining the network architecture itself. The paper demonstrates ways to incorporate these priors in the operator through examples in image reconstruction, Navier-stokes PDE and linear programming."}, "weaknesses": {"value": "1. The paper demonstrates empirically that standard training results in such a regular implicit operator operator in some cases, but to the best of my understanding, it does not explore how or if the training guarantees this. \n2. Another limitation of the theoretical result is the assumption of bounded domain. While the paper notes that this is not a major limitation for the results in the paper, how it affects other real-world problems is not well-understood.\n3. Writing: Many parts of paper are dense and a bit difficult to read. It will be useful if proof sketches are included for the main theorems (especially 2.4 and 2.5)."}, "questions": {"value": "1. Some typos: \nLine 69 - demonstrating -> deomonstrate\nLine 395 - LInaer programm -> linear program\n\n2. Line 947: $diam(X) + 1$ can be large, Why is it a meaningful upper bound? This uses the assumption that $X$ is bounded, but even then it can be really large."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Y23UXFFSF2", "forum": "Pwnf1vsucu", "replyto": "Pwnf1vsucu", "signatures": ["ICLR.cc/2026/Conference/Submission23774/Reviewer_FMZf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23774/Reviewer_FMZf"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23774/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985504196, "cdate": 1761985504196, "tmdate": 1762942798505, "mdate": 1762942798505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}