{"id": "YxsfxAvJv4", "number": 1665, "cdate": 1756902962382, "mdate": 1759898196098, "content": {"title": "WorldSense: Evaluating Real-world Omnimodal Understanding for Multimodal LLMs", "abstract": "We introduce WorldSense, the first benchmark to assess the multi-modal video understanding, that simultaneously encompasses visual, audio, and text inputs. In contrast to existing benchmarks, our WorldSense has several features: (i) collaboration of omni-modality, we design the evaluation tasks to feature a strong coupling of audio and video, requiring models to effectively utilize the synergistic perception of omni-modality; (ii) diversity of videos and tasks, WorldSense encompasses a diverse collection of 1,662 audio-visual synchronised videos, systematically categorized into 8 primary domains and 67 fine-grained subcategories to cover the broad scenarios, and 3,172 multi-choice QA pairs across 26 distinct tasks to enable the comprehensive evaluation; (iii) high-quality annotations, all the QA pairs are manually labeled by 80 expert annotators with multiple rounds of correction to ensure quality. Based on our WorldSense, we extensively evaluate various state-of-the-art models. The experimental results indicate that existing models face significant challenges in understanding real-world scenarios (65.1% best accuracy). By analyzing the limitations of current models, we aim to provide valuable insight to guide development of real-world understanding. We hope our WorldSense can provide a platform for evaluating the ability in constructing and understanding coherent contexts from omni-modality.", "tldr": "We introduce WorldSense, the first benchmark to assess models' omni-modal understanding ability.", "keywords": ["OmniModality", "Multimodal LLMs", "Benchmark", "Real-World Understanding"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a469cf8cd21e059fea970d54d6e3bab2bc1f7a93.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces WorldSense, a new benchmark for evaluating the omnimodal understanding capabilities of multimodal LLMs. The benchmark simultaneously incorporates visual, audio, and text inputs to assess a model's ability to comprehend and reason about real-world scenarios. The paper also presents a detailed evaluation of existing multimodal LLMs on the WorldSense benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The WorldSense Benchmark seems well annotated and comprehensively considers visual and auditory information, which is meaningful for evaluating a model's ability to understand video.\n\n- The experiments are sufficient and convincing."}, "weaknesses": {"value": "- In terms of task design, although WorldSense includes a list of tasks, the paper does not clearly show the special design targeted at the integration of audio and visual information. To me, it seems many tasks either assess audio understanding independently or visual understanding independently, rather than evaluating the understanding of audio and video as a whole.\n\n-  There are already some benchmarks for evaluating audio-visual video understanding, such as AVUT [1] and DailyOmni [2]. The paper did not discuss or compare with them. What are the main differences between WorldSense and these benchmarks?\n\n[1] Yang et al, Audio-centric Video Understanding Benchmark without Text Shortcut, arXiv preprint arXiv:2503.19951\n\n[2] Zhou et al, Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment across Modalities, arXiv preprint arXiv:2505.17862"}, "questions": {"value": "- About the evaluation settings.  In Line 316, the paper mentions that the models are tested \"following the recommended pre-processing procedures\". What are the specific test settings for each model (such as the frame rate for video frame extraction, the maximum number of extracted frames, etc.)? Could the different pre-processing settings between models lead to incomparable results?\n\n- The audio-visual LLMs evaluated in the paper seem a bit weak on WorldSense. Could the authors provide an analysis of recent powerful omni models like Qwen3-Omni and Video-SALMONN 2? This would establish a more robust and current baseline, and also help to verify if the significant challenges highlighted by the benchmark are persistent even for the latest generation of models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bul2PCO4zy", "forum": "YxsfxAvJv4", "replyto": "YxsfxAvJv4", "signatures": ["ICLR.cc/2026/Conference/Submission1665/Reviewer_E5me"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1665/Reviewer_E5me"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761553700913, "cdate": 1761553700913, "tmdate": 1762915850250, "mdate": 1762915850250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes WorldSense, a benchmark designed for omni-modal video understanding. Specifically, to answer questions in WorldSense, an MLLM’s response must rely on both video and audio information. Experiments on the proposed WorldSense benchmark reveal the limitations of current MLLMs in omni-modal reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Requiring both video and audio modalities for accurate responses to each question in WorldSense facilitates a more comprehensive evaluation of current MLLMs in omni-modal reasoning.\n2. Experimentally, the performance drop of current video-audio MLLMs indicates that the fusion between modalities is ineffective or even detrimental."}, "weaknesses": {"value": "1. While WorldSense emphasizes real-world omni-modal perception, understanding, and reasoning, the benchmark primarily consists of QA pairs. I believe that interactive question answering would be more practical for real-world scenarios. Moreover, isn’t the term \"omni-modal\" somewhat overstated, given that the benchmark only includes video, audio, and text modalities?\n2. Lack of analysis on why the fusion of open-source audio and video models failed. While I wouldn’t tend to reject the paper for this reason, providing such an analysis would offer the community deeper insights than merely presenting the conclusion."}, "questions": {"value": "1. #82–83: There’s a typo ,“THe”.\n2. #129–130: Pay attention to the spacing between the image title and the main text. It currently looks a bit confusing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lZgMmlJ2Qz", "forum": "YxsfxAvJv4", "replyto": "YxsfxAvJv4", "signatures": ["ICLR.cc/2026/Conference/Submission1665/Reviewer_zBjg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1665/Reviewer_zBjg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979962645, "cdate": 1761979962645, "tmdate": 1762915850032, "mdate": 1762915850032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose WorldSense, an omnimodal benchmark including video, audio, and text. WorldSense requires joint reasoning over synchronized visual and audio inputs to evaluate the ability of existing MLLMs. It contains 3,172 MC QA pairs, spanning various domains and subcategories. The paper evaluates open-source and proprietary MLLMs on WorldSense, showing that current MLLMs are having trouble in integrating audio and visual data effectively in realistic setting."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Solid motivation. The paper focus on omnimodal reasoning, requiring the models to utilize visual and audio inputs together to answer the question. This designed correlations make it distinct from most existing MLLM benchmarks.\n2. High-quality human-reviewed annotations. Unlike recent benchmarks, WorldSense is reviewed and revised by human expert, instead of relying solely on LLMs. This is a guarantee for the quality of the benchmark. It is very rare these days.\n3. Comprehensive experiments. The paper conducted comprehensive experiments on existing models, including open-source ones and proprietary ones, providing a benchmarking foundation for future research."}, "weaknesses": {"value": "1. Video caption is not a good representative for \"text\" modality. This makes the \"omni\" a little overclaim. Given real-world constraints, audio–video may already suffice for evaluating omnimodality, as seen in emerging “world models.”\n2. The question types are restricted in multiple-choice QA. This is common in existing benchmarks, but given the ability of MLLMs, free-form answers can yield deeper insights and align with user-end usage.\n3. The benchmark is currently focusing on perception and recognition. Given the lengths of the dataset (141.1s) and the requirement for multiple modalities, it may be possible to curate a subset for higher-level reasoning tasks."}, "questions": {"value": "1. How does WorldSense handle temporal reasoning—do questions depend on sequential context or only short clips? An ablation study on image / a few frames would elaborate this.\n2. This is beyond the scope of this paper. But is it possible to extend WorldSense to an open-ended or generative benchmark? This would make it more aligned with user-end usage."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5alQqqTm2t", "forum": "YxsfxAvJv4", "replyto": "YxsfxAvJv4", "signatures": ["ICLR.cc/2026/Conference/Submission1665/Reviewer_Fj9V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1665/Reviewer_Fj9V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980928587, "cdate": 1761980928587, "tmdate": 1762915849851, "mdate": 1762915849851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces WorldSense, a new benchmark designed to evaluate how MLLMs understand and reason about real-world video-audio-text inputs. WorldSense integrates both audio and video to test true omni-modal understanding.\nThe dataset includes 1,662 synchronized videos across 8 domains and 67 categories, with 3,172 multiple-choice QA pairs spanning 26 different tasks. These questions require both audio and visual cues to answer correctly. The authors also evaluate various open-source and proprietary MLLMs, showing that even advanced models like Gemini 2.5 Pro only achieve 65.1% accuracy, exposing significant gaps in real-world multimodal reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A novel audio-video benchmark focused on omni-modal understanding.\n- Expert curated by annotators and manual QA design for better quality.\n- Paper is well-written."}, "weaknesses": {"value": "- Although, the dataset is diverse, the distribution of question difficulties across categories or cognitive levels is not very clear. Some tasks might be more perception-heavy than reasoning-heavy, which could bias model comparison."}, "questions": {"value": "- How do you ensure that questions cannot be answered from text transcripts alone?\n- Will the benchmark be publicly released, and under what license?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yboAMMPRLl", "forum": "YxsfxAvJv4", "replyto": "YxsfxAvJv4", "signatures": ["ICLR.cc/2026/Conference/Submission1665/Reviewer_ZaHB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1665/Reviewer_ZaHB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1665/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056838677, "cdate": 1762056838677, "tmdate": 1762915849618, "mdate": 1762915849618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}