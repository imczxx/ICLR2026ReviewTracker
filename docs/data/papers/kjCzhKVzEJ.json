{"id": "kjCzhKVzEJ", "number": 8034, "cdate": 1758053483540, "mdate": 1759897812793, "content": {"title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning", "abstract": "Reinforcement learning (RL) has shown promise in training agentic models that move beyond static benchmarks to engage in dynamic, multi-turn interactions. Yet, the ultimate value of such agents lies in their ability to assist users, a setting where diversity and dynamics of user interaction pose challenges. In this work, we propose **UserRL**, a unified framework for training and evaluating user-centric abilities through standardized gym environments paired with simulated users. We systematically vary turn-level reward assignment and trajectory-level score calculation to analyze how different formulations affect learning under the GRPO algorithm. Our experiments across Qwen3 models reveal that: (i) SFT cold start is critical for unlocking initial interaction ability and sustained RL improvements; (ii) deliberate trajectory scoring yields more efficient and effective multi-turn interactions; and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training, open-source simulators (e.g., Qwen3-32B) remain a cost-effective and transferable option. Together, these results highlight that careful design of reward shaping and user simulation choice is as crucial as model scale, and we establish UserRL as a practical pathway for developing robust user-centric agentic models.", "tldr": "We present UserRL, a framework with diverse gym environments for training and evaluating user-centric agentic models through standardized interaction and reward shaping.", "keywords": ["LLM agents", "RL reward design", "User-centric interaction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/444f84f4a0ce435103221d04017baad27689dbff.pdf", "supplementary_material": "/attachment/71d84cdf359ccb0d2d99f573912c254f9ba23efa.zip"}, "replies": [{"content": {"summary": {"value": "The paper positions itself within the agentic LLM literature. The goal is to develop LLM agents that are more user-centric. Their first contribution is a new gym environment with 8 new tasks that can be used to train and evaluate LLM agents in terms of user assistance. Their pipeline uses LLM-based user simulation and multi-turn RL. Their second contribution is the extension of GRPO to the multi-turn setting by decoupling multi-turn and trajectory level feedback, essentially allowing for different reward distribution among turns. The empirical results with user simulators show improvement over raw LLMs in the proposed gym environments. A proof-of-concept real user study is presented but it is severely limited (5 participants)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- UserRL Gym looks like a meaningful contribution for developing/evaluating LLM-based agents. The design seems sensible to me, and the specific environments are interesting. (I am not from the LLM community, so I can’t fully assess novelty.)\n\n- The proposed reward shaping methods look sensible, and the insight provided is useful to understand the design choices.\n\n- The objective—a multi-turn extension of GRPO with decoupled turn-level shaping and trajectory-level scoring—seems like a natural idea and potentially novel.\n\n- Experiments appear strong: the proposed method outperforms raw open-source models and also closed-source models in most tasks. It’s not surprising that Gemini wins in SearchGym, so that result doesn’t bother me."}, "weaknesses": {"value": "- The paper states that the equalized approach uses $\\tilde{r}_t = c$ and (as written) this seems to imply that GRPO gives the same reward to every token within a turn. My reading of a recent paper [1] suggests this is a bit more nuanced: in vanilla GRPO, each completion does receive a single outcome-based advantage that’s applied uniformly to all its tokens (in the policy term), but this objective is equivalent to assigning step-level rewards to shared-prefix spans; when prefixes overlap across completions, different steps (and thus tokens) within a turn can receive different effective signals. I’d like the authors to clarify this, preferably in light of [1].\n\n- The analysis of the decoupling (turn-level shaping vs. trajectory-level scoring) could be made more precise, again considering the above nuance from [1].\n\n- Positioning/novelty is hard for me to judge because Related Work is moved entirely to the appendix. The brief intro paragraphs are not enough (for a non-LLM RL reviewer) to place the work clearly.\n\n- Results wording: the statement “Equalized/R2G setting achieves the best performance across 4B and 8B models” should be made precise that this is average performance across all tasks. If we look task-by-task, it loses to other models in 6/8 tasks.\n\n- On user study: as it stands (n=5, all CS PhD students), I would treat this as a proof-of-concept rather than a user study. We cannot really infer anything from this.\n\n\n\n[1] GRPO is Secretly a Process Reward Model, 2025,  \tarXiv:2509.21154"}, "questions": {"value": "1. On the GRPO point: do you really mean GRPO gives the same reward to each token within a turn? How does that reconcile with the GRPO-as-PRM view (shared-prefix/step-level signals) in [1]? If you intend the “uniform per turn” description in the outcome-reward sense, can you please state that explicitly and discuss when shared-prefix structure induces non-uniform token-level signals?\n\n2. For the decoupled objective, in what regimes (task types, group sizes, sampling temperature) does it help beyond what standard GRPO already induces through shared prefixes? Any cases where decoupling hurts?\n\nPlease free to respond to any points raised in Weaknesses as well.\n\n[1] GRPO is Secretly a Process Reward Model, 2025,  \tarXiv:2509.21154"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mz7fPOddSe", "forum": "kjCzhKVzEJ", "replyto": "kjCzhKVzEJ", "signatures": ["ICLR.cc/2026/Conference/Submission8034/Reviewer_zKr9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8034/Reviewer_zKr9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761741782090, "cdate": 1761741782090, "tmdate": 1762920029151, "mdate": 1762920029151, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UserRL, a framework for training user-centric agents via reinforcement learning (RL) with a focus on dynamic, multi-turn user interactions. A core, compelling idea is its alignment with the realistic scenario where user intentions clarify gradually through interaction—a critical yet underexplored aspect of human-AI collaboration. The work introduces 8 standardized Gym environments covering skills like persuasion, intention understanding, and personalized recommendation, alongside a flexible reward mechanism (Equalized/R2G) and SFT cold-start strategy. Key contributions include: (1) addressing the lack of unified training frameworks for diverse, evolving user behaviors; (2) demonstrating that open-source models trained with UserRL can outperform closed-source counterparts in user interaction tasks; and (3) validating the effectiveness of decoupling turn-level and trajectory-level rewards for multi-turn RL."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Alignment with Real-World Intent Evolution**: The idea that user intentions *gradually clarify through interaction*—is highly relevant to practical AI assistant use cases. \n- **Methodological Clarity**: The design of 8 standardized Gym environments with explicit interaction rules and the detailed breakdown of reward mechanisms (Equalized/R2G, SFT cold-start) provide a reproducible framework for training agents to navigate evolving user intentions.\n- **Open-Source Focus**: Demonstrating that open-source models (Qwen3 series) can achieve strong performance in user interaction tasks (surpassing GPT-4o, Gemini-2.5 in some Gyms) highlights accessibility and scalability, especially for researchers studying intention-aware AI."}, "weaknesses": {"value": "- **Ambiguous Capability Boundaries of SFT and RL**: While the paper emphasizes SFT as a “cold start” for RL, it does not clearly delineate which capabilities (e.g., basic interaction norms vs. strategic intention-mining) come from SFT vs. RL. Ablations comparing “SFT-only” vs. “SFT+RL” would clarify how each component contributes to handling *evolving user intentions*.\n- **Lack of Zero-Shot Transfer Experiments**: The generalizability of UserRL-trained agents to *unseen* domains (where intention evolution might follow different patterns, e.g., medical consultation) is untested. Zero-shot evaluations on novel Gyms would be critical to assess if the framework truly learns *generalizable strategies* for intention clarification.\n- **Impact on General Task Performance**: It is unknown whether the training process degrades performance on foundational benchmarks (e.g., math, code). Evaluating trade-offs between intention-aware interaction skills and general capabilities is necessary to gauge the approach’s practicality.\n- **Limited Real-User Scale and Diversity**: Expanding to a larger, more diverse user pool (e.g., non-technical users, different demographics) would strengthen claims about the framework’s ability to handle *naturally evolving intentions* in the wild."}, "questions": {"value": "1. Could you clarify the *exact capabilities* imparted by SFT vs. RL in the context of *evolving user intentions*? For example, please compare performance metrics of “SFT-only” models and “SFT+RL” models in Gyms where intention clarification is central (e.g., IntentionGym, TravelGym) to isolate their respective contributions.\n2. What is the performance of UserRL-trained agents in **zero-shot transfer** to entirely new domains (e.g., healthcare or education) where user intentions might evolve differently? Please include these experiments to demonstrate the generalizability of strategies for intention clarification.\n3. Does training with UserRL negatively impact performance on *general* benchmarks (e.g., math, code,)? If so, how significant is the trade-off, and does it affect the agent’s ability to support intention-driven tasks (e.g., using math skills to fulfill a user’s budget-related travel needs)?\n4. Could you expand real-user testing to a more diverse population (e.g., non-technical users, different age groups) and report on how performance varies when handling *naturally evolving, heterogeneous intentions*?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "See weakness and questions."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BNK7Lkqvly", "forum": "kjCzhKVzEJ", "replyto": "kjCzhKVzEJ", "signatures": ["ICLR.cc/2026/Conference/Submission8034/Reviewer_KG5K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8034/Reviewer_KG5K"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748455862, "cdate": 1761748455862, "tmdate": 1762920028474, "mdate": 1762920028474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### *Summary*\n\n- The authors propose the UserRL framework, which uses the Interact Tool to encompass multiple gym environments for multi-turn RL with LLM-based user simulation. The authors propose a GRPO-like setup that reconciles trajectory-based advantages for different environments while also including the possibly to have turn-level advantages. The authors include 8 gym environments (3 novel data sources) compatible with UserRL, on which they conduct experiments of various models.\n\n### *Contributions*\n\n- UserRL codebase and framework based on a unified tool interface with 8 gym envs, 3 of which are novel  \n- An evaluation of various turn-based reward schemes that decouples turn-based reward shaping and trajectory-level rewards  \n- Evaluation metrics that go beyond simple task success metrics for user-centric RL\n\n(Side note: Appendix B describes the contributions more clearly than the introduction)"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### *Originality*\n\n- 8 new gym environments, including 3 with newly curated datasets.\n\n### *Quality*\n\n- Interesting results relating to scale and benchmark overfitting\n\n### *Clarity*\n\n- Paper is well-structured, and mostly well-written\n\n### *Significance*\n\n- This interface will be useful to the community if properly implemented and maintained."}, "weaknesses": {"value": "### *Originality*\n\n- Related works in the appendix is always a gamble because it makes it hard for someone who is not deeply familiar with the relevant literature to evaluate the novelty of your work.   \n- The related work section does not *compare and contrast* to previous work. This makes it hard to evaluate the novelty of the proposed contributions. It would be good to highlight the *differences* with previous works, rather than listing related works.\n\n### *Quality*\n\n- Unclear how easy it is to build on UserRL, either to extend it or to use it in conjunction with e.g. verifiers, or introduce more complex agent-like user simulations  \n- No validation of the quality of the proposed Gym environments\n\n### *Clarity*\n\n- Figure 1 is disconnected from its caption. For such a complex figure, it’s good to “walk through” the figure and explain what’s going on.  \n- Weird syntax in some places, e.g. L259 “Importantly, this design reflects user-centric:”. So far I haven’t seen anything that really prevents clear understanding, but these mistakes definitely “break the flow”.  \n- The position of appendix B is off. This seems like it fits much more at the end of the introduction because it highlights the contributions. Furthermore, the discussion of “novelty” is difficult because it is decoupled from the related works in appendix A."}, "questions": {"value": "The main obstacle for me is understanding how this work positions itself against what is already out there. I am familiar with GRPO, verl, verifiers, etc., but I am not deeply familiar with this literature. There are so many proposed frameworks for RL these days that the distinct value of the framework needs to be extremely clear for me to evaluate its originality, and therefore its significance and quality. Therefore, my main question is: \n\n- What are the closest existing works to UserRL and how is your work different? Why are these differences meaningful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KIcJkAltCC", "forum": "kjCzhKVzEJ", "replyto": "kjCzhKVzEJ", "signatures": ["ICLR.cc/2026/Conference/Submission8034/Reviewer_1A49"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8034/Reviewer_1A49"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749265606, "cdate": 1761749265606, "tmdate": 1762920028105, "mdate": 1762920028105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### *Summary*\n\n- The authors propose the UserRL framework, which uses the Interact Tool to encompass multiple gym environments for multi-turn RL with LLM-based user simulation. The authors propose a GRPO-like setup that reconciles trajectory-based advantages for different environments while also including the possibly to have turn-level advantages. The authors include 8 gym environments (3 novel data sources) compatible with UserRL, on which they conduct experiments of various models.\n\n### *Contributions*\n\n- UserRL codebase and framework based on a unified tool interface with 8 gym envs, 3 of which are novel  \n- An evaluation of various turn-based reward schemes that decouples turn-based reward shaping and trajectory-level rewards  \n- Evaluation metrics that go beyond simple task success metrics for user-centric RL\n\n(Side note: Appendix B describes the contributions more clearly than the introduction)"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### *Originality*\n\n- 8 new gym environments, including 3 with newly curated datasets.\n\n### *Quality*\n\n- Interesting results relating to scale and benchmark overfitting\n\n### *Clarity*\n\n- Paper is well-structured, and mostly well-written\n\n### *Significance*\n\n- This interface will be useful to the community if properly implemented and maintained."}, "weaknesses": {"value": "### *Originality*\n\n- Related works in the appendix is always a gamble because it makes it hard for someone who is not deeply familiar with the relevant literature to evaluate the novelty of your work.   \n- The related work section does not *compare and contrast* to previous work. This makes it hard to evaluate the novelty of the proposed contributions. It would be good to highlight the *differences* with previous works, rather than listing related works.\n\n### *Quality*\n\n- Unclear how easy it is to build on UserRL, either to extend it or to use it in conjunction with e.g. verifiers, or introduce more complex agent-like user simulations  \n- No validation of the quality of the proposed Gym environments\n\n### *Clarity*\n\n- Figure 1 is disconnected from its caption. For such a complex figure, it’s good to “walk through” the figure and explain what’s going on.  \n- Weird syntax in some places, e.g. L259 “Importantly, this design reflects user-centric:”. So far I haven’t seen anything that really prevents clear understanding, but these mistakes definitely “break the flow”.  \n- The position of appendix B is off. This seems like it fits much more at the end of the introduction because it highlights the contributions. Furthermore, the discussion of “novelty” is difficult because it is decoupled from the related works in appendix A."}, "questions": {"value": "The main obstacle for me is understanding how this work positions itself against what is already out there. I am familiar with GRPO, verl, verifiers, etc., but I am not deeply familiar with this literature. There are so many proposed frameworks for RL these days that the distinct value of the framework needs to be extremely clear for me to evaluate its originality, and therefore its significance and quality. Therefore, my main question is: \n\n- What are the closest existing works to UserRL and how is your work different? Why are these differences meaningful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KIcJkAltCC", "forum": "kjCzhKVzEJ", "replyto": "kjCzhKVzEJ", "signatures": ["ICLR.cc/2026/Conference/Submission8034/Reviewer_1A49"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8034/Reviewer_1A49"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749265606, "cdate": 1761749265606, "tmdate": 1763678493525, "mdate": 1763678493525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The experimental details and gymnasium environments are thorough and well-documented in the main and supplemental material. The gymnasia themselves, while important within the authors' proposed framework, do not appear to be substantially novel contributions (on their own). The authors' argue that their UserRL framework provides flexibility in reward-based training across these gymnasia. Here, the novelty of this framework appears to lie within the turn-level reward shaping. In their Appendix Section B outlining their contributions, this corresponds to point (ii), which seems substantively to be the most important component of this UserRL framework. The authors do include interesting experimental results, particularly the authors' finding that scaling model size does not necessarily yield improvements in their empirical analysis."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "UserRL introduces a unified and extensible framework for training and evaluating user-centric agents through eight newly designed gym environments and simulated users. Its decoupling of turn-level reward shaping from trajectory-level scoring represents a novel methodological contribution, enabling controlled analysis of multi-turn reinforcement learning (RL) dynamics. The integration of user simulation and standardized tool interfaces provides an innovative approach to studying interactive alignment and reward design. It systematically explores multiple reward-shaping strategies and trajectory-level scoring methods, supporting conclusions with detailed experiments across model sizes and simulators."}, "weaknesses": {"value": "The decoupling of the trajectory and turn based scoring is not sufficiently demonstrated. \nAdditionally, the authors accurately identify limitations in their Discussion that mitigate the importance of this contribution, without further engineering for heterogenous environments. \nWhile their framework shows promise and the identified gaps/applications are important, this work does not substantially address its motivations. I believe further demonstration of the importance of this novel framework is necessary to justify this as a substantial contribution\nTheir outlined contributions, while interesting and appreciated, do not sufficiently demonstrate novelty for publication (or at least, are not compellingly justified beyond their statement)."}, "questions": {"value": "- As the authors state, this turn-based reward differentiation alone does not appear to be sufficient for a *universal* training strategy. Environment/Gymnasium specific training appears still necessary. This importantly tempers the ability of this work to address some of the authors' proposed motivations, namely that their heterogenous environments address the diversity of user interactions. \n\n- Is there a principled rationale for the selection of reward-shaping? That is, a priori, could a user decide (or even define) gynmasia specific reward-shaping that maintains the authors' proposed \"decoupling\"? This would appear to mitigate (although not solve) the comment above by providing greater flexibility in training across environments. \n\n- Could the authors' further substantiate their claim that \"UserRL generalizes GRPO to multi-turn interactive settings by decoupling turn-level reward shaping from trajectory-level scoring\"? Perhaps this follows from the form of their J_{UserRL} equation, but it is currently not obvious from their proposed framework how they achieve this \"decoupling\". This is a specific but pivotal point, as this claim seems to be the primary proposed contribution of UserRL. \n\n- Can the authors' justify their selection of training and test gyms? Is it posible to simulate independent data for the inverse of their selection (or any other arbitrary configuration of gymnasia)? They may be helpful to demonstrate a sense of robustness of their UserRL framework. \n\n- The sample size for the real-user study (n=5) is underwhelming, and this fact is not apparent in the main text but deep into the Appendix. This should be transparently reported in the main text when discussing the real-user results. Conclusions based on the real-user analysis should be more modest, given this small (and possibly biased, including PhD students who we may presume to be quite familiar end-users) sample. This is not transparently reported. \n\n- Some of the notation is not immediately clear. For example, in the J_{UserRL} equation, $i=1, ..., n$ indexes different trajectories within a single gym (e.g. repeated \"experiments\"/observations)? It could be useful to explicitly collect some of this notation (i.e. i trajectories, L_t tokens per trajectory time, etc.) rather than stating them throughout the text (space/organization permitting)\n\n- Can the trajectory scorer vary across gymnasia? Or is this fixed across environments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wlJcEUB2BT", "forum": "kjCzhKVzEJ", "replyto": "kjCzhKVzEJ", "signatures": ["ICLR.cc/2026/Conference/Submission8034/Reviewer_7pMg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8034/Reviewer_7pMg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8034/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762192405, "cdate": 1761762192405, "tmdate": 1762920027749, "mdate": 1762920027749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}