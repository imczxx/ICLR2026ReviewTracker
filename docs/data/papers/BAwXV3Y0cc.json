{"id": "BAwXV3Y0cc", "number": 4695, "cdate": 1757748761537, "mdate": 1759898019583, "content": {"title": "TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise Temporal Distance", "abstract": "Designing dense rewards is crucial for reinforcement learning (RL), yet in robotics it often demands extensive manual effort and lacks scalability. \nOne promising solution is to view task progress as a dense reward signal, as it quantifies the degree to which actions advance the system toward task completion over time.\nWe present TimeRewarder, a simple yet effective reward learning method that derives progress estimation signals from passive videos, including robot demonstrations and human videos, by modeling temporal distances between frame pairs.\nWe then demonstrate how TimeRewarder can supply step-wise proxy rewards to guide reinforcement learning.\nIn our comprehensive experiments on ten challenging Meta-World tasks, \nwe show that TimeRewarder dramatically improves RL for sparse-reward tasks,\nachieving nearly perfect success in 9/10 tasks with only 200,000 interactions per task with the environment. This approach outperformed previous methods and even the manually designed environment dense reward on both the final success rate and sample efficiency.\nMoreover, we show that TimeRewarder pretraining can exploit real-world human videos,\nhighlighting its potential as a scalable approach path to rich reward signals from diverse video sources.", "tldr": "", "keywords": ["Reward Learning", "Robotic Manipulation", "Vision-based RL"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6d1034e4506162738887612d22568b572b80417e.pdf", "supplementary_material": "/attachment/3f39281a618fd871aff7e9691597f0c6eeb44d81.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a reward-learning method that turns passive videos (robot or human) into dense, step-wise rewards for RL. In detail, they learn a model to predict a frame-wise temporal distance between two observations. At RL time, the predicted temporal distance between adjacent frames serves as a progress reward, optionally combined with a sparse success signal. The authors emphasize three training choices: (i) implicit negatives via forward/backward ordering to encode regress vs. progress, (ii) exponentially weighted pair sampling to focus on short temporal gaps, and (iii) two‑hot discretization for stable training. For experiments, the method reportedly reaches near‑perfect success on 9/10 tasks with 200k interactions and, in most cases, outperforms both prior video‑based contenders and the environment’s dense reward on Meta‑World tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Simple objective with clear inductive bias. Framing reward learning as temporal distance prediction is intuitive and easy to implement.\n- Thorough ablations on design choices. The study isolates the contributions of implicit negatives, weighted sampling, and two‑hot discretization. It is useful for practitioners."}, "weaknesses": {"value": "- Originality is thinner than claimed. Temporal Distance Classification (TDC) was introduced in [1]; it discretizes time differences and trains a classifier on video frame pairs, which is very close to this paper’s core target. Related pretext tasks like Time‑Contrastive Networks and Temporal Cycle Consistency similarly extract progress‑like signals from temporal structure [2,3], and Shuffle&Learn / “arrow‑of‑time” also leverage ordering cues [4]. The paper cites TCN but does not cite TDC/TCC or clearly differentiate from them.\n- Connections to recent progress‑from‑video rewards are not discussed. TimeRewarder is close in spirit to VIP (value‑implicit pretraining), GVL (VLMs as in‑context value estimators), Rank2Reward (temporal ranking for shaped reward), and HashReward (progress reward with online hashing). The paper positions against these but omits or soft‑pedals some important links and failure modes already discussed there (e.g., distribution shift and online refinement) [5–8].\n- The paper gestures at potential‑based shaping $R(o_t, o_{t+1})=V(o_t) - \\gamma V(o_{t+1}))$ but the learned signal is pairwise $F_\\theta(o_t, o_{t+1})$, not an explicit potential $V(o). Without showing that $F_\\theta$ ntegrates to a state potential (or at least is path‑independent), there is no policy‑invariance guarantee. In the absence of this, shaped rewards may induce loops or reward hacking in off‑expert states [9].\n\n## Reference\n[1] Playing Hard Exploration Games by Watching YouTube.\n\n[2] Time-Contrastive Networks: Self-Supervised Learning from Video. \n\n[3] Temporal Cycle-Consistency Learning.\n\n[4] Learning and Using the Arrow of Time.\n\n[5] VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training.\n\n[6] Vision Language Models are In-Context Value Learners.\n\n[7] Rank2Reward: Learning Shaped Reward Functions from Passive Video.\n\n[8] Imitation Learning from Pixel-Level Demonstrations by HashReward.\n\n[9] Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yl2kVmCxl7", "forum": "BAwXV3Y0cc", "replyto": "BAwXV3Y0cc", "signatures": ["ICLR.cc/2026/Conference/Submission4695/Reviewer_igvB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4695/Reviewer_igvB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761378394585, "cdate": 1761378394585, "tmdate": 1762917519087, "mdate": 1762917519087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Time rewarder presents a method of deriving a reward function from expert demonstrations through predicting task progress. Their reward model predicts the relative distance between two frames, normalized from -1 to 1, measuring both forward and negative progress. Time rewarder beats out chosen baselines, including both learning from observations only (OT ADS GAIfO) and other IRL methods (VIP Rank2Reward PROGRESSOR). They also include experiments that co-train their reward with human videos, and a provide a theoretical motivation for progress rewards."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "This paper includes strong baseline comparisons across multiple tasks. They also included ablation studies explaining each of their design decisions, such as descritization, negative sampling, and relative vs. absolute progress. It is well written and well executed. The results on cross-embodiment reward transfer are also strong."}, "weaknesses": {"value": "Using task progress or temporal ordering as a reward signal is not particularly novel. Several prior works (VIP, Rank2Reward, PROGRESSOR) already use temporal structure for reward learning. They note that other ranking-based methods are difficult to optimize reliably, and that rank2reward and PROGRESSOR report poor performance on out-of-distribution states. However, for their data used to train the ranking function, they collect demonstrations “under a deliberately diverse initialization protocol.”  \n\nAdditionally, Time Rewarder uses a CLIP-pretrained ViT backbone. Prior work like RoboCLIP [1] has shown that a CLIP-pretrained representation can produce meaningful reward signals. It would help to isolate the contribution of the proposed method from that of the pretrained features. One way to do this would be a comparison using a non-contrastively pretrained backbone.\n\n[1]  Sumedh Anand Sontakke, Jesse Zhang, Séb Arnold, et al. RoboCLIP: One Demonstration is Enough to Learn Robot Policies."}, "questions": {"value": "* Given that the demonstration set is deliberately diverse, does the time rewarder require state coverage of expert demonstrations to ensure relative progress does not go out of distribution?  How does Time Rewarder perform under the standard expert demonstrations for Meta World?\n* The behavior cloning policy seems to perform poorly. If the authors could explain why this occurs in their setting, it would help contrast with their method. \n* What enables TimeRewarder to outperform dense-environment rewards? \n* For co-training on the human videos, if you use both the human videos and the 100 meta-world demos, does this further improve performance compared to just the 100 demos?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pCDBQ4lrBS", "forum": "BAwXV3Y0cc", "replyto": "BAwXV3Y0cc", "signatures": ["ICLR.cc/2026/Conference/Submission4695/Reviewer_QyHg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4695/Reviewer_QyHg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761514983236, "cdate": 1761514983236, "tmdate": 1762917518344, "mdate": 1762917518344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TimeRewarder, a method for learning dense rewards from action-free expert videos.\nThe key idea is to model task progress as a temporal distance prediction problem between frame pairs.\nBy self-supervisedly learning the temporal gap between two observations, the model implicitly captures how far the agent has progressed toward task completion.\nThe approach integrates three main components: implicit negative sampling, exponentially weighted pair sampling, and two-hot discretization.\nTheoretically, the learned temporal distance corresponds to a potential-based reward-shaping term, ensuring Bellman consistency.\nExperiments on ten Meta-World manipulation tasks show that TimeRewarder's rewards remain highly monotonic on unseen expert trajectories, successfully distinguish failed rollouts, and enable reinforcement learning agents to achieve top success rates.\nOverall, TimeRewarder is simple, theoretically grounded, and empirically strong."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a simple yet effective idea, treating reward learning as temporal-distance estimation, which provides a clean, self-supervised way to learn dense progress signals without requiring expert actions or environment rewards.\n\n2. The connection between temporal distance and potential-based reward shaping gives the method solid theoretical justification and ensures consistency with RL principles.\n\n3. Evaluations on ten Meta-World tasks are extensive, showing high sample efficiency and outperforming both prior progress-based methods and imitation baselines.\n\n4. Through implicit negative sampling, the model can recognize and penalize regressive or failed behaviors, improving policy learning stability.\n\n5. The ablation studies carefully isolate and validate the contribution of each design component, reinforcing the method’s architectural soundness."}, "weaknesses": {"value": "1. The cross-domain generalization largely stems from the CLIP backbone's semantic representations rather than the proposed temporal-distance formulation itself. The method benefits from CLIP's object-centric and domain-invariant features, but this reliance makes it unclear how well TimeRewarder would perform with less powerful or task-specific encoders.\nThe paper does not analyze how different visual backbones or representations affect performance, making it hard to disentangle the contribution of TimeRewarder from that of pretrained features.\n\n2. The core idea, learning task progress from temporal consistency, builds on prior progress-based reward learning work (e.g. VIP, PROGRESSOR, Rank2Reward) with incremental architectural refinements rather than a fundamentally new principle.\n\n3. The method assumes that expert trajectories reflect smooth, monotonic advancement toward a goal. This assumption breaks in multi-stage or looping tasks where temporal distance no longer correlates with true progress.\n\n4. The reward predictor is fixed during RL, which limits adaptability to out-of-distribution states encountered in exploration."}, "questions": {"value": "1. In the human-to-robot transfer experiment, was one model trained per task, or a single model shared across all three tasks?\n\n2. How much of TimeRewarder's generalization depends on the CLIP backbone? It would be helpful to include an ablation using non-pretrained or weaker encoders (e.g., a randomly initialized ViT or a task-specific CNN) to quantify the backbone's contribution.\n\n3. The reward network is frozen during reinforcement learning. Would online fine-tuning or adaptive updating improve robustness to out-of-distribution states encountered during policy exploration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ndt0XV4Cy8", "forum": "BAwXV3Y0cc", "replyto": "BAwXV3Y0cc", "signatures": ["ICLR.cc/2026/Conference/Submission4695/Reviewer_GauR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4695/Reviewer_GauR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637731666, "cdate": 1761637731666, "tmdate": 1762917517978, "mdate": 1762917517978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TimeRewarder, a method that learns a dense, step-wise reward for reinforcement learning directly from passive expert videos by predicting the temporal distance between pairs of frames. The authors propose learning a model that outputs per-step rewards along an agent’s trajectory that signal progress in successful completion of the task, combined with weighted sparse task reward. To stabilize their proposed method, the use the following:\n1. *Implicit Negative Sampling*: The word sampling is misleading here. They mean to say that their temporal distance reward can be positive or negative depending on the order of the frames in the recorded trajectory\n2. *Exponentially Weighted Pair Sampling*: Sample adjacent frames more frequently than distant frames\n3. *Two-hot Discretization*: An auxiliary loss to stabilize training; this seems to stem from an intuition that classification losses are more numerically stable than regression losses in deep learning.\n\nThe evaluate their results in simulation (MetaWorld) and compare with some relevant competitors."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation and problem setting\n  - Addresses the practical challenge of dense reward design for manipulation from only passive videos, without action annotations.\n  - Connects progress-based shaping to a directly learnable quantity—temporal distance—which is readily available from videos.\n2. Simple idea to benefit from human demonstrations provided using videos"}, "weaknesses": {"value": "**Writing quality and clarity**\nThe paper’s presentation can be improved. Several sections, particularly the method description and experimental setup, lack clarity. The idea can be stated in a far simpler way than the current form. \n\n**Overstated empirical claims**\nThe paper overstates the efficacy of the proposed method. Reported improvements are not statistically significant in several plots, and standard errors overlap in almost all cases. The tone implies strong superior performance, but the evidence supports minimal gain at best. \n\n**Limited applicability beyond simple goal-reaching tasks**\nThe method appears well suited for short-horizon goal-reaching tasks  (go from point A to point B). It is unclear how it would perform in environments with loops, subgoals, or frequent revisits to the same state. Because the reward is based on framewise temporal distance, states with high visitation counts is likely to confuse the model and degrade performance.\n\n**Weak cross-domain evidence**\nThe claim of strong cross-domain generalization is not sufficiently supported. The real human video experiments appear tightly controlled: object positions and scene layouts closely match the simulation, with only the robot hand replaced by a human one. This suggests limited diversity and minimal domain shift. Additional experiments with different backgrounds, object appearances, and camera viewpoints would be needed to substantiate the cross-domain claim."}, "questions": {"value": "1. **Statistical significance of results:**\n   In most figures (for example, Figures 3, 5, 6b, and 7), the standard error bands of different methods overlap substantially. On what basis do you claim that TimeRewarder performs better? The results as shown do not appear statistically significant.\n\n2. **Generalization to out-of-distribution trajectories:**\n   How does TimeRewarder handle out-of-distribution data? With only a few demonstrations, it seems unlikely that the model can generalize to every possible trajectory. In Meta-World, what happens if the robot begins from a different initial configuration or follows a novel trajectory? How would this affect the predicted reward? Would this be a fundamental limitation of the approach?\n\n3. **Weighting of the success reward:**\n   Why is a separate weighting factor for the success reward needed in Equation 7? How sensitive is performance to the choice of $\\alpha$, and how difficult is it to tune in practice?\n\n4. **Choice of discretization bins:**\n   The two-hot discretization uses (K = 20) bins. How was this number selected, and how sensitive are the results to this value?\n\n5. **Transfer to real robots:**\n   All reported experiments appear to be conducted in simulation. How do you expect the method to extend to real-world robotic systems, given practical challenges such as lighting variation, occlusion, and partial observability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EfqevpsKQB", "forum": "BAwXV3Y0cc", "replyto": "BAwXV3Y0cc", "signatures": ["ICLR.cc/2026/Conference/Submission4695/Reviewer_X7My"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4695/Reviewer_X7My"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759610060, "cdate": 1761759610060, "tmdate": 1762917516816, "mdate": 1762917516816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}