{"id": "x5HDAixMaK", "number": 7199, "cdate": 1758011326099, "mdate": 1759897867045, "content": {"title": "Kron-LoRA: Hybrid Kronecker-LoRA Adapters for Scalable, Sustainable Fine-tuning", "abstract": "Fine-tuning massive pre-trained language models across many tasks demands adapters that are both parameter-efficient and expressive. We introduce Kron-LoRA, a hybrid adapter that combines Kronecker-structured factorization with low-rank LoRA compression—an integration that, to our knowledge, has not been explored in the literature on parameter-efficient fine-tuning or matrix approximation. Kron-LoRA achieves up to 4× fewer parameters than standard LoRA while retaining similar expressivity. Experiments on DistilBERT, Mistral-7B, LLaMA-2-7B, and LLaMA-3-8B across eight benchmarks show that Kron-LoRA matches or exceeds LoRA baselines with modest memory savings and only a 5–8% speed overhead. In sequential fine-tuning, it also delivers competitive cross-task transfer despite using only one-quarter of the adapter parameters. Kron-LoRA thus offers a scalable, sustainable solution for multi-task adaptation of large language models.", "tldr": "Kron-LoRA integrates Kronecker structure with low-rank LoRA, yielding up to 4× parameter reduction while preserving accuracy and enabling efficient multi-task and sequential fine-tuning of LLMs.", "keywords": ["Kron-LoRA", "Kronecker factorization", "LoRA", "parameter-efficient fine-tuning", "quantization-friendly adapters", "multi-task adaptation", "continual learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/02912bd536936f8648d37a2030a8206565196984.pdf", "supplementary_material": "/attachment/5a7d8ce30a4d029ec28bf964fa18a49f132df40a.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Kron-LoRA, a novel adapter design that combines Kronecker-product factorization with low-rank LoRA decomposition to achieve high parameter efficiency in fine-tuning large language models (LLMs).\nUnlike LoRA, which applies a pure low-rank update to frozen model weights, Kron-LoRA factorizes each task-specific update into a Kronecker structure, allowing structured compression and reuse of shared subspaces."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. First integration of Kronecker structure and LoRA.\nThe combination of Kronecker factorization with LoRA is original and well-motivated. Prior work explored Kronecker adapters (KronA, AdaKron, MoKA) or LoRA variants separately, but not their hybridization.\n\n2. Strong parameter-efficiency gains.\nThe method achieves up to 4× parameter reduction while maintaining similar or better accuracy compared to LoRA-8 across all backbones (Table 2–3). The analysis clearly shows that Kronecker structure provides multiplicative rank expansion, preserving expressivity even at low parameter budgets."}, "weaknesses": {"value": "1. Conceptual simplicity vs. depth.\nWhile novel, the combination of Kronecker and LoRA is a natural hybrid rather than a deeply theoretical contribution. The paper’s strength lies in practicality and empirical rigor, not fundamental mathematical insight.\n\n2. Limited ablation on Kronecker dimensions.\nThe choice of d seems heuristic. It’s unclear how sensitive results are to this partition, or whether learned Kronecker shapes could further improve results.\n\n3. Minor computational overhead.\nKron-LoRA incurs a 5–8% slowdown and only marginal memory savings (~0.8%). While acceptable, it suggests the method trades compute for parameter compactness.\n\n4. No comparison to other compression baselines.\nThe paper omits comparisons with AdaLoRA, DoRA, and PiSSA, which could clarify whether Kron-LoRA’s gains stem mainly from the Kronecker structure or rank allocation differences."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yUhnkierbC", "forum": "x5HDAixMaK", "replyto": "x5HDAixMaK", "signatures": ["ICLR.cc/2026/Conference/Submission7199/Reviewer_qn2e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7199/Reviewer_qn2e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760546924223, "cdate": 1760546924223, "tmdate": 1762919352087, "mdate": 1762919352087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce Kron-LoRA, a hybrid adapter that combines Kronecker-structured factorization with low-rank LoRA. Kron-LoRA requires up to 4x fewer parameters than LoRA and performs similarly at the price of 5-8% overhead.\n\nThe proposed method can be summarized as 1) compute the task-specific update as a Kronecker product between A and B, and 2) compress B via rank-k LoRA factorization. I would appreciate if the authors could compare the rank and #parameters of various PEFT methods (especially LoRA ones).\n\nIn terms of experiments, multiple models are used from DistilBERT, Mistral, and LLaMA-2/3. Datasets are the ones used in the original LoRA paper. Since GLUE-base tasks are old, I would encourage the authors to conduct additional experiments. Moreover, since one of the main motivation of the work is that LoRA becomes costly when supporting hundreds of tasks, I would expect the authors to conduct analysis with O(10) tasks.\n\nIn the first experiment, why authors are comparing LoRA-8 and KronA while the latter has 10x fewer parameters? It is not a surprised that the performance drop is 20%. Please conduct fairer analysis with LoRA being smaller. Table 2 and 3 do not address the issue and sometimes the gain of Kron-LoRA is very small. I couldn't find details on how tuning each algorithm has been done, hence I'm little bit skeptical of the results.\n\nRegarding the speed and memory experiments, please include all the models in the analysis, not only LLaMA-2-7B. I also suspect that larger models would indicate an overhead higher than 8%. Finally, the continual learning experiment is interesting. I would love to see all possible pairwise experiments with a given model instead of only ARC/HS.\n\nOverall, the idea is sound and the paper well written. However, I do feel that multiple experiments are missing and would help to understand what is happening. I would encourage the authors to use another dataset than GLUE tasks and do more analysis/experiments with including mean/std or CIs.\n\nNote:\n- L053 - ... they either forgo rank-r expressiveness or incur additional computational overhead --> like this paper (5-8% overhead)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Sound methodology\n- Very well written paper"}, "weaknesses": {"value": "- Weak experimental results: more recent datasets, more analysis and ablations."}, "questions": {"value": "Could you add ablations & additional experimentation of varying d_A1 and d_A2?\nCould you add a comparison of the rank and #parameters of various PEFT methods (especially LoRA ones)?\nCould you describe more in depth how do you tune each algorithm? Similarly, with such limited improvement, could add confidence intervals/stds?\nSee comments for additional questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hhERQXNUDi", "forum": "x5HDAixMaK", "replyto": "x5HDAixMaK", "signatures": ["ICLR.cc/2026/Conference/Submission7199/Reviewer_oJrL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7199/Reviewer_oJrL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761499233050, "cdate": 1761499233050, "tmdate": 1762919351758, "mdate": 1762919351758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes a method for parameter-efficient fine-tuning (PEFT) of LLMs. Existing PEFT methods include LoRA which decomposes the update weight matrices into two low rank matrices and KronA which uses Kronecker product of matrices for the weight update. The proposed method (Kron-LoRA) combines the two by replacing one of the matrices in KronA with its LoRA equivalent (decomposing it into two low rank matrices). Authors experiment on NLU tasks and show that Kron-LoRA performs comparably to LoRA while using significantly fewer parameters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The proposed idea is simple. The paper is mostly clearly written and easy to understand. In the experiments on the NLU tasks, the proposed method performs comparably to LoRA while employing close to $4\\times$ fewer parameters. The authors show that KronA alone uses significantly fewer parameters but also significantly underperforms LoRA."}, "weaknesses": {"value": "1. **Insufficient baselines:** The empirical analysis severely lacks important baselines and comparison with SOTA PEFT approaches and LoRA variants. The proposed method is a simple combination of KronA and LoRA and thus must include comparison with these approaches with a similar number of trainable parameters. However, comparison KronA with is provided on a single backbone model and with a setting that uses just 10% of parameters (2.3M for KronA vs 21.3M for LoRA). There are no comparisons except with LoRA on any other dataset / backbone LLM. It is necessary to additionally compare the work with SOTA approaches to help understand its benefits. Lack of any such comparisons makes it hard to evaluate the proposed method. \n2. **Lack of intuition/motivation:** There is no clear reasoning for why the matrix in KronA must be replaced with a low rank decomposition as in LoRA. Is the parameter count in KronA high enough that it requires low rank decomposition? In their work, authors of KronA show that it outperforms LoRA with similar parameter count. Is it not possible to adjust the dimensions of KronA to have a similar parameter count as Kron-LoRA here instead of using the low-rank decomposition? \n3. **Non-typical experimental setup:** The experimental set-up is not clear. Typically, for the commonsense reasoning benchmark (Hu et al., 2023), the training set of the 8 sub-tasks are combined and 1-3 epochs of fine-tuning is performed (e.g., refer Hu et al., 2023, Liu et al., 2024, Wang et al., 2025). However, in Kron-LoRA, it is not clear if fine-tuning is performed individually on each of the sub-tasks. Kron-LoRA also uses 30 epochs of fine-tuning, a lower batch size of 8 and no warmup. It is not clear why this deviation from typical evaluation is needed."}, "questions": {"value": "1. Compare proposed method with both KronA and LoRA at *similar* parameter count. \n2. Provide an explanation for why further parameter count in KronA is necessary and why the proposed Kron-LoRA is a good way to achieve it. \n3. Provide discussion and comparisons with SOTA PEFT methods (e.g. DoRA (Liu eta al., 2024, MiLoRA (Wang et al., 2025), MOA [a], LoRA+ [b]) and methods that focus on significant parameter count reduction compared to LoRA (AdaLoRA (Zhang et al., 2023), DyLoRA (Valipour et al., 2022), VeRA [c], LoRA-XS [d], NOLA [e], Tiered-LoRA [f], BS-LoRA [g]). \n4. Provide details on the experimental setup and reasoning for differences with typical setup (see weakness point 3 above).\n\n**References** \\\n[a] Cao, Jie, Tianwei Lin, Hongyang He, et al. “MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models.” arXiv preprint arXiv:2506.05928, 2025. \\\n[b] Hayou, Soufiane, Nikhil Ghosh, and Bin Yu. “LoRA+: Efficient Low Rank Adaptation of Large Models.” Proceedings of the 41st International Conference on Learning Representations (ICLR), 2024. PMLR 235: 17783-17806 \\\n[c] Kopiczko, Damian J., et al. “VeRA: Vector-Based Random Matrix Adaptation.” arXiv preprint arXiv:2310.11454, 2023. \\\n[d] Bałazy, Klaudia, et al. “LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters.” ECAI 2025 \\\n[e] Abbasi Koohpayegani, Soroush, et al. “NOLA: Compressing LoRA Using Linear Combination of Random Basis.” International Conference on Learning Representations (ICLR), 2024 \\\n[f] Renduchintala, Ashish, et al. “Enhancing Parameter Efficiency of LoRA with Weight Tying.” Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Long Papers (NAACL), 2024. ACL Anthology \\\n[g] Zhou, Yuhua, et al. “BSLoRA: Enhancing the Parameter Efficiency of LoRA with Intra-Layer and Inter-Layer Sharing.” International Conference on Machine Learning (ICML), 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u1tr1BShOA", "forum": "x5HDAixMaK", "replyto": "x5HDAixMaK", "signatures": ["ICLR.cc/2026/Conference/Submission7199/Reviewer_sZ8f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7199/Reviewer_sZ8f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7199/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762104266271, "cdate": 1762104266271, "tmdate": 1762919351327, "mdate": 1762919351327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}