{"id": "ETzntM8ISs", "number": 7903, "cdate": 1758042179115, "mdate": 1759897823377, "content": {"title": "Reasoning-Intensive Regression", "abstract": "AI researchers and practitioners increasingly apply large language models (LLMs) to what we call reasoning-intensive regression (RiR), i.e., deducing subtle numerical scores from text. Unlike standard language regression tasks, e.g., for sentiment or similarity, RiR often appears instead in ad-hoc problems such as rubric-based scoring, modeling dense rewards in complex environments, or domain-specific retrieval, where much deeper analysis of context is required while only limited task-specific training data and computation are available. We cast four realistic problems as RiR tasks to establish an initial benchmark, and use that to test our hypothesis that prompting frozen LLMs and finetuning Transformer encoders via gradient descent will both often struggle in RiR. We then propose MENTAT, a simple and lightweight method that combines batch-reflective prompt optimization with neural ensemble learning.  MENTAT achieves up to 65% improvement over both baselines, though substantial room remains for future advances in RiR.", "tldr": "", "keywords": ["reasoning", "regression"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11b49ae631c4e18b73297fe9eb99f6fe66c65e07.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes “Reasoning-Intensive Regression (RiR)”,  a subset of text-to-score problems that purportedly require numerical reasoning rather than shallow feature extraction. It classifies four different tasks such as math error detection, instruction following, pairwise RAG comparison, and essay grading into continuous targets to form a benchmark, arguing that standard LLM prompting and small-encoder fine-tuning both struggle. The authors also introduce MENTAT, combining batched prompt evolution with a small neural aggregator over multiple rollouts, and advocate CCC (concordance correlation coefficient) alongside NMSE to avoid variance-collapse pathologies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies a failure mode of distance-only metrics (NMSE) in regression based reasoning tasks and motivates CCC to capture agreement\n\n- Presents a simple, lightweight method (MENTAT) that is shown to improve over frozen prompting and a finetuned encoder on the proposed tasks in the evaluation\n\n- Provides an initial, multi-task benchmark with a clear claim that some real-world scoring problems need deeper analysis than typical sentiment/similarity regression"}, "weaknesses": {"value": "- Conceptual motivation for “RiR” and the three-level taxonomy is not well motivatied. RiR is described as “fuzzy,” and the levels are explicitly “informal,” with no clear definition on what tasks could be classified this way. This needs to be strengthened further\n\n- Task taxonomy and task definitions for levels is under-justified. Several tasks (e.g., math) are converted into regression by design (predict fraction until first error), which evaluates a surrogate metric rather than native task success. The authors don’t justify why this surrogate is decision-relevant, nor report native metrics (e.g., accuracy, exact match) alongside the other metrics they report. I would love to hear the author side argument on this.\n\n- MENTAT novelty is unclear relative to prior prompt-optimization. The “batch-reflect” prompt evolution over worst-case subsets looks close in spirit to existing prompt-evolution methods; the draft does not provide a thorough, controlled comparison [1] .\n\n\n- Aggregator/loss choice lacks justification. The order-invariant MLP over rollout stats and the training objective (emphasizing CCC/NMSE) are presented without ablations on alternative aggregators (non-parametric, rank-based) or loss functions.\n\n\n- It is unclear what role does NeoBERT play and whether it is fair comparison in the light of the experiments. The paper mixes a finetuned encoder (NeoBERT) with frozen LLMs; this crosses regimes. Matched-budget baselines (e.g., LoRA-tuned small LLMs, stronger encoders) are missing, so the relative claims are hard to interpret.\n\n[1] prompterator: Iterate efficiently towards more effective prompts. Sucik et al, 2023"}, "questions": {"value": "Task casting: For math and pairwise-RAG, why is the regression surrogate (e.g., fraction until first wrong step) the right target?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VUxcucIQDn", "forum": "ETzntM8ISs", "replyto": "ETzntM8ISs", "signatures": ["ICLR.cc/2026/Conference/Submission7903/Reviewer_2fyW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7903/Reviewer_2fyW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973203365, "cdate": 1761973203365, "tmdate": 1762919931708, "mdate": 1762919931708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of reasoning-intensive regression, which requires the LLMs to deduce subtle numerical scores from text. The authors proposes a prompt optimization paradigm by LLM self reflection and showcase the improvement compared to baseline head fine-tuning and prompt hand-crafted fine-tuning methods."}, "soundness": {"value": 4}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The study of reasoning-intensive regression of LLMs is indeed of crucial importance and there lacks work that seriously investigate the limitation of existing LLMs in use cases like LLM as a judge. The direction that goes beyond hand crafted prompt tuning is interesting and appealing."}, "weaknesses": {"value": "1. The benchmarking tasks in Section 2 is rather confusing. Why we choose these tasks as benchmarks for reasoning-intensive regression? There lacks sufficient discussion on the coverage of these tasks and what is the performance of existing either open source or closed source LLMs.\n\n2. The proposed method MENTAT requires additional training and inference burden, there lacks a clear description of the motivation behind this method. In addition, there lacks explanation of what types of data the additional MLP is trained on. What is the input and output of the MLPs? What is the scale of this additional training part upon the original LLMs? What is overall the additional inference cost of current method compared to the standard fixed template of existing LLMs or few shot templates?\n\n3. Disclaimer: I am not familiar with this field of reasoning-intensive regression. But I do think there is huge space to improve on the writing side to make the paper readable for broader audience. I have read many papers, including those in the area I am not familiar with, and this is really one of the few times I found a paper's wordings and technical descriptions make me feel super puzzled almost throughout the paper, making it extremely hard to follow. I would highly recommend adding more relevant equations to explain the tasks and metrics more detailedly, instead of using long sentences and only word descriptions of the methodology. Overall, I do not think this work does a good job in explaining clearly the method proposed."}, "questions": {"value": "See questions in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "750UJgnGxL", "forum": "ETzntM8ISs", "replyto": "ETzntM8ISs", "signatures": ["ICLR.cc/2026/Conference/Submission7903/Reviewer_WmpZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7903/Reviewer_WmpZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045625601, "cdate": 1762045625601, "tmdate": 1762919931362, "mdate": 1762919931362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Reasoning-Intensive Regression (RiR), an interesting category of tasks where models performs detailed reasoning and output precise numeric values. It presents a benchmark of tasks such as math error detection, essay grading, showing that standard fine-tuning often collapses to mean predictions, while prompting large models yields reasoning but poor numeric calibration. To address this, the authors propose MENTAT, a lightweight method that first optimizes prompts, then does multiple rollouts, get regression outputs from them, and combine them using an external MLP. The paper is well written however, at times it felt overly verbose. The methodology itself is sound, but noveltywise, it felt like a mixture of prompt optimization and self consistency. More importantly, based on the framing of the task, the paper misses important baselines to compare against to showcase the efficacy of MENTAT."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces Reasoning-Intensive Regression, which is a timely task to investigate into when LLMs are sufficiently powerful to finish important tasks. \n2. The four different benchmark datasets are quite interesting to be framed for reasoning-intensive regression, although I believe the task choices could be even more well thought out."}, "weaknesses": {"value": "1. MENTAT seems like using self-consistency on top of prompt optimization. I am not sure whether about the contribution in that aspect, as it is well known that both of them should substantially boost performance.\n2. \"optimizing prompts for RiR tasks has the fairly unique property that the patterns across examples are at least as important as the per-example error.\" - I believe this is a general theme of most automated prompt optimization methods. More importantly, authors claim that \"MENTAT’s prompt evolver is centered around asking the LLM to jointly reason about tens of mistakes at once\". I am not sure it is accurate given methods like APO/ PE2 / TextGrad has already covered those areas. \n\n3. I think the authors need to cover more baeslines here. Modern Prompt optimization methods should definitely be covered as baselines to prove the efficacy of the suggested approach. Also, the original self consistency can be a good baseline to compare against.\n\n[1] Pryzant R, Iter D, Li J, Lee YT, Zhu C, Zeng M. Automatic prompt optimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495. 2023 May 4.\n[2] Ye Q, Axmed M, Pryzant R, Khani F. Prompt engineering a prompt engineer. arXiv preprint arXiv:2311.05661. 2023 Nov 9."}, "questions": {"value": "Please check the weaknesses. The claims need to be substantiated and coverage of prompt optimization methods also needs to be improved, although I understand that is not the primary focus of the problem itself."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ug9BT2tssa", "forum": "ETzntM8ISs", "replyto": "ETzntM8ISs", "signatures": ["ICLR.cc/2026/Conference/Submission7903/Reviewer_i7ig"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7903/Reviewer_i7ig"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142434913, "cdate": 1762142434913, "tmdate": 1762919931044, "mdate": 1762919931044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MENTAT, a streamlined framework to improve the LLM's performance on reasoning-intensive regression (RiR) tasks. This is a two stage process, starting with auto prompt optimization, followed by ensemble learning. It only involes inference on frozen LLMs, and does not require large training datasets, making it lightweight and applicable in practical real world settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The MENTAT framework automates the prompt engineering process, which can be very useful, especially in complex scenarios like RiR, where traditionally requires extensive amount of human involvement.\n- Ensemble learning is an interesting way to take advantage of the probabilistic nature of LLMs, and increase the regression quality."}, "weaknesses": {"value": "- When introducing MENTAT in Sec.3, it lacks important details, specifically authors should explain\n  - Phase 1\n    - How to select the bottom $\\sqrt{n}$ rollouts in the training set. Are the rollouts ranked by a combination of CCC/NMSE? If so, is the ratio constant, and is the ratio the same as the one used for MLP training?\n    - How are the prompts structured in `error analysis` and `prompt refinement`? Does the setup vary between models and/or tasks? Should include all the prompts used in the Appendix.\n    - Consider include the conversation of a full iteration. This can be added to the Appendix.\n- In Sec.4, results lack ablation study and analysis of the MENTAT framework\n  - The performance gap between different veresions of MENTAT makes me wonder if the success of MENTAT is primarily driven by the aggregation of the LLM's probabilistic rollout or is it due to a better prompt generated from the automated process. Authors should include ablation study wrt basic/detailed human crafted prompts (HCP), specifically `HCP Avg` and `HCP MLP` in the results, and compare with their MENTAT counter parts to better analyse the framework. For HCP MLP, training should use the same amount of data and HPT as in MENTAT Phase 2.\n  - Authors should add a figure for the distribution of `{Var(all rollouts for question i)}` for each task with HCP and MENTAT Prompt. This should help analyse if aggregation is the key in obtaining a better regression performance.\n  - To help understand the effect of prompt optimization, authours can include a `NMSE/CCC vs Iteration` plot in the training process, consider increasing the total iterations for long term analysis. As stated in Appendix.C, MLP is very lightweight, and it should be feasible to include plots of both with and without training the aggregator at each iteration.\n  - Add variance to Table.1,2 if space allows. If not, this can be included in the Appendix.\n- In Appendix.F, include all the LLM discovered prompts for entries that appear in Table.1,2"}, "questions": {"value": "- In Sec.2, regarding the dataset and their evalatuion metrics,\n  - Detecting Mathematical Errors, why filter out problems with correct solutions that existed in the original dataset? I think this should be included to truly reflect the real world setting.\n  - Instruction Following, original paper used binary label for each requirement $r_i$, and labels are averaged to produce the final score. Why change to $s_i \\in [0,1]$ for each $r_i$, and adopt harmonic mean?\n  - Pairwise RAG Comparison, why not use the evaluation metric from the original paper, namely RAG-QA Arena?\n- How much data were used to train MLP?\n- Table.1 mentioned that NeoBERT performs better with \"1000 training + 500 validation\", why not add an additional column to the results?\n- Why not use a universal evaluation model (potentially more powerful than the test model) in error analysis and prompt refinement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M2FdhCEJfE", "forum": "ETzntM8ISs", "replyto": "ETzntM8ISs", "signatures": ["ICLR.cc/2026/Conference/Submission7903/Reviewer_yiZN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7903/Reviewer_yiZN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762160604569, "cdate": 1762160604569, "tmdate": 1762919930531, "mdate": 1762919930531, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the concept of reasoning-intensive regression, a type of language regression task that requires intensive reasoning from an LLM. To benchmark it, the author designed a benchmark with 4 tasks, spanning different levels of required reasoning capability. Lastly, the author designed a combined evolution and neural aggregation method to better solve this benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written, the benchmark and the methodology are well-presented."}, "weaknesses": {"value": "* While presenting a new benchmark for RIR, the paper lacks an error analysis addressing the current models’ or methods’ pitfalls on the benchmark.\n* Additionally, regarding the different levels of regression tasks and their corresponding benchmark designs, it would be better to include some quantitative or qualitative comparisons showing the differences between level-3 reasoning-intensive tasks and levels 2 and 1 (e.g., average CoT length, confidence, self-consistency, etc.).\n* For the experiment setup with Mentat, I feel that a very important practical question is how much data is allocated to train or supervise the method (in phase 1 and phase 2). Data efficiency is crucial, especially since collecting regression data is costly. Therefore, a data-point efficiency experiment is needed (beyond the two options of 100 and 500 budget).\n* I’m also curious — in Table 1, how would the method compare to few-shot learning with carefully selected few-shot examples? This seems to be an important baseline as well."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V4kEgu2fKL", "forum": "ETzntM8ISs", "replyto": "ETzntM8ISs", "signatures": ["ICLR.cc/2026/Conference/Submission7903/Reviewer_pSw3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7903/Reviewer_pSw3"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762247067570, "cdate": 1762247067570, "tmdate": 1762919929974, "mdate": 1762919929974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper defines a new task family called Reasoning-Intensive Regression (RiR)—language-based regression problems that require explicit multi-step reasoning instead of shallow feature extraction. The authors argue that standard encoder fine-tuning collapses to narrow score ranges, while directly prompting frozen LLMs yields coarse, discretized outputs.\nThis paper establish an initial benchmark of four diverse RiR tasks (mathematical-error detection, instruction-following quality, pairwise RAG comparison, and essay grading) and propose MENTAT, a lightweight two-phase method combining batch-reflective prompt optimization with neural ensemble aggregation.\nAcross all tasks, MENTAT improves Concordance Correlation Coefficient (CCC) over both encoder and prompting baselines, showing that simple prompt evolution plus multi-rollout calibration can recover both reasoning depth and numeric precision."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed Reasoning-Intensive Regression (RiR) benchmark captures an emerging but under-explored problem space—tasks requiring both deep reasoning and precise numeric scoring (e.g., reward modeling, rubric-based evaluation). This formulation is well-motivated and relevant to ongoing LLM-judging and alignment research.\n2. The authors convincingly argue that normalized MSE can be misleading due to output collapse and advocate using the Concordance Correlation Coefficient (CCC), which jointly considers variance, mean alignment, and correlation. This insight is both intuitive and empirically supported, representing a meaningful contribution to LLM evaluation methodology.\n3. MENTAT’s two-phase structure—batch-reflective prompt optimization followed by neural aggregation—is simple, effective, and closely mirrors how humans iteratively refine prompts and aggregate judgments. It provides measurable gains while remaining lightweight and reproducible across models."}, "weaknesses": {"value": "1. The mathematical RiR task relies on ProcessBench, which explicitly annotates the first erroneous reasoning step. While this allows continuous regression scoring, such step-level annotations are rare in most math datasets, limiting scalability and generalizability.\n2. Instruction-Following uses gpt-oss-20b (an open-source LLM) while other tasks use GPT-5 or GPT-4.1. The authors briefly claim this is “for reproducibility and generalization validation,” but no comparison is given to show GPT-5 behaves similarly on that task. \n3. Although the paper claims MENTAT is lightweight, it involves multiple LLM inference rounds and K-fold rollouts per sample. The actual compute cost, token budget, or latency trade-off is not quantified. Moreover, comparisons to recent prompt-optimization or ensemble calibration methods are absent, leaving uncertainty about relative efficiency."}, "questions": {"value": "1. Your prompt evolution stage always selects approximately √n of the worst samples for reflection. Have you experimented with smaller or larger batches, or grouping by error type rather than just score percentile?\n2. Could the MLP aggregation in Phase 2 be replaced by a non-trainable closed-form rule (e.g., median plus variance correction)? If possible, please provide a comparison table between simple aggregation baselines (average, median, trimmed mean) and the learned MLP."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fzu3fjiOvP", "forum": "ETzntM8ISs", "replyto": "ETzntM8ISs", "signatures": ["ICLR.cc/2026/Conference/Submission7903/Reviewer_MtER"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7903/Reviewer_MtER"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission7903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762257820728, "cdate": 1762257820728, "tmdate": 1762919929581, "mdate": 1762919929581, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}