{"id": "bq8HqoYLQK", "number": 6790, "cdate": 1757995861313, "mdate": 1759897893811, "content": {"title": "Visual Prompting with Iterative Refinement for Design Critique Generation", "abstract": "Feedback is essential in all design processes, such as user interface (UI) design. Automating design critiques can significantly enhance design workflow efficiency. Although existing vision language models (VLMs) excel in many tasks, they often struggle with generating high-quality design critiques---a complex task that requires producing detailed design comments that are visually grounded in a given design's image. Building on recent advancements in iterative refinement of text output and visual prompting methods, we propose a multimodal iterative refinement and visual prompting framework for UI critique that takes an input UI screenshot and design guidelines and generates a list of design comments, along with corresponding bounding boxes that map each comment to a specific region in the screenshot. The entire process is driven completely by VLMs, which iteratively refine both the text output and bounding boxes (in a mutually conditioned manner), using few-shot samples tailored for each step. We evaluated our approach using Gemini-1.5-pro and GPT-4o, and found that human experts generally preferred the design critiques generated by our pipeline over those by the baseline, with the pipeline reducing the gap from human performance by 50\\% for one rating metric. To assess its generalizability to other multimodal tasks, we applied our pipeline to open-vocabulary object and attribute detection, and experiments showed that our method also outperformed the baseline.", "tldr": "We propose a VLM-based framework that iteratively refines text and visual grounding to produce grounded user interface design critiques, outperforming baselines and generalizing to other multimodal tasks.", "keywords": ["User Interface Design Critique", "Multimodal LLM", "Visual Grounding", "Prompting Techniques", "VLM"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07493b792e32a9ff5fb78305c774f694759efde4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a new system for automating design critiques, primarily targeting user interface design.  The method combines visual prompting and iterative refinement to create a design critique visually grounded by bounding boxes around problematic UI elements.  \n\nThe method does not require additional training and is evaluated with two frontier VLMs, Gemini-1.5-pro and GPT-4o.   Quantitative results show improvements over a baseline method (Duan et al.) using a human evaluation.  Ablation studies validate each step of the proposed pipeline.  In addition, the iterative refinement of the bounding box prediction sub-task is shown to generalize to an open vocabulary object detection benchmark.  \n\nExtensive qualitative results in the appendix show the performance of the technique and include comparisons with the baseline method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method is sound and coherent.  Each step in the pipeline makes good sense and ablation studies also validate these gains.  \n\nA human evaluation of the design critiques is performed.  This is currently the best way measure the performance for functionality of this kind.\n\nThe paper shows improved performance over baseline methods both qualitatively and quantitatively.  \n\nThe proposed method does not require fine tuning and consequently can capitalize on improvements in frontier VLMs, which is a rapidly advancing technology."}, "weaknesses": {"value": "While this is a solid piece of work, the gains are made by relatively obvious extensions of existing techniques.  For example, extending  iterative refinement [Madaan et al. (2023) and Xu et al. (2024a)] to bounding box prediction.\n\nIt’s a little hard to tell how much the gains in results would really assist a UI designer.  The quantitative gains appear relatively small, although it's hard to assess the scale of the numbers.  The qualitative comparisons with the baseline method in the appendix were helpful, but without seeing a large number of such comparisons picked at random it’s very hard to know if the performance gain would be noticeable. \n\nThe system is passed a list of “Design Guidelines” in addition to an image of the UI.   It's not clear whether these have any impact on the output.  I didn't see any evaluation of this.\n\nThe entire system is relatively complex.   These days I become nervous that the next generation of VLMs will be able to get comparable performance from a single prompt without the additional complexity."}, "questions": {"value": "What does \\subscript{tn} stand for in the table column headings?   Is it just an abbreviation for -1.5-pro and -4o?  \n\nThe cost analysis in appendix A6 didn’t give me any feeling for how much more expensive the pipeline would be to run over the Duan et al. baseline method.   I would be interested to understand if the method was more expensive to run.   I see you make 6 or 7 VLM/LLM calls, but I'm not sure how many input and output tokens were used and how that compares to the baseline.  \n\nIn Figure 1, the \"Design Guidelines\" section is to small too read without extreme zoom.   The output \"Design comments\" in the figure do not align with anything I could see in the “Design Guidelines”.   Do these guidelines have any impact at all?  I didn't see any metrics for this.  What happens if you add a very striking guideline like \"My brand requires all text in the app to be in glowing neon purple\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TojtWwsiki", "forum": "bq8HqoYLQK", "replyto": "bq8HqoYLQK", "signatures": ["ICLR.cc/2026/Conference/Submission6790/Reviewer_y32e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6790/Reviewer_y32e"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760538736469, "cdate": 1760538736469, "tmdate": 1762919064681, "mdate": 1762919064681, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a prompt-based VLM pipeline for automated UI design critique generation without model training, using iterative refinement and visual prompting. ​ It improves over the baseline in comment quality and bounding box accuracy for the UI critique task and generalizes to image object grounding tasks, outperforming the baseline in visual grounding accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- A VLM pipeline improves single VLM for UI design judgement.\n- Both visual and textural validation modules improve accuracy.\n- Shows generalization to other visual grounding tasks."}, "weaknesses": {"value": "- The method and idea are simple, with limited novelty. No model training is done. No new dataset is involved.\n- The method is not tailored much to the target problem, except for a few in context samples used in prompts.\n- The critique results is still far from human expert."}, "questions": {"value": "The authors are encouraged to study more UI design principle themselves to better understand the challenges and have more specialized design for the problem.\nNeed higher quality human rating to judge the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RNa5XkT8ii", "forum": "bq8HqoYLQK", "replyto": "bq8HqoYLQK", "signatures": ["ICLR.cc/2026/Conference/Submission6790/Reviewer_yrbX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6790/Reviewer_yrbX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111353905, "cdate": 1762111353905, "tmdate": 1762919064170, "mdate": 1762919064170, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-stage pipeline for critique generation in UI design feedback. The method decomposes the task into multiple visual-language model (VLM) modules that iteratively generate and refine textual critiques and their corresponding bounding boxes (bboxes) grounded on UI screenshots. The proposed system includes text generation, text filtering, bbox generation and refinement, validation, and text refinement steps, each handled by separate VLMs to avoid self-bias.\n\nThe paper introduces a new dataset, UICrit, containing ~11K human-annotated critiques and bounding boxes. Evaluation combines IoU for visual grounding, comment similarity using sentence-BERT embeddings, and human expert evaluation of critique validity and set ranking. Results show incremental improvements over a previous pipeline baseline (Duan et al., 2024a), and the authors also demonstrate limited transfer to OVAD/OVD tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**1. Novel Task Formulation**\n\nThe paper clearly defines a spatially grounded critique generation task, integrating both text feedback and visual localization.\n\n**2. Systematic Pipeline Design**\n\nModular decomposition (generation, filtering, refinement) and VLM-based iterative feedback is conceptually clean and potentially extensible.\n\n**3. Dataset Contribution**\n\nThe UICrit dataset, with paired text–bbox annotations, could be useful for future multimodal critique or visual reasoning research."}, "weaknesses": {"value": "**1. Limited Empirical Depth**\n\nExperiments are restricted to a small set of baselines and models (Gemini-1.5-Pro, GPT-4o).\nThe ablations are shallow; there is no analysis of failure cases, generalization across domains, or robustness.\n\n**2. Marginal Quantitative Gains**\n\nAlthough IoU and similarity scores increase slightly with each module, absolute performance remains low (e.g., IoU < 0.36).\nHuman evaluation improvements are modest and may not be statistically significant.\n\n**3. Overly Complex but Technically Shallow**\n\nDespite multiple modules, each stage is implemented via prompting rather than genuine model innovation.\nThe approach is more of a prompt engineering pipeline than a novel algorithmic contribution.\n\n**4. Evaluation Ambiguity**\n\nComment similarity (cosine on sentence embeddings) is not a reliable proxy for critique quality.\nHuman evaluation lacks sufficient detail: number of annotators, inter-rater reliability, and consistency are missing."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cKqZXSsS6Z", "forum": "bq8HqoYLQK", "replyto": "bq8HqoYLQK", "signatures": ["ICLR.cc/2026/Conference/Submission6790/Reviewer_xXTv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6790/Reviewer_xXTv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6790/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762165786978, "cdate": 1762165786978, "tmdate": 1762919063711, "mdate": 1762919063711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}