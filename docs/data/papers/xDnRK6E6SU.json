{"id": "xDnRK6E6SU", "number": 23406, "cdate": 1758343293729, "mdate": 1759896816582, "content": {"title": "Inference-time Correction of Errors in AI-Generated Chest X-ray Radiology Reports", "abstract": "Automated radiology report generators are being increasingly explored in clinical workflow pilots, particularly for chest X-ray imaging. However, their factual correctness with respect to the description of the findings has often been less than accurate, making their adoption slow and requiring detailed verification by clinical experts. In this paper, we propose an automatic report correction method that uses both image and textual information in automated radiology reports to spot identity and location errors in findings through fact-checking models. Prompts for a pre-trained large language model are then generated from the analysis of these errors to produce corrected sentences by selectively modifying target findings described in the automated report sentences.  We show that this method of report correction, on the average, improves the report quality between 17-30% across various SOTA report generators over multi-institutional chest X-ray datasets.", "tldr": "", "keywords": ["Fact-checking", "regressor models", "Chest X-ray", "radiology reports"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3e0496107890cb38c20855329e216c0bbd3c1ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel inference-time report correction framework that improves the factual accuracy of AI-generated chest X-ray radiology reports. The core idea is to use a fact-checking model (FC) to identify incorrect or hallucinated findings in automatically generated reports and then correct those findings using a prompt-based LLM approach."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The work addresses a critical issue: factual errors in AI-generated radiology reports, which hinder real-world adoption.\n2. Unlike many prior methods focused on improving the report generator itself, this method applies a non-invasive correction layer at inference time. This makes it usable with existing, deployed models—a highly pragmatic solution.\n3. The evaluation is fairly comprehensive. The method wasn't just tested on one model. It was validated against 7 different SOTA report generators across  several public, multi-institutional datasets (e.g., ChestImaGenome, MS-CXR, VinDr-CXR)"}, "weaknesses": {"value": "1. The system can only correct findings already mentioned in the generated report. It cannot add new findings that the original generator omitted entirely.\n2. The method does not address errors in severity (e.g., \"mild\" vs. \"severe\" pneumonia) or measurements. It is primarily focused on the presence, absence, and location of core findings\n3. The entire pipeline depends on the accuracy of the initial FFL (finding extraction) and anatomical localization algorithms. An error in one of these upstream steps could cause the FC model to check the wrong thing, leading to an incorrect prompt and a flawed correction.\n4. The system relies on bounding boxes for localization. While standard, bounding boxes are an approximation. Segmentation mask here can be a good option.\n5. The entire system is built on extracting findings into the FFL (fine-grained finding patterns) format. While the paper cites high accuracy for this extraction , this format may be less expressive than other structured representations, like the entity-relation graphs produced by RadGraph. It's unclear if the FFL pattern can capture complex relationships between findings, which could be a source of error that this system might miss. Similar approach has been adopeted in [1] for reference using RadGraph:\n[1] Semantic Consistency-Based Uncertainty Quantification for Factuality in Radiology Report Generation. NAACL 2025.\n6. The 5 prompt templates used to guide the LLM are simple (e.g., \"Remove 'X'\"). This might be insufficient for very complex sentences where such an edit could break the grammatical flow. \n7. There are mismatch in tenses. Some sentence is written is past vs some are written in present tense. Also, there are minor typos in the paper (e.g, would-> wold)"}, "questions": {"value": "1. Could the authors elaborate on the technical challenges of adding an \"omission detection and insertion\" module? For instance, could the FC model be used to check for a set of expected findings (beyond just those mentioned) and identify high-confidence omissions? How might the prompt generation (Table 2)  be adapted to insert a completely new finding or sentence, rather than just edit an existing one?\n\n2. The paper notes that errors in severity (e.g., \"mild\" vs. \"severe\") and measurements are out of scope. : Is this a fundamental limitation of the chosen FFL (fine-grained finding pattern) representation, which seems to focus on the \"core finding name\" and \"laterality\"? Or is it a limitation of the FC model's training?\n\n3. Was the choice of bounding boxes purely for convenience (e.g., availability of datasets like ChestImaGenome ), or are there architectural reasons?\n\n4. While the examples in Table 4 are effective, complex radiology sentences can be brittle. Have the authors analyzed the failure rate of this specific LLM correction step? For instance, in what percentage of cases does the LLM, when given a simple edit prompt, produce a corrected sentence that is grammatically incorrect or semantically nonsensical, even if it successfully executes the requested edit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "13SMXWC4W8", "forum": "xDnRK6E6SU", "replyto": "xDnRK6E6SU", "signatures": ["ICLR.cc/2026/Conference/Submission23406/Reviewer_oJgk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23406/Reviewer_oJgk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862002840, "cdate": 1761862002840, "tmdate": 1762942648156, "mdate": 1762942648156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an inference-time method to improve the accuracy of AI-generated chest X-ray reports by detecting and correcting factual and location errors without retraining the original model. The proposed method first parses the generated report into structured fine-grained clinical findings and then builds a large synthetic dataset to model realistic findings and locations. A multimodal fact-checking (FC) network evaluates whether each reported finding is supported by the image and whether its anatomical location is consistent. When discrepancies are detected with high confidence, the system applies rule-based prompts to a lightweight language model to make minimal and focused edits to the original sentences, rather than rewriting the full report. This correction strategy reduces the risk of introducing new errors and preserves the valid content generated by the original model. Experiments across multiple datasets and different radiology report generators demonstrate consistent improvements in clinical correctness and grounding metrics."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The proposed model works at inference time, which requires no retraining or modification of the original report-generation model. \n\n* The multimodal fact-checking model checks whether each reported finding is present in the image and whether its stated anatomical location is correct. \n\n* Generates a large synthetic dataset that mimics realistic reporting mistakes, enabling robust training of the correction model. \n\n* Shows reliable performance gains across different datasets and a variety of report-generation systems."}, "weaknesses": {"value": "* The authors claimed their method was novel which is not true. The method closely follows the architecture and pipeline introduced in the MICCAI 2025 phrase-grounded fact-checking paper (https://papers.miccai.org/miccai-2025/paper/3526_paper.pdf ). The system structure and schematic design appear highly similar, which raises questions about the level of architectural novelty. The MICCAI paper has not been cited and discussed. \n\n* Figure 3 appears taken from the MICCAI 2025 framework, suggesting that the main contribution was already published before this submission. Therefore, this paper doesn't provide any substantial insights into the methodological direction. \n\n* The primary difference from the MICCAI work seems to be the addition of an LLM-based correction module driven by the fact-checking output. While this is useful, it may be viewed as a minor extension of prior work rather than a fundamentally new method. \n\n* There's no radiologist or human evaluation performed, making it difficult to judge the true clinical relevance of the corrections and how meaningful the improvements are in practice."}, "questions": {"value": "* Can the authors clearly articulate the conceptual and architectural differences between your framework and the MICCAI 2025 phrase-grounded fact-checking system? Specifically, what new capabilities or insights are introduced beyond adding an LLM-based correction module? \n\n* Can the authors report statistics on how often the LLM is triggered and the distribution of correction types? Which correction types helped more to improve the performance? \n\n* Have the authors considered including radiologists' or clinicians' review to assess the clinical meaningfulness of corrections?"}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "The authors claimed their method was novel which is not true. The method closely follows the architecture and pipeline introduced in the MICCAI 2025 phrase-grounded fact-checking paper (https://papers.miccai.org/miccai-2025/paper/3526_paper.pdf ). The system structure and schematic design appear highly similar, which raises questions about the level of architectural novelty. The MICCAI paper has not been cited and discussed. Figure 3 appears taken from the MICCAI 2025 framework, suggesting that the main contribution was already published before this submission."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nQNAlozrQl", "forum": "xDnRK6E6SU", "replyto": "xDnRK6E6SU", "signatures": ["ICLR.cc/2026/Conference/Submission23406/Reviewer_746C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23406/Reviewer_746C"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901611831, "cdate": 1761901611831, "tmdate": 1762942647946, "mdate": 1762942647946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an automatic report correction pipeline which utilizes a pattern matching and a localization algorithm on findings. A trained fact-checking model verifies errors and an LLM is used to rewrite the report after the error analysis."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The pipeline shows metric improvements which highlights the weaknesses of the current automated report generators."}, "weaknesses": {"value": "- The writing is very difficult to follow.\n- The proposed pipeline looks engineered, composing of many components that makes sense but not fully justified. It looks hard to generalize the findings."}, "questions": {"value": "- The pipeline seems to rely on many components being good. An ablation study could show how critical each component is and whether the pipeline can be simplified."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-"}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hl1e2JZ65Y", "forum": "xDnRK6E6SU", "replyto": "xDnRK6E6SU", "signatures": ["ICLR.cc/2026/Conference/Submission23406/Reviewer_4eQq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23406/Reviewer_4eQq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970322020, "cdate": 1761970322020, "tmdate": 1762942647699, "mdate": 1762942647699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an inference-time correction framework for automatically generated chest X-ray radiology reports. The proposed method combines a fact-checking model with a LLM that performs selective sentence-level corrections based on FC-guided prompts. The approach identifies factual and spatial inconsistencies between generated reports and image evidence, and modifies only erroneous findings instead of rewriting entire sentences. Experiments show consistent quality improvements of 17–30% in report-ground-truth similarity metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-inference-time correction combining vision–language verification with LLM-driven editing is pretty neat\n- Well-designed experiments across multiple datasets and baselines\n- Strong motivation, transparent architecture and dataset description"}, "weaknesses": {"value": "- The system handles only presence/absence and localization errors, omitting severity, measurement, and omission (missing-finding) errors \n- training on synthetic error patterns may not fully capture real generator error distributions. Validation on more realistic or human-annotated error sets would strengthen claims.\n- No statistical significance reported, some results (BLEU/SBERT) may have limited clinical interpretability\n- I feel like there are more and better metrics that you could compare to? what about the standard clinical metrics? Things like SRRBert, F1Chexbert, GREEN, RadCliq, FineRadScore, RateScore etc. I think ReXrank and RadEval provides a good summary of things to compare to."}, "questions": {"value": "- How sensitive is the correction accuracy to the IOU threshold and the veracity label Ep from the FC model?\n- Did you evaluate inter-run variability in LLM corrections\n- Can the RadCheck synthetic data be extended to modalities beyond chest X-ray?\n- Why did you pick these metrics as opposed to others?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZGXsoKx741", "forum": "xDnRK6E6SU", "replyto": "xDnRK6E6SU", "signatures": ["ICLR.cc/2026/Conference/Submission23406/Reviewer_WaKm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23406/Reviewer_WaKm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23406/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011900609, "cdate": 1762011900609, "tmdate": 1762942647484, "mdate": 1762942647484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}