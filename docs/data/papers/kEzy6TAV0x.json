{"id": "kEzy6TAV0x", "number": 7224, "cdate": 1758012146436, "mdate": 1759897865386, "content": {"title": "Causal-Adapter: Taming Text-to-Image Diffusion for Faithful Counterfactual Generation", "abstract": "We present Causal-Adapter, a modular framework that adapts frozen text-to-image diffusion backbones for counterfactual image generation. Our method enables causal interventions on target attributes, consistently propagating their effects to causal dependents without altering the core identity of the image. In contrast to prior approaches that rely on prompt engineering without explicit causal structure, Causal-Adapter leverages structural causal modeling augmented with two attribute regularization strategies: prompt-aligned injection, which aligns causal attributes with textual embeddings for precise semantic control, and a conditioned token contrastive loss to disentangle attribute factors and reduce spurious correlations. Causal-Adapter achieves state-of-the-art performance on both synthetic and real-world datasets, with up to 91\\% MAE reduction on Pendulum for accurate attribute control and 87\\% FID reduction on ADNI for high-fidelity MRI image generation. These results show that our approach enables robust, generalizable counterfactual editing with faithful attribute modification and strong identity preservation.", "tldr": "We present Causal-Adapter, a modular method that tames frozen text-to-image diffusions for counterfactual image generation. The method enables causal interventions, consistently propagates their effects to dependent attributes and preserves identity.", "keywords": ["Counterfactual Image Generation;Causality;Diffusion;Image Editing"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e4ed3ac74042273f11e61979a6ec5adb4b1d90b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes to do counterfactual image editing task using a known/assumed causal graph and observed semantic attributes.\nGiven the causal graph and observed semantic attributes, the paper proposes to train a causal adapter that learns the causal mechanisms between attributes (though not the causal mechanism between attributes and image).\nKey challenges with current T2I is *continuous* attribute control and attribute entanglement.\nBecause attributes can be entangled in text-to-image models, the paper proposes two techniques (PAI and CTC) to mitigate this (spurious/improper) attribute entanglement.\nThe paper then provides several experiments on pendulum toy dataset, CelebA and brain scan images."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a simple adapter to enable causal attribute edits given a causal graph and observed semantic attributes.\n- The paper proposes two regularizations to encourage better disentanglement of semantic attribute injection. \n- The paper gives some evidence of the qualitative benefits of their approach over certain baselines."}, "weaknesses": {"value": "- Assumes known causal graph and observed causal attributes. This is a significant limitation that should not be brushed off. The authors even argue that prior causality based methods for learning representations (e.g., CausalVAE) are not good. While agreed that it is a very difficult problem, this should have a more nuanced discussion and the limitation should be acknowledged as a causal graph for all semantic attributes is not always known and neither are causal semantic attributes always observed.\n\n- Experiments seem limited in several cases either because of different setup of baselines or non-diffusion baselines:\n\n  - The experiments are related to CausalVAE but (at least from my understanding) CausalVAE does not assume knowledge of the graph or observed semantic attributes. Thus, I'm not sure it is a fair comparison. Do DisDiffAE and CausalDiffAE assume access to the causal graph and observed semantic attributes?\n\n  - The experiments on CelebA only compare to VAE, HVAE and GAN. No diffusion baselines are included, which is concerning as diffusion baselines would be more comparable. VAE-based and GAN-based methods have not be the standard for a long time. Why not include the baselines you mentioned in Figure 2 that are diffusion based?\n\n- The paper has multiple small components and modifications that make it difficult to take away generalizable insight. There does not seem to be a unifying theme per se but more like a combination of different architecture and training ideas to improve performance via engineering. This reduces the overall impact of the paper since there are not 1-2 major takeaways that could be generalized to new scenarios. \n\n- The abduction step assumes that the the frozen diffusion model correctly recovers the \"causal\" exogenous noise. This implicitly assumes that the diffusion model is the \"correct\" causal mechanism that maps from semantic attributes to an image (or at least counterfactually equivalent mechanism, see bijective causal models, https://arxiv.org/abs/2302.02228). Yet, this may not be the case---i.e., there may be a true mapping between causal attributes and image that is different than the frozen diffusion model. This should be carefully noted and explained.\n\n- (Minor) Method requires DDIM inversion for editing."}, "questions": {"value": "See weaknesses above for several questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ujVzQyocxh", "forum": "kEzy6TAV0x", "replyto": "kEzy6TAV0x", "signatures": ["ICLR.cc/2026/Conference/Submission7224/Reviewer_LQ8E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7224/Reviewer_LQ8E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752879800, "cdate": 1761752879800, "tmdate": 1762919369138, "mdate": 1762919369138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors target the problem of generating more sound visual counterfactuals by integrating causal knowledge into a frozen text-to-image generation model with diffusion backbones.\nThe contribution is twofold. In the first part, they propose a prompt-aligned injection, which allows the causal attributions to align with text embeddings. The second part aims to prevent spurious correlations by introducing an additional loss to disentangle attribute factors.\nThey tested the proposed method on both synthetic and real-world datasets, such as human faces and MRI images."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The presentation of this work is generally very good, except that most of the figures contain very small text that is not readable in printed papers.\n2. I appreciate the detailed illustration of the comparison with previous literature in Figure 2, though the text is indeed too small to read. This strength also extends to the main text, where the authors describe previous studies in detail and make clear comparisons with their own work.\n3. The experiments are quite thorough, involving three datasets and visualizations of the results.\n4. The evaluation is comprehensive, covering four types of metrics."}, "weaknesses": {"value": "1. The steps of training and inference for the causal adapter are a bit hard to follow.\n2. It seems that this framework requires a pre-defined causal graph as well as annotations of the attributes in the causal graph, which is quite a constraint. If that is the case, it should be discussed more thoroughly in the paper.\n3. The motivation of the work appears to encourage the correlation of attributes when generating counterfactuals (as illustrated in Fig. 1, where the proposed model seems to perform better at changing correlated attributes. that is, editing more attributes than non-causal models). At the same time, a loss is introduced to prevent spurious correlations. From the machine’s point of view, correlation is correlation; how would the model distinguish between spurious and acceptable correlations?\n4. It also seems that the results on CelebA are based on a single seed, without standard deviations from multiple runs. This makes the results somewhat difficult to evaluate, especially for the Minimality part."}, "questions": {"value": "1. In Figure 2, do the first and second groups (VAE or GAN, and diffusion SCM) also include conditional diffusion models without a causal graph (or, say, only with attributes but not the causal relationships between attributes)? Because in the text, the works you listed are not all with causal graphs. If that is the case, it would be better to specify at the beginning of this paragraph that the SCM could downgrade to conditional generation with only one attribute, without a causal group, when there are no parent nodes.\n2. In Tables 1 and 2, is the first row the target semantic attributes Y for the training of the causal adapter, and is the second row (e.g., do(p)) the inference goal?\n3. In Figure 3, it seems that the generations from your model are a bit blurry. Why is that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tjnWftdShm", "forum": "kEzy6TAV0x", "replyto": "kEzy6TAV0x", "signatures": ["ICLR.cc/2026/Conference/Submission7224/Reviewer_njF3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7224/Reviewer_njF3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885116623, "cdate": 1761885116623, "tmdate": 1762919368814, "mdate": 1762919368814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight, plug-in module that adapts a frozen text-to-image diffusion backbone for faithful counterfactual image generation. The core contributions are a PAI mechanism that maps each causal attribute to a learnable prompt token so cross-attention can align semantics with spatial features, and a CTC objective that disentangles attribute tokens to reduce leakage and spurious correlations. Compared with prompt-only editing, this adaptive, causally informed design yields strong, generalizable control across synthetic and real domains with large gains in effectiveness and fidelity. The paper also notes evaluation caveats in OOD cases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is sharply motivated by a clear diagnosis of text-to-image editing limits, established via a targeted motivational study.\n\n- It introduces a lightweight, plug-in causal-adapter that steers a frozen diffusion backbone with explicit causal semantics, offering a practical, modular path to faithful counterfactual generation without retraining the base model.\n\n- The two core techniques are well-motivated and empirically shown to align attribute semantics, disentangle tokens, and curb spurious correlations while preserving identity.\n\n- The experimental section is broad and convincing."}, "weaknesses": {"value": "- The work provides no formal theoretical guarantees. \n\n- Its evaluation hinges on intervention classifiers and can break in OOD regimes, meaning reported effectiveness can be confounded by classifier limitations rather than true causal faithfulness.\n\n- Code is promised only upon acceptance."}, "questions": {"value": "- The evaluation relies on intervention classifiers that can mislabel valid counterfactuals. What robustness checks did you run to deconfound this?\n\nOverall, this is a good paper. I will not argue if this paper is accepted."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0cFP5qzi0E", "forum": "kEzy6TAV0x", "replyto": "kEzy6TAV0x", "signatures": ["ICLR.cc/2026/Conference/Submission7224/Reviewer_iWyj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7224/Reviewer_iWyj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946158328, "cdate": 1761946158328, "tmdate": 1762919368435, "mdate": 1762919368435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Causal-Adapter, a framework for generating counterfactual images. It uses pretrained text-to-image diffusion models for causal interventions based on the Structural Causal Model (SCM). The motivation is that standard text-to-image editing methods lack explicit causal structure, leading to inconsistent edits and failure to handle attribute entanglement. The main contribution is claimed to be a lightweight, pluggable adapter that injects causal semantics into the diffusion backbone without full retraining. This is supported by two regularization strategies: Prompt-Aligned Injection (PAI) for semantic alignment and a Conditioned Token Contrastive (CTC) Loss to disentangle attribute representations. The framework was evaluated on synthetic (Pendulum), CelebA, and ADNI datasets, using metrics such as MAE/F1, FID, LPIPS, and the minimality of CLD."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper's claimed contributions are a study demonstrating the inadequacy of prompt-tuning for causal tasks, the Causal-Adapter framework itself, and the PAI/CTC regularization methods. The primary impact stems from its efficiency and modularity, allowing a frozen foundation model to be adapted for specialized, causally aware tasks (e.g., medical imaging) without computationally expensive retraining. The evaluation spans three different domains (physics, human faces, and medical imaging) and includes an ablation study that validates the necessity of the PAI and CTC components. The writing is well-structured and clear."}, "weaknesses": {"value": "The framework's primary limitation is the lack of novelty in the method and its assumption of a known, correctly specified causal graph; it was not evaluated under conditions of a misspecified or unknown graph. \nA significant flaw exists in the evaluation protocol: the \"Effectiveness\" classifiers, trained on biased data, penalize the model for successfully generating valid, correlation-breaking counterfactuals (e.g., a \"bearded female\" is misclassified). \nThe \"Minimality\" metric (CLD) is an indirect proxy that does not directly verify the invariance of non-descendant attributes. \nThe paper also lacks a direct comparison to alternative paradigms for causal control, such as methods that use semantic-level guidance during the sampling process."}, "questions": {"value": "Was there any reason you did not compare or cite the following works:\n\nChao, P., Blöbaum, P., Patel, S. and Kasiviswanathan, S.P., 2023. Modeling causal mechanisms with diffusion models for interventional and counterfactual queries. arXiv preprint arXiv:2302.00860.\n\nLyu, M., Yang, Y., Hong, H., Chen, H., Jin, X., He, Y., Xue, H., Han, J. and Ding, G., 2024. One-dimensional adapter to rule them all: Concepts diffusion models and erasing applications. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 7559-7568).\n\nYeganeh, Y., Farshad, A., Charisiadis, I., Hasny, M., Hartenberger, M., Ommer, B., Navab, N. and Adeli, E., 2025. Latent Drifting in Diffusion Models for Counterfactual Medical Image Synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 7685-7695)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3zSCoAinD0", "forum": "kEzy6TAV0x", "replyto": "kEzy6TAV0x", "signatures": ["ICLR.cc/2026/Conference/Submission7224/Reviewer_gS9o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7224/Reviewer_gS9o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7224/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993907344, "cdate": 1761993907344, "tmdate": 1762919368104, "mdate": 1762919368104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}