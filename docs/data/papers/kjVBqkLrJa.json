{"id": "kjVBqkLrJa", "number": 25608, "cdate": 1758369462497, "mdate": 1759896713467, "content": {"title": "Style2Shape: Image Style Guided 3D Shape Material Generation", "abstract": "This paper presents Style2Shape, a novel framework for generating physically-based rendering (PBR) materials for 3D models from a single reference image. Unlike existing methods limited by the diversity of procedural material libraries or producing non-editable representations, our approach combines procedural ma-terials with generated textures via differentiable rendering. Our key insight is that procedural parameters ensure reflectance correctness while generated textures capture arbitrary appearances-their learnable combination achieves both physi-cal plausibility and visual fidelity. The framework operates in three stages: (1) structure-guided appearance transfer that synthesizes geometrically-aligned su-pervision, (2) hybrid PBR material initialization that retrieves procedural mate-rials based on physical properties and generates complementary textures for ap-pearance details, and (3) physics-based optimization jointly refining all compo-nents through differentiable rendering. Extensive experiments demonstrate that our approach generates high-fidelity results, producing editable PBR materials that faithfully reproduce reference appearances while maintaining physical plau-sibility. The generated assets are structured to be compatible with standard 3D rendering workflows.", "tldr": "", "keywords": ["Material Generation; Differentiable Rendering; Procedural Materials; Appearance Transfer; Physically-Based Rendering"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/381919a74bdc1f7e494544cf0e4f4e72544557fa.pdf", "supplementary_material": "/attachment/a6fa5e25a86596f96c1fd0e111a426eac7d34bbf.zip"}, "replies": [{"content": {"summary": {"value": "Style2Shape is a framework for generating PBR materials for 3D models conditioned on a reference image. It combines procedural materials with generative techniques to balance physical plausibility and visual fidelity. The pipeline has three stages: (1) structure-guided appearance transfer, (2) hybrid PBR material initialization, and (3) differentiable material optimization. The method is compared against two baselines, PSDR-Room and Material Palette, and reports superior performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation. Procedural materials provide physical correctness but have limited expressivity, while generative textures are expressive but not guaranteed to be physically valid. The proposed hybrid approach is well motivated.\n2. Empirical results indicate better visual fidelity than the baselines."}, "weaknesses": {"value": "1. Several relevant texture generation methods are missing, for example TexGaussian and Meta3DGen.\n2. The paper compares only to two procedural methods, omitting many generative approaches with the same problem setting, for example RomanTex, Hunyuan3D-2.0, TEXGen, SyncMVD, and Paint3D. Without these, the evaluation is not fully convincing.\n3. The ablation focuses on incorporating generated textures, leaving other design choices under explained. For example, how does the VGG term contribute in Exp(9)? How much does the Progressive Optimization Strategy help compared to direct optimization?\n4. The paper claims preprocessing via SAMesh extends to objects beyond the six categories, but no substantive examples are provided, aside from the dress in Fig. 12 which lacks distinct material parts. In addition, semantic segmentation does not guarantee material homogeneity within segments, and it may not merge spatially separated regions of the same material. This limits generalization.\n5. The method assumes an optimal viewpoint that covers all material regions. This can hold for simple objects such as tables and chairs, but often fails for complex geometry such as human characters and garments, which constrains applicability.\n6. Section indices are inconsistent. There is no index before RELATED WORK, and MATERIAL GENERATION FOR 3D OBJECT is incorrectly labeled as 1.1.\n\nTexGaussian: Generating High-quality PBR Material via Octree-based 3D Gaussian Splatting\nMeta 3d texturegen: Fast and consistent texture generation for 3d objects\nRomanTex: Decoupling 3D-aware Rotary Positional Embedded Multi-Attention Network for Texture Synthesis\nHunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation\nTEXGen: a Generative Diffusion Model for Mesh Textures\nText-guided texturing by synchronized multi-view diffusion.\nPaint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models"}, "questions": {"value": "How many objects were used for quantitative evaluation, and is this the same set used in the user study?\n\nIn Table 1, why does performance degrade for the bag category?\n\n“For small material segments where texture details are less critical, we assign uniform materials without retrieval to maintain computational efficiency.” What uniform materials are assigned in practice, and how are their parameters chosen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1e18GiOA5M", "forum": "kjVBqkLrJa", "replyto": "kjVBqkLrJa", "signatures": ["ICLR.cc/2026/Conference/Submission25608/Reviewer_oWHJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25608/Reviewer_oWHJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761612922739, "cdate": 1761612922739, "tmdate": 1762943492218, "mdate": 1762943492218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TLDR: A complex, three-stage framework generating PBR materials from single image.\n\nThis paper proposes Style2Shape, which generates PBR materials for 3D models from a single reference image. Extending PSDR-Room (retrieves procedural materials and optimizes parameters), this work adds texture generation via image editing models to capture fine-grained appearances. However, the approach assumes albedo and roughness/metallic are independent (problematic), relies on multiple external dependencies, and uses multiiple ad-hoc components that create a fragile cascade."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Tries to combine image generated textures with procedural patterns.\n- Transfer results look interesting."}, "weaknesses": {"value": "- Overly complex and fragile cascade system that have multiple ad-hoc components - failure in any stage causes irreversible harm. Three sequential stages with heavy external dependencies (parts segmentation, image editing model, RGB-X decomposition, procedural library, differentiable renderer).\n- Problematic independence assumption: Stage 2 separately retrieves roughness/metallic and generates albedo, but in reality these are correlated. Inconsistency between these two components will make the material not physically plausible. And to really see if the reflectance make sense, we need to see a view-varying video.\n- Assumes simplistic textures - method may not generalize to other type of textures.\n- \"Editability\" claimed but not demonstrated - no actual editing examples\n- Have to find the best pose of the 3D shape so the image can capture most of the parts"}, "questions": {"value": "1. How this work will inspire and stimulate future work in this direction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bNE6U4wgB7", "forum": "kjVBqkLrJa", "replyto": "kjVBqkLrJa", "signatures": ["ICLR.cc/2026/Conference/Submission25608/Reviewer_rigK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25608/Reviewer_rigK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983998254, "cdate": 1761983998254, "tmdate": 1762943492008, "mdate": 1762943492008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Style2Shape, a novel and comprehensive framework for generating editable, physically-based rendering (PBR) materials for a 3D model from a single reference image. The core contribution is a hybrid material representation that synergistically combines procedural materials and AI-generated textures. The authors argue that this approach leverages the strengths of both: procedural materials ensure physical correctness of reflectance properties (e.g., roughness, metallicity), while generative models capture arbitrary and complex visual appearances from the reference."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The central contribution—a learnable blend of procedural materials and generated textures—is a powerful and elegant idea. It effectively combines the strengths of both paradigms, achieving results that are simultaneously physically plausible and visually faithful to an arbitrary reference.\n\nThe three-stage pipeline is very well-designed. Stage 1 intelligently solves the critical domain gap problem. Stage 2 provides a strong initialization that is crucial for the success of the high-dimensional optimization. Stage 3 uses a progressive optimization strategy to robustly converge to a high-quality solution. This structured approach demonstrates a deep understanding of the problem's complexities.\n\nA major strength is that the final output consists of standard, editable PBR texture maps. This makes the generated assets directly usable in modern 3D engine or rendering software."}, "weaknesses": {"value": "My comments primarily echo and expand upon those points, framing them as areas for improvement.\n\nThe proposed \"maximin\" criterion for optimal viewpoint selection is logical, but it does not address scenarios where some material segments are inherently occluded from any single viewpoint (e.g., the inside of a cup, the underside of a complex chair). The paper does not specify how materials are generated for segments that are not visible from the chosen optimal view. This omission represents a potential failure point for complex geometries.\n\nThe success of Stage 1 heavily relies on a state-of-the-art, prompt-guided image editing model (referred to as \"GPT-Image-1\"). The paper would be strengthened by demonstrating that the framework is robust to the choice of the image model, for instance, by showing results with open-source alternatives like ControlNet or InstructPix2Pix.\n\nThe framework's material retrieval and subsequent optimization are contingent on the quality of the decomposed BRDF maps from Stage \n\n2. The paper cites RGB-X, but in practice, single-image intrinsic decomposition models often struggle with complex, non-uniform lighting, strong cast shadows, or highly specular surfaces. The real-world performance of such models can be suboptimal, which could lead to inaccurate initialization of roughness and metallic properties, potentially hindering the optimization process or leading to physically incorrect results. The paper would benefit from a discussion on how errors from this decomposition stage propagate and how the framework might mitigate them."}, "questions": {"value": "Could the authors clarify the specific model used for the structure-guided image editing? Have they experimented with publicly available models (e.g., based on Stable Diffusion with ControlNet)? How sensitive are the final results to the quality of the image generated in this first stage?\n\nThe viewpoint selection method aims to maximize visibility, but for complex models, it's inevitable that some material segments will remain unseen from any single optimal viewpoint. How does the framework handle material generation for these occluded parts? Are they assigned a default material, do they inherit properties from adjacent visible segments, or is there another mechanism in place?\n\nGiven that models like RGB-X can produce artifacts or inaccurate maps under challenging lighting, how does the system perform when the initial Irgh and Imet maps are noisy or incorrect? Does the final differentiable optimization stage have the capacity to correct for a poor initialization stemming from decomposition errors, or does it typically converge to a suboptimal result?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2XLzUoSI9z", "forum": "kjVBqkLrJa", "replyto": "kjVBqkLrJa", "signatures": ["ICLR.cc/2026/Conference/Submission25608/Reviewer_RvFp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25608/Reviewer_RvFp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051206555, "cdate": 1762051206555, "tmdate": 1762943491777, "mdate": 1762943491777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Style2Shape, a framework designed to generate materials from a single reference image. The pipeline comprises three stages: (1) aligning 3D geometry with the reference image, (2) initializing material parameters through a retrieval mechanism, and (3) refining procedural materials via differentiable rendering. The system is engineering-complete, and the presented results demonstrate the effectiveness. \n\nHowever,  the core modules and overall pipeline strongly overlap with MaPa (SIGGRAPH 2024): both use 2D diffusion generation as a bridge to produce aligned images, both rely on a procedural material library for retrieval, and both refine procedural parameters with differentiable rendering. The primary distinction lies in the modality of input that Style2Shape accepts the image as input, while MaPa is conditioned on text.  The authors do not clearly validate its unique advantages or contributions over MaPa. \n\nWhile the system is well-engineered, the overlap in methodology limits its originality. I think the contribution may fall short of the innovation threshold expected at ICLR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- A complete three-stage pipeline that jointly addresses multiple subproblems in alignment, initialization, and refinement"}, "weaknesses": {"value": "- Novelty\n\n  The paper has high overlap with MaPa and lacks direct comparison or justification of unique contributions.\n\n- Clarity\n\n  The paper’s exposition could be improved. For instance, in line 43, it points out the drawbacks of procedural material generation, yet in line 46, it begins to praise its advantages, which creates a logical inconsistency and may confuse readers.\n\n- Experiments\n\n  1. The paper proposes a three-stage pipeline, yet lacks corresponding ablation studies to verify the importance of each component. For example, there is no experiment showing the effect of misalignment when the input image is not properly aligned with the geometry.\n  2. In line 36, the authors mention the issue of specular ambiguity as a motivation for introducing procedural modeling, yet the presented results do not include materials with strong specular highlights, such as metals, it is not sure the performance of the proposed method for such cases.\n  3. The paper lacks comparison to several relevant methods, such as MaterialMVP (ICCV 2025), which also supports image as the condition and generates the object material. A comparative analysis would be beneficial to highlight the strengths of the proposed formulation, making the paper more solid\n\n- Figure\n\n  Some of the text in the figure is low-resolution and difficult to read. The authors are encouraged to improve the rendering quality or resolution\n\n- Technical limitation\n\nThe paper attempts to use text prompts to generate seamless patterns. However, it cannot guarantee true seamlessness with only text prompts constraints."}, "questions": {"value": "1. In the supplementary materials, is Style2Shape/images/bag-005/seamless_0 intended to be the output SVBRDF folder? If so, it is evident that the normal map in this directory is incorrect. Moreover, the result is not truly seamless, as I mentioned.\n\n2. The normal maps depicted in Fig. 12 appear excessively smooth. It is unclear whether this is due to a downsampled resolution in the visualization or if the predicted normals themselves inherently lack high-frequency detail. \n\n3. Most of the results are demonstrated on furniture models with relatively simple geometry. Is that possible to apply the method to the complex model and other types (like a metallic dragon)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rtUmFhx5gj", "forum": "kjVBqkLrJa", "replyto": "kjVBqkLrJa", "signatures": ["ICLR.cc/2026/Conference/Submission25608/Reviewer_Hn2v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25608/Reviewer_Hn2v"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137463589, "cdate": 1762137463589, "tmdate": 1762943491580, "mdate": 1762943491580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}