{"id": "fasU6t3hL4", "number": 18710, "cdate": 1758290311932, "mdate": 1759897085996, "content": {"title": "Adversarial examples for heuristics in combinatorial optimization: An LLM based approach", "abstract": "This work employs LLMs to generate adversarial examples for heuristics in combinatorial optimization.\nThe problem, given a heuristic for an optimization problem, is to generate a problem instance where the heuristic performs poorly. We find improved adversarial constructions for well-known heuristics for k-median clustering, bin packing, the knapsack problem, and a generalization of Lov\\'asz's gasoline problem. Specifically, we adapt the FunSearch framework [Romera-Paredes et al., Nature 2023] to obtain adversarial constructions for these problems. We note that using FunSearch is crucial to our improved constructions --- local search does not give comparable results. \nThe advantage of FunSearch is that it produces structured instances that yield theoretical insights which are post-processed and generalized by a human researcher while other metaheuristics usually produce only unstructured instances that are harder to generalize.", "tldr": "We explore the potential of FunSearch to generate adversarial examples for heuristics in combinatorial optimization.", "keywords": ["combinatorial optimization", "FunSearch", "LLMs", "analysis of algorithms"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d7aba52b8fe210c1acfab3a84c8e27ef46eefa1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper applies FunSearch (LLM-based evolutionary program search) to generate adversarial instances for combinatorial optimization heuristics. The authors investigate four problems: Nemhauser-Ullmann knapsack heuristic, Best-Fit bin packing, k-median hierarchical clustering, and iterative rounding for the gasoline problem. They propose \"Co-FunSearch\" combining automated generation with manual refinement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Rigorous mathematical follow-up: Unlike pure black-box approaches, the authors provide formal proofs for most claims (Theorems 3.1-3.4).\n\n2. Honest reporting: The limitations section (5.4) honestly reports failures.\n\n3. Diverse problem selection: Four different combinatorial optimization domains demonstrate some generality."}, "weaknesses": {"value": "1. The paper doesn't establish why finding adversarial instances via LLM is an important research problem. The paper doesn't demonstrate that these adversarial instances have any practical implications. Are these contrived constructions or do they reveal fundamental algorithmic weaknesses?\n\n2. Results may already be known or trivial: The bin packing instance is suspiciously simple - the authors should check whether this construction appears implicitly in prior work.\n\n3. Gasoline extrapolation from Table 2 to general claims lacks justification. Need either formal proof of scaling or results for larger instances.\n\n4. \"Co-\" prefix admits heavy expert refinement needed, contradicting automated discovery narrative. Contribution breakdown between LLM and human unclear."}, "questions": {"value": "Given that (1) heavy human expert refinement is required (\"Co-\"), (2) success rate is ~50% or lower, (3) several results have questionable novelty/correctness, and (4) practical impact is unclear - can you articulate a clear value proposition for why the community should pursue LLM-based adversarial instance generation over traditional mathematical analysis? What specific advantages does this approach offer that justify its costs and limitations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IdLe7E9dS5", "forum": "fasU6t3hL4", "replyto": "fasU6t3hL4", "signatures": ["ICLR.cc/2026/Conference/Submission18710/Reviewer_QmxA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18710/Reviewer_QmxA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815418500, "cdate": 1761815418500, "tmdate": 1762928416780, "mdate": 1762928416780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of finding hard instances for a fixed algorithm for a fixed problem. The studied problems are all NP-hard, namely Knapsack, Bin packing, k-median in hierarchical clustering, and Lovasz's gazoline problem. Specific approximation algorithms are considered for each of these problems. The goal is to use artificial intelligence to generate instances which maximize the approximation ratio of the considered algorithms. Instead of generating the instance itself, a compact description is generated using a large language model artificial intelligence. The prompt asks the AI to come up with a worse instance, and provides a first python program generating an instance.\n\nI find this approach quite crazy, in my own humble experience, lower bound instances have to be constructed by hand, and computers turned out to be bad in generating hard instances. But investigating the power of LLMs for this task is an interesting approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper succeeds to find an instance which breaks a conjecture from 2024 about a specific algorithm for the gazoline problem.  For other problems the LLM gave the authors ideas to improve the generated instance. So this work shows that LLM could help in algorithm design. I don't know much about LLMs, so I don't have the right background to evaluate the paper. But I am into algorithm design, and hence any new method that could help is welcomed."}, "weaknesses": {"value": "Overall I am still hungry after reading the paper. From what I have seen in the algorithm community, is that most hard instances consists of some fractions, to which some epsilon values have been added or removed. Also I think the approach could benefit from the design of a grammar describing compactly instances, rather than generating a Python program, because the search space is more targeted to a form of aimed instances."}, "questions": {"value": "You don't give the evaluation function to the LLM. Explain this choice.\n\nLine 372, explain what each dimension means. I guess first is weight, and second is profit.\nSeveral times you mention a choice of temperature, which I only know in the context of simulated annealing. Maybe explain."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4xRC76Usz3", "forum": "fasU6t3hL4", "replyto": "fasU6t3hL4", "signatures": ["ICLR.cc/2026/Conference/Submission18710/Reviewer_cqeS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18710/Reviewer_cqeS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839810132, "cdate": 1761839810132, "tmdate": 1762928416266, "mdate": 1762928416266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method for finding adversarial instances for combinatorial optimization heuristics based on FunSearch. They use LLMs for creating problem instances in a form of Python programs where given heuristic does not perform well. The authors argue that this representation is more interpretable than less structured instance vectors obtained for example by local search. The paper focuses on heuristics for four combinatorial optimization problems including the knapsack problem, bin packing, hierarchical clustering and a variant of the gasoline problem. The authors find tighter lower bounds than the ones that were known before for these problems using their method of Co-FunSearch."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality \n\nThe paper is based upon existing FunSearch technique but it introduces novel elements such as including experts in the search pipeline in their Co-FunSearch variant. They also apply FunSearch to the combinatorial optimization domain. The authors also find novel theoretical results for the considered problems which seem to be not have been known before this work. \n\n2. Quality\n\nThe paper presents numerous experiments and tests proposed methodology for four important problems thus providing a solid experimental base for their results.\n\n3. Clarity\n\nThe paper is well-written and easy to follow.\n\n4. Significance\n\nCombinatorial optimization heuristics have important practical and theoretical meaning. Constructing adversarial examples for them provides additional insights into the nature of the problems for which they were created. Using an interpretable representation of such adversarial examples such as Python-code can be useful for further analysis of these examples and constructing novel heuristics which would be more robust."}, "weaknesses": {"value": "1. (Significance) Co-FunSearch relies on collaboration with human experts which might be a bottleneck for large-scale applications of the method. As the authors state (lines 076-077), expert modifications were essential for generating meaningful insights. Thus, the methodology can be used as a supporting tool but not as a standalone process for generating adversarial examples. \n\n2. (Clarity) Even though local search is known in the field of combinatorial optimization, could you specify in more detail how exactly you perform it since it is one of the baselines in your experiments."}, "questions": {"value": "1. Can Co-FunSearch be made fully autonomous by using agents for refining found programs (lines 076-077, Figure 1c, 3c)?\n\n2. The authors argue that solutions found by their method are more interpretable and symmetric when compared to solutions found by local search (lines 066-067). It would be interesting to see whether they are also more robust to noise. For example, if we take a vector provided by local search such as the one in the footnote 1 (lines 106-107) and add some small noise to its element, will it maintain it's properties as an adversarial example for a given heuristic? What if we do the same to solutions provided by your method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AaqiDpLeLU", "forum": "fasU6t3hL4", "replyto": "fasU6t3hL4", "signatures": ["ICLR.cc/2026/Conference/Submission18710/Reviewer_D4Qs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18710/Reviewer_D4Qs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987359872, "cdate": 1761987359872, "tmdate": 1762928415563, "mdate": 1762928415563, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends FunSearch to generate adversarial examples for heuristics for combinatorial optimization. Specifically, the authors look at knapsack, bin packing, k-median clustering, and the gasoline problem. The value of the task is generating practical samples that are hard to approximate for a certain solver, closing the gap to the worst-case upper bound."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Using LLM to tackle math and optimization problems is a trending research topic. \n* The proposed Co-FunSearch framework seems to outperform FunSearch in the experimental evaluations.\n* An ablation study is provided in the experiment section."}, "weaknesses": {"value": "* While the idea of identifying worst-case instances for existing heuristics is conceptually interesting, it is unclear whether this direction achieves the same level of impact or technical novelty as improving the heuristics themselves — as demonstrated, for example, in FunSearch and its case studies.  \n\n* Significant revisions in writing and presentation are necessary before the technical contributions of this work can be properly evaluated.  \n    * The introductory paragraph discussing AI advancements in biology, chemistry, and mathematics is tangential to the main topic and should be removed to improve focus.  \n    * A substantial portion of the paper is devoted to describing problem definitions, heuristics, and code snippets, yet key implementation details of the proposed method are missing. It remains unclear how Co-FunSearch differs from FunSearch, or how it can be concretely instantiated with a given heuristic. The authors should include algorithmic diagrams or pseudo-code in a dedicated “Method” section to clarify these aspects.  \n    * If the authors wish to elaborate on problem formulations, additional context and explanations are needed for readers unfamiliar with the examples. For instance, the so-called “famous gasoline problem” attributed to Lovász is insufficiently explained; as presented, it is not interpretable and unlikely to be “famous” to the broader audience.  \n    * In line 214, the authors state:  \n      > “The main goal in all these problems is to search for a vector v which optimizes the given objective.”  \n      \n      This statement lacks precision. It is unclear  \n        1. what the “vector v” represents and how it fits within the different search paradigms described, and  \n        2. what the “given objective” is, i.e., which function or metric is actually being optimized."}, "questions": {"value": "* In the footnote on page 2, \n    > For instance, one of the local-search-generated lists outperforming FunSearch was: [0.003031, 0.005466,\n0.006098, 0.007283, 0.021158, 0.068030, 0.073417, 0.170490, 0.202092, 0.219287, 0.306771, 0.375912,\n0.540358].\n   \n    It is unclear how this is relevant to a \"discernible pattern\" and please provide explanations. \n* To compute the approximate ratio, the optimal value is required. How do you know the optimal value if the problem parameter is always changing? Is the scalability of this framework bottlenecked by solving for the optimal value?\n* How many problem instances are used to calculate Table 1? How are they collected? How do you determine the size of the problem instance?\n* In Table 1, what does \"Previous Best Lower Bound\" mean? \n* In Table 1, what is the \"Best-Fit\" problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yf1jHZo2ns", "forum": "fasU6t3hL4", "replyto": "fasU6t3hL4", "signatures": ["ICLR.cc/2026/Conference/Submission18710/Reviewer_zZp2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18710/Reviewer_zZp2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762234937326, "cdate": 1762234937326, "tmdate": 1762928413456, "mdate": 1762928413456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}