{"id": "psDyG1FDzX", "number": 15102, "cdate": 1758247754859, "mdate": 1759897328677, "content": {"title": "Large Language Models as a Computable Surrogate to Solomonoff Induction", "abstract": "The rapid advancement of large language models (LLMs) calls for a rigorous theoretical framework to explain their empirical success. \nWhile significant progress has been made in understanding LLM behaviors, existing theoretical frameworks remain fragmented in explaining emergent phenomena through a unified mathematical lens.\nWe establish the first formal connection between LLM architectures and Algorithmic Information Theory (AIT) by proving two fundamental results: (1) the training process computationally approaches Solomonoff prior through loss minimization interpreted as program length optimization, and (2) Under the assumption that $M(x_{1:t})\\approx \\overline{M}(x_{1:t})$, LLMs' next-token prediction implements a form of surrogate Solomonoff induction. \nWe leverage AIT to provide a heuristic, unified theoretical explanation for in-context learning, few-shot learning, and scaling laws.\nFurthermore, our theoretical insights lead to a principled method for few-shot example selection that prioritizes samples where models exhibit lower predictive confidence. We demonstrate through experiments on diverse text classification benchmarks that this strategy yields significant performance improvements, particularly for smaller model architectures, when compared to selecting high-confidence examples.\nOur framework bridges the gap between theoretical foundations and practical LLM behaviors, providing both explanatory power and actionable insights for future model development.", "tldr": "", "keywords": ["Large Language Models", "Solomonoff Induction", "Solomonoff Induction"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/020ac5f8057f9af0927037fe8f6efdf0f45ed885.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors study the behavior of LLMs through Algorithmic Information Theory. They argue that the LLM training process\ncan be viewed as approximating the Solomonoff prior and, under the assumption that the approximate Solomonoff prior \ninduced by the model is close to the true one, LLM next-token prediction can be viewed as a form of approximate \nSolomonoff induction. Then, based on these observations, they propose an uncertainty-based method to choose examples \nin few-shot learning and conduct experiments to show the validity of this method."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Overall, the paper is easy to follow and clear, and the use of those colored boxes does make it easier to locate \n  important information. \n* The use of Solomonoff complexity in this setting is novel and seems to be interesting."}, "weaknesses": {"value": "The main issue of this paper is that, while the authors claim the paper to be theoretical, the theoretical results \nare non-rigorous at best. \n* In Theorem 1, the authors claim that the approximate Solomonoff prior induced by the model $\\bar{M}(x)$ will approach \n  the true Solomonoff prior $M(x)$ as the training loss decreases. However, not only the authors do not define what \n  (class of) losses they are considering and provide little justification on this statement. They cite \n  [Delétang et al., 2023] and [Wan, 2025] to justify it, but I do not think this is proved anywhere in these two papers.\n* The second theoretical part of the paper (approximate Solomonoff induction) relies on the assumption that \n  $\\bar{M}(x) \\approx M(x)$. I do not see why this is reasonable even in some idealized case. The Solomonoff prior $M(x)$\n  aggregates all possible programs that generate $x$ as a prefix, while the $\\bar{M}(x)$ considers a single model and \n  vary only the random seed. Therefore, I do not think $\\bar{M}(x)$ can be used to approximate $M(x)$ (unless we interpret the model as a \n  universal Turing machine and view the seed as the program to be simulated, which is very different from the usual \n  way of interpreting LLMs and this type of behavior cannot be expected from any conventionally-trained LLM).\n* In addition, for the empirical part of the paper, uncertainty-based few-shot sample selection is not a new idea and \n  has at least appeared in, say, [1].\n\n\n[1] Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, Tong Zhang. Active Prompting with Chain-of-Thought for Large Language Models. 2023."}, "questions": {"value": "See the Weakness section for details. \n* Could you further justify point 2 of Theorem 2?\n* Could you explain why it is reasonable to expect $\\bar{M}(x) \\approx M(x)$, at least in some idealized settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bVmBuz7xpK", "forum": "psDyG1FDzX", "replyto": "psDyG1FDzX", "signatures": ["ICLR.cc/2026/Conference/Submission15102/Reviewer_DQVi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15102/Reviewer_DQVi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760653526259, "cdate": 1760653526259, "tmdate": 1762925427285, "mdate": 1762925427285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a formal bridge between LLMs and Algorithmic Information Theory (AIT), arguing that (i) LLM training ``computationally approaches'' the Solomonoff prior by interpreting loss minimization as an implicit program-length optimization, and (ii) LLM next-token prediction acts as a computable surrogate for Solomonoff induction under an approximation assumption. It then uses this lens to give unified explanations for in-context learning, few-shot learning, and scaling laws, and introduces a few-shot example selection rule (pick low-confidence correct examples). This selection rule improves accuracy on three text-classification benchmarks (SMS, emotion, and AG News) with Qwen and Llama models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents an interesting theoretical connection between LLMs and the Solomonoff prior. \n\n2. The paper argues how context, few examples, and more compute/parameters drive predictions toward a target distribution. The authors show that low-confidence correct examples are more beneficial for ICL than easy examples."}, "weaknesses": {"value": "1. The novelty of the paper lies in the connection between LLMs and Solomonoff induction, but from the perspective of the evaluation (ICL with correct samples that have low confidence) this has limited novelty. \n\n2. Experiments evaluate only few-shot classification with confidence-based selection across three datasets and four instruction-tuned models. There’s no ablation on prompt length, budget K, alternative selection criteria (entropy, gradient-free influence, diversity), or tasks beyond classification.\n\n3. Results show consistent gains for ``low-confidence'' selection, but it is not clear where the models make mistakes. An error analysis would be good. \n\n4. The paper would benefit from a proofread. See for example, ``Thus, the LLM as a whole functions as a deterministic Turing machine.''"}, "questions": {"value": "What kinds of errors do the models make?\n\nHow are the samples in the prompt selected (beyond being correct and low confidence?)\n\nHow many samples in the prompt are there?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x3l7IGyFH0", "forum": "psDyG1FDzX", "replyto": "psDyG1FDzX", "signatures": ["ICLR.cc/2026/Conference/Submission15102/Reviewer_uMSW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15102/Reviewer_uMSW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125268450, "cdate": 1762125268450, "tmdate": 1762966457045, "mdate": 1762966457045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors analyze the well-known properties and capabilities of autoregressive LLMs through the lens of Solomonoff priors and induction. In particular, they claim that LLM training can be framed as learning a computable approximation to the Solomonoff prior over language sequences (i.e., log-loss minimization in LLM training is implicitly a search over minimum description-length programs that generated the data), and that this result implies that LLM inference can be interpreted as approximate Solomonoff induction. Based on these connections, they analyze the in-context learning behavior of LLMs. They propose a few-shot example selection approach based on their framework and empirically validate its effectiveness on a small number of text-based prediction tasks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper proposes a theoretical framework for understanding in-context learning for autoregressive LLMs.\n- Based on the theoretical connections claimed in the paper, the authors propose and empirically validate a method for few-shot example selection, which led to improved performance on a few text-based prediction tasks.\n- The paper provides succinct descriptions of relevant background and the overall setup in an accessible manner, and the writing is generally clear and easy to follow."}, "weaknesses": {"value": "- While the discussed connections to Solomonoff induction are conceptually appealing, the core finding seems to be that language modeling reduces to compression (last paragraph of Section 4.1). This has been noted in prior works [1] and therefore potentially limits the novelty of findings. I think the paper could benefit from explicitly articulating their main novel contributions in contrast to existing works in this space.\n- An aspect that is central to the analysis seems to be the assumption that $M(x_{1:t}) \\approx \\bar{M}(x_{1:t})$, but this claim is not theoretically or empirically justified. In particular, Section 4.1 provides a conceptual recipe for how one could construct computable approximations to the Solomonoff prior from LLMs, but it is unclear whether this is indeed a \"good\" approximation. Subsequent sections hinge on whether this assumption, and I think the theoretical grounding is overall somewhat weak.\n- Empirical support of the main findings of the paper are limited. The only experiment included is on their few-shot example selection approach, which only pertains to one specific implication of the proposed framing and limited in scope.\n- The notations used throughout the paper could be improved for better clarity.\n\nReferences:\n[1] Language Modeling is Compression (Delétang et al., 2024)"}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "guQ9iEIJDU", "forum": "psDyG1FDzX", "replyto": "psDyG1FDzX", "signatures": ["ICLR.cc/2026/Conference/Submission15102/Reviewer_1Ner"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15102/Reviewer_1Ner"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132729910, "cdate": 1762132729910, "tmdate": 1762925425283, "mdate": 1762925425283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper points out that explaining the powerful emergent capabilities of LLMs through a unified mathematical framework remains a challenge and further proposes a formal theoretical framework to bridge the LLM architectures and algorithmic information theory. The paper proves two essential insights regarding LLMs: (1) the training process of LLMs computationally approaches Solomonoff prior, and (2) LLMs' next-token prediction implements a form of surrogate Solomonoff induction."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This paper focuses on theoretical understanding of LLMs' mechanisms, which is an essential problem.\n- The paper bridges the LLM architectures and algorithmic information theory, which provides a new perspective of understanding LLMs."}, "weaknesses": {"value": "- The writing of the paper is not clear and should be further polished. For example, since $M$ is not defined before, it is confusing to see it in the Abstract section.\n- The paper seems to overclaim its contribution. The connection between the Turing Machine and LLMs is not clear. The authors just view the LLMs as Turing Machines and lack the corresponding explanations or demonstrations.\n- The proofs are not well managed and arranged."}, "questions": {"value": "- The definition in Section 3 formulates the mechanism of LLMs in a text completion mode. Is this definition established for all cases in this paper?\n- What is the working mechanism of a prefix UTM? What is the connection between the prefix UTM and LLMs? It would be better to provide clear explanations in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "7w6KI1obSt", "forum": "psDyG1FDzX", "replyto": "psDyG1FDzX", "signatures": ["ICLR.cc/2026/Conference/Submission15102/Reviewer_TAUy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15102/Reviewer_TAUy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141307736, "cdate": 1762141307736, "tmdate": 1762925424360, "mdate": 1762925424360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel theoretical framework arguing that Large Language Models (LLMs) can be formally understood as computable surrogates for the uncomputable Solomonoff induction. The paper's primary contribution is establishing this link via two claims: first, that LLM training is a computable approach to the Solomonoff prior by reframing loss minimization as program length optimization, and second, that LLM inference implements surrogate induction. This AIT-based theory motivates a practical and counter-intuitive few-shot selection strategy: prioritizing samples where the model has the *lowest* predictive confidence. This low-confidence method is empirically validated on text classification benchmarks, demonstrating significant performance gains over a high-confidence baseline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Reasons to Accept\n\n- High Novelty: The paper establishes the first formal, constructive link between modern LLM architectures and the foundational principles of Algorithmic Information Theory (AIT).   \n- Explanatory Power: The AIT framework provides a unified lens for explaining disparate emergent phenomena like in-context learning, few-shot adaptation, and scaling laws.\n- Practical Contribution: The deep theory leads directly to a practical, counter-intuitive, and effective few-shot selection algorithm (low-confidence sampling).   \n- Strong Empirical Validation: The low-confidence selection method consistently and significantly outperforms the high-confidence baseline across all tested models (Qwen, Llama) and datasets (SMS, EMOTION, AG NEWS)."}, "weaknesses": {"value": "Reasons to Reject\n\n\n- Theory-Experiment Mismatch: There's a disconnect between the universal scope of the AIT theory and the narrow scope of the experiments (three simple text classification tasks). \n- Simpler Alternative Explanations: The success of the low-confidence method can be explained by simpler, well-known concepts like \"hard negative mining,\" which the authors fail to discuss or rule out.\n- The paper's most critical limitation (the $M \\approx \\overline{M}$ assumption) is relegated to Appendix A 1 rather than being addressed upfront in the main paper."}, "questions": {"value": "1.  In the derivation of Theorem 3 (Section 4.2), what is the origin and justification for the $2t$ term in the approximation for $|e(x_{1:t})_{(2)}|$?\n2.  How do you disentangle your AIT-based explanation for the success of low-confidence sampling from simpler, established concepts like hard negative mining? A comparison against a *random selection* baseline seems essential and is currently missing.\n3.  Have you tested your few-shot selection method on more complex, \"algorithmic\" reasoning tasks (e.g., math or code generation) where a \"universal induction\" framework would be more meaningfully tested?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rgfN0MJ4x6", "forum": "psDyG1FDzX", "replyto": "psDyG1FDzX", "signatures": ["ICLR.cc/2026/Conference/Submission15102/Reviewer_4cuM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15102/Reviewer_4cuM"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15102/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762236445249, "cdate": 1762236445249, "tmdate": 1762925423904, "mdate": 1762925423904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}