{"id": "RuIPoQBbNW", "number": 5615, "cdate": 1757922991201, "mdate": 1762919016847, "content": {"title": "VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning", "abstract": "Visual effects (VFX) are crucial for the expressive power of digital media, yet their creation remains a major challenge for generative AI. Prevailing methods often rely on the one-LoRA-per-effect paradigm, which is resource-intensive and fundamentally incapable of generalizing to unseen effects, thus limiting scalability and creation. To address this challenge, we introduce VFXMaster, the first unified, reference-based framework for VFX video generation. It recasts effect generation as an imitation task, enabling it to reproduce diverse dynamic effects from a reference video onto a target content. Critically, it demonstrates remarkable generalization to unseen effect categories. Specifically, we design an in-context conditioning strategy that prompts the model with a reference example. We use an in-context attention mask to precisely decouple and inject the essential effect attributes, allowing a single unified model to master the effect imitation without information leakage. In addition, we propose an efficient one-shot effect adaptation mechanism to boost generalization capability on tough unseen effects from a single user-provided video rapidly. Extensive experiments demonstrate that our method effectively imitates various categories of effect information and exhibits outstanding generalization to out-of-domain effects. To foster future research, we will release our code, models, and a comprehensive dataset to the community.", "tldr": "We introduce VFXMaster, the first universal, reference-based framework for visual effect video generation.", "keywords": ["Computer Vision", "Diffusion Model", "Controllable Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/97321615e3fb0685ee39e762423d7a9999ee3277.pdf", "supplementary_material": "/attachment/9b00874d4598544ffef33a5703b3c1e19e7bd8d7.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents VFXMaster, a diffusion model that can transfer the visual effect from a reference video to a target video. The core idea is to employ a reference-based in-context learning method, where both reference and target prompt and video sequences are concatenated and fed into a MM-DiT model. An in-context attention mask is used to guarantee the information flow and prevents leakage of non-effect content. In additional, the author also proposes one-shot effect adaptation when the test effect is out of the domain. A small set of newly-inserted concept enhance tokens is fine-tuned on the single reference video while the base model is frozen."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework does not need one lora per effect, increasing the scalability of the model.\n\n2. Strong empirical results are presented. It achieves better performance compared to competitors like VFX Creator and Omini-Effects."}, "weaknesses": {"value": "1. The major concern is the limited novelty of the proposed method. The proposed in-context conditioning for VFX generation is quite straightforward. The example-query in-context learning is already common in the generation field, many works adopts a similar idea (i.e. IP-Adapter, PuLID). The in-context attention mask is also not new.\n\n2. There is no comprehensive studies on the design of attention mask. In ablation, only with and without attention mask results are compared. However, more ablation on different choices of masks are needed.\n\n3. The proposed in-context conditioning can only be applied to MM-DiT framework like CogVideoX. For recent state-of-the-art framework like Wan, which adopts the cross attention mechanism, the proposed method is not applicable. This significantly reduce the generalization ability of the method.\n\n4. Also, adopting such in-context conditioning will significantly increasing the training and inference cost, as the attention window size is significantly enlarged."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WfxSqnvcb2", "forum": "RuIPoQBbNW", "replyto": "RuIPoQBbNW", "signatures": ["ICLR.cc/2026/Conference/Submission5615/Reviewer_umU3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5615/Reviewer_umU3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761471076982, "cdate": 1761471076982, "tmdate": 1762918161015, "mdate": 1762918161015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "d6snNvXwTJ", "forum": "RuIPoQBbNW", "replyto": "RuIPoQBbNW", "signatures": ["ICLR.cc/2026/Conference/Submission5615/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5615/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762919015549, "cdate": 1762919015549, "tmdate": 1762919015549, "mdate": 1762919015549, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VFXMaster, a novel framework for generating dynamic visual effects (VFX) in videos. The core problem it addresses is the poor scalability and lack of generalization of existing methods, which often rely on a \"one-LoRA-per-effect\" paradigm. VFXMaster reframes VFX generation as a reference-based imitation task, leveraging in-context learning. Given a reference video showcasing a specific effect and a target image, the model can reproduce that effect on the target content.\n\nThe main contributions are:\n\n1. A Unified, Reference-Based Framework: VFXMaster is proposed as the first unified model capable of learning a general VFX imitation capability from a diverse dataset, rather than memorizing a closed set of effects.\n\n2. In-Context Conditioning with an Attention Mask: The model is conditioned on a reference prompt-video pair as an \"example\" and a target prompt-image pair as a \"query\". A novel in-context attention mask is designed to isolate and inject only the essential effect attributes from the reference, preventing content leakage (e.g., transferring the background or subject from the reference video).\n\n3. Efficient One-Shot Effect Adaptation: To improve generalization to challenging, Out-of-Domain (OOD) effects, the paper proposes a method to fine-tune a small set of \"concept-enhance tokens\" on a single user-provided video, allowing the model to quickly adapt to new effects with minimal computational cost.\n\n4. A New Dataset and Evaluation Metric: The authors have collected a new dataset of 10k VFX videos and propose a new VLM-based evaluation metric, VFX-Cons., to assess effect generation quality in terms of occurrence, fidelity, and content leakage.\n\nThe experiments demonstrate that VFXMaster outperforms existing methods on in-domain effects and shows strong generalization to unseen effects, particularly when augmented with the one-shot adaptation mechanism."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel Problem Formulation: The most significant strength is the shift from specialized, closed-set VFX models to a unified, general-purpose imitation framework. By framing the task as in-context learning, the paper presents an elegant solution to the critical challenges of scalability and generalization that have limited prior work.\n\n2. Effective Architectural Design: The in-context attention mask is a crucial and well-motivated component. The ablation study convincingly demonstrates its necessity, showing a catastrophic drop in performance without it. This highlights that the authors have not just concatenated inputs but have carefully engineered the information flow to achieve the desired outcome (effect transfer without content leakage).\n\n3. Strong Empirical Evaluation: The paper is supported by a thorough and multi-faceted experimental evaluation. The inclusion of quantitative metrics, qualitative comparisons, detailed ablation studies, a data scaling analysis, and a user study provides a robust body of evidence for the method's effectiveness.\n\n4. Valuable Community Resources: The commitment to release the model, code, and the newly curated 10k video dataset is a major strength. This will lower the barrier to entry for other researchers and serve as a benchmark for future work on generalizable VFX generation."}, "weaknesses": {"value": "1. Ambiguity and Potential Flaw in the VFX-Cons. Metric: The paper's new metric, VFX-Cons., is calculated as (EOS + EFS + CLS) / 3. However, the paper states, \"CLS is only meaningful when EFS is True.\" The formula does not reflect this dependency. For example, if a video has the effect occur (EOS=True) but the fidelity is wrong (EFS=False), what is the value of CLS? If it is judged as True (no leakage), the score would be (1 + 0 + 1) / 3 = 0.67. If it is judged as False, the score is (1 + 0 + 0) / 3 = 0.33. The CLS score seems ill-defined when EFS is false, and averaging them masks the individual performance and the logical dependency. This is a significant weakness in a claimed contribution.\n\n2. Fairness of Baseline Comparison: The baseline CogVideoX* is fine-tuned on the same data, but it lacks the reference-based architectural modifications of VFXMaster. The comparison therefore demonstrates that the proposed architecture is better than a generic I2V model for this task, which is expected. A stronger baseline might have involved a simpler method for incorporating reference information into CogVideoX (e.g., via cross-attention to the reference video's features) to better isolate the benefits of the proposed in-context learning paradigm versus simpler reference-based conditioning.\n\n3. Lack of Discussion on Limitations and Failure Cases: The paper presents very successful qualitative results but does not include a discussion on the method's limitations or common failure modes. For which types of effects does it still struggle? Does the one-shot adaptation sometimes lead to overfitting on the single reference video? For instance, does it fail to separate style from motion in complex effects? A dedicated \"Limitations\" section would make the work more complete and scientifically rigorous.\n\n4. Minor Terminological Point on \"One-Shot\": The \"one-shot effect adaptation\" involves fine-tuning on a single video with data augmentations. This creates a mini-dataset from one sample for multiple training steps. While this is an efficient adaptation, the term \"one-shot\" can sometimes imply inference with a single example without any gradient updates. Clarifying the exact number of training steps and the process would add precision to the \"efficient\" claim."}, "questions": {"value": "1. Regarding the VFX-Cons. metric: Could you please clarify how the final score is calculated, specifically addressing the dependency of CLS on EFS? The current formula (EOS + EFS + CLS) / 3 seems to ignore this dependency. How is CLS evaluated if EFS is False? Would a hierarchical or weighted scoring system that reflects this dependency be more appropriate? Also, which specific VLM was used for the evaluation (the paper cites a future work, Comanici et al., 2025)?\n\n2. Regarding the baseline comparison: Can you elaborate on why the direct fine-tuning of CogVideoX is considered a sufficient baseline? Did you consider or experiment with any alternative, simpler ways to condition the baseline model on the reference video, which might provide a more direct comparison to your in-context learning approach?\n\n3. Could you discuss the primary failure modes or limitations of VFXMaster? Are there specific categories of visual effects (e.g., those involving complex fluid dynamics, intricate topological changes, or very subtle textural effects) where the model's performance degrades?\n\n4. For the \"efficient one-shot effect adaptation,\" could you provide more detail on the fine-tuning process? Specifically, how many training steps or epochs are performed on the augmented data from the single video? This would help in quantifying the actual computational cost and \"efficiency\" of the adaptation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u4mGAtB2JV", "forum": "RuIPoQBbNW", "replyto": "RuIPoQBbNW", "signatures": ["ICLR.cc/2026/Conference/Submission5615/Reviewer_P4Mo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5615/Reviewer_P4Mo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813165095, "cdate": 1761813165095, "tmdate": 1762918160417, "mdate": 1762918160417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper solves the problem of visual effect generation. Most previous work mostly adopt the one-LoRA-per-effect framework to generate visual effect. Differently, this paper designs a unified, reference-based pipeline for visual effect video generation. An in-context attention mask is introduced to decouple and inject the essential effect attributes. This enables a single unified model to handle the effect imitation without information leakage. An efficient one-shot effect adaptation mechanism is also proposed to boost the generalization capability of the proposed approach on difficult unseen effects from a single user-provided video rapidly.\n\nExperiments show that the proposed approach receives good results compared to previous works and the authors claim that the code associated with this paper would be made publicly available."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper presents a unified reference-based pipeline for visual effect video generation with an in-context attention mask. Compared to previous works, the motivation of this paper is clear. Instead of tuning one lora for each visual effect, this paper aims to handle all visual effects in a single framework, which is meaningful for this research topic.\n\n- The visual results and numerical results look great compared to previous works."}, "weaknesses": {"value": "- In the introduction section, the authors say that most previous VFX generation methods are based on Lora finetuning and they list many references in the second and third paragraphs. However, in the experiment section, it seems that the authors did not compare their approach with these mentioned works. It is hard to say that the proposed approach performs better than these models.\n\n- Presenting a unified model for VFX generation is an interesting work. However, the technical contributions of this paper is not significant enough. The masking strategy has already been used in previous LLM design. Though the proposed masking strategy does work in the proposed framework, I cannot see some new techniques are proposed.\n\n- In the caption of Fig. 2, the authors split it into three parts. However, the authors did not clearly explain which point corresponds to which part in the figure. I think this should be further explained.\n\n- The datasets scaling part in the ablation study looks strange. When we talk about scaling, we often refers to data of different magnitudes. This paper only shows the results based on the 10k dataset. Maybe the title of the paragraph should be changed."}, "questions": {"value": "- The fond size in most figures are really small. It really takes me some time to see the content.\n\n- In addition, the authors are also encouraged to reorganize the figure presentations. They really look too small as I see there are still some spaces that are not used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LmePxWwLdo", "forum": "RuIPoQBbNW", "replyto": "RuIPoQBbNW", "signatures": ["ICLR.cc/2026/Conference/Submission5615/Reviewer_CQuf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5615/Reviewer_CQuf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877522063, "cdate": 1761877522063, "tmdate": 1762918159969, "mdate": 1762918159969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a reference-based framework for visual effect (VFX) video generation, called VFXMaster. Specifically, by learning from reference effects via in-context learning, VFXMaster integrates diverse effects into a single model. In addition, with a small set of learnable concept-enhance tokens, VFXMaster can deal with Out-of-Domain (OOD) effects. Extensive experiments demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The writing is fluent and logically coherent, exhibiting strong readability.\n* The proposed method is highly efficient, requiring only a small number of model parameters to be fine-tuned in order to learn various VFX effects.\n* Comprehensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "* The proposed method lacks novelty. For the in-domain training part, it appears to be a straightforward extension of Custom Diffusion to the video domain. The out-of-domain part, on the other hand, resembles a modified version of prompt tuning.\n* The ablation study appears somewhat coarse. According to Table 2, the Attention Mask has a significant impact on the performance of VFXMaster. Conducting a single, superficial ablation on the Attention Mask is insufficient. It would be helpful to adjust the Attn Mask shown in Figure 1 or visualize the interactions between different Q and V pairs within the attention weights.\n* For the training of the OOD VFX, how does the time cost compare to injecting a new VFX concept from scratch?"}, "questions": {"value": "See 'Weaknesses'.\nIf my questions can be addressed, I would be happy to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xtxrRcewGL", "forum": "RuIPoQBbNW", "replyto": "RuIPoQBbNW", "signatures": ["ICLR.cc/2026/Conference/Submission5615/Reviewer_5k99"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5615/Reviewer_5k99"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902174554, "cdate": 1761902174554, "tmdate": 1762918159662, "mdate": 1762918159662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}