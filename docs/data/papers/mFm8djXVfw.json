{"id": "mFm8djXVfw", "number": 14428, "cdate": 1758235143192, "mdate": 1759897371042, "content": {"title": "Empirical Robustness of Pixel Diffusion Undermines Adversarial Perturbation as Protection against Diffusion-based Mimicry", "abstract": "Diffusion models have demonstrated impressive abilities in image editing and imitation, raising growing concerns about the protection of private property. A common defense strategy is to apply adversarial perturbations that can mislead a diffusion model into generating bad-quality images. However, existing research has almost entirely focused on latent diffusion models while overlooking pixel-space diffusion models. Through extensive experiments, we show that nearly all attacks designed for latent diffusion models, as well as adaptive attacks aimed at pixel-space diffusion models, fail to compromise the latter. Our analysis suggests that the weakness of latent diffusion models arises mainly from their encoder, whereas pixel-space diffusion models exhibit strong empirical robustness to adversarial perturbations. We further demonstrate that pixel-space diffusion models can serve as an effective purifier by removing adversarial patterns generated for latent diffusion models and preserving image integrity, which in turn allows them to bypass most existing protection schemes. These findings challenge the assumption that adversarial perturbations provide reliable protection for diffusion models and call for a reevaluation of their role as a protection mechanism.", "tldr": "", "keywords": ["diffusion model;style mimicry;protection;safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dff2f5d2d66074839d91ca709fb8a6d98efeeb03.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper examines existing methods that protect images from unauthorized editing by generative diffusion models through adversarial perturbations. Based on extensive experiments, the authors find that these protection mechanisms are largely effective only against Latent Diffusion Models (LDMs), while proving ineffective against Pixel-space Diffusion Models (PDMs). Their empirical analysis attributes this robustness gap to the encoder in LDMs, which amplifies perturbations, whereas PDMs perform denoising directly in pixel space, preserving a higher distributional overlap. Leveraging this observation, the authors propose PDM-Pure, a simple purification framework built upon strong PDMs (implemented via SDEdit with DeepFloyd-IF). This demonstrates that it can effectively remove protective adversarial perturbations and bypass existing defense methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed research gap is potentially impactful, urging the community to rethink the image protection manners against Pixel-space\nDiffusion Models (PDMs).\n- Clear empirical separation between LDMs and PDMs. The paper evaluates multiple architectures (U-Net and DiT), datasets/resolutions, and budgets. Consistently, attacks devastate LDMs but have a negligible effect on PDMs. The breadth of models/perturbations tested strengthens the central empirical claim.\n- The paper proposes a simple, practical purifier with strong results. PDM-Pure is appealingly minimal—SDEdit with a strong PDM—and yet defeats several state-of-the-art protections. Showing cross-task recovery (inpainting, textual inversion, LoRA) is particularly compelling for the security threat narrative (i.e., protections are not merely degraded; they can be bypassed).\n- Writing and presentation. The exposition is clear; the figures communicate well; the listed attack variants and experimental knobs are presented in reasonable detail."}, "weaknesses": {"value": "- **Empirical robustness $\\neq$ robustness**. The paper frames PDM robustness almost as a property of the pixel space itself. But the evidence is empirical and limited to a set of popular PDMs, datasets, and attack templates. Robustness guarantees are not established; the Lipschitz bound sketch is suggestive but not a model-tight, formal theoretical guarantee.\n- **Attack surface is slightly narrow.** While the authors implement many known ingredients (PGD/SDS/EoT/feature-layer attacks), several ideas are not explored, for example: Non-$\\ell_\\infty$ budgets (e.g., spectral/patch/structure aware constraints) that are still imperceptible but couple to denoiser receptive fields.\n- More Experiments can be added to further strengthen the claim: (1) Different noise budgets and norms: Beyond $\\ell_\\infty$ up to 16/255;\nvisibility-matched $\\ell_2$; structured perturbations. (2) PDM denoiser feature analysis against attacks:  Analysis of the statistical characteristics of the latent space of the PDM’s denoiser against different attack methods is suggested to further justify the robustness. (3) Human studies: Perceptual integrity and style preservation after purification (especially for artists).\n- **Security framing is slightly narrow.** Even if adversarial-noise protections erode after the proposed PDM-Pure, alternate defenses (e.g., robust watermarking/fingerprints) remain. The paper would be stronger by situating results within a broader countermeasure, clarifying what is and isn’t undermined. For example, can your PDM-Pure remove the watermark proposed in [1]? \n\n### Minor:\n- **Slight overgeneralization in the conclusions.** Statements like “no existing attacks have proven effective in attacking PDMs, which means no protection can be achieved by fooling a PDM-based image editor” on PDMs read stronger than the evidence warrants.\nThe experiments are solid but cannot support universality claims.\n\n[1] Wen, Yuxin, et al. Tree-ring watermarks: Fingerprints for diffusion images that are invisible and robust, NeurIPS 2023"}, "questions": {"value": "- According to the paper [1], The Attack 7 is to craft the adversarial perturbation in a pre-trained VAE latent space to create perceptual subtle perturbation and the attacking loss is defined by the feature distance extracted by the PDM’s denoising network. But in the Appendix F you stated that the feature is extracted by the middle-layer of VAE. Did you implement their method correctly? Could you provide the pseudocode and parameters of your implementation for clarification? \n- The attack budget is defined as pixel-wise difference, do you think there are other perceptual metrics can be the budget to stress test the PDMs? For example, maybe when the pixel-wise difference is high, the perceptual loss is still at a reasonable level, and the attack might still be successful. How will you justified that your empirical robustness is not realized by relative low attack budget tested?\n\n[1] Shih et. al. Pixel Is Not a Barrier: An Effective Evasion Attack for Pixel-Domain Diffusion Models, AAAI 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XwijznycyG", "forum": "mFm8djXVfw", "replyto": "mFm8djXVfw", "signatures": ["ICLR.cc/2026/Conference/Submission14428/Reviewer_K2WL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14428/Reviewer_K2WL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717777505, "cdate": 1761717777505, "tmdate": 1762924835938, "mdate": 1762924835938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on an important gap on adversarial robustness of pixel-space diffusion models (PDMs). Empirical results show that successful adversarial attacks on Latent Diffusion Models (LDMs) cannot work well on PDMs. The authors also provide some theoretical analysis. A simple defense technique has also been proposed to bypass existing protecting schemes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper highlights that PDMs exhibit notable robustness against adversarial attacks, in contrast to LDMs. The authors support this claim with both empirical results and theoretical analysis, providing convincing evidence for their findings.\n2. The paper introduces a reasonable and effective technique to bypass perturbation-based protection schemes. The proposed approach is well-motivated and demonstrates practical utility in overcoming existing defenses."}, "weaknesses": {"value": "1. Simple Method. The proposed approach is straightforward; however, it does not introduce a novel or effective attack method for PDMs. Additionally, the contribution of PDM-Pure appears limited. Given the significant findings presented in the paper, it would be valuable for the authors to explore ways to enhance the adversarial robustness of LDMs, such as by modifying their architectures or tuning their parameters.\n2. Lack of In-depth Theoretical Analysis. From Lines 313 to 315, the authors primarily incorporate previous empirical results into their theoretical analysis, which limits the rigor of the theoretical framework. To strengthen the conclusions presented in Line 320, the authors are encouraged to provide more detailed and comprehensive theoretical results.\n3. Lack of Important Baseline. The paper only presents a few bad cases for Diffpure in Figure 17, which is insufficient for a comprehensive evaluation. Moreover, detailed results for Diffpure are missing from Table 3. \n4. Minor proofreading issues. \n    (1) Inappropriate citation format: The citation formats in Line 313 and Line 361 should be checked, particularly the usage of \\citet and \\citep.\n    (2) Spelling errors: There are several spelling issues, such as “textcollorbule” in Line 221 (which should be properly formatted in LaTeX), “Moresults” in Line 239 (which may be intended as “More results”), and “since” in Line 320 (which should be capitalized as “Since”)."}, "questions": {"value": "1. Can the authors propose a new method to attack PDMs? If not, can they enhance the PDM-Pure algorithm by considering at least one additional metric (e.g., FID score, inference time, etc.), as suggested in Weakness 1?\n2. Can the authors strengthen the theoretical analysis, as discussed in Weakness 2?\n3. Can the authors provide detailed experimental results for Table 3, as highlighted in Weakness 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bi5d4Jw4VV", "forum": "mFm8djXVfw", "replyto": "mFm8djXVfw", "signatures": ["ICLR.cc/2026/Conference/Submission14428/Reviewer_XxTy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14428/Reviewer_XxTy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804401858, "cdate": 1761804401858, "tmdate": 1762924835455, "mdate": 1762924835455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work shows that private images protection using current adversarial perturbation methods is not a valid protection. The authors analyze the differences of LDMs and PDMs, and found that the generations of the latter does not suffer from adversarial perturbations in protected images, allowing them to bypass many proposed protections. The authors propose to use this PDMs ability to build a strong adversarial perturbation watermark-purifier, showing that it succeeds in removing protections, with much smaller caveats in generative quality compared to previous purifiers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work seems significant. While removing watermarks is not an exactly a hard task, it is interesting to see why adversarial perturbations could compromise generation ability of LDMs and not PDMs. The writing is well organized, and easy to follow. The authors present some interesting insights and a wide range of attacks and purification methods comparison. Small thing, but the authors are also helping the reader with information where it would be good to read a pdf and zoom-in image to see the qualitative difference."}, "weaknesses": {"value": "- The figures are referenced far from where they are in the paper, it is a bit unordered\n- The limitations were not discussed\n- directions or other methods of private image protection against proposed purifier are not discussed, only stated as potential future work"}, "questions": {"value": "What are the limitations?\nIs your purification method costly compared to other purifiers?\nWhat are the possible directions to create a better defenses as mentioned near the conclusion as future work? Do you have any experiments on that?"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "The authors present a much better than before watermark eraser with no proposition of a better protection for private images."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XYMRsiURug", "forum": "mFm8djXVfw", "replyto": "mFm8djXVfw", "signatures": ["ICLR.cc/2026/Conference/Submission14428/Reviewer_SZtC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14428/Reviewer_SZtC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814091752, "cdate": 1761814091752, "tmdate": 1762924834367, "mdate": 1762924834367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper compares latent diffusion models (LDMs) and pixel-space diffusion models (PDMs) under \"adversarial perturbations\" designed to protect copyright. Under a unified PGD-style diffusion attack, the authors report that LDMs degrade substantially while PDMs remain largely unaffected. Motivated by this, they propose PDM-Pure: running SDEdit in pixel space with a strong PDM to \"purify\" protected images and restore editability. The paper provides (i) attack-side comparisons across budgets and metrics for LDM vs PDM, and (ii) a purification table where multiple published protections (SDS/Photoguard/Mist/AdvDM/etc.) are evaluated for SDEdit using FID. \n\nOverall, the paper shows that PDMs exhibit strong empirical robustness to adversarial perturbations that reliably compromise LDMs, the authors attribute the gap to encoder vulnerability, and propose PDM-Pure against adversarial perturbations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear and repeatable LDM–PDM gap under a unified attack protocol\n\n- Useful mechanistic intuition (PDMs exhibit significantly stronger empirical robustness compared to LDMs, with some possible direction of explanations)\n\n- Practical and simple PDM-Pure purifier\n\n- Purification evaluation covers many published perturbations\n\n- Relevance to real editing pipelines (inpainting/TI/LoRA)"}, "weaknesses": {"value": "Writing & Presentation:\n- Table 3 and Figure 6 should be adjusted for clearer flow.\n\nTheory, Methods & Experiments:\n1. The inference \"no existing attacks have proven effective against PDMs, therefore no protection can be achieved by fooling a PDM-based image editor\" does not follow. The reason current protective perturbations primarily target LDMs is that LDMs are more advanced and efficient, and thus far more widely used in academia and industry than PDMs.\n\n2. The claim that \"LDMs are more vulnerable than PDMs\" is mainly built on a unified PGD-style attack; however, representative published perturbations (e.g., SDS/Photoguard/Mist/Glaze/AdvDM) are not tested in table1. This weakens the persuasiveness of the central claim.\n\n3. The methodological novelty is limited. PDM-Pure is essentially SDEdit in pixel space; the contribution leans more toward a systematic empirical study and a paradigm recommendation, and it requires broader and more rigorous evidence.\n\n3. For “THE ENCODER IS VULNERABLE,” the experimental support should include corresponding ablation studies to strengthen the causal interpretation. (Also, to my knowledge, some published work has already noted this point.)\n\n4. The goal of adversarial perturbation is to protect data, while image generation encompasses multiple task types (e.g., identity learning, style mimicry, image editing). However, the key Table 3 evaluates only image editing and reports only FID. How does PDM-Pure perform on other task types? (Not only the visual comparison, but also the quantitative results)\nNeed more experiments on additional image-generation tasks and including more metrics in the key table to validate the method’s effectiveness."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "This method is designed to remove adversarial noise from images—even though such noise is used to protect data copyright—thereby posing a clear dual-use risk: the proposed 'purifier' could be used to defeat protection tools deployed by creators and rights holders."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xlSg71EpAe", "forum": "mFm8djXVfw", "replyto": "mFm8djXVfw", "signatures": ["ICLR.cc/2026/Conference/Submission14428/Reviewer_Sk76"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14428/Reviewer_Sk76"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997292433, "cdate": 1761997292433, "tmdate": 1762924833877, "mdate": 1762924833877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}