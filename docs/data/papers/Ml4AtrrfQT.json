{"id": "Ml4AtrrfQT", "number": 18686, "cdate": 1758290112274, "mdate": 1759897087658, "content": {"title": "Peng's Q($\\lambda$) for Conservative Value Estimation in Offline Reinforcement Learning", "abstract": "We propose a model-free offline multi-step reinforcement learning (RL) algorithm, Conservative Peng's Q($\\lambda$) (CPQL).\nOur algorithm adapts the Peng's Q($\\lambda$) (PQL) operator for conservative value estimation as an alternative to the Bellman operator.\nTo the best of our knowledge, this is the first work in offline RL to theoretically and empirically demonstrate the effectiveness of conservative value estimation with the *multi-step* operator by fully leveraging offline trajectories.\nThe fixed point of the PQL operator in offline RL lies closer to the value function of the behavior policy, thereby naturally inducing implicit behavior regularization.\nCPQL simultaneously mitigates over-pessimistic value estimation, achieves performance greater than (or equal to) that of the behavior policy, and provides near-optimal performance guarantees --- a milestone that previous conservative approaches could not achieve.\nExtensive numerical experiments on the D4RL benchmark demonstrate that CPQL consistently and significantly outperforms existing offline single-step baselines.\nIn addition to the contributions of CPQL in offline RL, our proposed method also contributes to the framework of offline-to-online learning.\nUsing the Q-function pre-trained by CPQL in offline settings enables the online PQL agent to avoid the performance drop typically observed at the start of fine-tuning and attain robust performance improvement.", "tldr": "This paper introduces CPQL: Conservative Peng's Q($\\lambda$), mitigates overly-pessimistic value estimation, achieves the performance greater than (or equal to) that of the behavior policy, and provides near-optimal performance guarantees.", "keywords": ["Offline reinforcement learning", "Offline-to-online settings", "Multi-step operator"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/471c939ced2dfbdbf33cd056e8a3d45fbc0f33b8.pdf", "supplementary_material": "/attachment/68ffef4d75528ac789c8d95bdc44f928e5a029fd.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents Conservative Peng’s $Q(\\lambda)$ (CPQL), a model-free offline multi-step reinforcement learning algorithm that utilizes the Peng’s $Q(\\lambda)$ operator for conservative value estimation. CPQL achieves superior performance on standard benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The experimental evaluation is comprehensive.\n2. The algorithm demonstrates superior performance across the majority of tasks."}, "weaknesses": {"value": "1. A primary concern is that the claim of \"multi-step\" does not intuitively address the over-optimistic value estimation challenge in offline reinforcement learning. If the multi-step approach were fundamentally effective, it raises the question of why conservative value estimation, which is widely used in offline RL, is still necessary.\n\n2. The authors assert that their method mitigates over-pessimistic Q-values, a challenge already addressed in other approaches such as MCQ. A comparison of the Q-value curves between CPQL and MCQ would help clarify the extent of this improvement.\n\n3. Can CPQL be viewed as a result of simply modifying the target value $y$ in CQL? Would it be possible to perform a similar substitution in MCQ to further demonstrate the effectiveness of the \"multi-step\" approach? What impact would this have on the results?\n\n4. While the authors conduct offline-to-online experiments, they do not compare their method against recent state-of-the-art algorithms, such as WSRL, which has demonstrated superior performance in the AntMaze domain."}, "questions": {"value": "Please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a6dIJ8YPU1", "forum": "Ml4AtrrfQT", "replyto": "Ml4AtrrfQT", "signatures": ["ICLR.cc/2026/Conference/Submission18686/Reviewer_qHcg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18686/Reviewer_qHcg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638107756, "cdate": 1761638107756, "tmdate": 1762928388218, "mdate": 1762928388218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "CPQL (Conservative Peng's $\\mathrm{Q}(\\lambda)$ ) integrates the conservative Q penalty structure of Conservative Q-Learning (CQL) with the multi-step backup of Peng's $\\mathrm{Q}(\\lambda)$. To address the limitation of standard CQL, which suffers from restricted reward propagation leading to over-pessimism, CPQL introduces $\\lambda$ to leverage trajectory-level information for long-term reward propagation. This allows the algorithm to stably utilize multi-step information without importance sampling while implicitly capturing the characteristics of the behavior policy in its Q-values. While empirical results demonstrates that CPQL outperforms existing baselines on the D4RL benchmark, the authors further provide theoretical evidence to prove that CPQL guarantees that the learned policy always performs at least as well as the behavior policy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "$\\textbf{(1)}$\n\nAccording to the authors, Conservative Peng's $Q(\\lambda)$ (CPQL) represents the first attempt to incorporate multi-step temporal difference learning into the offline reinforcement learning setting. The method combines Peng's $\\mathrm{Q}(\\lambda)$ operator with a conservative Q-penalty, aiming to utilize multi-step information through the hyperparameter $\\lambda$, while maintaining the stability of conservative value estimation through the hyperparameter $\\alpha$. Unlike many prior approaches that rely on additional auxiliary networks-such as behavior policy estimators, importance-sampling correction models, or ensemble-based uncertainty estimators, CPQL performs this integration without additional components. This design allows the algorithm to propagate reward signals more effectively across trajectories, potentially capturing richer temporal dependencies without introducing additional instability.\n\n$\\textbf{(2)}$\n\nCPQL extends the mixture policy convergence property of Peng's $Q(\\lambda)$ to the offline reinforcement learning setting, providing a theoretical analysis of how multi-step learning interacts with conservative value estimation. The study examines both the role of the hyperparameter $\\alpha$, which controls the degree of conservatism, and the parameter $\\lambda$, which determines how strongly the mixture policy reflects the characteristics of the behavior policy. Specifically, while Theorem 2 proves that the policy learned by CPQL guarantees performance that is at least equal to or better than that of the behavior policy, Theorem 3 shows that, with an appropriately chosen $\\lambda$, the sub-optimality gap can be reduced by effectively balancing conservatism and multi-step backup. Overall, these theoretical results strengthen the justification for CPQL's design choices and suggest that it provides an interpretable and mathematically grounded framework for policy improvement.\n\n\n$\\textbf{(3)}$\n\nWhile the conservative Q-penalty in CQL plays an important role in suppressing overestimation of out-of-distribution actions, it often leads to an overly pessimistic value function, which can limit policy improvement. To mitigate this issue, CPQL introduces a multi-step temporal difference update, allowing reward signals to be propagated over longer horizons within offline trajectories. By leveraging trajectory-level information, the algorithm captures a more accurate relationship between actions and long-term returns, alleviating the problem of over-pessimism without compromising the stability provided by conservative regularization. Such balance between conservatism and optimism contributes to stable performance not only during purely offline training, but also in off-to-on fine-tuning, where the learned policy can adapt to online interaction without suffering from the sharp performance drop often observed in overly conservative methods."}, "weaknesses": {"value": "$\\textbf{(1)}$ \n\nThe authors suggest that this work may represent an early attempt to extend Conservative Q-Learning into an N-step formulation, where multi-step temporal difference updates are incorporated within a conservative value estimation framework. However, the proposed method primarily integrates an existing algorithm, Peng's $Q(\\lambda)$, into the established CQL framework, and its core idea appears to be more of an adaptation or refinement of prior methodologies rather than a fundamentally new algorithmic contribution. Moreover, much of the theoretical basis related to PQL has already been extensively discussed in previous studies, and the theoretical analyses of CPQL closely follow the analytical structure developed for CQL. Consequently, while the paper provides an interesting perspective on reinterpreting conservative value learning through a multi-step lens, its methodological originality appears to be relatively limited compared to earlier works.\n\n$\\textbf{(2)}$ \n\nAccording to the authors, the interaction between $\\lambda$, which controls the degree to which the learned mixture policy follows the behavior policy, and $\\alpha$, which determines the level of conservatism, is designed to regulate the balance between stability and policy improvement. Theoretically, a higher-quality behavior policy is expected to benefit from a larger $\\lambda$, encouraging the learned policy to stay closer to the behavior policy, while a lower quality or inconsistent policy would require a smaller $\\lambda$ to prevent the accumulation of erroneous transitions. However, the experimental results in domains such as HalfCheetah do not appear to consistently reflect this theoretically suggested relationship. Moreover, although Figure 1 claims that the presence of $\\lambda$ makes $\\alpha$ less sensitive in CPQL compared to CQL, it is not entirely convincing from the presented results that such reduced sensitivity actually holds in practice.\n\n$\\textbf{(3)}$ \n\nWhile CPQL conceptually allows for richer reward propagation across trajectories, its empirical validation has so far been limited to relatively simple or short-horizon tasks within the D4RL benchmark. As a result, it remains uncertain whether the proposed method can generalize its effectiveness to more complex offline RL environments that involve longer temporal dependencies or higher behavioral variability. For instance, evaluations on more diverse and realistic benchmarks-such as non-goal conditioned settings in frameworks like OGBENCH: BENCHMARKING OFFLINE GOAL-CONDITIONED RL[1]-could provide further insight into whether CPQL consistently maintains a stable balance between conservatism and policy performance across a wider range of task dynamics. \n\n(reference)\n[1] Park, Seohong, et al. \"Ogbench: Benchmarking offline goal-conditioned rl.\" International Conference on Learning Representations (2025)"}, "questions": {"value": "$\\textbf{(1)}$\n\nGiven that CPQL is a multi-step method for offline reinforcement learning, it seems reasonable to consider the trajectory length $n$ as an important hyperparameter in the implementation.\nIn the current paper, $n$ is fixed to 5 for all experiments, but I believe that both the performance and the behavior of the hyperparameters $\\alpha$ and $\\lambda$ could vary significantly depending on the choice of $n$.\nCould the authors clarify the motivation for fixing $n=5$ in all experiments? Additionally, have the authors conducted (or considered) any experiments analyzing how performance or the interaction between $\\alpha$ and $\\lambda$ changes as $n$ varies?\n\n\n$\\textbf{(2)}$\n\nIn Figure 1, the experimental results on the sensitivity of the conservatism parameter $\\alpha$ suggest that, for the same $\\alpha$ values, CPQL tends to outperform CQL.\nMoreover, it appears that as $\\lambda$ increases, the sensitivity of CPQL to $\\alpha$ becomes smaller, indicating that the method may be more robust to the choice of $\\alpha$ under higher $\\lambda$ values.\nI am curious whether similar patterns were observed consistently across other tasks or environments.\n\nCould the authors provide additional results or discussions that show whether this trend holds beyond the task presented in Figure 1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AXNXxsAc5r", "forum": "Ml4AtrrfQT", "replyto": "Ml4AtrrfQT", "signatures": ["ICLR.cc/2026/Conference/Submission18686/Reviewer_MQiE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18686/Reviewer_MQiE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809109547, "cdate": 1761809109547, "tmdate": 1762928387668, "mdate": 1762928387668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Conservative Peng’s $Q(\\lambda)$ (CPQL) for offline RL. The main idea is to replace the standard Bellman operator in the conservative Q-learning (CQL) objective with the Peng’s $Q(\\lambda)$ operator. The authors present theoretical results showing that CPQL can achieve a tighter suboptimality gap than standard CQL, as the introduction of $\\lambda$ could be used to mitigate the CQL 's excessive pessimism. They also provide extensive numerical experiments and ablation studies demonstrating the effectiveness of CPQL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Interesting idea: Bringing Peng’s $Q(\\lambda)$ from online RL to the offline setting (where the behavior policy is fixed offline) is interesting, and can potentially motivate future research. \n\n2. Mitigates over-pessimism and improves robustness: Introducing $\\lambda$ alongside $\\alpha$ adds a practical control knob that balances conservatism and extrapolation risk, helping to mitigate excessive pessimism and showing reduced hyperparameter sensitivity in experiments.\n\n3. Theory is solid and sound.\n\n4. Comprehensive experiments and thorough ablations"}, "weaknesses": {"value": "1. Although the authors note in the last paragraph of Section 4.1 that we should focus on how an appropriately chosen $\\lambda$ mitigates Q-value overestimation rather than on the additional bias it introduces, I would still suggest adding a discussion of the impact of this introduced bias. One potential direction in my mind is to analyze how the bias affects the suboptimality gap of the learned policy $\\hat{\\pi}$ (not just the mixture policy), but other forms of analysis are also welcome. In short, it would help to quantify how much we gain vs. how much we sacrifice, by introducing Peng's $Q(\\lambda)$ into offline RL.\n\n2. In Algorithm 1, it is not fully clear which policy is ultimately evaluated/deployed: the mixture $\\lambda \\pi_\\beta +(1-\\lambda)\\pi_\\phi$ or the learned actor $\\pi_\\phi$ itself. I suggest to clarify this explicitly in Algorithm 1"}, "questions": {"value": "Could you discuss more on the trade-offs introduced by incorporating Peng’s $Q(\\lambda)$ into offline RL? In addition, please clarify the main limitations of the proposed framework in the \"Conclusion\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c26UtYDVYP", "forum": "Ml4AtrrfQT", "replyto": "Ml4AtrrfQT", "signatures": ["ICLR.cc/2026/Conference/Submission18686/Reviewer_W4Kk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18686/Reviewer_W4Kk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940629614, "cdate": 1761940629614, "tmdate": 1762928387240, "mdate": 1762928387240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Conservative Peng’s Q(λ) (CPQL), an offline RL algorithm that replaces the single-step Bellman operator in CQL with the multi-step Peng’s Q(λ) operator. The authors prove conservative lower bounds on value estimates, establish policy-improvement guarantees w.r.t. the behavior policy, and derive a λ–α sub-optimality trade-off. This creates implicit behavior regularization without explicit policy constraints or extra networks, reducing both overestimation (from OOD actions) and over-pessimism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. First (to my knowledge) to pair PQL with conservative value estimation and analyze it theoretically for offline RL. Theorems 1–3 extend CQL-style results: (i) conservative lower bounds; (ii) policy improvement over behavior; (iii) an explicit λ–α trade-off with detailed proofs.\n\n2. Algorithm 1 is straightforward; no behavior-policy estimation (unlike MCQ/CSVE/EPQ).\n\n3. On MuJoCo, Adroit, AntMaze, CPQL is best or second-best on 22/29 tasks; sensitivity to α is substantially lower than CQL (clear practical win). Alternative multi-step operators and offline-to-online results reinforce the story."}, "weaknesses": {"value": "1. Theory mostly treats the exact evaluation case; neural critics + replay introduce approximation error and potential instability (classic issue with λ-returns). Provide a finite-sample / approximation note (even informal), or an experiment tracking Bellman error vs. λ and critic drift under function approximation. A short stability plot (loss, TD error, divergence events) would help.\n\n2. Recent multi-step offline methods (e.g., Uni-O4) are conceptually close. Either add Uni-O4 (or the closest available multi-step competitor) to Table/Fig. 2 or clearly justify incompatibility. A paragraph contrasting CPQL’s no-IS λ-returns vs. methods relying on IS/retrace would clarify the novelty.\n\n3. Multi-step targets add compute; the paper omits time-to-X (e.g., to 1M steps) vs. CQL/IQL. Add a runtime table and learning-curve area-under-curve (AUC). If CPQL converges faster for similar wall-clock, that strengthens the practical claim."}, "questions": {"value": "1. Algorithm 1 line 7 subtracts γⁿ α_td log π, but α_td = 0 except at the final step. What role does this term play? Would omitting it entirely affect stability or theoretical guarantees? \n\n2. What is the runtime cost of CPQL relative to CQL and IQL? Please report wall-clock time (to 1 M steps) and sample-efficiency comparisons.\n\n3. Are there tasks where single-step conservative Bellman updates (λ = 0) outperform CPQL? If so, how can practitioners detect when multi-step λ > 0 is harmful?\n\n4.Could you compare to recent multi-step offline RL algorithms such as Uni-O4 (2024) or clarify conceptual differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "90rbZYw3Kh", "forum": "Ml4AtrrfQT", "replyto": "Ml4AtrrfQT", "signatures": ["ICLR.cc/2026/Conference/Submission18686/Reviewer_jSat"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18686/Reviewer_jSat"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984934409, "cdate": 1761984934409, "tmdate": 1762928386806, "mdate": 1762928386806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}