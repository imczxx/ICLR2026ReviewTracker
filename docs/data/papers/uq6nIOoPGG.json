{"id": "uq6nIOoPGG", "number": 22376, "cdate": 1758330279338, "mdate": 1759896869527, "content": {"title": "Learning Coarse-Grained Representations: An Exploration of Mutual Information via Hyperspherical Density", "abstract": "We revisit InfoMax for representation learning, using hyperspherical geometry with a non-parametric von Mises-Fisher kernel density estimator and differential entropy. This method is minimal with no asymmetry and trains stably. Results are competitive on smaller datasets such as CIFAR-10, STL-10 and LC25000, but lags behind modern baselines on ImageNet-1000. Experiments show that weakening the global entropy term consistently helps classification accuracy, suggesting that strict mutual information classification favors coarse grouping over fine discrimination.", "tldr": "We propose a novel SSL method using hyperspherical density estimation for MI maximization. It learns strong coarse-grained representations but struggles with fine-grained details, suggesting a gap in current MI theory.", "keywords": ["self-supevised learning", "mutual information", "kernel density estimation", "von misses-fisher distribution", "representation learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2a593a53217b0a8aa787593922ce8a41cbcd8e37.pdf", "supplementary_material": "/attachment/c251e627bd79a7f6967395e2f54b5f2cc01aa653.zip"}, "replies": [{"content": {"summary": {"value": "The work deals with self-supervised learning of visual representation, exploring the InfoMax principle with a restricted hyperspherical geometry. The densities to compute the entropy are estimated with the  von Mises-Fisher non parametric kernel density estimator and the mutual information approximated by a difference of the global differential entropy and the local one."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* while several works seem quite close from the proposed one (see weaknesses) it seems that the use of a direct non-parametric density estimation wit vMF and an approximation of the mutual information by a difference of differential entropies has never been proposed. It is theorically new *and* interesting.\n\n* a large part of the paper is dedicated to the ablation of the approach. Three hyperparameters are studied quantitavelly and qualitative results are also reported.\n\n* the full code is provided as supplementary material, ensuring that it will be released to the community to favour reproducibility. The code is particularly well written and structured."}, "weaknesses": {"value": "* the proposed work should position itself:\n  - in relation with [b], where two properties of contrastive representation learning is studied on the Hypersphere.  After their Theorem 1, they relate their work with  feature distribution entropy estimation (via a von Mises-Fisher kernel density estimation) and the InfoMax principle. \n  - in relation with [d] which interpret DINO features (the same as those used in the paper) as a von Mises-Fisher mixture model\n\n* the quantitative evaluation are reported with a ResNet-18 backbone only, except of the proposed approach that is estimated with a ViT-16 backbone. One can regret that this last backbone is not used for baseline approaches.\n\n**minor**:\n  - line 030: SSL is not defined, it should be first written on line 024: \"Self-supervised learning (SSL) methods...\"\n  - line 045: ICA is not defined (independent component analysis). By the way, ICA was \"grounded\" by several principles, including contrast maximization by Comon (1991) and Joint Approximate Diagonalization of Eigenmatrices by Cardoso and Souloumiac in 1993.Regarding the Infomax principle, Bell and Sejnowsky indeed relied on it, but Nadal and Parga showed in 1994 that Infomax was equivalent to redundancy reduction principle, opening the path to its usage for ICA.\n  - line 053: MI should be defined previously (mutual information on line 046)\n  - line 091: \"we an\" --> \"we can\"\n  - line 101: \"Models trained on LC25000 were trained for 25000 epochs\" --> \"Models were trained on LC25000 for 25000 epochs\"\n  - implementation details (line 098-103) may be better placed in section 4. In particular the citation of datasets (currently n lines 107-108) should be before the details currently n line 101-103 \n  - baseline method in Table 1, while known, should be cited in the text\n  - line 395: the actual published paper should be cited rather than the arxiv report for ( Kalapos and Gyires-Tóth, 2024) [a]\n  - while different, it may worth to position the work w.r.t [c]\n\n[a] A. Kalapos and B. Gyires-Tóth, \"Whitening Consistently Improves Self-Supervised Learning,\" 2024 International Conference on Machine Learning and Applications (ICMLA), Miami, FL, USA, 2024, pp. 448-453, \n\n[b] Wang and Isola (2020) [Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere](https://proceedings.mlr.press/v119/wang20k/wang20k.pdf), ICML\n\n[c] Li et al (2024) Probabilistic Contrastive Learning with Explicit Concentration on the Hypersphere, arxiv:2405.16460\n\n[d] Govindarajan et al (2023) DINO as a von Mises-Fisher mixture model, ICLR"}, "questions": {"value": "* why the top-5 accuracy is ont reported for LC-25000 in Table 1 ?\n* why the performance on ImageNet-1k is not reported for the baselines in Table1 ? At least from previous papers...\n* which K is used for K-NN in Table 1? \n* what is the value of $\\kappa$ for the main results of section 4? Are $\\alpha$ and $\\beta$ set to one (as said line 091)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Ethic concern is not addressed but due to the positioning of the paper, there is no reason to do so."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DI0oO2E93O", "forum": "uq6nIOoPGG", "replyto": "uq6nIOoPGG", "signatures": ["ICLR.cc/2026/Conference/Submission22376/Reviewer_AFGX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22376/Reviewer_AFGX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761149078303, "cdate": 1761149078303, "tmdate": 1762942191258, "mdate": 1762942191258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the InfoMax principle for self-supervised representation learning, proposing a simple implementation based on hyperspherical geometry and non-parametric von Mises-Fisher (vMF) kernel density estimation to compute global and local differential entropies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Simplicity and conceptual clarity in method."}, "weaknesses": {"value": "Limited Novelty and Technical Contribution\n\n- The proposed formulation appears to be a relatively direct combination of established components—namely hyperspherical embeddings, von Mises–Fisher (vMF) kernels, and differential entropy estimation.\n- The paper does not present a substantial theoretical advancement or novel derivation beyond existing InfoMax- or kernel-density-based self-supervised learning (SSL) frameworks.\n\nPoor Scalability and Weak Empirical Results\n\n- The performance reported in Table 1 is markedly low, contradicting the paper’s claims; even on datasets with a small number of classes, the method underperforms.\n- Figures 1 and 2 omit results for several experimental settings, and there is no comparison against standard SSL benchmarks, which undermines the empirical credibility of the work.\n- Similarly, Figures 3 and 4 report results only on small or relatively simple datasets, further limiting the strength of the experimental validation.\n- The claim that the learned representations are “coarse-grained” is largely descriptive and lacks rigorous quantitative validation.\n- No ablation or analysis is provided to substantiate the connection between this phenomenon and mutual-information behavior in a measurable way.\n\nLack of Theoretical Grounding\n\n- The paper fails to formally justify how the proposed estimator approximates true mutual information.\n- From the perspective of mutual-information estimation, the work does not convincingly demonstrate either theoretical rigor or empirical superiority; a comprehensive comparison against existing MI estimators is essential.\n- In contrast to prior MI estimators that do not rely on hyperspherical geometric assumptions, the proposed approach is considerably more restrictive.\n- The observation that relaxing the global entropy term improves classification lacks analytical explanation and may merely reflect optimization dynamics rather than a genuine information-theoretic insight."}, "questions": {"value": "- In Equation (4), the formulation no longer corresponds to mutual information when both α and β differ from 1. Could you clarify the rationale for setting these parameters to values other than 1, and how this choice affects the interpretation of the objective?\n- What is the basis for claiming that the proposed method learns coarser representations? Could you provide a theoretical explanation of why the proposed loss function encourages such coarse-grained structure, and offer empirical evidence demonstrating that this property indeed emerges in the learned embeddings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cdFb6AAuFD", "forum": "uq6nIOoPGG", "replyto": "uq6nIOoPGG", "signatures": ["ICLR.cc/2026/Conference/Submission22376/Reviewer_ZWUq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22376/Reviewer_ZWUq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761699314196, "cdate": 1761699314196, "tmdate": 1762942190913, "mdate": 1762942190913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes using a vMF (von Mises–Fisher) kernel as a mutual-information estimator for self-supervised learning.\nThe authors evaluate the method on four datasets with Grad-CAM, CLS-to-patch attention analyses, and hyperparameter ablation studies."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper proposes using a vMF kernel as a mutual-information estimator for self-supervised learning.\nThe authors evaluate the method on four datasets with Grad-CAM, CLS-to-patch attention analyses, and hyperparameter ablation studies."}, "weaknesses": {"value": "1. The paper proposes using a vMF kernel as an MI estimator for self-supervised learning but does not explain how it differs from existing estimators such as InfoNCE or what advantages it offers.\n\n2. The proposed loss is algebraically similar to the InfoNCE loss, differing mainly in the number of positive pairs and its interpretation as a vMF-based entropy estimator, which weakens the novelty of the method.\n\n3. The results in Table 1 do not demonstrate any clear advantage of the proposed vMF-based self-supervised method compared with other baselines.\n\n4. Figure 1 does not convincingly show that the trained model focuses on the main object, as the Grad-CAM activation is broadly distributed over non-object regions."}, "questions": {"value": "Please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "isv1JZrWkn", "forum": "uq6nIOoPGG", "replyto": "uq6nIOoPGG", "signatures": ["ICLR.cc/2026/Conference/Submission22376/Reviewer_q5N6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22376/Reviewer_q5N6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739104512, "cdate": 1761739104512, "tmdate": 1762942190609, "mdate": 1762942190609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the InfoMax objective function for self-supervised representation learning. The authors estimate mutual information using a von Mises-Fisher (vMF) kernel density estimator under hyperspherical geometry (i.e. embeddings have unit length). They evaluate their method on several datasets (STL, CIFAR, LC-25000, and ImageNet-1K) using standard data augmentations to form positive pairs. The linear probing results for classification show that the method achieves competitive performance on smaller datasets but falls behind on ImageNet-1K."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1- the motivation for using vMF kernels is clear, and the objective function is minimal and theoretically grounded.\n\n2- good empirical insight from ablations on projector dimension and band width parameter."}, "weaknesses": {"value": "1- the paper feels incomplete. The direction and main research question are not well defined. It’s unclear whether the goal is to analyze InfoMax behavior or to improve performance.\n\n2- more experiments, analyses, and discussion are needed. For instance, comparisons to alternative entropy estimators or normalization schemes (e.g., stop-grad, whitening) would add clarity.\n\n3- the study of the bandwidth parameter κ closely parallels the temperature parameter in contrastive learning. This connection could be more explicitly analyzed."}, "questions": {"value": "1- can you analyze how your loss function relates to the alignment and uniformity objectives (Wang & Isola, 2020)? Under similar assumptions, what does your objective correspond to?\n\n2- can you elaborate on the motivation for using vMF kernel estimation instead of InfoNCE or other parametric estimators?\n\n3- is the main goal of your work to achieve strong downstream performance, or primarily to analyze the behavior of InfoMax under certain assumptions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X8Au8FYVqZ", "forum": "uq6nIOoPGG", "replyto": "uq6nIOoPGG", "signatures": ["ICLR.cc/2026/Conference/Submission22376/Reviewer_YLtS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22376/Reviewer_YLtS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22376/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966601172, "cdate": 1761966601172, "tmdate": 1762942190229, "mdate": 1762942190229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}