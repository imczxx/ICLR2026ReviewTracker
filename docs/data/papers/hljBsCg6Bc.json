{"id": "hljBsCg6Bc", "number": 14605, "cdate": 1758239791352, "mdate": 1763726191971, "content": {"title": "CompactDP: Category-Aware Feature Compactness for Differential Privacy", "abstract": "The rapid growth of AI models raises critical privacy concerns due to their tendency to memorize training data, making them vulnerable to extraction and membership inference attacks (MIAs). Traditional privacy-preserving methods like DP-SGD often degrade model utility and exacerbate accuracy disparities across sub-populations, limiting their applicability in sensitive fields. We observe that compact class-wise feature manifold embeddings inherently reduce privacy risks by smoothing probability density functions (PDFs), which diminishes the influence of individual training samples and lowers memorization. Leveraging this insight, we propose \\textit{Class-wise Compactness for Privacy} (C4P), a noise free feature contraction framework that directly addresses the root cause of privacy leakage, sparse, high-dimensional features, via feature contraction rather than relying solely on gradient noise. C4P can be trained independently and achieves a superior privacy-utility trade-off, with empirical privacy grantee comparable to DP-SGD ($\\epsilon=1$). C4P attains 99.22\\% accuracy while limiting MIA risk to 0.4997 on CIFAR10. Extensive experiments on FashionMNIST and MedicalMNIST further validate its favorable utility-privacy trade-off across diverse metrics. Our framework provides a principled and efficient solution for privacy-preserving deep learning in critical domains such as healthcare and finance.", "tldr": "CompactDP targets the root cause of leaks: sparse, high-dimensional features. Category-aware feature contraction that tightens scattered data distributions where individual training samples get memorized.", "keywords": ["Data Privacy", "DP-SGD", "Feature Space Compactness", "Membership Inference Attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fcf642b83bb46b8371ece17f5e577c1d91edb714.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a cluster-based method to process features before private training to make them more separable. This is achieved by applying a feature map $g_\\phi$  trained with KDE. The proposed method reduces the sensitivity of features, thus enhancing performance. Experiments on fine-tuning ViT with CIFAR-10, FashionMNIST, and MedicalMNIST show that the proposed method achieves better performance than directly using DP-SGD without preprocessing the datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper applies a cluster-based method to train a feature map that reduces per-sample sensitivity.\n\n2.  Experiments on CIFAR-10, FashionMNIST, and MedicalMNIST demonstrate better performance than directly using DP-SGD."}, "weaknesses": {"value": "1. The paper is not well-structured and the main selling point is not highlighted, making it hard to understand.\n\na) The main contribution of this paper is not proposing a \"new\" DP notion as the proposed CompactDP is not a new DP notion. The main contribution is to apply a feature map $g_\\phi$ to reduce the sensitivity of the gradient.Thus, the title, abstract, and introduction should be rewritten to emphasize the importance of feature quality in DP-training which motivates the idea of using clustering method to pre-process the features. So, I would suggest not using CompactDP as a main contribution of this paper.\n\nb) The theoretical foundation of using KDE is not new in representation learning, thus, Section 3 should be moved to appendices rather than in the main body. The main contribution of Section 3, such as DP anlyaisis and sub-sampling, is straight forward based on existing DP theory once the feature map $g_\\phi$ is given. Thus, I do not think it is a good idea to highlight this section as a main contribution.\n\nOverall, the main contribution of this paper should be applying a feature map $g_\\phi$ to reduce the sensitivity, which is important in practice. So please highlight this and the other contrbutions are incremental.\n\n2. The application of clustering methods to enhance feature separability is an established concept in representation learning; for instance, the introduction of a feature map $g_\\phi$ is a classic technique in the field. Therefore, the primary contribution of this paper lies in adapting representation learning to DP training. However, many of the representation learning techniques discussed are not novel. For example, [A] employs a clustering method for feature learning that results in more compact features. Similarly, the KDE method used in the current work is a classic approach, whereas more recent methods like GANs and VAEs (referenced in [A] as [17,18]) have achieved advanced density estimation. Furthermore, the goal of creating more compact and separable features is a well-documented objective in representation learning. Seminal works such as [B] and [C] demonstrate that deep learning naturally extracts increasingly compact and separable features as the network depth increases.\n\nThis paper should discuss these foundational representation learning literatures and others to properly contextualize its contribution within the existing body of work.\n\n\n\n[A] Deep Clustering for Unsupervised Learning of Visual Features. Caron et al., ECCV 2018.\n\n[B] Prevalence of neural collapse during the terminal phase of deep learning training.  Papyan et al., PNAS, 2020.\n\n[C] A Law of Data Separation in Deep Learning. He and Su, PNAS, 2022.\n\n\n3. The idea of using clustering to make features more separable is also not new in DP training. [D] linked representation quality to DP fine-tuning, showing that better, more separable features (where greater compactness implies higher quality) lead to higher performance. Furthermore, [D] applied PCA, a clustering method with a linear feature map $g_\\phi$ , which also enhanced the performance of DP fine-tuning, achieving 95% accuracy when privately fine-tuning a ViT on CIFAR-10—the same experimental result as this paper. Other works that highlight the importance of feature quality and compactness include [E] and [F]. Therefore, these related works should be discussed in greater depth in this paper.\n\n[D] Neural Collapse Meets Differential Privacy: Curious Behaviors of NoisyGD with Near-perfect Representation Learning. Wang et al., ICML, 2024\n\n[E] On the benefits of public representations for private transfer learning under distribution shift. P Thaker et al., NeurIPS 2024.\n\n[F] Why Is Public Pretraining Necessary for Private Model Training? Ganesh et al., ICML, 2023.\n\n4. The MIA results in Table 3 are confusing. While DP-SGD is more private than SGD, the results show that SGD on compact features has a lower MIA success rate (i.e., is more private) than DP-SGD on compact features. How can this phenomenon be explained?\n\n5. The most significant weakness of the proposed method is that the training of the feature map $g_\\phi$ is data-dependent, as the loss function in Eq. (6) depends on the data. Therefore, the training algorithm should be made private; otherwise, it may lead to additional privacy leakage. However, the authors appear to use a non-private algorithm to train it. I would recommend using DP-SGD to minimize loss function (6) for training $g_\\phi$ . Additionally, a MIA should be applied to $g_\\phi$  itself to assess its privacy."}, "questions": {"value": "See the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2QCbzhndnV", "forum": "hljBsCg6Bc", "replyto": "hljBsCg6Bc", "signatures": ["ICLR.cc/2026/Conference/Submission14605/Reviewer_ppG3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14605/Reviewer_ppG3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888357130, "cdate": 1761888357130, "tmdate": 1762924987538, "mdate": 1762924987538, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a cluster-based method to process features before private training to make them more separable. This is achieved by applying a feature map $g_\\phi$  trained with KDE. The proposed method reduces the sensitivity of features, thus enhancing performance. Experiments on fine-tuning ViT with CIFAR-10, FashionMNIST, and MedicalMNIST show that the proposed method achieves better performance than directly using DP-SGD without preprocessing the datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper applies a cluster-based method to train a feature map that reduces per-sample sensitivity.\n\n2.  Experiments on CIFAR-10, FashionMNIST, and MedicalMNIST demonstrate better performance than directly using DP-SGD."}, "weaknesses": {"value": "1. The paper is not well-structured and the main selling point is not highlighted, making it hard to understand.\n\na) The main contribution of this paper is not proposing a \"new\" DP notion as the proposed CompactDP is not a new DP notion. The main contribution is to apply a feature map $g_\\phi$ to reduce the sensitivity of the gradient.Thus, the title, abstract, and introduction should be rewritten to emphasize the importance of feature quality in DP-training which motivates the idea of using clustering method to pre-process the features. So, I would suggest not using CompactDP as a main contribution of this paper.\n\nb) The theoretical foundation of using KDE is not new in representation learning, thus, Section 3 should be moved to appendices rather than in the main body. The main contribution of Section 3, such as DP anlyaisis and sub-sampling, is straight forward based on existing DP theory once the feature map $g_\\phi$ is given. Thus, I do not think it is a good idea to highlight this section as a main contribution.\n\nOverall, the main contribution of this paper should be applying a feature map $g_\\phi$ to reduce the sensitivity, which is important in practice. So please highlight this and the other contrbutions are incremental.\n\n2. The application of clustering methods to enhance feature separability is an established concept in representation learning; for instance, the introduction of a feature map $g_\\phi$ is a classic technique in the field. Therefore, the primary contribution of this paper lies in adapting representation learning to DP training. However, many of the representation learning techniques discussed are not novel. For example, [A] employs a clustering method for feature learning that results in more compact features. Similarly, the KDE method used in the current work is a classic approach, whereas more recent methods like GANs and VAEs (referenced in [A] as [17,18]) have achieved advanced density estimation. Furthermore, the goal of creating more compact and separable features is a well-documented objective in representation learning. Seminal works such as [B] and [C] demonstrate that deep learning naturally extracts increasingly compact and separable features as the network depth increases.\n\nThis paper should discuss these foundational representation learning literatures and others to properly contextualize its contribution within the existing body of work.\n\n\n\n[A] Deep Clustering for Unsupervised Learning of Visual Features. Caron et al., ECCV 2018.\n\n[B] Prevalence of neural collapse during the terminal phase of deep learning training.  Papyan et al., PNAS, 2020.\n\n[C] A Law of Data Separation in Deep Learning. He and Su, PNAS, 2022.\n\n\n3. The idea of using clustering to make features more separable is also not new in DP training. [D] linked representation quality to DP fine-tuning, showing that better, more separable features (where greater compactness implies higher quality) lead to higher performance. Furthermore, [D] applied PCA, a clustering method with a linear feature map $g_\\phi$ , which also enhanced the performance of DP fine-tuning, achieving 95% accuracy when privately fine-tuning a ViT on CIFAR-10—the same experimental result as this paper. Other works that highlight the importance of feature quality and compactness include [E] and [F]. Therefore, these related works should be discussed in greater depth in this paper.\n\n[D] Neural Collapse Meets Differential Privacy: Curious Behaviors of NoisyGD with Near-perfect Representation Learning. Wang et al., ICML, 2024\n\n[E] On the benefits of public representations for private transfer learning under distribution shift. P Thaker et al., NeurIPS 2024.\n\n[F] Why Is Public Pretraining Necessary for Private Model Training? Ganesh et al., ICML, 2023.\n\n4. The MIA results in Table 3 are confusing. While DP-SGD is more private than SGD, the results show that SGD on compact features has a lower MIA success rate (i.e., is more private) than DP-SGD on compact features. How can this phenomenon be explained?\n\n5. The most significant weakness of the proposed method is that the training of the feature map $g_\\phi$ is data-dependent, as the loss function in Eq. (6) depends on the data. Therefore, the training algorithm should be made private; otherwise, it may lead to additional privacy leakage. However, the authors appear to use a non-private algorithm to train it. I would recommend using DP-SGD to minimize loss function (6) for training $g_\\phi$ . Additionally, a MIA should be applied to $g_\\phi$  itself to assess its privacy."}, "questions": {"value": "See the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2QCbzhndnV", "forum": "hljBsCg6Bc", "replyto": "hljBsCg6Bc", "signatures": ["ICLR.cc/2026/Conference/Submission14605/Reviewer_ppG3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14605/Reviewer_ppG3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888357130, "cdate": 1761888357130, "tmdate": 1763727058234, "mdate": 1763727058234, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CompactDP, a framework that improves the trade-off between utility and privacy in differentially private machine learning. The core idea is to addresses the root cause of privacy leakage—sparse, highdimensional features—via feature contraction. CompactDP first performs a category-aware feature contraction in the representation space, pulling same-class samples into tight clusters. It also provides a theoretical analysis based on Rényi Differential Privacy that formally connects the geometric contraction ratio to an amplification of the privacy guarantee. Finally, the authors conduct experiments on datasets like CIFAR-10 to demonstrate the method's effectiveness, reporting improvements in the privacy-utility trade-off compared to baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. This paper's main conceptual contribution is its shift away from passive noise addition to actively optimizing the feature space for privacy. This framing of privacy as a geometric property is a novel idea that promises to be a fruitful direction for new research.\n\nS2. This paper provides a formal linkage between the geometric contraction (η_c) and the RDP guarantee. This theoretical contribution provides a solid foundation for the method.\n\nS3. The experimental validation is compelling. In particular, the 'CompactDP + DP-SGD' variant shows a SOTA privacy-utility trade-off, significantly outperforming the DP-SGD baseline. The visualizations (e.g., Figures 4 & 5) are highly effective in building intuition for why the method works."}, "weaknesses": {"value": "W1. The CompactDP framework seems to be designed only for transfer learning where the backbone model is frozen. It is unclear how CompactDP would work in standard end-to-end training. This limits the method's general applicability.\n\nW2. The paper introduces fairness as a core motivation but never explains how the proposed feature contraction mechanism leads to the observed fairness improvements shown in Table 3, leaving the fairness benefits as an unexplained side effect rather than an integral part of the design."}, "questions": {"value": "Overall, this paper is well-structured. I have the following concerns:\n1. The privacy guarantee of the entire framework is calculated after the feature contraction step. My main concern is that the privacy cost of training the contraction network (g_φ) itself is not analyzed. This network is trained on the features of private data, yet its own privacy leakage is ignored in the final budget. The current theoretical analysis provides an end-to-end guarantee for a model trained on the output of g_φ, but not for the entire pipeline including the training of g_φ.\n\n2. This method introduces new hyperparameters (e.g., λ & h). While the paper provides a default for the kernel bandwidth h, the strategy for co-tuning with DP-SGD's own parameters is not discussed in detail.\n\n3. This paper makes several very strong claims. Saying CompactDP appears 'optimal' for medical imaging is a very strong word. It would be more appropriate by rephrasing to 'offers a highly favorable balance'. The claim of \"fundamentally breaking the traditional DP trade-off\" could be softened to 'fundamentally improving'.\n\n4. There are some minor typos issues that could be corrected. For example, in the sentence '...analysis are listed in in Appendix. J,' the word 'in' is repeated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3xd8dd12hO", "forum": "hljBsCg6Bc", "replyto": "hljBsCg6Bc", "signatures": ["ICLR.cc/2026/Conference/Submission14605/Reviewer_fji8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14605/Reviewer_fji8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922712417, "cdate": 1761922712417, "tmdate": 1762924986955, "mdate": 1762924986955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CompactDP, a framework that trains a class-conditional feature contraction network to make each class’s representations more compact.\nIt argues that this geometric contraction reduces sensitivity and therefore amplifies Differential Privacy (DP) guarantees so less noise is required.\nThe method also proposes class-adaptive privacy budget allocation.\nVia numerous experiments (on CIFAR-10, FashionMNIST, MedMNIST), the paper claims simultaneously higher accuracy, lower membership-inference risk, and improved fairness vs. both non-private and DP-SGD baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The idea to explicitly optimizing class-conditional feature compactness to enhance privacy is interesting/novel and a promising research direction.\n* The proposed class-adaptive privacy allocation connects well with recent work on DP/fairness and addresses real-world concerns about disparate utility impacts.\n* The paper includes extensive empirical evaluation across multiple datasets/model architectures.\n* The writing/organization are clear, there are many good visualizations too."}, "weaknesses": {"value": "## Weaknesses:\n\n* The class-conditional feature contraction network (the CompactDP process), i.e., class separation, anchor selection, network training is performed directly on the sensitive/private data and is therefore data-dependent.\nThese steps must be done under DP mechanisms and included in the overall DP analysis.\nHowever, the paper treats them as \"free\" post-processing operations and does not account for their contribution to the privacy budget.\nI believe this breaks end-to-end DP.\n\n* Empirical results appear inconsistent/implausible without additional details.\n\t* For instance, on several occasions, CompactDP and CompactDP+DP-SGD with epsilon=1 achieve better accuracy than the non-private baseline, e.g., Table 1 and 2.\n\t* MIA vs. DP-SGD with epsilon=1 baseline (Table 1) achieves AUC 0.74 which exceeds what is theoretically achievable given the privacy budget.\n\t* MIA vs. CompactDP+DP-SGD with epsilon=1 (Table 1) achieves AUC of 0.33. This is worse than random guessing; flipping the in/out labels would achieve AUC of 0.67, which is a very high leakage for such a low epsilon.\n  * In general, the paper claims it \"achieves state-of-the-art privacy and utility guarantees without accuracy loss,\" and even reports higher accuracy than non-private baselines, attributing this to its contraction mechanism that \"this contraction effect reduces the presence of outlier samples.\" \nHowever, this directly contradicts established understanding of the privacy–utility trade-off: improving utility typically requires some degree of memorization, particularly of outlier or rare samples, which inherently reduces privacy [1, 2].\n\n* The paper lacks essential details about MIA evaluation. There is no definition and no relevant prior work is cited. It is unclear which specific attack variant is used, what threat model is assumed (black-box or white-box), and whether/how many shadow models are employed.\nMoreover, the MIA metrics like \"MIA Advantage\" is not defined/explained. As a sidenote, even DP is never explained.\n\n* The paper does not provide pseudocode or implementation details (or open-source code) of CompactDP, nor does it include the accompanying DP analysis (see also point 1). This makes the work difficult to reproduce and does not allow for independent verification of the claimed DP guarantees, which is very important for any DP algorithm.\n\n\n## Minor Weaknesses:\n* The paper would benefit from citing additional relevant work -- references supporting the three critical shortcomings of DP discussed in Section 1, prior studies on MIAs, and potentially [3].\n* Figure 3 appears in the main text but is never referenced or discussed there (only in Appendix I).\n* There are a few typos -- \"THe combination,\" \"We quantifies and visualizes.\"\n\n\n## References:\n[1] Feldman, Does Learning Require Memorization? A Short Tale about a Long Tail. In ACM SIGACT, 2020\n\n[2] Feldman et al., What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation. In NeurIPS, 2020\n\n[3] Kulynych et al., Disparate Vulnerability to Membership Inference Attacks. In PoPETS, 2022"}, "questions": {"value": "Please refer to weaknesses, additionally:\n* How is the contraction network g_phi trained with respect to DP?\n* Within the contraction network, how are privacy costs from class separation and anchor selection steps handled?\n* What is the threat model and other relevant MIA details?\n* How are the hyperparameters (h, nu, lambda, gamma, etc.) chosen?\n* Where exactly (Berrada et al., 2023) \"demonstrated that feature distance distributions correlate with empirical leakage?\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sNRyu2uR4m", "forum": "hljBsCg6Bc", "replyto": "hljBsCg6Bc", "signatures": ["ICLR.cc/2026/Conference/Submission14605/Reviewer_x425"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14605/Reviewer_x425"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953497263, "cdate": 1761953497263, "tmdate": 1762924986403, "mdate": 1762924986403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes CompactDP, a method that seeks to amplify differential privacy (DP) guarantees by contracting the representation space, making representations more compact, reducing sensitivity and disparate impact due to using DP-SGD. The authors provide extensive theoretical arguments and empirical results to illustrate their method’s efficacy in terms of privacy protections, utility tradeoff, and fairness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "[S1] Points out how compact representations may strengthen DP guarantees of DP-SGD based training via amplification guarantees, as demonstrated theoretically and empirically.\n\n[S2] Extensive theoretical discussion methodically justifies their claims w.r.t. privacy and fairness using a Renyi-DP-based framework, touching upon various aspects, viz. privacy amplification for different samples, lowering of sensitivity, etc.\n\n[S3] The framework provides improvements over DP-SGD in terms of membership inference protection, utility, and fairness in the few settings explored.\n\n[S4] The empirical evaluation includes a breadth of model architectures, data domains, and DP-SGD based methods."}, "weaknesses": {"value": "[W1] The actual implementation’s details are not described and the reader is asked to parse it solely from Figure 2. For the sake of avoiding ambiguity and for the sake of good writing practices, especially for a venue like ICLR, a self-contained and precise subsection on the proposed architecture is highly advisable and simply cannot be skipped. One cannot ask the reader to guess the particulars of a method from a figure alone, this is a significant flaw in presentation.\n\n[W2] Overclaim: “Feature contraction amplifies privacy guarantees by $\\eta^2$ , enabling exponentially stronger bounds” No, it is clear that the amplification is *quadratic*, not exponential.\n\n[W3] Does not compare against any *fair DP-SGD baselines* (Esipova et al, FairDP, etc.), only against standard SGD, DP-SGD, and one ablated baseline, which does not certify this method’s performance w.r.t. SoTA methods, and not against any fair DP-SGD methods, therefore potentially inflating their fairness contributions to the SoTA.\n\n[W4] For CompactDP results, what is the value of $\\varepsilon$ or Renyi DP params? See Table 1. If it has a high $\\varepsilon$, comparing against DP-SGD with $\\varepsilon = 1$ will be very unfair! Any value of $\\varepsilon$ for CompactDP will sequentially compose with DP-SGD, therefore, CompactDP + DP-SGD is not a fair comparison against DP-SGD. Alternatively, if CompactDP has no formal privacy guarantees yet by itself defends against MIAs, that is an extraordinary claim that is not substantiated or explained.\n\n[W5] The experiments are solely conducted for $\\varepsilon = 1$. This does not illustrate the method’s behavior in different privacy regimes, making the empirical evaluation very limited. It is important to observe the proposed method’s vulnerability/fairness/utility in different privacy regimes (low $\\varepsilon$ and higher ones).\n\n[W6] Additionally, how is CompactDP even trained without DP-SGD? The paper states that the last classification layer in the CompactDP architecture is implemented by a standard DP-SGD method, so presenting results without DP-SGD makes no sense. Is it simply doing SGD instead of DP-SGD in that case? This further illustrates why it is *essential* to have a proper methodology (sub)section and unreasonable to ask the reader to deduce the method’s particulars from a figure alone."}, "questions": {"value": "[Q1] Can you please add a self-contained brief and unambiguous discussion on the proposed architecture as described in Figure 2? Also discuss how CompactDP may be used without DP-SGD, as presented in the experiments. This will address W1 and W6.\n\n[Q2] Can you fix any overclaims, such as that pointed out in W2?\n\n[Q3] Please include comparisons against SoTA *fair DP-SGD* paradigms (such as [1], [2], this list is not exhaustive and the authors should make sure to include all SoTA fair DP-SGD methods). I cannot corroborate your contribution (and the significance of it) w.r.t. fairness based on a comparison against DP-SGD and ablated baselines (arising from your method) alone.\n\n[Q4] Please add results spanning different privacy regimes, viz. for lower and higher values of $\\varepsilon$.\n\n[Q5] Please update your tables by taking into account the value of $\\epsilon$ of CompactDP. Currently, without this quantity, all of the comparisons against baselines (DP-SGD, ablated baselines) are unfair. If that is already taken into account or if CompactDP has no formal DP guarantees, explain in depth how CompactDP without DP-SGD works and endows privacy guarantees (and what the relevant value of $\\varepsilon$ is, if applicable). This is, again, an extraordinary claim made/result presented without any privacy-related explanation. I’d personally expect that a reduction in MIA success would come with some differentially private guarantees (indeed, using a DP auditing method might assign an upper bounded value of $\\varepsilon$ to CompactDP only), intentionally added or not, and therefore using DP-SGD with $\\varepsilon=1$ would make the analysis unfair, as the total value of the privacy loss would exceed $\\varepsilon=1$.\n\nThe paper’s theoretical discussion is very well done and convincing. However, the empirical analysis is flawed and leaves a lot to be desired and corrected. Pending the authors addressing these concerns, despite the promise of this paper, I cannot support the paper in its current form.\n\n## References\n\n[1] Esipova, Maria S. et al. “Disparate Impact in Differential Privacy from Gradient Misalignment.” ArXiv abs/2206.07737 (2022): n. pag.\n\n[2] Tran, Khang et al. “FairDP: Achieving Fairness Certification with Differential Privacy.” 2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) (2023): 956-976."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9hf8zdhBMz", "forum": "hljBsCg6Bc", "replyto": "hljBsCg6Bc", "signatures": ["ICLR.cc/2026/Conference/Submission14605/Reviewer_C74L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14605/Reviewer_C74L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14605/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014150517, "cdate": 1762014150517, "tmdate": 1762924985776, "mdate": 1762924985776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}