{"id": "10JEfJtiJM", "number": 13997, "cdate": 1758226646341, "mdate": 1763654753599, "content": {"title": "The Deleuzian Representation Hypothesis", "abstract": "We propose an alternative to sparse autoencoders (SAEs) as a simple and effective unsupervised method for extracting interpretable concepts from neural networks. The core idea is to cluster differences in activations, which we formally justify within a discriminant analysis framework. To enhance the diversity of extracted concepts, we refine the approach by weighting the clustering using the skewness of activations. The method aligns with Deleuze's modern view of concepts as differences. We evaluate the approach across five models and three modalities (vision, language, and audio), measuring concept quality, diversity, and consistency. Our results show that the proposed method achieves concept quality surpassing prior unsupervised SAE variants while approaching supervised baselines, and that the extracted concepts enable steering of a model’s inner representations, demonstrating their causal influence on downstream behavior.", "tldr": "We propose a novel method for extracting interpretable concepts from a model's activations, by representing the differences between samples.", "keywords": ["Mechanistic Interpretability", "Concept Extraction", "Explainability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1403967379da694f7be70001e832beab24d4f98.pdf", "supplementary_material": "/attachment/197658c036b88b476c7b58807988cccd9ec7b5ef.zip"}, "replies": [{"content": {"summary": {"value": "The authors present a method for finding interpretable concepts in language models, and compare it to a tentpole of the field, Sparse Autoencoders (SAEs). The method takes a relatively small number of differences of activations of a model, weights their distances by their skew, and then performs k-means. The authors apply this method to interpret five models across the modalities of text, images, and audio. They assess the ability of their method to find interpretable concepts, especially by measuring whether their concepts can act as probes for a priori classes like labels in datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is straightforward, and appears computationally light-weight. The paper is easy to follow."}, "weaknesses": {"value": "The paper uses language models which are outside the tradition of the previous works in this field, which make it hard to assess the papers claims. In particular, this paper studies DeBERTa and the encoder of BART, which are relatively old encoder-based models trained on masked language modeling, whereas recent SAE work has focused on decoder-based models trained on next-token-prediction, such as the family of Pythia models (Cunningham et al, 2023; Paulo and Belrose, 2025; Wang et al, 2025), Gemma 2 (Lieberum et al, 2024; Karvonen et al, 2025), and GPT-2 Small (Chaudhary and Geiger, 2024; Marks, Paren, et al, 202).\n\nThe SAEs used as points of comparison for the author's methods were trained by the authors, and may have been severely undertrained. Per Appendix B, these SAEs appear to have trained them on datasets of size 18k-288k (or that many sequences, with an unknown number of tokens per sequence), with with a learning rate of 1e-5. In contrast, the similarly-sized autoencoders in (Cunningham et al, 2023) had \"a learning rate of 1e-3 and are trained on 5-50M activation vectors for 1-3 epochs\", and (Bricken et al, 2023) used \"8 billion datapoints\". If this paper's SAEs were undertrained, the paper's claim that \"the proposed method achieves concept quality surpassing prior unsupervised SAE variants\" would need to be caveated with \"in conditions of data scarcity\".\n\nThe claims in section 2.4 of \"Lossless Steering\" are not necessarily an advancement over prior work. The authors claim that steering with SAEs \"introduce[s] reconstruction error and information loss\", which they avoid by \"steering directly in the activations space\". But prior work such as (Marks, Rager, et al, 2025) accounts for this reconstruction error, and in effect steers directly in the activation space. While the authors do demonstrate some ability to steer in Section 3.3 and Figures 3 and 4, the paper does not measure whether their approach introduces changes in downstream behavior or if those changes are more significant than steering with SAE vectors (as in Durmus et al, 2024).\n\nSparse autoencoders may not be the correct baseline for this type of approach. Sparse autoencoders are usually trained on a huge corpus of activations with completely varied meanings. The author's method is tested on relatively small, relatively narrow datasets, that focus on a single topic (e.g. movie reviews in the IMDB dataset, or paintings in the WikiArt dataset). More appropriate baselines might include PCA or ICA (the later of which the authors include)."}, "questions": {"value": "This paper departs heavily from the standards of the field along several axes, including choice of model, techniques, and evaluation metrics. Because of this, this review has lower confidence. The authors are encouraged to reply to this review and address the concerns raised, especially:\n\n1. Whether the SAEs have received sufficient training. This can be addressed by quantifying the number of datapoints the SAEs were trained on, quantifying the Fraction of Variance Explained by the SAEs, providing a graph of reconstruction loss over the training process, or by adding a pre-trained SAE to the set of benchmarks.\n\n2. Adding additional baselines to table 1, such as a probe trained on the entire activation space, a set of 6144 random vectors, and a pre-trained SAE (see the question below).\n\n3. Clarifying details of their method, in particular 1) how the clusters are initialized, and 2) how incorporating inverse skew to the distance expression in 152-154 changes the algorithm. \n\n-----\nOther questions for the authors:\n\n1. Do you have a name for your method?\n\n2. When performing your method, how did you initialize your k-means clusters?\n\n3. When running the algorithm for this experiment, what was the number of samples (N)? Is N the train size in Table 4?\n\n4. When applying your method, training the sparse autoencoders, and evaluating concept quality (Section 3.1), which activations did you use? For example, the IMDB dataset consists of a sequence of tokens, and BART and DeBERTa therefore produce one activation vector per token per layer. Which layer did you use, and did you use activations for every token or just some? \n\n6. Lines 152-154: Is the expression for d(d_i, C) here used when assigning d_i to a cluster? If so, isn't the scaling factor of 1/\\mu_3(d_i) irrelevant to identifying the closest cluster center? And if so, why does skewness weighting improve performance in Table 3?\n\n7. Lines 156-159: The paper says \"Both pair sampling and KMeans clustering run in linear time and memory with respect to dataset size N and activation dimension D, demonstrating scalability of our approach towards large datasets, or large models.\" Isn't this also true of sparse autoencoders? In particular, the memory requirements are linear in D and k (where k is the number of features, and k<N) and the run time is linear in D and N.\n\n8. Would it be possible to include additional baselines for your results in Table 1? In particular, it would be illustrative to see the probe loss for:\n\n- A probe trained on the entire activation space, acting as an upper bound of linear information which can be extracted from the model.\n\n- A set of 6144 random directions, acting as a baseline for how non-information-carrying directions behave.\n\n- A pre-trained SAE, such as Gemma Scope (Lieberum et al, 2024) or a Pythia SAE (https://huggingface.co/EleutherAI/sae-pythia-70m-32k).\n\n---- \nReferences:\n\n(Cunningham et al, 2023) Sparse Autoencoders Find Highly Interpretable Features in Language Models. https://arxiv.org/abs/2309.08600\n\n(Paulo and Belrose, 2025) Sparse Autoencoders Trained on the Same Data Learn Different Features. https://arxiv.org/pdf/2501.16615\n\n(Wang et al, 2025) Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures. https://openreview.net/pdf?id=2J18i8T0oI\n\n(Lieberum et al, 2024) Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2. https://arxiv.org/html/2408.05147v1\n\n(Karvonen et al, 2025) SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability. https://arxiv.org/pdf/2503.09532\n\n(Chaudhary and Geiger, 2024) Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge in GPT-2 Small. https://arxiv.org/pdf/2409.04478\n\n(Marks, Paren, et al, 2024) Enhancing Neural Network Interpretability with Feature-Aligned Sparse Autoencoders. https://arxiv.org/pdf/2411.01220\n\n(Bricken et al, 2023) Towards Monosemanticity: Decomposing Language Models With Dictionary Learning. https://transformer-circuits.pub/2023/monosemantic-features\n\n(Marks, Rager, et al, 2025) Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models. https://arxiv.org/pdf/2411.01220\n\n(Durmus et al, 2024) Evaluating feature steering: A case study in mitigating social biases. https://www.anthropic.com/research/evaluating-feature-steering"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UUu2LOoD6r", "forum": "10JEfJtiJM", "replyto": "10JEfJtiJM", "signatures": ["ICLR.cc/2026/Conference/Submission13997/Reviewer_Jr9h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13997/Reviewer_Jr9h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918467714, "cdate": 1761918467714, "tmdate": 1762924493189, "mdate": 1762924493189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an alternative to sparse autoencoders (SAEs) for extracting interpretable “concepts” from neural network activations. The method clusters differences in activations between randomly sampled data pairs rather than the activations themselves, motivated by Deleuze’s philosophy of “difference and repetition.” To mitigate the dominance of highly skewed activation dimensions, the authors introduce inverse-skewness weighting in the clustering step. Conceptually, the approach can be viewed as an unsupervised form of discriminant analysis and enables lossless steering by directly modifying activations along learned concept directions.\n\nExperiments span five pretrained models across three modalities (vision, text, audio), comparing the proposed approach against various SAE variants (Vanilla, Gated, JumpReLU, Matryoshka, TopK) as well as ICA and supervised LDA. The primary evaluation metric is probe loss, which measures how well one-dimensional logistic probes can recover dataset attributes (e.g., style, sentiment, or class). Additional metrics include Maximum Pairwise Pearson Correlation (MPPC) for consistency and effective rank for diversity. Ablations test the role of (1) using differences, (2) clustering vs. SAE-based extraction, and (3) the inverse-skewness weighting. The authors report that their method achieves the best probe loss across most tasks, high consistency, and interpretable steering behavior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I find this work to provide a good alternative to SAEs towards interpretability. It constitutes a simple and nice approach grounded in discriminant analysis and clustering, while the inverse-skewness weighting is an interesting modification to improve concept diversity.\n\nThe empirical evaluation considers multiple modalities and architectures, while the quantitative evaluation avoids the commonly considered sparsity-reconstruction tradeoff and uses the probe loss and MPPC towards concept exploration."}, "weaknesses": {"value": "The use of KMeans on activation differences is conceptually interesting, but it’s unclear how representative the randomly sampled pairs are. Could the sampling procedure bias the extracted concepts?\n\nIs the number of concepts fixed a priori? Is this the value that dictates the number of clusters for KMeans? How does the method fair when considering different values? \n\nThe inverse skewness weighting needs a further expansion. The inspiration is the Feature-Weigthed KMeans, but do any of its properties hold when the skewness is considered? Were there any empirical observations that skewed features dominate clustering? \n\nIn the ablation, the improvements from inverse-skewness weighting are modest in terms of the probe loss, while the effective rank exhibits substantial increase. On the other hand TopK SAE on the differences has approximately 3x the effective rank, again with minor differences in the probe loss. How can one interpret these results?\n\nThe focus on probe loss as the main interpretability metric is questionable — what are the “expected attributes,” and how well do these correspond to genuine conceptual disentanglement?\n\nThe paper claims that vanilla SAEs get “much lower concept quality and diversity,” yet many of the numerical differences (e.g., Table 1) seem small — are these differences statistically significant?\n\nIn the appendix A: Implementation details, it is mentioned that for training TopKSAEs, the chosen k was set to 32. Was that the optimal setting for the architecture, or did the authors consider only this value?\n\nWhy did the authors consider a subset of ImageNet instead of the full dataset?\n\nThe steering section feels superficial — qualitative examples are minimal, and it’s unclear whether the effects generalize."}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XpXXpVp4pW", "forum": "10JEfJtiJM", "replyto": "10JEfJtiJM", "signatures": ["ICLR.cc/2026/Conference/Submission13997/Reviewer_rHVe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13997/Reviewer_rHVe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921518179, "cdate": 1761921518179, "tmdate": 1762924492684, "mdate": 1762924492684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an unsupervised method for extracting interpretable concepts from neural networks by clustering activation differences rather than reconstructing activations. The approach samples pairwise differences, applies K-Means clustering weighted by inverse skewness, and frames this as unsupervised discriminant analysis inspired by Deleuze's philosophical view of concepts as differences. Evaluation across five models and three modalities uses probe loss and MPPC metrics, with qualitative steering demonstrations."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written, and I found at least 4 strong points in my opinion. \n\nS1. Simplicity. The method has an appealingly simple pipeline, is easy to understand and reproduce compared to SAE variants with multiple hyperparameters.\n\nS2. Broad empirical evaluation. The paper provides extensive experiments across three modalities (vision, text, audio), five models, and multiple datasets, with systematic probe loss evaluation across 874 attributes. This breadth is commendable.\n\nS3. Competitive probe loss results. Table 1 shows the method achieves lower probe loss than several SAE baselines on many tasks, which is interesting given the simplicity of the approach.\n\nS4. Consistency analysis. The MPPC evaluation (Table 2) provides useful insights into run-to-run stability, an important but often overlooked aspect of concept extraction methods."}, "weaknesses": {"value": "Even if I like the work, I notice several flaws, some major (labeled M) and some minor (labeled m). Below I detail these concerns:\n\nM1. Lack of operational definition for \"concept.\" Section 2.1 lists desiderata but never provides a clear, falsifiable definition of what constitutes a concept beyond achieving low probe loss. The philosophical framing around Deleuze adds narrative color but doesn't translate into concrete, testable predictions that distinguish this approach from standard clustering or discriminant analysis. What specific properties should concepts have under the Deleuzian view that wouldn't be expected from other perspectives (e.g concept as direction with cosine distance is kind of what we use right now) ? Without this, the philosophical motivation feels more like post-hoc rhetorical framing than genuine theoretical grounding.\n\nM2. Insufficient justification for the core assumption. Why should neural network internals organize themselves at the level of activation difference clusters? The paper assumes that recurring pairwise differences correspond to meaningful internal structure, but provides no theoretical or empirical justification for this inductive bias. Section 2.3's connection to discriminant analysis relies on very strong assumptions (isotropy, diagonal covariances proportional to identity) that are rarely satisfied in practice. The paper should either: empirically verify these assumptions hold in the studied models (measure per-layer isotropy and correlate with method performance), provide a relaxed theoretical analysis for realistic conditions, or show the method recovers known phenomenology in synthetic or well-understood models. As it stands, it appears the method performs a standard mathematical operation (clustering differences) without explaining why this operation should reveal the computational structure of neural networks.\n\nM3. Effective rank as diversity measure is unconvincing. Section 3.4 uses effective rank to justify the skewness weighting, but effective rank only measures spread of singular values, not semantic diversity or redundancy. Two concept sets could have identical effective rank but very different semantic coverage, and the anisotropy of the space is not helping... The paper should validate diversity claims with complementary metrics such as pairwise cosine similarity distributions between concept vectors or perceptual similarity of the images cluster.  \n\nM4. \"Lossless steering\" claim requires more proofs. Section 2.4 calls steering \"lossless\" because operations are reversible in activation space. However, the downstream network is not always linear -- steering one direction may affect representation geometry in orthogonal directions through subsequent layer transformations. The paper should define \"lossless\" precisely (what exactly is preserved?), verify with control probes that steering concept i doesn't inadvertently shift unrelated concepts j, k,. The current qualitative examples (Figures 3-4) are suggestive but insufficient to support strong selectivity claims.\n\nM5. Missing highly relevant baselines. For vision tasks, I strongly recommend to cite [1] that is doing AA for style transfer as well as include BatchTopK [2] in the table, Spade wich is a distance based SAE [3] and Archetypal SAE [4] which talk about instability of SAE and is now quite used with Dinov2. I believe these methods are natural comparisons for the clustering-based approach and their absence weakens the competitive positioning.\n\n[1] Unsupervised Learning of Artistic Styles with Archetypal Style Analysis, Wynen & al\n\n[2] BatchTopK Sparse Autoencoders, Bussman & al\n\n[3] Projecting Assumptions: The Duality Between Sparse Autoencoders and Concept Geometry, Hindupur & al\n\n[4] Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept Extraction in Large Vision Models, Fel & al\n\nM6. Discriminant analysis connection needs tightening. Section 2.3 asserts that optimal class separation reduces to activation differences under isotropy assumptions. However, these conditions $\\Sigma_A \\propto \\Sigma_B \\propto I$ are very restrictive and unlikely in transformer representations, the paper doesn't measure whether these conditions actually hold layer-by-layer, and no analysis is provided for the anisotropic case which is more realistic. Please either empirically verify the assumptions in your models or derive a more general result that applies under weaker conditions. Otherwise, the theoretical motivation appears fragile.\n\nM8. Evaluation relies entirely on labeled data. While the method is unsupervised, evaluation uses probe loss on labeled attributes. As the limitations section notes, this penalizes interpretable concepts that don't align with provided labels. The paper would benefit from qualitative evaluation protocols that don't depend on labels, analysis of discovered concepts that probe loss wouldn't capture, and discussion of what \"interpretability\" means independent of dataset-specific annotations. To come back to M1, I believe you may have an interesting implicit definition of what you call concept here in fact, can you write it down ?\n\nNow for the minor points, \n\nm1. Figure 2 is unreadable. The resolution and font sizes on page 3 make the pipeline diagram very difficult to parse. Please provide a higher-resolution version with larger text and clearer visual hierarchy.\n\nm2. Inverse skewness weighting details. The paper mentions using 1/skewness but doesn't specify how near-zero skewness is handled (epsilon clipping?), whether weights are normalized across dimensions or concepts, or whether absolute or signed skewness is used after direction flipping. \n\nm3. Writing polish. Several passages in Sections 2.3 and 5 could be tightened. Figure captions (especially Figure 4) should be more explicit about steering magnitudes and procedures. A final editing pass would improve readability.\n\nm4. Concept count parity. Table 1 should explicitly state the number of concepts used by each method if they differ, since probe loss can benefit from larger concept dictionaries."}, "questions": {"value": "Cf Major point 1-8 and minor points 1-4"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "apJKOKnKw3", "forum": "10JEfJtiJM", "replyto": "10JEfJtiJM", "signatures": ["ICLR.cc/2026/Conference/Submission13997/Reviewer_929m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13997/Reviewer_929m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021387548, "cdate": 1762021387548, "tmdate": 1762924492191, "mdate": 1762924492191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The current paper proposes to cluster *differences* in activations of a neural network, instead of raw activations itself, to interpret its behavior. Across a series of models and modalities, the paper shows improved probing performance, consistency of identified \"features\", and good steering ability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I really like the perspective of engaging with the rich literature on concepts in philosophy and using that to motivate interpretability approaches, but I wish the paper went deeper on this narrative."}, "weaknesses": {"value": "Reading through, I was first quite excited about the paper's idea and narrative (to look at activation differences), but, as it currently stands, I think the paper's operationalization of its core idea falls short of its promise. Main apprehensions are listed below.\n\n- Use of clustering to define \"concepts\": The paper currently takes differences of activations and simply run a clustering protocol (Kmeans) to extract \"concepts\" from it. While I'm not necessarily a big fan of SAEs, that approach assumes a specific model of representations (linear representation hypothesis) and hence tries to isolate \"features\" (directions in the learned dictionary) that interfere with each other as minimally as possible, but are also substantially larger than model dimensionality. This induces, or at least we hope that it induces, a monosemantic representation (latent code) such that we can now look at a direction in isolation to interpret what it represents. My main apprehension with the paper is that there is no such affordance achieved by the proposed method. The cluster centroids are in now way more meaningful than base activations are by default. If they are, then authors should have offered examples to demonstrate how. \n\n- Quantitative analysis: The proposed analysis does not meaningfully relate to interpretability in my opinion. Probing on activation differences (diffs) seems as likely to succeed as base activations. Even if this is not perfectly true, probing performance on base activations helps contextualize the improvement diffs help achieve. As it stands, Table 1 has numbers that are very close to each other and I'm unsure if there's any meaningful improvement in performance. \n\n- On consistency and clustering: If I understand correctly, Fel et al. [1] follow a similar protocol as this paper to define a dictionary learning protocol akin to SAEs (called Archetyal analysis) and show that improves consistency for theoretically meaningful reasons. At the very least a discussion is warranted, but more broadly, I think the paper already addresses the challenge of consistency that this paper claims to fix too.\n\n[1] https://arxiv.org/abs/2502.12892\n\nMinor comment \n\n- I think the idea of defining concepts at differences is both philosophically and operationally solid, but the intro and writing doesn't quite do justice to this. There was a good opportunity here to highlight how existing interpretability approaches assume definitions on concepts that fit into different perspective. For example, SAEs' notion of concepts is similar to prototype theory, while more dynamic approaches (like attention probes) are heading in the direction of conceptual role semantics. The Deleuzian lens offers a rich alternative to think about and I'd have appreciated this discussion, along with other frameworks' summary. It would have been even better if an argument were made in the text for why the Deleuzian perspective is better or worse than others."}, "questions": {"value": "In table 3, I do not understand why TopK SAEs trained on activation differences do not work as well as steering by cluster centroids. Can the authors comment what's going on here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "deDlhHrYOx", "forum": "10JEfJtiJM", "replyto": "10JEfJtiJM", "signatures": ["ICLR.cc/2026/Conference/Submission13997/Reviewer_L8Fx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13997/Reviewer_L8Fx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13997/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762406922699, "cdate": 1762406922699, "tmdate": 1762924491742, "mdate": 1762924491742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}