{"id": "cAoUgzEtZ0", "number": 4259, "cdate": 1757648450319, "mdate": 1759898043298, "content": {"title": "Representation Convergence: Mutual Distillation is Secretly a Form of Regularization", "abstract": "In this paper, we argue that mutual distillation between reinforcement learning policies serves as an implicit regularization, preventing them from overfitting to irrelevant features. We highlight two separate contributions: (i) Theoretically, for the first time, we prove that enhancing the policy robustness to irrelevant features leads to improved generalization performance. (ii) Empirically, we demonstrate that mutual distillation between policies contributes to such robustness, enabling the spontaneous emergence of invariant representations over pixel inputs. Ultimately, we do not claim to achieve state-of-the-art performance but rather focus on uncovering the underlying principles of generalization and deepening our understanding of its mechanisms.", "tldr": "A novel theoretical framework for formalizing generalization in reinforcement learning.", "keywords": ["Online Reinforcement Learning", "Generalization Theory", "Representation Learning", "Knowledge Distillation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c3943eeceb8d445029cebb55907dedcf2c916b5d.pdf", "supplementary_material": "/attachment/348e1da6bfa4984539c7c07a241674132310766c.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies generalization in reinforcement learning by examining the robustness of learned features. It proposes (1) a theoretical framework showing how policies sensitive to “rendering” features that do not generalizable suffer from worse performance in expected returns and (2) the conjecture that mutual distillation between two agents regularizes representation learning toward such robust, invariant features and thus improves generalization.\n\nThe paper derives bounds relating expected test returns to terms measuring representation robustness and train/test differences, and empirically investigates whether mutual distillation indeed yields more robust policies on Procgen environments. The empirical results exhibit strong performance gains of the mutual distillation variant over PPO and several controls."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to follow for the most part and has a clear motivated given a widespread interest in generalization and representation learning in RL.\n- exploring generalization in RL through distillation is well-motivated and interesting. As far as I know, this technique is not exhaustively researched / understood in the context of RL.\n- The formalization of rendering families and the decomposition of generalization error into robustness and train-test divergence components is intuitive and provides a neat mathematical framing. I believe this framing ends being equivalent to classical POMDP definitions, so it may be worth to align language (e.g., rendering function - emission function / observation function). \n- MDPO performs consistently better than PPO and other baselines on Procgen tasks, even when controlling for model size or training budget (at least partly)."}, "weaknesses": {"value": "**Theoretical novelty and completeness**:\n\nI found that the novelty of the theoretical exposition is overstated in serveral places. One of the main theorems (Theorem 3.3) mirrors the first-order performance difference bound from Schulman et al. (2015, TRPO) so closely that I believe it should be cited as a known existing bound. \nThe paper furthermore claims to be the first to “ first to provide a rigorous proof of {the} intuition {that robustness to irrelevant features enhances generalization performance}\", which is an overstatement in this phrasing in my view. Several prior works in the literature provide generalization (some as part of sample-complexity) bounds explicitly dependent on representation adequacy. A non-exhaustive list: \n\n- kernel RL and kernel complexity as a measure of representation capacity: Yeh et al. (2023)\n- from a causality perspective: Kallus et al. (2020), Suau et al. (2023)\n- from a symmetry and invariance perspective: Weltevrede et al. (2025)\n\nThe latter of the above even considers distillation for deep RL policies and should most certaintly be discussed in this work. \n\n**Disconnect between theory and practice**\nThe biggest weakness of this paper in my view is the strong disparity between the derived theoretical claims and the conjectured effect of mutual distillation. The argument or intuition for how mutual distillation learns more robust features is very handwavy. For example it is not obvious at all that policies from different intialization or different batch orders or other factors will learn significantly different features. The work above indeed does show a related effect theoretically using neural tangent kernel theory (Weltevrede et al.). Knowing that feature learning is difficult to treat theoretically in deep learning I would suggest an alternative path would be to support the argument more empirically. For example with experiments showing indeed that different initialization / different exploration seeds / different batch orders etc. lead to feature learning of distinct spurious features which can then be eliminated through mutual distillation. \n\n**Presentation**\nThe paper has several issues with presentation and clariity. Apart from several writing / language issues, the are also a number of ambiguities: \n- Inconsistency in robustness metric. The robustness term is defined as a maximum total variation distance max TV distance, yet Table 1 lists values larger than 1.\n- Algorithm 1 implies two agents collecting data separately; it is unclear whether the 50M total steps refer to per agent or shared. The appendix does not explicitly resolve this. Without this clarification, comparisons to PPO may reflect more total experience rather than representational effects.\n- Adversarial/robustness experiments underspecified. The procedure for generating “adversarial renderings” (random CNNs) lacks some detail, and, importantly, motivation. I find this a very perculiar way of generating adversarial examples, could the authors elaborate this choice?\n- some quantities (e.g., $L_\\pi$) is not defined in such a way that the paper is self-sufficient. One should not need to refer to TRPO to recall its definition.\n- Despite being in the title, the term \"representation convergence\" is never formally defined. I'm not following what this term aims to imply, what converges to what? \n\n\n- Yeh, Sing-Yuan, et al. \"Sample complexity of kernel-based q-learning.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.\n- Kallus, Nathan, and Angela Zhou. \"Confounding-robust policy evaluation in infinite-horizon reinforcement learning.\" Advances in neural information processing systems 33 (2020): 22293-22304.\n- Suau, Miguel, Matthijs TJ Spaan, and Frans A. Oliehoek. \"Bad habits: Policy confounding and out-of-trajectory generalization in RL.\" arXiv preprint arXiv:2306.02419 (2023).\n- Weltevrede, Max, et al. \"How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning.\" arXiv preprint arXiv:2505.16581 (2025)."}, "questions": {"value": "- can you provide a theoretical statement (e.g., in linearized form) showing that mutual distillation dynamics reduce the $R$-robustness term? \n- how do you define the “representation convergence” concept?\n- do the mutual distillation learners together use the same amount of samples as PPO baselines? If not, how is data shared or counted? \n- could you describe the random CNNs used to generate alternate renderings and the motivation behind it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M8MyraikyZ", "forum": "cAoUgzEtZ0", "replyto": "cAoUgzEtZ0", "signatures": ["ICLR.cc/2026/Conference/Submission4259/Reviewer_kbQ7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4259/Reviewer_kbQ7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761563352244, "cdate": 1761563352244, "tmdate": 1762917259661, "mdate": 1762917259661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a theoretical and empirical study arguing that mutual distillation between reinforcement learning policies acts as an implicit regularization to prevent overfitting to irrelevant features, thereby improving generalization. It claims to be the first to theoretically prove that enhancing policy robustness to irrelevant features leads to better generalization. The paper introduces Mutual Distillation Policy Optimization (MDPO), which uses Deep Mutual Learning (DML) via a KL divergence loss between to (otherwise independently) trained PPO agents. Empirically, MDPO shows improved generalization performance over a PPO baseline on the hard level configuration of the Procgen benchmark and demonstrates enhanced robustness to visual disturbances like random convolutions or changes in brightness, contrast, saturation or hue.  \n\n**Recommendation:**\\\nI recommend to reject the paper because it is lacking in certain fundamental areas: insufficient positioning with respect to related work,  issues with clarity regarding the significance of theoretical results, empirical results, and the novelty of the ideas/approach."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The central idea of using mutual distillation to induce robustness to spurious correlations in the training data is interesting. \n- The empirical results for MDPO look promising.\n- The analytical experiments of the robustness of the MDPO policy to visual disturbances and the quality of the learned representations in Sections 5.3 and 5.4 are very insightful."}, "weaknesses": {"value": "- The positioning of this work is severely lacking, especially concerning previous literature.\n\t-  The paper seems to miss several related works on topics such as:\n\t\t- Representation learning in RL (for example, [1,2]) \n\t\t- Policy distillation for generalization (for example, [3,4]) \n\t\t- Mutual distillation (for example, [5])\n\t\t- Overfitting to training data in RL (for example, [6,7,8])\n\t\t- The above are not necessarily exhaustive. \n\t- The claims regarding theoretical novelty are strong, but difficult for me to verify due to insufficient discussion of existing work, and my own lack of knowledge on this specific topic.  \n\t- The novelty of the motivation in Section 4 is difficult to judge, since it lacks comparison with related ideas in the existing literature (for example, see [7,8]). \n- The theoretical contribution has some issues:\n\t- The core theoretical claim is formulated too strongly. Corollary 3.8 only shows an increase in the _lower bound_ on generalization performance as robustness increases. However, increasing a lower bound does not guarantee an increase in actualized performance, which is what is promised in the abstract and introduction. \n\t- Clarity of the theoretical framework is lacking. For example, the proofs depend on two policies $\\pi$ and $\\tilde{\\pi}$, but there is no mention of what the significance of these two policies are or how they are related to the overall narrative of the paper. \n\t- A discussion on the significance of the theoretical results is missing. Whether increasing a lower bound has any bearing on the realized performance depends on whether the bound is vacuous or not. This means the bounds derived in Section 3 would benefit from analysis or discussion on how tight they are. For example, the bound's dependence on the relative performance of two arbitrary policies $\\pi$ and $\\tilde{\\pi}$ complicates the interpretation of its significance. \n- The motivation for mutual distillation seems conceptually incomplete. The logic presented in Sections 4.1 and 4.2 hinges on the policies encountering different spurious correlations due to collecting different trajectories as a result of different initializations. However, the DML mechanism's goal is to regularize the two policies to converge to the same policy. This seems to introduce a paradox: in order to benefit from the two policies collecting different training data, they need to be regularized to be the same. \n- Experimental details are missing, making it difficult to judge the validity and significance of the results. For example, the paper claims significant improvements, but there is no mention of number of seeds used, what the shaded areas in the figures denote, or how significance was tested."}, "questions": {"value": "- Does it matter how $\\pi$ and $\\tilde{\\pi}$ are related? Can we view them through the lens of the two policies used in mutual distillation somehow? \n- The policy $\\pi$, $\\tilde{\\pi}$ and the linear approximation $L_\\pi(\\tilde{\\pi})$ (the only positive term in the lower bound in Corollary 3.8), seem to originate from [9], which assumes $\\pi$ and $\\tilde{\\pi}$ are close to each other. Does this mean the lower bound will only be non-vacuous if $\\pi$ and $\\tilde{\\pi}$ are similar? How does this impact the significance of the derived bounds?\n- Line 200: \"During the training process, we can only empirically bound $\\mathfrak{D}_{train}$\" Why? \n- Figure 2: \"(Right) Through mutual distillation via DML, two policies regularize each other to converge toward a more robust hypothesis space,...\" I agree that they are regularized towards each other, but why would that be toward a more robust hypothesis space, and not just any other part of the non-robust hypothesis space?\n- Why is an algorithm that regularizes two policies towards each other, the solution for the problem identified in Figure 1 that requires the two policies to collect substantially different data?\n- For the experiments, how many seeds are used and what do the shaded regions indicate? Also, how is significance of the results determined?\n\n\n**Things to improve that did not impact decision:**\n- There many missing words, making sentences incomplete (for example, line 58 and 65 of the introduction). \n- Section 2.1: Stating that a function $f: X \\to [0,1]$ does not sufficiently define it as a probability distribution (it is missing the constraint that all the probabilities sum up to 1). \n- Section 2.1: The authors seem to introduce a new MDP framework with the rendering function in the background section. Either this is new, which means it should not be in the background section, or it is an existing framework from somewhere else, which means it is missing a citation. \n\t- The framework also seems reminiscent of a specific type of contextual MDP [10], perhaps it could be useful to frame this work within the CMDP framework. \n- Table 2: It would be interesting to also include the training performance for this experiment.\n- Figure 3: Does the x-axis for MDPO include the timesteps of both agents (since they collect data independently)? In other words, at timestep 50 million in the figure, the individual MDPO policies will only have trained on 25 million steps each? If not, I feel this figure is slightly misleading. \n\n**References:**\\\n[1] Learning Invariant Representations for Reinforcement Learning Without Reconstruction. Zhang et al. 2021\\\n[2] Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL. Mazoure et al. 2022\\\n[3] Learning Dynamics and Generalization in Reinforcement Learning. Lyle et al. 2022\\\n[4] How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning. Weltevrede et al. 2025\\\n[5] Dual Policy Distillation. Lai et al. 2020\\\n[6] On the Importance of Exploration for Generalization in Reinforcement Learning. Jiang et al. 2023\\\n[7] Policy Confounding and Out-of-Trajectory Generalization in RL. Suau et al. 2024\\\n[8] Exploration Implies Data Augmentation: Reachability and Generalisation in Contextual MDPs. Weltevrede et al. 2025\\\n[9] Trust Region Policy Optimization. Schulman et al. 2015\\\n[10] A survey of zero-shot generalisation in deep reinforcement learning. Kirk et al. 2023"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3f3dx3fNkC", "forum": "cAoUgzEtZ0", "replyto": "cAoUgzEtZ0", "signatures": ["ICLR.cc/2026/Conference/Submission4259/Reviewer_zv9o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4259/Reviewer_zv9o"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841139477, "cdate": 1761841139477, "tmdate": 1762917259396, "mdate": 1762917259396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper attempts to prove the conjecture that the policy which is robust to irrelevant features would lead to improved generalization performance. In addition, they propose mutual distillation of policies to achieve such robustness and presents the intuition behind that. While they do not show state-of-the-arts performance, they present proof-of-concept for their approach with basic regularization baseline on all environments of Procgen benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Strengths:\n\n1. This work presents a formal proof of a long standing assumption that robustness of policy against irrelevant features improves generalization. In particular, they derive a lower bound for generalization performance that includes minimization of a robustness term, which is defined how a policy is influenced by two different rendering (perturbation) functions.\n\n2. The paper is presented in a clear and well-organized way. Especially, Fig. 1 and 2 helps the readers to better understand the intuition and impact of DML in the discussed setting. \n\n3. Legitimate ablations are conducted and the results validate the claims."}, "weaknesses": {"value": "### Weaknesses:\n\n1. The proposed method has been validated only on the ProcGen benchmark. Experiments on more diverse set up is needed to show the applicability of such methods. \n\n2. While I understand that the target is not to outperform the state-of-the arts, but how DML stands against other data augmentation based approaches such as [1] are not evident. While the authors present result with SPO, it seems SPO performance itself is not upto the current standard. \n\n3. The proposed method relies on multiple policies for distillation. However, the computational overhead compared to single policy methods is not discussed.\n\n[1] Raileanu, Roberta, et al. \"Automatic data augmentation for generalization in reinforcement learning.\" Advances in Neural Information Processing Systems 34 (2021): 5402-5415."}, "questions": {"value": "I am wondering how MDPO will scale to large number of policies beyond only two policies as discussed. Can you share some insights on that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "037v07Ey1g", "forum": "cAoUgzEtZ0", "replyto": "cAoUgzEtZ0", "signatures": ["ICLR.cc/2026/Conference/Submission4259/Reviewer_xv4v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4259/Reviewer_xv4v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998035149, "cdate": 1761998035149, "tmdate": 1762917259139, "mdate": 1762917259139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretical framework to demonstrate that improving the policy robustness to irrelevant features enhances its generalisation performance. The paper further shows that deep mutual learning forms an implicit regularisation and prevents policy from overfitting to irrelevant features. Empirical results are given on the ProcGen benchmark, designed to test generalisability under controlled environments, as well as for toy examples. The proposed method, mutual distillation policy optimisation, demonstrates benefits as compared to the selected baseline approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a new theoretical framework to investigate generalisation issues in deep RL. Generalisation in RL is a major and actively researched topic. The insights provided by the paper will have far reaching impact."}, "weaknesses": {"value": "Although impactful, the experimental evaluation is limited. In the sense that, it doesn't demonstrate the phenomenon exists, beyond testing on the ProcGen benchmark and presenting performance. Also, apart from the toy example. \n\nThere are other methods focusing on distillation (mutual or peer). However, these papers seem not to be mentioned in the paper. It would be good to see a comparison, for example, \n\n* Periodic Intra-Ensemble Knowledge Distillation for Reinforcement Learning, https://arxiv.org/pdf/2002.00149\n\n* Robust Domain Randomised Reinforcement Learning through Peer-to-Peer Distillation, https://arxiv.org/pdf/2012.04839 (this has been cited in the paper) \n\n* Online Policy Distillation with Decision-Attention, https://arxiv.org/pdf/2406.05488\n\nWhile the theoretical framework seems to be strong, the paper lacks interpretability. More experiments would be helpful to understand the impact of mutual distillation on the representation space. \n\nRunning multi distillation may increase the computational overhead, which has not beed discussed in the paper. \n\nMutual distillation offers regularisation by reducing the reliance on irrelevant features. However, the generalisability of the approach hasn't been investigated."}, "questions": {"value": "1) Have the authors analysed or discussed the additional cost introduced by mutual distillation? \n\n2) Has the method been tested across tasks with different sources of irrelevant variation, or is it specific to the evaluated benchmarks?\n\n3) Can the representational changes be visualised or quantified to support the theoretical claims?\n\n4) Could the authors comment on how their approach differs from or relates to the works mentioned above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rW9eLahMJ2", "forum": "cAoUgzEtZ0", "replyto": "cAoUgzEtZ0", "signatures": ["ICLR.cc/2026/Conference/Submission4259/Reviewer_7ZFX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4259/Reviewer_7ZFX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4259/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762087812677, "cdate": 1762087812677, "tmdate": 1762917258852, "mdate": 1762917258852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}