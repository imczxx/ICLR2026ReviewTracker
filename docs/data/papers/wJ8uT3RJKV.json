{"id": "wJ8uT3RJKV", "number": 11666, "cdate": 1758202935167, "mdate": 1763599863526, "content": {"title": "Towards Federated Reinforcement Learning Free of Problem-Parameter-Based Tuning", "abstract": "Federated reinforcement learning (FRL) has emerged as a promising framework for decentralized decision-making in privacy-sensitive environments. However, its practical deployment is often impeded by the need for extensive hyperparameter tuning, a challenge further exacerbated by non-stationary data distributions and environment heterogeneity across agents. To overcome these limitations, we propose a \\textbf{problem-parameter-free FRL framework} that eliminates manual stepsize selection through a novel integration of adaptive strategies and momentum-based updates. To further address environment heterogeneity, we introduce carefully designed control variates at both the client and server levels. Building on this framework, we develop two algorithms: \\textbf{PFedPG-VR}, which integrates an adaptive scheme into variance-reduced policy gradient updates, and \\textbf{PFedPG-HA}, which refines the approach using Hessian-aided corrections. Through rigorous theoretical analysis, we prove that both algorithms achieve state-of-the-art convergence rates, with a sample complexity of $\\mathcal{O}(\\epsilon^{-3})$ and a communication complexity of $\\mathcal{O}(\\epsilon^{-2})$. In addition, they enjoy linear convergence speedups with respect to the number of agents and local update steps at each global round. Notably, our methods eliminate the reliance on environment heterogeneity bounds required in previous FRL approaches, significantly broadening their applicability. Extensive experiments on benchmark FRL tasks further demonstrate the superior performance and robustness of our proposed algorithms compared to existing baselines.", "tldr": "", "keywords": ["Federated reinforcement learning", "adaptive optimization", "adaptive stepsize", "environment heterogeneity"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/329e6eea71df67dd2b9b612c1a3103137ddc460d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates Federated Reinforcement Learning (FRL) under environment heterogeneity. The authors propose a problem-parameter-free FRL algorithm that does not require hyperparameter tuning (learning rates/stepsizes) or rely on unknown problem-specific parameters (e.g., smoothness constants, heterogeneity bounds, gradient variances) via a local adaptive stepsize. They develop federated policy gradient algorithms (PFEDPG-VR, PFEDPG-HA) with momentum and prove that these achieve state-of-the-art convergence rates."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a genuine practical problem - the need for manual hyperparameter tuning in federated reinforcement learning (FRL) that depends on unknown problem-specific parameters.\n\n1. They demonstrate that the proposed algorithm can achieve state-of-the-art convergence rates.\n\n1. They demonstrate the superior performance of the proposed method compared with baselines FEDSVRPG-M (Wang et al. 2024a) and PAVG."}, "weaknesses": {"value": "1. Limited Novelty Beyond Stepsize: The core algorithmic structure, employing momentum-based updates combined with variance reduction (VR) or Hessian-aided (HA) techniques for heterogeneous FRL, closely follows the approach proposed by Wang et al. (2024a). The primary novel component appears to be the integration of the adaptive stepsize mechanism to achieve the \"parameter-free\" property for tuning. The paper could benefit from a more detailed discussion clarifying the specific algorithmic design differences compared to Wang et al. (2024a) beyond the stepsize rule itself and elaborating on how the normalization specifically circumvents the reliance on problem constants in the stepsize formulas and what the technical contribution is in deriving the same convergence rate with the adaptive stepsize.\n\n2. Unclear dependency of problem-specific parameters in the convergence rate bound: It seems that the convergence bound is dependent on many problem-specific parameters (G, M, W, H, $\\gamma$, R), which are roughly written as $\\hat{L}$ and $\\overline{L}$ in Theorem 4.4 and 4.5. However, given that the consequence of using a learning rate not considering problem-specific parameters may appear in the convergence rate, characterizing the dependency of the parameters on the convergence rate seems very critical. It would be better to present the exact effect of the problem-specific parameters on the convergence more clearly in the theorems.\n\n3. Lack of comparison on the dependency of problem-specific parameters: The paper does not analyze whether the dependence of its convergence rate on the aggregated constants ($\\hat{L}$, $\\overline{L}$) is better, worse, or the same compared to the dependence shown in Wang et al. (2024a) (which uses problem-dependent stepsizes). Since the main contribution is being \"parameter-free,\" it is crucial to understand if this advantage in tuning comes at the cost of potentially worse dependencies on the problem-specific parameters in the convergence rate bound. This comparative analysis regarding the rate's parameter dependency is missing.\n\n\n[1] Han Wang, Sihong He, Zhili Zhang, Fei Miao, and James Anderson. Momentum for the win: Collaborative federated reinforcement learning across heterogeneous environments. arXiv preprint arXiv:2405.19499, 2024a."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9JiR8jgVd4", "forum": "wJ8uT3RJKV", "replyto": "wJ8uT3RJKV", "signatures": ["ICLR.cc/2026/Conference/Submission11666/Reviewer_wY6d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11666/Reviewer_wY6d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632104602, "cdate": 1761632104602, "tmdate": 1762922723160, "mdate": 1762922723160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a federated reinforcement learning (FRL) framework that removes the need for manual step-size tuning, enabling privacy-preserving intelligent decision-making in distributed settings. The framework unifies adaptive policy and momentum updates with carefully designed control variables at both the client and server, thereby obviating manual step-size selection. The authors further propose two algorithms: TFEDPG-VR, which employs a variance-reduced policy-gradient update, and TFEDPG-HA, which enhances performance via a Hessian-aided adjustment. Theoretical analysis shows that both algorithms achieve optimal sample complexity O(ε^-3) and communication complexity O(ε^-2), with linear speedups as the number of participating agents and local update steps increase. Experiments corroborate superior performance and robustness, demonstrating that the methods effectively avoid hyperparameter tuning even under numerous unknowns, thus simplifying practical deployment and improving adaptability and generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Although I am not major in the federate learning but only for reinforcement learning. From the perspective of a researcher in the field of reinforcement learning, this paper excels in terms of methodological innovation, theoretical integrity, and experimental completeness, I think it is a solid paper."}, "weaknesses": {"value": "The empirical evaluation lacks statistical rigor. Learning curves appear to be based on a single run without confidence intervals or standard deviations. Reporting results over multiple independent seeds with appropriate statistical measures (e.g., mean ± standard deviation or 95% confidence intervals) is necessary to substantiate the claimed performance and improve the credibility of the results."}, "questions": {"value": "You can observe the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Psco0gKlrC", "forum": "wJ8uT3RJKV", "replyto": "wJ8uT3RJKV", "signatures": ["ICLR.cc/2026/Conference/Submission11666/Reviewer_hFbc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11666/Reviewer_hFbc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815526061, "cdate": 1761815526061, "tmdate": 1762922722797, "mdate": 1762922722797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel problem-parameter-free framework for Federated Reinforcement Learning (FRL) to address the critical challenge of manual hyperparameter tuning in heterogeneous environments. The authors propose two algorithms, PFedPG-VR and PFedPG-HA, which integrate adaptive step sizes, momentum-based updates, and control variates, achieving state-of-the-art convergence guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**Novel and Impactful Framework:** The proposed problem-parameter-free FRL framework is both innovative and practically significant. It eliminates the need for manual hyperparameter tuning, thereby saving substantial time and improving the stability of the learning algorithms.\n\n**Free of Arbitrary Heterogeneity:** The use of momentum-based update removes the need for restrictive bounded-heterogeneity assumptions. Consequently, the convergence guarantees are independent of any additive error terms related to the degree of heterogeneity.\n\n**State-of-the-Art Theoretical Performance:** The proposed algorithms are proved to attain the optimal sample complexity of $O(\\varepsilon^{-3})$ and communication complexity of $O(\\varepsilon^{-2})$, matching the state of the art for policy-based federated reinforcement learning."}, "weaknesses": {"value": "**Weakness 1: Unclear Contributions of Algorithmic Design.** The algorithmic design contributions are not clearly justified. Both the control variate and normalization components lack sufficient theoretical or intuitive motivation in the main text. While the paper claims that these techniques help remove heterogeneity-dependent parameters, their specific roles in achieving this effect remain unclear. Moreover, recent work [1] achieves convergence under arbitrary heterogeneity without incorporating either control variates or normalization, suggesting that these components may not be essential to the claimed theoretical improvements. This issue is particularly important given that, according to the ablation study, removing the control variate component leads to only marginal differences in practical performance compared to the full algorithm. A more detailed explanation is needed to clarify the concrete contributions of these design choices.\n\n**Weakness 2: Lack of Reward Heterogeneity Experiments.** While the proposed algorithms are theoretically designed to handle arbitrary environment heterogeneity, the experimental validation primarily focuses on heterogeneity in transition dynamics and initial state distributions. No explicit experiments are provided to assess performance under reward heterogeneity, where agents have different reward functions.\n\n[1] Wang, Han, et al. \"Momentum for the Win: Collaborative Federated Reinforcement Learning across Heterogeneous Environments.\" International Conference on Machine Learning. PMLR, 2024."}, "questions": {"value": "1. How do control variates specifically contribute to the “problem-parameter-free” framework? From the ablation study, removing control variates seems to result in very similar performance. Theoretically, prior work [1] achieves convergence under arbitrary heterogeneity without using control variates. \n\n2. The paper suggests that normalization is a key design for achieving a parameter-free learning rate. Could the authors provide a more technical or intuitive explanation to support this claim?\n\n3. Could the authors include experiments on reward heterogeneity, as it is another common form of environmental heterogeneity?\n\n4. Could the authors compare the convergence results presented in Theorems 4.5 and 4.6 with those in [1]?\n\n[1] Wang, Han, et al. \"Momentum for the Win: Collaborative Federated Reinforcement Learning across Heterogeneous Environments.\" International Conference on Machine Learning. PMLR, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cW9oiHrLVS", "forum": "wJ8uT3RJKV", "replyto": "wJ8uT3RJKV", "signatures": ["ICLR.cc/2026/Conference/Submission11666/Reviewer_78RH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11666/Reviewer_78RH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832979656, "cdate": 1761832979656, "tmdate": 1762922722412, "mdate": 1762922722412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a “problem-parameter-free” FRL framework with two instantiations: PFEDPG-VR (variance-reduced policy gradient) and PFEDPG-HA (adds Hessian-vector corrections). Core ingredients are: (i) client & server control variates, (ii) client-side momentum, and (iii) per-step normalization (fixed-length local updates). The stepsizes (η,λ,β) are set as functions of (N,K,T), avoiding unknown smoothness/variance/heterogeneity constants. The theory shows convergence to an ε-stationary point with sample complexity O(ε−3) and communication O(ε−2), with linear speedups in N and K. Experiments on random tabular MDPs and MuJoCo (CartPole, HalfCheetah) report moderate gains and robustness to learning-rate choice."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Clear engineering of several stabilizing tricks (control variates, momentum, normalized steps) into a coherent FRL algorithm.\n\n* No heterogeneity magnitudes are required to set hyperparameters, unlike prior FRL that bake heterogeneity bounds into stepsizes. The rates still hold without “neighborhood” convergence caveats.\n\n* The analysis preserves linear gains in agents and local updates while keeping communication as commonly observed in FRL works. The proofs seem clear and easy to follow (I haven’t checked all of them in full detail but generally seem OK)"}, "weaknesses": {"value": "* The algorithms recombine known components already explored in FRL/RL (client momentum, control-variate corrections akin to SCAFFOLD, Variance reduction, Hessian-aided updates) and the paper seems combinatorial. The headline rates (sample O(ε−3), communication O(ε−2), linear N,K speedups) are the same prior work which is underwhelming; the main sell/novel bit is “tuning-free” hyperparameters depending only on (N,K,T). It is not very valuable in practice since the learning rates are generally “tuned” as the upper bounds are generally loose.\n\n* No heterogeneity bounds claim feels oversell. The analysis still assumes bounded log-gradients/Hessians (Assump. 4.1), bounded gradient variance (4.2), and bounded IS-weight variance (4.3). In heterogeneous RL, IS weights can blow up; assuming a global bound is strong. Assumption 4.3 also seems particularly strong and i guess will generally be false without restricting the proximity of $\\theta_1,\\theta_2$, because importance weight can have unbounded or heavy–tailed variance when policies place near–zero mass on actions supported by the other policy. It would be better to cite papers making the assumption next to them and not collectively.\nIn the theorems you called \\hat{L} a constant that depends on H, \\gamma, R_{max} etc however I feel the dependence should be explicitly shown as a part of sample complexity bounds as done in most of RL and FRL works cited in your paper.\n\n* Since the whole sell of the paper (strength no 2) is not having to tune learning rates can it becomes important to discuss how you lose on methods which allow tuning in terms of rates/upper bounds and how you lose when the heterogeneity is high, however the paper only says they are equivalent to works which allows tuning in terms of their dependence on $epsilon$, I would encourage the authors to atleast discuss the dependence on heterogeneity parameters like $\\sigma$, IS weight variance compared to prior works since they have a same rate in $\\epsilon$. I imagine you might lose out on some performance gains compared to these methods which allow environment adaptive learning rate tuning.\n\n* There are some places where the notation and sometimes theory has some minor typos that cause confusion: For eg. in thm 1 you set the learning rate to 1/\\sqrt{KT} but in table 1 it become 1/KT, Several places jump from conditional E[⋅∣\\mathcal{F}_t]to unconditional without comment. Add a subscript $t$ each time. It's otherwise hard to follow downstream which expectations are conditional. Some polishing will help."}, "questions": {"value": "* If the same learning rate schedules work across different heterogeneity conditions, you might definitely lose on performance compared to methods that do allow manual hyperparameter tuning as commonly done in practice. This is not discussed in the paper. Depending on the discussion, one might be able to use the rates suggested here as a good starting point, irrespective of environment heterogeneity conditions followed by tuning to improve performance (as it is a common practice). Such a discussion will be beneficial to assessing whether the results are interesting in a federated RL context beyond combining gains from commonly known ideas in the literature like Importance sampling, variance reduction (STORM), control variates.\n\n* I understand that the paper is mainly theoretical, but I have a question about the experimental results presented. The baselines appear to use fixed LRs (e.g., 0.005/0.6), while your method is auto-set by theory. Please sweep baselines over sensible grids and report best-of-sweep. Is that being done and the best curves are being reported here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Im3vvxdVpl", "forum": "wJ8uT3RJKV", "replyto": "wJ8uT3RJKV", "signatures": ["ICLR.cc/2026/Conference/Submission11666/Reviewer_MNvV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11666/Reviewer_MNvV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909758790, "cdate": 1761909758790, "tmdate": 1762922721955, "mdate": 1762922721955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}