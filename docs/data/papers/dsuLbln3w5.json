{"id": "dsuLbln3w5", "number": 21670, "cdate": 1758320356430, "mdate": 1759896909500, "content": {"title": "Neural USD: An object-centric framework for iterative editing and control", "abstract": "Amazing progress has been made in controllable generative modeling, especially over the last few years. However, some challenges remain. One of them is precise and iterative object editing. In many of the current methods, trying to edit the generated image (for example, changing the color of a particular object in the scene or changing the background while keeping other elements unchanged) by changing the conditioning signals often leads to unintended global changes in the scene. In this work, we take the first steps to address the above challenges. \n\nTaking inspiration from the Universal Scene Descriptor (USD) standard developed in the computer graphics community, we introduce the “Neural Universal Scene Descriptor” or Neural USD. In this framework, we represent scenes and objects in a structured, hierarchical manner. This accommodates diverse signals, minimizes model-specific constraints, and enables per-object control over appearance, geometry, and pose. We further apply a fine-tuning approach which ensures that the above control signals are disentangled from one another. We evaluate several design considerations for our framework, demonstrating how Neural USD enables iterative and incremental workflows.", "tldr": "This paper introduces \"Neural USD,\" a flexible, model-agnostic conditioning structure inspired by the Universal Scene Descriptor (USD), enabling iterative editing of appearance, geometry, and pose in generative image models.", "keywords": ["neural", "editing", "universal", "scene", "descriptors", "image", "generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a575cab7e7839f5f19c0c7c622fb45f9d671677.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces \"Neural USD,\" an object-centric framework designed to enable precise and iterative editing of generative models. Taking inspiration from the Universal Scene Descriptor (USD) standard in computer graphics, the authors propose representing a scene's components (objects, background) as assets with distinct, hierarchical attributes: appearance, geometry, and pose. The core technical contribution is a fine-tuning strategy that uses paired images from video sequences to train a generative model to disentangle these control signals. The paper demonstrates that this framework allows for object-level manipulations, such as changing pose, appearance, or geometry, and replacing objects or backgrounds, while aiming to keep other scene elements consistent."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clever Conceptual Bridging: The core idea of adapting the structured, hierarchical USD standard from computer graphics to the conditioning of diffusion models is both novel and elegant. It provides a principled-sounding approach to a problem often tackled with less structured methods.\n\nCore Training Strategy: The method of using paired images ($I_{src}$ for appearance/geometry, $I_{tgt}$ for pose) to force the model to learn disentangled representations is a key insight and a strong methodological contribution."}, "weaknesses": {"value": "The paper's primary qualitative example, Figure 1, seems to undermine its central claim of disentangled control. In Fig 1(b), the stated operation is a \"Pose\" change. However, the background has clearly changed in appearance compared to Fig 1(a). Furthermore, the object's own appearance (the orange chair) also appears to have different lighting/shading in 1(b).\n\nMissing Critical Ablation Studies (Especially on Geometry): The framework's complexity (requiring pose, appearance, and geometry) is not sufficiently justified. The authors have not provided a crucial ablation study to demonstrate the necessity of the geometry (e.g., depth map) signal. How does the model perform with only Pose + Appearance conditioning? As user paste the warped cropped region into background, then inpaint the image, it seems to get the similar result? Without this ablation, it is impossible to assess the contribution of the geometry component. This is a significant gap in the experimental validation.\n\n(Minor Weakness): As noted, the main paper is surprisingly sparse on qualitative results, relegating most examples to the appendix.  Given that the paper does not fill the 9-page, key supporting results (especially those that successfully demonstrate the claims from Fig 1) should have been included in the main body."}, "questions": {"value": "Same as weakenss."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w9ALE3GWaG", "forum": "dsuLbln3w5", "replyto": "dsuLbln3w5", "signatures": ["ICLR.cc/2026/Conference/Submission21670/Reviewer_sNZo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21670/Reviewer_sNZo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761742851509, "cdate": 1761742851509, "tmdate": 1762941884590, "mdate": 1762941884590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Neural USD. In general, it can be seen as a ControlNet with a finer grid for object control. The paper generates a dataset with detailed annotations such as depth, boxes, and many others, then uses this information to fine-tune a pre-trained image model. The fine-tuned model shows great results in image control capability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper targets a great problem for finer control of the objects in an image. The proposed method, although simple, is pretty straightforward and effective.\n\n2. The datasets in the paper can benefit future research.\n\n3. The demonstrated results are good."}, "weaknesses": {"value": "1. Since the model is still a learning-based image-to-image model, keeping other objects unchanged is not guaranteed. I can see obvious background change in Figure 1."}, "questions": {"value": "I do not see major weaknesses or questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AlzO5NDVSc", "forum": "dsuLbln3w5", "replyto": "dsuLbln3w5", "signatures": ["ICLR.cc/2026/Conference/Submission21670/Reviewer_PGAs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21670/Reviewer_PGAs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905235453, "cdate": 1761905235453, "tmdate": 1762941884129, "mdate": 1762941884129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a new object-centric generative framework inspired by USD standard, aiming to unify representation, control, and editing in image and 3D content generation. The core idea of this method is to tokenize each object in a scene, capturing its geometry, appearance, pose, and material attributes, into a structured latent representation that can condition diffusion or transformer-based generative models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea is interesting to me, that is a unified, structured conditioning standard inspired by USD, enabling disentangled control over object appearance, geometry, and pose in generative models.\n2. The ability to perform multi-step, fine-grained, object-level edits without unintended global changes, a clear improvement over existing conditioning methods (e.g., ControlNet, InstructPix2Pix).\n3. The format is architecture-agnostic and supports diffusion, DiT, and transformer models through tokenized conditioning, improving portability and generalizability."}, "weaknesses": {"value": "1. Global scene changes still occur, as I observed from the visual results. It is somewhat overclaimed, as the abstract and introduction sections stated.\n2. It remains unclear whether the fusion happens at the feature level (joint embedding) or via concatenated conditioning channels.\n3. Uses Stable Diffusion v2.1 as backbone, leading to lower image quality than state-of-the-art diffusion models; I still believe it should be adopted in more powerful backbones (e.g., Flux)."}, "questions": {"value": "Please refer to the weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cG9j47eVP3", "forum": "dsuLbln3w5", "replyto": "dsuLbln3w5", "signatures": ["ICLR.cc/2026/Conference/Submission21670/Reviewer_WFwE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21670/Reviewer_WFwE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008807119, "cdate": 1762008807119, "tmdate": 1762941883747, "mdate": 1762941883747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work enables precise control over an object’s appearance, geometry, and pose. By encoding geometry, depth maps, and bounding boxes into the image generation model, it can control the placement of specific objects within specific scenes, as well as edit their pose/appearance, background, and viewpoint. Moreover, it supports an iterative editing workflow, allowing continuous refinement of scene composition and object attributes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tAchieves disentangled control over an object’s appearance and geometry in a simple and straightforward manner.\n2.\tEnables more precise control of objects, with quantitative metrics outperforming previous methods.\n3.\tDesigns a stable iterative 3D editing workflow, allowing sequential replacement of pose, appearance, object, and background, while preserving the results of previous edits during new editing steps."}, "weaknesses": {"value": "1.\tThe paper repeatedly emphasizes that it can edit an object’s pose while keeping other attributes of the source image unchanged (as stated in the abstract, section 4.2). However, in practice, the camera pose and object pose are not successfully disentangled — pose editing often causes a change in the viewing angle instead of the object moving relative to the scene (see Fig. 9).\n2.\tThis work follows a technical route very similar to Neural Assets, with comparable metrics (see Figs. 7 and 8), yet lacks corresponding visual comparisons.\n3.\tThe background after editing differs noticeably from the original image (Figs. 5 and 6), and the object texture and appearance exhibit clear artifacts (e.g., Fig. 6 (c)). The authors attribute this to the base image model not being state-of-the-art, but this explanation remains unverified. The presence of such visible artifacts after even a single edit undermines the benefit of an iterative workflow, since preventing cumulative errors across edits is one of its key goals.\n4.\tThe contributions are relatively limited.\nContribution 1 claims that the model learns disentangled control signals from video image pairs, but this training paradigm was already introduced by Neural Assets. The main difference lies in the additional encoding of geometry, enabling finer-grained disentanglement.\nDiscussion of Contribution 2 is provided in the previous point 3.\nContribution 3 claims improved accuracy based on Fig. 8, but the figure shows no significant improvement over Neural Assets.\n5.\tIn Figs. 5 and 6, the 3D bounding boxes before and after editing should be marked."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KpVwsvPHgc", "forum": "dsuLbln3w5", "replyto": "dsuLbln3w5", "signatures": ["ICLR.cc/2026/Conference/Submission21670/Reviewer_Mwtn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21670/Reviewer_Mwtn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21670/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762170037287, "cdate": 1762170037287, "tmdate": 1762941883294, "mdate": 1762941883294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}