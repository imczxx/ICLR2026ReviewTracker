{"id": "Iq6o8A8NVA", "number": 14378, "cdate": 1758234142846, "mdate": 1759897374060, "content": {"title": "Manifold-Matching Autoencoders", "abstract": "We propose Manifold-Matching Autoencoders (MMAEs), a simple yet effective framework that aligns autoencoder latent spaces with precomputed geometric references. This is accomplished by using distance-based regularization to match latent and reference distance matrices, enabling the same architecture to achieve different data representations by simply changing the reference embedding. We demonstrate that MMAEs achieve scalable topological control in high-dimensional settings where existing methods become computationally intractable. One key finding is that aligning with PCA yields unexpected benefits: MMAEs achieve SOTA preservation of the original data structure, comparable to sophisticated topological autoencoders, while maintaining significantly better reconstruction quality and more efficient computation. When combining with VAEs, the present regularization has the effect of concentrating variance in fewer dimensions. This balance between structure preservation, variance concentration, and reconstruction fidelity enables superior generative capabilities, including clearer interpolations and more effective discovery of semantically meaningful latent directions for attribute manipulation.", "tldr": "We align autoencoder latent spaces with pre-computed embeddings using distance-based regularization to control topology. We show benefits in downstream tasks such as generation of synthetic images.", "keywords": ["manifold learning", "autoencoders", "topology preservation", "dimensionality reduction", "representation learning", "geometric regularization", "unsupervised learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a09b4ef162ebf334fbd584a8e35fce62f0f8a898.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a Manifold-Matching Autoencoders (MMAEs), a framework that aligns autoencoder latent spaces with precomputed geometric references. This is accomplished by using distance-based regularization to match latent and reference distance matrices."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper's organization is mostly fine, the main ideas are easy to follow."}, "weaknesses": {"value": "1) The main weakness is a limited scientific novelty. In a nutshell, authors propose to add a distance-based regularizer between real-latent spaces to an AE objective. \n\n2) Experimental results in Fig. 3 are very hard to interpret. Consider presenting results in a Table or by a different visualization.\nComparison with SOTA methods like IVIS, RTD-AE, PacMAP, PHATE, and MDS are missing.\nConsider evaluating linear correlation, the triplet distance ranking accuracy, Wasserstein distance between persistence barcodes (Moor et al., 2020b; Trofimov et al., 2023)\n\n3) Figure 4 is hard to interpret. Consider using quantitative evaluation, maybe one can train a classifier for detecting quality of interpolation or use human assessment by services like Mechanical Turk.\nA comparison with SOTA disentanglement methods like DAVE, beta-VAE, beta-TCVAE, FactorVAE are missing.\nConsider evaluating standard disentanglement metrics like MIG, FactorVAE score MIG, SAP, DCI."}, "questions": {"value": "1) How do you select a regularization coefficient lambda?\n2) What is your interpretation of experimental results in Fig. 1? It seems to be no significant difference after addition of MM regularizer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yM4GltOwYY", "forum": "Iq6o8A8NVA", "replyto": "Iq6o8A8NVA", "signatures": ["ICLR.cc/2026/Conference/Submission14378/Reviewer_VzYg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14378/Reviewer_VzYg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554475221, "cdate": 1761554475221, "tmdate": 1762924795742, "mdate": 1762924795742, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new dimensionality reduction method called Manifold-Matching Autoencoder (MMAE). This is essentially a parametrized version of a well-known Multidimensional Scaling (MDS), where the pairwise distances are obtained from precomputed embeddings, for example, using PCA. As in MDS, the loss is calculated between the normalized pairwise distance matrices and added to the standard reconstruction loss. The authors report experiments in two scenarios -- visualization and generation (MMVAE)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Text**: The paper is nicely structured and easy to follow. The idea is explained in detail.\n - **Experiments**: two setups are explored -- generative (VAE) and NLDR."}, "weaknesses": {"value": "- **Idea**: The method depends entirely on precomputed embeddings (e.g., PCA, UMAP), which already provide a geometric structure. It is not clearly explained why training a parametric autoencoder is required beyond simply using these embeddings directly. The paper argues that topological autoencoder methods fail in higher-dimensional bottlenecks due to the computational costs of persistence homology; however, just like MMAE, it also operates on the pairwise distances and doesn't depend on the dimension, so this claim is questionable and not supported by the experimental evidence.\n - **Metrics and results**: RMSE between distance matrices appears identical to the training loss (Eq. 3). Reporting it as a performance metric risks circular reasoning and does not reflect true global structure preservation. To substantiate claims of global and local structure preservation, additional metrics should be included, like clustering quality (ARI, Silhouette) for local structure, and global topology metrics from TopoAE or RTD-AE. Figure 6 (II) shows that MRRE (which measures rank correlation of pairwise distances) is lower for MMVAE than for TopoVAE on the reported datasets, contradicting the claim that MMVAE best preserves global structure. The authors do not discuss these inconsistencies. The claim that MMAE captures global topology is unsupported without evaluation on synthetic manifolds with known structure (e.g., spheres, Swiss rolls, rings). Current datasets (MNIST, F-MNIST, CIFAR-10, CelebA) have complex, unknown topology, so qualitative “global structure preservation” is difficult to verify. Many improvements reported for “global structure” may derive from alignment with PCA rather than genuine topology preservation. Without baseline comparisons to raw PCA or UMAP embeddings in the results tables, it is hard to isolate the effect of the proposed regularization."}, "questions": {"value": "- How exactly is the RMSE score computed? Why do lower MRRE values indicate better performance, and how should we interpret the discrepancies where TopoVAE achieves lower MRRE than MMVAE (Figure 6 (II))?\n - To substantiate the claim of global structure preservation, could you provide results on datasets with known topology (e.g., Spheres & rings from TopoAE)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jgRbq0Zwnt", "forum": "Iq6o8A8NVA", "replyto": "Iq6o8A8NVA", "signatures": ["ICLR.cc/2026/Conference/Submission14378/Reviewer_jyBR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14378/Reviewer_jyBR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851076036, "cdate": 1761851076036, "tmdate": 1762924795237, "mdate": 1762924795237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to enhance an autoencoder by aligning the latent space with precomputed reference embeddings. In particular, the alignment is done through matching the pairwise distances of the latent vectors with those of the reference embeddings. The authors argue that this approach better preserves the topological structure of high-dimensional data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper originates from a good motivation: preserving the data topology in the latent space. Topological data analysis (TDA) approaches are either too expensive in computation or too narrowly applicable (e.g., to only 2D/3D visualization). The proposed method addresses the drawbacks of TDA by regularizing an autoencoder, a simple and elegant approach. However, despite its simplicity, the paper has not done enough theoretical/empirical exploration (see the Weakness section below).\n\nThe authors touch on the concept of variance concentration/hierarchy, which appears to be a good merit of the proposed method. However, the exposition of this concept is insufficient (also see the Weakness section below)."}, "weaknesses": {"value": "Despite a good motivation, the paper leaves more to be desired.\n\nThe theoretical foundation is weak. The proposed distance regularization looks ad hoc. It is unclear why the authors propose such a regularization among many obvious alternatives.\n\nIt might be helpful if the authors could set the stage with more background information on topology/manifold and topological data analysis, as justification for the proposed method.\n\nA weakness of the method is that it leaves open what a good reference embedding is. The proposal is more like a framework, which can be applied to any reference embedding, but it is unclear what good references are.\n\nThe authors should put more exposition on variance concentration: what it is, why it is a good thing, why the method leads to it, etc.\n\nThe experiment results are not compelling. For example, in Figure 4, the interpolation quality for 3DShapes is poor. The smallest balls for methods (b), (c), and (f) and the largest balls for methods (a), (d), and (e) have significant distortions.\n\nThe fonts in the figures are too small to be legible.\n\nFor CIFAR10/MNIST/F-MNIST datasets, the authors use overly simple neural networks. They should at least try convolution layers for images. It is suspected that the neural networks, not just the training loss, are key to the quality of Figures 4 and 5."}, "questions": {"value": "See the Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7p3H7h2vS4", "forum": "Iq6o8A8NVA", "replyto": "Iq6o8A8NVA", "signatures": ["ICLR.cc/2026/Conference/Submission14378/Reviewer_ECfY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14378/Reviewer_ECfY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761854983006, "cdate": 1761854983006, "tmdate": 1762924794606, "mdate": 1762924794606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Manifold-Matching Autoencoders (MMAE), which learn latent representations by matching pairwise distances to a reference embedding (typically produced by a manifold learning method). The quality of dimensionality reduction is evaluated on high-dimensional datasets and claimed to outperform methods such as Topological Autoencoders (TopoAE) and Geometry-Regularized Autoencoders (GAE). Combined with a VAE objective, the model is claimed to support semantic interpolation even in high-dimensional latent spaces. The paper also reports a concentration phenomenon in latent-space variance and posits this as a driver of improved interpolation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The approach is scale-invariant and does not require the latent dimensionality to equal the intrinsic manifold dimension, potentially simplifying autoencoder design and enabling high-dimensional latents.\n- It is intriguing that geometry preservation appears to improve semantic interpolation quality, although the underlying rationale remains unclear."}, "weaknesses": {"value": "- **Limited novelty.** Similar ideas that preserve the geometry of embeddings/graphs already exist. Direct comparisons are missing, e.g.\n  - SPAE: Singh & Nag (2021), Structure-preserving deep autoencoder-based dimensionality reduction for data visualization, IEEE SNPD.\n  - GGAE: Lim, Kim, Lee, Jang & Park (2024), Graph Geometry-Preserving Autoencoders, ICML.\n\n  The paper needs to position MMAE against these and related geometry/graph-preserving autoencoders with clear conceptual and empirical distinctions. To my knowledge, the proposed regularizer is conceptually very similar to SPAE when formulated to preserve reference distances.\n- **Clarity and reproducibility.** Key experimental details are unspecified or under-specified, including:\n  -\tHow reference distance matrices are constructed (PCA/UMAP settings).\n  -\tTrain/validation/test splits.\n  -\tStatistical reporting (confidence intervals).\n- **Empirical significance.** From Figures 5–6, superiority over baselines is not compelling; differences appear small and sometimes ambiguous. Stronger baselines (e.g., SPAE, GGAE) and ablations (e.g., latent-dim sensitivity) are needed.\n- **Unconvincing explanation of variance concentration.** The reported concentration in latent variance is not theoretically justified, nor is its causal link to improved semantic interpolation established.\n- **Positioning vs. graph/geometry-preserving objectives.** It remains unclear why MMAE should outperform graph- or geometry-preserving autoencoders when using the same reference distances. If the advantage arises from the VAE regularizer, the latent dimensionality, or the scale-invariance property of the loss, this should be clarified and verified through ablation studies."}, "questions": {"value": "- Please refer to the points raised in the weaknesses section.\n- Why can MMAE perform better than GAE even under identical reference distance matrices? A thorough analysis and ablation study of this point would be helpful.\n- How does MMAE compare to SPAE? What are the possible advantages and disadvantages of the obtained latent representations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wI7Yd32ztT", "forum": "Iq6o8A8NVA", "replyto": "Iq6o8A8NVA", "signatures": ["ICLR.cc/2026/Conference/Submission14378/Reviewer_qnQE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14378/Reviewer_qnQE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14378/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131964474, "cdate": 1762131964474, "tmdate": 1762924794100, "mdate": 1762924794100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}