{"id": "MTxDx1FJJX", "number": 11945, "cdate": 1758204841525, "mdate": 1759897543577, "content": {"title": "UniE2F: A Unified Framework for Event-to-Frame Reconstruction with Diffusion Model", "abstract": "Event cameras, also known as dynamic vision sensors, record high-frequency pixel intensity changes as discrete ``events''. They offer remarkable advantages, including high dynamic range, high temporal resolution, low power consumption, and absence of motion blur, all while utilizing minimal bandwidth. In this paper, we explore leveraging the generative prior of a pre-trained video diffusion model to reconstruct high-quality video frames from sparse event data. Specifically, we first establish a baseline model by directly applying event data as a condition to synthesize videos. Then, based on the physical correlation between the event stream and video frames, we further introduce the event-based inter-frame residual guidance to enhance the accuracy of video frame reconstruction. Furthermore, we extend our method to video frame interpolation and prediction in a zero-shot manner by modulating the reverse diffusion sampling process, thereby creating a unified event-to-frame reconstruction framework. Experimental results on real-world and synthetic datasets demonstrate that our method significantly outperforms previous approaches both quantitatively and qualitatively. The code will be publicly available.", "tldr": "", "keywords": ["Event-based Video Frame Reconstruction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/791ff1db2a0d01ca9c80f642f3a3b08b3cb67d6c.pdf", "supplementary_material": "/attachment/b018fc7b984238e5278de154dce1f86e40cc1ac8.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose UniE2F, a framework for event-to-frame video reconstruction using a fine-tuned Stable Video Diffusion model. While the residual guidance and zero-shot adaptation are positioned as core contributions, they are conceptually weak, insufficiently motivated for real-world scenarios, and lack rigorous empirical validation. The residual guidance builds on known ideas without demonstrating clear necessity or advantage, and the zero-shot claims are not convincingly benchmarked. Overall, the work offers limited novelty and fails to establish meaningful improvements over prior art."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces a technically sound residual guidance mechanism and a unified zero-shot adaptation strategy, both grounded in prior concepts. While synthetic results are strong and ablations are thorough, the novelty is limited and real-world relevance remains unclear. The appendix and video were informative."}, "weaknesses": {"value": "The paper's introduction and problem setup focus exclusively on reconstruction. It never motivates why the main contributions, i.e., event-based VFI or VFP are important problems to solve or what their real-world applications are.\n\nA key benefit of event cameras is their high temporal resolution, allowing reconstruction at any arbitrary time. The proposed interpolation method (confirmed in App. D) generates a fixed sequence of frames, not a single frame at an arbitrary timestamp t. This misses the primary advantage of the sensor.\n\nOne significant weakness for applicability is the prohibitive computational cost. Appendix F (Table 8) reveals the method requires 46,753 MB (~47 GB) of VRAM and 245.364 TMACs, which is orders of magnitude more than competitors (e.g., HyperE2VID at 1052 MB and 0.060 TMACs). The 48-second latency for 12 frames (Sec 5.1) makes this method completely unusable for any practical or real-time application, which is the entire point of event cameras.\n\nThe paper claims SOTA on real-world reconstruction, but this is only true for MSE/SSIM. The LPIPS score (Table 1) is significantly worse than prior work (0.674 vs 0.562). This major discrepancy, which suggests poor perceptual quality, is never discussed or explained.\n\nThe key \"zero-shot\" contribution (VFI/VFP) does not generalize well. Table 2 shows it is clearly outperformed by standard baselines (e.g., CBMNet) when they are simply retrained on the target data. This suggests the \"unification\" is more of a curiosity on synthetic data than a robust, general-purpose tool."}, "questions": {"value": "Major: \n\n- The LPIPS scores in Table 1 (Reconstruction) and Table 2 (VFI/VFP) are consistently and significantly worse than baselines on real-world data. This contradicts the excellent qualitative results in Figure 3. Can you explain this discrepancy? Is the LPIPS metric failing, or are the qualitative examples cherry-picked? This is a crucial point of confusion.\n\n- The computational costs (Table 8, ~47GB VRAM) and latency (48s) are prohibitive and render the method unusable for any practical event-camera application. The limitation section (App. G) mentions this, but I'd like to ask: Do you believe this is a fundamental limitation of using large diffusion models for this task, or do you have concrete evidence that distillation/pruning can bridge the orders-of-magnitude gap to competitors?\n\n- Your interpolation method (Sec 4.3, App. D) generates a fixed sequence of frames (e.g., 10 frames between $V_0$ and $V_{11}$). Why did you not pursue a more \"event-native\" approach, such as reconstructing a single frame at an arbitrary timestamp $t \\in (0, 11)$?\n\n- Could you expand on the motivation and real-world applications for zero-shot event-based interpolation and prediction? The paper currently justifies reconstruction but not these other tasks.\n\nMinor:\n- The introduction, preliminary, and related work sections are quite lengthy, spanning nearly four pages, yet they include redundant background and omit discussion of several key prior works. Could the authors clarify their criteria for selecting related work, particularly regarding early image reconstruction methods and event stacking approaches that are not cited?\n\nSuggestions: \nFigure 1 illustrates general diffusion model concepts but does not appear to convey any paper-specific insights.\nThere are more important tables from the appendix that can be moved to the main paper if such sections become shorter."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sldjn1tI60", "forum": "MTxDx1FJJX", "replyto": "MTxDx1FJJX", "signatures": ["ICLR.cc/2026/Conference/Submission11945/Reviewer_JWWk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11945/Reviewer_JWWk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761096568404, "cdate": 1761096568404, "tmdate": 1762922950165, "mdate": 1762922950165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified framework UniE2F for event-to-frame reconstruction. By fusing the generative prior of pre-trained video diffusion models with the physical characteristics of event data, it enhances the quality of reconstructing high-fidelity video frames from sparse events, and its effectiveness has been verified through experiments. The main contributions and ideas of the paper can be summarized as follows:\n\n1. Unified Task Framework\n(1) Event-driven Frame Reconstruction: Based on the pre-trained Stable Video Diffusion (SVD) model, event data is encoded into 3-channel tensors as conditional inputs. The model learns the mapping relationship between events and video frames through fine-tuning, establishing fundamental reconstruction capabilities.\n(2) Inter-frame Residual Guidance Mechanism: Leveraging the physical correlation between events and inter-frame brightness changes, a ResNet-based inter-frame residual prediction module is introduced. The latent variables of the diffusion model are optimized via gradient descent to improve the temporal consistency and accuracy of reconstructed frames.\n(3) Zero-shot Interpolation and Prediction: By modulating the score function of the reverse diffusion process, the prior knowledge from the reconstruction task is transferred to video frame interpolation and prediction tasks without additional training, enabling unified handling of \"reconstruction-interpolation-prediction\".\n\n2. Core Technical Innovations An event-based inter-frame residual guidance strategy is proposed. Theoretical proof demonstrates that its gradient aligns with the tangent space of the data manifold learned by the diffusion model, ensuring the optimization process does not compromise generation quality. A score function modulation method is designed, which uses the latent variable deviation of reference frames to correct the sampling process, enhancing the temporal consistency and visual fidelity of interpolation/prediction tasks.\n\n3. Data Construction and Experimental Validation Training data is synthesized from real-world videos, and tests are conducted on both real-world and synthetic datasets. Extensive experiments and ablation studies verify that UniE2F outperforms most existing state-of-the-art (SOTA) models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a systematic three-stage event-to-frame reconstruction framework: Event-Based Video Frame Reconstruction → Inter-Frame Residual Prediction Training → Inter-Frame Temporal Residual Guidance. Ablation studies validate the effectiveness of each component. Innovatively integrating the generative prior of the pre-trained Stable Video Diffusion (SVD) model with the physical characteristics of event data, the method achieves unified handling of \"video frame reconstruction-interpolation-prediction\" through three core modules: event-conditioned fine-tuning, inter-frame residual guidance, and score function modulation. It breaks through the limitation of traditional methods confined to single tasks and can adapt to interpolation and prediction tasks in a zero-shot manner without additional training.\n\n2. An event-based inter-frame residual guidance mechanism is designed. It predicts inter-frame residuals via ResNet and optimizes the latent variables of the diffusion model combined with gradient descent. Meanwhile, it is theoretically proven that this gradient aligns with the tangent space of the data manifold, ensuring the optimization does not compromise generation quality and the reconstruction error is bounded. This effectively enhances the temporal consistency and detail accuracy of the reconstructed frames.\n\n3. Relevant experiments verify the effectiveness of the proposed modules. Compared with other methods, UniE2F achieves state-of-the-art (SOTA) performance.\n\n4. The paper is logically structured and easy to understand."}, "weaknesses": {"value": "1. Testing is only conducted on sequences extracted from TrackingNet and HS-ERGB, without validation on other datasets, making it impossible to demonstrate the true effectiveness and generalization of the method.\n\n2. Specific details of the used Stable Video Diffusion (SVD) are not provided, such as the pre-trained model employed, parameter count, and other relevant specifications.\n\n3. The originality is insufficient. From a methodological perspective, introducing residual guidance optimization is one of the core innovations of the paper, but the ablation experiments show that its improvement on performance is not significant.\n\n4. Figure 2 (the method framework) could be more detailed. For example, relevant components of the training phase should be added.\n\n5. Although the method achieves state-of-the-art (SOTA) performance on the synthetic test set, it exhibits suboptimal performance in most cases on real-world datasets.\n\n6. Compared with suboptimal methods, while this method achieves certain performance improvements, its computational cost and memory footprint are several times or even dozens of times higher than those of traditional methods."}, "questions": {"value": "1. The paper only verifies performance on the TrackingNet and HS-ERGB datasets, without involving datasets for extreme scenarios such as low light and fast motion. How robust is the method in such complex scenarios? Will performance degrade due to a sudden increase in event sparsity or noise interference?\n\n2. Regarding the used SVD model, the network structure details and parameter count are not specified. Could you provide supplementary explanations? Have you tried SVD models with different parameter counts?\n\n3. Could the method be trained on real-world datasets and verified for effectiveness on real-world datasets?\n\n4. Is the way of encoding event data into 3-channel tensors (sum of all events, sum of positive events only, sum of negative events only) the optimal choice? Have you tried temporal dimension encoding (e.g., event occurrence frequency)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D6mwVij5AE", "forum": "MTxDx1FJJX", "replyto": "MTxDx1FJJX", "signatures": ["ICLR.cc/2026/Conference/Submission11945/Reviewer_Bq1Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11945/Reviewer_Bq1Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761519805360, "cdate": 1761519805360, "tmdate": 1762922949630, "mdate": 1762922949630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an event-to-frame reconstruction method based on a stable video diffusion model. The approach leverages the physical correlation between event streams and video frames to guide and enhance the quality of event-based reconstruction. Furthermore, the authors extend this method to video interpolation and prediction tasks, with experimental results demonstrating its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method achieves strong performance in event-based reconstruction, interpolation, and frame prediction tasks, outperforming previous approaches.\n\n2. The paper presents comprehensive results, including quantitative comparisons, images, videos, and animations."}, "weaknesses": {"value": "1. Unfair comparison. The proposed UniE2F is fine-tuned from the Stable Video Diffusion (SVD) model, which itself has been pre-trained on large-scale datasets covering tasks such as video generation and reconstruction. In contrast, the comparison methods were trained only on synthetic datasets, making the comparison potentially unfair. Since UniE2F naturally benefits from SVD’s strong pretrained prior, the authors should provide results without pretrained weights to enable a fairer evaluation.\n\n2. Excessive computational cost. As shown in Table 8, UniE2F requires orders of magnitude (up to 1000×) more computation than other methods, while offering limited performance improvement. This greatly undermines the advantages of event cameras, such as low power consumption and high temporal resolution. The authors should consider introducing more efficient strategies—for example, knowledge distillation or transfer learning—to significantly reduce computational cost while maintaining reconstruction quality.\n\n3. Lack of real event camera data. All experiments were conducted on synthetic datasets, which exhibit a clear domain gap from real-world event data. The authors should evaluate their method on real datasets such as HQF, IJRR, and MVSEC to demonstrate robustness and generalization.\n\n4. Lack of diversity in event representations. In event-based reconstruction, researchers commonly use voxel grids as event representations. The authors adopt a different representation but do not explain the rationale. Moreover, how do various representations—such as EST, ECM, and Voxel Grid—affect the reconstruction results? The authors should discuss this.\n\n5. The authors use a ResNet to predict inter-frame residuals, but given that event cameras capture data with high temporal resolution, they can theoretically provide event information for any time interval. Why predict intermediate residuals instead of directly aggregating events between frames? The authors should clarify this design choice.\n\n6. In line 180, the authors mention using a three-channel event representation. What are the advantages and underlying rationale for this choice? The justification should be provided.\n\n7. Since the pretrained SVD model can already perform video frame interpolation (VFI) and video frame prediction (VFP) tasks using image inputs alone, it appears that the event data serves only as an auxiliary input. The authors should report results showing how UniE2F performs with only image input or only event input in VFI and VFP tasks, to better illustrate the true contribution of event information."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jTJLRGJZa9", "forum": "MTxDx1FJJX", "replyto": "MTxDx1FJJX", "signatures": ["ICLR.cc/2026/Conference/Submission11945/Reviewer_V449"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11945/Reviewer_V449"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761571135206, "cdate": 1761571135206, "tmdate": 1762922949260, "mdate": 1762922949260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniE2F, a unified diffusion-based framework for reconstructing, interpolating, and predicting video frames from event camera data. UniE2F leverages a pre-trained video diffusion model and introducing an event-based residual guidance mechanism. The framework treats event-to-frame reconstruction as a conditional generation process and achieves state-of-the-art performance on both synthetic and real datasets. The approach demonstrates strong generalization across multiple event-driven vision tasks while maintaining a coherent generative formulation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel residual-guided conditioning: The proposed event-based inter-frame residual guidance is an elegant and physically interpretable mechanism that connects asynchronous event streams with frame-level brightness changes. By explicitly modeling the residuals between consecutive latent frames, the diffusion process receives direct gradient cues from event dynamics, leading to sharper textures and temporally consistent reconstructions.\n\n2. Unified generative formulation: The same residual-guided diffusion framework can handle event-to-frame reconstruction, interpolation, and prediction without retraining or architectural modification. This unification is conceptually clean and practically useful.\n\n3. Zero-shot frame interpolation and prediction: The framework is extended to perform zero-shot video frame interpolation and future frame prediction. By modulating the reverse diffusion sampling process, the same architecture can handle not only reconstruction but also interpolation and prediction tasks, demonstrating flexibility and strong generalization without additional training."}, "weaknesses": {"value": "1. Limited methodological clarity: The paper proposes a residual-guided diffusion mechanism but provides insufficient details on its implementation. In particular, the normalization of the residual signal, its integration within denoising steps, and its weighting against the diffusion prior remain unclear, limiting reproducibility and interpretability.\n\n2. Computational inefficiency: The framework relies on a pre-trained video diffusion backbone, which is typically expensive in both computation and memory. The paper lacks a detailed analysis of inference speed, GPU memory usage, and scalability to long sequences or real-time deployment.\n\n3. Limited comparison with recent baselines: The paper does not include a comparison with RE-VDM or other recent state-of-the-art event-to-video diffusion methods. Including these baselines would provide a clearer picture of the method's relative performance and strengthen the empirical evaluation."}, "questions": {"value": "1. Loss weighting: If the residual contributes to the loss, what is the weighting factor s and how sensitive is the performance to different s values? Table 3 only shows comparisons for s = 0 and s = 0.1. Have the authors conducted a more thorough ablation study to evaluate the effect of this weighting?\n\n2. Choice of linear guidance schedule in Table 3:   In Table 3, the paper compares different guidance strategies, including linear increasing and decreasing residual guidance. Could the authors clarify the motivation for adopting a linear schedule?  Was this choice empirically found to be optimal, or is it mainly for simplicity?  Have other non-linear schedules (e.g., constant, exponential, cosine) been tried, and how do they affect reconstruction quality or temporal consistency?\n\n3. Generalization and robustness: How does the method perform under extreme lighting, very sparse event streams, or noisy events? Are there failure cases, and what types of motions or event patterns cause degradation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vPw8zIbLfT", "forum": "MTxDx1FJJX", "replyto": "MTxDx1FJJX", "signatures": ["ICLR.cc/2026/Conference/Submission11945/Reviewer_FKni"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11945/Reviewer_FKni"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756307216, "cdate": 1761756307216, "tmdate": 1762922948809, "mdate": 1762922948809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}