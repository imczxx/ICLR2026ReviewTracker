{"id": "XD43lfRCg6", "number": 2089, "cdate": 1756986921069, "mdate": 1759898170272, "content": {"title": "Preserving Forgery Artifacts: AI-Generated Video Detection at Native Scale", "abstract": "The rapid advancement of video generation models has enabled the creation of highly realistic synthetic media, raising significant societal concerns regarding the spread of misinformation. However, current detection methods suffer from critical limitations. They often rely on preprocessing operations like fixed-resolution resizing and cropping, which not only discard subtle, high-frequency forgery artifacts but can also cause distortion and significant information loss. Furthermore, these methods are frequently trained and evaluated on outdated datasets that fail to capture the sophistication of modern generative models. To address these challenges, we introduce two key contributions: a new large-scale dataset and benchmark, as well as a novel detection framework. We present a comprehensive dataset of over 140K videos from 16 state-of-the-art open-source and leading commercial generators. In addition, we curate Magic Videos Testset, featuring ultra-realistic videos produced through a meticulous generation and filtering pipeline. In addition, we propose a novel detection framework built on the Qwen2.5-VL Vision Transformer, which processes videos at their native spatial resolution and temporal duration. This native-scale approach preserves high-frequency details and spatiotemporal inconsistencies that are often lost during conventional preprocessing. Extensive experiments show that our method achieves state-of-the-art performance across multiple benchmarks. Our work underscores the importance of native-scale processing and establishes a robust new baseline for AI-generated video detection.", "tldr": "Native Resolution AI Video Detection", "keywords": ["AI-Generated Video Detection", "Video Generation", "AIGC Detection"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c09c2ce83e952216716870a95912f0dedd0c5567.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores AI-generated video detection. It curates a dataset consisting of videos created from 15 or 18 generative models, as well as a novel framework that processes videos at their native spatial resolution and temporal duration, avoiding destructive preprocessing like resizing and cropping that mess up subtle forgery artifacts. The framework is built on the Qwen2.5-VL Vision Transformer, which uses 3D patchification directly on the raw video tensor, preserving the high-frequency details essential for detection. The experiment result shows improvement compared to baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. AI-generated video detection is an important topic\n2. Providing a curated dataset of Gen-AI videos is a good contribution\n3. The proposed detection framework shows improvement compared to baseline methods."}, "weaknesses": {"value": "1. The proposed detection methodology is simply a combination of existing approaches and does not present a novel contribution\n\nDetail: The core detection framework is a direct application of the existing Qwen2.5-VL Vision Transformer using its native 3D patchification strategy, which the authors note is adopted from prior work (Bai et al., 2025). While applying this to the forgery detection task is a valid contribution, the paper presents it as a 'novel detection framework'  when the architectural novelty itself is minimal. The primary novelty seems to be the dataset and the hypothesis that native-scale processing is superior, rather than a new detection architecture.\n\n2. The presence of artifacts is vaguely assumed but not explicitly defined or proven to exist.\n\nDetail: The authors do not define, visualize, or analyze what these artifacts are. A significant weakness is the lack of any model explainability (e.g., activation maps or gradient-based analysis) to prove that the native-scale model is actually focusing on these \"pixel-level artifacts\", while the 224p-resized model is not. The entire premise, while intuitive, is treated as an assumption rather than a proven scientific finding.\n\n3. The use of downgraded input by cropping or resizing is argued to be the main fault of previous works. However, it is not explained or experimented with in connection with the actual baseline used.\n\nDetail: Since the proposed method also performs poorly when given low resolution input, it is not surprising that baseline methods perform poorly when given low resolution input. This makes it impossible to know if the proposed method is superior due to its architecture or simply because it's the only one allowed to see the high-resolution data. A fair comparison would require adapting baselines to also accept high-resolution inputs or, at a minimum, comparing all models at the same fixed resolutions (e.g., 224p, 480p, 720p).\n\nOther issues:\n- The proposed detection method is shown to have a much higher cost in training with memory and GPU hours.\n- Discrepancy in the number of generative models used (abstract: 16; Introduction: 15 and 18; appendix: 15)"}, "questions": {"value": "1. Can baseline detection methods be retrofitted to use the high-resolution inputs for training and inference? \n2. Following the above question, if given high-resolution input in both training and inference, how does the baseline method compare with the proposed method?\n\n3. Can the artifacts be identified or visualized?\n4. Following the above question, can the artifacts be removed or interfered with so that the detection method fails?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ff31uFjGXf", "forum": "XD43lfRCg6", "replyto": "XD43lfRCg6", "signatures": ["ICLR.cc/2026/Conference/Submission2089/Reviewer_9HXd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2089/Reviewer_9HXd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761584814740, "cdate": 1761584814740, "tmdate": 1762916015472, "mdate": 1762916015472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an AI-generated video detector that (1) builds a  dataset and (2) trains a detector on native spatial resolution and temporal duration using Qwen2.5-VL ViT with 3D patchification, so that high-frequency, position-dependent artifacts aren’t destroyed. On three families of benchmarks (GenVideo, DVF, and their Magic Videos), it reports consistently higher performance than prior image-detectors, deepfake-detectors, and video backbones. The core narrative is: current detectors are undertrained on modern, high-quality video generators, and downsampling design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Timely problem & data refresh. Most detectors are indeed lagging behind 2024–2025 video generators; this paper explicitly targets that gap and gathers content from new models, including commercial/API ones. That’s rare and useful.\n2. Clear empirical story about resolution. The experiment results supports the authors' claim well and the motivation is clear.\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The paper addresses two real but relatively well-recognized issues — resolution-destructive preprocessing and stale generator distributions. The proposed solution mainly combines (i) a recent, high-capacity video/VL backbone and (ii) a refreshed multi-generator dataset. The detector architecture itself is largely standard, without forgery-specific inductive biases. Data curation process is also well defined in previous fake video detection work. Thus the contribution is more of an engineering consolidation.\n2. Dataset release / licensing / reproducibility is unclear. Several of the listed video sources (Kuaishou, Luma, MovieGen) are API/commercial. The paper doesn’t yet make it clear what exactly will be released, how prompts will be shared, and how others can reproduce “Magic Videos”. For ICLR this is important.\n3. No fairness breakdown. Real videos come from different sources than synthetic ones; there can be source, watermark, or codec biases. The paper doesn’t fully rule that out. It's important to demonstrate the performance on unseen real videos from very different source such as KITTI, etc. to demonstrate the generalizability of the proposed model.\n4. Limited analysis on real-world perturbations. The core claim is about preprocessing destroying artifacts, but actual attackers / platforms will introduce their own compress-and-resize chains. It would be good to see: scale jitter & heavy H.264/HEVC compression and see whether “native-scale” still wins.\n5. Incomplete discussion of most recent related work such as [1, 2]. \n\nReference\n1. Distinguish Any Fake Videos: Unleashing the Power of Large-scale Data and Motion Features. 2024\n2. How Far are AI-generated Videos from Simulating the 3D Visual World: A Learned 3D Evaluation Approach. 2025"}, "questions": {"value": "1. How sensitive is the detector to platform compression? Your main argument is “don’t downsample to 224.” But if a platform already did that, can your model still outperform the older 224-trained models? A controlled experiment with platform-style compression would be convincing.\n2. How do you prevent source leakage? Since real videos come from specific real video datasets and generated ones are from VBench/MovieGen/etc., detectors might be learning source signatures. Do you have a cross-source test where the real videos share encoding with the synthetic ones?\n3. Happy to see more discussion with latest related work in the related area. This would help make the contribution clear."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TIVbjYsABV", "forum": "XD43lfRCg6", "replyto": "XD43lfRCg6", "signatures": ["ICLR.cc/2026/Conference/Submission2089/Reviewer_KYH3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2089/Reviewer_KYH3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881414243, "cdate": 1761881414243, "tmdate": 1762916015070, "mdate": 1762916015070, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the limitations of AI-generated video detection, highlighting that fixed-resolution preprocessing removes high-frequency forgery artifacts and that outdated datasets struggle to represent modern models. The authors build a dataset of about 140K videos from 18 generative models and real sources such as Kinetics and MSVD, and design a realism-oriented test set called Magic Videos. They propose a detection framework based on Qwen2.5-VL ViT, which processes videos at their native spatial and temporal scales using 3D patch tokenization to preserve forgery details. Experiments are conducted on GenVideo, DVF, and Magic Videos."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Builds a large-scale dataset with over 140,000 videos from 18 generative models, covering mainstream AI video generation technologies and offering a research resource.\n\nConducts experiments on multiple benchmarks, including the self-built Magic Videos test set and public datasets, ensuring comparability and reliability.\n\nEmploys a dynamic resolution processing framework that preserves detection performance and demonstrates generalization ability across datasets.\n\nDesigns ablation experiments that are systematic and support the main conclusions.\n\nIntegrates Flash Attention and LoRA optimizations, reflecting concern for efficiency and practical deployment."}, "weaknesses": {"value": "Lacks analysis of the model’s decision-making mechanism, failing to verify whether the model truly learns forgery-related cues rather than biased features in the data.\n\nProvides insufficient discussion of computational efficiency, with no concrete results on inference speed or resource requirements for real-world deployment.\n\nThe training data source is limited, relying mainly on the VBench dataset, which may introduce hidden biases.\n\nThe failure case analysis is inadequate, lacking exploration of the scenarios and causes where the model fails."}, "questions": {"value": "Regarding computational efficiency, the paper mentions optimization techniques such as Flash Attention, but lacks specific key metrics such as inference speed and memory usage. Could you provide detailed efficiency data under typical hardware configurations to assess the feasibility of this method in real-world scenarios?\n\nFor Magic Videos, how is the \"indistinguishable to humans\" claim verified?\n\nExperimental results show significant performance differences across different generators (from 72.26% to 85.12% in Table 2). Have you analyzed the specific reasons for these differences? Are there any specific types of generated content or technical approaches that are blind spots for the current method?\n\nCould you provide a more in-depth interpretability analysis, such as attention visualization or feature analysis, to demonstrate that the model truly learns meaningful forgery traces, rather than relying on other superficial features in the data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gqkUK22Pxe", "forum": "XD43lfRCg6", "replyto": "XD43lfRCg6", "signatures": ["ICLR.cc/2026/Conference/Submission2089/Reviewer_X5mz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2089/Reviewer_X5mz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891373084, "cdate": 1761891373084, "tmdate": 1762916014833, "mdate": 1762916014833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the pressing need for effective detection methods for AI-generated videos, which is a highly valuable area of research. The authors provide an in-depth exploration of the current state of research, highlighting its shortcomings, including the development of video generation models, detection of generated images, and existing methods for detecting generated videos. Two major challenges are identified in the detection of AI-generated videos: 1) The fixed-resolution preprocessing through operations like cropping and downsampling leads to information loss and coarse-grained detection; 2) Current detection methods are typically trained on outdated synthetic data sources, which are insufficient for handling videos produced by the latest high-quality generative frameworks.\nIn response to these issues, the authors construct a high-quality and diverse dataset sourced from state-of-the-art generative models and propose a novel detection framework built upon Qwen2.5-VL, which processes videos in their native spatial resolution and temporal length. This approach preserves crucial forgery artifacts often lost in conventional preprocessing steps, such as resizing or cropping.\nThrough extensive experiments, the authors demonstrate the effectiveness of their method and reasonably discuss the remaining challenges. Their work significantly advances the field, providing a robust foundation for future AI-generated video detection efforts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This paper addresses the significant limitations of existing methods for detecting AI-generated videos, highlighting the urgent need for effective detection strategies in light of the rapid development of video generation technologies.\n2. The authors construct a novel, large-scale dataset by utilizing cutting-edge video generation tools, which ensures that the dataset is diverse and high-quality, effectively supporting the proposed detection framework."}, "weaknesses": {"value": "1.The section on \"3D Video Patchifying at Native Scale\" lacks sufficient novelty. Although the paper claims that the model is trained at native resolution, this approach was already introduced in Qwen2.5-VL [1]. The focus of the method seems to be more on engineering optimization rather than presenting a fundamentally new contribution to the field. Additional exploration of novel techniques or improvements beyond existing methods would strengthen this section.\n2.The paper lacks a more detailed analysis of the proposed method, particularly regarding how the model differentiates between real and generated content. While the authors claim the use of native resolution processing, there is little discussion on which specific parts of the video (e.g., temporal inconsistencies, high-frequency artifacts, or motion patterns) the model focuses on to distinguish between real and synthetic videos. A deeper exploration of the key features the model uses to make this distinction would help clarify the strengths of the approach and provide insights into its decision-making process.\n3.While the use of dynamic resolution processing significantly enhances performance, it introduces additional computational overhead. This increased complexity may limit the model’s feasibility for real-world deployment, particularly on resource-constrained devices or in scenarios requiring real-time detection.\n4.The proposed dataset structure is highly unreasonable; each class in the test set has very few samples, making the statistical results highly likely to be biased.\n5. The experiments were insufficient and were not tested on the latest dataset GenVidBench.\n[1]Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025."}, "questions": {"value": "see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pRrqlpnfsX", "forum": "XD43lfRCg6", "replyto": "XD43lfRCg6", "signatures": ["ICLR.cc/2026/Conference/Submission2089/Reviewer_A394"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2089/Reviewer_A394"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903664385, "cdate": 1761903664385, "tmdate": 1762916014453, "mdate": 1762916014453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}