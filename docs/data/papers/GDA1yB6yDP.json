{"id": "GDA1yB6yDP", "number": 25245, "cdate": 1758365682163, "mdate": 1759896728037, "content": {"title": "Not Search, But Scan: Benchmarking MLLMs on Scan-Oriented Academic Paper Reasoning", "abstract": "With the rapid progress of multimodal large language models (MLLMs), AI already performs well at literature retrieval and certain reasoning tasks, serving as a capable assistant to human researchers, yet it remains far from autonomous research. The fundamental reason is that current work on scholarly paper reasoning is largely confined to a search-oriented paradigm centered on pre-specified targets, with reasoning grounded in relevance retrieval, which struggles to support researcher-style full-document understanding, reasoning, and verification. To bridge this gap, we propose ScholScan, a new benchmark for scholarly paper reasoning. ScholScan introduces a scan-oriented task setting that asks models to read and cross-check entire papers like human researchers, scanning the document to identify consistency issues. The benchmark comprises 1,800 carefully annotated questions drawn from 9 error families across 13 natural-science domains and 715 papers, and provides detailed annotations for evidence localization and reasoning traces, together with a unified evaluation protocol. We assessed 15 models across 24 input configurations and conduct a fine-grained analysis of MLLM capabilities across error families. Across the board, retrieval-augmented generation (RAG) methods yield no significant improvements, revealing systematic deficiencies of current MLLMs on scan-oriented tasks and underscoring the challenge posed by ScholScan. We expect ScholScan to be the leading and representative work of the scan-oriented task paradigm.", "tldr": "We present ScholScan, a scan-oriented benchmark for full-paper scholarly reasoning that requires models to build a paper-level evidence view; spanning 1,800 questions from 715 papers, which exposes MLLM gaps and shows RAG ineffective.", "keywords": ["Multimodal Large Language Models; Academic Paper Reasoning; Scan-Oriented Reasoning"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/49cc34d84563f82a0411d2ea1c053215d0925474.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ScholScan, a novel scan-oriented reading benchmark designed to address the limitation of Multimodal Large Language Models (MLLMs) in overlooking holistic and cross-document understanding. The benchmark's questions are initially generated through a sampling and generation process, then manually screened to form the final set. Corresponding metrics are employed to evaluate model performance, and multi-dimensional experiments provide valuable insights into future directions for model development."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "ScholScan effectively captures a key aspect of human reading: the ability to process full documents or read across multiple articles. The data construction and filtering process is methodologically sound. The evaluation metrics are multi-faceted, reasonable, and well-aligned with human judgments. Consequently, the experimental results offer valuable insights that can guide the future development of Multi-modal Large Language Models (MLLMs)."}, "weaknesses": {"value": "The development of ScholScan heavily relies on the use of a Large Language Model (LLM), specifically Gemini-2.5 Pro. Although manual screening and validation were subsequently incorporated to ensure quality, the questions generated by the LLM may inherently differ from those posed by humans."}, "questions": {"value": "- On line 88, there appears to be a typo. \"Process-Aware Rvaluation Framework\" should be corrected to \"Process-Aware Evaluation Framework\".\n- Some entries in the bibliography seem overly long, which takes up considerable page space. Please consider condensing them.\n- In the section on evaluation metrics, several hyper-parameters are introduced (e.g., 0.8, 0.9, 0.6). The rationale for selecting these specific values is not entirely clear. To strengthen the paper, I would recommend including a justification for these choices. For instance, a sensitivity analysis or an ablation study could demonstrate the robustness and appropriateness of the selected parameters."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fuGV03x9Nb", "forum": "GDA1yB6yDP", "replyto": "GDA1yB6yDP", "signatures": ["ICLR.cc/2026/Conference/Submission25245/Reviewer_co8M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25245/Reviewer_co8M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790487594, "cdate": 1761790487594, "tmdate": 1762943379074, "mdate": 1762943379074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ScholScan, a benchmark designed to evaluate multimodal large language models (MLLMs) on scan-oriented scholarly reasoning: instead of answering prespecified, “search-oriented” questions, models receive one or more full academic papers and must proactively scan the entire document to uncover consistency issues. Key aspects:\n- Task formulation: “target-absent” audit-style queries (e.g., “Assess the Methods section for Measurement & Operationalization issues”) requiring evidence construction across the whole paper.\n- Dataset: 1,800 questions across 715 papers from 13 natural-science domains, covering 9 error families (Research Question & Definitions; Design & Identifiability; Sampling & Generalizability; Measurement & Operationalization; Data Handling & Preprocessing; Computation & Formulae; Inference & Conclusions; Referential & Citation Alignment; Language & Expression). Instances come from two sources: (i) LLM-edited accepted papers (synthetic error injection) and (ii) sampling verifiable issues from ICLR public reviews of rejected papers. Cross-paper citation inconsistencies are generated.\n- Process-aware evaluation: A unified evaluation pipeline parses open-ended model outputs into existence, evidence localization (Dice-like score with over-reporting penalty), reasoning-chain prefix-match, and a penalty for unrelated/hallucinated errors.\n- Experiments: 15 models with 24 input settings (image vs OCR text) and 8 RAG frameworks. Main findings: (a) overall performance is weak; (b) reasoning-optimized models perform better; (c) text (OCR) inputs often outperform image inputs, but images can matter for formula/table-heavy tasks; (d) standard RAG brings little benefit; an RL-based visual-centric RAG (VRAG-RL) shows the most promise on image inputs.\n- Analyses: Correlations across error types suggest latent “skill” dimensions; evidence-and-reasoning workload correlates with failures; failure mode breakdown (omission vs hallucination); human–machine agreement for the judge-extractor shows strong Spearman correlations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A New \"Scan-Oriented\" Task Paradigm: The paper formalizes a novel and challenging task setting that moves beyond simple question-answering to require exhaustive, document-level understanding and consistency checking without pre-specified targets.\n- The ScholScan Benchmark: A comprehensive and large-scale benchmark consisting of 1,800 questions derived from 715 scientific papers across 13 natural science domains. The benchmark covers 9 distinct families of scientific errors, from issues in experimental design to referential inconsistencies.\n- High-Quality Data and Annotation: The dataset was constructed through a rigorous process involving both LLM-based error generation/sampling and meticulous verification by 10 domain experts, ensuring the errors are subtle, realistic, and verifiable. The annotations include not just the final answer but also evidence locations and reasoning traces.\n- A Process-Aware Evaluation Framework: The authors propose a detailed evaluation protocol and a composite scoring metric that assesses a model's performance on multiple dimensions: correctly identifying an error's existence, localizing all relevant evidence, completing the correct reasoning chain, and avoiding the hallucination of unrelated errors.\n- Extensive Empirical Analysis: The paper presents a thorough evaluation of 15 state-of-the-art MLLMs (across 24 configurations) and 8 Retrieval-Augmented Generation (RAG) methods. The results show that all current models, including proprietary ones like GPT-5 and Gemini 2.5 Pro, perform poorly on this task, and that standard RAG techniques fail to provide any significant improvement, highlighting the unique challenges posed by the scan-oriented paradigm."}, "weaknesses": {"value": "- Dependence on LLM-based Evaluation: The evaluation protocol relies on GPT-4.1 to parse model responses and extract structured information (evidence, reasoning steps). While the authors validate this with a human-machine consistency check (Appendix F), which shows strong correlation, this dependency introduces a potential point of failure or bias. The evaluation of one model is contingent on the capabilities of another, which is a common but not ideal practice. The evaluation could be influenced by the specific style or formatting of the model being evaluated.\n- Limited Scope of \"Generated\" Errors: The data generation process relies on an LLM (Gemini 2.5 Pro) to inject errors into high-quality papers. While curated by experts, these synthetic errors might have subtle, systematic patterns that differ from genuine human errors. The benchmark could be further strengthened by including more examples of errors \"sampled\" from real-world peer reviews or errata, although the authors are transparent about their methodology.\n- Ambiguity in the \"Reasoning Process Score\": The reasoning score (`S_reasoning`) is based on a prefix match of the reasoning chain. This assumes a single, linear \"gold\" reasoning path. However, a valid logical argument could potentially be structured in different ways or use slightly different intermediate steps. This metric might unduly penalize models that arrive at the correct conclusion through a logically sound but differently structured argument."}, "questions": {"value": "- Beyond RAG - Other Potential Architectures: The RAG analysis is excellent. However, have the authors considered or could they comment on other architectural approaches that might be better suited for \"scan\" tasks? For example, models with explicit memory B, graph-based representations of the document's claims and evidence, or iterative self-refinement loops where the model repeatedly scans the paper to build and verify a \"consistency map.\" A brief discussion in the conclusion could enrich the paper.\n- LLM-as-Judge Robustness: Regarding the GPT-4.1-based evaluation, did the authors observe any failure modes? For instance, did the evaluator struggle with particularly verbose or poorly structured responses from certain open-source models? Providing some qualitative examples of the evaluator's performance could further strengthen confidence in this method.\n- Feasibility of Human Evaluation: The paper mentions high agreement between the GPT-4.1 pipeline and expert annotations on a subset. Given the complexity of the task, what would be the estimated cost or time for a full human evaluation of a single model's output on the benchmark? A brief comment on this could help contextualize the necessity of the automated pipeline.\n- Suggestion on Score Calibration: The final scores are quite low across the board (the best model scores ~30). This is a strong testament to the benchmark's difficulty. However, it can make it hard to differentiate between poorly performing models. Have the authors considered if a simpler, \"binary\" success metric (e.g., did the model identify the correct error type and at least one piece of correct evidence?) could complement the main score and provide another view on performance, especially for the lower-performing models? This is not a required change, but a suggestion for a potential additional analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4zEGtauGFR", "forum": "GDA1yB6yDP", "replyto": "GDA1yB6yDP", "signatures": ["ICLR.cc/2026/Conference/Submission25245/Reviewer_a2Tc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25245/Reviewer_a2Tc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858878692, "cdate": 1761858878692, "tmdate": 1762943378842, "mdate": 1762943378842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ScholScan, a benchmark for scholarly reasoning that contrasts with existing search-oriented evaluations. ScholScan uses a scan-oriented question design: models are required to read entire papers and derive all necessary concepts and inferences solely from the provided documents, rather than answering pre-specified queries grounded in retrieval. The benchmark covers 13 scientific domains and 9 error types, enabling a comprehensive assessment of error detection. This paper evaluates multiple models and further highlights the paradigm difference by testing the RAG method, finding that simple retrieval offers no improvement on scan-oriented tasks. These results suggest that current MLLMs still struggle with addressing such scan-oriented tasks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Introduction of a new paradigm:** This paper proposes a novel scan-oriented paradigm, which differs fundamentally from traditional search-oriented settings with pre-specified queries. In this setup, models must autonomously and comprehensively understand the entire document and identify the error by themselves. \n\n**Comprehensive evaluation and analysis:** This paper conducts large-scale evaluations covering both instruction-following and thinking-style models, providing detailed statistical analyses of errors. It further reveals that model performance deteriorates as the required number of documents and reasoning steps increases, offering valuable diagnostic insights. \n\n**Well-structured evaluation metric:** The proposed metric jointly measures error detection, evidence accuracy, and reasoning quality. It rewards correct identification of target errors, penalizes noisy or irrelevant evidence, and assesses reasoning completeness through prefix matching."}, "weaknesses": {"value": "**Synthetic task generation and authenticity concerns:**\nAll benchmark tasks are generated by LLMs rather than collected from real-world review scenarios, which raises questions about the authenticity of the task distribution. Although the authors include human verification, the underlying errors are created by modifying accepted papers via LLMs. As a result, the error types and frequency distributions may still deviate from those naturally occurring in real research writing. Moreover, since the tasks are generated using Gemini-2.5-Pro, this may partially explain its superior performance on the benchmark.\n\n**Potential data contamination and long-term validity:**\nThere is also a risk of benchmark contamination as newer models may already include papers from sources such as ICLR 2024/2025 or Nature Communications in their pretraining corpora. Over time, this contamination could become increasingly severe, diminishing the benchmark’s long-term validity. In particular, for tasks generated from the \"sampling\" method based on the public reviews, future models may memorize or overfit to these examples, reducing the benchmark’s long-term validity."}, "questions": {"value": "Have you analyzed why model performance remains low across all error categories, even when oracle evidence is provided? Is this primarily due to limitations in the models’ reasoning capability, or are there other contributing factors?\n\nAdditionally, have you evaluated human performance under the same oracle condition? From the current results, it seems that even with oracle evidence, the benchmark is still extremely challenging."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KM8SU9A4iX", "forum": "GDA1yB6yDP", "replyto": "GDA1yB6yDP", "signatures": ["ICLR.cc/2026/Conference/Submission25245/Reviewer_78Mn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25245/Reviewer_78Mn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872237269, "cdate": 1761872237269, "tmdate": 1762943378514, "mdate": 1762943378514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ScholScan, a new benchmark designed to evaluate multimodal large language models (MLLMs) on “scan-oriented” scholarly reasoning—i.e., the ability to read and cross-check entire academic papers like human researchers to identify internal consistency issues. Unlike conventional “search-oriented” paradigms that rely on pre-specified questions and localized retrieval, ScholScan presents target-absent queries that require holistic document understanding. The benchmark includes 1,800 expert-annotated questions across 9 scientific error types, drawn from 715 papers in 13 natural science domains, with detailed evidence localization and reasoning traces."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly articulates a meaningful distinction between search-oriented and scan-oriented reasoning paradigms, highlighting a critical gap in current MLLM capabilities for full-document scientific verification.\n\n2. The dataset construction process is rigorous: 10 domain experts performed dual reviews with third-party adjudication. Of 3,500 initial candidates, 1,700 were discarded and 1,541 revised—demonstrating strong quality control.\n\n3. The evaluation is comprehensive: 15 models under 24 input configurations and 8 RAG frameworks are tested. Results consistently show poor performance across the board, and notably, RAG offers no significant gains—underscoring the challenge of scan-oriented tasks."}, "weaknesses": {"value": "1. The definition of “scan-oriented” remains somewhat vague. Figure 1 contrasts target-prespecified vs. target-absent questions, which is clearer than the term “scan-oriented” itself. It would help to explicitly state that “scan-oriented” means processing the entire document (text or image) without relying on retrieved chunks—as opposed to RAG’s fragment-based approach.\n\n2. While the 9 error types span the scientific workflow (Figure 3), it is not convincingly demonstrated that all require full-document understanding. Many instances are derived from OpenReview comments, which may reflect localized, detail-level critiques rather than issues demanding cross-section or cross-paper reasoning.\n\n3. Model performance is universally low (e.g., even under Oracle RAG, average score is only 24.5/100). This raises the question: how would human experts perform on these tasks? Without a human baseline, it’s unclear whether the low scores reflect model limitations or simply extreme task difficulty.\n\n4. Several figures and tables lack sufficient clarity:\n   - Figures 4 and 5 omit axis labels.\n   - Figure 5’s “evidence locations” is not well-defined—how are these locations counted or validated?\n   - Table 2 reports RAG results but does not describe the detailed setting or related references of these RAG methods, making it hard to understand."}, "questions": {"value": "1. Scan-oriented processing requires ingesting the full paper, which can be very long. Given current context-length and attention limitations, could this approach become impractical for some real-world use? How does performance scale with document length?\n\n2. Section 4.4 notes that stronger models have fewer zero-score cases but more hallucinations. Yet Figure 5 shows Gemini suffering from high omission and hallucination rates—seemingly at odds with its relatively higher scores in Table 1. Can the authors reconcile this apparent contradiction?\n\n3. Figure 6 suggests performance degrades with more reasoning steps or evidence locations. However, this may simply reflect inherently harder tasks rather than a causal effect of reasoning length. Could the authors provide an analysis of task difficulty (e.g., via human expert ratings) to disentangle complexity from model limitations?\n\n4. The Oracle RAG condition does yield significant gains, implying that high-quality retrieval is helpful. Doesn’t this suggest the core issue is not the RAG paradigm itself, but the inadequacy of current retrievers? Might future advances in retrieval close part of the gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "chqZrMNEJW", "forum": "GDA1yB6yDP", "replyto": "GDA1yB6yDP", "signatures": ["ICLR.cc/2026/Conference/Submission25245/Reviewer_HMvn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25245/Reviewer_HMvn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898159001, "cdate": 1761898159001, "tmdate": 1762943378140, "mdate": 1762943378140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ScholScan, a new benchmark meant to evaluate multimodal large language models (MLLMs) on “scan-oriented” scholarly paper reasoning rather than traditional “search-oriented” QA. This paper introduces a task setting where the model is given one or more full academic papers and a target-absent instruction (e.g., “Assess the Methods section for Measurement & Operationalization issues”) and curated 1,800 questions with 9 families of scientific errors. The authors found that existing models perform badly in this new task, and this can not be simply addressed by a RAG pipeline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. **Task**: The paper draws a sharp conceptual line between search-oriented QA (“find X in this paper”) and scan-oriented critique (“is anything off about this study?”). This framing is intuitive, well motivated, and reflects a real gap between “LLM as literature assistant” and “LLM as junior reviewer.” This is an important task with pressing needs. \n\n2. **Dataset scale, diversity, and annotation rigor**: the benchmark curation process covers good scale (715 papers across 13 scientific domains (including physics, chemistry, computer science, biology, etc.) and 1,800 instances spanning nine error families is impressively broad and goes beyond narrowly technical CS-only corpora. There is rigorous human review of the generated data instances. \n\n3. **Strong analysis and observations**: OCR text often outperforms raw page images overall (likely because long-context language modeling is currently better than long-context vision-language modeling). Longer reasoning chains actually correlate with worse scores. They report that classical retrieval does not solve the “scan-oriented” problem because the hardest part is deciding what to look for in the first place. These are very valuable insights immediately transferrable to the AI-for-science community."}, "weaknesses": {"value": "1. **Synthetic edit to accepted papers**: A large fraction of benchmark items are created by prompting Gemini 2.5 Pro to insert coordinated edits into accepted/high-quality papers, and then asking models to catch those edits. The quality of this generated split of dataset is bounded by the capacity of Gemini 2.5 Pro. As the paper later finds, even best models so far struggle with the \"scan-oriented\" problems. This would hurt the quality of the benchmark. \n\n2. **Evaluation metric design**: The final score S(m) is an involved product of existence, evidence localization Dice (with squared penalties for over-reporting), reasoning prefix match, and a nonlinear hallucination penalty. There are many tunable constants (e.g. the 0.8 subtraction, exponent 1.5 in the hallucination penalty). It’s not obvious these specific choices are robust or interpretable across domains. \n\n3. **Leakage concern**: The benchmark is partially consisted of high-quality ICLR-accepted papers that future models will train on. Since the benchmark curation process involves extensive human annotations, data leakage problem becomes a major problem."}, "questions": {"value": "1. Can you break down, as exact percentages (or counts): (1) generated-edited accepted papers (Gemini edits), (2) sampled-from-rejected-papers (ICLR reviews), (3) cross-paper citation consistency cases?\n\n2. You mention that human evaluation “confirms high agreement” between the GPT-4.1 extraction/metric pipeline and expert annotations. Can you report some quantitative results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WaRUQUn2XF", "forum": "GDA1yB6yDP", "replyto": "GDA1yB6yDP", "signatures": ["ICLR.cc/2026/Conference/Submission25245/Reviewer_6ouw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25245/Reviewer_6ouw"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission25245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986213682, "cdate": 1761986213682, "tmdate": 1762943377897, "mdate": 1762943377897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}