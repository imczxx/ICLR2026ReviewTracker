{"id": "euw68T4r1E", "number": 10261, "cdate": 1758165389576, "mdate": 1759897662338, "content": {"title": "BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution", "abstract": "Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time\nevaluation from human perspectives to assess the quality of model responses. In\nthe coding domain, manually examining the quality of LLM-generated content\nis extremely challenging, as it requires understanding long chunks of raw code\nand deliberatively simulating code execution. To this end, we introduce BigCodeArena, an open human evaluation platform for code generation back-ended\nwith a comprehensive and on-the-fly execution environment. Built on top of Chatbot Arena, BigCodeArena features to enable the execution of LLM-generated\ncode and allows humans to interact with the execution process and outcomes. We\ncollected over 14K raw code-centric conversation sessions across 10 widely used\nLLMs, spanning 10 programming languages and 8 types of execution environments. Among these conversations, we identify more than 4.7K multi-turn samples\nwith pairwise human preference. Further analysis uncovers the underexplored\npreferences of LLMs in fine-grained domains characterized by tasks, languages,\nand frameworks. To systematically examine code understanding and generation capabilities of frontier LLMs, we curate two benchmarks based on the collected data,\nnamely BigCodeReward and AutoCodeArena. For BigCodeReward, we\npostprocess the 4.7K conversations and evaluate the consistency between reward\nmodels and human preference. The evaluation shows that most LLMs have superior performance in judging coding preferences when the execution results are\ngiven. Inspired by the findings, we propose AutoCodeArena, an automatic Elo\nrating benchmark designed to assess the coding quality of LLMs without humans.\nWe find that proprietary LLMs like GPT-5, Claude-Sonnet-4, and Claude-Opus-4\nstill lead the performance in code generation among the recent emerging models.\nTo democratize transparent evaluation of code generation in the wild, we aim to\nestablish BigCodeArena as a long-term project.", "tldr": "", "keywords": ["code generation", "llm", "evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0c0633601b524699f2ce025ee484aff243ca60fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an open crowdsourced evaluation platform for code generation that uses an on-the-fly execution environment.\nThis helps collecting more reliable human preference data, by mitigating errors from human judgment based only on static code review."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* Human preference collection: Execution feedback towardsmore robust human preference data. \n* Extensive collection: 10 languages and 8 execution environments with interactive debugging. \n* Benchmarks: BIGCODEREWARD and AUTOCODEARENA would help future research."}, "weaknesses": {"value": "* Sensitivity to quality:  As its robustness heavily depends on the quality and consistency of human-provided feedback, more analysis to justify possible low-quality inputs from human (see questions below)\n* Human cost: Collecting human preference data, especially across multiple languages and interactive environments can be expensive and time-consuming. Reporting time/infracost per each data point would be interesting\n* Model train:  It remains unclear how effectively these signals could be integrated into training pipelines when sufficiently collected. Discussion whether this can be used for training better model would be useful."}, "questions": {"value": "Any answer to weaknesses would help rebuttal\nSpecifically, how can we ensure reliability and consistency of human feedback?\nHow can we quantify or filter out low-quality feedbacks?\nHow sensitive are the evaluation metrics or model performance results to noisy feedbacks?\nWhat is the average time and cost per annotation or preference collection, and whether automation can reduce cost.\nHow scalable to expand to new languages or execution contexts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cIMgWd9RlX", "forum": "euw68T4r1E", "replyto": "euw68T4r1E", "signatures": ["ICLR.cc/2026/Conference/Submission10261/Reviewer_umg5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10261/Reviewer_umg5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761452696654, "cdate": 1761452696654, "tmdate": 1762921618503, "mdate": 1762921618503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BigCodeArena, an open human evaluation platform for code generation, featuring real-time execution and interactive debugging environments. The authors collected over 14K code-centric conversation sessions across 10 large language models, including 4.7K multi-turn sessions with pairwise human preference labels.\nBuilding on this dataset, the paper further proposes two benchmarks: (i) BigCodeReward — evaluating the consistency between reward model judgments and human preferences over 4.7K annotated conversations; and (ii) AutoCodeArena — an automated Elo-based benchmark for assessing coding quality without human evaluators.\nThe study highlights that (1) proprietary and open-source LLMs show comparable reliability in judging code quality, (2) execution feedback substantially improves preference alignment, and (3) GPT-5 achieves the best overall code generation quality among recent models."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- The work establishes a robust and transparent data collection framework for execution-based human evaluation. \n- The paper provides comprehensive design details on sandboxing, environment configuration, and preference aggregation, enhancing reproducibility.\n- The proposed benchmarks (BigCodeReward, AutoCodeArena) form a meaningful step toward automated and scalable evaluation of code generation systems."}, "weaknesses": {"value": "- The presentation could be improved for clarity and precision. Some parts, including figures and methodological descriptions, are confusing and would benefit from clearer exposition (see Questions).\n- The analysis feels somewhat limited, particularly on the code generation side. While Elo-based comparisons are informative, a deeper breakdown of error types, execution failures, or qualitative behavior differences across models would strengthen the findings."}, "questions": {"value": "- Section 3 states that annotators conducted \"multi-turn conversations with at least two user–model exchanges,\"\" but it remains unclear whether these interactions occurred independently with each model or jointly within the pairwise evaluation. \n\n- In Figure 3, the meanings of All Data, Environment Matched, and Language Matched are insufficiently specified. It is unclear whether these settings refer to pair-sampling constraints during evaluation (i.e., which model pairs are compared) or to post-hoc averaging/grouping criteria applied when aggregating Elo scores."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fyh3qhE5Ms", "forum": "euw68T4r1E", "replyto": "euw68T4r1E", "signatures": ["ICLR.cc/2026/Conference/Submission10261/Reviewer_gpiC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10261/Reviewer_gpiC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904844044, "cdate": 1761904844044, "tmdate": 1762921618123, "mdate": 1762921618123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "-BigCodeArena builds on Chatbot Arena to collect a dataset of 4.7k preferences.\n- From this data, the authors introduce BigCodeReward to evaluate the consistency between reward models and human preferences.\n- Additionally, the authors also introduce AutoCodeArena to automate the process of preference collection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles an important topic, addressing the need for reliable human evaluation of LLM-generated code.\n- It is generally well-written and easy to follow. \n- The authors demonstrate multiple uses of the collected data, showing how human preferences can power both BigCodeReward and AutoCodeArena."}, "weaknesses": {"value": "- The paper lacks actionable insights or design recommendations for practitioners—either developers using coding assistants or researchers building LLMs—reducing its practical significance. Relatedly, Several components of the work appear incremental relative to prior platforms such as Chatbot Arena and WebDevArena; similarly, AutoCodeArena resembles prior automated evaluation setups like Arena-Hard. The paper could better articulate what unique insights BigCodeArena contributes beyond integrating existing ideas or infrastructure.\n- It is unclear how BigCodeReward handles noisy or inconsistent human preferences, which could affect the reliability of results.\n- I could not find analysis or results on the response rate or completion statistics for the optional sub-questions, leaving unclear how representative these annotations are. \n- There is no comparison to existing leaderboards or ranking trends (e.g., Chatbot Arena), and the observation that frontier proprietary models remain strongest is unsurprising. At a cursory glance, there seems to be a strong correlation."}, "questions": {"value": "Please address each of the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZpcWdirrQ4", "forum": "euw68T4r1E", "replyto": "euw68T4r1E", "signatures": ["ICLR.cc/2026/Conference/Submission10261/Reviewer_XV5J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10261/Reviewer_XV5J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930178718, "cdate": 1761930178718, "tmdate": 1762921617571, "mdate": 1762921617571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BIGCODEARENA, an open and execution-based human evaluation platform for code generation models. Specifically, the platform is built upon Chatbot Arena, which integrates real-time code execution, interactive debugging, and pairwise preference collection to produce a more reliable measure of model performance. Moreover, the authors collect 14K code-centric conversations across 10 LLMs, 10 programming languages, and 8 execution environments, from which 4.7K multi-turn preference samples are curated."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors correctly identify a key limitation in existing human evaluation: humans often cannot judge code quality without execution. By integrating executable environments, this work provides a more realistic and reliable evaluation protocol.\n2. The writing is good, and the paper is easy to follow."}, "weaknesses": {"value": "1. The novelty is limited. While execution-based evaluation is impactful, the conceptual novelty mainly lies in combining existing ideas (Chatbot Arena + executable sandbox). The methodology may be viewed more as engineering integration than a new algorithmic or theoretical contribution.\n2. AUTOCODEARENA relies on LLM-as-a-Judge (Claude-3.7-Sonnet), which may itself introduce bias. The paper lacks calibration or agreement analysis between automated and human judgments.\n3. Although expert volunteers are mentioned, the annotation process (e.g., inter-annotator agreement, quality metrics, error analysis) is not deeply evaluated, which could raise concerns about reliability.\n4. I think the topic of the paper is not well-suited for ICLR. The paper looks more like a platform description rather than a research paper with clear methodological novelty."}, "questions": {"value": "The author should provide more insight into the paper, for example, whether we can use the analysis behind the platform to further improve the current LLMs. Or how we can better help humans to use LLMs to generate code."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "MYBVJR0EsA", "forum": "euw68T4r1E", "replyto": "euw68T4r1E", "signatures": ["ICLR.cc/2026/Conference/Submission10261/Reviewer_WBdo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10261/Reviewer_WBdo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049511161, "cdate": 1762049511161, "tmdate": 1762921617139, "mdate": 1762921617139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}