{"id": "0XjUSPBVfW", "number": 14055, "cdate": 1758227679240, "mdate": 1763749980481, "content": {"title": "Natural gradient Bayesian sampling automatically emerges in canonical cortical circuits", "abstract": "Accumulating evidence suggests the canonical cortical circuit, consisting of excitatory (E) and diverse classes of inhibitory (I) interneurons, implements Bayesian posterior sampling. However, most of the identified circuits' sampling algorithms are simpler than the nonlinear circuit dynamics, suggesting complex circuits may implement more advanced algorithms. Through comprehensive theoretical analyses, we discover the canonical circuit innately implements natural gradient Bayesian sampling, which is an advanced sampling algorithm that adaptively adjusts the sampling step size based on the local geometry of stimulus posteriors measured by Fisher information. Specifically, the nonlinear circuit dynamics can implement natural gradient Langevin and Hamiltonian sampling of uni- and multi-variate stimulus posteriors, and these algorithms can be switched by interneurons. We also find that the non-equilibrium circuit dynamics when transitioning from the resting to evoked state can further accelerate natural gradient sampling, and analytically identify the neural circuit's annealing strategy. Remarkably, we identify the approximated computational strategies employed in the circuit dynamics, which even resemble the ones widely used in machine learning. Our work provides an overarching connection between canonical circuit dynamics and advanced sampling algorithms, deepening our understanding of the circuit algorithms of Bayesian sampling.", "tldr": "", "keywords": ["Canonical neural circuits", "Continuous attractor networks", "Bayesian sampling", "Natural gradient Monte Carlo", "Annealing strategy", "non-equilibrium"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/49e6bb9a5fc3ae24542c85ae7bbc2d0410aea381.pdf", "supplementary_material": "/attachment/e3f65d34751bb53f13e4e2725dad944fa536ad8d.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a theoretical analysis of how canonical cortical circuits, composed of excitatory (E) and inhibitory (PV, SOM) neurons, can implement natural gradient Bayesian sampling. The core contribution is the finding that, under a uniform prior, the bump height of the E-neuron population response encodes the Fisher Information (FI) of the posterior. This mechanism dynamically controls the sampling time constant of the bump's position, allowing the circuit to biologically implement Natural Gradient Langevin Sampling (NGLS). The authors further demonstrate that incorporating SOM neurons, which introduce a momentum term, upgrades the circuit to perform Natural Gradient Hamiltonian Sampling (NGHS). The work also connects other computational strategies, such as annealing and regularization, to the emergent properties of the E-I circuit dynamics."}, "soundness": {"value": 4}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a comprehensive theoretical bridge between the dynamics of canonical E-I circuits and advanced Bayesian sampling algorithms.\n\n2. It demonstrates how distinct neuron populations and their specific interactions can mechanistically implement components of these advanced sampling strategies.\n\n3. It also offers novel insights into the computational role of non-equilibrium dynamics as an innate annealing strategy that can accelerate sampling.\n\n4. The theoretical claims are well-supported by numerical simulations."}, "weaknesses": {"value": "A major concern is that the work's core findings appear to be an incremental step over Sale & Zhang (2024). The authors acknowledge their work builds on this previous study , which had already established the circuit's Langevin and Hamiltonian sampling properties. The main new finding is that the E-neuron response height encodes the FI, thus upgrading the circuit to NGLS/NGHS. However, this critical finding is true only when the prior is uniform. This assumption limits the generality and remains a constraint on the work's overall impact.\n\nThe writing and clarity of the paper can be further improved. Some notations are a little bit confusing or not adequately explained in the main text. (E.g., $\\xi$ in Eq. (1a), $p(s)$ in \"We will leave the subjective prior p(s)...\", and some inconsistent notations between main texts and appendix.) This makes it difficult to fully understand the theoretical analysis. Besides, although the figures are generally neat and beautiful, legends in Figure 2E&F&K are confusing. Different colors used in these figures are not explained."}, "questions": {"value": "See the weakness section for my major concerns.\n\n1. Could the authors please elaborate on the significance of their findings beyond Sale & Zhang (2024)? Given that the key NGLS/NGHS finding is constrained to a uniform prior, how much of the circuit's behavior is already captured by LS/HS models from the previous work?\n\n2. How would the proposed mechanism (E bump height encodes FI) break down if a non-uniform prior is introduced? Would the circuit still perform some approximation of NGLS, or would the direct link between bump height and FI be lost?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "23uHisfPLQ", "forum": "0XjUSPBVfW", "replyto": "0XjUSPBVfW", "signatures": ["ICLR.cc/2026/Conference/Submission14055/Reviewer_uXx1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14055/Reviewer_uXx1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761383880204, "cdate": 1761383880204, "tmdate": 1762924540719, "mdate": 1762924540719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "There has in recent years been much interest in sampling-based probabilistic inference in neural networks as a model for various probabilistic computations in the brain. This paper builds on a line of work by Wen-Hao Zhang and collaborators, in particular [Sale and Zhang, NeurIPS 2024](https://proceedings.neurips.cc/paper_files/paper/2024/file/f463d31ed2fdd7b0ec585c041ec1baa8-Paper-Conference.pdf), that aims to show that the dynamics of the location of a bump attractor state in a mean-field-style model for the canonical cortical microcircuit can sample interesting distributions. Its contribution is to try to show that the dynamics of the bump can approximate not just naive Langevin or Hamiltonian Langevin dynamics (as is done in Sale and Zhang), but natural gradient Hamiltonian Langevin dynamics, which can enable faster convergence to the target distribution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper is timely, and its headline premise - that an efficient sampling algorithm naturally emerges from a standard model for cortical circuits - is clearly of interest to a broad computational neuroscience audience."}, "weaknesses": {"value": "The biggest weakness of the paper is that, contrary to what the title and the abstract would lead the reader to believe, the authors do not actually show that their model implements full Riemannian Langevin dynamics. Rather, they show that the circuit can approximate the *diagonal* of the inverse of the Fisher information matrix, rather than the full matrix inverse. The authors try to present this as a feature rather than a bug by saying that the diagonal approximation is commonly used in machine learning, but I fundamentally think that it is not appropriate to say that the network is performing emergent Riemannian MCMC in that case (I would also note that most of the cited ML applications involve non-Gaussian distributions with non-constant Fisher information matrices, for which estimating the Fisher is much more difficult than in the Gaussian case). This is a crucial conceptual feature of Riemannian MCMC that the authors miss even when they introduce the idea of using natural gradients around equation (5): not only the size, but the structure, of the sampling noise is adapted to the geometry of the problem. This discrepancy also raises the need for some more experimental tests, which I detail under **Questions**. \n\nOn the whole, I do not see this paper as being a sufficient conceptual advance relative to Sale and Zhang - which already shows evidence for Hamiltonian-like acceleration - given that the result is restricted to problem-adapted step sizes rather than adaptation of the structure of the metric, i.e., of the principal axes of variance of the sampling noise. I thus disagree with the overall tone (\"remarkably\", \"unprecedentedly\", etc)."}, "questions": {"value": "- What is the relationship of this manuscript to https://openreview.net/forum?id=BpBW4gJofo, also submitted to ICLR, with which it overlaps substantially? Even the figures look substantially similar, and the model is, so far as I can tell, precisely the same. Compare, for instance, eq.  (15) of this submission with eq. (17) of that submission. There the presentation is in terms of \"adaptive step sizes\" rather than approximate natural gradient, but the setting and overall scope of results are fundamentally the same. Some clarification here is required. \n\n- Given that the circuit implements a diagonal approximation to the Fisher matrix, I think a few more experimental probes are required. Figure 3A shows convergence rates in KL over time, but I think it is critical to test how this depends on the dimensionality and structure of the target distribution. I would expect the gap between the full Riemannian MCMC and the diagonal approximation to grow as the problem dimensionality increases and also as the different dimensions become more correlated. Looking at the cited paper by Masset and colleagues (NeurIPS 2022) that is the main related work to which the authors compare their results, those authors showed that both of those factors lead to slowdown both in linear rate networks and in spiking networks that approximate the rate dynamics. \n\n- The decomposition in eq. (15) seems non-standard relative to the usual way of decomposing the dynamics of a multivariate OU process into symmetric and skew-symmetric parts, as is done in the cited work of Ma, Fox, and colleagues (NeurIPS 2015). Could you also present the dynamics in that form? \n\n- How does the accuracy of sampling depend on the number of neurons used? From a neuroscience perspective, this is important as it determines the minimum size of each circuit module in the setup for sampling multi-dimensional distributions that the authors present. So far as I can tell, this is not probed systematically."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)"]}, "details_of_ethics_concerns": {"value": "Submission https://openreview.net/forum?id=BpBW4gJofo overlaps substantially with this paper. See my first Question for details."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KEJNytoePE", "forum": "0XjUSPBVfW", "replyto": "0XjUSPBVfW", "signatures": ["ICLR.cc/2026/Conference/Submission14055/Reviewer_BBUn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14055/Reviewer_BBUn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678217564, "cdate": 1761678217564, "tmdate": 1762924539974, "mdate": 1762924539974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global reply"}, "comment": {"value": "## Significance and contribution of our study\nOur theoretical neuroscience study aims to understand the algorithm of canonical neural circuits. An exact and analytical mapping between nonlinear recurrent circuit dynamics and Bayesian algorithms is largely missing due to complex, nonlinear circuit dynamics. Therefore, we perform comprehensive math analysis on an analytically solvable nonlinear circuit model, which for the first time builds __an analytical mapping between the nonlinear circuit and the NG sampling in the stimulus subspace__. These are the major theoretical contributions of the current study and are acknowledged by most reviewers.\n\n\n## The novelty of the present paper compared with Sale 2024\nThe present study gains significant __algorithmic understanding__ as well as __technical/theoretical expansion__ compared with Sale 2024. \n1.   Our work for the first time shows the canonical circuit with two types of interneurons implements __NG__ Langevin sampling (Sec. 4 and 5) and Hamiltonian sampling (Sec. 6) of the entire family of posteriors **without** any manual intervention of the circuit activities or parameters. \n2. We further __analytically__ identify the __approximation strategy__ used in the circuit to estimate the Fisher information (FI), including the **regularization** in FI inversion (The recurrent E input in Sec. 4.2 and $\\alpha$ in Eq. 5), and diagonal FI matrix approximation (Eq. 14). It is a surprise that the approximation strategy in neural circuit is the same as conventional ML algorithms.\n3. We analytically identify an embedded the __annealing strategy__ in the circuit (Eq. 3b and Sec. 4.3). \n- __Theoretical analysis__: This conceptual advance was only possible because we developed a more comprehensive analytical framework. Sale 2024 only projected the dynamics onto the position mode (Eq. 10 in Sale 2024), while the present study projected the circuit dynamics onto both position mode (Eq. 12a) and the __height__ mode (Eq. 12b). The height mode projection is the cornerstone for identifying the NG sampling in the circuit dynamics. We formally demonstrate that:\n1. The height $U_E$ (Eq. 3b) acts as the **dynamic time constant** of the position dynamics (Eq. 3a).\n2. It dynamically **encodes the Fisher Information** of the probability distribution.\n\n## Our research route\nOur research route is __from neural circuits to their internal models and algorithms__. We start from the circuit model widely used in neuroscience research and theoretically discover its internal model and algorithms. It is a complementary technical route compared with conventional approach starting by assuming (complex) generative models and seeking their circuit implementation, which, however, usually faces a dilemma that the derived circuit may lack biological plausibility. Since both research routes share a common goal of linking the circuit and the internal model and algorithms, and hence the __direct link__ in our study will be the one of the most valuable results to the field.\nEven if our discovered Gaussian model is relatively simple, it presents a novel approach in the field and it can motivate the field to revise the circuit model to incorporate a more complex internal model which can be facilitated by our established analytical approach (see the Generalization section below). Consequently, the NG circuit sampling with an internal Gaussian model remains valuable to the field.\n\n## Generalization: recurrent circuit as latent sampler in deep generative models\nOur analytical mapping between circuit and algorithm provides a solid basis to generalize into more complex tasks. Due to the space limit, it is implausible to incorporate comprehensive theoretical analysis of NG sampling of non-Gaussian, multimodal distributions in the manuscript, so we add an Appendix G in the revised manuscript to discuss some mechanisms. In addition, to illustrate the generalization of our NG Gaussian posterior sampling circuit, we add a proof-of-concept example in a new section before the Discussion and a new Fig. 5 in the revised manuscript.\nIn this example, we propose the __NG Gaussian sampling circuit can be a latent sampler in deep generative models__. We are motivated by the convention in deep learning that the deep networks utilize encoders and decoders to map complex data distributions into simple Gaussian distributions in latent space. In this way, our NG Gaussian sampling circuit serves as a biologically plausible network implementation for a latent space sampler in deep generative models. Note that although deep generative models have achieved great success in applications, their whole network architecture still has a huge gap to neural circuits by lacking recurrent network architecture. Our analysis shows the NG sampling in the canonical circuit is maintained in the latent space in deep generative models (Fig. 5G)."}}, "id": "bWyGx5sbMH", "forum": "0XjUSPBVfW", "replyto": "0XjUSPBVfW", "signatures": ["ICLR.cc/2026/Conference/Submission14055/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14055/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14055/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763738283779, "cdate": 1763738283779, "tmdate": 1763738530127, "mdate": 1763738530127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the work of Sale & Zhang (2024) on Bayesian sampling in canonical cortical circuits to show that these circuits naturally implement natural gradient (NG) sampling algorithms. The authors propose that the canonical circuit, consisting of excitatory neurons and parvalbumin/somatostatin interneurons, can implement both natural gradient Langevin and Hamiltonian sampling. The key mechanism is that the total activity of E neurons (bump height $U_E$​) monotonically increases with the posterior's Fisher information, which automatically adjusts the sampling step size based on the local geometry of the posterior distribution. The authors demonstrate through theoretical analysis that the circuit implements NG Langevin sampling in the reduced E+PV circuit, adding SOM neurons enables NG Hamiltonian sampling, non-equilibrium dynamics during the transition from resting to evoked states further accelerates sampling through an intrinsic annealing strategy, and coupled circuits can sample multivariate posteriors using diagonal Fisher information matrix approximations analogous to techniques in machine learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: the paper makes a theoretical contribution by identifying that canonical cortical circuits naturally implement natural gradient sampling, extending beyond the naive Langevin and Hamiltonian sampling identified in Sale & Zhang (2024). The connection between E neuron bump height and Fisher information provides a novel functional interpretation of circuit dynamics. The identification of computational approximations in the circuit (regularization via recurrent connections, diagonal FIM approximation) that parallel ML techniques is conceptually interesting.\n2. Quality: the mathematical analysis is rigorous and follows established methods for analyzing continuous attractor networks. The authors provide detailed perturbative analysis, eigenmode decomposition, and explicit mappings between circuit dynamics and NG sampling algorithms. The derivations connect circuit parameters (bump height $U_E$​) to Fisher information and sampling step sizes in a principled way.\n3. Clarity: the paper is generally well-structured. The progression from naive sampling to NG sampling is clearly explained, and the figures are effective in illustrating the main concepts, in particular showing how bump height scales with Fisher information and how this determines sampling time constants. The supplementary materials provide thorough derivations.\n4. Significance: the potential unification of natural gradient sampling within the canonical circuit architecture is conceptually appealing. The model makes concrete predictions about the relationship between E neuron population activity, Fisher information, and sampling efficiency. The connection to ML approximation strategies (regularization, diagonal FIM) bridges neuroscience to machine learning."}, "weaknesses": {"value": "1. I am not sure about the computational necessity of sampling in the circuit model if not to encode posterior uncertainty. The authors state that \"a single snapshot of $r_F$ parametrically conveys the whole stimulus likelihood\" (Eq. 8), meaning a population vector readout is sufficient. Given this, it's unclear what computational advantage NG sampling provides over simpler population coding schemes like probabilistic population codes (PPC), which can also perform Bayesian inference with linear readouts but without the complexity of maintaining sampling dynamics. Since the posterior uncertainty is entirely determined by the feedforward input rate $r_F$ (which controls likelihood precision $\\Lambda$), the neural variability does not represent posterior uncertainty in the way that sampling-based models typically propose. For the Gaussian likelihoods and uniform priors assumed in this framework, deterministic inference methods (like direct computation of the posterior mean and variance) would be exact and more efficient. The authors do not provide quantitative comparisons of computational costs, convergence speed, or accuracy against such alternatives.\n2. Moreover, restrictive assumptions limit the generality of this approach. The framework relies heavily on several assumptions. First, the model assumes Gaussian feedforward tuning curves (Eq. 1e) leading to Gaussian likelihoods (Eq. 8). However, real sensory likelihoods are often non-Gaussian and multimodal, and one of the purported strengths of sampling-based approaches is that they can represent arbitrary distributions. The authors do not address how the circuit would handle non-Gaussian inference problems. Second, the model assumes a 1D ring attractor, and its specific eigenmode structure is essential for the perturbation analysis. While the authors show a 2D extension (Fig. A4) that couples two ring attractors, this is still a rather restrictive latent structure which presumably not all canonical circuits possess, and how to scale to higher-dimensional feature spaces without this specific structure remains unclear. Finally, the analysis uses uniform priors throughout. This eliminates one of the key computational challenges of Bayesian inference, which is to show that the prior can reflect the statistics of its inputs. The authors do not demonstrate that the circuit can implement informative non-uniform priors or flexibly switch between different prior distributions.\n3. The circuit already receives critical information that undermines claims of adaptive inference. Specifically: the feedforward input $r_F$ directly encodes the likelihood mean $\\mu_z$ and precision $\\Lambda$ (Eq. 8), meaning the posterior parameters are essentially pre-computed and fed into the circuit rather than inferred. Moreover, while the authors claim the circuit \"adaptively adjusts the sampling step size,\" this adaptation is entirely driven by the feedforward input intensity $R_F$, which is externally provided rather than computed by the circuit itself. The recurrent weights ($w_{EE}$, $w_{SE}$, etc.) are also precisely tuned to match the required sampling parameters (Eqs. 11, Fig. 4E), but the authors do not mention how these precise weight configurations would be acquired or whether synaptic plasticity could maintain them.\n4. The authors briefly mention that deterministic inference circuits require \"complicated nonlinear functions,\" but this claim is specific to their problem structure (Gaussian distributions, linear-Gaussian dynamics). However, they do not provide quantitative comparisons of computational costs relative to deterministic approaches, convergence speed relative to standard (non-NG) sampling, accuracy trade-offs between their approach and alternatives, and performance relative to the single previous natural gradient sampling circuit study (Masset et al., 2022). The claim that NG sampling provides advantages over naive sampling is not substantiated with systematic quantitative comparisons.\n5. There is limited biological justification for key mechanisms in the model. While the circuit architecture is based on known connectivity patterns, the assumption that recurrent E weights ($w_{EE}$) act as a regularization parameter (analogous to $\\alpha$ in Eq. 5) is mathematically convenient but lacks biological justification. How would the circuit \"know\" to set this weight to prevent numerical instabilities in Fisher information inversion? Moreover, the non-equilibrium annealing strategy is presented as an \"emergent property,\" but the functional advantage of this particular annealing schedule over other possible dynamics is not demonstrated."}, "questions": {"value": "1. How would the circuit handle non-Gaussian likelihoods, which are common in real sensory processing? Can you provide numerical experiments or extensions showing the framework handles cases where the Gaussian assumption breaks down?\n2. Given that the likelihood can be read out with a population vector (linear decoder) from $r_F$, what specific computational advantages does NG sampling provide over probabilistic population codes (PPC) or direct computation of posterior parameters? Can you provide quantitative comparisons (e.g. in terms of inference accuracy or speed, or computational cost)?\n3. For Gaussian likelihoods and uniform priors, the posterior is also Gaussian with analytically computable parameters. What is the computational advantage of approximating this via sampling when exact solutions are available? Under what conditions would sampling-based inference be preferred?\n4. How would the circuit acquire the precise weight configurations required for NG sampling (e.g., relationships in Eqs. 11, 14, Fig. 4E)? Do you propose these synaptic weights are learned, and if so, through what learning rule?\n5. The claim that recurrent E input acts as regularization (like $\\alpha$ in Eq. 5) is interesting, but how does the biological circuit \"know\" to set $w_{EE}$ to prevent numerical instabilities in Fisher information inversion? What mechanisms would maintain this relationship as environmental statistics change?\n6. In multivariate cases, the circuit uses diagonal FIM approximation rather than full FIM. Can you quantify the loss in performance relative to full FIM?\n7. How does the NG sampling in your circuit compare to naive Langevin/Hamiltonian sampling, deterministic inference, and the NG sampling circuit in Masset et al. (2022)? I would appreciate specific metrics like convergence time, sample efficiency, and accuracy.\n8. The non-equilibrium annealing is claimed to accelerate sampling (Fig. 2K), but by how much compared to equilibrium sampling? How does this depend on circuit parameters?\n9. How sensitive is the NG sampling to mismatches between the assumed circuit parameters and the true parameters? For example, what happens when $w_{EE}$ deviates from the value required for proper regularization, or when Fisher information is estimated incorrectly?\n10. Can the circuit handle time-varying Fisher information (e.g., if the feedforward input statistics change over time)? How quickly can the circuit adapt its sampling strategy to new posterior geometries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Bn0TIG5Heb", "forum": "0XjUSPBVfW", "replyto": "0XjUSPBVfW", "signatures": ["ICLR.cc/2026/Conference/Submission14055/Reviewer_g6vX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14055/Reviewer_g6vX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762616608085, "cdate": 1762616608085, "tmdate": 1762924539559, "mdate": 1762924539559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the work of Sale & Zhang (2024) on Bayesian sampling in canonical cortical circuits to show that these circuits naturally implement natural gradient (NG) sampling algorithms. The authors propose that the canonical circuit, consisting of excitatory neurons and parvalbumin/somatostatin interneurons, can implement both natural gradient Langevin and Hamiltonian sampling. The key mechanism is that the total activity of E neurons (bump height $U_E$​) monotonically increases with the posterior's Fisher information, which automatically adjusts the sampling step size based on the local geometry of the posterior distribution. The authors demonstrate through theoretical analysis that the circuit implements NG Langevin sampling in the reduced E+PV circuit, adding SOM neurons enables NG Hamiltonian sampling, non-equilibrium dynamics during the transition from resting to evoked states further accelerates sampling through an intrinsic annealing strategy, and coupled circuits can sample multivariate posteriors using diagonal Fisher information matrix approximations analogous to techniques in machine learning.\n\nAs a side note, I wish to disclose that I used a local LLM to help edit my initial draft of this review to improve clarity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Originality: the paper makes a theoretical contribution by identifying that canonical cortical circuits naturally implement natural gradient sampling, extending beyond the naive Langevin and Hamiltonian sampling identified in Sale & Zhang (2024). The connection between E neuron bump height and Fisher information provides a novel functional interpretation of circuit dynamics. The identification of computational approximations in the circuit (regularization via recurrent connections, diagonal FIM approximation) that parallel ML techniques is conceptually interesting.\n2. Quality: the mathematical analysis is rigorous and follows established methods for analyzing continuous attractor networks. The authors provide detailed perturbative analysis, eigenmode decomposition, and explicit mappings between circuit dynamics and NG sampling algorithms. The derivations connect circuit parameters (bump height $U_E$​) to Fisher information and sampling step sizes in a principled way.\n3. Clarity: the paper is generally well-structured. The progression from naive sampling to NG sampling is clearly explained, and the figures are effective in illustrating the main concepts, in particular showing how bump height scales with Fisher information and how this determines sampling time constants. The supplementary materials provide thorough derivations.\n4. Significance: the potential unification of natural gradient sampling within the canonical circuit architecture is conceptually appealing. The model makes concrete predictions about the relationship between E neuron population activity, Fisher information, and sampling efficiency. The connection to ML approximation strategies (regularization, diagonal FIM) bridges neuroscience to machine learning."}, "weaknesses": {"value": "1. I am not sure about the computational necessity of sampling in the circuit model if not to encode posterior uncertainty. The authors state that \"a single snapshot of $r_F$ parametrically conveys the whole stimulus likelihood\" (Eq. 8), meaning a population vector readout is sufficient. Given this, it's unclear what computational advantage NG sampling provides over simpler population coding schemes like probabilistic population codes (PPC), which can also perform Bayesian inference with linear readouts but without the complexity of maintaining sampling dynamics. Since the posterior uncertainty is entirely determined by the feedforward input rate $r_F$ (which controls likelihood precision $\\Lambda$), the neural variability does not represent posterior uncertainty in the way that sampling-based models typically propose. For the Gaussian likelihoods and uniform priors assumed in this framework, deterministic inference methods (like direct computation of the posterior mean and variance) would be exact and more efficient. The authors do not provide quantitative comparisons of computational costs, convergence speed, or accuracy against such alternatives.\n2. Moreover, restrictive assumptions limit the generality of this approach. The framework relies heavily on several assumptions. First, the model assumes Gaussian feedforward tuning curves (Eq. 1e) leading to Gaussian likelihoods (Eq. 8). However, real sensory likelihoods are often non-Gaussian and multimodal, and one of the purported strengths of sampling-based approaches is that they can represent arbitrary distributions. The authors do not address how the circuit would handle non-Gaussian inference problems. Second, the model assumes a 1D ring attractor, and its specific eigenmode structure is essential for the perturbation analysis. While the authors show a 2D extension (Fig. A4) that couples two ring attractors, this is still a rather restrictive latent structure which presumably not all canonical circuits possess, and how to scale to higher-dimensional feature spaces without this specific structure remains unclear. Finally, the analysis uses uniform priors throughout. This eliminates one of the key computational challenges of Bayesian inference, which is to show that the prior can reflect the statistics of its inputs. The authors do not demonstrate that the circuit can implement informative non-uniform priors or flexibly switch between different prior distributions.\n3. The circuit already receives critical information that undermines claims of adaptive inference. Specifically: the feedforward input $r_F$ directly encodes the likelihood mean $\\mu_z$ and precision $\\Lambda$ (Eq. 8), meaning the posterior parameters are essentially pre-computed and fed into the circuit rather than inferred. Moreover, while the authors claim the circuit \"adaptively adjusts the sampling step size,\" this adaptation is entirely driven by the feedforward input intensity $R_F$, which is externally provided rather than computed by the circuit itself. The recurrent weights ($w_{EE}$, $w_{SE}$, etc.) are also precisely tuned to match the required sampling parameters (Eqs. 11, Fig. 4E), but the authors do not mention how these precise weight configurations would be acquired or whether synaptic plasticity could maintain them.\n4. The authors briefly mention that deterministic inference circuits require \"complicated nonlinear functions,\" but this claim is specific to their problem structure (Gaussian distributions, linear-Gaussian dynamics). However, they do not provide quantitative comparisons of computational costs relative to deterministic approaches, convergence speed relative to standard (non-NG) sampling, accuracy trade-offs between their approach and alternatives, and performance relative to the single previous natural gradient sampling circuit study (Masset et al., 2022). The claim that NG sampling provides advantages over naive sampling is not substantiated with systematic quantitative comparisons.\n5. There is limited biological justification for key mechanisms in the model. While the circuit architecture is based on known connectivity patterns, the assumption that recurrent E weights ($w_{EE}$) act as a regularization parameter (analogous to $\\alpha$ in Eq. 5) is mathematically convenient but lacks biological justification. How would the circuit \"know\" to set this weight to prevent numerical instabilities in Fisher information inversion? Moreover, the non-equilibrium annealing strategy is presented as an \"emergent property,\" but the functional advantage of this particular annealing schedule over other possible dynamics is not demonstrated."}, "questions": {"value": "1. How would the circuit handle non-Gaussian likelihoods, which are common in real sensory processing? Can you provide numerical experiments or extensions showing the framework handles cases where the Gaussian assumption breaks down?\n2. Given that the likelihood can be read out with a population vector (linear decoder) from $r_F$, what specific computational advantages does NG sampling provide over probabilistic population codes (PPC) or direct computation of posterior parameters? Can you provide quantitative comparisons (e.g. in terms of inference accuracy or speed, or computational cost)?\n3. For Gaussian likelihoods and uniform priors, the posterior is also Gaussian with analytically computable parameters. What is the computational advantage of approximating this via sampling when exact solutions are available? Under what conditions would sampling-based inference be preferred?\n4. How would the circuit acquire the precise weight configurations required for NG sampling (e.g., relationships in Eqs. 11, 14, Fig. 4E)? Do you propose these synaptic weights are learned, and if so, through what learning rule?\n5. The claim that recurrent E input acts as regularization (like $\\alpha$ in Eq. 5) is interesting, but how does the biological circuit \"know\" to set $w_{EE}$ to prevent numerical instabilities in Fisher information inversion? What mechanisms would maintain this relationship as environmental statistics change?\n6. In multivariate cases, the circuit uses diagonal FIM approximation rather than full FIM. Can you quantify the loss in performance relative to full FIM?\n7. How does the NG sampling in your circuit compare to naive Langevin/Hamiltonian sampling, deterministic inference, and the NG sampling circuit in Masset et al. (2022)? I would appreciate specific metrics like convergence time, sample efficiency, and accuracy.\n8. The non-equilibrium annealing is claimed to accelerate sampling (Fig. 2K), but by how much compared to equilibrium sampling? How does this depend on circuit parameters?\n9. How sensitive is the NG sampling to mismatches between the assumed circuit parameters and the true parameters? For example, what happens when $w_{EE}$ deviates from the value required for proper regularization, or when Fisher information is estimated incorrectly?\n10. Can the circuit handle time-varying Fisher information (e.g., if the feedforward input statistics change over time)? How quickly can the circuit adapt its sampling strategy to new posterior geometries?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Bn0TIG5Heb", "forum": "0XjUSPBVfW", "replyto": "0XjUSPBVfW", "signatures": ["ICLR.cc/2026/Conference/Submission14055/Reviewer_g6vX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14055/Reviewer_g6vX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762616608085, "cdate": 1762616608085, "tmdate": 1763399206357, "mdate": 1763399206357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyses the dynamics of E-I circuits (including two classes of I neurons, PV and SOM) with ring architectures, and through extensive mathematical derivations and also numerical simulations shows that they perform particular forms of sampling (natural gradient Langevin and Hamiltonian) from simple (mostly 1D Gaussian) distributions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The fundamental goal of relating neural circuit dynamics to (sampling-based) inference is interesting, and there is a lot of nice ideas for working out such a relationship (e.g. the overall magnitude of neural activities acting as regulator of step size, or the putative role of SOM neurons in sampling). The mathematical analyses are extensive, and the paper is generally well written."}, "weaknesses": {"value": "1. There are a number of seemingly arbitrary choices in model construction:\n\n- Why is PV vs. SOM cell-mediated inhibition taken to be global, divisive (in firing rates), and instantaneous vs. local, subtractive (in synaptic inputs), and finite time-scale (referring to both the time constants of the I cells, and their effects on E cells), respectively? (E.g. if anything, I would have thought PV inhibition is more local than SOM is.)\n\n- Why are only E but not I cell synaptic inputs noisy?\n\n2. Overall, the circuit is shown to be able to sample from a 1D Gaussian (or the 1D marginals of a 2D Gaussian, see below). Sampling becomes particularly useful for high dimensional joint posteriors, and with more complicated distributions. It's unclear if such more challenging forms of sampling can be solved by this approach (although the Discussion does mention some future directions in this regard). Specifically, the Gaussian represents a special case for NG Langevin sampling, in which the FI is constant, and as such, equivalent to just changing the (effective) time constant. So it's unclear if the circuit actually continues to accurately approximate NG Langevin in the more general case, when the FI changes as a function of the sample (again, I acknowledge the mentioning of this in the Discussion). The setup also seems to require that the latent variable is directly encoded in a Gaussian-Poisson population code, so that the likelihood is a Gaussian, whose precision scales linearly with the input firing rates. Again, it is unclear how this can be generalized to cases of practical interest, in which neither of these assumption will hold generally.\n\n3. Based on the derivations, in order for the circuit to implement NG Langevin sampling, U_E should scale linearly with FI (Eq.11). Yet, in the simulations, when w_EE is sufficiently large, there is a strong threshold nonlinearity in the relationship between the two (Fig.2C). Furthermore, the authors suggest that U_EE (controlled by w_EE) acts as a regularizer \"improving the numerical stability in inverting the FI when it is small or ill-conditioned\". However, the empirical relationship between U_E and FI (Fig.2C) seems to be such that specifically at the small FI values, where regularization is supposed to be useful, there is no difference between small and large w_EE values. Indeed, the results shown in Fig.2F barely show any advantage for a larger w_EE in the small FI regime (and it's somewhat conspicuous why w_EE/w_c is not shown). All this leaves it unclear what the role of w_EE is in sampling, and whether it really implements the kind of regularization the paper proclaims. This is a problem, because this seems to be the only function suggested for recurrent excitation in the circuit, and otherwise a purely feedforward circuit seems best (see e.g. Fig.2E).\n\n4. It is unclear what's the advantage of the \"non-equilibrium\" \"annealing\" strategy of the circuit at stimulus onset, shown in Fig.2I-K. First, large differences in the steady state bump heights (at different values of w_EE; Fig.2J) translate to minimal differences in sampling speed gains (Fig.2K). In fact, if anything, it seems that the case when the bump height barely grows at stimulus onset (w_EE/w_c=0) results in slightly faster sampling. I couldn't find what U_E the \"equilibrium NG\" sampler used, but I suspect it used the large value corresponding to the stimulus being on. What about an \"equilibrium NG\" sampler that always uses the low U_E corresponding to the stimulus being off? If I am right, there is no special advantage to the \"non-equilibrium\" \"annealing\" the authors focus on — there is simply an advantage of using low U_E as long as possible. \n\nI also found the terminology here somewhat fanciful (in that it made the effects that are described here sound more fancy than they really are). The term \"annealing\", in the context of sampling, typically refers to procedures that make the sampler sample progressively different target distributions (or an optimizer to optimize progressively different objective functions, as in simulated annealing). This is not the case here — the sampler samples from the same posterior distribution throughout the period of \"annealing\", just with different time constants. The term \"non-equilibrium\" dynamics usually refers to autonomous dynamical systems. This is also not the case here — the autonomous dynamics of the system here itself changes during the \"non-equilibrium\" phase of the experiment because the inputs to the system change. \n\n5. The bivariate setup is a little confusing. The main text makes it sound as if the combined circuit sampled from the joint posterior, and a paragraph and Fig.A2A-B is devoted to explaining the properties of the correlated prior the circuit implements. In turn, such a correlated prior is interesting because it also makes the posterior correlated (especially because the likelihoods are independent). However, then Fig.3 (and its caption) emphasizes how each module samples the corresponding marginal posterior. Indeed, I found no demonstration that the joint posterior is correctly sampled (unless the precisions shown in Fig.A2D are somehow related to joint precisions). But then how is this more useful than two decoupled circuits, already covered by the preceding sections?\n\nA more minor point is that Eq.13 suggest that the prior implemented by cross-module weights is a bivariate Gaussian. But then the text states that it stores \"an associative (correlational) stimulus prior with each marginal uniform\" — it would be useful to point out that this is consistent with a bivariate Gaussian as a degenerate case. \n\n6. Taking together the concerns above, there is something slightly odd about what the proposed neural circuit achieves computationally in the end: it samples from a likelihood that is already parametrically represented (in a very easily readable form) in the input. If my interpretation of the bivariate case is correct (point 5), there is no combination with a nonuniform prior, or with some other likelihood. If this is the case, then there seems to be no additional benefit to the operation of this circuit compared to what its input already provides. \n\n7. The role of attractors in the circuit dynamics is confusing. The starting point for all the mathematical analysis is based on the existence of attractor states in the network. However, if these are truly attractor states, then they persist even in the absence of a stimulus — this is a problem because if the stimulus disappears (or more generally, changes), we would not want the circuit to maintain its representation of the posterior that was based on the (previous) stimulus. Indeed, based on Table 1, it seems that recurrent weights were chosen to be smaller than w_c (0.8w_c), which is \"the smallest value that allows the network to maintain persistent activity even when there is no feedforward input\" — i.e. the network is *not* in the parameter regime in which it has attractors. \n\nSo, it is unclear, whether the presented networks do or do not have attractor states, and in the former case, how they can usefully perform sampling, and in the latter case, how the mathematical derivations serve their understanding.\n\n***\n\nMinor issues:\n\nl.59: \"linear dynamics of Langevin and Hamiltonian samplings [...] used in machine learning (ML) research\" Is it really true that parctical sampling algorithms used in ML research use linear dynamics? \nl.205: \"We will leave the subjective prior p(s) unspecific\": \"p(s)\" → \"p(z)\"\nl.215: \"is resulted from\" → \"results from\"\nl.258: \"we investigate the how the\" → \"we investigate how the\"\nl.260: \"It is because\" What is because?\nl.269: \"the circuit with fixed weights flexibly sampling likelihoods\": \"sampling → \"samples\"\nl.308: \"sampling step size will gradually decreases\": \"decreases\" → \"decrease\"\nl.350: \"denots\" → \"denotes\"\nl.357: \"To ease of understanding\": \"To\" → \"For\"\nl.410: \"satisfys\" → \"satisfies\"\nl.465: \"flexibly\" → \"flexibility\"\n\nFig.2: the orange-red colors are barely distinguishable\nFig.2F: what's the black dashed line?\nFig.2H: what are the solid vs dashed lines?\nFig.A2: what is Lambda_s (I couldn't find its definition)?"}, "questions": {"value": "1. Is there any biological evidence to back up the modeling choices mentioned in point 1 above?\n\n2. Is there any evidence that the network is able to sample from non-Gaussian posteriors, and in cases in which the likelihood is not given by a Gaussian-Poisson population code?\n\n3. Is there any robust evidence (beyond what is currently shown in Fig.2C) that non-zero w_EE has a useful functional role in the circuit?\n\n4. Is there any evidence that \"annealing\", rather than just a generally low value for U_E, is specifically useful for sampling after stimulus onset?\n\n5. Is there any evidence that the samples produced by the network faithfully represent the correlations under a joint posterior distributions? If so, how can they usefully perform sampling, and if not, how do the mathematical derivations serve their understanding?\n\n\n6. Is there a way to demonstrate that the network represents a posterior which cannot be trivially decoded already from its input?\n\n7. Are there attractors in the intrinsic dynamics of the network?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "92cCvCeBvS", "forum": "0XjUSPBVfW", "replyto": "0XjUSPBVfW", "signatures": ["ICLR.cc/2026/Conference/Submission14055/Reviewer_fZPj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14055/Reviewer_fZPj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762645710310, "cdate": 1762645710310, "tmdate": 1762924538591, "mdate": 1762924538591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}