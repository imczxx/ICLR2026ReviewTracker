{"id": "KFjtRqVnKH", "number": 14124, "cdate": 1758228796299, "mdate": 1759897388702, "content": {"title": "SafeDialBench: A Fine-Grained Safety Evaluation Benchmark for Large Language Models in Multi-Turn Dialogues with Diverse Jailbreak Attacks", "abstract": "With the rapid advancement of Large Language Models (LLMs), the safety of LLMs has been a critical concern requiring precise assessment. Current benchmarks primarily concentrate on single-turn dialogues or a single jailbreak attack method to assess the safety. Additionally, these benchmarks have not taken into account the LLM's capability to identify and handle unsafe information in detail. To address these issues, we propose a fine-grained benchmark SafeDialBench for evaluating the safety of LLMs across various jailbreak attacks in multi-turn dialogues. Specifically, we design a two-tier hierarchical safety taxonomy that considers 6 safety dimensions and generates more than 4000 multi-turn dialogues in both Chinese and English under 22 dialogue scenarios. We employ 7 jailbreak attack strategies, such as reference attack and purpose reverse, to enhance the dataset quality for dialogue generation. Notably, we construct an innovative auto assessment framework of LLMs, measuring capabilities in detecting, and handling unsafe information and maintaining consistency when facing jailbreak attacks. Experimental results across 19 LLMs reveal that Yi-34B-Chat and GLM4-9B-Chat demonstrate superior safety performance, while Llama3.1-8B-Instruct and o3-mini exhibit safety vulnerabilities.", "tldr": "We construct a fine-grained benchmark featuring a two-tier hierarchical taxonomy across 6 distinct dimensions. Using 7 jailbreak attack methods, we generate over 4,000 multi-turn dialogues across 22 different scenarios in both English and Chinese.", "keywords": ["Large language models", "multi-turn", "safety", "benchmark", "jailbreak"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/98961e1832e95be32e24f8fa4ecd9a1e9a02dccd.pdf", "supplementary_material": "/attachment/308d5334f0a5e8afb36af7bccb52e155c59886f7.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces SafeDialBench, a fine-grained, bilingual (EN/ZH) benchmark to evaluate LLM safety in multi-turn dialogues under diverse jailbreak attacks. It proposes a two-tier safety taxonomy spanning six dimensions (Fairness, Legality, Morality, Aggression, Ethics, Privacy), constructs 4,053 dialogues (3–10 turns) across 22 scenarios, and applies 7 attack strategies (e.g., reference attack, purpose reverse, role play, fallacy attack). The authors pair this dataset with an automatic, capability-oriented evaluation framework that scores models’ ability to (i) identify unsafe risk, (ii) handle unsafe information, and (iii) maintain consistency across turns. Experiments on 19 LLMs show varied safety profiles (e.g., Yi-34B-Chat, MoonShot-v1, and ChatGPT-4o performing strongly; Llama3.1-8B-Instruct and o3-mini weaker in places)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "+ Unlike many prior benchmarks limited to single-turn or single-attack setups, SafeDialBench covers 6 safety dimensions, 22 scenarios, and 7 jailbreak methods, with detailed comparison to prior work.\n\n+ The three-ability scheme reflects realistic safety failures in dialogues; scoring focuses on the last turn with the minimum-score rule to reflect that one bad response can compromise safety."}, "weaknesses": {"value": "- Although human checks exist, auto-evaluation relies on LLM judges (GPT-3.5 Turbo, Qwen-72B). This may encode judge-model biases into scores and induce evaluation gaming. More detail on judge calibration, blind settings, and robustness to prompt phrasing would strengthen claims.\n\n- Seven jailbreak families are valuable, yet the authors also acknowledge a need to incorporate additional methods as attacks evolve (e.g., tool-use leakage, retrieval-augmented exploits).\n\n- The taxonomy distinguishes these, but their operational boundaries and independence could be clearer; some examples (e.g., abuse, self-harm) might overlap both. The authors reference prior works but further disambiguation would aid deployers."}, "questions": {"value": "1. What are the differences between single-turn and multi-turn attacks? I understand the basic differences, but any technical differences between these two?\n\n2. Any theoretical discovery for jailbreaking attack and defense through the benchmark study?\n\n3. The phenomenon of jailbreaking attacks is well-known; then, what is the contribution to the research of LLM safety through the benchmark study?\n\n4. How sensitive are scores to the exact evaluation prompt or to small wording changes? Did you run ablations with alternative rubrics or judge models beyond GPT-3.5/Qwen-72B?\n\n5. Since evaluation emphasizes the last response, how do you detect earlier unsafe turns that are later “washed out” by a final refusal? Would turn-level maxima (worst-turn) or cumulative penalties change conclusions?\n\n6. I am confused to the labeling process. I understand human experts are involved into the multiturn interaction generation process, but how these ground-turth evaluation scores are obtained like in Fig. 8? (I can understand you can benchmark other LLM to output these scores) I did not find a clear description in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mjlYUSeyAW", "forum": "KFjtRqVnKH", "replyto": "KFjtRqVnKH", "signatures": ["ICLR.cc/2026/Conference/Submission14124/Reviewer_ppDF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14124/Reviewer_ppDF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675625876, "cdate": 1761675625876, "tmdate": 1762924590602, "mdate": 1762924590602, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SafeDialBench, a benchmark for testing LLM safety in multi-turn dialogues with diverse jailbreak attacks. It includes 4,000+ bilingual (EN/ZH) dialogues across 22 scenarios and evaluates models on three abilities — identifying, handling, and maintaining consistency against unsafe content."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It covers multi-turn, bilingual, multi-attack settings, which is more realistic than prior single-turn tests.\n2. The experiment is comprehensive, and the paper is easy to follow."}, "weaknesses": {"value": "1. The paper relies mostly on works from 2022 –  2024, with few citations from 2025 despite the rapid emergence of new jailbreak, safety-alignment, and multi-turn evaluation studies.\n2. While the benchmark reveals valuable diagnostic insights, the paper offers no concrete direction for mitigating multi-turn safety failures. It would be helpful to include conceptual or empirical suggestions.\n3. The proposed “two-tier hierarchical safety taxonomy” is descriptive but not empirically justified. Several categories (e.g., Morality vs. Ethics) appear semantically overlapping.\n4. The results in Table 2 reveal very narrow score ranges (typically 6.6–8.3) across all models and dimensions, suggesting insufficient differentiation power in the scoring rubric. The current 1–10 scale, combined with the “minimum-score-taking” rule, produces clustered averages with little variance. The evaluation could be strengthened to increase contrast and analytical depth."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A9LRdyAyBX", "forum": "KFjtRqVnKH", "replyto": "KFjtRqVnKH", "signatures": ["ICLR.cc/2026/Conference/Submission14124/Reviewer_M8wx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14124/Reviewer_M8wx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896267046, "cdate": 1761896267046, "tmdate": 1762924589704, "mdate": 1762924589704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark of multi-turn LLM safety with 6 safety dimensions and 4k dialogues under 22 scenarios. 7 attack methods are evaluated based on the victim models of 19 LLMs. Many experimental results are shown for analysis, and some findings are shown in the experiments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The benchmark is comprehensive in terms of categories and scenarios. Also, the experiments involve many LLM models, which are impressive. The presentation is clear, and the paper is well written."}, "weaknesses": {"value": "- My first concern is the human involvement in the dialogue question design. Since human design is not efficient, many multi-turn jailbreak attack methods [1,2] have been developed to automate the generation of questions. So, why not use the existing automatic multi-turn jailbreak queries as initial questions?\n\n- Besides, in the benchmarked attack methods, many of them are newly developed, and I wonder how they are effective compared to existing multi-turn jailbreak attack methods [1,2].\n\n- The evaluation adopts the lowest score from multiple judges, which may not be problematic for consistent criteria if the judges are not consistent. Some empirical justifications are needed. Also, it is not clear why the threshold of score 7 is adopted instead of other values, because the scores seem to be still high in Fig. 6, even though for many turns.\n\n- As a benchmark paper, it is expected to compare the results of existing defense methods [3,4] as the baselines for the benchmark, to show if existing defense methods easily defend the attack benchmark.\n\n---\n\n[1] Ren et al. Derail Yourself: Multi-turn LLM Jailbreak Attack through self-discovered clues, 2024\n\n[2] Russinovich et al. Great, Now Write an Article About That: The Crescendo Multi-Turn LLM Jailbreak Attack, 2024\n\n[3] Lu et al. X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability, 2025\n\n[4] Hu et al. Steering Dialogue Dynamics for Robustness against Multi-turn Jailbreaking Attacks, 2025"}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MNemwR94nq", "forum": "KFjtRqVnKH", "replyto": "KFjtRqVnKH", "signatures": ["ICLR.cc/2026/Conference/Submission14124/Reviewer_PrRz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14124/Reviewer_PrRz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971370317, "cdate": 1761971370317, "tmdate": 1762924589062, "mdate": 1762924589062, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, a new benchmark called SafeDialBench is introduced for evaluating the safety of Large Language Models (LLMs) in the context of multi-turn dialogues. The authors argue that existing benchmarks are insufficient, as they often focus on single-turn interactions, are monolingual, or rely on a single jailbreak attack method. The proposed benchmark addresses these limitations by providing a comprehensive two-tier safety taxonomy(with six dimensions) and a fine-grained evaluation framework. Many of the current LLMs are included in the evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. In general, the paper is well written with good structure and is easy to follow.\n\n2. The authors present the critical gaps of current benchmarks, which are single-turn focus with limited jailbreaks and propose a valid benchmark to address them.\n\n3. The proposed benchmark makes use of seven different jailbreak strategies (scene construct, purpose reverse, role play, topic change, reference attack, fallacy attack, probing questions), addresses a range of realistic red-teaming strategies and importantly focuses on multi-turn escalation, which better models real-world exploitation.\n\n4. It is good to have human-in-the-loop generation with quality control."}, "weaknesses": {"value": "1. The 7 attack methods, while diverse, are known strategies. The field of jailbreaking is adversarial and evolves rapidly. There's a risk that models will quickly be \"patched\" against these specific 7 attacks, or even over-fit to this benchmark. \n\n2. The paper treats a fixed response scoring as “successfully attacked” (ASR). The rationale for choosing this score as the cutoff should be defended empirically with sensitivity analysis or motivated by human agreement patterns.\n\n3. The minimum-score metric is intuitive but seems harsh. How does it correlate with human judgments of overall dialogue safety?"}, "questions": {"value": "Please address the questions mentioned in the weaknesses, and also check the questions below.\n\n1. The authors mention \"reasoning models\" like DeepSeek-R1 with an excellent case study on how a model's \"chain-of-thought\" can be manipulated. Do you believe this issue is a side-effect of being tuned for helpfulness and complex reasoning, or is it simply a failure of safety alignment that is distinct from the reasoning capability?\n\n2. For the provided dialogues, how many were generated starting from each assistant model (GPT-4 vs. Doubao vs. ChatGLM)? Is performance or difficulty correlated with the assistant used during generation? Please provide some more details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qdPm3LMUla", "forum": "KFjtRqVnKH", "replyto": "KFjtRqVnKH", "signatures": ["ICLR.cc/2026/Conference/Submission14124/Reviewer_Xaiv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14124/Reviewer_Xaiv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14124/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762206155746, "cdate": 1762206155746, "tmdate": 1762924588472, "mdate": 1762924588472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}