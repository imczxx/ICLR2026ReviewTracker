{"id": "XklgQBPw1x", "number": 18776, "cdate": 1758290773942, "mdate": 1759897081824, "content": {"title": "Latent Guided Sampling for Combinatorial Optimization", "abstract": "Combinatorial Optimization problems are widespread in domains such as logistics, manufacturing, and drug discovery, yet their NP-hard nature makes them computationally challenging. Recent Neural Combinatorial Optimization (NCO) methods leverage deep learning to learn solution strategies, trained via Supervised or Reinforcement Learning. While promising, these approaches often rely on task-specific augmentations, perform poorly on out-of-distribution instances, and lack robust inference mechanisms. Moreover, existing latent space models either require labeled data or rely on pre-trained policies. In this work, we propose LGS-Net, a novel latent space model that conditions on problem instances, and introduce an efficient inference method, Latent Guided Sampling (LGS), based on Markov Chain Monte Carlo and Stochastic Approximation. We show that the iterations of our method form a time-inhomogeneous Markov Chain and provide rigorous theoretical convergence guarantees. Empirical results on benchmark routing tasks show that our method achieves state-of-the-art performance among learning-based NCO baselines.", "tldr": "We introduce a novel latent space model for Combinatorial Optimization and propose an efficient inference method based on Markov Chain Monte Carlo and Stochastic Approximation, with theoretical convergence guarantees.", "keywords": ["Probabilistic Inference", "Combinatorial Optimization", "Markov Chain Monte Carlo", "Vehicle Routing Problem"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b567627b6973a913570ab3275c7151928d48a876.pdf", "supplementary_material": "/attachment/66299dbd18f0e77ceb51c050c4444ef7d7d9c0aa.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes LGS-Net, a latent-space model for Neural Combinatorial Optimization (NCO). The approach trains an encoder to map a problem instance into a distribution for a latent variable.  Based on a sample of this distribution, the decoder generates a distribution over the solution space which then is sampled to obtain solutions. At inference time, the authors introduce Latent Guided Sampling (LGS) — an MCMC-based method augmented with stochastic approximation updates to the decoder parameters, allowing test-time adaptation. Theoretical analysis establishes convergence guarantees for both fixed and adaptive parameter settings. Experiments on TSP and CVRP benchmarks show that LGS-Net achieves state-of-the-art or near-optimal performance, sometimes improving upon strong baselines such as COMPASS and EAS by small margins."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "- The paper addresses recognized limitations of prior NCO approaches (e.g., data requirements, lack of inference-time optimization) and positions the work within a clear conceptual framework.\n\n- Principled combination of latent modeling and RL. The unification of stochastic approximation with MCMC inference in a learned latent space is elegant and theoretically grounded.\n\n- Theoretical soundness. The convergence proofs are rigorous and build on established MCMC theory, adapted to a time-inhomogeneous setting.\n\n- Comprehensive experimental comparison. The authors benchmark against numerous recent NCO baselines and demonstrate consistent improvements, confirming the competitiveness of the approach."}, "weaknesses": {"value": "- Unclear role of the concept of the latent space: During inference this method employs a local search based on MCMC updates and weight updates of the decoder. This iterative search evaluates the solution quality of intermediate samples in solution space. Hence this method can hardly be considered a latent CO method since it does not learn to solve CO problems via latent space dynamics but relies on generating samples in solution space and evaluating the cost function for them. More generally, the role of the stochastic latent variable is not clear.\n\n- Incremental novelty. The method extends earlier latent-space NCO works such as CVAE-Opt and COMPASS. The architectural and training components are similar, with the main novelty residing in the inference mechanism.\n\n- Limited empirical diversity. All experiments are on Euclidean routing problems (TSP/CVRP ≤ 150 nodes). The approach’s generality to other combinatorial settings (e.g., knapsack, scheduling) remains untested.\n\n- Small empirical margins. Performance gains over prior SOTA are modest (typically < 0.2 % gap). It is unclear whether these differences are statistically significant and practically meaningful given the added complexity of MCMC + SA inference."}, "questions": {"value": "- Why is the latent variable needed?\n- Why not use only a decoder model that takes as input the problem instance x instead of the latent z(x)?\n- Why is the latent variable stochastic and why couldn't a deterministic representation work as well?\n- Can you show the benefit of having an encoder and the stochastic variable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7ARKUSIoo2", "forum": "XklgQBPw1x", "replyto": "XklgQBPw1x", "signatures": ["ICLR.cc/2026/Conference/Submission18776/Reviewer_TFKR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18776/Reviewer_TFKR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823896794, "cdate": 1761823896794, "tmdate": 1762999990670, "mdate": 1762999990670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LGS-Net, a latent-space neural model for combinatorial optimization (e.g., TSP, CVRP). Similar to [Chalumeau et al., 2023], it learns instance-conditioned latent representations using reinforcement learning, yet performs inference through a new scheme based on MCMC called Latent Guided Sampling (LGS).\n\nContributions:\n- A new latent-space NCO model (LGS-Net) that conditions on problem instances.\n- A provably convergent inference method (LGS) integrating MCMC and stochastic approximation (SA).\n- SOTA results on TSP and CVRP benchmarks (in distribution and slightly out of distribution), outperforming prior methods like POMO, COMPASS, and EAS, with strong generalization to unseen problem sizes."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is overall well-written, gives good credit to related works, and performs decent experiments and ablations.\n- Strong theoretical grounding regarding convergence.\n- Standard experiments on TSP and CVRP: n in [100, 125, 150].\n- SOTA results on these benchmarks, even beaten the solver for CVRP 100-125."}, "weaknesses": {"value": "The paper and method suffer from a few weaknesses.\n- Additional complexity of the inference scheme: MCMC and SA must bring a significant overhead.\n- Limited experimental scope: I assume this would transfer well to other combinatorial problems like job-shop scheduling or graph problems, but the paper only tests on TSP and CVRP."}, "questions": {"value": "1. How would you think about applying such a training and inference scheme outside of this NCO setup?\n\n2. How realistic with respect to real problems are the assumptions made in the convergence analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X9JhjPLlCT", "forum": "XklgQBPw1x", "replyto": "XklgQBPw1x", "signatures": ["ICLR.cc/2026/Conference/Submission18776/Reviewer_BuX2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18776/Reviewer_BuX2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949473274, "cdate": 1761949473274, "tmdate": 1762999990329, "mdate": 1762999990329, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new inference strategy for neural solvers applied to combinatorial optimization, with a primary focus on routing problems but applicability beyond them. The approach is based on latent-space models coupled with a subsequent latent-space search. It includes a training phase that anticipates the inference-time search budget and presents a coupled training–inference procedure. The method establishes close connections with prior inference strategies in the literature, namely EAS, COMPASS, and CVAE-Opt. The paper provides both mathematical guarantees and empirical results on two widely studied combinatorial optimization problems (TSP & CVRP) evaluated in- and out-of-distribution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Significance: The use of inference-time strategies for combinatorial optimization (CO) problems is highly relevant. The proposed method achieves state-of-the-art performance with statistically significant improvements on reference benchmarks, evaluated on two well-studied problems (TSP and CVRP), both in- and out-of-distribution. I appreciate that the paper provides not only SOTA results but also mathematical justification for the proposed approach.\n\nOriginality: Most underlying concepts already exist, and the general idea of latent-space search is not new (which the authors acknowledge clearly), but the combination of latent-space modeling, MCMC sampling, and gradient-based updates is novel and supported by solid mathematical analysis. While there is no major paradigm shift, the proposed approach presents an interesting and meaningful contribution.\n\nClarity: The paper makes a commendable effort to explain the method, and it presents the mathematical components and proofs clearly. The main ideas are easy to follow, and the inclusion of figures, algorithms, and plots effectively supports the reader’s understanding.\n\nQuality: The paper engages well with the existing literature, being transparent about its close connections to COMPASS, EAS, and CVAE-Opt. It presents the mathematical aspects rigorously and includes results and extended materials in the appendix. Overall, the paper demonstrates good quality, though there remains room for improvement (see weaknesses)."}, "weaknesses": {"value": "**W1. Weak motivation and unclear justification of contributions.**\n\nThe main motivation of the work remains somewhat weak. Lines 51–54 and 57–60 present the limitations of prior work (EAS and COMPASS) that LGS is supposed to address (lines 60–62). This section is crucial, as it motivates the entire paper, yet it is not fully convincing.\n(i) The authors claim that EAS “fine-tunes” the policy, which poses computational challenges, but LGS also performs fine-tuning.\n (ii) The statement that “COMPASS enforces independence between the problem instance and the latent space structure” would benefit from clarification, specifically, how limiting this assumption truly is, and why relaxing it is expected to yield substantial performance gains. \n\nMoreover, the results and ablation studies do not provide evidence that the “instance dependence” introduced in LGS is responsible for the observed performance improvements. The only ablation reported in the main paper concerns the inference-time search/sampling component. One could therefore wonder whether applying the LGS sampling and parameter-update mechanism to a COMPASS checkpoint would suffice. Overall, the paper does not convincingly establish that the proposed modifications address the stated weaknesses of prior methods. Clearer intuitions about why the proposed approach constitutes a conceptual improvement, and corresponding empirical validation through targeted ablations, would strengthen the work considerably.\n\nAnother motivation mentioned in the introduction is the removal of reliance on augmentation tricks. However, it is not clear how LGS achieves this, as other methods such as COMPASS and EAS do not fundamentally rely on augmentation either. They may use it, but they function well without it. The authors should clarify what makes LGS different in this regard; if there is no clear distinction, this point should not be used as a central motivation.\n\n**W2. Missing ablation studies.**\n\nRelated to the previous point, some important ablations are missing. The only ablation presented concerns the sampling method, whereas one of the main motivations, conditioning on the instance, is not ablated. Ideally, this should be tested explicitly. Alternatively, the authors could revise their narrative to avoid presenting this aspect as a central feature of the method without empirical support.\n\n**W3. Questionable timing results.**\n\nThe timing results reported in Tables 1, 4, and 5 appear inconsistent, which raises concerns about the validity of the experimental study. The times reported for POMO, EAS, and COMPASS do not align with expectations, and the reported runtime for LGS is also surprising.\n\nFirst, the times for POMO and COMPASS should be roughly identical, as both share the same bottlenecks, the forward pass and environment rollouts. Their only difference is the CMA-ES update, which is negligible (see the original COMPASS paper). Therefore, one should expect similar runtimes across the benchmark, which is not the case (e.g., 10M vs. 20M on TSP100; 1h30 vs. 40M on TSP150).\n\nSecond, the reported time comparison between POMO and the other methods lacks coherence, as POMO is sometimes faster and sometimes slower, although it should consistently be equal or faster.\n\nThird, EAS and COMPASS are reported to take the same time, which is implausible, since EAS involves backpropagation steps that should make it slower.\n\nFinally, LGS-Net is reported to have the same runtime as COMPASS, even though it includes regular parameter updates. It should therefore be slower (perhaps only slightly if updates are infrequent), but this needs discussion if the times are indeed identical.\n\nOverall, I am confident that there are issues with the reported timings. The authors should carefully verify and correct these results, and provide more detailed information on the time performance of their algorithm. Beyond hindering fair comparison between COMPASS and LGS-Net, these inconsistencies undermine confidence in the experimental section as a whole.\n\n**W4. Clarity and presentation.**\n\nThe clarity of the paper could be improved. Figure 1 could include more detail to illustrate key components. It is somewhat confusing how the encoding interacts with the existing encoding that AM and POMO architectures are already using; the authors should be explicit about what differs in LGS, and how the latent space relates (or not) to the original embedding space. Additionally, certain elements, such as the “proposal distribution” (used in Algorithm 1 but not discussed in Section 4), should be better introduced.\n\nMinor suggestions:\n- “To learn solution strategies” (line 14) sounds awkward.\n- It is acceptable to use “NCO” without specifying “learning-based NCO,” as the term already implies learning; this would simplify some sentences (e.g., lines 24 and 39).\n- Line 52: stating that EAS is a SOTA search method is inaccurate, as it is outperformed on several benchmarks by COMPASS, PolyNet, and MEMENTO. The sentence should be softened to “among the leading methods for search-based RL.” MEMENTO [1] (NeurIPS 2025, Spotlight) could also be added to the related work.\n- Line 62: “rely on the augmentation trick” is misleading, most methods are agnostic to it, even if some experiments make use of it.\n- Line 113: COMPASS does not learn the latent space; it uses a fixed prior, and the policy is trained to exploit it effectively.\n- Line 116: “removes the need for a pre-trained policy” should be clarified, why does this hold?\n- Line 251. Small suggestion to swap theta and phi to follow the order in which encoder and decoder are mentioned.\n- Line 382. I believe the usual dataset used in the NCO literature for TSP100 has 10000 instances, and TSP125 and TSP150 have 1000 instances. \n\n[1] Memory-Enhanced Neural Solvers for Routing Problems, Neurips 2025"}, "questions": {"value": "It would be valuable to provide stronger motivation for the design choices introduced in the proposed method. For instance, why was the POPPY loss from COMPASS removed? Why was a latent-space encoder introduced? And why are all decoder parameters updated if the learned latent space is already expected to capture much of the variability? I can anticipate some of the rationale behind these decisions, but I would really appreciate hearing the authors’ explicit perspective. Clarifying these points, ideally supported by additional ablation studies, could substantially strengthen the manuscript.\n\nIn Section 4.1, it is also worth noting that the baseline AM/POMO architecture already includes an encoder. It would therefore help to clarify what the new encoder adds, and what type of information is expected to be captured in the latent space that is not already represented in the existing embedding.\n\nIt would also be helpful to report the dimensionality of the latent space used in the experiments.\n\nRegarding the reported timings: could the authors confirm whether these values are accurate? Given the inconsistencies mentioned earlier, I am concerned about the validity of the experimental results.\n\nFinally, on line 116, the paper states that LGS “removes the need for a pre-trained policy.” Could the authors elaborate on why this is the case?\n\nOverall, I find the work promising and I am genuinely keen to support it. However, several concerns remain that need to be addressed to lean toward acceptance. I sincerely hope that the authors will be able to provide clarifications and additional results during the rebuttal phase to help advance this discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "-"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gzdrbN7WeH", "forum": "XklgQBPw1x", "replyto": "XklgQBPw1x", "signatures": ["ICLR.cc/2026/Conference/Submission18776/Reviewer_eBRY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18776/Reviewer_eBRY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978689234, "cdate": 1761978689234, "tmdate": 1762999990698, "mdate": 1762999990698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LGS-Net is an RL-based latent variable neural combinatorial optimization model that requires no labels. It uses instance-conditioned latent embeddings and trains via a cost-weighted, entropy-regularized objective function. The key innovation is the Latent Guided Sampling (LGS) inference procedure, which updates decoder parameters through stochastic approximation while running interacting MCMC chains in latent space. It provides convergence guarantees for both fixed and adaptive parameters. The model achieves learning-based SOTA on TSP and CVRP benchmarks, and even outperforms LKH3 on some CVRP settings."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The instance-conditioned latent model eliminates the need for pre-computed solutions or pre-trained policies unlike CVAE-Opt and COMPASS, and demonstrates clear performance improvements. LGS inference combines interacting MCMC with real-time parameter updates, consistently outperforming all alternatives including DE, CMA-ES, active search, and gradient-based finetuning. The convergence proof for time-inhomogeneous Markov chains is a non-trivial theoretical contribution for this problem class. The experiments include multiple datasets, comparisons with and without augmentation, runtime reports, and detailed hyperparameters, demonstrating excellent reproducibility."}, "weaknesses": {"value": "The adaptive chain convergence relies on Assumption 4, but the paper only provides high-level justification. There is a lack of sufficient conditions linking Algorithm 1's specific step-size choices and gradient-variance conditions, creating a gap between theory and implementation. \nThe empirical study is limited to Euclidean routing (TSP/CVRP), restricting the demonstration of generality. Experiments on non-Euclidean problems or other combinatorial domains would help establish the versatility of the latent formulation. \nAlthough negative gaps versus LKH3 on CVRP are reported, verification via exact solvers or optimality certificates is absent, making the evaluation protocol ambiguous."}, "questions": {"value": "What is the sensitivity to the choice of latent dimension d_z and bounded diameter R? The ablations only vary K and update schedules, not d_z or R."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YG6tPQQIC4", "forum": "XklgQBPw1x", "replyto": "XklgQBPw1x", "signatures": ["ICLR.cc/2026/Conference/Submission18776/Reviewer_4pVj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18776/Reviewer_4pVj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002390558, "cdate": 1762002390558, "tmdate": 1762999990664, "mdate": 1762999990664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}