{"id": "kboK7ad4Cn", "number": 20516, "cdate": 1758306960593, "mdate": 1759896973755, "content": {"title": "LoRA is All You Need for Safety Alignment of Reasoning LLMs", "abstract": "Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex problems that were previously out of reach.  To ensure LLMs do not assist with harmful requests, safety alignment fine-tuning is necessary in the post-training phase. However, safety alignment fine-tuning has recently been shown to significantly degrade reasoning abilities, a phenomenon known as the ``Safety Tax”. In this work, we show that using LoRA for SFT on refusal datasets effectively aligns the model for safety without harming its reasoning capabilities. This is because restricting the safety weight updates to a low-rank space minimizes the interference with the reasoning weights. Our extensive experiments across four benchmarks covering math, science, and coding show that this approach produces highly safe LLMs—with safety levels comparable to full-model fine-tuning—without compromising their reasoning abilities. Our ablation studies further identify three key factors in LoRA: (1) rank-$1$ updates are sufficient to achieve the best reasoning and safety performance, (2) the up projection layers are the most critical modules, with LoRA applied to them alone achieving even better results, and (3) middle layers are more effective than early or late layers. Together, these findings show that strong safety and reasoning can be achieved at minimal computational cost when updates are applied in the right places. Additionally, we observe that LoRA induces weight updates with smaller overlap with the initial weights compared to full-model fine-tuning. Finally, while our attempts to further reduce this overlap yield only modest improvements on some tasks, they highlight the potential of developing methods that more reliably optimize the reasoning–safety tradeoff.", "tldr": "", "keywords": ["LLM", "safety alignment", "reasoning", "post-training", "finetuning", "SFT", "LoRA"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c74cc04288c2c6ed2aafbdbdce2dd8ece33bcd8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates whether the Safety Tax can be mitigated using only LoRA fine-tuning. Through extensive ablations on rank, module, and layer configurations, the authors show that low-rank LoRA updates can preserve reasoning performance while achieving strong safety alignment. They further analyze the geometric structure of LoRA updates, suggesting that their low alignment with reasoning weights explains the reduced interference and improved reasoning–safety balance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Demonstrates that LoRA-only fine-tuning can mitigate the Safety Tax while preserving reasoning ability — an interesting and practical finding.\n- Well-designed ablations reveal key factors (rank = 1, MLP up-projection, middle layers) that contribute most to the reasoning–safety trade-off.\n- Provides a geometric perspective on why LoRA interferes less with reasoning, through alignment and subspace analyses."}, "weaknesses": {"value": "- The paper makes a strong claim that “LoRA is all you need” to address the safety–reasoning trade-off. While the presented results are intriguing, the current experimental scope is insufficient to substantiate this claim. A wider range of backbones and model sizes should be evaluated to demonstrate consistency across architectures and scales (R1-1.5 ~ R1-32B, s1, Qwen3, etc).\n\n- Moreover, prior analyses (Jain et al., 2024; Wei et al., 2024) about low-rank safety directions generalize to general LLMs, not just reasoning-centric LRMs. Therefore, the authors’ argument should extend beyond LRMs to general LLMs. Since the Safety Tax is not unique to reasoning models but also arises in other capabilities, the proposed hypothesis should be validated across diverse tasks beyond reasoning.\n\n- In Figure 7, the alignment difference between full fine-tuning and LoRA appears marginal, which weakens the causal claim.\n\n- The trade-off plots contain multiple points per configuration, yet their meanings are under-explained (e.g., epoch checkpoints, random seeds, or hyperparameter variations). The large variance across these points also raises concerns about training stability and reproducibility.\n\n- Finally, the evaluation would be more convincing if it included adaptive or adversarial safety tests—such as obfuscation, paraphrased jailbreaks, or multi-turn exploit prompts—that better reflect real-world attack surfaces and robustness challenges.\n\n[1] Jain, Samyak, et al. \"What makes and breaks safety fine-tuning? a mechanistic study.\" Advances in Neural Information Processing Systems 37 (2024): 93406-93478.\n\n[2] Wei, Boyi, et al. \"Assessing the brittleness of safety alignment via pruning and low-rank modifications.\" arXiv preprint arXiv:2402.05162 (2024)."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "O5XChlgYXl", "forum": "kboK7ad4Cn", "replyto": "kboK7ad4Cn", "signatures": ["ICLR.cc/2026/Conference/Submission20516/Reviewer_DsR5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20516/Reviewer_DsR5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900551710, "cdate": 1761900551710, "tmdate": 1762933939589, "mdate": 1762933939589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This  paper explores how Lora performs on safety alignment task and find that it can effective improve safety score without droping reasoning ability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) This empirical study is careful and fairly comprehensive. Results on multiple benchmarks are reported. \n\n(2) The findings that Lora can successfully avoid the trade-off between reasoning ability and model safety is interesting and useful.\n\n(3) This paper is well-structured and easy to follow."}, "weaknesses": {"value": "(1) The benchmarked models lack diversity — all reasoning models are derived from DeepSeek. It remains unclear whether the findings generalize to other reasoning models or architectures, such as GPT-OSS-20B or GPT-OSS-120B.\n\n(2) This paper lacks theoretical analysis explaining why the LoRA technique can mitigate the “safety tax” issue. A deeper investigation or theoretical justification would strengthen the claims.\n\n(3) The safety evaluation pipeline may have limitations. Safety is automatically assessed using Llama-Guard-3-8B, which could introduce bias. Incorporating multi-metric safety evaluations (e.g., jailbreak or red-teaming tests) would provide more comprehensive and convincing evidence."}, "questions": {"value": "(1) Can the main findings generalize to other reasoning models such as GPT-OSS-20B?\n\n(2) In Figure 3(b), why does r = 64 yield the lowest model safety performance? Is this a consistent phenomenon observed across different models? The paper explains that the “middle-rank” setting might be harder to optimize, but is there any empirical or theoretical evidence supporting this explanation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vxmOIDZnEk", "forum": "kboK7ad4Cn", "replyto": "kboK7ad4Cn", "signatures": ["ICLR.cc/2026/Conference/Submission20516/Reviewer_twiY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20516/Reviewer_twiY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987662457, "cdate": 1761987662457, "tmdate": 1762933939019, "mdate": 1762933939019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets \"Safety Tax\", which is preserving the model's reasoning capacity while aligning for safety. The paper utilizes LoRA on a refusal dataset to effectively align the model for safety while preserving the reasoning capacity. The authors show that using r=1 rank achieves the best reasoning-safety tradeoff."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper shows simple yet effective usage of LoRA in finetuning for safety alignment. \n- The results show that the LoRA-trained model is both safe and has high reasoning performance in different benchmarks."}, "weaknesses": {"value": "- The paper does not exhibit weight-level selectivity; instead, it adopts a more coarse-grained perspective, assuming that all parameters contribute collectively to the model’s reasoning capability. The selectivity applied is primarily at the layer or module level.\n\n- The paper lacks of theoretical ground for its claims and has an experimental approach.\n\n- The proposed post-hoc method doesn't improve the reasoning performance, but the authors also claim that the method needs more development.\n\n- To me, the paper needs a more professional tone. The narration should support the claims with references or experimental data, which are given in lines 184-187.\n\n- There is only one dataset used for the safety evaluation.\n\n- The paper focuses on only one family of models."}, "questions": {"value": "- Are the findings true for the different architectures?\n- Why is only one family of models selected?\n- Can authors compare their approach with other fine-tuning safety-alignment methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hCdnX4PbS8", "forum": "kboK7ad4Cn", "replyto": "kboK7ad4Cn", "signatures": ["ICLR.cc/2026/Conference/Submission20516/Reviewer_4WFt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20516/Reviewer_4WFt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145553012, "cdate": 1762145553012, "tmdate": 1762933938697, "mdate": 1762933938697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}