{"id": "rpnPKosij5", "number": 22066, "cdate": 1758325551069, "mdate": 1759896888209, "content": {"title": "Enhancing LLM Reasoning with Retrieval-Augmented Logical Chains and Test-Time Adaptation", "abstract": "Large language models (LLMs) excel on knowledge-intensive tasks but often fail on complex, multi-step reasoning requiring explicit inference and logical coherence. Retrieval-augmented generation (RAG) grounds outputs in external text, yet retrieved content is typically unstructured and misaligned with step-wise reasoning. We introduce LogicalChain, a framework that explicitly integrates structured logical chains—interpretable, step-by-step derivations linking context to conclusions. We build a large corpus of chains from domain-rich sources (e.g., expert guidelines, worked solutions) and train a contrastive retriever to fetch task-relevant inference paths. To close the instance–step misalignment at inference, we propose \\emph{TTT–RAG}, a test-time adaptation pipeline that fine-tunes the LLM on retrieved chains and documents \\emph{during} inference, tailoring behavior without updating global weights. Experiments show consistent gains across \\textbf{medical} and \\textbf{general multi-hop} domains: on MedQA, TTT–RAG lifts Qwen2.5–7B–Instruct from 53.8% to 70.1% (14B: 73.8%), and on MedMCQA to 62.1% (14B: 64.3%). Beyond the medical domain, TTT–RAG improves general multi-hop reasoning, reaching 45.1/42.8 (7B) and 48.5/44.6 (14B) on MultiHopQA/2Wiki, surpassing strong CoT baselines (e.g., rStar) and RAG systems (MedRAG, i-MedRAG). These results indicate that injecting structured reasoning pathways at test time yields scalable, interpretable, and state-of-the-art performance for complex reasoning tasks across domains", "tldr": "", "keywords": ["Reasoning", "RAG", "Medical", "QA"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3c672e8392ee56f86071e8704aa296a6f2d2b171.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LogicalChain with TTT-RAG, a framework designed to enhance large language model reasoning on complex multi-step tasks by combining structured logical chains with test-time adaptation. The authors construct a corpus of interpretable, step-by-step derivations from domain-rich sources and train a contrastive retriever to fetch task-relevant inference paths.  At test time, they propose TTT-RAG which first performs lightweight fine-tuning on retrieved chain exemplars to elicit a provisional reasoning chain, then retrieves step-aligned documents to verify and repair each reasoning step. The framework is evaluated primarily on medical QA benchmarks (MedQA, MedMCQA) and general multi-hop reasoning datasets (MultiHopQA, 2WikiMultiHopQA), showing substantial improvements over baselines including MedRAG, i-MedRAG, and rStar. On MedQA, TTT-RAG with Qwen2.5-7B achieves 70.1% accuracy (up from 53.8% baseline) and on 2WikiMultiHopQA reaches 42.8% accuracy for the 7B model and 44.6% for the 14B model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a genuine limitation in current RAG systems by highlighting that retrieval conditioned only on the global query often misses the specific failing step in multi-hop reasoning, which is a valid observation grounded in empirical evidence. The experimental scope is reasonably comprehensive, covering both medical domain-specific benchmarks and general multi-hop QA datasets, which demonstrates some generalizability of the approach."}, "weaknesses": {"value": "The related work section significantly underrepresents recent step-wise and iterative RAG methods that directly address the same problem space. Most critically, R3-RAG (2025) which uses reinforcement learning to teach LLMs step-by-step reasoning and retrieval achieves substantially better results on 2WikiMultiHopQA with Qwen2.5-7B reaching 62.3% accuracy compared to this work's 42.8%, and Llama-3.1-8B achieving 61.0% on the same benchmark. The paper also omits discussion of other highly relevant approaches including FAIR-RAG which introduces structured evidence assessment with iterative refinement cycles. Some other notable algorithms that were not considered as baseline include DRAGIN, which offers adaptable dynamic test time decision on retrieval and content."}, "questions": {"value": "1- How does TTT-RAG compare directly to R3-RAG on the same benchmark splits and evaluation metrics\n2-What are the wall-clock latency costs per query for TTT-RAG compared to R3-RAG and other step-wise iterative methods, not just token counts, since test-time fine-tuning introduces additional computational overhead that may not be reflected in token usage alone?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "63iG4OThc8", "forum": "rpnPKosij5", "replyto": "rpnPKosij5", "signatures": ["ICLR.cc/2026/Conference/Submission22066/Reviewer_72Si"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22066/Reviewer_72Si"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761778068171, "cdate": 1761778068171, "tmdate": 1762942045344, "mdate": 1762942045344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LogicalChain and TTT-RAG, a framework that combines retrieval-augmented generation with test-time training to improve LLM reasoning on complex, multi-step tasks. The key contributions are: (1) construction of a large corpus of structured logical chains from domain-rich sources, (2) a contrastive retriever trained to fetch task-relevant inference paths, and (3) TTT-RAG, which performs test-time adaptation by fine-tuning the LLM on retrieved chains during inference. The authors evaluate the proposed methods on medical benchmarks (MedQA, MedMCQA) and general multi-hop reasoning tasks (MultiHopQA, 2Wiki) and show substantial improvements over baselines like MedRAG and rStar."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies the challenge that LLMs keep reasoning chains latent, and RAG systems retrieve at the question level rather than the step level.\n2. The combination of structured logical chains + test-time training + step-indexed retrieval is novel.\n3. The paper shows consistent and substantial improvements across multiple medical and general-domain benchmarks."}, "weaknesses": {"value": "1. The comparison in the experiments might not be fair. The open-source reasoning models are all trained on math reasoning, while the benchmarks are in medical and general domain. The domain gap might be the reason why these models suffer.\n2. The method adds significant test-time cost. Test-time training on every query adds +32s per question (Table 3), which is not practical considering its performance gain.\n3. Since the authors already created a reasoning dataset, a better set of baselines should use these data to perform SFT, RLVR, or in-context learning, which incurs much lower online computation cost.\n4. There is no systematic evaluation of synthesized chain quality. What if GPT-4 hallucinates reasoning steps? How many of 2M+ chains were validated?"}, "questions": {"value": "1. How does TTT compare with traditional SFT, which adds nearly no inference-time costs? Would MedRAG/i-MedRAG improve if trained on the same generated logical chain corpus?\n2. How does the method compare with RLVR-trained methods or using in-context learning instead of TTT?\n2. MedQA should be a multiple-choice dataset. Why report ROUGE-1 and AtomicCov?\n3. What does Compare-Ex. mean in line 101?\n4. For the claim in line 168, is there any empirical support or reference? Also, what does the superscript star mean?\n5. Does one need to create new domain templates when using the method in a new domain? How about mixed-domain applications?\n6. Llama 3 isn't a reasoning model in Section 5.1.\n7. What is the instruction given to the human evaluators?\n8. Please add the dataset name in Figure 3's caption.\n9. In Table 3, why CoT+RAG incurs +26s cost, while only RAG and only CoT both introduce 0s cost?\n10. In Appx. A3, what is the data source for the data construction process?\n11. The WTransition in line 161 might be a typo. Also, please check the grammar between lines 162-168. The format between lines 238-246 is also improvable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8tPA4B6nnV", "forum": "rpnPKosij5", "replyto": "rpnPKosij5", "signatures": ["ICLR.cc/2026/Conference/Submission22066/Reviewer_Wg6F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22066/Reviewer_Wg6F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965228560, "cdate": 1761965228560, "tmdate": 1762942043401, "mdate": 1762942043401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LogicalChain + Test-Time-Training RAG (TTT-RAG), a framework that explicitly integrates structured logical chains relevant to the questions’ intent and perform test-time training to align model generation with retrieved reasoning. This paper also build large corpus of chains and train a contrastive retriever to fetch task-relevant inference paths. The proposed LogicalChain + TTT-RAG framework improves both medical and general multi-hop question answering performances."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper introduces LogicalChain + TTT-RAG, which shows strong empirical gains on medical (MedQA, MedMCQA) and math domains, outperforming strong baselines such as MedRAG and rStar.\n- This paper introduces a large corpus (2M+ pairs) of document and logical chain pairs from existing resources. Comparing to previous work that just focus on test-time retrieval augmentation for reasoning, this paper also train a retriever model tailored for this method.\n- This paper trains a dual-stage retriever that retrieves and rerank logical chain instead of using off-the-shelf retriever models.\n- Expert-annotated examples and manual spot-checks are used to validate the quality of retriever.\n- Efficiency analysis are included in the paper."}, "weaknesses": {"value": "- More details of the experiments are needed: What corpus do you use to evaluate MedRAG and other baselines? Are you using the provided corpus by them (namely wikipedia, pubmed, textbooks and statpearls)? Without ablations showing: (1) baseline methods using the same 2M+ corpus, or (2) TTT-RAG using baseline corpora, we cannot determine if the gains come from superior method design or simply having more/better training data. This is critical for establishing the method's actual contribution.\n- Section 6.4 evaluates on math domains but: (1) never states whether math-specific logical chains were constructed or if medical chains were used; (2) provides no discussion of domain transfer or what 'robustness' means in this context; (3) shows weaker gains (+2.8 points on MATH) compared to medical domain (+16.3 on MedQA), suggesting potential overfitting. Please state clearly whether this is zero-shot domain transfer, or there is a separate math corpus."}, "questions": {"value": "- Please fix the citation style (e.g., line 35-36; line 39-40). Use \\citet and \\citep accordingly.\n- The citation for MedRAG is missing. There are actually multiple MedRAG online. If it is Xiong et al. (2024), it might be better to include the citation in line 149, where MedRAG first appears in the full text.\n- In Table 9, do the textbooks and wikipedia corpora contain domains beyond medicine? What does “both” mean in this context?\n- What fraction is synthetic vs. human-authored chains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "05rTc921g9", "forum": "rpnPKosij5", "replyto": "rpnPKosij5", "signatures": ["ICLR.cc/2026/Conference/Submission22066/Reviewer_urFP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22066/Reviewer_urFP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994367533, "cdate": 1761994367533, "tmdate": 1762942042058, "mdate": 1762942042058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TTT–RAG, an enhanced RAG method where authors 'test-time-train' a model to retrieve a set of query-relevant reasoning chains, producing a 'provisional' step-indexed chain. They then retrieve knowledge and verify at each transition before generating a final solution. They test with both simple CoT and RAG-augmented baselines, finding their method outperforms similar-sized models in both the medical domain and general purpose multi-hop reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The step-wise idea of synthesizing candidate reasoning chains, and verifying reasoning iteratively (retrieve-and-verify at each transition) is interesting and seems especially relevant to domains such as medical, where verifiable logic is critical."}, "weaknesses": {"value": "- The novelty of the test-time-training mechanism is unclear, and it’s not obvious why weight updates are needed if more conventional inference-time solutions (such as simple ICL) could achieve the same purpose. Extensive further analysis is required to better position and compare this this with other more conventional approaches.\n- Missing some important analysis that would shed light into the behavior of their step-level retrieval verification mechanism. For example, how often and at what stages (ie, transitions) are failures most commonly observed?\n- The feels hastily written and very difficult to follow, with many typos, grammar and presentation errors throughout.\n\nOverall, the paper needs significant rewriting and clearer exposition."}, "questions": {"value": "1. Section 6.1 / Table 1: how are the human ratings calculated? Is it a head-to-head / win rate versus your method? If not, can you provide the performance of your method?\n2. Table 1: why report ROUGE instead of accuracy on MedQA?\n3. How is step-level entailment (“atomic coverage”) calculated? What metric?\n4. Section 6.4: are the ablations (small-batch size, parallel multi-query) performed only on math data? Why not on the initial domains? What are the range and increments you tested? You describe 1/2/4/8, yet the figure indicates 20–100 for retention percentage. Is there even a figure for the batch size ablation? The second figure is also missing a y-axis label.\n4a.  In what scenarios do you believe it is reasonable to share gradient updates across queries (multi-query setting)? There seem to be many uncertainties and potential for malicious or adversarial attacks.\n\n\nPresentation / Typos / Grammar Issues:\n\n- l161: “Wtransition” \n- l165: “Define a” — grammar\n- Missing parentheses around the citations in the introduction.\n- Figure 2: bottom-right of the diagram is particularly confusing.\n- Figure 1: “Ours logical chain” — missing a colon. Additonally, please use consistent symbols/emoji and label these (e.g., indicate what is an LLM response vs a meta-comment vs an emoji just to highlight the behavior).\n- Section title: “5 Experiment” → “5 Experiments?”\n- Figure 3 caption: references a top and bottom? (seems like some your caption text, figures, and section text are misligned)\n- Section 6.3: capitalize “Table”.\n- Table 3: color-code the checks and x’s."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "98Tobj1CFz", "forum": "rpnPKosij5", "replyto": "rpnPKosij5", "signatures": ["ICLR.cc/2026/Conference/Submission22066/Reviewer_rTMX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22066/Reviewer_rTMX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762285759224, "cdate": 1762285759224, "tmdate": 1762942041358, "mdate": 1762942041358, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}