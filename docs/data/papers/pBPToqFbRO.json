{"id": "pBPToqFbRO", "number": 7820, "cdate": 1758037464905, "mdate": 1759897830151, "content": {"title": "Discovering Forbidden Topics in Language Models", "abstract": "Refusal discovery is the task of identifying the full set of topics that a language model refuses to discuss. We introduce this new problem setting and develop a refusal discovery method, Iterated Prefill Crawler (IPC), that uses token prefilling to find forbidden topics. We benchmark IPC on Tulu-3-8B, an open-source model with public safety tuning data. Our crawler manages to retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale the crawl to a frontier model using the prefilling option of Claude-Haiku.\n\nFinally, we crawl three widely used open-weight models: Llama-3.3-70B and two of its variants finetuned for reasoning: DeepSeek-R1-70B and Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with censorship tuning: The model exhibits thought suppression behavior that indicates memorization of CCP-aligned responses. Although Perplexity-R1-1776-70B is robust to censorship, IPC elicits CCP-aligned refusals answers in the quantized model. Our findings highlight the critical need for refusal discovery methods to detect biases, boundaries, and alignment failures of AI systems. Code and project details are available at https://anonymous.4open.science/r/forbidden-topics. Content warning: This paper contains examples of sensitive language.", "tldr": "", "keywords": ["LLM", "Audit", "Refusal"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0e89a39bbb4a5da4af24b2dd9aac9dc0204ece22.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Authors introduce the problem of *refusal discovery*, a novel problem domain of discovering all the topics an LLM has been aligned to avoid. A method called IPC (Iterated Prefill Crawler) is introduced, inspired by web crawling, which \"crawls\" the LLM's refusal space by iteratively discovering forbidden topics through prefilling and using those topics as seeds to discover new ones.\n\nAuthors discover CCP censorship in DeepSeek through this method, showing refusal of queries including terms and topics known to be politically sensitive to the CCP. Authors furthermore discover that quantization can reintroduce refusals to \"decensored\" models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Authors introduce an entirely new problem space: \"refusal discovery\". I am effectively convinced by the paper that this a very important new space of research.\n\nThe paper is written clearly and understandably.\n\nAuthors do a very thorough investigation of refusal behaviors.\n\nThe discovery about quantization reintroducing censorship is particularly fascinating, with a real-world \"decensored\" model. I'm convinced this is high-impact.\n\nThe discovery that R1 uses the think tags as a refusal pattern is an interesting finding."}, "weaknesses": {"value": "Line 394, the Figure reference is broken (shows as ???).\n\nWhat number of topics does IPC discover that are discarded due to inconsistent refusal? You state that you do this filtration, but it would be interesting to see the actual numbers to understand the method's efficacy. Something like:\n\nTotal terms generated by prefill: 5000\n├ Duplicates filtered: 3000 (60%)\n├ Novel terms tested: 2000 (40%)\n    ├ Consistent refusals (≥50%): 300 (15%)  ← \"discovered topics\"\n    └ Inconsistent refusals (<50%): 1700 (85%) ← discarded\n\nI want a sense of how many \"false leads\" the method produces. Something like a \"funnel diagram\"?\n\nAlso, the threshold of 50% for this filtering feels somewhat arbitrary. What are these \"near the threshold\" topics? It would be very interesting to see the whole \"spectrum\" of refusal topics, not just those that happen to be above some arbitrary threshold.\n\nThis connects in my mind to the Elo score analysis and the concept of \"refusal strength\". You claim that higher Elo is equivalent to stronger refusal, and that these topics are more robust to rephrasing. But the way you discover the Elo score is through this pairwise comparison method, which just measures what the model *thinks* is a more sensitive topic. There is no analysis of the alignment between the model's *perceived* restrictions it has, and the actual *reality* of how strong its refusals are for a given topic. This is a pretty big gap in the paper, which if addressed would allow me to substantially raise its score. You need to validate the Elo against: 1) refusal rate, 2) robustness to jailbreaks. The Elo score right now seems to be entirely the model's self-assessment of sensitivity, and nothing else. *But you claim it's showing stronger refusals, with no validation this is the case*. This is a *serious overclaim*.\n\nAnother concern I have is the language around the CCP censorship. I think it would probably be a good idea for this language to be rewritten to be a bit more neutral. \"Censorship\" is a loaded term that is not necessarily exactly appropriate: we are talking about refusal and alignment, and which topics are included in these refusal categories. The CCP for political reasons includes topics that are not restricted in the West, but fundamentally we are talking about the same mechanisms. I would recommend switching the language to be more along the lines of \"refusal topics aligned with known CCP content restrictions\" or something along these lines.\n\nSome ablations on the number of templates, the temperature settings, deduplication thresholds, etc. Would be fantastic and also considerably strengthen the paper.\n\nIf the above weaknesses are addressed I'd be more than willing to raise my score, *potentially significantly*. I believe this is a strong paper with just a few serious gaps."}, "questions": {"value": "The paper vaguely seems to contradict various papers that have come out in the last little while that imply that models cannot particularly introspect. Why do you think introspection *specifically about what they can and can't talk about* seems to work, as opposed to other types of introspection (e.g. about what knowledge the models do or don't have)?\n\nThe refusal detection (stage 3) is done with a fresh conversation context right? I think this should be explicitly stated, but I'm assuming this is what is being done, otherwise I would consider this portion of the pipeline invalid. Would recommend stating for clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VtWPqFhHAu", "forum": "pBPToqFbRO", "replyto": "pBPToqFbRO", "signatures": ["ICLR.cc/2026/Conference/Submission7820/Reviewer_PV5x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7820/Reviewer_PV5x"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761252530344, "cdate": 1761252530344, "tmdate": 1762919865325, "mdate": 1762919865325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce the novel problem of refusal discovery: identifying the full set of topics that a language model refuses to discuss. They propose a method called iterated pre-fill crawling, which uses prefill in a model's chain of thought to have the model list topics it believes it has been trained to refuse. These topics are then separately tested for refusal behavior and used to seed future prompts in the crawl.\n\nThe authors first evaluate their method on Tulu-3-8B, a model with known safety training data that provides a ground truth set of forbidden topics. They demonstrate that their approach retrieves more forbidden topics compared to a prompting baseline, albeit at higher cost. They next explore their method on larger closed-source models, including Claude Haiku, and open-weight models such as Llama-3-70B, DeepSeek-R1-70B, and Perplexity's decensored R1-70B variant. Among their most notable findings, they discover that the R1-70B model exhibits significant CCP-related censorship patterns, validate that Perplexity-R1-1776-70B has been effectively decensored, and additionally find that after quantization, Perplexity-R1-1776-70B once again exhibits CCP-aligned refusals."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- The problem of refusal discovery is novel, interesting, and highly relevant—particularly given that even open-weight models often do not share details of their training data.\n- The paper takes thoughtful steps to evaluate a method that is inherently difficult to assess deeply. The experiments on Tulu-3-8B, where ground truth forbidden topics are known, provide valuable validation.\n- The fact that a prompting baseline could not discover the CCP-related topics in R1-70B, while the prefill-based method succeeded, provides compelling evidence that the prefill technique offers meaningful advantages.\n- The findings regarding R1-70B's CCP censorship, Perplexity's decensoring efforts, and the reemergence of censorship after quantization demonstrate that this technique can already yield impactful and practical discoveries.\n- One small detail I appreciate is the use of embedding similarity and LLM-as-a-judge for topic clustering when presenting the sets of refused topics. This makes the results significantly more interpretable."}, "weaknesses": {"value": "- The presentation of results on deployed models feels underdeveloped. Table 2 is the only table showing results from crawling popular models, and I wish there were more detail and analysis in this direction. For example:\n\n  - A key advantage of the IPC method is that the prompting baseline failed to find CCP topics in R1-70B, but this finding is currently buried as a single sentence in the Tulu-8B results section.\n  - The quantization results on decensored R1 are fascinating and arguably the most impactful findings in the paper, yet they are presented too briefly—just a pair of qualitative examples and the high-level claim that \"quantization restored censorship.\" This finding deserves more depth and analysis.\n  - There is minimal discussion of shared versus unique forbidden topics across models. Currently, readers must interpret Table 2 on their own to draw conclusions.\n\n\n- Section 4.3 on thought suppression in DeepSeek is interesting but tangential to the main narrative. It would be better placed in the appendix, freeing space for deeper analysis of core results.\n- I wish there were more effort to evaluate IPC's performance in the closed training data setting. While there is no ground truth in these settings, reasonable analyses could still be conducted—for example, comparing topics discovered in a single run against a larger set of topics aggregated across multiple runs and models. It is clear that IPC works and finds interesting results, but it would be valuable to provide some sense of the coverage achieved by the method."}, "questions": {"value": "- I found the results on closed-source models quite compelling, and hope you could expand on the existing results—for example, providing more comparisons to baselines, analyzing forbidden topics unique to specific models, or elaborating on the quantization findings. Concretely, fully developing these results would increase my ratings of soundness and presentation. \n\n- I'm also curious whether you have any data or analysis that could speak to IPC's coverage on models with closed training data.  I'd find even some preliminary results in this direction helpful for understanding how thoroughly IPC covers the refusal space. For instance:\n  - Consistency of discovered topics across multiple runs with different seeds\n  - A \"kitchen sink\" evaluation where all topics found across runs/models are tested on each model to establish a weak ground truth for coverage"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xf3rtSqsCx", "forum": "pBPToqFbRO", "replyto": "pBPToqFbRO", "signatures": ["ICLR.cc/2026/Conference/Submission7820/Reviewer_DhTr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7820/Reviewer_DhTr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761337776320, "cdate": 1761337776320, "tmdate": 1762919864700, "mdate": 1762919864700, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces refusal discovery, the task of identifying the full set of topics that a language model refuses to discuss. The authors propose an automated method called Iterated Prefill Crawler (IPC), which uses prefilling attacks to nudge models into revealing forbidden topics, and then iteratively expands the discovered set through semantic filtering and refusal-verification steps. The method does not rely on access to training data or gradients, and instead treats the model as a black box.\n\nThe method is first validated on Tulu-3-8B, where ground-truth safety topics are known, outperforming a direct-prompt baseline. It is then applied to several large open- and closed-weight models. The crawler interestingly uncovers notable differences in refusal behavior across models, such as censorship-aligned refusals in DeepSeek-R1."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper introduces an important yet novel problem formulation and proposes a sensible method to address the problem. Specifically, it introduces the unsupervised curation of the topics an LLM refuses to discuss. \n\nIt is well motivated, and well positioned, especially in so far as the need for methods beyond supervised dataset testing. They suggest a novel solution via prompting/prefilling CoT. Additionally, they propose a ‘crawling’ method that takes already found ‘forbidden topics’ and uses them as seeds for discovery of new (likely related) forbidden topics. Extensive analysis is done for a wide range of models of what \ntopics are refused.\n\nTheir crawling method discovers more forbidden topics for DeepSeek-R1 than the naïve baseline.  Interesting analysis of correlation between the occurrence of a cut-short CoT process, i.e  the CoT is simply <Thinking> </Thinking>, and the refusal of \ntopics."}, "weaknesses": {"value": "- Missing sensible baselines and ablations. The prompt efficiency of the baseline method is far better than the IPC method, suggesting that some improvement in prompt engineering/context could close the gap. For example, incorporating adaptivity, e.g prompt including current list of forbidden terms, into the baseline, may improve things. Also, this would serve as an ablation that tests the importance of the different elements of the proposed method, e.g. seeding vs pre-filling.\n\n- Lack of clarity in final performance of crawler vs baseline(s) across all models. In the Results section, there would ideally be a clear table indicating the main results of the Crawler method vs baseline(s) across all models. This is more challenging given the ground truth is not known, but a comparative analysis between the topics discovered between different methods can be used to provide quantitative metrics.\n\n- Lack of example rollouts/details about the verification method for refusing a topic.\n\nPresentation:\n- Lack of clarity in crawling methods, for example, how ‘branching’ works in the web crawler method.\n- Figures lack clarity in presentation; Figure 4 is unintuitive and hard to understand."}, "questions": {"value": "I found the iterative prompting procedure unclear. After the first iteration, my current understanding is that prompts follow the structure\n<User> Explain “ALREADY FOUND sensitive topic”. However, it is not clear how the CoT is re-initialized in subsequent iterations. Is the prompt always re-seeded with the exact same prefilling setup <Assistant> <think> I remember the full list of forbidden topics includes:\nfor each round? If so, please state this explicitly in the method description. If not, please clarify in more detail how each discovered term becomes a seed for further exploration and describe the full prompt format used after the first iteration."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZqwenC3v3s", "forum": "pBPToqFbRO", "replyto": "pBPToqFbRO", "signatures": ["ICLR.cc/2026/Conference/Submission7820/Reviewer_3dqM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7820/Reviewer_3dqM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864971743, "cdate": 1761864971743, "tmdate": 1762919864256, "mdate": 1762919864256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates refusal discovery,comprehensively identifying topics that language models refuse to discuss. To do so a Iterated Prefill Crawler (IPC) is used, where token prefilling attacks are used to systematically discover forbidden topics without requiring access to training data. The comparison is Tulu-3-8B and compared are Claude-Haiku-3.5, Llama-3.3-70B, DeepSeek-R1-70B, and Perplexity-R1-1776-70B. Significant censorship patterns are demonstrated for DeepSeek-R1-70B.  Surprisingly, quantization reintroduced censorship in Perplexity-R1-1776-70B despite claims of decensoring."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The methodology is practical, requiring only API access with prefilling capabilities rather than model weights or gradient access. \nThe benchmarking on Tulu-3-8B with known ground truth establishes credibility before scaling to proprietary models. \nThe discovery of thought suppression as a censorship mechanism in DeepSeek-R1 is both technically interesting and concerning from a transparency perspective. \nThe finding that quantization can reintroduce censorship in supposedly \"decensored\" models is particularly valuable for practitioners and highlights the importance of auditing final deployed systems rather than development checkpoints. \nThe paper provides extensive experimental details, prompt templates, and discusses limitations transparently, making the work reproducible and honest about its scope."}, "weaknesses": {"value": "The paper relies on prefilling to discover and validate refusals.\nThe paper is missing topics that require different jailbreaking techniques or exhibiting confirmation bias. \nThe baseline comparison is weak. \nThe authors acknowledge but that DeepSeek-R1 and Tulu-3 differ fundamentally in architecture, training objectives, and data, It is not clear to me how these differences in data translate into what looks like differences in refusal. Take the examples given, if no literature on Tiananmen Square in 1989 is used for training, the \"censorship\" is a function of the data selected for training. Here then the question what is a proper corpus for any of these models."}, "questions": {"value": "When a model refuses to list forbidden topics (as in your baseline), this is itself informative. By focusing only on successful elicitations, are you introducing systematic bias? Models that are more resistant to prefilling attacks might appear less censored simply because they're harder to audit.\n\nYou work with both English and Chinese content, but your deduplication uses English embeddings (OpenAI's model) even after translating Chinese to English. How does this affect measurement equivalence? Are you measuring the same construct of \"refusal\" across languages, or are there culture-specific aspects you're missing?\n\nYour method assumes the model will reveal forbidden topics when properly prompted. How do you handle adversarial models that are trained to resist refusal discovery?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7aOM3agO90", "forum": "pBPToqFbRO", "replyto": "pBPToqFbRO", "signatures": ["ICLR.cc/2026/Conference/Submission7820/Reviewer_YPeL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7820/Reviewer_YPeL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989557829, "cdate": 1761989557829, "tmdate": 1762919863800, "mdate": 1762919863800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}