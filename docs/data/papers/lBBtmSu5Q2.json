{"id": "lBBtmSu5Q2", "number": 15988, "cdate": 1758258115506, "mdate": 1759897268645, "content": {"title": "On Fine-Grained I/O Complexity of Attention Backward Passes", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in processing long-context information. However, the quadratic complexity of attention computation with respect to sequence length poses significant computational challenges, and I/O aware algorithms have been proposed. This paper presents a comprehensive analysis of the I/O complexity for attention mechanisms, focusing on backward passes by categorizing them into small and large cache scenarios. Using the red-blue pebble game framework, we establish tight bounds on I/O complexity across all cache sizes. We confirm that the de facto standard I/O aware algorithm FlashAttention is optimal for both forward and backward passes for the large cache size scenario. For small cache sizes, we provide an algorithm that improves over existing methods and achieves tight bounds. Additionally, we extend our analysis to sparse attention, a mainstream speeding-up approach, deriving fine-grained lower bounds for both forward and backward passes and both small and large caches. Our findings complete the theoretical foundation for I/O complexity in attention mechanisms, offering insights for designing efficient algorithms of LLM training and inference.", "tldr": "", "keywords": ["Attention", "I/O Complexity", "Backward Passes."], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/01811e72654ff83fed677e818da1fd5a5b41c8ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors consider the I/O complexity of Attention gradient computation. In hardware, data is typically arranged hierarchically, with data stored in an unbounded memory, and computation occurring in a bounded cache. To compute, data is moved into the cache, computation occurs, and the result is saved in memory. Since data movement is typically more expensive that computation, I/O complexity measures only the data movements. The goal of I/O complexity is to design algorithms minimizing I/Os. Given the prevalence of attention and the success of the FlashAttention algorithm, it is a practically important question to understand whether the training process can be optimized w.r.t. I/O complexity.\n\nThe authors give I/O optimal bounds for the computation of attention gradient when restricted to algorithms using standard matrix multiplication. The authors also consider sparse attention, and give lower bounds for algorithms using standard matrix multiplication in this setting. \n\nWhile the statement of the main result is interesting, the techniques are identical to prior work, and in fact the main result can be obtained immediately from the lower bound for the forward pass. Furthermore, the lower bound on sparse attention is not well substantiated without a matching upper bound (or at least some improvement over the naive algorithm). Thus I recommend reject."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The authors study a practically interesting problem, and give tight results. \n\nThey initiate the study of sparse I/O attention."}, "weaknesses": {"value": "The main result (lower bound for attention gradient computation) is essentially immediate from prior work. In particular, a previous paper proves that any algorithm that computes the attention matrix already requires the FlashAttention lower bound. Since attention gradient computation involves a n x d and d x n matrix product, this immediately implies the desired lower bound. Similarly, the new upper bound for gradient computation in the small cache setting is a consequence of the equivalence with matrix multiplication (the easy direction - using matrix multiplication we can compute attention gradients).\n\nThe sparse attention lower bound is not well motivated if there is no matching upper bound, or at least some improvement on the trivial algorithm. Even if this is hard to prove, there should be some discussion towards what the obstacles are."}, "questions": {"value": "What are the main obstacles towards designing I/O efficient algorithms for sparse attention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lm2R5wKOeo", "forum": "lBBtmSu5Q2", "replyto": "lBBtmSu5Q2", "signatures": ["ICLR.cc/2026/Conference/Submission15988/Reviewer_KW4D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15988/Reviewer_KW4D"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803133194, "cdate": 1761803133194, "tmdate": 1762926197012, "mdate": 1762926197012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper extends the analysis of I/O complexity of exact attention appearing in [Dao, 2022] and [Saha & Ye, 2024], specifically providing tight bounds on the I/O complexity of the attention backwards pass using the red-blue pebble game framework. The results suggest that the popular FlashAttention algorithm is optimal in both forwards and backwards modes in the large cache regime (most practically relevant), while providing an improved algorithm in the small cache regime. The authors also extend the analysis to the sparse attention regime."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper's derivations seem to be solid and rigorous, to the best of my understanding.\n- The paper extends the results appearing in the previous work, thus completing the I/O complexity analysis for both forwards and backwards passes, small and large cache regimes, as well as dense and sparse attention.\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "Overall, the paper seems to be a direct extension of [Saha & Ye, 2024], adding tight bounds for the I/O complexity of attention backwards pass. However, the results seem to directly mirror the prior work; the authors utilise the same framework, and provide similar asymptotic bounds and conclusions. Due to this, my impression is that the work, although mathematically solid, seems to be incremental. The small-cache algorithm, as well as theoretical derivations seem to follow directly from [Saha & Ye, 2024], and from the practical perspective do not offer a significant contribution (as noted in the paper, the large- cache regime is more practically relevant, and FlashAttention is proven to be optimal). Due to this, my impression is that the scope of the paper is not quite sufficient for publication in ICLR."}, "questions": {"value": "- Could the authors clarify how their small-cache algorithm differs/complements the similar proposition from [Saha & Ye, 2024]?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LQ7hZbsseI", "forum": "lBBtmSu5Q2", "replyto": "lBBtmSu5Q2", "signatures": ["ICLR.cc/2026/Conference/Submission15988/Reviewer_3EBW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15988/Reviewer_3EBW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909592687, "cdate": 1761909592687, "tmdate": 1762926196647, "mdate": 1762926196647, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes the I/O (cache ↔ memory) complexity of the backward pass of exact softmax attention under standard GEMM, using the red–blue pebble framework. It proves *matching upper and lower bounds across all cache sizes*, with a phase transition at ($M = \\Theta(d^2)$) ($M$ is the cache size and $d$ is attention head dimension). \n\nIn the large-cache regime ($M=\\Omega(d^2)$), the bounds match FlashAttention’s behavior and establish optimality; in the small-cache regime ($M=o(d^2)$), the paper gives a strictly better algorithm (and matching lower bound) than FlashAttention. It also gives lower bounds for sparse attention, recovering the dense case as a special case."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "### Originality\n\n* Provides the first matching upper and lower bounds for the backward pass of exact attention for all cache sizes with a clean phase transition at ($M=\\Theta(d^2)$) (Theorem 1.1).  \n* Extends to sparse attention with lower bounds that recover the dense case as a special instance. \n\n### Quality\n\n* Uses the red–blue pebble framework rigorously and states Theorem 1.1 with an explicit formula covering both regimes.  \n* Gives matching bounds in each regime: large-cache upper (Thm 4.1) and lower (Thm 4.2), small-cache upper via Algorithm 6 (Thm 4.3) and lower (Thm 4.4).    \n\n### Clarity.\n\n* Figure 1 clearly contrasts the paper’s tight bound (red) with FlashAttention’s upper bound (blue dashed) and marks the cross-point ($M=\\Theta(d^2)$). \n* Theorems in §4 are presented as informal versions which helped readabillity.  \n\n### Significance\n\n* In the large-cache regime, results match FlashAttention and establish optimality; in the small-cache regime, Algorithm 6 is provably better than FlashAttention."}, "weaknesses": {"value": "1. **Positioning vs prior work could be tighter.** The paper clearly cites Dao et al. (FlashAttention) and Saha & Ye for forward-pass tightness; it mentions Addanki et al. (streaming/approximate attention) in related work, but a compact comparison table clarifying different problem settings (exact vs approximate, streaming vs two-level memory) would help readers situate novelty. \n\n2. **Practical relevance narrative.** The paper *does* discuss when small-cache arises (e.g., per-SM caches on older GPUs) and even gives A100 vs GTX1060 examples; expanding this with a short table of device-level (M) estimates and typical head sizes (d) would strengthen the “why it matters” section."}, "questions": {"value": "1. **Scope vs Addanki et al. (2023).** Please add a small table clarifying the differences (objective: exact vs approximate; model: two-level I/O vs streaming; bounds reported) and why your results are not directly comparable numerically. \n\n2. **Multi-head attention.** Your bounds are given per head; what changes (if any) under (H) heads computed in parallel. Does tiling across heads alter the asymptotics or only the constants?\n\n3. **Device checklist.** Consider adding a table (SM/L1 size, datatype, typical ($d$)) for a few GPUs/edge devices to show where ($M \\lessgtr d^2$) actually falls."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nosDARxwy7", "forum": "lBBtmSu5Q2", "replyto": "lBBtmSu5Q2", "signatures": ["ICLR.cc/2026/Conference/Submission15988/Reviewer_qFFE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15988/Reviewer_qFFE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950307434, "cdate": 1761950307434, "tmdate": 1762926196280, "mdate": 1762926196280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The original FlashAttention paper provides upper I/O complexity bounds for the backward pass of the exact attention computation, but does not provide lower bounds. This raises the question: what is the optimal I/O complexity of the attention backward pass? This paper provides a lower bound as a function of cache size. Interestingly, they show that the lower bound changes at a crossover point where the cache size if $o(d^2)$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The authors show that there is room at small cache sizes, to potentially provide a speedup over FlashAttention by reducing I/O complexity.\n- The paper is pretty easy to follow and does quite a good job situating itself with respect to prior work."}, "weaknesses": {"value": "- The authors do not provide an implementation of their algorithm, and so they cannot demonstrate that it actually provides a speedup over FlashAttention. The claim that the “algorithm designed for small cache sizes would become relevant and useful”, is speculative. In my view, this is the most significant limitation of this work.\n- The result is only applicable for very small cache sizes, and does not apply to modern GPUs typically used for training (A100s, H100s, B200s).\n- This paper (like prior work before it) assume a two-level memory hierarchy. This may limit the applicability of the results, especially since newer chips include more complex memory hierarchies including"}, "questions": {"value": "- Does Algorithm 6 increase the FLOPs required — even if only by a constant factor?\n- Can the authors provide an implementation of their algorithm and demonstrate that it can provide a speed up on GPUs with small cache sizes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XeVdhPAtSV", "forum": "lBBtmSu5Q2", "replyto": "lBBtmSu5Q2", "signatures": ["ICLR.cc/2026/Conference/Submission15988/Reviewer_gi5z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15988/Reviewer_gi5z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762238579007, "cdate": 1762238579007, "tmdate": 1762926195814, "mdate": 1762926195814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}