{"id": "Ua6bRCHRFm", "number": 10796, "cdate": 1758182094360, "mdate": 1759897628284, "content": {"title": "D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks", "abstract": "Tasks involving high-risk–high-return (HRHR) actions, such as obstacle crossing, often exhibit multimodal action distributions and stochastic returns. Most reinforcement learning (RL) methods assume unimodal Gaussian policies and rely on scalar-valued critics, which limits their effectiveness in HRHR settings. We formally define HRHR tasks and theoretically show that Gaussian policies cannot guarantee convergence to the optimal solution. To address this, we propose a reinforcement learning framework that (i) discretizes continuous action spaces to approximate multimodal distributions, (ii) employs entropy-regularized exploration to improve coverage of risky but rewarding actions, and (iii) introduces a dualcritic architecture for more accurate discrete value distribution estimation. The framework scales to high-dimensional action spaces, supporting complex control domains. Experiments on locomotion and manipulation benchmarks with high risks of failure demonstrate that our method outperforms baselines, underscoring the importance of explicitly modeling multimodality and risk in RL.", "tldr": "We propose a model with discrete participants and distributive critics, which includes an action entropy-based exploratory strategy and a structure of dual distribution criticism.", "keywords": ["Deep Reinforcement Learning; Distributional Reinforcement Learning; Double Distributional Critics; Action Entropy"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/526cc2a9f5dda7b83e688003395e8520d4229476.pdf", "supplementary_material": "/attachment/61e98238fc1165bd48de1e43256787fe557f41fa.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles reinforcement learning in high-risk–high-return settings where peak returns lie in small, risky regions of the action space. It introduces D2C-HRHR, combining a discrete actor with entropy-regularized exploration and two distributional critics whose CDF-max target is intended to curb overestimation, and reports gains on BipedalWalkerHardcore and FetchPush. If the method is specified clearly and is reproducible, a practical recipe for robust learning in HRHR regimes could be valuable to the community."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "* The paper formalizes the HRHR setting and defines a concrete problem where high returns concentrate in small risky regions.\n* It introduces a dual distributional critic that combines two value estimates by taking the pointwise maximum of their cumulative distributions, and this design remains simple and modular because it only changes the critic and uses a C51 style projection that plugs into standard off policy pipelines.\n* The figures clearly depict the pipeline and make the distributional clipping and the discrete actor workflow easy to follow."}, "weaknesses": {"value": "* The HRHR notion equates risk with a low uniform-average rather than tail behavior under the policy, so the concept depends on an arbitrary reference distribution and may not reflect true risk. Please justify the choice or connect the definition to policy-induced tail metrics.\n* The core step in Eq. (31) is an unsupported and ill defined claim, it asserts $Cov_{a\\sim\\pi_\\theta}(a-\\mu_\\theta(s), A_{R2}(s,a)) < 0$, yet this covariance is a vector so the inequality makes sense only with an explicit componentwise statement and proof, neither is provided, therefore the proof does not establish the claimed learning bias and the motivation of Theorem 1 remains unproven.\n* The paper neither proves nor empirically isolates the benefit of the CDF-max dual critic, there is no formal lemma showing a pessimistic target under shared support and projection, and the ablations do not reveal whether CDF max or the dual critic drives the gains.\n* The method substantially increases compute by doubling critics and using large discrete supports, yet there is no analysis of time or resource costs, at minimum report wall clock and GPU hours.\n* Baseline fairness cannot be assessed because hyperparameters, tuning budgets, and implementation details are missing, at minimum provide per task hyperparameters and state whether HER or goal conditioning is enabled on Fetch tasks.\n* The paper claims “ten thousand trials,” in the experiment part, but the released code evaluates with n_eval = 100. Please clarify how 10,000 was obtained and provide the exact evaluation script and command.\n* The text refers to FetchPush v3 while tables use v4. Please standardize the version and specify the exact Gym or Gymnasium build.\n* Notation is inconsistent, the paper switches between $\\Omega$ and $R$ for the same regions. For example, Definition 1 uses $\\Omega_i$, but the very next lines in Definition 2 switch to $R_1$  without stating the relationship between these notations, which breaks traceability.\n* Terminology and wording are inconsistent, the paper overloads “atom” to mean both action bins and value atoms, and includes typos such as “double” misspelled, which reduces clarity."}, "questions": {"value": "Please address the weaknesses pointed out above. Additionally, answer the following questions:\n* Please clarify Eq. (31). Is the inequality meant componentwise? State the exact assumptions and give a correct proof or revise the claim so the update sign is rigorously justified.\n* Please disclose full experimental settings for all baselines and your method. Include per task hyperparameters, network widths, training steps, seed handling, and whether HER or goal conditioning is used. Provide commands and commit to reproduce tables.\n* Please quantify complexity and efficiency. Report wall clock to fixed returns, environment steps per second, GPU hours, and memory, and add a small scaling study over atom count and action dimension.\n* Please improve readability and notation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w5AKVjvalO", "forum": "Ua6bRCHRFm", "replyto": "Ua6bRCHRFm", "signatures": ["ICLR.cc/2026/Conference/Submission10796/Reviewer_PmdC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10796/Reviewer_PmdC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761585298820, "cdate": 1761585298820, "tmdate": 1762922011440, "mdate": 1762922011440, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers Reinforcement learning with continuous actions and the challenge that sometimes the best actions are hard to find because they may be very similar/close to other suboptimal actions. This is called High Risk High Reward scenario. The paper builds on distributional reinforcement learning to create a solution that targets this challenge. Specifically it uses discretization of the action space and double critic network."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper shows experimental results in complex tasks (bipedal robots) and the results are significantly better than other approaches."}, "weaknesses": {"value": "- Unfortunately, the paper is not well written, with many distracting typos, but also syntax errors like icomplete sentences. The most common is distributed versus distributional but there are others like critic vs criticism. Even if I try to ignore these typos, I can't find clear justifications for why the algorithm performs better than others in the experimental setup.\n- The term 'risk' may not be appropriate, if we think that it is used in other topics like Conditional Value at Risk, or risk sensitive markov decision processes.\n- The HRHR scenario is clear but then I don't find it particularly enlightening, not really requiring a formal proof and as much space in the main body. \n- The HRHR scenario relies on the fact that the variance of the policy randomness is larger than an area of high reward actions. This seems like quite a special case, and it is questionable how often it is encountered in practice. Additionally, one may always try to lower the variance of the policy as an easy fix.\n- Discretizing action spaces, especially if they have high dimensions, can be tricky due to computational complexity growing exponentially. It is not clear how it is addressed.\n- Important mathematical steps in page 5 are not clearly explained and are hard to follow."}, "questions": {"value": "Beyond addressing some of the above weaknesses, it is not clear to me why is the proposed approach better than alternatives in practice. What is the argument for this?\n\nI agree that distributional RL is more complicated in notations, but I would appreciate some clarity here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i9Vl6GNtXs", "forum": "Ua6bRCHRFm", "replyto": "Ua6bRCHRFm", "signatures": ["ICLR.cc/2026/Conference/Submission10796/Reviewer_vCzt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10796/Reviewer_vCzt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960700680, "cdate": 1761960700680, "tmdate": 1762922011044, "mdate": 1762922011044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets \"high-risk–high-return\" (HRHR) regimes where optimal actions live in narrow pockets that Gaussian policies average out. It (i) discretizes each continuous action dimension, (ii) learns Q-value distributions (not just scalars), (iii) uses a clipped double distributional critic to curb overestimation, and (iv) adds an entropy-gated exploration term keyed to the critic’s cumulative value distribution. On BipedalWalkerHardcore-v3 and FetchPush-v4, the method outperforms strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- HRHR is an interesting problem formulation, and makes Gaussian policies fail.\n- Empirical results in two environments are strong."}, "weaknesses": {"value": "- Limited domains that clearly demonstrate the HRHR framework. The two primary environments shown are limited and also not exactly falling under the domain of HRHR. The paper would benefit from several classes of environments where HRHR is a meaningful problem.\n- Even in HRHR, Gaussian policies may be optimal because the policy can still optimize one of the peaks of the Q-function. It is unclear what is the key failure mode of policy learning in multimodal Q-functions?\n- How does this paper compare against other baselines that are suited to multi-modal actors such as mixture of policies SAC / TD3, diffusion policies, or SAVO [1]?\n[1] Jain et al. \"Mitigating Suboptimality of Deterministic Policy Gradients in Complex Q-functions.\" Reinforcement Learning Conference.\n- The baselines performance in FetchPush-v4 is very low. Is this expected and what makes the proposed approach to have such a drastic improvement over baselines? Were the baselines tuned properly?"}, "questions": {"value": "see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1FebC8XcKb", "forum": "Ua6bRCHRFm", "replyto": "Ua6bRCHRFm", "signatures": ["ICLR.cc/2026/Conference/Submission10796/Reviewer_GGNy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10796/Reviewer_GGNy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997156174, "cdate": 1761997156174, "tmdate": 1762922010498, "mdate": 1762922010498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies high-risk–high-return (HRHR) RL settings where the highest rewards lie in narrow, risky action regions that are easy to miss with standard Gaussian policies. The authors (i) formally define HRHR scenarios and argue Gaussian policies can fail there; (ii) propose D2C-HRHR, which discretizes each continuous action dimension, learns a multidimensional discrete policy, and uses double distributional critics with a conservative (clipped) aggregation to curb over-estimation; and (iii) introduce an entropy-aware exploration term tied to value-distribution confidence. Experiments on BipedalWalkerHardcore-v3, FetchPush-v4, and a MuJoCo suite report consistent gains over C51, SAC, TD3, TQC, and SAC-Discrete."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Clear problem framing. The HRHR definition crisply captures when the average return of a risky region is lower despite having the global maximum, and the theorem illustrates why a Gaussian policy with variance larger than the grain  drifts toward safer suboptimal regions. This directly motivates discretization.  \nMethod simplicity & compatibility. The per-dimension discretization with a matrix policy is easy to implement and plug into existing actor-critic setups; the double distributional critics and conservative cumulative aggregation are a neat analogue of clipped double-Q for distributional critics. \nThe entropy bonus is gated by the value-distribution’s cumulative mass on lower-value atoms; low-confidence states get more entropy, high-confidence states don’t—simple and sensible."}, "weaknesses": {"value": "1. The multidimensional discrete actor assumes independence across action dimensions (row-wise sampling). While this avoids an explicit m^n enumeration, it may miss inter-dimensional couplings crucial in dexterous manipulation or legged control with coupled joints. Please discuss when factorization suffices, and whether an autoregressive or flow-based discrete policy could capture dependencies without exploding compute. \n\n2. Discretization increases output size (n×m) and critic heads (distributional atoms). Please quantify throughput, wall-clock, and GPU memory versus SAC/TQC/C51 on the main tasks, and include a sensitivity study for m (per-dim atoms) and N (critic atoms). Some hardware and hparams are reported, but compute trade-offs remain unclear.\n\n3. Over-estimation diagnostics. Plot critic CDFs/PDFs and show how the clipped cumulative rule changes value estimates vs. single-critic distributional baselines. \n\n4. The baselines are weak; the authors should add more discretization policy distribution works as baselines.\n\n5. Regarding the policy distribution, I recommend that the authors add the discretization policy distribution topic works.\n\nDiscretizing continuous action space for on-policy optimization, AAAI, 2020 \n\nDiscretizing Continuous Action Space With Unimodal Probability Distributions for On-Policy Reinforcement Learning, IEEE TNNLS, 2024."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fbrorNhype", "forum": "Ua6bRCHRFm", "replyto": "Ua6bRCHRFm", "signatures": ["ICLR.cc/2026/Conference/Submission10796/Reviewer_F3XF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10796/Reviewer_F3XF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10796/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762323681662, "cdate": 1762323681662, "tmdate": 1762922010018, "mdate": 1762922010018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}