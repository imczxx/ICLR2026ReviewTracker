{"id": "PJdMrK79Mo", "number": 7688, "cdate": 1758032151818, "mdate": 1759897839283, "content": {"title": "Sample-efficient and Scalable Exploration in Continuous-Time RL", "abstract": "Reinforcement learning algorithms are typically designed for discrete-time dynamics, even though the underlying real-world control systems are often continuous in time. In this paper, we study the problem of continuous-time reinforcement learning, where the unknown system dynamics are represented using nonlinear ordinary differential equations (ODEs). We leverage probabilistic models, such as Gaussian processes and Bayesian neural networks, to learn an uncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily maximizes a weighted sum of the extrinsic reward and model epistemic uncertainty. This yields a scalable and sample-efficient approach to continuous-time model-based RL. We show that COMBRL achieves sublinear regret in the reward-driven setting, and in the unsupervised RL setting (i.e., without extrinsic rewards), we provide a sample complexity bound. In our experiments, we evaluate COMBRL in both standard and unsupervised RL settings and demonstrate that it scales better, is more sample-efficient than prior methods, and outperforms baselines across several deep RL tasks.", "tldr": "We propose COMBRL, a scalable continuous-time RL algorithm that balances reward and epistemic uncertainty using intrinsic rewards, achieving sublinear regret and strong performance in both supervised and unsupervised settings.", "keywords": ["continuous-time reinforcement learning", "model-based RL", "intrinsic rewards", "epistemic uncertainty", "exploration-exploitation trade-off"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/19c4e92d9ff7a93107e13ee1bf086386f69e71d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces COMBRL, a continuous-time optimistic model-based RL framework that learns probabilistic ODE dynamics and plans policies by maximizing a reward-plus-uncertainty objective governed by a single $\\lambda$ schedule. They analyze both reward-driven and unsupervised (intrinsic-only) regimes, proving sublinear regret and sample-complexity bounds, and demonstrate on Gymnasium and DeepMind Control benchmarks that COMBRL improves sample efficiency over OCORL, PETS, and mean-planning baselines while also handling time-adaptive sensing and control."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Clean optimism formulation: a single $\\lambda$ schedule unifies reward-driven and intrinsic exploration, allowing practitioners to traverse between task-focused and unsupervised regimes.\n- Theory for both regimes: sublinear regret for the reward-driven case and sample-complexity guarantees for pure intrinsic learning extend discrete-time optimism analysis to continuous-time ODE settings.\n- Model-agnostic dynamics learning: the framework accommodates Gaussian processes, Bayesian neural nets, ensembles, and other uncertainty-aware estimators, and remains solver/discretization agnostic through its continuous-time formulation.\n- Empirical coverage: experiments span standard Gymnasium and DMC control tasks, include time-adaptive sensing/control, and report downstream-task performance gains using the learned models, showing consistent sample-efficiency improvements over OCORL, PETS, and mean-planning baselines."}, "weaknesses": {"value": "- Limited scope of benchmarks: results stop at modest Gymnasium/DMC tasks; harder domains (e.g., Humanoid, Dog) and sparse-reward environments (e.g., Maze2D) that would stress exploration are absent, so the empirical evidence for better exploration remains less convincing.\n- Missing modern baselines: comparisons exclude recent model-free optimistic or exploration-focused controllers such as BRO [1], so it is unclear whether COMBRL advances the empirical state of the art beyond older baselines like OCORL and PETS.\n- $\\lambda$ sensitivity and exploration role not sufficiently explored: tables in the Appendix show $\\lambda$ varying considerably across tasks. Plots seem to show contradictory results for how exploration strength impacts performance or when higher bonuses help versus hurt.\n\n[1] Nauman et al., Bigger, Regularized, Optimistic: scaling for compute and sample-efficient continuous control, 2024."}, "questions": {"value": "- In the fixed-rate experiments, do the authors use the default Gym/DMC sampling interval, or do they employ a finer measurement-selection schedule? Please quantify any difference (e.g., actual step size) and discuss how sensitive COMBRL is to noisy or finite-difference estimates of $\\dot{x}$ under that schedule.\n- Table 1 and Table 5 list quite different $\\lambda$ values. Figure 2 credits intrinsic bonuses for COMBRL’s gains, yet Figure 4’s ablation sometimes peaks at $\\lambda = 0$, suggesting exploration may hurt. Could the authors reconcile these results by clarifying how the hand-tuned and auto-tuned schedules were obtained, explaining when exploration helps versus harms (e.g., Hopper), and showing how the auto-tuned $\\lambda$ evolves over time (plots)?\n- COMBRL’s continuous-time focus is compelling, yet BRO [1] remains the state-of-the-art optimistic model-free continuous-control method with a similar exploration rationale—what prevents a direct comparison to show whether COMBRL’s advantages persist against that baseline?\n- Since the method hinges on epistemic bonuses, could the authors provide plots showing how model uncertainty decreases over episodes, especially in the unsupervised regime?\n- Given the exploration focus, could the evaluation be extended to harder DMC benchmarks such as Humanoid-run or Dog-run, and to sparse-reward settings like Maze2D, to substantiate the claimed advantages beyond the current tasks?\n\nI find the continuous-time and unsupervised RL framing compelling; if the authors can address the questions above, I would be inclined to raise my score.\n\n[1] Nauman et al., Bigger, Regularized, Optimistic: scaling for compute and sample-efficient continuous control, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hcdLiRxtw6", "forum": "PJdMrK79Mo", "replyto": "PJdMrK79Mo", "signatures": ["ICLR.cc/2026/Conference/Submission7688/Reviewer_LpWF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7688/Reviewer_LpWF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761089795968, "cdate": 1761089795968, "tmdate": 1762919747592, "mdate": 1762919747592, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the continuous-time model-based RL problem where the system dynamics are represented by nonlinear ODEs. The proposed algorithm, COMBRL, is built on uncertainty-aware probabilistic models (GPs or BNNs) and aims to optimize a reward-plus-uncertainty objective in continuous time. The work focuses on two settings: reward-driven RL and unsupervised RL. In experiments, the proposed COMBRL achieves strong empirical results across different continuous-time RL benchmarks and is sample-efficient compared to other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- I am not an expert in this area but I agree that the study of the continuous-time RL is promising. In addition, this work further explores the unsupervised setting, which is important and interesting.\n- The reward-plus-uncertainty objective makes sense and the results look good."}, "weaknesses": {"value": "1. My main concern is that the continuous-time RL is already studied in previous work such as [1], in which the system is also modelled by ODEs and the model is built on GP. This makes the novelty incremental.\n2. In Figure 2, the baseline approaches are only mean planner and PETS. It would be better to provide more comparisons with other state-of-the-art methods on these GYM and DMC tasks.\n\n\n\n\n[1] Efficient exploration in continuous-time model-based reinforcement learning, NeurIPS 2023."}, "questions": {"value": "The current settings and tasks are too simple, I wonder can this algorithm be applied to some real-world problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yxYG21Der8", "forum": "PJdMrK79Mo", "replyto": "PJdMrK79Mo", "signatures": ["ICLR.cc/2026/Conference/Submission7688/Reviewer_8afj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7688/Reviewer_8afj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575664138, "cdate": 1761575664138, "tmdate": 1762919746995, "mdate": 1762919746995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces COMBRL, a continuous-time optimistic model-based reinforcement learning algorithm designed to balance task performance and model exploration. The authors establish theoretical performance guarantees and validate the approach empirically across a suite of continuous-time deep RL benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The theoretical studies are solid."}, "weaknesses": {"value": "The primary weakness is the numerical evaluation. When ground-truth values are available, experiments should report coverage rates of the true value; limiting results to the averaged returns and standard deviations does not provide an adequate assessment."}, "questions": {"value": "See the weakness. The author may consider to work on a simulated environment for which the true values are know, and experiments should report covedrage rates of the true value."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IrXRSqBZsM", "forum": "PJdMrK79Mo", "replyto": "PJdMrK79Mo", "signatures": ["ICLR.cc/2026/Conference/Submission7688/Reviewer_F13n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7688/Reviewer_F13n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880612389, "cdate": 1761880612389, "tmdate": 1762919746669, "mdate": 1762919746669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses continuous-time reinforcement learning (RL), where system dynamics are modeled by unknown nonlinear ordinary differential equations (ODEs). The authors propose COMBRL, a model-based RL algorithm that uses probabilistic models to learn an uncertainty-aware approximation of the ODE dynamics. COMBRL balances maximizing extrinsic reward and reducing model epistemic uncertainty, enabling efficient exploration."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Continuous-time model-based RL is an under-explored area with practical relevance, e.g., in robotic control.\n- The paper includes extensive experiments evaluating design choices and hyperparameter sensitivity across multiple benchmarks.\n- Theoretical guarantees are provided."}, "weaknesses": {"value": "- In Figure 3, COMBRL’s performance on the primary task is modest: in 5 out of 7 tasks, its error bars overlap substantially with those of the Mean Planning baseline, raising questions about practical gains.\n- The paper assumes familiarity with several methods (e.g., SAC, iCEM, and TaCoS) without brief introductions, which hinders accessibility.\n- The novelty is unclear. While continuous-time RL is valuable, the core idea—-balancing reward maximization and epistemic uncertainty in the objective function—-is not novel. The contribution appears to be an integration of existing components rather than a conceptual advance. Clarifying the key insight beyond \"applying model-based RL to continuous time\" would strengthen the paper.\n- Algorithm 1 lacks sufficient detail. Critical aspects, such as how the optimistic objective is solved, how rollouts are collected, and how the dynamics model is updated, are omitted. Including a more descriptive outline would improve reproducibility."}, "questions": {"value": "- How were hyperparameters (e.g., the trade-off coefficient $\\lambda$) set (static or annealing) for the experiments in Figure 3? Was tuning performed per environment, and if so, what protocol was used?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZYwmUUhPZB", "forum": "PJdMrK79Mo", "replyto": "PJdMrK79Mo", "signatures": ["ICLR.cc/2026/Conference/Submission7688/Reviewer_G4x3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7688/Reviewer_G4x3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7688/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003506564, "cdate": 1762003506564, "tmdate": 1762919746238, "mdate": 1762919746238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}