{"id": "6kFtFYpFmS", "number": 11549, "cdate": 1758201488688, "mdate": 1762931882521, "content": {"title": "Pedestrian Attribute Recognition via Hierarchical Cross-Modality HyperGraph Learning", "abstract": "Current Pedestrian Attribute Recognition (PAR) algorithms typically focus on mapping visual features to semantic labels or attempt to enhance learning by fusing visual and attribute information. However, these methods fail to fully exploit attribute knowledge and contextual information for more accurate recognition. Although recent works have started to consider using attribute text as additional input to enhance the association between visual and semantic information, these methods are still in their infancy. To address the above challenges, this paper proposes the construction of a multi-modal knowledge graph, which is utilized to mine the relationships between local visual features and text, as well as the relationships between attributes and extensive visual context samples. Specifically, we propose an effective multi-modal knowledge graph construction method that fully considers the relationships among attributes and the relationships between attributes and vision tokens. To effectively model these relationships, this paper introduces a knowledge graph-guided cross-modal hypergraph learning framework to enhance the standard pedestrian attribute recognition framework. Comprehensive experiments on multiple PAR benchmark datasets have thoroughly demonstrated the effectiveness of our proposed knowledge graph for the PAR task, establishing a strong foundation for knowledge-guided pedestrian attribute recognition.", "tldr": "", "keywords": ["Pedestrian Attribute Recognition; Knowledge Graph; Cross-Modal HyperGraph Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/a5419fe03938ab488d334a4a772029b5306aa199.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a knowledge graph–guided hierarchical hypergraph framework (KGPAR) that effectively bridges visual and semantic modalities for pedestrian attribute recognition. The method is technically novel and empirically strong, achieving balanced gains across multiple benchmarks (Tables 1–3). However, it lacks computational analysis, relies on fixed structural choices, and provides limited ablation diversity, which leaves scalability and generalization insufficiently demonstrated."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The method combines local and global hypergraphs to capture higher-order vision-text relations, effectively bridging the semantic gap in prior PAR approaches. It constructs interpretable attribute nodes with textual and visual embeddings, enhancing semantic richness and supporting knowledge-guided learning. Experiments are conducted across multiple benchmarks, and heatmap visualizations provide insightful interpretations of the model’s reasoning."}, "weaknesses": {"value": "See questions."}, "questions": {"value": "- The performance comparisons across multiple benchmarks do not consistently achieve the best results. Additionally, Section 4.3 is poorly written, containing largely redundant content that occupies significant space; it should instead focus on analyzing the underlying reasons for the observed performance.\n\n- The heatmap visualizations in Figure 3 are insightful for understanding attribute-level results, but they should be accompanied by qualitative comparisons with state-of-the-art methods.\n\n- It is recommended to report training and inference time, GPU memory usage, and FLOPs for both the baseline and the full model in Section 4.2.\n\n- Several important ablation studies are missing. Table 4 should be extended to include experiments on the loss coefficient α, different fusion strategies (concatenation vs. attention), and variations in graph depth."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VPdOhT8I6n", "forum": "6kFtFYpFmS", "replyto": "6kFtFYpFmS", "signatures": ["ICLR.cc/2026/Conference/Submission11549/Reviewer_M6P4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11549/Reviewer_M6P4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761415247239, "cdate": 1761415247239, "tmdate": 1762922640414, "mdate": 1762922640414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "asdtBdjxYX", "forum": "6kFtFYpFmS", "replyto": "6kFtFYpFmS", "signatures": ["ICLR.cc/2026/Conference/Submission11549/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11549/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762931742201, "cdate": 1762931742201, "tmdate": 1762931742201, "mdate": 1762931742201, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel multi-modal knowledge graph to model the connections between visual features, attribute texts, and broader visual contexts. It introduces a method for constructing this graph and a knowledge graph-guided cross-modal hypergraph learning framework to enhance standard PAR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a hierarchical cross-modal hypergraph learning framework that effectively integrates visual and textual modalities through a multi-modal knowledge graph, offering a fresh perspective on semantic reasoning for PAR.\n2. Extensive experiments on five benchmark datasets show competitive and balanced results across metrics, demonstrating the robustness and general applicability of the proposed model.\n3. The model provides clear visualization and detailed ablation studies, supporting its design choices and offering interpretable insights into attribute–region relationships."}, "weaknesses": {"value": "1. The proposed method relies on predefined body regions (e.g., head, upper, lower, foot), which cannot adapt to variations in pose, occlusion, or body proportions. This rigid division may lead to semantic misalignment between attributes and regions, such as misclassifying “hand accessories” as belonging to the “head” region when a pedestrian raises their hand.\n2. Both the local and global hypergraphs are constructed using co-occurrence statistics and fixed similarity thresholds. Such a static design limits the model’s ability to capture rare or long-tail attributes; the model may underperform when handling uncommon attribute combinations or subtle semantic relations.\n3. The M2PA-KG is built mainly from dataset co-occurrence and textual descriptions without incorporating external semantic priors or commonsense knowledge. So the “knowledge guidance” is largely statistical rather than semantic. \n4. The paper does not provide a systematic study on key hyperparameters, such as the loss weight α (Eq. 13), similarity threshold τ (Eq. 15), or the number of visual samples per node. It is unclear how stable or robust the model’s performance is under different parameter settings."}, "questions": {"value": "Please refer to ‘Weaknesses’."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eACDmCJ5W8", "forum": "6kFtFYpFmS", "replyto": "6kFtFYpFmS", "signatures": ["ICLR.cc/2026/Conference/Submission11549/Reviewer_cjTk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11549/Reviewer_cjTk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575503106, "cdate": 1761575503106, "tmdate": 1762922639758, "mdate": 1762922639758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a knowledge graph-guided hierarchical cross-modal hypergraph learning framework (KGPAR), which constructs a multi-modal pedestrian attribute knowledge graph (M2PA-KG) integrating visual and textual information to systematically model high-order relationships among pedestrian attributes and between attributes and visual features. Furthermore, it designs both local and global hypergraph modules to achieve joint modeling of fine-grained and global semantics, thereby significantly improving the accuracy of pedestrian attribute recognition."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This method introduces textual information and establishes local–global mapping relationships between text and visual features, enhancing the fusion and utilization of multimodal information.\n\n2. Experiments conducted on five datasets validate the effectiveness and generalization capability of the proposed approach.\n\n3. The visualization heatmaps enable readers to intuitively perceive the correspondence between pedestrian attributes and image regions."}, "weaknesses": {"value": "1. The performance on the PETA and PA100K datasets is worrying, and on the MSP60K dataset, it is significantly worse than the LLM-PAR method. Furthermore, the comparison method on the MSP60K dataset is inconsistent with other datasets, which is not explained.\n\n2. Whether fine-tuning is needed when using CLIP for local alignment is not mentioned. Whether CLIP can be directly applied to the alignment of fine-grained text descriptions and local images is also not explained.\n\n3. The threshold in Local HyperGraph has not undergone ablation experiments, and no explanation or interpretability is provided for the threshold selection. This method relies on predefined body regions for local mapping, and alignment errors are inevitable due to different threshold choices. However, the paper does not explain whether such errors lead to a performance degradation, or by how much. Therefore, ablation experiments on the threshold are crucial.\n\n4. The $\\alpha$ parameter in Equation 13 has not undergone ablation experiments, and even its specific value is not explained."}, "questions": {"value": "Please refer to \"Weaknesses\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0wsVqBmTT1", "forum": "6kFtFYpFmS", "replyto": "6kFtFYpFmS", "signatures": ["ICLR.cc/2026/Conference/Submission11549/Reviewer_Enh1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11549/Reviewer_Enh1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11549/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762155375442, "cdate": 1762155375442, "tmdate": 1762922639209, "mdate": 1762922639209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}