{"id": "S0MRfeGr5d", "number": 12090, "cdate": 1758205641366, "mdate": 1759897534700, "content": {"title": "StatsMerging: Statistics-Guided Model Merging via Task-Specific Teacher Distillation", "abstract": "As large models are increasingly deployed across various tasks, the limited GPU memory available for storing and executing task-specific models presents a growing bottleneck. Model merging has emerged as a promising solution to accommodate multiple large models within constrained memory budgets. While traditional multi-task learning methods attempt to merge common layers, they require labor-intensive annotated labels and incur significant computational overhead. Recent merging techniques aim to address this issue by combining models at inference time; however, these approaches often rely on simplistic heuristics, ignore weight distribution characteristics, assume architectural identity, or require access to test samples to infer merging coefficients, thereby limiting generalization and scalability. We present StatsMerging, a novel lightweight learning-based model merging method guided by weight distribution statistics without requiring ground truth labels or test samples. StatsMerging offers three key advantages: (1) It uniquely leverages singular values from singular value decomposition (SVD) to capture task-specific weight distributions, serving as a proxy for task importance to guide task coefficient learning; (2) It employs a lightweight learner StatsMergeLearner to model the weight distributions of task-specific pre-trained models, improving generalization and enhancing adaptation to unseen samples; (3) It introduces Task-Specific Teacher Distillation for merging vision models with heterogeneous architectures, a merging training paradigm that avoids costly ground-truth labels by task-specific teacher distillation. Notably, we present two types of knowledge distillation, (a) distilling knowledge from task-specific models to train StatsMergeLearner; and (b) for the first time, distilling knowledge from models with different architectures prior to merging, following a distill-then-merge paradigm. Extensive experiments across eight tasks demonstrate the effectiveness of StatsMerging. Our results show that StatsMerging outperforms state-of-the-art techniques, achieving overall accuracies of 94.5% for Vision and 77.6% for NLP, while further exhibiting strong generalization to unseen tasks, and robustness to image quality variations.", "tldr": "We present a novel lightweight learning-based model merging method guided by weight distribution statistics without requiring ground truth labels.", "keywords": ["Model merging", "efficient neural network", "SVD", "single value decomposition", "statistics-guided", "knowledge distillation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/56774e5899a252e341b8195d0e159f94c7c65e82.pdf", "supplementary_material": "/attachment/64429c68323ec6baf0abdc62c9fcdc8301509cb1.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes StatsMerging, a method for merging pre-trained models using weight distribution statistics to guide the learning of merging coefficients."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The use of SVD singular values as a proxy for task/layer importance is a reasonable extension of existing merging techniques. This is also used in some recently published model merging methods, such as TSVM (task singular vector merging).\n2. Handling heterogeneous architectures via distill-then-merge is a practical contribution."}, "weaknesses": {"value": "Below are some weaknesses of the manuscript and suggestions for improve the manuscript:\n\n1. The averaged performance of individual fine-tuned models could be shown in Figure 4 using a vertical line.\n2. Lack comparison with recent state-of-the-art training-free model merging methods, such as TSVM (task singular vector merging) and RegMean++. \n3. Mirror typos: \n    - At line 371 on page 7, should \"MEMoE\" be \"WEMoE\"?\n    -  At line 226-235 on page 5, please verify that all instances of the symbols $\\sigma_{r}$ and $\\sigma’_{r}$ are used correctly.\n4. SVD rank is fixed at 3 without ablation—why not 1, 5, or 10? How sensitive is performance to rank choice?\n5. Weight statistics are used but no justification for why these capture \"task importance\" better than alternatives like Fisher information used in Fisher merging."}, "questions": {"value": "1.Why does the accuracy increase so dramatically at about 420 steps as shown in Figure 8?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FZuu9pnSbJ", "forum": "S0MRfeGr5d", "replyto": "S0MRfeGr5d", "signatures": ["ICLR.cc/2026/Conference/Submission12090/Reviewer_kzYd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12090/Reviewer_kzYd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761483022832, "cdate": 1761483022832, "tmdate": 1762923059586, "mdate": 1762923059586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces StatsMerging, a statistics-guided model merging approach designed to consolidate multiple task-specific deep models—primarily in vision and NLP domains—without requiring ground truth labels or access to test samples. The architecture leverages Singular Value Decomposition (SVD)-derived statistics (mean, variance, norm, top singular values) from model weights and trains a lightweight StatsMergeLearner (SML) to predict adaptive merging coefficients through a novel task-specific teacher distillation process. The method extends to heterogeneous architectures and is validated across eight vision and seven NLP tasks, showing improvements over several baselines in average accuracy and robustness to data/label noise."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The article demonstrates the absolute advantages of this method across various task sets and datasets through extensive experiments.\n- The method proposed in the article does not require manually annotated labels; instead, it leverages pseudo-labels generated by existing models. This eliminates the need for manual annotation and provides certain support for the subsequent expansion of different scenarios.\n- The structural design of SML is very simple with low training costs, and its computational cost is much lower compared to other model merging methods."}, "weaknesses": {"value": "- There is a lack of certain theoretical or experimental explanations. For instance, it does not clarify the reason for choosing rank=3 when performing SVD.\n- The experimental details are insufficient. For example, it fails to specify the exact amount of data used for training StatsMerging and StatsMerging++ respectively.\n- There may be issues with the experimental results. For instance, the results of LW StatsMerging++ in Table 3 and Table 7 of the Appendix are inconsistent."}, "questions": {"value": "- Why is a rank of 3 chosen for SVD? Is it necessary to add ablation experiments on rank to identify the tradeoff between the overhead caused by rank and performance?\n- Can a graph showing the relationship between the amount of training data and performance be provided? Additionally, can the specific amounts of data used for StatsMerging and StatsMerging++ be confirmed?\n- Can the reason for the inconsistent results of LW StatsMerging++ in Table 3 and Table 7 of the Appendix be clearly explained? Alternatively, can the corrected experimental results be provided after re-evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rUJiMY135G", "forum": "S0MRfeGr5d", "replyto": "S0MRfeGr5d", "signatures": ["ICLR.cc/2026/Conference/Submission12090/Reviewer_RxsX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12090/Reviewer_RxsX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761561357126, "cdate": 1761561357126, "tmdate": 1762923059163, "mdate": 1762923059163, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight, learning-based model merging method named StatsMerging. The core idea is to adaptively predict the merging coefficients based on the weight distribution information."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and easy to follow.\n\n2. The experiments demonstrate that the proposed method achieves promising results."}, "weaknesses": {"value": "1. **Unclear motivation.** The usage of weight distributions plays a central role in the proposed method and serves as its main motivation. However, the paper lacks empirical or theoretical evidence to support the importance or effectiveness of this design choice.\n\n2. **Limited technical contributions.** The use of knowledge distillation has been extensively studied in the context of model merging and other areas of machine learning. Similarly, learning adaptive merging coefficients has also been well explored in prior model merging research, which limits the novelty of the proposed approach.\n\n3. **Reliance on training data.** The proposed method requires access to additional training data, whereas data-free model merging approaches have already been widely studied. This reliance may weaken the practical advantage of the proposed method."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tZDw4biCj2", "forum": "S0MRfeGr5d", "replyto": "S0MRfeGr5d", "signatures": ["ICLR.cc/2026/Conference/Submission12090/Reviewer_5Rvs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12090/Reviewer_5Rvs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932692408, "cdate": 1761932692408, "tmdate": 1762923058459, "mdate": 1762923058459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces StatsMergeLearner, a network that predicts scaling coefficients for model merging from statistical features of model weights, trained using pseudo-labels generated by teacher models. It also proposes a Merge+Distill framework to homogenise model architectures—a prerequisite for merging—and reports strong gains over prior work across vision and language tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) String Performance: Consistently outperforms Adamerging and WEMoE on the reported benchmarks.\n\n(2) Practical pipeline: The proposed Merge+Distill framework provides a workable path to architectural homogenisation, enabling broader applicability of merging."}, "weaknesses": {"value": "(1) Limited task scale: Evaluation is restricted to the 8-task vision suite; it omits the widely used 14- and 20-task benchmarks from [1] on ViT-B/32, ViT-B/16, and ViT-L/14, which are important for understanding scaling with respect to task count and backbone size.\n\n(2) Baselines: Missing comparisons to recent data-free merging methods such as Isotropic Merging[2], TSV-M[3], and KnOTS[4].\n\n(3) Ablations: Task-level StatsMerging is introduced but not compared against layer-wise StatsMerging across settings, making it unclear what the difference in performance looks like.\n\nReferences:\n\n [1] Wang, Ke, et al. \"Localizing task information for improved model merging and compression.\" arXiv preprint arXiv:2405.07813 (2024).\n\n[2] Marczak, D., Magistri, S., Cygert, S., Twardowski, B., Bagdanov, A. D., & van de Weijer, J. (2025). No task left behind: Isotropic model merging with common and task-specific subspaces. arXiv preprint arXiv:2502.04959.\n\n[3] Gargiulo, A. A., Crisostomi, D., Bucarelli, M. S., Scardapane, S., Silvestri, F., & Rodola, E. (2025). Task singular vectors: Reducing task interference in model merging. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 18695-18705).\n\n[4] Stoica, G., Ramesh, P., Ecsedi, B., Choshen, L., & Hoffman, J. (2024). Model merging with svd to tie the knots. arXiv preprint arXiv:2410.19735."}, "questions": {"value": "(1) Effect of extra validation data: The jump from LW-StatsMerging to LW-StatsMerging++ is substantial. Do other adaptation methods (e.g., Adamerging) show similar improvements as validation data increases? Additionally, since StatsMerging does not introduce any new hyperparameters of its own, what is the role of validation data? \n\n\n(2) Data sources: What exact data are used for training StatsMergeLearner and for distillation? Is it the training samples from each task? Also, is the setup strictly unlabeled for both training and validation, or is any labelled data employed?\n\n\n(3) Training cost: Line 332 states StatsMergeLearner is trained for 500 epochs, which seems heavy compared to, e.g., Adamerging’s ~500 steps, which is okay since it also outperforms it, but should not be regarded as a lightweight method. Do (a) the number of models being merged and (b) backbone size (e.g., ViT-L/14) influence the number of training steps used to train the StatsMergeLearner?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GNb9B7oGrz", "forum": "S0MRfeGr5d", "replyto": "S0MRfeGr5d", "signatures": ["ICLR.cc/2026/Conference/Submission12090/Reviewer_ksSD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12090/Reviewer_ksSD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12090/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007984525, "cdate": 1762007984525, "tmdate": 1762923058020, "mdate": 1762923058020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}