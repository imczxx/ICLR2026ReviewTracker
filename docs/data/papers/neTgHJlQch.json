{"id": "neTgHJlQch", "number": 2426, "cdate": 1757082854385, "mdate": 1759898148641, "content": {"title": "Mind the gap: A method for evaluating and comparing regional knowledge in LLMs", "abstract": "Large Language Models (LLMs) achieve strong results on general knowledge benchmarks, yet their coverage of region-specific entities—particularly from Latin America—remains limited. To address this gap, we propose CHOCLO, an entity-centric methodology for evaluating LLM knowledge of culturally relevant entities in Latin America. The methodology extracts structured facts from domain-specific resources and organizes them into knowledge graphs spanning nine categories, resulting in more than 44,000 entities and 130,000 questions. Evaluation is carried out through two complementary strategies. The first computes factual scores using token overlap, embedding similarity, LLM-as-a-judge, and multiple-choice accuracy. The second trains probing models that predict these scores directly from LLM embeddings, enabling generation-free evaluation. Results consistently show a regional disparity: GPT-5 and GPT-3.5 score markedly lower on Latin American entities compared to the U.S. and Europe, while models such as Mistral, DeepSeek, and QWEN underperform across all regions. Category-level analysis further reveals that fauna, flora, and traditions are comparatively better represented, whereas public figures and objects show the largest deficits. CHOCLO thus exposes systematic disparities in how LLMs encode Latin American knowledge and provides a step toward culturally inclusive benchmarks that support fairer global evaluation.", "tldr": "This work introduces a benchmark and evaluation framework to measure how well LLMs understand Latin American entities using knowledge graphs and probing methods, revealing consistent performance gaps compared to other regions.", "keywords": ["benchmark", "nlp", "LLMs", "evaluation", "entity linking", "knowledge graph", "cultural entities"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5c1ca715b281fa5652c6cf1c9cc33de938c47894.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Benchmarks often measure factual knowledge of LLMs in high resource languages or regions or related to high frequency entities.  The paper proposes a method called CHOLCO for evaluating the knowledge of LLMs across entities related to traditions, public figures, food and geography. The paper uses Wikidata to extract triples across three regions: Latin America, Europe and United States and converts it into a question. They evaluate LLMs on these questions along with a probe based evaluation technique to understand what the LLM knows about these rare entities."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a scalable approach to building a benchmark for less known entities across different regional contexts by using Wikidata to source triples and converting them to templated questions. It provides a comparison of different models' performance across the three regions: United States, Europe and Latin America. \n\n2. The paper compares the performance of different models on factual information across regions and highlights that models perform worse on information related to Latin America."}, "weaknesses": {"value": "1. There is no clear basis for the evaluation technique used in Sec 3.2.1 where the authors compute the LLM performance on their benchmark questions based on embedding similarity, lexical overlap, LLM as a judge and multiple choice accuracy. Using LLM as a judge would suffice in this scenario and it is not clear what value the other methods add. Methods such as lexical overlap are potentially noisy as LLMs tend to be verbose and the expected answer is usually a single location. \n\n2. Sec 3.1 talks about the properties used for building the dataset: \"country of origin (property P495), country of citizenship (P27), place of birth (P19), territorial location (P131), and geographic coordinates (P625).\" This seems to be a very limited set of properties which would always cause the label to be a location. This limits the diversity of the answers in the benchmark. \n\n\nPresentation: \n1. In Table 2, QWEN should be replaced with the entire model name."}, "questions": {"value": "1. For the evaluation setup, the authors evaluate GPT 3.5 Turbo and GPT-5 Mini, but not GPT-4 or GPT4o. Is there any specific reason for this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1EPjnrxr0H", "forum": "neTgHJlQch", "replyto": "neTgHJlQch", "signatures": ["ICLR.cc/2026/Conference/Submission2426/Reviewer_VdSb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2426/Reviewer_VdSb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794878234, "cdate": 1761794878234, "tmdate": 1762916233648, "mdate": 1762916233648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes regional knowledge of Latin America in LLMs. Specifically, this paper first extracts structured facts from domain-specific resources and constructs a knowledge graph containing 44,000 entities spanning 9 categories. Using this knowledge resource, this paper proposes CHOCLO, an entity-centric methodology for evaluating LLM knowledge of culturally relevant entities in Latin America. It evaluates the regional knowledge in LLMs using several techniques, including token overlap, embedding similarity, LLM-as-a-judge, and multiple-choice accuracy. This paper also trains a probing model to evaluate the factual score directly from LLM representations. This paper finds several interesting conclusions, such as most LLMs underperform in  Latin American knowledge."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic is interesting and meaningful to the community. Studying LLMs’ coverage of different regional knowledge is important for the broad applications of LLMs.\n2. The work presents a systematic analysis and comprehensive experiments. The experimental results reveal that current LLMs underperform on Latin American knowledge. This provides some guidance and insights for improving LLM knowledge coverage and supports the development of more diverse LLMs and broader applications for people all over the world."}, "weaknesses": {"value": "1. The authors construct a knowledge graph, but there are existing resources (e.g., Wikidata). The paper should analyze whether the constructed knowledge graph adequately captures Latin American knowledge. And what is the advantage compared to existing resources? Is this knowledge graph covering more Latin American knowledge?\n2. The methods used for experimental analysis are mostly existing techniques, which limits the paper’s technical novelty.\n3. The authors should evaluate the reliability of their evaluation approaches. For example, they can analyze the correlation between each evaluation method and human judgments, to validate the reliability of their evaluation methods.\n4. A more fine-grained analysis specific characteristics of Latin American knowledge is needed. The authors should discuss how Latin American knowledge differs from other regional knowledge and why LLMs underperform, such as insufficient training data or other factors, to guide further LLM development."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ttskrApZX9", "forum": "neTgHJlQch", "replyto": "neTgHJlQch", "signatures": ["ICLR.cc/2026/Conference/Submission2426/Reviewer_PyBt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2426/Reviewer_PyBt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905607456, "cdate": 1761905607456, "tmdate": 1762916233493, "mdate": 1762916233493, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CHOCLO, a framework to evaluate regional and culturally grounded knowledge about underrepresented regions in Large Language Models (LLMs). To do so, the authors curated a dataset with ~44k entities and ~130k questions, spanning across different categories adapted from CVQA: dish, flora, fauna, geography, object, public figure, tradition; ensuring broad thematic coverage while capturing cultural patterns. The authors argue that existing mainstream datasets are skewed, hence LLMs lack cultural knowledge, and therefore focus the analysis on the coverage of entities related to Latin America. CHOCLO uses structured knowledge graphs (KGs) to evaluate factual knowledge at the entity level via four complementary scoring methods, followed by a probing model to predict factual knowledge scores. Experiments show that GPT-3.5, GPT-5, Mistral, DeepSeek, and Qwen demonstrate performance disparities specifically with entities related to LATAM compared to the USA and Europe."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important aspect of LLMs - information inclusivity. \n2. The evaluation pipeline, containing structured KG-based QA and probing with 4 scoring methods, offers different aspects of understanding of factuality. \n3. The paper presents a detailed quantitative analysis at - cross-region and category level. The results confirm the disparities in information content in LLMs."}, "weaknesses": {"value": "1. The dataset curated for this evaluation relies entirely on Wikidata as the primary source of information. However, there is inherent coverage bias in Wikidata on region-specific knowledge. No analysis has been provided on that.\n2. The proposed framework is not technically novel. It combines a couple of existing, well-established methods to evaluate the region-specific LLM knowledge. Moreover, the semantic meaning of the predicted scores is not clear. It is missing statistical significance tests or NLI tests for a better understanding of predicted scores. \n3. The paper emphasises cultural knowledge inclusion in the LLMs, but considers LATAM as a homogenous region, hence also increasing the risk of over generalisation based on languages/linguistic features. The work would have benefited from some analysis based on that.\n4. It would be nice to have the framework tested out for CultureBench"}, "questions": {"value": "1. What is the impact of Wikidata coverage bias on your framework, and how to deal with it? \n2. How do you ensure the quality of the extracted triple? \n3. How do you find the agreement between the different scoring functions?\n4. Could the probing scores increase biases instead of mitigating?\n5. How do you avoid the overgeneralisation of the analysis done based on the assumption that LATAM is a homogeneous region?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ve1oPYtQlt", "forum": "neTgHJlQch", "replyto": "neTgHJlQch", "signatures": ["ICLR.cc/2026/Conference/Submission2426/Reviewer_1sck"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2426/Reviewer_1sck"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931291699, "cdate": 1761931291699, "tmdate": 1762916233377, "mdate": 1762916233377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}