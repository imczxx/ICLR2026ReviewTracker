{"id": "ND0q3wjNgW", "number": 13158, "cdate": 1758214272591, "mdate": 1759897459950, "content": {"title": "Fingerprinting LLMs via Prompt Injection", "abstract": "Large language models (LLMs) are often modified after release through post-processing such as post-training or quantization, which makes it challenging to determine whether one model is derived from another. Existing provenance detection methods have two main limitations: (1) they embed signals into the base model before release, which is infeasible for already published models, or (2) they compare outputs across models using hand-crafted or random prompts, which are not robust to post-processing. In this work, we propose LLMPrint, a novel detection framework that constructs fingerprints by exploiting LLMs’ inherent vulnerability to prompt injection. Our key insight is that by optimizing fingerprint prompts to enforce consistent token preferences, we can obtain fingerprints that are both unique to the base model and robust to post-processing. We further develop a unified verification procedure that applies to both gray-box and black-box settings, with statistical guarantees. We evaluate LLMPrint on five base models and around 700 post-trained or quantized variants. Our results show that LLMPrint achieves high true positive rates while keeping false positive rates near zero.", "tldr": "", "keywords": ["Large Language Models", "Fingerprinting", "LLM Provenance Detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b7ebf54cdde83bd0d88c42075f7afdaece6d985.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "LLMPrint aims to detect provenance of a given model passively, i.e., it does not require injecting or modifying the base model. For many token pairs ($w_j^+$, $w_j^-$), the authors optimize an injected prompt under the template “Randomly output a word from your vocabulary”, so that the base model mildly prefers $w_j^+$ for the first generated token, positioning each prompt near a decision boundary yet far from other tokens. The optimization process uses greedy coordinate gradient from LLM-attacks by Zou et al. The Verification compares the base’s bit string with a suspect model’s bit string (from gray-box log-probs or black-box sampling) using a statistically calibrated threshold. The paper reports experiments on five base models and different post-trained or quantized methods. \n\nThe method has made some strong assumptions, such as the base model and suspect models sharing the same token preferences under the fingerprint prompts. Instead of sampling from random prompts, the LLMPrint defines injected tasks where the model is enforced to present a preference between a selected token pair."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- Provenance verification is increasingly important as open-weight models proliferate and are post-processed (SFT/LoRA/quantization). This work advances passive testing without modifying the base model, a practical setup. Also, the assumption of black-box access to the suspect model is very practical. \n\n- A comprehensive collection of very recent works in provenance detection, demonstrating a community interest, also motivating the work very well."}, "weaknesses": {"value": "- The method’s decision-boundary intuition and the loss function (uniqueness + robustness) are not very clearly motivated. Is it universally agreed that the base model and suspect models share the same token preferences under the fingerprint prompts? Even two different base models can share the same preferences, right? Also, I am not convinced that \"if a suspect model reproduces these preferences under the same fingerprint prompts, it is likely derived from the base model. I understand, \"we introduce a statistical baseline that captures the expected accuracy of negative suspect models\" which is related to solving this concern, but then the \"an empiricial distribution\" can be stochastic, as it varies along with the evaluated model sets. \n\n- The core searching/optimization process uses GCG, which is already a widely-used method from 2023; I'm not sure about the technical novelty in this paper. The framework is from LLMmap and CoTSRF, but using GCG to find more effective prompts. The improved performance is built upon extra computing. \n\n- The paper curates validation negatives and many test negatives but does not show open-world behavior where the suspect is none of the known families (or a strong instruction-tuned model that shares tokenizer/architecture). The strong near-zero FPRs could partly reflect distributional overlap between base/negatives seen during prompt optimization.\n\n- \"while negative suspect models are post-trained or quantized versions derived from other base models\" my understanding is that all negative suspect models are from even different architectures and sizes? If so, the logit dimensions are already different."}, "questions": {"value": "- There are five seed base models in total; all evaluation pairs are built upon the five?\n\n- What is the intuition of \"located near the decision boundary of the base model\", when the model has a very \"flat\" predictive distribution (high entropy) over the vocabulary?\n\n- except for \"DeepSeek-R1-Distill-Qwen-1.5B\", it is unclear whether all other base models are instruction versions or base versions. Will this impact the result? e.g., instruction models as base models are more difficult to detect?\n\n- A distilled version of the model is not taken as a suspect model, right? Like, distilled DeepSeek is not taken as a suspect model of llama?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QUJCwQCobr", "forum": "ND0q3wjNgW", "replyto": "ND0q3wjNgW", "signatures": ["ICLR.cc/2026/Conference/Submission13158/Reviewer_iVDu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13158/Reviewer_iVDu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760478342226, "cdate": 1760478342226, "tmdate": 1762923871608, "mdate": 1762923871608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a fingerprinting method via prompt injection to audit whether a suspect model is positive or negative with respect to a base model. Specifically, the authors optimize a suffix in the prompt to trigger a specific preference in the first-token generation process. They conduct extensive evaluations on over 700 post-trained or quantized model variants."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Extensive evaluation, especially using so many model variants collected from HuggingFace.\n- Strong performance.\n- Well-structured paper."}, "weaknesses": {"value": "- The paper claims novelty in using prompt injection for LLM fingerprinting, but this framing feels overstated. From the perspective of techniques, LLMPrint operates almost identically to prior adversarial optimization methods such as TRAP (and other adversarial attacks based method [a,b]), which also learn suffixes to control model behavior. The difference lies in controlling token-level preferences instead of full outputs—seems more like a minor objective change than a conceptual innovation. Moreover, prior adversarial methods could also be viewed as a form of prompt injection, since they induce non-native behaviors and override natural outputs. The established literature has described these as adversarial attacks, which is technically more accurate and avoids confusion with the security-oriented notion of prompt injection. This reframing seems to be a strategic rebranding of a well-understood technique for the sake of novelty. I would also encourage the authors to provide a more in-depth and technically grounded discussion in the related work section to clearly articulate how their approach truly differs from prior adversarial attack based methods.\n\n- The authors claim their method's uniqueness comes from placing prompts near the decision boundary. However, this concept of near is ill-defined and controlled by a manually tuned hyperparameter \\alpha (e.g., 0.5). How near is enough to be unique, yet far enough to be robust? The paper provides no theoretical analysis or empirical study on the sensitivity of \\alpha. Without this, the central claim becomes arbitrary. It's possible the method works not because of the property of the decision boundary, but simply because the optimization finds a local minimum that happens to be transferable to its model variants, and the decision boundary is just a post-hoc rationalization. \n\n- Although the experimental results show that LLMPrint substantially outperforms TRAP, the paper does not clearly explain why this is the case. Is the improvement solely due to controlling behavior at the first-token level, or are there deeper factors that make the proposed optimization more effective? It would be good if the authors provide stronger theoretical or analytical insights to justify why their method is inherently better than other adversarial-attack-based approaches.\n\n-  The failure analysis in Section 5.3 attributes detection failures to a drop in the suspect model's benchmark performance. While the data shows a correlation, this attribution of causality is not sufficiently proven. The authors do not rule out confounding variables or the possibility that their fingerprints are simply not robust to the specific fine-tuning methods that also happen to degrade benchmark performance.\n\n[a] UTF: Undertrained Tokens as Fingerprints A Novel Approach to LLM Identification\n\n[b] ProFLingo: A Fingerprinting-based Intellectual Property Protection Scheme for Large Language Models"}, "questions": {"value": "- What are the differences between the proposed method and adversarial attack based methods?\n- Why is the proposed method inherently better than other methods?\n- Are the fingerprint prompts really near the decision boundary?\n\nPlease also see the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ijkQ7v6mVa", "forum": "ND0q3wjNgW", "replyto": "ND0q3wjNgW", "signatures": ["ICLR.cc/2026/Conference/Submission13158/Reviewer_n5a1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13158/Reviewer_n5a1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761579373084, "cdate": 1761579373084, "tmdate": 1762923871237, "mdate": 1762923871237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method to address the LLM provenance detection problem, where post-processing, such as post-training or quantization, makes it challenging to determine if a suspect model is derived from a known base model. The authors introduce LLMPrint, a passive detection framework designed to be verifiable under both gray-box and black-box access and robust to these post-processing modifications. The core mechanism involves exploiting the base model's inherent vulnerability to prompt injection to generate a set of \"fingerprint prompts\". Each prompt is optimized using an objective function to enforce a consistent token preference between a specific pair of tokens. The evaluation on five base models and around 700 suspect variants shows the method achieves high true positive rates while keeping false positive rates near zero."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Promising results.\n- Support both gray-box and black-box verification.\n- Large-scale evaluation."}, "weaknesses": {"value": "- **On the Novelty of the Proposed Method:** The paper's core contribution appears to be incremental. The method uses GCG to optimize prompts that elicit specific model preferences, which is conceptually similar to TRAP's optimization, perhaps resembling a special case of TRAP focused on single-token outputs. Furthermore, the loss function designed to find prompts near the \"decision boundary\" shares a strong resemblance to the objective in IPGuard. Notably, the authors' claim that IPGuard fails completely (0% TPR) due to lacking a robustness objective is surprising, as the original IPGuard paper does include a term to ensure robustness.\n\n- **On the Necessity of the Token-Pair Framework:** The paper's design is centered on pairs of tokens. However, the necessity of the negative token is not fully ablated. It may be beneficial for the authors to conduct an ablation study that removes the negative token.\n\n- **On the Limited Threat Model:** The paper's robustness claims are primarily evaluated against post-training and quantization. This overlooks other common and practical scenarios that can significantly alter an LLM's behavior. It is unclear how LLMPrint would perform against suspect models modified by techniques like model merging and distillation, or those deployed with different system prompts or even non-default sampling settings like a different temperature. It may be better for the authors to evaluate the fingerprint's robustness against these factors to validate the method's practical utility.\n\n- **On the Limited Practical Application:** The proposed method requires full white-box access to the base model to perform the GCG optimization. For the auditing and provenance tasks involving closed-source models (e.g., determining if a third-party API is illicitly using GPT-4), this method might not be feasible.\n\n- **On the Unexplained Bypassing of SOTA Detectors:** The paper claims that its fingerprint prompts, optimized using standard GCG, can bypass two SOTA prompt injection detectors with high success rates. This is a counter-intuitive and significant claim, as GCG-optimized prompts are often syntactically anomalous and should be statistically distinct from natural language. This surprising result may require a more detailed explanation and analysis to build confidence in the finding.\n\n- **On the Questionable Failure Analysis:** The failure analysis in Section 5.3 posits that misdetected positive models (false negatives) are a result of their own performance degradation on general benchmarks like MMLU. This line of reasoning may be problematic. In many real-world scenarios, a stolen model would be fine-tuned for a specific domain (e.g., legal or medical), which could naturally lower its score on general benchmarks while increasing its specialized value. This analysis may inadvertently suggest that LLMPrint is only effective at detecting near-identical copies of the base model and fails against models that have been meaningfully adapted.\n\n- **On the Excessive Query Overhead and Potential for Unfair Comparison:** The proposed method requires 300 prompts, each queried 100 times in the black-box setting, for a total of 30,000 queries per detection. This represents a very high overhead. This also raises concerns about the fairness of the baseline comparison, as TRAP used 1,000 total queries and LLMmap used only 8 prompts. Based on the paper's own ablation study (Appendix A.5), the TPR of LLMPrint appears to be very low when the number of queries (T) is small (e.g., T=10). This strongly suggests that the method's high reported performance may be a product of this massive query budget rather than a fundamentally more effective fingerprinting design."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7cQ5ykSpLc", "forum": "ND0q3wjNgW", "replyto": "ND0q3wjNgW", "signatures": ["ICLR.cc/2026/Conference/Submission13158/Reviewer_EvtS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13158/Reviewer_EvtS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639360524, "cdate": 1761639360524, "tmdate": 1762923870672, "mdate": 1762923870672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the provenance verification challenge of Large Language Models (LLMs) and proposes LLMPrint, a robust fingerprinting framework based on prompt injection and token attribution signals. \n\nUnlike traditional watermarking or output-based fingerprints, LLMPrint leverages the inherent prompt injection vulnerability of LLMs to optimize fingerprint trigger words that induce stable token preferences unique to each base model. These preferences are embedded as fine-grained token attribution distributions, enabling ownership verification through analysis of attribution responses (e.g., gradients or attention weights). \n\nThe method provides white-box verification that remains effective under fine-tuning, quantization, and LoRA adaptation, preserving both model quality and fingerprint persistence. \n\nExperiments on Llama-2, Mistral, and Falcon demonstrate that the fingerprints maintain both model uniqueness and robustness to post-processing operations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel use of token attribution for model fingerprinting.\n2. Empirical robustness to moderate fine-tuning and retraining.\n3. No performance degradation due to output modification.\n4. Transparent technical design and reproducibility plan.\n5. White-box verification offering strong ownership validation."}, "weaknesses": {"value": "1. Theoretical analysis is limited, Lack of theoretical grounding for robustness under strong distribution shifts.\n2. White-box assumption reduces practical applicability for closed models.\n3. Lack of quantitative fingerprint distinctiveness evaluation.\n4. Attribution extraction incurs significant computational cost for large models.\n5. Fine-tuning robustness tested only on moderate tasks, lacking domain-transfer evaluation.\n6. Limited evaluation under adversarial settings (e.g., targeted evasion of fingerprinting)."}, "questions": {"value": "1.Could fingerprint prompts be detected or removed by an adversary already aware of the LLMPrint mechanism? Please discuss the method’s resilience against prompt-based fingerprint removal attacks.\n2.Could the proposed approach be extended to identify model extraction or partial model stealing, where only subsets of weights or behaviors are replicated?\n3.How does the method perform under adversarial fine-tuning specifically designed to erase fingerprints? Could similar robustness be maintained when scaling to larger models (e.g., 7B, 13B)?\n4.Could instruction-tuning shifts or alignment artifacts (e.g., from SFT or RLHF) affect the stability of attribution fingerprints, and how might this be mitigated?\n5.Fingerprint suggestion word optimization currently relies on 1,000 GCG iterations. Could the authors analyze the trade-off between iteration count (e.g., 200/500/1500) and fingerprint strength (TPR) or runtime to verify whether 1,000 is the optimal balance? A comparison with other optimization algorithms (e.g., gradient descent) would help justify the use of GCG.\n6.Do the fingerprint hints detected by DataSentinel or PPL-Detector share common linguistic or statistical characteristics (e.g., semantic incoherence, abnormal perplexity)? Could these hints be refined to improve semantic coherence and bypass rate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wRD4HECZXA", "forum": "ND0q3wjNgW", "replyto": "ND0q3wjNgW", "signatures": ["ICLR.cc/2026/Conference/Submission13158/Reviewer_PWA5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13158/Reviewer_PWA5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13158/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963506819, "cdate": 1761963506819, "tmdate": 1762923870306, "mdate": 1762923870306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}