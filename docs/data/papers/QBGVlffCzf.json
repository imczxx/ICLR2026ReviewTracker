{"id": "QBGVlffCzf", "number": 15401, "cdate": 1758250976595, "mdate": 1759897309429, "content": {"title": "Multi-Agent Reinforcement Learning for Heterogeneous Large-Scale Blotto Games", "abstract": "The Colonel Blotto game, a classical resource allocation model in game theory, presents significant computational challenges when extended to large-scale heterogeneous settings due to combinatorial strategy space explosion and agent heterogeneity. We introduce a multi-agent reinforcement learning framework that effectively solves ultra-large-scale heterogeneous Blotto games involving thousands of agents and dozens of battlefields. Our approach formulates the problem as a decentralized partially observable Markov decision process and proposes a dual-path algorithmic architecture: Group-Mix enables precise credit assignment through type-aware value decomposition, while H-PPO ensures training stability via hierarchical curriculum learning. Theoretical analysis establishes the viability of centralized training with decentralized execution for Blotto games and demonstrates the strategy space compression achieved through type-sharing mechanisms. Experimental results validate that our method maintains stable learning and generates effective strategies in complex scenarios with 1,000 agents and 20 battlefields, demonstrating practical efficacy in ultra-large-scale settings previously considered computationally intractable.", "tldr": "An  multi-agent reinforcement learning framework that solves large-scale Blotto games via parameter sharing and hierarchical observation design.", "keywords": ["Multi-Agent Reinforcement Learning;Blotto Game;Large-Scale Resource Allocation;Value Function Factorization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c56dbc49c91a1658e18e5b718a4ce376e3a911b5.pdf", "supplementary_material": "/attachment/7d89173a71d1853ab951edc4bdd1a961a94747c8.zip"}, "replies": [{"content": {"summary": {"value": "This paper applies multi-agent reinforcement learning algorithms under the CTDE paradigm to large-scale heterogeneous Colonel Blotto game problems by designing a dual-path algorithmic framework combining Group-Mix and H-PPO. To validate this approach, the paper develops reinforcement learning environments for Colonel Blotto games at various scales. Experiments across different scenarios demonstrate that the proposed method exhibits competitive performance in complex, large-scale settings."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper introduces and proposes a multi-agent reinforcement learning environment for the large-scale heterogeneous Colonel Blotto game, which contributes to solving the Colonel Blotto problem.\n2. The paper successfully applies multi-agent reinforcement learning algorithms to the Colonel Blotto game and proposes two methods: Group-Mix and H-PPO."}, "weaknesses": {"value": "1. The novelty of the paper is limited. The distinctions between Group-Mix and QMIX, as well as between H-PPO and PPO, are not clearly articulated. Furthermore, the convergence proof presented does not seem to be an original contribution of this paper.\n2. The experimental section lacks critical details necessary for reproducibility. This includes the specific hyperparameter settings, the number of random seeds used for statistical significance, a complete list of baseline methods for comparison, and a thorough ablation study.\n3. The paper should introduce a more comprehensive introduction to the proposed Colonel Blotto game benchmark. Key details are missing, such as the performance of classical algorithms on this benchmark and a discussion of possible environment variants."}, "questions": {"value": "1. Could the authors clarify the primary distinctions between the \"single decision-maker\" model and multi decision maker model discussed in the paper? Furthermore, is the proposed method capable of solving traditional \"single decision-maker\" problems?\n2. It would be valuable to see a comparative analysis of the algorithm's performance and computational efficiency against other baseline methods, such as QMIX, MAPPO, and MADDPG.\n3. Could the authors discuss whether single-agent algorithms (e.g., DQN, PPO) can be directly applied to the multi-agent reinforcement learning environment proposed in this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i9IhkGSqW4", "forum": "QBGVlffCzf", "replyto": "QBGVlffCzf", "signatures": ["ICLR.cc/2026/Conference/Submission15401/Reviewer_LRob"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15401/Reviewer_LRob"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652325023, "cdate": 1761652325023, "tmdate": 1762925679023, "mdate": 1762925679023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates a solution for heterogeneous larg-scale blotto games. Approaching the problem as a decentralized POMDP problem, this paper incorporates ideas such as Group-Mix, H-PPO, and hierarchical curriculum learning. The proposed approach was tested in scenarios up to 1K agents and 20 battlefields."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper introduces the Blotto games quite well. It also has a good characterization of challenges involved and previous approaches to the problem."}, "weaknesses": {"value": "Overall, I found the description of the algorithm rather confusion. For example,What's the Relationship between GRU_theta_shared and MLP_theta_m? Is the Q values calculated from the Group-Mix Value Function Decomposition used for training of the H-PPO? Where does two pathways merge and interact with each other to make a decision? While the Figure 2 was helpful, Figure 1 was more confusing than it helped me to understand the algorithm. I also think that having a pseudocode algorithm would be helpful too.\n\nIn this regard, I think there's some room for improvement in the figures. While not critical, it's quite better to change lines if a word does not fit into a box in the figures. Center-aligning also helps since letters too close to box outlines can be difficult to read. Ideally, best figures are figures that so clearly describes the proposed approach that the reader should have a grasp of what the paper is about, even if they saw the figures before reading through the whole text.\n\nI also think the experiments are bit incomplete. It is nice that the authors tested their algorithm on various scale of Blotto games to get a sense on scalability. However, there are no comparison with other state-of-the-art algorithms in the field, so it is difficult to measure the contribution of this approach."}, "questions": {"value": "1. A better description of the algorithm. Currently, it is difficult to see the structural / theoretical contribution of the algorithm as it is difficult to understand how the algorithm is structured and works\n\n2. Comparison with the baseline to better understand the contribution of this work to existing works of Colonel Blotto Games."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yBHnX3VoYS", "forum": "QBGVlffCzf", "replyto": "QBGVlffCzf", "signatures": ["ICLR.cc/2026/Conference/Submission15401/Reviewer_W453"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15401/Reviewer_W453"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955142164, "cdate": 1761955142164, "tmdate": 1762925678665, "mdate": 1762925678665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper approaches solving large-scale heterogeneous Colonel Blotto games using a centralized training, decentralized execution framework. Using their proposed Group-Mix and Hierarchical PPO method, they claim strong performance on large versions of the game."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors consider solving large-scale extensions of Blotto."}, "weaknesses": {"value": "This paper holds very little resemblance to research works typically accepted to major machine learning conferences.\n\n- The introduction doesn't introduce the contributions or the proposed approach. The actual purpose of the paper is introduced after related works. This structure seems very odd and should be streamlined.\n- The related work section provides only high-level positioning. It does not clearly spell out how prior approaches address (or fail to address) the considered problem, nor how the proposed method differs from and builds on them.\n- Although core methods are cited, the paper doesn’t make explicit which components of the proposed approach derive from which prior works or how they are adapted.\n- There is a single experiment with zero ablations.\n- The experimental analysis is extremely insufficient, being a single paragraph.\n- There are no baselines for the proposed algorithm. No point of reference for whether this algorithm solves something better or worse than other methods.\n- There are no training details provided.\n- The acknowledgement section still contains the template paper instructions."}, "questions": {"value": "What is a baseline method that could be used to measure the relative performance of this method to prior work? I suggest comparing to a baseline to help readers understand when and why your method is preferable to prior work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y231tTITmD", "forum": "QBGVlffCzf", "replyto": "QBGVlffCzf", "signatures": ["ICLR.cc/2026/Conference/Submission15401/Reviewer_w1yK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15401/Reviewer_w1yK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956522674, "cdate": 1761956522674, "tmdate": 1762925678241, "mdate": 1762925678241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies large-scale, heterogeneous Colonel Blotto games by casting them as cooperative Dec-POMDPs and proposing a dual-path MARL framework: Group-Mix for type-aware value decomposition and H-PPO for stable, scalable training. Theoretically, the authors argue CTDE applicability under an IGM-style monotonic factorization and claim a strategy-space compression from $O(|A|^K)$ to $O(|A|^{|M|})$ through type sharing. Empirically, they formalize the Large-scale Heterogeneous Blotto Game and runs up to 1,000 agents and 20 battlefields."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly reformulates heterogeneous Blotto as a Dec-POMDP and enumerates the sources of difficulty (combinatorial joint action space, credit assignment under type heterogeneity, coordination effects). The LHBG definitions (agents, types, local observations, rewards) are easy to follow.\n2. Group-Mix explicitly aggregates same-type features before hypernetwork mixing, aiming to align the mixer with heterogeneity structure and improve credit assignment, which is an intuitively appealing inductive bias for large populations.\n3. The type-sharing mechanism (one policy/value head per type) is a sensible, principled way to reduce degrees of freedom and training variance for thousands of agents, and the paper articulates its impact on the strategy space."}, "weaknesses": {"value": "1. Single environment: the current evaluation is conducted in a single environment, which may limit the perceived generality and external validity of the results. To better demonstrate robustness and transfer, it would be helpful to include widely used MARL benchmarks and diverse tasks—for example, several maps from SMAC [1], as well as scenarios from GRF [2] and Hanabi [3]. \n2. Unclear improvements over existing MARL algorithm: the paper would benefit from a clearer articulation of how the proposed method advances beyond established MARL approaches. In particular, classical baselines such as MAPPO [4] (centralized critic with decentralized policies) and QMIX [5] (monotonic value factorization) set strong reference points. Please make explicit what design choices are new relative to these methods, which concrete challenges they target and why those choices should help in theory or practice. \n3. Limited experiments: at present, there are no baseline comparisons or ablation studies, which makes it difficult to judge effectiveness and attribute gains to specific design elements. Including competitive baselines (e.g., MAPPO, QMIX) and well-scoped ablations would substantially improve the evidential weight of the claims. What's more, there is no description of the network architecture, optimization details, or hyperparameters.\n\n[1] Samvelyan, Mikayel, et al. \"The starcraft multi-agent challenge.\" arXiv preprint arXiv:1902.04043 (2019).\n\n[2] Kurach, Karol, et al. \"Google research football: A novel reinforcement learning environment.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 04. 2020.\n\n[3] Bard, Nolan, et al. \"The hanabi challenge: A new frontier for ai research.\" Artificial Intelligence 280 (2020): 103216.\n\n[4] Yu, Chao, et al. \"The surprising effectiveness of ppo in cooperative multi-agent games.\" Advances in neural information processing systems 35 (2022): 24611-24624.\n\n[5] Rashid, Tabish, et al. \"Monotonic value function factorisation for deep multi-agent reinforcement learning.\" Journal of Machine Learning Research 21.178 (2020): 1-51."}, "questions": {"value": "1. Evaluation environments: There are many established MARL environments; could the authors validate their method in a broader range of scenarios, such as Hanabi or SMAC? This would better demonstrate the generality and robustness of the proposed algorithm.\n2. Baseline comparison: Could the authors compare their method with classical MARL algorithms such as MAPPO and QMIX to verify its effectiveness? Including such baselines would help clarify the algorithm’s relative advantages.\n3. Experimental details: Could the authors provide more detailed descriptions of the experimental setup, including the network architecture and hyperparameter choices? These details are essential for reproducibility and fair comparison.\n4. Opponent setup: Since the game involves two agents in an adversarial setting, what algorithm is used to control the opposing agent in the main experiments? Clarifying this is important for understanding the evaluation protocol and the fairness of the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tcE6NcwyPU", "forum": "QBGVlffCzf", "replyto": "QBGVlffCzf", "signatures": ["ICLR.cc/2026/Conference/Submission15401/Reviewer_3V1X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15401/Reviewer_3V1X"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15401/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966644905, "cdate": 1761966644905, "tmdate": 1762925677744, "mdate": 1762925677744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}