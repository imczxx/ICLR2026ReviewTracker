{"id": "ftANj24sfU", "number": 3311, "cdate": 1757398895347, "mdate": 1763725678333, "content": {"title": "Large-Scale Molecular Dynamics Simulation: Direct Interatomic Modeling with Dilated Message Passing", "abstract": "Large-scale molecular dynamics simulation is essential in understanding chemical and biological processes, necessitating the accurate and efficient modeling of interatomic interactions. Existing learning-based methods generally are based on message passing mechanisms; they either are not scalable or are too coarse to offer accurate modeling. We propose a new message passing framework that can effectively and efficiently model interatomic interactions for simulating large-scale molecular dynamics at full atomic resolution. Specifically, our framework is stacked with a sequence of message passing neural network layers, each realizing the message passing over a distinct and dilated star-structured path. These star-structured paths are constructed progressively along dilated regions to capture the distance-dependent interactions. The crux of our framework is that it resolves the problem of dense interatomic interactions of large-scale atomic systems with sparser and region-based message passing graphs. We evaluate the framework on four benchmarks: the MD22 (molecules with 42–370 atoms), the Chignolin (a 166-atom protein featuring diverse conformations), the AdK dataset (a protein trajectory with up to 3,000 atoms), and the MISATO dataset (over 10,000 heterogeneous protein-ligand complexes, including systems with up to 40,000 atoms). Comprehensive evaluations demonstrate that our approach delivers state-of-the-art performance overall across various benchmarks. In particular, it is the first learning-based method to achieve atomic-level accuracy in protein-ligand dynamics simulation while preserving computational efficiency.", "tldr": "A new machine learning framework overcomes scalability-accuracy trade-offs in large-scale molecular dynamics by employing dilated star-structured message-passing.", "keywords": ["Molecular Dynamics", "Graph Neural Network", "Machien Learning Force Field"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ba883bda3051397a8f9160ee2e450bbeaa64fe2.pdf", "supplementary_material": "/attachment/b043ff38f51c76ffd8e7564c08a48d2ac11c4cb4.zip"}, "replies": [{"content": {"summary": {"value": "The study shows a dilated operation in message passing and benchmarked it on several datasets for energy and force predictions. Overall speaking, the design lacks novelty; the experiments cannot fully support the conclusion; some results are suspicious; many professional terms are mistanken used; and the paper is poor written. It's far from the standard of ILCR."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. reasonable overiview and introduction on the background and latest progress in this field\n2. detailed hyperparametes are shown in supplementary materials."}, "weaknesses": {"value": "1. Lack of novelty\nThe dilated operation presented in this study is too simple and lacks novelty. Almost operations and equations in the model design are from ViSNet paper. The dilation on radius is just a tiny trick insteaf of a novel design as the dilation employed in CNN or other models has been demonstrated more than 10 years before.\n\n2. Suspicous results\nTable 1. There is no evaluation on MISATO dataset in the paper (Wang et al., 2024a). Where the results in \"AIMD\" and \"CHARMM27\" rows come from? \nTable 2. The authors claimed all the other models suffered from OOM issue. As PaiNN and EGNN are smaller than DKMP, why they all failed?\nTable 1 and Table 3. Why the compared models are inconsistent?\nTable 1 and Table 3. Why neglect the SoTA models, e.g., MACE-OFF, SO3Krate, Equiformer v2 for comparison?\nTable 3 and Figure 4. Why choose Equiformer rather than Equiformer v2 for comparison? Even for v2, it has been published for several years.\nFigure 5. It seems that all the other models except DKMP show abnormal energy flunctuations in NVE simulations. First, it does NOT show DKMP's supriority. Instead, it means the MD settings for other models are probably WRONG! Second, even DKMP's simulation show stable energy values. It has nothing to do with the statement \"throughout long-term MD simulations, more accurately capturing\nthe physics of real MD simulations for large molecular systems\"! 100ps simulation is too short. Where's the \"accurate\" come from? where the \"physics\" come from? What does \"real MD\" mean? Chignolin has only 166 atoms. It has nothing to do with \"large molecular system\"!\nThe author lack basic domain knowledge on MD simulations and the experimental results are highly suspicous!\n\n3. Cofunsed terminology\nPainn, MACE, Allegro, etc...  are NOT MD simulation methods. Instead, they are machine learning force fields. \nThe unit of force is kcal/ (mol*Angstrom) instead of kcal/mol/Angstrom. \nAs far as I know, Chignolin dataset has 2 million samples, insteaf of 10 thousand. https://figshare.com/articles/dataset/_strong_AIMD-Chig_exploring_the_conformational_space_of_166-atom_protein_strong_em_strong_Chignolin_strong_em_strong_with_strong_em_strong_ab_initio_strong_em_strong_molecular_dynamics_strong_/22786730\nThe confused and misused terminology show the authors lack adequate knowledge in this field.\n\n4. Poor written\nThe demonstration on model design is not clear. Almost operations in the model are directly from ViSNet. Too many confused terminology. And more grammer errors.."}, "questions": {"value": "The authors should seriously address the concerns shown in Weakness point by point to improve the quality of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8WhmPL77CM", "forum": "ftANj24sfU", "replyto": "ftANj24sfU", "signatures": ["ICLR.cc/2026/Conference/Submission3311/Reviewer_KRk4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3311/Reviewer_KRk4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653639841, "cdate": 1761653639841, "tmdate": 1762916660487, "mdate": 1762916660487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Dilated K-Star Message Passing (DKMP*) for large-scale MD at full-atom resolution. The key idea is to partition edges by distance into mutually exclusive, increasingly “dilated” K-star neighborhoods and stack one-layer MPNNs over these partitions. Two concrete variants are presented: (1) DKMPC with dilated radius-cutoff intervals for single-scale systems (MD22, Chignolin); and (2) DKMPR with dilated distance-ranking + a light graph-attention core (dropping strict equivariance) for various-scale protein–ligand systems (MISATO, AdK). Results claim SOTA accuracy/efficiency, including atomic-level next-step precision on protein–ligand systems up to ~40k atoms, and improved NVE stability on Chignolin."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Broad evaluation: S2EF on Chignolin/MD22 and S2S on MISATO/AdK; useful mix of small and very large systems\n2. Scalability: MISATO results include cases with 11k–40k atoms; baselines reportedly OOM while DKMPR runs in ≤~18 s/snapshot for the largest complex"}, "weaknesses": {"value": "1. Proposition 3.1 claims that DKMP* interactions are “immune to over-squashing.” While rewiring can mitigate message-passing bottlenecks, describing the model as “immune” is overstated unless bounded influence distortion is formally demonstrated—e.g., via curvature or flow-based analyses rather than adjacency-power arguments. I recommend tempering this claim or expanding the proof to include a modern formalism such as influence decay bounds, effective resistance, or discrete Ricci curvature, to substantiate robustness beyond intuitive edge-connectivity reasoning.\n2. Additional ablations that could strengthen the empirical analysis include: (1) Edge-budget control: compare one-shot KNN with K = M versus L dilated K-stars summing to M under identical total edge counts; (2) Partition strategy: random versus distance-sorted partitioning to demonstrate the impact of ordering; (3) Hyperparameter sensitivity: vary K and L to map accuracy–latency trade-offs and reveal potential non-trivial optima; (4) Mutual-exclusion analysis: with versus without edge reuse across layers to verify its necessity; and (5) Equivariance ablation: equivariant versus non-equivariant variants on MISATO with efficiency–accuracy curve\n3. The paper occasionally uses inconsistent notation (e.g., $N_{K}^{l}(i)$ vs. $N_{C}^{l}(i)$), which can obscure the hierarchy and semantics of neighborhood definitions. A unified notation scheme, accompanied by a concise schematic diagram (with notation and equations) illustrating the structure and information flow of DKMP layers, would substantially improve clarity and readability.\n4. While energy conservation is qualitatively demonstrated (Fig. 5), the paper lacks quantitative physical validations such as energy drift rates or RMSD stability analyses. Including velocity autocorrelation function (VACF) plots would also provide valuable insight into the system’s dynamical fidelity."}, "questions": {"value": "1. What is the time complexity as a function of (N, K, L, M) for both variants and contrast to dense-cutoff MPNN?\n2. How does your framework handle periodic boundary conditions (PBC)? If applicable, could you demonstrate its validity on at least one standard PBC benchmark?  \n3. A common and informative analysis is to examine potential decay—could you plot the learned potential to verify that it exhibits the correct decaying behavior with increasing interatomic distance?\n4. How does this sampling strategy ensure smooth transitions of energy and forces between consecutive MD frames? In other words, how does it avoid discontinuities caused by differences where one neighbor jumps from one edge set to a different edge set?\n5. Could you clarify how the model scales to extremely large systems (e.g., tens of thousands of atoms)? The main text currently lacks sufficient technical details on the specific design or computational strategies—such as memory partitioning, neighbor sampling, or distributed message-passing—that enable efficient handling of such large systems."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fDiMq4RCfm", "forum": "ftANj24sfU", "replyto": "ftANj24sfU", "signatures": ["ICLR.cc/2026/Conference/Submission3311/Reviewer_1rjP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3311/Reviewer_1rjP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791425762, "cdate": 1761791425762, "tmdate": 1762916660159, "mdate": 1762916660159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DKMP$^\\*$, a novel message passing framework for large-scale molecular dynamics (MD) simulations at full atomic resolution. It addresses the failure of current methods, which either suffer from over-squashing when stacked deep or prohibitive computational cost when the interaction radius is increased. DKMP$^\\*$ resolves this by stacking layers that each pass messages over distinct, sparse, \"dilated\" graphs, progressively capturing interactions at different distances.\n\nThe paper presents two implementations:\n\n* DKMP$^C$ (Dilating Radius Cutoff Interval): For single-scale systems.\n\n* DKMP$^R$ (Dilating Distance Ranking): For various-scale systems.\n\nThis approach is the first to achieve atomic-level accuracy on large-scale benchmarks like MISATO, successfully simulating protein-ligand systems with up to 40,000 atoms where all baselines failed ."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Significance & Originality**: This work is a significant breakthrough for large-scale MD. The core idea of using dilated, sparse message passing graphs (instead of one dense one) is a novel and effective solution to the scaling and over-squashing problems .\n\n**Performance**: The method achieves state-of-the-art results on four benchmarks. Most impressively, it is the only ML-based method shown to successfully run and maintain atomic-level accuracy on the largest, most complex systems in the MISATO dataset (up to 40,000 atoms), where all baselines failed due to out-of-memory (OOM) errors .\n\n**Quality & Clarity**: The paper is well-written, clearly motivating the problem (Figure 1) and solution. The experimental validation is strong, particularly the parameter analysis (Figure 6) which empirically proves its hypothesis: DKMP$^\\*$ benefits from deeper layers while baselines suffer from over-squashing ."}, "weaknesses": {"value": "I don't perceive any major weaknesses in this paper, though I feel it lacks an experimental validation. While the author finds the DKMP$^C$ (radius cutoff) implementation challenging to learn, I believe empirical evidence is still needed to substantiate this conjecture. Since EGNN inherently handles node/edge variations, it should be capable of learning."}, "questions": {"value": "**Impact of Omitting Equivariance**: Could you quantify the impact of omitting equivariance in the DKMP$^R$ model? How does an equivariant version perform on MISATO? Does it also fail with OOM errors like the baselines?\n\n**Long-Term Stability**: Why does the model's error (F-MSE) grow so large over long trajectories, even when its single-step prediction (N-MSE) is excellent? Does your spatial dilation approach have any blind spots for long-term temporal stability?\n\n**Implementation Crossover**: Have you experimentally confirmed that the DKMP$^C$ (radius cutoff) implementation is computationally inefficient on the various-scale MISATO dataset, as you hypothesize?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B4UiO6YEoD", "forum": "ftANj24sfU", "replyto": "ftANj24sfU", "signatures": ["ICLR.cc/2026/Conference/Submission3311/Reviewer_A2Zq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3311/Reviewer_A2Zq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967592751, "cdate": 1761967592751, "tmdate": 1762916659848, "mdate": 1762916659848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new framework, Dilated K-star Message-Passing (DKMP*), for simulating large-scale molecular dynamics (MD). The core problem addressed is that standard message passing neural networks (MPNNs) struggle to scale to large atomic systems due to computational cost (with large cutoffs) or information propagation issues like over-squashing (with many layers). The proposed solution is to stack a sequence of shallow MPNNs for each subgraphs\n . These graphs are constructed by partitioning the set of all atomic pairs based on their Euclidean distance, effectively creating a \"dilated\" receptive field that grows with each layer. This allows the model to directly capture interactions at various distances without incurring the computational cost of a dense graph or the propagation issues of deep GNNs.\n\nThe authors propose two concrete implementations:\n\nDKMPC (Dilating Radius Cutoff Interval): For single-scale systems, where edges are partitioned into concentric spherical shells.\nDKMPR (Dilating Distance Ranking): For various-scale systems, where a fixed number of neighbors are selected from progressively distant rank-ordered sets. This version also uses a non-equivariant attention mechanism for efficiency.\nThe framework is evaluated on several benchmarks, including MD22, Chignolin, AdK, and the large-scale MISATO dataset (up to 40,000 atoms). The results demonstrate state-of-the-art performance in both structure-to-energy-and-forces (S2EF) and structure-to-structure (S2S) tasks, significantly outperforming baselines on large systems where many other methods fail due to memory constraints."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Significance: The paper tackles a highly significant and challenging problem in computational science: creating accurate and efficient machine learning potentials for large-scale molecular systems. The ability to simulate protein-ligand complexes with tens of thousands of atoms at full atomic resolution, as demonstrated on the MISATO dataset, is a major step forward and has substantial implications for fields like drug discovery.\n\n2. Quality: The experimental evaluation is extensive and convincing. The method is benchmarked on a diverse set of four datasets, covering different system sizes, tasks (S2EF, S2S), and molecular types (small molecules, proteins, protein-ligand complexes).\nThe results are impressive, showing state-of-the-art accuracy while maintaining or improving computational efficiency. \n\n3. Clarity: The paper is well-written, and the core idea is presented clearly. Figure 1 provides a straight-forward visual intuition for the problem and the proposed solution. The formalization of the dilation mechanism via the four constraints in Eq. 2 is a clear and effective way to define the framework."}, "weaknesses": {"value": "Novelty in a Broader Context: While the application and specific formulation are novel, the underlying idea can be viewed as an architectural variation of existing principles. The method is essentially a sequence of MPNN blocks, each operating on a pre-determined, rewired graph. This connects it closely to the graph rewiring literature (e.g., Gutteridge et al., 2023), which also seeks to improve long-range information flow. The contribution could be framed more as a highly effective, structured rewiring strategy tailored for molecular physics, rather than an entirely new paradigm. The novelty is more in the successful engineering and application than in a fundamental algorithmic breakthrough.\n\nLack of Equivariance in DKMPR: The paper states that for the DKMPR model—the one used for the largest and most challenging systems—equivariance constraints are omitted for efficiency, inspired by AlphaFold3. This is a significant design choice that merits a more thorough justification and analysis. Equivariance is a fundamental inductive bias for physics simulation, ensuring that predictions transform correctly under rotations and translations. Dropping it risks the model's physical consistency and generalization. While the empirical results are strong, it is unclear if the model is learning approximate equivariance from the large dataset or if its success is limited to the data distribution seen during training. An ablation study quantifying the impact of this choice would greatly strengthen the paper.\n\nHandling of Long-Range Interactions: The dilation mechanism is a heuristic for capturing interactions at increasing distances. It is well-motivated for interactions that decay with distance, like van der Waals forces. However, it is less clear how this approach compares to principled methods for handling long-range electrostatic interactions, which decay slowly (1/r) and are critical in many biomolecular systems. The paper mentions Ewald-based methods (Kosmala et al., 2023) as orthogonal but does not discuss the limitations of its own approach in this context. A deeper discussion or an experiment on a system dominated by electrostatics would be insightful.\n\nMissing baselines and related works:\n1. SE(3) Equivariant Graph Neural Networks with Complete Local Frames;  ICML 2022;\n2. AlphaNet: Scaling Up Local Frame-based Atomistic Foundation Model, Npj Comput. Mater. (2025)"}, "questions": {"value": "Novelty and Graph Rewiring: Could you further elaborate on the relationship between DKMP* and graph rewiring methods like DRew? Both seem to address over-squashing by modifying graph connectivity to facilitate long-range information flow. Is it fair to characterize DKMP* as a deterministic, multi-stage rewiring strategy where the graph is rewired at each stage according to a distance-based partitioning?\n\nJustification for Dropping Equivariance: Regarding the non-equivariant DKMPR model: could you provide an ablation study or further analysis on the effect of removing the equivariance constraint? For example, how does a non-equivariant model perform if the test set molecules are rotated randomly compared to their training orientation? Does the model implicitly learn this symmetry from the data, and if so, what is the data requirement for this to occur?\n\nChoice of Hyperparameters L and C: In your parameter analysis, DKMPC's performance improves with the number of layers L. How should one choose the optimal L and maximum cutoff C? Is there a trade-off where increasing L too much creates overly sparse graphs in each message-passing step, potentially harming the learning of collective interactions within each distance shell?\n\nLong-Range Electrostatics: Your method captures interactions at longer distances through dilation. How do you see this approach performing on systems where long-range electrostatics are known to be dominant for the system's dynamics? Would the model be able to learn the (1/r) decay, or would it need to be integrated with a method like Neural P3M, as you suggest in your future work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HswV9xpgCT", "forum": "ftANj24sfU", "replyto": "ftANj24sfU", "signatures": ["ICLR.cc/2026/Conference/Submission3311/Reviewer_eAoj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3311/Reviewer_eAoj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3311/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762153967760, "cdate": 1762153967760, "tmdate": 1762916659648, "mdate": 1762916659648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}