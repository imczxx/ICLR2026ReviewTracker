{"id": "t11Z4GhGWG", "number": 17760, "cdate": 1758280207444, "mdate": 1759897155593, "content": {"title": "Why is the LLM unsure? Profiling the Causes of LLM Uncertainty for Adaptive Model and Uncertainty Metric Selection", "abstract": "Large Language Models (LLMs) frequently produce fluent yet factually inaccurate outputs, termed hallucinations, which compromise their reliability in real-world applications. Although uncertainty estimation offers a promising approach to detect these errors, existing metrics lack interpretability and offer limited insight into the underlying causes of uncertainty. In this work, we introduce a novel prompting-based framework for systematically analyzing the causes of uncertainty in LLM responses. We design dedicated indicators to quantify each distinct cause and profile how existing uncertainty metrics align with them. Our findings reveal systematic variations in uncertainty characteristics across metrics, tasks, and models. Leveraging these insights, we propose a task-specific metrics/models selection method guided by the alignment of uncertainty characteristics with task requirements. Experiments across multiple datasets and models demonstrate that our selection strategy consistently outperforms non-adaptive baselines, achieving 3-4\\% performance improvements and enabling more reliable and efficient uncertainty estimation for LLM deployment.", "tldr": "", "keywords": ["LLM uncertainty", "uncertainty decomposition"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9b9fca48e3ee1d82c8b0ddebde02379d41d0b427.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical issue of hallucinations in Large Language Models (LLMs) by proposing a novel framework to understand the underlying causes of model uncertainty. The authors identify four interpretable causes—Syntax Sensitivity, Semantic Ambiguity, Indecisiveness among Outputs, and Unconfidence when Challenged—and design a prompting-based pipeline to quantify each cause with dedicated indicators. By profiling how existing uncertainty metrics align with these causes across various tasks and models, the paper reveals systematic differences in their behavior. Leveraging these \"uncertainty profiles,\" the authors propose an adaptive method for selecting the most suitable uncertainty metric or model for a given task, which demonstrates consistent performance improvements over non-adaptive baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe study research topic is meaningful and sheds light on future research\n2.\tThe methodology is novel."}, "weaknesses": {"value": "1.\tThe paper should be further polished for better presentation.\n⦁\tIn Line225, what does $M$ means?\n⦁\tIn Figure4, what does cutoff ranks mean?\n⦁\tIn Table2, what do the two settings (worst, random) mean in detail?\n⦁\tIn Line 429, how is the NDCG metric computed? Is it proposed by yours? If this is a common metric, please cite with proper reference.\n2.\tThe setting is questionable:\n⦁\tThe authors define 4 possible causes of LLM uncertainty. Is such a taxonomy sound and complete? There is no evidence that these causes could span all causes of LLM uncertainty. The conclusion drawn based on this premise may be erroneous.\n⦁\tIn Table 1, the datasets include CommonsenseQA, MATH, TruthfulQA. However, in later experiments, the authors additionally introduced TriviaQA. The two settings are misaligned and the authors do not explain the hidden reason.\n⦁\tIn Figure3, the two indicators (SA, UC) are generally the lowest, and are presented with a very small value (e.g., 0.02). This also validates my concern about the validity of these four causes of LLM uncertainty.\n3.\tThe experiment result is hard to interpret\n⦁\tIn Table2, the random setting could give a >90% accuracy. The authors do no give a reason for this unusual phenomenon.\n⦁\tWhy the authors use a smaller K (K=5) for scenario 2, but use a larger K (K=8) for scenario 1?\n⦁\tIn the scenario2 setting, triviaQA's accuracy is extremely high compared to other datasets. The authors do not give an explanation for this."}, "questions": {"value": "1.\tIn Line378, the authors present a conclusion that 'there is no clear distinction between smaller and larger models in terms of their average uncertainty cause values'. However, this conclusion is given based on just one series of models (Gemma2). Llama3.2-3B and Llama3-8B are not strictly small/large models as their training data are different. Meanwhile, only two sizes for comparison is not sufficient to draw this conclusion. Could the author compare more sizes of LLMs using the same series of models? (Qwen2.5 series: 0.5B, 1.8B, 3B, 7B, 14B (which could be deployed in two 3090))\n2.\tThe four indicators are computed by prompting the LLMs to be evaluated. This may introduce bias, as the result is not only reflects the uncertainty score, but also reflects the instruction following ability of the LLM to be tested. Could the author try to separate any distracting factors to conduct the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GE5bqLBiIi", "forum": "t11Z4GhGWG", "replyto": "t11Z4GhGWG", "signatures": ["ICLR.cc/2026/Conference/Submission17760/Reviewer_PXL3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17760/Reviewer_PXL3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815238149, "cdate": 1761815238149, "tmdate": 1762927602981, "mdate": 1762927602981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a prompting-based framework that decomposes LLM uncertainty into four interpretable causes and defines indicators to estimate each cause. Using these indicators, the authors build uncertainty profiles for tasks, models, and existing uncertainty metrics, then use profile alignment to adaptively select models/metrics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Understanding the sources of LLM uncertainty is important for evaluation, deployment, and safety. The paper targets a question many researchers face.\n2. Framing uncertainty via SS/SA/IO/UC is intuitive and maps neatly to known interventions.\n3. Interesting solutions that considering adaptive uncertainty metric selection."}, "weaknesses": {"value": "1. Cause set may be incomplete. The four causes are plausible but not exhaustive. Other factors (e.g., retrieval grounding quality, tool-use failures, long-context drift, alignment-induced hedging) can also drive uncertainty. The paper should explicitly acknowledge this and discuss extension paths.\n2. The pipeline is sequential: paraphrasing → clarification → answering → self-check. Uncertainty introduced (or reduced) at earlier stages may leak into later ones, potentially inflating/deflating UC or IO.\n3. The paper would benefit from direct evidence that the SS/SA indicators capture what they claim."}, "questions": {"value": "1. Do you have human annotations of ambiguity or sensitivity to confirm that SA and SS correlate with actual ambiguity/surface-form brittleness rather than proxy artifacts?\n2. What happens if you shuffle the stages (e.g., answer before clarification) or run stages independently? How stable are UC/IO estimates under such changes?\n3. Citation format at Line 141 appears incorrect.\n4. From Section 5.2 onward, consider adding a one-sentence takeaway at the end of each paragraph to reinforce the main point and improve readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "476VOYoU4C", "forum": "t11Z4GhGWG", "replyto": "t11Z4GhGWG", "signatures": ["ICLR.cc/2026/Conference/Submission17760/Reviewer_bkEj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17760/Reviewer_bkEj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833047336, "cdate": 1761833047336, "tmdate": 1762927602571, "mdate": 1762927602571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a prompting-based framework to systematically analyze and profile the causes of uncertainty in LLM responses. The authors design interpretable indicators for four distinct causes of uncertainty (syntax sensitivity, semantic ambiguity, indecisiveness among outputs, and unconfidence when challenged), and empirically study how existing uncertainty metrics align with these causes. The work further introduces an adaptive model/metric selection strategy based on uncertainty profiles, achieving consistent improvements over non-adaptive baselines across multiple datasets and models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "⦁\tClear motivation and practical relevance for LLM deployment.\n⦁\tStrong empirical results and broad coverage of datasets/models.\n⦁\tThe framework is interpretable and actionable for downstream applications."}, "weaknesses": {"value": "⦁\tSome experimental and implementation details are lacking (e.g., ablation on indicator importance, prompt sensitivity).\n⦁\tThe method's effectiveness may depend on the quality of prompt engineering and may not generalize to all LLM architectures.\n⦁\tLimited discussion of failure cases or scenarios where the indicators may be misleading."}, "questions": {"value": "⦁\tHow sensitive are the results to the choice/design of prompts for each indicator?\n⦁\tCan the authors provide more details or ablation on the relative importance of each indicator?\n⦁\tHow does the framework perform on closed-source or instruction-tuned LLMs?\n⦁\tAre there cases where the indicators disagree or provide misleading signals?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2gjCdZnqZu", "forum": "t11Z4GhGWG", "replyto": "t11Z4GhGWG", "signatures": ["ICLR.cc/2026/Conference/Submission17760/Reviewer_9V5V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17760/Reviewer_9V5V"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924691821, "cdate": 1761924691821, "tmdate": 1762927602176, "mdate": 1762927602176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework to analyze the cause of LLM uncertainty, categorizing the causes into four types and proposing corresponding metrics to quantify each of them. The cause profiling results are used to guide the selection of existing uncertainty metrics based on the models and tasks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The taxonomy of four primary causes of LLM uncertainty is well motivated. It forms a good foundation for future work in analyzing the uncertainty causes for LLMs."}, "weaknesses": {"value": "1.\tThe design of the estimation pipeline for uncertainty causes has several fundamental issues, as outlined below:\n-\tFor estimating Syntax Sensitivity (SS), the quantitative metric used is doubtful. Model being able to paraphrase the original questions in syntactically very different ways does not necessarily mean that the model's outputs are sensitive to paraphrasing of the question, right? This metric simply measures how well the model is at paraphrasing a question.\n-\tIn estimation of the Indecisiveness among Outputs (IO), the answers are for different prompts, so simply using their likelihood in an entropy-calculation way does not really returns an entropy. Entropy is a property of one single distribution, but what we actually have here are multiple responses from multiple output distributions, each corresponding to one clarified question. This makes the entropy interpretation invalid.\n-\tFor estimating the Unconfidence while being Challenged (UC), it has the same problem as in IO estimation: the answers are not from the same distribution, each answer is from a different distribution (each corresponds to one clarified question), this makes the mathematical interpretation invalid.\n2.\tFrom the results in Table 1, the SS score only provides AUROC values slightly higher than 0.5 and the SA score even gives AUROC lower than 0.5. Considering the fact that a random guess can provide an AUROC score of 0.5, it seems that the proposed SS and SA scores do not really indicate any useful uncertainty information of the LLMs.\n3.\tThe literature review in the related work section is limited."}, "questions": {"value": "1.\tFor estimating the Semantic Ambiguity (SA), does the distance in the internal embedding space exactly reveal the difference in semantic meanings? Can you provide any supporting literatures?\n2.\tIn the “Metric-Level Attribution” paragraph, how do you calculate P(u,m) here for indicator value and metric score? How do you get the probabilities for them? This seems mathematically invalid. Please provide the concrete math equations for verifications.\n3.\tHow do we get the Metric-Vec? The cause indicator values are corresponding to specific model and task, but irrelevant to the uncertainty metrics. This is confusing.\n4.\tFor the tested SOTA uncertainty metrics, most of them are prompt-wise. Can you also include more response-wise uncertainty scores, e.g., semantic density [1]?\n\n[1] Xin Qiu, Risto Miikkulainen. Semantic density: Uncertainty quantification for large language models through confidence measurement in semantic space, Advances in Neural Information Processing Systems (NeurIPS), 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ed0KrwLIUa", "forum": "t11Z4GhGWG", "replyto": "t11Z4GhGWG", "signatures": ["ICLR.cc/2026/Conference/Submission17760/Reviewer_wKPH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17760/Reviewer_wKPH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17760/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983896280, "cdate": 1761983896280, "tmdate": 1762927601788, "mdate": 1762927601788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}