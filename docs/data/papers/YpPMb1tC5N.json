{"id": "YpPMb1tC5N", "number": 10591, "cdate": 1758176741761, "mdate": 1759897641811, "content": {"title": "S-ATM: Self-Boosting Visual Reasoning via Adaptive Token Merging", "abstract": "Vision-language models, often adapted from large language models, tend to show degraded reasoning capabilities when visual inputs are introduced. To address this issue, we propose S-ATM, a training-free decoding strategy that enhances visual reasoning without relying on external priors. For each input, two parallel pathways are constructed: one using the original image–text input and the other using a self-generated caption–text input. Their decoding distributions are adaptively merged at each step, with the merging weight guided by the model’s attention to visual tokens. A momentum-based smoothing mechanism further stabilizes this merging over time. We conduct comprehensive experiments on diverse visual reasoning benchmarks to demonstrate the effectiveness of S-ATM. Further analysis shows that S-ATM primarily activates at high-entropy forking tokens, which often correspond to reasoning transitions, and that momentum smoothing reduces decoding instability and maintains reasoning coherence. These findings underscore the role of token-level dynamics in supporting long-chain reasoning in VLMs.", "tldr": "We propose S-ATM, a training-free decoding strategy that mitigates reasoning degradation in VLMs without extra supervision or reliance on external models.", "keywords": ["VLM", "Vision Reasoning", "Token Merging"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/277b34f27b20f0f928af01d4cc9726d6daa35582.pdf", "supplementary_material": "/attachment/f3e8ab453719b161ee33e1cca8daa088361c84c7.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces S-ATM, a training-free decoding strategy that enhances visual reasoning capabilities in Vision-Language Models (VLMs). The method dynamically merges decoding outputs from two parallel pathways—a visual pathway using the original image input and a textual pathway using a caption generated by the same model. The merging is guided by a weighting factor derived from attention allocation over visual tokens, coupled with a momentum-based smoothing mechanism that stabilizes switches between the pathways.\n\nThe method is evaluated on visual reasoning benchmarks like MathVerse, MathVista, and MMStar across a range of model sizes (3B to 14B). S-ATM provides consistent improvements over baseline VLM decoding and outperforms caption-augmentation baselines. The authors also run plug-and-play experiments combining different VLMs and LLMs, showing additional capability fusion without extra training."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. the approach requires no retraining or external supervision. It enhances reasoning using only decoding-time modifications, making it easy to plug into existing VLMs.\n2. the method intelligently modulates the contribution of visual and textual pathways using visual attention signals and momentum filtering, improving reasoning coherence without sacrificing visual grounding.\n3. the paper provides thorough analysis on why S-ATM works, including identifying “forking tokens” where intervention matters and linking momentum smoothing to decoding stability."}, "weaknesses": {"value": "1. While the paper benchmarks S-ATM primarily on MathVerse, MathVista, and MMStar—datasets that are well-suited for testing visual mathematical reasoning—this narrow focus limits the scope of its demonstrated effectiveness. These datasets emphasize symbolic reasoning, geometry-based perception, or graph comprehension, which, while important, represent only a subset of real-world multimodal reasoning challenges. Broader evaluation across commonsense visual reasoning tasks (e.g., VCR, GQA), diagram-based or scientific illustrations (e.g., AI2D, ScienceQA), and multi-image, temporal, or story-based tasks (e.g., CLEVR, VizWiz, or VideoQA datasets) would better reflect the practical utility and general applicability of S-ATM.\n2. The approach relies heavily on self-generated captions. No ablation or analysis is provided on caption quality—e.g. what happens if the model generates low-quality descriptions due to hallucination or cluttered images.\n3. The paper does not include any quantitative analysis of the additional computational costs introduced by running two decoding pathways in parallel during inference. Since S-ATM requires simultaneous token generation across both the visual and caption-based textual streams, and further performs per-token attention analysis and momentum-based merging, it likely incurs additional latency and memory overhead relative to baseline decoding. For real-world systems that operate under strict resource or latency constraints (e.g., edge deployment, real-time QA), this omission makes it difficult to assess the practical scalability or deployment trade-offs of S-ATM.\n4. S-ATM is primarily applied on Qwen-based VLMs (Qwen2.5-VL and InternVL variants), which, while popular, represent only a subset of modern multimodal architectures. The absence of results on other widely used VLM families—such as LLaVA, Gemma, etc—restricts the generalizability of the findings. Since different VLMs use varying fusion mechanisms, visual encoders, and attention tokenization schemes, it’s unclear whether S-ATM’s adaptive merging and momentum smoothing would transfer as effectively to other architectures."}, "questions": {"value": "1. Does the method help with grounding consistency or reduce hallucination in image-based QA?\n2. What happens if visual attention is noisy or misallocated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ODnif8RoLd", "forum": "YpPMb1tC5N", "replyto": "YpPMb1tC5N", "signatures": ["ICLR.cc/2026/Conference/Submission10591/Reviewer_Bs6K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10591/Reviewer_Bs6K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963974795, "cdate": 1761963974795, "tmdate": 1762921860315, "mdate": 1762921860315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes S-ATM, a training-free decoding strategy that enhances visual reasoning in VLMs without using external priors. It adaptively merges two decoding pathways—original image–text and self-generated caption–text—guided by visual attention and stabilized via momentum smoothing. Experiments show improved reasoning coherence and stability, highlighting the role of token-level dynamics in long-chain visual reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper introduces a new decoding method for enhancing reasoning capability of VLM\n\n- With extensive experiments, the author demonstrated that S-ATM is effective in terms of accuracy"}, "weaknesses": {"value": "- Although I do not find the core idea itself particularly novel, I believe it is contributive and thus acceptable. The experiments also comprehensively evaluate the proposed S-ATM across various models in terms of accuracy, effectively demonstrating its effectiveness. However, since the proposed approach is a decoding method, I think showing its efficiency would be more practically meaningful. Because the method generates two pathways (text and vision) and performs adaptive weighting at the attention level to produce the final logits for decoding, the inference process seems potentially time-consuming. I would like to know how fast the actual inference speed is in practice.\n\n- Although the proposed method achieves higher performance than the baseline, I am curious about its reliability. This is because the approach involves several hyperparameters (e.g., $\\alpha_0$,  $\\epsilon$. etc), and merging two pathways at each layer may introduce additional variability. If these parameters must be newly adapted or optimized for each benchmark or model, it could raise concerns about the generalizability of the method.\n\n- In Table 1, how was the baseline score measured? Was it obtained by prompting the model to produce a direct answer, or did you employ Chain-of-Thought (CoT) prompting?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kdyZAjSxiJ", "forum": "YpPMb1tC5N", "replyto": "YpPMb1tC5N", "signatures": ["ICLR.cc/2026/Conference/Submission10591/Reviewer_Po6T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10591/Reviewer_Po6T"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970155055, "cdate": 1761970155055, "tmdate": 1762921859587, "mdate": 1762921859587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training-free, model-agnostic decoding strategy for vision–language models. The method runs two parallel reasoning paths—one conditioned on the image and one that relies on a self-produced textual description—and fuses their token predictions with a lightweight, step-wise gate. The goal is to exploit complementary strengths of image-grounded and language-only reasoning at inference time, requiring no finetuning or architectural changes. Experiments across a range of multimodal benchmarks report modest but consistent improvements, with ablations indicating that both the caption detour and the gating/smoothing heuristics contribute to the gains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea is novel to me, and is easy to follow. It only requires two parallel decoding paths with an attention-derived gate plus a one-state momentum smoother. The pipeline is straightforward. \n- Results on MathVerse, MathVista, and MMStar show modest but steady improvements.\n- The paper links improvements to high-entropy “forking” tokens and argues that smoothing reduces excessive pathway switching, offering a plausible account of when and why the method helps. It's interesting."}, "weaknesses": {"value": "> Technical\n\n1. I think take attention as a gating signal is brittle.  \n   - The image-attention ratio \\\\(\\alpha_{\\text{att}}\\\\) is taken from a chosen layer, averaged over heads/tokens, and then mapped linearly to \\\\(\\delta_t\\\\) with a hard heuristic. Which layer/heads are used is not specified, no evidence is provided that attention mass correlates with the *causal necessity* of vision at step \\\\(t\\\\). Attention is a weak proxy for importance.\n\n2. Momentum analysis rests on unrealistic assumptions.\n  -  The \"proof\" that momentum reduces switching frequency assumes i.i.d. Gaussian \\\\(\\alpha_t\\\\), then studies sign flips; actual \\\\(\\alpha_t\\\\) is bounded and derived from attention, and the smoothed series is not driven by i.i.d. noise. The derivation does not establish task accuracy improvements, only a toy decrease in flips.\n\n3.  S-ATM runs two full decoders in parallel and generates a long caption first . There is no wall-clock analysis or FLOPs comparison, nor a cost-normalized metric (e.g., accuracy per second). \n\n> Experiments\n\n4. No study of: \n  - which layer to read attention from\n  - effect of temperature and caption length\n  - probability vs logit-space fusion\n\n5. Benchmarks are mostly math/VQA. No tests on instruction-following multimodal datasets, long-context OCR-heavy tasks, or robustness."}, "questions": {"value": "1. Why logit mixing? Please justify Eq. (5) versus (i) probability-space mixture \\\\(p=\\alpha p_{\\text{vis}}+(1-\\alpha)p_{\\text{res}}\\\\) and (ii) log-linear/geometric fusion \\\\(\\log p \\propto \\alpha\\log p_{\\text{vis}}+(1-\\alpha)\\log p_{\\text{res}}\\\\). Any calibration of logit scales between pathways?\n\n2. You say \"a specific layer\" provides \\\\(A\\\\) (Eq. (3)). Which layer and why? How sensitive is S-ATM to this choice and to head selection? Any results averaging multiple layers or using value/FFN activations?\n\n3. The text states \"values <0.1 are mapped to \\\\([-\\varepsilon,\\varepsilon]\\\\)\" to obtain \\\\(\\delta_t\\\\) (Eq. (4)). Please provide the exact formula, bounds on \\\\(\\alpha_t\\\\), and an ablation over \\\\(\\alpha_0,\\varepsilon\\\\).\n\n4. Can you empirically link “fewer sign flips” to accuracy improvements while holding other factors constant? Also, replace the i.i.d. Gaussian assumption with a model consistent with your measured \\\\(\\alpha_t\\\\) statistics. \n\n5. What is the end-to-end latency and GPU memory overhead of S-ATM vs baseline for the same max tokens? Include captioning time \n\n6. How are forking tokens detected at test time without labels? Your effective-token definition depends on pathway disagreement; does an oracle-free detector exist if one wanted to apply S-ATM only at those positions? \n\n7. Did you try a learned scalar gate for \\\\(\\alpha_t\\\\) (frozen VLM with tiny regressor over hidden states) or confidence-aware gating (entropy/variance of each pathway)? \n\n8. If attention extraction is unavailable, can \\\\(\\alpha_t\\\\) be computed from pathway self-statistics and how does that compare? \n\n9. You merge *logits* linearly as \\\\(z_t = \\\\alpha_t z^{\\\\text{vis}}_t + (1-\\\\alpha_t) z^{\\\\text{res}}_t\\\\) . Why is logit-space interpolation appropriate given that logits are only defined up to affine transforms and may be differently calibrated across the two pathways? Did you compare against probability-space mixture \\\\(p = \\\\alpha_t \\\\operatorname{softmax}(z^{\\\\text{vis}}_t) + (1-\\\\alpha_t) \\\\operatorname{softmax}(z^{\\\\text{res}}_t)\\\\) or *log-linear* fusion \\\\(\\\\log p \\\\propto \\\\alpha_t \\\\log p^{\\\\text{vis}} + (1-\\\\alpha_t) \\\\log p^{\\\\text{res}}\\\\)? Any temperature or scale calibration to align the two streams before fusion?\n\n10. The “theory” models \\\\(\\\\alpha_t\\\\) as i.i.d. \\\\(\\\\mathcal{N}(0,\\\\sigma^2)\\\\) and studies sign flips after momentum. But \\\\(\\\\alpha_t\\\\) is derived deterministically from bounded attention statistics and is highly autocorrelated. Can you re-derive the result under a bounded, autocorrelated process?\n\n11. Beyond reducing “switch frequency,” is there measurable improvement in probability calibration or search stability (e.g., lower variance of token log-likelihoods across beams)? Any curves showing accuracy vs. switch-frequency trade-off while sweeping \\\\(\\\\gamma\\\\) and \\\\(\\\\varepsilon\\\\)? \n\n12. You use temperature 0.1 throughout. How do outcomes change with higher temperatures or nucleus/top-k sampling? Does higher randomness amplify mis-calibration between branches and thus favor probability-space or log-linear fusion over logit mixing? \n\n13. Some cells show small drops (e.g., InternVL3.5-14B on certain MathVista partitions). Can you quantify when S-ATM hurts—e.g., perception-dominated items where visual grounding is essential? A per-category error breakdown would help. \n\n14. Running two full decoders plus caption generation likely doubles latency and memory. What is the end-to-end wall-clock and GPU RAM overhead vs. baseline for the same max tokens? \n\n15. When is forward guidance harmful? Consider a case where attention briefly dips on image tokens during visually crucial steps (e.g., small visual cue). The mapping that pushes \\\\(\\\\alpha_t\\\\) toward the textual pathway might wrongly suppress vision. Any safeguards (e.g., hard floor on \\\\(\\\\alpha_t\\\\) when visual entropy is high)?\n\n\n\n\n\n\n\n\nOverall, I like this work. However, I still have many concerns regarding the technical issues and experiments. I will adjust the rating based on the authors' feedback/rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OuyXXFMBJd", "forum": "YpPMb1tC5N", "replyto": "YpPMb1tC5N", "signatures": ["ICLR.cc/2026/Conference/Submission10591/Reviewer_DNrB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10591/Reviewer_DNrB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976396331, "cdate": 1761976396331, "tmdate": 1762921858988, "mdate": 1762921858988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed S-ATM, a training-free decoding strategy for VLMs. Across MathVerse, MathVista, and MMStar, S-ATM yields consistent but modest gains (typically ~+0.5–3%, up to ~+5% on some MathVerse subsets) and can further improve via a plug-and-play variant that uses an external LLM on the textual path. Analyses suggest S-ATM mostly “kicks in” at high-entropy forking tokens—decision points where the two paths disagree—and that smoothing reduces harmful path-switching during long-chain reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Simple, training-free mechanism that is easy to bolt onto existing VLMs. \n\n2. The paper has a broad evaluation on three benchmarks and multiple model sizes shows generally positive gains, with larger boosts on math-reasoning subsets."}, "weaknesses": {"value": "1. Decoding cost roughly doubles at inference (two parallel passes + attention inspection each step), but runtime/latency overhead isn’t reported. Thus I feel the gains of the approach are modest and limited.\n\n2. Momentum stability is motivated by a simplified IID Gaussian sign-flip analysis; it’s insightful but far from the true non-Gaussian, autoregressive dynamics of modern decoders. \n\n3. Benchmarks skew toward visual math/VQA; generalization to OCR-heavy, chart/diagram QA, video VQA, or real-world multi-image tasks remains untested. For example, you can conduct experiments on Video VQA benchmarks such Video-MME.\n\n4. Plug-and-play improvements rely on stronger LLMs for the textual path. We don't know if this gain comes from powerful baseline."}, "questions": {"value": "See weaknesses for more details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RwlioyraDi", "forum": "YpPMb1tC5N", "replyto": "YpPMb1tC5N", "signatures": ["ICLR.cc/2026/Conference/Submission10591/Reviewer_CAKB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10591/Reviewer_CAKB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10591/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762223870318, "cdate": 1762223870318, "tmdate": 1762921858415, "mdate": 1762921858415, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}