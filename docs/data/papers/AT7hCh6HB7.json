{"id": "AT7hCh6HB7", "number": 8887, "cdate": 1758101265899, "mdate": 1759897757216, "content": {"title": "Moving Beyond Diffusion: Hierarchy-to-Hierarchy Autoregression for fMRI-to-Image Reconstruction", "abstract": "Reconstructing visual stimuli from fMRI signals is a central challenge bridging machine learning and neuroscience.\nRecent diffusion-based methods typically map fMRI activity to a single high-level embedding, using it as fixed guidance throughout the entire generation process. However, the fixed guidance collapses hierarchical neural information and mismatches the stage-dependent demands of image reconstruction.\nIn this paper, we propose MindHier, a coarse-to-fine fMRI-to-image reconstruction framework built on scale-wise autoregressive modeling.\nMindHier introduces three components: a Hierarchical fMRI Encoder that extracts multi-level neural embeddings, a Hierarchy-to-Hierarchy Alignment scheme that enforces layer-wise correspondence with CLIP features, and a Scale-Aware Coarse-to-Fine Neural Guidance strategy that injects the neural embeddings into autoregression at matching scales.\nThese designs together facilitate a hierarchical reconstruction process, first synthesizing global semantics and then refining local details, akin to human visual perception. MindHier thus establishes an efficient and principled alternative to diffusion-based paradigms.\nExtensive experiments on the NSD dataset demonstrate that MindHier achieves state-of-the-art semantic fidelity, 4.67$\\times$ faster inference, and more deterministic results than the diffusion-based baselines. Code will be released.", "tldr": "MindHier, a coarse-to-fine autoregressive framework, uses scale-aware guidance to inject hierarchical neural features for fMRI-to-image reconstruction, surpassing diffusion models in speed, stability, and semantic accuracy.", "keywords": ["fMRI-to-Image Reconstruction", "Coarse-to-Fine Generation", "Scale-wise Autoregressive Modeling", "Scale-aware Neural Guidance"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d971a65af5fd7cbac4e73da83453c19a62f14d3f.pdf", "supplementary_material": "/attachment/4c62c7a187e703cec5053e498c62253fc922130f.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents MindHier, a coarse-to-fine framework for fMRI-to-image reconstruction based on scale-wise autoregressive modeling. Unlike diffusion models that rely on a fixed global embedding, MindHier captures multi-level neural information through a hierarchical fMRI encoder, aligns fMRI and CLIP features layer by layer, and injects embeddings at matching scales for progressive reconstruction. Experiments on the NSD dataset demonstrate improved semantic fidelity, faster inference, and more stable outputs compared to diffusion-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The hierarchy-to-hierarchy alignment and scale-aware guidance are well-motivated and technically sound.\n* Consistent gains in semantic metrics (CLIP 96.4%) and large inference speedup without loss of quality.\n* Deterministic generation avoids stochastic variability common in diffusion models."}, "weaknesses": {"value": "* The paper claims to address the \"mismatch between fixed neural guidance ... reconstruction\", yet prior works have already explored similar ideas. Neuropictor [1] introduced ControlNet to incorporate low-level structure during decoding, and DecoFuse [2] explicitly decomposed fMRI features into \"what\" and \"where\" pathways aligned with brain hierarchies. These efforts are not discussed, and the paper frames the problem as if no prior solution existed.\n* More comparison, Neuropictor [1], MindTuner [3], and other recent approaches are missing.\n* Figure 2(b) is difficult to interpret: the design motivation and the functional role of each block are unclear, making the overall mechanism hard to understand.\n* The claimed advantage in reconstruction stability over diffusion models is not fully convincing, as many existing diffusion-based methods already mitigate stochasticity through techniques such as guidance scale tuning or ControlNet conditioning.\n* The emphasis on faster inference time is somewhat questionable. Efficiency is valuable, but its practical impact in fMRI decoding remains unclear.\n* The performance improvement over existing methods is modest, raising doubts about whether the proposed complexity yields a meaningful advance.\n\n[1] Huo et al. Neuropictor: Refining fmri-to-image reconstruction via multi-individual pretraining and multi-level modulation.\n\n[2] Li et al. DecoFuse: Decomposing and Fusing the\" What\",\" Where\", and\" How\" for Brain-Inspired fMRI-to-Video Decoding.\n\n[3] Gong et al. Mindtuner: Cross-subject visual decoding with visual fingerprint and semantic correction."}, "questions": {"value": "* How does MindHier fundamentally differ from adaptive-guidance approaches like Neuropictor or DecoFuse beyond adopting autoregression?\n* Could you clarify the purpose of each block in Fig. 2(b) and the rationale behind the hierarchy-to-hierarchy mapping?\n* What is the expected real-world impact of reducing inference time for fMRI decoding tasks?\n* How significant are the reported improvements statistically, and do they justify the added model complexity?\n* Could you quantify how much each proposed module contributes to the performance improvement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bqNeFGG2oy", "forum": "AT7hCh6HB7", "replyto": "AT7hCh6HB7", "signatures": ["ICLR.cc/2026/Conference/Submission8887/Reviewer_mNCs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8887/Reviewer_mNCs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809816655, "cdate": 1761809816655, "tmdate": 1762920645160, "mdate": 1762920645160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MindHier, an fMRI‑to‑image reconstruction framework that replaces the diffusion-based pipeline with a autoregressive one. The method (i) learns a Hierarchical fMRI Encoder whose intermediate blocks are aligned layer‑by‑layer to CLIP’s vision backbone, and injects these fMRI features as scale‑aware guidance during generation. On NSD, the approach achieves compelling performance with faster inference than diffusion-based method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is, to my knowledge, the first to systematically test a scale‑wise AR pipeline for fMRI reconstruction, rather than diffusion. The coarse‑to‑fine conditioning matches the generative schedule of AR, and the hierarchy‑to‑hierarchy alignment is cleanly specified\n- Achieves competitive performance with SOTA while running in 2.64 s per image. The paper explains the speedups via a single forward pass and concentrated computation at low resolutions in AR.\n- Ablation experiments quantify (i) the value of hierarchical supervision across blocks, (ii) CLIP‑layer mapping trade‑offs , and (iii) the benefit of coarse to fine guidance.\n- Fig. 4 shows lower trial‑to‑trial variability than a diffusion baseline, consistent with the deterministic AR start from fMRI features . (But see my questions below.) \n- Data split details, PyTorch‑style pseudo‑code, and a code‑release commitment are provided"}, "weaknesses": {"value": "- While I acknowledge the value of validating the method on a different architecture, although it achieves the best score on several metrics, there are multiple cases where the variances suggest marginal improvement or almost no improvement over prior work. Although the emphasis on inference speed is understandable, the quantitative gains appear limited overall.\n- The paper is primarily an engineering contribution; deeper neuro discussion would strengthen it. In Fig. S1, the paper states that EVC in red and higher‑order visual cortices in blue, but the overlays do not obviously track standard visual regions.\n- Although §4.3 is explicitly labeled “qualitative” and therefore does not include quantitative evaluation, showing a handful of examples is not sufficient to substantiate the claims. It remains unclear what the method enables that prior approaches could not. If the authors wish to advance these claims, please quantify each sub‑claim and then provide qualitative interpretation on top of the numbers.\n- I could not find explicit specifications for the HFE depth, the number of AR scales, or the codebook size, nor ablations over these choices."}, "questions": {"value": "- In Fig. S1, the paper states that EVC in red and higher‑order visual cortices in blue, but the overlays do not obviously track standard visual regions. Could you clarify?\n- In Fig. 4. if the model is deterministic, why do we see non‑identical repetitions in some examples? Which parts of the pipeline are stochastic?\n- Please detail the hardware and settings used to report 2.64 s and the 4.67× speedup.\n- In Table 4 column header, “Eff ↑”, but Eff is a distance and should be ↓?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tMpn941V8C", "forum": "AT7hCh6HB7", "replyto": "AT7hCh6HB7", "signatures": ["ICLR.cc/2026/Conference/Submission8887/Reviewer_3r2V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8887/Reviewer_3r2V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824742995, "cdate": 1761824742995, "tmdate": 1762920644696, "mdate": 1762920644696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper replaces diffusion with a scale-aware autoregressive (AR) generator conditioned on a hierarchical fMRI encoder that aligns to intermediate CLIP features. The idea is “forest-before-trees”: coarse scales use deeper, high-level brain features; finer scales use shallower ones. On NSD, the method reports good high-level identification and faster inference than diffusion baselines.\n\nOverall: promising direction, but several method details are under-specified and the experimental comparisons aren’t yet on fully solid ground."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Autoregression instead of diffusion: Clear potential for speedups, simpler likelihood training, and more direct optimization of image tokens (rather than only pushing CLIP embeddings).\n\n- Hierarchy-to-hierarchy conditioning: Aligning fMRI to multiple CLIP layers—and using those layers across scales—fits the coarse→fine generation narrative and is technically interesting.\n\n- Semantic metrics look strong: The method seems particularly good at high-level, identity/semantic recognition, which is where most practical interest is today."}, "weaknesses": {"value": "1. Under-specified alignment mechanics. The paper does not clearly explain how fMRI features are matched to CLIP layer outputs: token vs CLS usage, pooling, projection dimensions, normalization, or the exact shapes at each level. This affects both reproducibility and interpretation of the model. Similarly, loss weighting and SoftCLIP details are missing. Multiple MSE/contrastive terms are simply summed, but there’s no report of weighting, temperature, negative sampling, or sensitivity analyses. Given that evaluation leans on CLIP-style metrics, this raises target–metric coupling concerns.\n\n2. Start-token information bottleneck. The description suggests collapsing the deepest fMRI feature into a single start token. That risks throwing away critical information right at the first coarse scale. A learnable projection of the full feature (or multiple tokens) would be more principled.\n\n3. Hand-crafted scale↔hierarchy schedule. The mapping from image scale to CLIP/fMRI layer is fixed rather than learned. When K (scales) and M (feature levels) don’t match, some levels repeat or are skipped. A learned router/attention over {e_m} per scale would be more robust.\n\n4. Conditioning path is muddy. It’s unclear when the model uses cross-attention vs. AdaLN/FiLM-style conditioning vs. a “selective attention mask,” and where each sits in the block. This matters for understanding how and when brain information actually influences generation.\n\n5. AR claim vs. non-causal blocks. If the backbone uses non-causal transformers, how is the AR factorization enforced (masking, teacher forcing, decoding strategy)? Please make the training/inference story self-consistent.\n\n6. “Deterministic” vs. best-of-N selection. If multiple candidates are generated and the best is picked by CLIP similarity, claims about determinism/consistency need to be backed by single-shot results and by selection that only uses fMRI-derived embeddings (not ground truth).\n\n7. Ablations are thin and subject-limited. Key ablations (K/M sweeps, learned routing vs. fixed schedule, removing the contrastive term, different start-token strategies, with/without “†” low-level cue) should be run and reported, preferably beyond a single subject.\n\n8. Interpretability figures look off and lack statistical grounding. The brain maps appear asymmetric and not obviously aligned with canonical visual cortex topology. More importantly, if the encoder was trained to mimic CLIP hierarchy, qualitative maps can be circular. Please add ROI-wise linear encoding with cross-validated R² and noise ceilings, and run RSA/CKA against both brain RDMs and CLIP to show added brain-specific structure rather than merely recapitulating CLIP."}, "questions": {"value": "What I’d like to see in the rebuttal (specific and actionable)\n\n- Define the “† low-level feature” and how it is injected; either give baselines an equivalent cue or drop the “†” rows.\n\n- AR training/inference details: causal masking, teacher forcing, decoding strategy; also report single-shot results (no best-of-N) and, if using selection, ensure it only references fMRI-derived embeddings. Loss/temperature/weighting hyper-params and a short sensitivity analysis.\n\n- Ablations: K/M sweeps; learned routing vs. fixed schedule; start-token variants; with/without contrastive term; with/without the “†” cue."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KXFu8fWqr8", "forum": "AT7hCh6HB7", "replyto": "AT7hCh6HB7", "signatures": ["ICLR.cc/2026/Conference/Submission8887/Reviewer_u1wj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8887/Reviewer_u1wj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032459925, "cdate": 1762032459925, "tmdate": 1762920644236, "mdate": 1762920644236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new pipeline for reconstructing images from fMRI beta-values on the NSD dataset. Instead of relying on the rather common approach of decoding a single visual embedding from fMRI (e.g. CLIP), which conditions a diffusion-based image-generation model (e.g. via U-Net cross-attention layers), the paper uses a multi-scale architecture that simultaneously aligns various layers of a CLIP visual encoder to the successive blocks of a transformer fMRI encoder (with the output transformer block being aligned both with the output layers of the visual encoder and of an additional text encoder). Then, the activations from these aligned blocks are used to condition an autoregressive image generator based on the Switti model (Voronov et al, 2024), to inform the reconstruction process both at coarse- and detailed-level. The paper shows superior reconstruction performance against recent fMRI-to-Image baselines on the NSD dataset on semantic metrics, 4x inference speedup and more stable / deterministic reconstructions given the same fMRI sample."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to follow and the experiments are detailed\n- The use of an autoregressive generation model for image reconstruction from fMRI has been largely unexplored and the paper is a rather welcomed initiative in the field\n- The reconstructions shown are impressive and the stability of reconstructions is a rather attractive feature for potential future BCI applications\n- Important ablations on the hierarchical module are presented and lead to interesting conclusions (e.g. earlier features providing increasing low-level metrics and later layers globally decreasing performance)"}, "weaknesses": {"value": "- A number of claims are rather strong compared to the results supporting them. For example, at L339 it is claimed that the author's framework is 'fundamentally more efficient' than ME2 but it is not clear what set of results supports the 'fundamental' aspect of this claim: the inference time bottleneck for each pipeline is not detailed, and thus it is unclear if this performance gain in inference-time is a property of the fMRI-to-Image pipelines or linked to the efficiency of the diffusion vs autoregressive image generation models at hand.\n- The paper reports superior performance on image similarity metrics, which is great. However these metrics are known to be unreflective of qualitative / human evaluation and the paper only compares a very small set (5 images) of reconstructed images to baselines, which is insufficient to support the claims from Section 4.3 (the reconstructed images are arguably very close to the ones from ME2 and the distinctive characteristics of MindHier are supported only by two of these images).  These claims would be better supported by showing a larger set reconstructions, and a more representative set of success and failure modes for this task. \n- Similarly, the consistency of multiple generations is compared only to a single baseline, which undermines the claim of superior performance for MindHier. Again, this claim would be better supported by showing more images, and more baselines.\n- The same comment holds for the bounding box analysis (from UMBRAE), which does not show the same results for baselines, and thus makes it impossible to assess whether MindHier is actually an improvement.\n- The repeated statement that previous approaches rely on the decoding of a single high-level embedding is incorrect and misleading, as several previous methods e.g. Brain Captioning (Ferrante et al), MindEye (Scotti et al) use additional low-level data from the stimuli to condition the reconstruction of images (i.e. Depth Maps or VAE latents)\n- The approach is applied only on the NSD dataset and its conclusions could be better supported if the analysis included another dataset (e.g. THINGS-Image, BOLD5K, ...)"}, "questions": {"value": "- Typo L316: 'Two-way identification refers to percent correct if ...'\n- In Table 1, it is unclear how the the auxiliary low-level image is used for conditioning the image generation of the autoregressive model to gain competitive performance against baselines using a low-level-feature (E.g. MindEye2)\n- It is not easy to understand exactly how the outputs of the transformer blocks condition the autoregressive generation via cross-attention. There is a mention of AdaLN but the expected fMRI-decoding audience may not be able to grasp the details of this procedure without further details. In particular, are the weights of the autoregressive model fine-tuned during the process or left frozen ?\n- It is not explicitly stated whether the 'text encoder output' / 'visual encoder output' and 'fMRI encoder output' have the same dimensionality, allowing for the Eq1+Eq2 loss. For example, most CLIP implementations have a different output shape for text and image, how are these reconciled to be both trained against the last fMRI encoder's block output ? \n- The description of f_k in the scale-aware guidance is a bit difficult to follow. My understanding of f1 is clear, but I'm not sure what f_k is: it is described as 'residuals', but it is unclear that all 2D token maps have the same shape (because of the notation h_k x w_k for their size, which seems to depend on k). If these 2D maps have different dimensions depending on k, how are residuals defined ?\n- I'm not sure how the claim of MindEye2 being 'highly optimized' is supported. It is also unclear whether the inference-time gains are mostly due to the diffusion model from ME2 being slow (i.e. SDXL), or if the bottleneck is the inference of ME2's brain-to-CLIP module itself. A more detailed analysis of where the inference time is spent in each pipeline and which components from MindHier / ME2 actually contribute to the claimed 4x factor would increase clarity (e.g. time spent image generation model vs fMRI encoding module). \n- Some recent fMRI-to-Image baselines comparable with ME2 are missing (e.g. NeuroPictor)\n- There are various known issues with the NSD dataset (e.g. poor categorical diversity, see Kamitani et al). The impact of the paper could be increased by including an analysis on an additional dataset.\n- How is the multi-scale conditioning strategy tied to autoregressive nature  of the image generation model ? Couldn't a diffusion-based model be informed similarly of various scales (at various denoising steps) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aY7REfqG70", "forum": "AT7hCh6HB7", "replyto": "AT7hCh6HB7", "signatures": ["ICLR.cc/2026/Conference/Submission8887/Reviewer_DrWf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8887/Reviewer_DrWf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762184790086, "cdate": 1762184790086, "tmdate": 1762920643809, "mdate": 1762920643809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a well-motivated and technically sound alternative to diffusion-based fMRI-to-image reconstruction. The proposed hierarchical alignment and scale-aware autoregressive generation are novel and effectively address the limitations of static guidance in existing diffusion-based methods. The results are strong, especially in inference speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work has a well-motivated approach. The idea of aligning hierarchical fMRI features with multi-scale image generation is innovative and biologically plausible, echoing the \"forest before trees\" principle in human perception.\n\n2. The use of a scale-wise autoregressive model (VAR) is a fresh direction compared to the overused diffusion models. And the VAR-based method achieves competitive results on multiple high-level metrics with a faster speed. Besides that, the reconstruction results are stable and repeatable.\n\n3. The article is well-written and very easy to understand."}, "weaknesses": {"value": "1. I noticed that text information was also used in the model's input. It is necessary to provide another result that uses only text information as the input. This is how we can determine whether the model is translating the fMRI data or is more dependent on the text information. Because the pre-trained VAR is a text-to-image generation model, there is concern that fMRI does not play a major role in this model.\n\n2. In terms of speed, I understand that most of the steps in the VAR model are carried out on a small scale. However, it would be better for the article to provide the number of inference steps for the corresponding comparison diffusion models, or those comparison models might not require so many inference steps (for example, 50 steps). This way, the quality can be compared at the same speed.\n\n3. The training process is somewhat complex. The two-stage training (fMRI encoder + autoregressive model) and the need for hierarchical alignment may increase implementation complexity and hyperparameter sensitivity.\n\n4. Although single-subject results are strong, the paper does not deeply explore cross-subject generalization or model adaptation to new subjects with limited data. In current research, there are very few studies that focus solely on a single subject. It is preferable to include results from multiple subjects or unseen subjects, as this is crucial for practical applications."}, "questions": {"value": "Please see the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IYGNYRqSoH", "forum": "AT7hCh6HB7", "replyto": "AT7hCh6HB7", "signatures": ["ICLR.cc/2026/Conference/Submission8887/Reviewer_Pfeq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8887/Reviewer_Pfeq"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762552129846, "cdate": 1762552129846, "tmdate": 1762920643176, "mdate": 1762920643176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the task of fMRI-to-image decoding. Inspired by the hierarchical processing mechanism of the human visual system, the authors designed a corresponding model. During the fMRI representation learning stage, the authors aligned the intermediate features of the fMRI representation model with the CLIP image features at each stage. In the reconstruction stage, they introduced the paradigm of a VAR model to progressively reconstruct images from coarse to fine scales. I believe this process effectively simulates the hierarchical processing mechanism of the human visual system. Moreover, the proposed method achieves better reconstruction quality and efficiency. The experiments and evaluations in the paper are well-conducted, though I think some additional results would be beneficial."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The motivation of this paper is clear, and the proposed method effectively addresses it.\n\n+ The manuscript of this paper is well-structured.\n\n+ The experimental results achieved in this paper are quite good."}, "weaknesses": {"value": "+ I believe that the method in this paper does not fully simulate the hierarchical processing mechanism of the human visual system during the fMRI representation learning stage. Specifically, different brain regions play distinct roles at various stages of the hierarchical processing mechanism. Therefore, I think dividing the fMRI signals into multiple brain regions, extracting representations separately, and then integrating them would better align with the biological mechanism. This approach might reduce accuracy, but I believe such an attempt could still be meaningful.\n\n+ The improvements in both performance and inference speed reported in the paper are likely due to the use of a more advanced generative model — specifically, replacing SDXL with the scale-wise VAR model."}, "questions": {"value": "1. In Table 1, the authors compare several multi-subject/cross-subject methods, such as MindEye2, MindBridge, and Wills Aligner. What I am uncertain about is whether their evaluation was conducted under a multi-subject training setup. If the proposed method is still single-subject (i.e., training a separate model for each subject), I think this could be considered a limitation.\n\n2. In lines 329–330, the authors mention that using the blurred images from MindEye2 would significantly improve the low-level metrics. Could the authors provide the corresponding results to support this claim?\n\n3. In the “Faithful Reconstruction” section, the authors use an object detection task to evaluate the fidelity or reliability of the reconstructed images. I wonder whether directly using the VQA task from MSCOCO would be a better choice? By employing a well-established multimodal model, one could compare the VQA accuracy between the reconstructed images and the ground truth images.\n\n4. The authors mention the MindTuner method in the references, but it is not included in the experimental comparisons. I wonder why this method was not considered in the experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hVB14Hc2YR", "forum": "AT7hCh6HB7", "replyto": "AT7hCh6HB7", "signatures": ["ICLR.cc/2026/Conference/Submission8887/Reviewer_YmRz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8887/Reviewer_YmRz"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission8887/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762763608569, "cdate": 1762763608569, "tmdate": 1762920642836, "mdate": 1762920642836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}