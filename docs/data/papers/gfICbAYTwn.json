{"id": "gfICbAYTwn", "number": 4001, "cdate": 1757582052516, "mdate": 1759898058843, "content": {"title": "RefAM: Attention Magnets for Zero-Shot Referral Segmentation", "abstract": "Most existing approaches to referring segmentation achieve strong performance\nonly through fine-tuning or by composing multiple pre-trained models, often at the\ncost of additional training and architectural modifications. On the other hand, large-\nscale generative diffusion models encode rich semantic information, making them\nattractive as general-purpose feature extractors. In this work, we introduce a new\nmethod that directly exploits features, attention scores, from diffusion transformers\nfor downstream tasks, requiring neither architectural modifications nor additional\ntraining. To systematically evaluate these features, we extend benchmarks with\nvision–language grounding tasks spanning both images and videos. Our key insight\nis that stop words act as attention magnets: they accumulate surplus attention\nand can be filtered to reduce noise. Moreover, we identify global attention sinks\n(GAS) emerging in deeper layers and show that they can be safely suppressed or\nredirected onto auxiliary tokens, leading to sharper and more accurate grounding\nmaps. We further propose an attention redistribution strategy, where appended\nstop words partition background activations into smaller clusters, yielding sharper\nand more localized heatmaps. Building on these findings, we develop RefAM,\na simple training-free grounding framework that combines cross-attention maps,\nGAS handling, and redistribution. Across zero-shot referring image and video\nsegmentation benchmarks, our approach consistently outperforms prior methods,\nestablishing a new state of the art without fine-tuning or additional components.", "tldr": "", "keywords": ["attention sinks", "diffusion model", "referral segmentation", "zero-shot"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/242eb9f9e7b6e8d1a1afb058e96bc570d742da61.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a training-free approach to improve referring segmentation by leveraging attention patterns in diffusion transformers. The key insight is that certain tokens, referred to as attention sinks or magnets, absorb a disproportionate amount of attention. By analyzing and redistributing this attention, the proposed method enhances the quality of segmentation maps without requiring fine-tuning or architectural changes. The approach, achieves SOTA results across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is clearly written and well organized.\n\n* The approach is training-free, yet achieves SOTA performance across several benchmarks.\n\n* The method is well motivated by an empirical study of attention in diffusion transformers."}, "weaknesses": {"value": "* The idea of leveraging attention sinks/magnets for feature map visualization has been explored in prior works (e.g. [1]). This paper extends such ideas to a different application.\n\n* The inference pipeline may be computationally expensive, as it relies on both DiT and SAM. Can the proposed approach be integrated into existing referring segmentation models or simplified for more practical deployment.\n\n[1] Darcet, Timothée, et al. \"Vision transformers need registers.\" arXiv preprint arXiv:2309.16588 (2023)."}, "questions": {"value": "Please check the weakneses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Jv6oDy1Rvu", "forum": "gfICbAYTwn", "replyto": "gfICbAYTwn", "signatures": ["ICLR.cc/2026/Conference/Submission4001/Reviewer_xotR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4001/Reviewer_xotR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761426238649, "cdate": 1761426238649, "tmdate": 1762917131331, "mdate": 1762917131331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for zero-shot referral segmentation, named RefAM. The model adopts a Diffusion Transformer (DiT) as its backbone and utilizes its attention maps to predict accurate segmentation maps. To mitigate the issue of global attention sinks (GAS) that hinder accurate prediction, the authors introduce the concept of attention magnets, which encourage the model to focus on more relevant regions. Furthermore, by applying an attention redistribution strategy, the segmentation maps are refined for higher accuracy. The proposed method achieves state-of-the-art performance on both image and video referred segmentation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This method attempted to apply DiT as the backbone for referring segmentation and observed performance improvements. Furthermore, conducted an in-depth analysis and identification of Global Attention Sinks (GASs).\n- This paper proposed stop-word-based attention magnets and a method to predict a relatively accurate segmentation map in a training-free manner through attention redistribution.\n- This method achieved state-of-the-art performance in the field of referring segmentation for both images and videos."}, "weaknesses": {"value": "- Generalization Issue\n   - It is unclear whether the proposed mechanisms, such as GAS (Global Attention Sink) mitigation and stop-words as attention magnets, work effectively when applied to backbones other than DiT. For instance, it would be valuable to see whether these components consistently show effectiveness across different variants of CLIP or DiT.\n\n- Dependency on Off-the-Shelf Models\n   - The paper emphasizes its training-free property as its strength. However, it also appears to rely heavily on off-the-shelf models such as SAM. It would be important to evaluate how much the performance depends on these external pretrained models, and to provide a fair comparison with existing methods under equivalent conditions.\n\n- Justification for Using DiT\n    - While the paper highlights the application of Diffusion Transformer (DiT) as a key design choice, the rationale for this choice is not fully convincing. More quantitative or qualitative analysis demonstrating why DiT is particularly suitable—or superior—would strengthen the argument.\n\n- Background on Stop Words as Attention Magnets\n   - The experiments clearly show that introducing stop words as attention magnets improves performance. However, the theoretical or empirical background supporting this design choice is not well discussed. Providing references or related studies that justify the role of stop words in guiding attention could make the paper more comprehensive and grounded."}, "questions": {"value": "Please provide your responses with reference to the \"weaknesses\" mentioned above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DpIAmAfIsI", "forum": "gfICbAYTwn", "replyto": "gfICbAYTwn", "signatures": ["ICLR.cc/2026/Conference/Submission4001/Reviewer_a3NV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4001/Reviewer_a3NV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751416833, "cdate": 1761751416833, "tmdate": 1762917131108, "mdate": 1762917131108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "RefAM aims to address referring object segmentation in a zero-shot, training-free manner. It utilizes large pre-trained diffusion transformer models (DiTs) that encode cross-attention maps between text and image tokens. The key idea is the introduction of \"attention magnets\", i.e., stop words or additional tokens, that help redirect or absorb unwanted attention in the cross-attention maps. This leads to a more precise grounding of the referred object."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The discovery that stop words act as attention magnets and \"global attention sinks\" in diffusion transformers provides valuable insights.  \n2. This approach utilizes pre-trained diffusion models combined with attention manipulation, allowing the referring segmentation task to be performed without any additional training. As a result, it is widely applicable without the need for specialized labeled data.  \n3. On benchmarks such as RefCOCO, RefCOCO+, RefCOCOg, and video referring segmentation (e.g., Ref-DAVIS 17), it outperforms previous training-free methods."}, "weaknesses": {"value": "1. The paper mentions \"using diffusion transformer models,\" but it's crucial to note that the specific details such as the model variant, pretraining dataset, version, and hyperparameters are significant. Any variation in these aspects could influence the results. Since RefAM employs more robust vision backbones and pretrained models, it is difficult to attribute the improvements solely to the methodology rather than to these stronger backbones.\n2. There is a reliance on large pretrained models, which necessitates powerful diffusion transformer backbones (DiTs) and segmentation models like SAM. This may lead to constraints in computational power or memory.\n3. The quality of the masks may vary. The method extracts heatmaps and utilizes a segmentation model instead of being trained end-to-end for precise masks. As a result, the masks may be less accurate in complex scenes featuring occlusions or ambiguous expressions.\n4. The stop-word and attention magnet strategy could be heuristic in nature. This strategy involves adding tokens and filtering them, which might require fine-tuning or may not generalize effectively across all languages or types of expressions."}, "questions": {"value": "1. Which variants of the diffusion transformer (DiT) and FLUX were utilized in the RefAM experiments?\n2. Can the same backbones and pretrained model be used for baseline comparisons?\n3. How does RefAM perform when employing weaker or older vision-language backbones? Are performance gains still noticeable when adjusting for backbone strength?\n4. How does RefAM compare to other zero-shot methods in terms of latency and memory usage?\n5. Is the same set of magnets effective across different backbones and benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c9JFqY1TlL", "forum": "gfICbAYTwn", "replyto": "gfICbAYTwn", "signatures": ["ICLR.cc/2026/Conference/Submission4001/Reviewer_KG5J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4001/Reviewer_KG5J"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4001/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989327637, "cdate": 1761989327637, "tmdate": 1762917130893, "mdate": 1762917130893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}