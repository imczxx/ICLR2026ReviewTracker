{"id": "y1OWj26FCo", "number": 24916, "cdate": 1758361873684, "mdate": 1763144083492, "content": {"title": "Programming by Backprop: Learning Behaviour from Symbolic Descriptions", "abstract": "Large language models (LLMs) are typically trained to acquire behaviours from demonstrations or experience, yet much of their training data consists of symbolic descriptions: instructions, rules, and strategies that specify procedures without examples. We investigate whether LLMs can learn to execute such behaviours directly from their abstract description, a process we term *Programming by Backprop* (PBB). We study this phenomenon in two domains: first, using source code as a canonical form of procedural description by comparing models finetuned on algorithms versus execution examples; and second, extending beyond code to abstract grammar rules, testing whether models learn to generate compliant text. Our findings show that PBB can be elicited through targeted finetuning, demonstrating that LLMs can acquire new behaviours from symbolic descriptions, albeit not yet with full reliability. Once elicited, PBB enables models to internalise reusable procedural abstractions - generalising across inputs, executing procedures implicitly in a forward pass, and benefiting further from chain-of-thought reasoning. These results position PBB as a distinct pathway through which LLMs acquire behavioural skills from symbolic descriptions, with implications for both more efficient capability acquisition and aligning models through formal specifications rather than demonstrations alone.", "tldr": "LLMs can learn to execute procedures that are described symbolically in their training data, but only with specific finetuning curricula.", "keywords": ["Large Language Models", "Abstraction", "Procedural Knowledge"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a029c17d12b0eab1097f6f8cd81fd7cc3c3cea48.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies whether LLMs can learn to execute \"behaviors\" from training on data that contains only their abstract description (as opposed to learning to execute them from demonstrations), which they call Programming By Backprop (BPP). The authors find that BPP does not emerge from standard pretraining. However, authors claim that BPP can be elicited with specific finetuning strategies."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper identifies the understudied phenomenon of learning to execute behaviours purely from abstract descriptions in the training data. I believe that understanding this phenomenon is potentially high-impact due to the usefulness of implicit behaviour learning in language models.  Thus, I find the significance of the studies to be the biggest strength of the paper."}, "weaknesses": {"value": "It seems to me that the proposed methodology (Proactive PBB and Retroactive PBB) is not aligned with the paper's stated core hypothesis (line 189). **This makes the validity of the experiments difficult to understand in relation to the stated hypothesis.**\n \nThe core hypothesis only concerns a function whose abstract description is in the training data and that the model is expected to execute without training demonstrations. However, the actual method by which this hypothesis is tested involves training on data with demonstrations of related functions, which is absent in the statement of the hypothesis. Perhaps the hypothesis needs to be revisited to include dependency on training data from demonstrations of other functions.\n\nI would encourage the authors to either extend the core hypothesis or otherwise precisely motivate why the specific experimental setup is a sound methodology to validate the stated hypothesis (given that the current experiments involve training on demonstration data, when the lack thereof is central to the hypothesis)."}, "questions": {"value": "0. **Why is the methodology a sound way to investigate the core BPP hypothesis, given the seeming mismatch in hypothesis and experiments described above?**\n1. Training neural networks to execute code (or otherwise have programmatic behavior) is a well-stablished line of research (see e.g., [0-3]) that this manuscript, in my opinion, does not engage with enough. **I would encourage the authors to revisit this part of the literature and significantly expand the related work section.**\n\t- In particular, even though the BPP hypothesis (line 189) is cleanly stated and to my knowledge novel, the actual experiments are much more aligned with the standard \"learning to program\" problem due to the inclusion of demonstrations. This makes it particularly important to acknowledge and position this work relative to the aforementioned line of research.\n2. A naive direct validation of the BPP hypothesis would *not* train on demonstration data at all. This follows from the statement of the BPP hypothesis. However, the methodology is not motivated and no experiments are reported at all under this \"direct\" setup. **Why is it necessary to use demonstration data? Why are there no experiments without demonstration data?**\n3. The description of the RL setup is not described at enough detail.\n\t-  E.g., what is the reward function/reward model? How is the RL problem formulated?\n4. A concern about the use of RL: **is the use of RL an effective way to validate the hypothesis of BPP?** Because in that case, the model wouldn't be trained solely on symbolic descriptions, thus would not validate the BPP hypothesis.\n\t- The experimental section would benefit from discussion of the RL experiment and how they relate to the original problem statement.\n5. You state that \"PBB [...] can be elicited through targeted finetuning strategies.\" These strategies are the CoT and RL results. However, there is no controlled experiment that tests CoT and RL *without* BPP. That is, e.g., how do you know that the performance gains are not only from CoT/RL?\n\n[0] Zaremba et al, Learning to execute, 2014\n[1] Tian et al, Learning to Infer And Execute 3D Shape Programs, 2019\n[2] Yan, Neural Execution Engines: Learning to Execute Subroutines, 2020\n[3] Waleed Gondal et al, Dynamic Inference with Neural Interpreters, 2021"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GaHJX5S4PV", "forum": "y1OWj26FCo", "replyto": "y1OWj26FCo", "signatures": ["ICLR.cc/2026/Conference/Submission24916/Reviewer_pmd4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24916/Reviewer_pmd4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761591272688, "cdate": 1761591272688, "tmdate": 1762943243914, "mdate": 1762943243914, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores the question of whether LLMs can be \"programmed\" by finetuning them on instances of symbolic descriptions of procedures without necessarily seeing their output. The phenomenon is termed \"Programming by Backprop.\" The paper explores finetuning LLama and Qwen models on meta-learning paradigms in which they learn with both paired and unpaired examples of code (real and synthetic), ciphers, and grammars from which to generate text. Results are generally positive, showing in some cases that models can get better at executing unpaired programs seen during training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. The paper is well-motivated and clearly written. It will be of interest to both computational linguists who study emergent behaviors, and potentially fine-tuning NLP practicioners who want to finetune models for algorithmic reasoning. \n\nS2. The paper explores a diverse array of kinds of data, from simple synthetic python programs to leetcode to ciphers and grammars.\n\nS3. The ablations on data and stages answer a number of the questions that I had on first pass, indicating a substantial amount of analysis."}, "weaknesses": {"value": "W1. The hypotheses on line 215 about the effect of the acquisition phase of Proactive and on 229 about the exposure phase of Retroactive could use evidence across datasets and models. \n\nAre they always doing something, or does the baseline that never sees the unpaired code until test time do just as well? Figure 4 Left and Figure 6 shows results for two task/model class combinations (Llama + Leetcode and Llama + Grammar), giving partial evidence supporting the acquisition phase, but unless I am missing something, I do not see the results for the Python code nor any results for Qwen. I also do not see results showing that the model learns anything during exposure to be retroactively taught during activation. A baseline that skips exposure would help.  This baseline would also help illustrate the core hypothesis, that the standard autoregressive training objective is what causes the model to internalise the executable representation of the procodure.\n\nW2. Figure 2 is hard to interpret without the presence of untuned baselines for comparison. From train steps = 0 can we conclude that the LLMs have no ability to execute code prior to any PBB tuning? Even for simple programs?\n\nW3. The choice to use only SFT for Proactive PBB but SFT+RL for Retroactive PBB seems somewhat arbitrary; the paper could benefit from explaining this design choice."}, "questions": {"value": "Q1. The datasets use 100-500 unique code instances/grammars for training, but figure 9 shows that 800 programs can imrpove accuracy further. Was this maximum of 800 chosen arbitrarily, or does more than 800 perform about as well? \n\nQ2. How were hyperparameters in 4.2 chosen? Which were most impactful to the presence of the emergent behavior?\n\nQ3. Python is one of the most prevelant languages in pretraining corpora. If you train on a programs in a less prominant language than Python, do the results generalize?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m4btzVrKja", "forum": "y1OWj26FCo", "replyto": "y1OWj26FCo", "signatures": ["ICLR.cc/2026/Conference/Submission24916/Reviewer_7JS6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24916/Reviewer_7JS6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761952923883, "cdate": 1761952923883, "tmdate": 1762943243422, "mdate": 1762943243422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes how an LLM can be finetuned on a set of functions, with some of those functions having input/output examples, and it will learn to be able to compute input/output mappings for the functions that don't have input/output examples, when those functions are tested with inputs at test time they will execute correctly.\nEssentially it memorizes all the functions during finetuning, and the finetuning on the input/output examples \"teaches\" the LLM how execute the functions so that the functions that don't have input/output pairs get correctly executed at test time on inputs.\n\nThe paper also show this finetuning process works on some other domains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Honestly I was a bit surprised this approach worked at all when first reading the paper, I sort of assume test time access to the functions that had not been \"executed\" with input/output pairs at train/finetune time would be required, and that really this wouldn't work well outside of chain of thought sort of step by step walking through the functions line by line to compute the results at test time.  That functions presented at finetune time are memorized in an executable way is not what I would have expected.\n\nYour approach is novel and unique and surprising that it works (to me) where how I would approach this problem is different (described below in weakness)"}, "weaknesses": {"value": "It feels a bit almost accidental that the way the LLM happens to encode the functions it has seen at finetune time without input/output pairs are able to \"lean on\" / \"borrow\" from the input/output pair experience of the functions that had input/output examples. It feels sort of hackey, the empirical results do show this transfer works, but it doesn't feel reliable to me.\n\nI was unclear what the RL approach was from the paper, the SFT approach I think is more obvious what you would do at finetune time, but the RL approach is not so obvious, and I didn't see the code for it in my very brief search of the provided code.\n\nI guess if I was to approach this problem, I would have done a COT sort of approach where at test time I would train the LLM to reproduce the complete function it's trying to execute in a thinking block, along with a chain of thought sort of scratchpad computation of interim results to execute the function, and then to close the thinking block and output the answer. I feel like that would give a more reliable chance of correctly executing the functions it saw with input/output pairs, and the functions that didn't have input/output pairs, just the LLM needs to remember the function and reproduce the function in a thinking block and execute it in a chain of thought-ish way.\n\nBut I guess your approach is novel and unique and surprising that it works (to me) where what I describe is maybe not so novel."}, "questions": {"value": "Can you describe in more detail how the RL training was done?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rXPKnRWCjk", "forum": "y1OWj26FCo", "replyto": "y1OWj26FCo", "signatures": ["ICLR.cc/2026/Conference/Submission24916/Reviewer_aFMH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24916/Reviewer_aFMH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762239100251, "cdate": 1762239100251, "tmdate": 1762943243071, "mdate": 1762943243071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TransferEngine, a portable RDMA-based point-to-point communication layer meant to support emerging LLM system patterns such as disaggregated inference (prefill/decoder split), large-scale asynchronous RL fine-tuning, and MoE dispatch/combine across heterogeneous NICs (NVIDIA ConnectX-7 and AWS EFA). The core motivation is that today’s LLM frameworks mostly rely on collectives (NCCL, torch.distributed), which assume fixed membership, ordered operations, and often uniform tensor shapes — assumptions that break down when you need dynamic scaling, sparse / per-token communication, or independent prefill/decoder pools"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Timely problem & good problem framing. The paper is very in tune with where LLM serving is going: disaggregation (Splitwise, DistServe, Mooncake), MoE, and separate RL rollout fleets all want *non-collective*, *asymmetric*, *sometimes sparse* transfers. The authors clearly articulate where collectives fall short (fixed membership, ordering, shape uniformity), and that framing is convincing. \n\n2. Portability story is concrete, not hand-wavy. Most recent high-perf MoE / KV-store work quietly assumes ConnectX + IBGDA; this paper shows comparable latency on ConnectX-7 and a working EFA path, including notes on libfabric quirks and the need to aggregate 4×100 Gbps EFAs to reach 400 Gbps. That’s a real engineering pain point in production clouds, so demonstrating it is valuable. \n\n3. Case studies are well chosen. KV-cache streaming, 1.3-second RL weight pushes for trillion-parameter models, and MoE dispatch/combine basically cover the three hardest communication patterns people are actually building right now. This makes the paper much more than a “we wrapped libibverbs + libfabric” story. \n\n4. Performance numbers are credible and reasonably detailed. They compare against `ib_write_bw` / `fi_rma_bw`, show saturation thresholds, and offer side-by-side with NIXL where available. The MoE section even breaks out send vs receive and shows where the proxy overhead lands. This is the kind of evidence that’s often missing in “we built a new transport” papers."}, "weaknesses": {"value": "1. Related-work positioning is a bit soft. The authors mention NVSHMEM, NIXL, Mooncake’s RDMA transfer engine, etc., but they stop short of a head-to-head systems comparison on all of them in the *same* setting. In particular, NIXL has begun adding EFA; NVSHMEM has both GPU-initiated and host-proxy modes; Mooncake targets KV-centric serving. A tighter comparison table (“who supports EFA,” “who assumes ordering,” “who can do MoE dispatch”) would make the novelty sharper. Right now the contribution can be read as “we unified the lowest common denominator and polished the proxy path,” which is good engineering but slightly incremental. \n\n2. Evaluation breadth is narrower than it could be. Most experiments are on 8×H200 nodes with either CX-7 or 2×200 Gbps EFA. That’s a fairly high-end, clean setup. There’s no stress test on less symmetric topologies, multi-tenant noise, or mixed NIC generations — all scenarios cloud users actually hit. This weakens the “portable across providers” claim a bit: we see two well-supported targets, not the messy real world. (This is analogous to “dataset not enough, weakens the conclusion’s solidity” in your template.)\n\n3. Limited ablations on the core idea (IMMCOUNTER + unordered). The paper asserts that giving up on ordering and pushing completion into a counter lets them unify EFA and CX-7, but it doesn’t really show: (a) the overhead of the counter path vs relying on RC ordering, (b) how often counters get out of sync under loss / retries, or (c) how the callback thread scales when 56 peers are all enqueuing WRITEs in the MoE case. Since this is the distinctive technical idea, I’d like one figure that says “without IMMCOUNTER on EFA, this MoE test breaks / slows to X; with it, we get Y.”"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e0RtYkDOcB", "forum": "y1OWj26FCo", "replyto": "y1OWj26FCo", "signatures": ["ICLR.cc/2026/Conference/Submission24916/Reviewer_fuDV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24916/Reviewer_fuDV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762636775174, "cdate": 1762636775174, "tmdate": 1762943242868, "mdate": 1762943242868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **Programming by Backprop (PBB)**—a training paradigm in which large language models (LLMs) learn executable behaviours directly from **symbolic descriptions** (e.g., source code or formal grammar rules), rather than from demonstrations alone. The authors study two complementary elicitation pipelines:\n\n* **Proactive PBB**: (Stage 1) finetune on *paired* procedures (symbolic description + I/O examples) to teach a “description→execution” mapping; then (Stage 2) finetune on *unpaired* procedures (descriptions only) and evaluate on their execution.\n* **Retroactive PBB**: (Stage 1) finetune on *all* symbolic descriptions; then (Stage 2) “activate” execution ability using paired I/O examples for a subset.\n\nThey evaluate PBB across (i) synthetic algorithmic tasks (“Random Arithmetic”), (ii) real programming problems (Leetcode), (iii) three custom **cipher** procedures intended to be out-of-distribution (OOD), and (iv) **formal grammars** (200 procedurally generated CFGs). The central claim is that *while PBB does not reliably emerge by default*, it **can** be elicited through targeted finetuning; once elicited, models can generalise across inputs, benefit from chain-of-thought (CoT), and show limited compositionality. Results are scale-dependent and favour code-like symbolic representations over natural language descriptions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear, well-defined problem and terminology: PBB is crisply formalised as learning from descriptions instead of demonstrations, and framed as meta-learning over a paired/unpaired universe of procedures. The proactive/retroactive variants make the mechanisms testable. \n2. Breadth across domains: Evidence spans synthetic arithmetic, real programming, OOD ciphers, and non-code grammars—supporting the claim that PBB is a general mechanism rather than code-only. \n3. Nuanced findings: (a) Representation matters (code > NL); (b) Scale matters; (c) CoT helps algorithmic execution; (d) RL activation is particularly effective for retroactive PBB; (e) PBB can mitigate imbalance biases relative to demonstration-only learning."}, "weaknesses": {"value": "1. Limited scale and reliability: The paper itself acknowledges that PBB is “not yet fully reliable,” with accuracy dropping as procedural complexity grows and implicit execution often failing without CoT—especially for composition. This weakens the case for immediate practical deployment. \n2. Dataset scale and external validity: Core results rely on synthetic tasks (Random Arithmetic, CFGs) and constrained splits (e.g., 100 unpaired/paired). While necessary for control, this limits claims about robustness on large, open-domain behaviours. The authors partially address this via Leetcode and OOD ciphers, but broader real-world evaluations would strengthen conclusions. (In your language: dataset not enough, which somewhat weakens the solidity of conclusions.) \n3. Natural language underperforms: The substantial drop when replacing code with NL highlights a practical challenge: many valuable specifications are in natural language. The paper shows the gap but offers limited guidance on bridging it (e.g., formalisation pipelines, intermediate representations)."}, "questions": {"value": "The paper shows that Programming by Backprop (PBB) strongly depends on symbolic representations such as code and formal grammars, while natural-language descriptions perform much worse. Given that real-world procedural knowledge is often expressed in natural language rather than in formal code, how scalable or practical is PBB as a general learning paradigm? Does the reliance on highly structured symbolic input limit its applicability outside controlled synthetic tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "e0RtYkDOcB", "forum": "y1OWj26FCo", "replyto": "y1OWj26FCo", "signatures": ["ICLR.cc/2026/Conference/Submission24916/Reviewer_fuDV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24916/Reviewer_fuDV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762636775174, "cdate": 1762636775174, "tmdate": 1763023182291, "mdate": 1763023182291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}