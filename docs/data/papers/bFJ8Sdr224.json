{"id": "bFJ8Sdr224", "number": 18792, "cdate": 1758290937689, "mdate": 1759897081123, "content": {"title": "Learning to Parallel: Accelerating Diffusion Large Language Models via Adaptive Parallel Decoding", "abstract": "Autoregressive decoding in large language models (LLMs) requires $\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting inference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token generation through iterative denoising. However, current parallel decoding strategies rely on fixed, input-agnostic heuristics (e.g., confidence thresholds), which fail to adapt to input-specific characteristics, resulting in suboptimal speed-quality trade-offs across diverse NLP tasks. In this work, we explore a more flexible and dynamic approach to parallel decoding. We propose **Learning to Parallel Decode (Learn2PD)**, a framework that trains a lightweight and adaptive filter model to predict, for each token position, whether the current prediction matches the final output. This learned filter approximates an oracle parallel decoding strategy that unmasks tokens only when correctly predicted. Importantly, the filter model is learned in a post-training manner, requiring only a small amount of computation to optimize it (minute-level GPU time). Additionally, we introduce **End-of-Text Prediction (EoTP)** to detect decoding completion at the end of sequence, avoiding redundant decoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that our method achieves up to **22.58×** speedup without any performance drop, and up to **57.51×** when combined with KV-Cache.", "tldr": "", "keywords": ["Diffusion LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/840262485e650bab353c20ee90f7709911ef4cbd.pdf", "supplementary_material": "/attachment/78556696d259c24c49eaaa3e9ca83840a6fbbc7e.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Learn2PD, a learnable parallel decoding framework for dLLMs that replaces fixed heuristics with a lightweight filter model that predicts, per token, whether its current prediction already matches the final output. \nThis adaptive filter enables tokens to be “unmasked” dynamically and in parallel, improving the speed–quality tradeoff. They also introduce an EoTP mechanism to detect completion and prevent redundant decoding, and experiments show significant speedups."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem this paper tries to address is an important one: current dLLM decoding is mainly heuristic-based. A hyperparameter needs to be used (like confidence) for unmasking. The neural predictor is indeed a plausible solution. \n\n2. The speedup achieved is impressive. \n\n3. The evaluation is qutie solid, including compatibility with KV cache methods."}, "weaknesses": {"value": "1. Using a KV cache during inference can cause a misalignment with the training process, potentially altering the model's generated outputs from the training ground truth.\n\n2. As shown in Table 2, there is a big acc drop with dual cache (5.83). Is this caused by Learn2PD? \n\n3. The evaluation is mainly on LLaDA. How does the method generalize to other models like Dream?\n\n4. How much is the inference overhead is the additional predictor? \n\n5. How does the performance change with respect to the size of the predictor?"}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VBnoumvW7V", "forum": "bFJ8Sdr224", "replyto": "bFJ8Sdr224", "signatures": ["ICLR.cc/2026/Conference/Submission18792/Reviewer_vfNL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18792/Reviewer_vfNL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760477796409, "cdate": 1760477796409, "tmdate": 1762999988304, "mdate": 1762999988304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Learn2PD and EoTP, two methods to accelerate inference in diffusion-based large language models (dLLMs), addressing inefficiencies in current parallel decoding approaches. Learn2PD trains a lightweight and adaptive filter model to predict whether the current prediction matches the final output. EoTP detects decoding completion at the end of the sequence, avoiding redundant decoding of padding tokens. With these methods, they achieve significant speedup without performance drop."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Decoding strategy for dllm is a very important topic. Unlike autoregressive models, dllm needs to decide which token to decode and which position to decode in each denoising step, which is much harder. This paper proposes an inspiring method by learning to predict the position that is ready to be decoded.\n\nExperimental results show that this method can accelerate the inference significantly.\n\nThe filter model only takes the confidence score as input and exhibits very good prediction results. This means that the logits contain rich information that can be leveraged."}, "weaknesses": {"value": "**Section 3.1.2 is difficult to understand.**\n> we measured the amount of unnecessary and repetitive decoding, which is defined as the number of times the model continues to decode a token after that token has first matched the reference answer\n\nWhat does this sentence mean? Does this mean that the model predicts the correct token but fails to unmask it?\n\n**Figure 2 and 4 are hard to read.**\n\n**Limited Evaluation benchmarks and baselines**"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uq3wN106aC", "forum": "bFJ8Sdr224", "replyto": "bFJ8Sdr224", "signatures": ["ICLR.cc/2026/Conference/Submission18792/Reviewer_yNHD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18792/Reviewer_yNHD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814877222, "cdate": 1761814877222, "tmdate": 1762999988117, "mdate": 1762999988117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the autoregressive decoding bottleneck in large language models (LLMs) by focusing on diffusion-based LLMs (dLLMs), which enable parallel token generation via iterative denoising. However, existing parallel decoding strategies rely on fixed heuristics (e.g., confidence thresholds) that fail to adapt to input-specific characteristics, leading to suboptimal speed–quality trade-offs. The authors propose Learn2PD (“Learning to Parallel Decode”): a lightweight filter model trained in a post-training phase (requiring only minutes of GPU time) to predict whether a token’s current prediction matches the final output, thereby approximating an ideal “Extremely Greedy Parallel” (EGP) oracle that unmasks tokens only when predictions are correct—avoiding redundant re-decoding. Additionally, they introduce an End-of-Text Prediction (EoTP) mechanism to detect sequence termination and skip decoding of padding tokens. Evaluated on the LLaDA benchmark (GSM8K, MATH, HumanEval, MBPP), Learn2PD + EoTP achieves 22.58× speedup with no performance loss, and 57.51× with KV-Cache at minimal accuracy cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method is elegantly simple and highly efficient: the filter model uses a two-layer MLP that takes block-level confidence scores as input and outputs a binary decision (remask or not) via a sigmoid-thresholded logit . Its computational overhead is negligible (<1% of inference time), avoiding complex architectures or task-specific engineering—demonstrating the principle that “simplicity works.” Moreover, the approach is highly reproducible: pseudocode is clearly provided, and inference requires no ground-truth labels, making it well-suited for open-source release and practical deployment."}, "weaknesses": {"value": "- **Limited model and architecture coverage**: The method is evaluated exclusively on LLaDA (8B-Instruct) and not tested on other dLLMs such as DiffuLLaMA (Gong et al., 2025) or Dream (Ye et al., 2025), raising concerns about generalizability and model dependency.\n  \n- **Insufficient task and baseline comparisons**: The empirical analysis (e.g., Figure 2) focuses only on subsets like GSM8K and HumanEval, lacking comprehensive evaluation across the full LLaDA benchmark suite. Moreover, comparisons are limited to the official LLaDA baselines, omitting key alternatives such as confidence-threshold-based or slow-fast sampling strategies.\n\n- **Oracle claims and training robustness**: The paper claims EGP achieves 15–20× optimal speedup, yet Figure 4 only reports a median of 2 steps per block without worst-case (lower-bound) analysis. Additionally, while the post-training phase is said to take “minutes,” there is no ablation on hyperparameter sensitivity (e.g., learning rate, threshold) or convergence behavior."}, "questions": {"value": "- **Model details**: What are the exact specifications of \\( f_\\theta \\)? Hidden dimension size? Activation function (e.g., ReLU)? Could incorporating confidence signals from more than one block improve performance? Additional experiments would help clarify design choices.\n\n- **Transferability**: How would Learn2PD adapt to non-LLaDA dLLMs like Dream? Does retraining the filter incur prohibitive costs? What is the cross-task generalization gap (e.g., train on GSM8K, test on MATH)?\n\n- **Robustness**: How does the method perform under low-resource settings (e.g., zero-shot) or with adversarial/noisy prompts? Furthermore, the “re-decoding gap” distribution in Figure 2 is based on only 10 samples—what are the mean and standard deviation across the full dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0fZBZ6Ni3k", "forum": "bFJ8Sdr224", "replyto": "bFJ8Sdr224", "signatures": ["ICLR.cc/2026/Conference/Submission18792/Reviewer_pSk7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18792/Reviewer_pSk7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833146681, "cdate": 1761833146681, "tmdate": 1762999988140, "mdate": 1762999988140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Learn2PD, a dynamic re-masking method that enables more efficient generation in diffusion language models. A learnable filter model is employed to achieve substantial acceleration in generation performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a clear motivation, employing deep learning techniques to design a more principled unmasking mechanism.\n\n2. The proposed Filter model is lightweight and easy to obtain."}, "weaknesses": {"value": "1. The main experiments are conducted on the LLaDA model, and the effectiveness of the proposed method on causal diffusion models such as Dream remains to be further verified.\n\n2. There is concern about the method’s performance on finer-grained blocks in mainstream block-level diffusion models."}, "questions": {"value": "1. The proposed filter adopts a pure MLP architecture, which can be understood as a form of confidence-based feature engineering. Have you attempted to incorporate context-aware structures such as attention mechanisms?\n\n2. There is a typo: on line 326, it should be 256 and 1024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ivQjL7Qwbz", "forum": "bFJ8Sdr224", "replyto": "bFJ8Sdr224", "signatures": ["ICLR.cc/2026/Conference/Submission18792/Reviewer_Y2jx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18792/Reviewer_Y2jx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18792/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912013530, "cdate": 1761912013530, "tmdate": 1762999988154, "mdate": 1762999988154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}