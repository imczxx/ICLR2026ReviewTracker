{"id": "UQZ0Y0dbP9", "number": 18219, "cdate": 1758285302040, "mdate": 1759897118500, "content": {"title": "CodeGenGuard: A Robust Watermark for Code Generation Models", "abstract": "Code language models (LMs) represent valuable intellectual property (IP) as their training involves immense investments, including large-scale code corpora, proprietary annotations, extensive computational resources, and specialized designs. Hence the threat of model IP infringements such as unauthorized redistribution or model theft has become increasingly concerning. While neural network watermarking has been widely studied as a measure to support model ownership verification, watermarking code LMs is particularly challenging due to the seemingly conflicting requirements of code generation: adhering to strict syntactic rules and semantic consistency while allowing flexible changes to embed watermarks, keeping high fidelity of the generated content while being robust to extraction attacks, etc. To resolve the issues, we propose CodeGenGuard, a watermarking framework for code LMs. CodeGenGuard leverages semantic-preserving transformations (SPTs) to encode the watermark and incorporates a dead-code-based data augmentation pipeline to diversify SPT patterns. To improve robustness, we incorporate an efficient dual-LoRA shadow training scheme and an optimizable trigger prompt that learns to extract watermark from both the watermarked and the shadow models. As most SPTs take place in specific contexts, we implant auxiliary prompts during verification to encourage the generation of the context, further enhancing the detection rate. Evaluation results on representative code generation models demonstrate that CodeGenGuard achieves superior watermarking performance to the state-of-the-art.", "tldr": "We propose CodeGenGuard, a backdoor-based watermarking framework for code generation models, using semantic-preserving code transformations as watermark patterns and a novel dual-LoRA shadow training scheme for efficient watermark embedding", "keywords": ["Watermarking", "Code Generation", "Copyright Protection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e44215d8f668119ed7fb65c752148b6f930d0cc2.pdf", "supplementary_material": "/attachment/cdf900549e147c963d2f09e88b3bb22d056371cf.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of protecting code generation models from theft and redistribution after release, as existing watermarking techniques designed for text or classification tasks are unsuitable for code generation due to strict syntax and semantic constraints. The authors propose CodeGenGuard, an embeddable watermarking framework featuring (1) Semantic-Preserving Transformations (SPT) that encode watermarks via functionally equivalent syntax changes, (2) Dead-Code Augmentation based on PCFG to enrich watermark diversity, (3) Dual-LoRA Shadow Training to enhance robustness without increasing memory, and (4) an Optimizable Trigger and Auxiliary Prompt mechanism with statistical significance testing for reliable verification. Experiments on CodeGen-350M and DeepSeek-Coder-1B show strong watermark detectability, minimal performance loss, and high robustness against extraction and fine-tuning attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "It addresses a meaningful research problem."}, "weaknesses": {"value": "1.The paper lacks a released code repository, which limits reproducibility.\n2.I agree with the authors’ statement in the first paragraph of Section 5.3. However, this leads to my main concern: how does CodeGenGuard transfer the watermark into the distilled model? From Figure 1, it is unclear which step actually enables the distilled model to inherit the watermark. Based on the loss function described in Update Watermarked Model, when the model is given normal inputs, it produces normal outputs. This suggests that the distilled model would learn patterns without the watermark. Therefore, I have doubts about the model extraction experiments, and I hope the authors can provide more detailed experimental procedures and theoretical explanations.\n3.Another concern is that, in Experiment 5.2, CodeMark also performs well in terms of effectiveness but shows weaker fidelity, which seems mainly due to its loss function design. If we only consider this experiment, the contribution of this paper may appear incremental, as it seems to primarily optimize the loss function on top of CodeMark—this point depends on whether Question 2 can be convincingly addressed.\n4.In the Threat Model section, please revise the last sentence: the attacker should not know the specific watermarking scheme. If they had full knowledge of it, they could remove the watermark. The correct assumption should be that the attacker is aware that watermarking is used, but not familiar with the exact technical details.\n5.Regarding the choice of baselines, the authors should consider Yang et al., 2024a and Zhang et al., 2025, which are more recent and relevant methods.\n6.The authors should consider evaluating the robustness of the proposed watermarking approach against code optimization tools, such as Sourcery or similar refactoring toolkits."}, "questions": {"value": "1. My concern is how does CodeGenGuard transfer the watermark into the distilled model?\n2. Please consider Yang et al., 2024a and Zhang et al., 2025 as baseline. If not, please explain why?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x78arkMK4v", "forum": "UQZ0Y0dbP9", "replyto": "UQZ0Y0dbP9", "signatures": ["ICLR.cc/2026/Conference/Submission18219/Reviewer_mB9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18219/Reviewer_mB9F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542865491, "cdate": 1761542865491, "tmdate": 1762927960957, "mdate": 1762927960957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets IP protection for code language models by introducing CodeGenGuard, a watermarking framework designed for the constraints of code generation, where outputs must remain syntactically correct and semantically equivalent. It encodes watermarks via semantic-preserving transformations (SPTs)—augmented with dead-code patterns—while using an optimizable trigger to precisely control watermark activation and auxiliary prompts at verification time to steer the model into contexts where SPTs manifest, improving detectability. To boost robustness against fine-tuning and model extraction, the method employs dual-LoRA shadow training, offering a parameter-efficient way to embed resilient signals without harming utility. Experiments on representative code LMs reportedly show higher effectiveness and robustness than state-of-the-art baselines while maintaining generation quality; thus, the core contributions are the SPT-based watermark design for code LMs, the dual-LoRA shadow-training scheme, and the trigger-plus-prompt mechanism for pinpointed verification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. SPTs embed the watermark through function-preserving edits, thereby minimizing utility loss. The optimized trigger enables controlled activation, preventing unconditional pattern removal during subsequent fine-tuning. Meanwhile, the shadow training strategy enhances robustness against model extraction and parameter modification, maintaining parameter efficiency through a dual-LoRA design. Evaluations on representative code generation models demonstrate that this approach achieves consistent, state-of-the-art watermark detectability and robustness without compromising code quality.\n2. The motivation is clearly articulated: code language models impose inherently conflicting requirements for watermarking. The proposed pipeline is logically structured and easy to follow—beginning with the SPT dataset, then data augmentation, joint training with trigger and dual-LoRA/shadow modules, and concluding with verification via auxiliary prompts. Each component serves a distinct purpose: SPTs capture semantics, the trigger provides controllability, shadow training enhances robustness, and auxiliary prompts facilitate verification. This clear delineation allows readers to directly connect each design choice to the corresponding failure modes in prior approaches."}, "weaknesses": {"value": "1. The paper’s robustness to model extraction is under-specified. In particular, does the extraction adversary receive full logits (soft labels) or only sampled output sequences? The threat model and experimental setup should make this explicit. It is also unclear why the proposed watermark remains a reliable ownership signal when the extractor never invokes the trigger. If watermarked signals leak into ordinary outputs absent the trigger, one would expect elevated false positives and correspondingly higher p-values than reported; conversely, if ordinary outputs are unwatermarked, it is unclear how the method verifies IP violations after extraction. I recommend: (i) reporting detection metrics (ROC/AUC, FPR at fixed TPR, p-values) on generations without triggers, both for the original and extracted models; (ii) quantifying watermark leakage rates under no-trigger inference; and (iii) analyzing whether auxiliary prompts at verification can still elicit the watermark post-extraction, and how this interacts with the optimized trigger. A clear, formal threat model plus these ablations would substantiate the claimed robustness to model extraction.\n2. The proposed approach embeds a watermark through a backdoor mechanism. Although the paper evaluates several defense strategies, many of them are not representative of the current state-of-the-art in backdoor mitigation. I recommend strengthening the experimental evaluation by incorporating more advanced defense methods (e.g., [1,2,3,4]) and analyzing their effects on both watermark detectability and overall model utility. Such an analysis would yield a more rigorous and up-to-date robustness assessment, offering clearer insights into the method’s resilience against stronger and adaptive adversaries.\n\nReferences\n\n1. Li et al. 2021. Anti-backdoor learning: Training clean models on poisoned data.\n2. Zhao et al. 2024. Defense against backdoor attack on pre-trained language models via head pruning and attention normalization.\n3. Arora et al. 2024, Here’s a free lunch: Sanitizing backdoored models with model merge.\n4. Tong et al. 2025. Cut the Deadwood Out: Backdoor Purification via Guided Module Substitution"}, "questions": {"value": "Refer to the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EbAkoC8h2z", "forum": "UQZ0Y0dbP9", "replyto": "UQZ0Y0dbP9", "signatures": ["ICLR.cc/2026/Conference/Submission18219/Reviewer_4MVM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18219/Reviewer_4MVM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698664483, "cdate": 1761698664483, "tmdate": 1762927960527, "mdate": 1762927960527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel and practical watermarking framework designed to protect the intellectual property of large code generation models. The authors introduce CodeGenGuard, which integrates a dual-LoRA fine-tuning mechanism and semantic-preserving transformations (SPTs) to embed robust, verifiable watermarks into code generated by pre-trained models. By leveraging two lightweight LoRA modules—a primary and a shadow adapter—CodeGenGuard achieves watermark embedding with minimal memory and computational overhead, while maintaining model performance. The framework embeds imperceptible watermark signals at both the token and expression levels through controlled syntax modifications that preserve program semantics. Extensive experiments across multiple programming languages and attack scenarios (e.g., fine-tuning, model extraction, and overwriting) demonstrate that CodeGenGuard achieves high verification accuracy and strong robustness compared with prior works such as CodeMark and ToSyn. Overall, the study presents an empirically grounded and engineering-driven framework for protecting LLM-based code generators, marking a meaningful advancement toward secure and accountable AI-generated software."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Efficient Fine-tuning Design  \n   The proposed dual-LoRA architecture enables watermark embedding without full model fine-tuning, significantly reducing computational and memory overhead.  \n   This design makes the method lightweight and more deployable in real-world applications.\n\n2. Black-box Verification  \n   The framework supports watermark detection without requiring internal model access, aligning well with realistic API-based or commercial model scenarios.  \n   This enhances the practical applicability of the approach to closed-source environments.\n\n3. Empirical Robustness  \n   Experimental results demonstrate that CodeGenGuard maintains strong watermark detectability under common attack settings (e.g., fine-tuning, model extraction).  \n   This suggests the framework provides reasonable robustness in practical deployment.\n\n4. Clarity and Coherent Technical Design  \n   A notable strength of CodeGenGuard lies in the clarity and coherence of its technical design and rationale.  \n   The paper offers well-structured explanations of its main modules — including the semantic-preserving transformation (SPT) taxonomy, the dual-LoRA shadow training mechanism, and the auxiliary prompt-based verification process — each supported by explicit reasoning for their inclusion.  \n   This makes the overall system logically consistent and easy for readers to understand."}, "weaknesses": {"value": "1. Limited Theoretical Analysis  \n   The paper focuses primarily on engineering design and empirical validation, but lacks a deeper theoretical foundation or formal guarantees regarding watermark detectability, robustness, or false-positive rates.  \n   As a result, the security properties of the method remain largely empirical.\n\n2. Overhead and Complexity Not Fully Quantified  \n   While the paper claims that the dual-LoRA design reduces computational and memory costs, it does not provide explicit measurements or comparisons to baseline fine-tuning strategies.  \n   The efficiency advantage, though plausible, remains qualitatively described rather than quantitatively proven.\n   \n\n3. Lack of concrete SPT transformation examples   \n    Although the paper proposes a taxonomy of Semantic-Preserving Transformations (SPTs) and emphasizes their role in embedding watermarks, it does not provide explicit code-level illustrations for each SPT pattern. The absence of concrete “before-and-after” code examples makes it difficult for readers to fully understand how these transformations are implemented in practice and how they affect code semantics or readability.\n\n4. Unclear random trigger design   \n    The paper introduces random triggers \\(r\\) to ensure the model generates normal code for irrelevant inputs, but it does not explain how these triggers are generated or sampled. Without concrete code examples or generation details, it is difficult to understand their distribution, neutrality, or implementation in practice.\n\n5. Unclear statistical testing procedure   \n    The paper outlines the null and alternative hypotheses for watermark verification but does not specify how the hypothesis test is actually conducted. It remains unclear whether the authors employed a t-test, a frequency-based test, or another statistical method. This lack of detail limits transparency and reproducibility of the verification process.\n\n6. Unclear Dual Lora Design   \n    While the paper introduces the dual-LoRA training scheme as a parameter-efficient alternative to full shadow training, the mathematical formulation of how the two LoRA modules are actually merged remains insufficiently explained. Although the authors define the watermark and shadow models as \\(F_{\\{wm,shd\\}} = F(W_0 + A_{\\{wm,shd\\}}B_{\\{wm,shd\\}})\\) and later state that the final model is \\(W_{wm} = W_0 + A_{wm}B_{wm}\\), it is unclear how these two low-rank matrices interact or whether their gradients are jointly or alternately optimized during training. Moreover, while three losses (\\(L_{wm}, L_{norm}, L_{neg}\\)) are defined separately, the paper does not specify a unified optimization objective or a weighting scheme showing how these components are integrated into a single training loss. To improve clarity, the authors are encouraged to provide an explicit mathematical expression of the overall loss function (e.g., \\(L_{total} = \\lambda_1 L_{wm} + \\lambda_2 L_{norm} + \\lambda_3 L_{neg}\\)) and to detail how the dual-LoRA parameters are merged or combined in the final watermarked model. Such clarification would significantly strengthen the technical transparency and reproducibility of the proposed approach.\n\n7. The paper misses several relevant references in code watermarking. It should include DeCoMa [A1] for dataset-level watermark detection, and [A2] for post-training traceable model watermarking similar to ToSyn. Adding these works would provide a more complete contextual grounding. \n\n   [A1]Yuan Xiao, Yuchen Chen, Shiqing Ma, Haocheng Huang, Chunrong Fang, Yanwei Chen, Weisong Sun, Yunfeng Zhu, Xiaofang Zhang, and Zhenyu Chen. 2025. DeCoMa: Detecting and Purifying Code Dataset Watermarks through Dual Channel Code Abstraction. Proc. ACM Softw. Eng. 2, ISSTA, Article ISSTA075 (July 2025), 24 pages. https://doi.org/10.1145/3728952 \n\n   [A2] Tom Sander, Pierre Fernandez, Alain Durmus, Matthijs Douze, and Teddy Furon. 2025. Watermarking makes language models radioactive. In Proceedings of the 38th International Conference on Neural Information Processing Systems (NIPS '24), Vol. 37. Curran Associates Inc., Red Hook, NY, USA, Article 664, 21079–21113."}, "questions": {"value": "1.  Robustness of dead-code-based SPTs under output filtering\n    In the paper, rare SPT patterns are augmented using dead-code insertion generated from probabilistic context-free grammars (PCFGs). However, based on empirical experience, models tend to memorize and reproduce these fixed dead-code fragments verbatim during fine-tuning. If an adversary fine-tunes the stolen model but filters out dead-code outputs during generation, legitimate users would never observe those injected patterns. How would this affect the watermark detectability, particularly for rare SPTs introduced through dead-code augmentation? Furthermore, since the paper extends to multi-bit watermarking (Section D.3.2) that relies on a larger variety of SPTs, how resilient would such augmented, dead-code-based SPTs remain under this type of output filtering? Finally, how might the effectiveness results reported in Section D.3.1 change if dead-code outputs are systematically suppressed during generation?\n\n2. The Pass@1 results in Figure 4 show a substantial performance drop for CodeMark compared to both the clean and CodeGenGuard models, particularly on the MBPP benchmark. Could the authors explain the underlying theoretical reason for such a large degradation in generation quality? Including additional evaluation on HumanEval would strengthen the paper’s empirical section. Since HumanEval provides shorter and more uniform tasks with lower result variance, it could complement MBPP by offering a more stable and interpretable measure of post-training fidelity after watermark embedding or fine-tuning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ky0blXM1Dw", "forum": "UQZ0Y0dbP9", "replyto": "UQZ0Y0dbP9", "signatures": ["ICLR.cc/2026/Conference/Submission18219/Reviewer_Qkxp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18219/Reviewer_Qkxp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727982832, "cdate": 1761727982832, "tmdate": 1762927960123, "mdate": 1762927960123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CodeGenGuard, an LM code watermarking framework that leverages semantic-preserving transformations to encode watermarks and incorporates dead-code-based data augmentation to diversify SPTs. \n\nMoreover, CodeGenGuard applies a dual-LoRA shadow training scheme for robustness and introduces auxiliary prompts for detection rates.\n\nIn addition, the paper is well-written and easy to follow."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work involves some innovative strategies, including \n\n(1)  The dead code-based data augmentation strategy could overcome the issue of low-frequency SPTs in natural code.\n\n(2) Auxiliary prompts provide additional semantic cues for watermark verification.\n\n(3) Two LoRA adapters are trained to reduce the computational overhead during backdoor embedding.\n\nIn addition, in the experiment, (4) CodeGenGuard is robust against model extraction and fine-tuning after extraction."}, "weaknesses": {"value": "Some concerns about this work are as follows.\n\n(1) For cross-language watermarking, many SPTs specific to Python cannot be applied to other languages. \nThe work only validates a limited number of SPTs in other languages.\n\n(2) In practical scenarios, an attacker could download a public model and perform standard full-parameter fine-tuning or LoRA fine-tuning. \nThus, in addition to the custom fine-tuning attacking experiments, evaluations on direct fine-tuning attacks are necessary.\n\n(3) Shadow training is designed for specific types of knowledge distillation. \nCan it enable CodeGenGuard to generalize to more types of model attacks?\n\n(4) The watermark verification relies heavily on auxiliary prompts, making the verification process is highly vulnerable to attackers."}, "questions": {"value": "Please kindly respond to the concerns 1 to 4 in the above Weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pL4IsTOChS", "forum": "UQZ0Y0dbP9", "replyto": "UQZ0Y0dbP9", "signatures": ["ICLR.cc/2026/Conference/Submission18219/Reviewer_NtAK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18219/Reviewer_NtAK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18219/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922572478, "cdate": 1761922572478, "tmdate": 1762927959689, "mdate": 1762927959689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}