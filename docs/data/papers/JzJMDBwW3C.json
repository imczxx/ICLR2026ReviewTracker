{"id": "JzJMDBwW3C", "number": 13494, "cdate": 1758218569919, "mdate": 1759897433066, "content": {"title": "Holon-of-Thought: Improving robustness in Large Language Models via structured framework", "abstract": "Large Language Models (LLMs) excel in language comprehension and generation tasks but frequently face challenges in scenarios demanding rigorous logical reasoning or strict adherence to problem conditions. In such reasoning, errors propagate through intermediate steps, hallucinatory outputs violate key problem conditions, and complex problems are often handled in a simplistic, chain-like manner. We propose Holon-of-Thought (HoT), a structured reasoning framework. HoT explicitly extracts problem conditions and enforces their adherence. It dynamically decomposes complex problems into verifiable subtasks and solves them through a four-stage pipeline: condition extraction, path exploration, adaptive decomposition, and aggregation. The experimental results show that HoT improves the accuracy of the inference and enhances the robustness. This establishes a new paradigm for reliable LLM-based reasoning in mathematics and logic.", "tldr": "We propose HoT, a structured reasoning framework that improves LLM accuracy and robustness.", "keywords": ["Structured Reasoning", "Chain-of-Thought", "Reliable LLM Reasoning", "Condition-Aware Inference"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/51b04929736da84b0c4a49856a7b5024d7ca9293.pdf", "supplementary_material": "/attachment/f8d129386702f75208976a8477716cc1d417fac0.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Holon-of-Thought (HoT), a framework designed to enhance the robustness of large language models (LLMs). The HoT framework comprises the following modules: Condition Extraction, Tree Explorer, Path Scoring, Optimal Path Selection, Adaptive Domain Decomposition, and Resolution and Aggregation. Experiments on the math and logic dataset demonstrate that HoT improves upon a set of baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and well-structured, making it easy to follow."}, "weaknesses": {"value": "- Motivation: The motivation for this work raises some concerns. Numerous studies have already addressed the rigor of LLM reasoning by incorporating external symbolic solvers, which provide strong guarantees of logical soundness and rigorousness. The authors criticize LLMs’ limitations in rigorous reasoning and conditional following, yet their proposed framework relies entirely on LLMs with prompt engineering, an approach that still lacks formal guarantees of rigor.\n\n- Methodological Justification: The rationale behind the framework’s design choices is not well explained. Why were these specific modules or processes selected? What underlying principles or hypotheses motivated the design? At present, the methodology reads more like a technical implementation report, without providing conceptual insights into why or how it achieves improvement.\n\n- Empirical Performance: The reported performance gains on most benchmarks appear minor.\n\n- Analysis Depth: The paper would benefit from deeper qualitative or diagnostic analysis to help readers better understand the framework’s behavior, strengths, and limitations."}, "questions": {"value": "- What is the motivation of HoT*?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DyYivB52F1", "forum": "JzJMDBwW3C", "replyto": "JzJMDBwW3C", "signatures": ["ICLR.cc/2026/Conference/Submission13494/Reviewer_cRbi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13494/Reviewer_cRbi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760784157479, "cdate": 1760784157479, "tmdate": 1762924111907, "mdate": 1762924111907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The structured reasoning framework \"Holon-of-Thought (HoT)\" proposed in this paper addresses core pain points of Large Language Models (LLMs) in rigorous reasoning tasks, such as error propagation, hallucinatory outputs, and oversimplified handling of complex problems. It designs a four-stage pipeline of \"Condition Extraction - Path Exploration - Adaptive Decomposition - Aggregation\". The experimental part covers two typical types of reasoning tasks: mathematical tasks (GSM8K, ASDiv, SVAMP) and logical tasks (OpenBookQA, StrategyQA)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The innovation is clear: the paper profoundly analyzes the root causes of reasoning errors in Large Language Models (LLMs) (relying on statistical correlations rather than formal logic during training, and neglecting explicit/implicit conditions), and proposes a core design of \"conditions throughout the entire process + sub-problem isolation\" in a targeted manner.\n2. The experimental design is rigorous: the experimental setup covers multiple task types (mathematics + logic), multiple models (Qwen2.5:7b-instruct, DeepSeek-V3), and multiple baselines (CoT, CoT-SC, ReAct, etc.). It adopts a two-dimensional evaluation of \"accuracy + robustness (TV/IVM)\" and verifies the necessity of the four core modules through ablation experiments.\n3. The engineering practicality is strong: HoT is designed as a \"model-agnostic\" framework, which is implemented only through prompt engineering or lightweight API calls, without the need for retraining LLMs, thus avoiding the high computing power cost of traditional reasoning enhancement methods (such as Fine-tuning)."}, "weaknesses": {"value": "1. The \"binary classification decision logic\" (DecomposeFlag=True/False) of the Adaptive Decomposition module does not provide specific decision-making criteria.\n2. In the Tree Explorer module, the \"path evaluation criteria\" only mention \"anticipated accuracy, operational feasibility, and computational complexity,\" lacking a specific definition of Score_{LLM}.\n3. There is confusion regarding performance: it seems that the HoT method itself cannot bring significant performance improvement, and the performance improvement mainly comes from the voting mechanism? If so, would other methods combined with the voting mechanism perform better?"}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6f3hh76yFp", "forum": "JzJMDBwW3C", "replyto": "JzJMDBwW3C", "signatures": ["ICLR.cc/2026/Conference/Submission13494/Reviewer_b3J1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13494/Reviewer_b3J1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705565229, "cdate": 1761705565229, "tmdate": 1762924111525, "mdate": 1762924111525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Holon-of-Thought (HoT), a structured reasoning framework for Large Language Models (LLMs). It aims to improve robustness, logical consistency, and condition adherence in reasoning tasks.\nHoT consists of a four-stage pipeline:\n1. Condition Extraction – explicitly identifies both explicit and implicit problem constraints.\n2. Tree Explorer – generates and scores multiple reasoning paths, then selects the optimal one.\n3. Adaptive Domain Decomposition – decides whether to decompose complex problems into subproblems.\n4. Resolution and Aggregation – solves each subproblem independently and aggregates the results.\n\nThe paper benchmarks HoT against several reasoning frameworks (CoT, CoT-SC, ReAct, IAP, AoT) across five datasets (GSM8K, ASDiv, SVAMP, OpenBookQA, StrategyQA) and shows improvements in accuracy and stability. The authors further provide variance-based robustness metrics (TV, IVM) and ablation studies, demonstrating each module’s contribution."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. HoT presents a well-structured and modular pipeline that highlights condition awareness and adaptive decomposition. The focus on robustness and logical consistency is both sensible and well-motivated.\n2. The introduction of TV and IVM as robustness metrics is a valuable contribution, providing concrete measures for evaluating the stability of reasoning.\n3. The experiments span diverse reasoning benchmarks and include comprehensive comparisons with multiple baselines. The ablation study offers clear insights into the contribution of each component, with Condition Extraction identified as the most influential.\n4. The framework naturally improves interpretability by explicitly revealing condition sets and reasoning trajectories, allowing for clearer understanding and analysis of the model’s decision process."}, "weaknesses": {"value": "1. While the framework is well-structured, the paper lacks a deeper theoretical justification or analysis of why the modular decomposition guarantees robustness. There is no formal reasoning or error-bound discussion—results are largely empirical. Meanwhile, the novelty of the article is relatively low. Trying different XoT methods has long been proven to be effective, but it does not bring about an essential improvement.\n2. The selected dataset is relatively simple, and the improvement is not very significant. It’s unclear how much of the improvement arises from better prompt engineering versus structural innovations.\n3. Baselines like ReAct and CoT do not require as much computational effort as HoT, and the comparison under different computing budgets lacks fairness. Although HoT claims to be efficient, the paper doesn’t provide detailed runtime or token cost analyses. The added multi-step reasoning (especially decomposition and path exploration) likely increases inference time, which isn’t quantitatively assessed.\n4. There is a problem with the reference format in the paper."}, "questions": {"value": "Refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5glZMjbLQp", "forum": "JzJMDBwW3C", "replyto": "JzJMDBwW3C", "signatures": ["ICLR.cc/2026/Conference/Submission13494/Reviewer_dvti"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13494/Reviewer_dvti"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821790799, "cdate": 1761821790799, "tmdate": 1762924110577, "mdate": 1762924110577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Holon-of-Thought (HoT), a structured, multi-stage prompting framework designed to improve the robustness and accuracy of LLM reasoning. HoT operates via a four-module pipeline: (1) Condition Extraction, to identify problem constraints; (2) Tree Explorer, to select an optimal solution path (out of N=3); (3) Adaptive Domain Decomposition, to decide if a problem needs division; and (4) Resolution and Aggregation, to solve the problem(s). The authors evaluate HoT on math and logic benchmarks, claiming SOTA accuracy over baselines like CoT-SC and AoT, and improved \"robustness\" (defined as lower accuracy variance)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Addressing the brittleness, error propagation, and condition-violation of LLM reasoning is a timely research direction. The 4-stage HoT prompt-engineering framework is well-described, intuitive, and clearly illustrated in Figures 1 and 2. The logic of first identifying constraints and then planning a solution path is well-motivated.\n\nThe ablation study (Table 3) provides clear evidence for the importance of Module 1 (Condition Extraction). The largest performance drop occurs when this module is removed, validating the paper's core premise. The paper deserves credit for attempting to measure robustness (Section 4.1.4, 4.2.3) beyond simple accuract. The appendix provides the full prompts, which is good for reproducibility."}, "weaknesses": {"value": "A key weakness is the paper's limited novelty, as its architecture is largely derivative of existing, uncompared work. Module 2 (Tree Explorer) appears to be a simplified, fixed-branch (N=3) version of Tree of Thoughts (ToT), and Module 3 (Adaptive Decomposition) re-uses concepts from other decomposition methods like ThoT (Thread of Thought) and RDOLT(Recursive Decomposition of Logical Thoughts). The paper fails to cite or benchmark against these obvious predecessors, making it impossible to determine if this specific combination offers any new, meaningful contribution to the field.\n\nThe paper positions HoT as a structured reasoning framework but fails to compare against its most direct competitors. The current baselines (CoT-SC, AoT) are insufficient. This is the most significant flaw. Please evaluate HoT against Tree of Thoughts (ToT). The comparison should be normalized for cost (e.g., HoT with N=3 paths vs. ToT with a similar branching factor). Please evaluate against at least one other explicit decomposition method (e.g., ThoT or RDOLT) to validate Module 3.\n\nThe paper's strongest results (92.2% avg, Table 1) do not come from the HoT framework itself. They come from \"HoT*,\" a 3-way ensemble of CoT, HoT, and a HoT-ablation (lines 365-367). This obscures the core contribution. Please clarify the contribution by removing the 'HoT*' result from the main comparison table. The paper's claims must rest on the performance of the HoT framework itself, not an ensemble.\n\nAll experiments seen to be run on N=200 samples, which is a tiny fraction of benchmarks like GSM8K. On N=200, the 1.0% gain of HoT over AoT (91.0 vs 90.0) is a difference of only 2 samples.\n\nThe authors explicitly state they \"eliminate actual API calls or tool integration\" and \"simulate\" actions in natural language. The core contribution of ReAct is the synergistic loop of generating an Action, having an external tool execute it, and receiving a new Observation. By removing this loop, the paper's baseline is just a Chain-of-Thought prompt formatted with \"Thought\" and \"Action\" labels, not a true Reasoning-Acting system. Please implement ReAct correctly (with a simple tool like a calculator for math problems)"}, "questions": {"value": "Please check weaknesses specified above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fLb582Zis7", "forum": "JzJMDBwW3C", "replyto": "JzJMDBwW3C", "signatures": ["ICLR.cc/2026/Conference/Submission13494/Reviewer_eGHE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13494/Reviewer_eGHE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13494/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955131358, "cdate": 1761955131358, "tmdate": 1762924109773, "mdate": 1762924109773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}