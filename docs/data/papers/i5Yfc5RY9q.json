{"id": "i5Yfc5RY9q", "number": 16644, "cdate": 1758267185478, "mdate": 1763702053300, "content": {"title": "Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets", "abstract": "Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a *tree-structured state space* when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the *directed acyclic graph (DAG) state space*. This leads to restricted exploration of compositional paths and is biased toward the chosen path. Generative Flow Networks (GFlowNets) are powerful for efficient exploring and generalizing over state spaces, particularly those with a DAG structure. However, prior GFlowNets-based language models operate at the token level and remain confined to tree-structured spaces, limiting their potential. In this work, we propose **F**low **o**f **S**pan**S** (**FOSS**), a principled GFlowNets framework for span generation. FoSS constructs a dynamic span vocabulary by segmenting the retrieved text flexibly, ensuring a DAG-structured state space, which allows GFlowNets to explore diverse compositional paths and improve generalization. With specialized reward models, FoSS generates diverse, high-quality text. Empirically, FoSS improves MAUVE scores by up to 12.5\\% over Transformer on text generation and achieves 3.5\\% gains on knowledge-intensive tasks, consistently outperforming state-of-the-art methods. Scaling experiments further demonstrate FoSS benefits from larger models, more data, and richer retrieval corpora, retaining its advantage over strong baselines.", "tldr": "", "keywords": ["generative flow networks", "language models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f0ce3fd0c8446d95b7319da1fcd57887fa4ce748.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes FoSS (Flow of SpanS), a span-based language generation framework that models generation as trajectories on a DAG (instead of the usual token-level tree) and trains a span-generating policy with Generative Flow Networks (GFlowNets). The authors introduce a DAG-inducing probabilistic span segmentation to produce multiple segmentation trajectories per sentence, construct a dynamic span vocabulary from retrieved documents, and instantiate a span policy (prefix encoder + span encoder) trained with a subtrajectory-balance GFlowNet objective. A combined reward (fine-tuned LM likelihood + a preference model) steers generation toward fluent, human-like outputs. Empirically, FoSS reports substantial MAUVE gains over strong baselines (CoG, kNN-LM, RETRO, Transformer variants) and wins in GPT-4 preference comparisons; ablations show the DAG design and reward components are important."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear methodological novelty — DAG + GFlowNets for spans.\nModeling span selection as actions yields a DAG where the same final string can be composed via multiple span paths; using GFlowNets to learn a forward policy which samples terminal sequences with probability proportional to reward is a principled fit for such DAG-structured compositional spaces. This is a clean conceptual advance over token-level GFlowNet applications. \n\n\n\n2. Practical span segmentation and dynamic vocabulary.\nThe DAG-inducing probabilistic span segmentation (probabilistic early stopping on forward maximum matching) creates multiple offline trajectories per document and makes the DAG explicit for training. The dynamic vocabulary construction (token vocab + substrings from retrieved documents, 2–8 tokens) is a sensible engineering choice to make the action space manageable while expressive.\n\n3. Thoughtful hybrid training recipe.\nThe hybrid online/offline batching (policy samples + reward-prioritized replay + training-derived trajectories) and pre-fine-tuning of policy components to stabilize sparse-reward GFlowNet training are appropriate design choices for the large combinatorial space. Algorithm 1 clearly describes the training loop. \n\n\n4. Extensive empirical evaluation.\nThe paper reports (i) MAUVE and Diversity on WikiText-103 and Law-MT, (ii) GPT-4 preference evaluations, (iii) downstream knowledge tasks (TruthfulQA, OpenBookQA, ARC, MedMCQA, Med-USMLE), (iv) scaling experiments (memory, training data fraction, model size), and (v) ablations showing the DAG and reward components matter. The breadth of evaluation supports the central claims."}, "weaknesses": {"value": "1. Reward design — potential for reward hacking / calibration issues.\nThe reward R = exp(α log p_LM + (1−α) log p_PM) combines LM likelihood and a learned preference model. Preference models can be brittle and susceptible to distributional shift / reward hacking; the paper briefly mentions score-centering regularizer but lacks a systematic analysis of sensitivity to α, PM training data composition, or adversarial failures. More diagnostics would strengthen confidence. \n\n2. Computational cost / scalability concerns not fully quantified.\nTraining uses eight A800 GPUs with gradient accumulation and LoRA; the span index and retrieval (up to k=1024 at inference) plus GFlowNet trajectory sampling could be expensive. The latency table (Table 5) is referenced, but details are not shown in the snippet; a clearer cost vs. baseline comparison (GPU hours, wall-time, memory) is needed to judge practicality at scale. \n\n3. Single retrieval per prefix — diversity tradeoff.\nThe authors acknowledge FoSS does only one retrieval per prefix (vs. token-level retrieval in kNN-LM), which reduces latency but may limit diversity; while FoSS improves MAUVE/diversity over many baselines, kNN-LM sometimes shows higher diversity under nucleus sampling. The paper states this as a limitation but does not explore hybrid retrieval-frequency regimes. \n\n4. Ablation could be deeper.\nAblations remove DAG (only tokens), LM, or PM, but the paper doesn’t show results for: (a) different max span lengths, (b) varying the retrieval k, (c) different segmentation probability schedules P (the stochastic early-stop thresholds), or (d) alternatives to the uniform backward policy (they chose uniform suffix PB). These choices could materially affect performance and stability."}, "questions": {"value": "1. Segmentation sensitivity: How sensitive are results to the choice of span length bounds (2–8 tokens) and the probability schedule P used in DAG-inducing segmentation? Can you provide an ablation or plot showing MAUVE / Diversity vs. max span length and vs. different stochastic early-stop rates?\n\n2. Reward hyperparameters & robustness: How was α (tradeoff between LM and PM) selected? Do generated distributions sharply change for small α perturbations? Have you observed reward-hacking behaviors (e.g., overly short/high-likelihood degenerate outputs) and how were they mitigated? Please report any experiments on robustness of PM (overfitting to training references, ensemble PMs, or rejection sampling). \n\n3. Computational and latency accounting: Could you provide concrete training GPU-hour numbers and inference latency breakdowns (per prefix average, top-k retrieval, span scoring) compared to each baseline (Transformer, CoG, kNN-LM)? Table 5 is mentioned but not visible in my copy — please expand on it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "No5aET3tAv", "forum": "i5Yfc5RY9q", "replyto": "i5Yfc5RY9q", "signatures": ["ICLR.cc/2026/Conference/Submission16644/Reviewer_z7qh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16644/Reviewer_z7qh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16644/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659247763, "cdate": 1761659247763, "tmdate": 1762926706814, "mdate": 1762926706814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel span-based text generation framework using GFlowNets. The vocabulary is extended to include multi-word phrases, and the decoding process is reformulated as a Directed Acyclic Graph (DAG) problem. Extensive analyses and experiments are conducted to demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper connects well with relevant literature. Each component of the proposed FoSS model is discussed in relation to previous studies, making the distinctions from existing work clear.\n\n2.While the method combines ideas from GFlowNets and dynamic span vocabularies (neither originally introduced by the authors), it tackles non-trivial challenges in integrating these concepts. The authors address these challenges in a thoughtful and technically sound manner.\n\n3.The paper provides detailed explanations of each component and implementation aspect, ensuring reproducibility and transparency.\n\n4.The experimental section is extensive, covering in-domain, out-of-domain, and downstream evaluations that collectively validate the proposed approach."}, "weaknesses": {"value": "1.Lack of comparison with more recent and important baselines. As mentioned by the authors in the related work, two recent and important studies on dynamic vocabularies [1][2] were not included in the main experimental comparisons.\n\n2.For scalability, the largest model evaluated in the scalability experiments is GPT2-XL (~1.5B parameters). It would be valuable to test the approach on larger and more recent models such as LLaMA 3 or Qwen 3. If direct training is infeasible due to resource constraints, a cross-model comparison (e.g., FoSS on GPT2-XL vs. vanilla LLaMA 3/Qwen 3) could still provide useful insights.\n\n3.Although inference-time latency is reported, the paper lacks discussion of the training overhead compared to other baselines.\n\n[1] RETRIEVAL IS ACCURATE GENERATION, ICLR 2024\n\n[2] Nearest Neighbor Speculative Decoding for LLMGeneration and Attribution, NeurIPS 2024"}, "questions": {"value": "N.A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1WX6mzTZ53", "forum": "i5Yfc5RY9q", "replyto": "i5Yfc5RY9q", "signatures": ["ICLR.cc/2026/Conference/Submission16644/Reviewer_WyH2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16644/Reviewer_WyH2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16644/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761809406127, "cdate": 1761809406127, "tmdate": 1762926706451, "mdate": 1762926706451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper describes an approach to generating text, Flow of SpanS (FoSS), that involves using multi-token spans of existing text.  This situates it with some other recent work that also uses multi-token spans like e.g. CoG retrieving and assembling contextually relevant phrases; it differs in that it recognizes that this corresponds to a DAG-structured internal state space, which then permits the application of GFlowNets as a way of working with this structure.  This also situates it with respect to other work that uses GFlowNets, but this other work typically is applied just to single token-level generation.  Evaluations are carried out with respect to several appropriate text generation baselines under the MAUVE and Diversity metrics, showing strong performance by FoSS.  There are also evaluations on downstream tasks and ablation studies, as well as explorations of scaling."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "* Overall, I think this is a great paper.  The idea is a very neat one, the sort that comes from recognizing the structure of data and realizing what this implies for methods to apply.\n\n* The experiments are quite comprehensive, and show the effectiveness of FoSS, with e.g. quite large improvements in MAUVE scores (Table 1) and strong preferences for FoSS text (Table 2).  Beyond the values of the metrics, the case study in Fig 5 in the appendix gives a feel for how much better the text generated by FoSS can be.  I could see this approach being widely used.  (The latency analysis of App F is also encouraging as to the practicality of using the method.)\n\n* There are several insights throughout the paper as well, as for example in the ablation discussion on the role of the PM component in Eqn (3)."}, "weaknesses": {"value": "Nothing at all major.\n\n* There are a few places where some additional remarks on setup could help.  For instance, there’s no discussion of greedy vs nucleus in terms of setup, what they might tell us, etc; the numbers are just presented in Table 1.  Table captions in general could be a bit more informative."}, "questions": {"value": "No questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HOAXtINDly", "forum": "i5Yfc5RY9q", "replyto": "i5Yfc5RY9q", "signatures": ["ICLR.cc/2026/Conference/Submission16644/Reviewer_4EKG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16644/Reviewer_4EKG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16644/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891460706, "cdate": 1761891460706, "tmdate": 1762926706178, "mdate": 1762926706178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces FoSS, a span-based language modeling framework that builds an explicit DAG over segmentations and learns a GFlowNet policy to traverse it. Concretely, span actions (length 2–8 tokens) are drawn from a dynamic, retrieval-augmented vocabulary; training uses subtrajectory balance with a hybrid online/offline regimen, a uniform backward policy over suffixes, and a composite reward combining a fine-tuned LM likelihood and a preference model. On open-ended generation (WikiText-103), FoSS improves MAUVE over strong baselines, is preferred by GPT-4 on multiple criteria, and shows gains on several knowledge-intensive multiple-choice tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Casting span generation as an explicit DAG and optimizing it with a GFlowNet is a clean way to expose multiple compositional paths, addressing a known bias of tree-structured token decoders. I find the core idea both interesting and promising.\n\n- The experiments are comprehensive and the results are highly positive. FoSS improves MAUVE both in-domain and out-of-domain (Table 1), shows positive GPT-4 preferences (Table 2), and is competitive on QA tasks (Table 3). The ablation study showing a drop in performance without the DAG structure supports the central claim that “many-paths-per-state” matters (Table 4)."}, "weaknesses": {"value": "- Section 4.3 (“Scaling behavior”) is somewhat misleading. In Figures 2 and 3, the x-axis is rendered with equal spacing while the labels are logarithmically spaced (0.001 → 1.0 with tripling and 0.47 → 15 with doubling). Please use a logarithmic x-axis and plot against numeric proportions to avoid misrepresenting the trends. Figure 4 should use the number of parameters as the x-axis instead of model names. The sizes for GPT-2 Small, Medium, Large, and XL are 124M, 355M, 774M, and 1.5B, respectively. Once corrected, the diminishing returns with model scaling appear quite strong, so I am not sure this method would truly generalize to larger models.\n- The En-Wiki datastore (used for WikiText-103 evaluation) likely contains near-duplicates of test articles. Even if baselines share the same memory, absolute improvements can be inflated. Please filter out any pages overlapping with the WikiText-103 test set, or at least check and report overlap statistics.\n- Although you cite a paper claiming that strong LLM-based evaluations mostly align with human judgments, other work suggests this alignment can be dataset-dependent (https://aclanthology.org/2025.acl-short.20/). The reliability of LLM-based evaluation remains debatable. A small human study (e.g., 100 pairs) would strengthen the conclusion."}, "questions": {"value": "* How many training tokens and GPU-hours are used for FoSS vs. each baseline?\n* Why do spans have lengths of 2–8 tokens? What motivated this design choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PLMxDQ5tO2", "forum": "i5Yfc5RY9q", "replyto": "i5Yfc5RY9q", "signatures": ["ICLR.cc/2026/Conference/Submission16644/Reviewer_JUcH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16644/Reviewer_JUcH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16644/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981028265, "cdate": 1761981028265, "tmdate": 1762926705698, "mdate": 1762926705698, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}