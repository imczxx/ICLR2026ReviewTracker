{"id": "dwPdYFqVWO", "number": 3942, "cdate": 1757570639673, "mdate": 1759898061787, "content": {"title": "Bridging Draft Policy Misalignment: Group Tree Optimization  for Speculative Decoding", "abstract": "Speculative decoding accelerates large language model (LLM) inference by letting a lightweight draft model propose multiple tokens that the target model verifies in parallel. Yet existing training objectives optimize only a single greedy draft path, while decoding follows a tree policy that re-ranks and verifies multiple branches. This draft policy misalignment limits achievable speedups. We introduce Group Tree Optimization (GTO), which aligns training with the decoding-time tree policy through two components: (i) Draft Tree Reward, a sampling-free objective equal to the expected acceptance length of the draft tree under the target model, directly measuring  decoding performance; (ii) Group-based Draft Policy Training, a stable optimization scheme that contrasts trees from the current and a frozen reference draft model, forming debiased group-standardized advantages and applying a PPO-style surrogate along the longest accepted sequence for robust updates. We further prove that increasing our Draft Tree Reward provably improves acceptance length and speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B, DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4\\% and yields an additional 7.7\\% speedup over prior state-of-the-art EAGLE-3. By bridging draft policy misalignment, GTO offers a practical, general solution for efficient LLM inference. Code and draft models are available at https://anonymous.4open.science/r/GTO-ICLR-348F/.", "tldr": "We introduce GTO, which optimizes draft-tree reward to provably increase acceptance length and achieve >7% faster speculative decoding than EAGLE-3, while fine-tuning existing draft models.", "keywords": ["Speculative Decoding; Draft Tree Reward; Tree Optimization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04354699d2957d57bff2d63ff2ecbee49b4a308a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how to bridge evolving draft policies and model behaviors in large language models. The authors propose a two-stage method: a policy bridging module based on contrastive training between draft policy outputs and model outputs, and a misalignment diagnostic using representation-space probes. Experiments show modest improvements in alignment detection and behavior correction compared to direct fine-tuning and reward-model-based baselines."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses a practical problem in alignment: bridging evolving policies and model behaviors.\n\n2. Conceptually coherent design combining contrastive alignment and probing.\n\n3. The theory is good."}, "weaknesses": {"value": "1. Unfair experimental comparison: All proposed methods are trained on top of an existing draft model with additional compute (e.g., 200 GPU hours for a 7B model), whereas baselines are not given the same training budget. The most natural baseline should be continued training of the draft model with new data under the same GPU hours, and this is completely missing (e.g., further training eagle 3, hass, griffin for 200 GPU hours on a 7B model with new training data). This omission makes the reported improvements difficult to interpret.\n\n2. Misleading “sampling-free” claim: The method is not truly sampling-free. Although no RL trajectory sampling is used, model outputs are still required for every node in the policy tree. The phrasing gives a false impression that no generation or sampling is needed, which is inaccurate.\n\n3. Excessive compute for modest gains: Training a single-layer Transformer for 7B over a 5-layer policy tree costs about 200 GPU hours, resulting in about 7% improvement. This is too expensive, near the time consumption of training EAGLE from scratch."}, "questions": {"value": "1. Why is there no baseline where the draft model is simply continued to train with new data for the same number of GPU hours as the bridging module?\n\n2. Can the authors clarify the exact sense in which their method is “sampling-free”? It seems draft model outputs are still needed.\n\n3. What would the bridging performance look like under stricter compute constraints (e.g., 10–20 GPU hours)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JWupgwPi0X", "forum": "dwPdYFqVWO", "replyto": "dwPdYFqVWO", "signatures": ["ICLR.cc/2026/Conference/Submission3942/Reviewer_kt8J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3942/Reviewer_kt8J"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760960383764, "cdate": 1760960383764, "tmdate": 1762917104518, "mdate": 1762917104518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the draft policy misalignment in speculative decoding for LLMs, where the draft model is trained using a greedy, single-path objective but deployed under a tree-based decoding policy. The authors propose Group Tree Optimization (GTO), a new training framework that explicitly aligns the training objective with the decoding-time policy. GTO introduces (1) a Draft Tree Reward, a sampling-free, differentiable objective measuring the expected acceptance length of the draft tree under the target model, and (2) a Group-based Draft Policy Training algorithm that leverages group-standardized advantages and a PPO-style surrogate to stabilize optimization. Theoretically, the paper proves that maximizing the Draft Tree Reward provably improves acceptance length and decoding efficiency. Experiments across dialogue (MT-Bench), code generation (HumanEval), and math reasoning (GSM8K) tasks on multiple LLMs (e.g., LLaMA-3.1-8B, Vicuna-13B) show that GTO increases acceptance length by 7.4% and yields 7.7% additional speedup over EAGLE-3, establishing it as a general and effective method for bridging training–decoding misalignment in speculative decoding"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper identifies and targets an important but previously overlooked research problem, draft policy misalignment between training and decoding in speculative decoding. This conceptual reframing is interesting and provides a unifying explanation for the inefficiencies of prior methods such as EAGLE, GRIFFIN, and HASS.\n- Based on the identified problem, the authors proposed GTO, which is sound and well-implemented with a theoretical guarantee.\n- Empirical results are strong, even at high decoding temperatures where existing methods may degrade. The analyses and interpretations of the results looks solid.\n- Overall the paper is clearly written and well-organized. Code and demos are available for reproducibility."}, "weaknesses": {"value": "- All experiments are conducted on the LLaMA family and its derivatives (Vicuna, DeepSeek), leaving open whether GTO generalizes to other popular architectures such as Qwen and Gemma. Since speculative decoding behavior can vary significantly across model families due to differences in tokenization and calibration, including at least one non-LLaMA model would strengthen the empirical claims of generality.\n- While GTO introduces clear gains in decoding efficiency, the paper provides limited quantitative analysis of training-time overhead (I appreciate the statements in Appendix D). The two-phase group optimization and tree-level reward computation likely introduce nontrivial additional compute and memory costs. A clearer breakdown (e.g., training FLOPs or wall-clock comparison against EAGLE-3) would help justify the practical cost–benefit balance of adopting GTO in large-scale deployment settings."}, "questions": {"value": "- How sensitive is GTO to the choice of group size m, reference model quality, or the number of sampled trees per prefix? Could you elaborate on how these hyperparameters were chosen in Appendix B.3 and whether GTO remains stable across a wide range of values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yZVDqxe9Z8", "forum": "dwPdYFqVWO", "replyto": "dwPdYFqVWO", "signatures": ["ICLR.cc/2026/Conference/Submission3942/Reviewer_AxA5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3942/Reviewer_AxA5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808035205, "cdate": 1761808035205, "tmdate": 1762917104339, "mdate": 1762917104339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the draft policy misalignment problem in Speculative Decoding, where the drafter is trained on greedy paths, while drafting utilizes tree-expansion methods. The authors propose Group Tree Optimization (GTO), an RL framework designed explicitly for aligning training with tree-drafting procedures. It introduces a Draft Tree reward and a Group-based Draft Policy Training scheme, achieving approximately 7% improvements in acceptance length and surpassing prior state-of-the-art baselines, such as EAGLE-3."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Proposes a principled RL framework that directly optimizes the expected acceptance length in tree-based speculative decoding, eliminating the need for sampling.\n- Bridges the gap between training and inference by aligning the drafter’s objective with the tree decoding policy.\n- Demonstrates **strong empirical performance** across diverse LLMs and tasks, consistently outperforming prior methods like EAGLE-3 and HASS.\n- The approach is model-agnostic and computationally efficient."}, "weaknesses": {"value": "- W1: Transferability. GTO trains the drafter explicitly on the drafting method, such as EAGLE, which may restrict the usage with other drafting methods.\n- W2: Robustness. It appears that the drafter is trained to fit a specific tree configuration during the training phase. The paper did not explore extrapolation or interpolation of the tree configuration."}, "questions": {"value": "- W1: Can a drafter trained for EAGLE be used with HASS, or vice versa?\n- W2: Is it possible to dynamically update the tree configuration during the inference stage while maintaining performance in the trained settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1fj7Saf5AF", "forum": "dwPdYFqVWO", "replyto": "dwPdYFqVWO", "signatures": ["ICLR.cc/2026/Conference/Submission3942/Reviewer_yvuE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3942/Reviewer_yvuE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896869676, "cdate": 1761896869676, "tmdate": 1762917104129, "mdate": 1762917104129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Group Tree Optimization, a novel training framework designed to accelerate LLM inference by solving a fundamental issue in speculative decoding: draft policy misalignment. Existing methods typically optimize the draft model based on maximizing the likelihood of generating a single greedy path, whereas the actual decoding process uses a tree policy that re-ranks and verifies multiple branches. The authors empirically demonstrate that this misalignment is severe: the greedy path is pruned 19–34% of the time, and the accepted path matches the greedy path only 36–49% of the time. GTO aligns training with the tree-based decoding policy using two core components: draft tree reward and group-based draft policy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper clearly identifies \"draft policy misalignment\" as a fundamental limitation that weakens the effectiveness of modern draft model training techniques.\n2. The group-based policy training effectively mitigates the typical challenges associated with optimizing sparse, tree-level rewards. The ablation studies confirm the necessity of the debiasing step (which yields a +5.0% SR gain at T=0) and highlight the effectiveness of the Log-Sum-Exp aggregator. Furthermore, GTO's compatibility is demonstrated by applying it as a fine-tuning step on draft models initialized by GRIFFIN and HASS, achieving consistent gains."}, "weaknesses": {"value": "1. Clarity on Training Complexity Overhead: The authors acknowledge that GTO increases training-time compute due to the two-phase procedure and the need for grouped draft tree construction and verification. While they state this cost is amortized in deployment, the paper lacks a clear, quantitative discussion in the main body (Section 3) of the computational complexity added by Phase II compared to the standard token-level loss (L_{token}) calculation.\n2. The PPO-style optimization is performed using a likelihood ratio (s_i) calculated only along the longest accepted sequence (\\hat{S}_i). This is a critical design choice, as it focuses the gradient update entirely on the most successful path within the group. While this likely maximizes the speedup metric, a clearer justification is needed in Section 3.2 explaining why updating based solely on \\hat{S}_i (Eq. 11 and 12) is preferred over updating all accepted sequences or averaging their contributions, in the context of improving the overall tree reward R_t."}, "questions": {"value": "1. Expand the discussion in Section 4.2 to provide deeper insight into the qualitative differences between groups of size 1 and size 32."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hlW8138uFH", "forum": "dwPdYFqVWO", "replyto": "dwPdYFqVWO", "signatures": ["ICLR.cc/2026/Conference/Submission3942/Reviewer_MEFU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3942/Reviewer_MEFU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3942/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973461178, "cdate": 1761973461178, "tmdate": 1762917103963, "mdate": 1762917103963, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}