{"id": "OkjB6PWJEA", "number": 23559, "cdate": 1758345453815, "mdate": 1759896807837, "content": {"title": "Effective and Efficient Jailbreaks of Black-Box LLMs with Cross-Behavior Attacks", "abstract": "Despite recent advancements in Large Language Models (LLMs) and their alignment, they can still be jailbroken, i.e., harmful and toxic content can be elicited from them. While existing red-teaming methods have shown promise in uncovering such vulnerabilities, these methods struggle with limited success and high computational and monetary costs. To address this, we propose a black-box *Jailbreak method with Cross-Behavior attacks* (JCB), that can automatically and efficiently find successful jailbreak prompts. JCB leverages successes from past behaviors to help jailbreak new behaviors, thereby significantly improving the attack efficiency. Moreover, JCB does not rely on time- and/or cost-intensive calls to auxiliary LLMs to discover/optimize the jailbreak prompts, making it highly efficient and scalable. Comprehensive experimental evaluations show that JCB significantly outperforms related baselines, requiring up to 94% fewer queries while still achieving 12.9% higher average attack success. JCB also achieves a notably high 37% attack success rate on Llama-2-7B, one of the most resilient LLMs, and shows promising zero-shot transferability across different LLMs.", "tldr": "We introduce a black-box method that uses cross-behavior learning to quickly and automatically discover effective jailbreak prompts.", "keywords": ["Large Language Models", "Black-Box Jailbreaks", "Red-Teaming"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e4db17eee6d381c0dcf066693952c84735f440c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces **JCB**, a black-box *cross-behavior* jailbreak method that automatically generates and perturbs seed prompts (using part-of-speech preserving synonym substitution) and leverages a scorer LLM to reuse successful prompts across behaviors, enabling efficient, automated discovery of jailbreaks. \nEmpirically, JCB attains substantially higher attack success while using far fewer queries (up to 94% fewer) than prior black-box baselines, shows strong zero-shot transferability across LLMs, and includes analyses of efficiency, transferability, and comparison with high-complexity methods like AutoDAN-Turbo. The authors highlight JCB’s practicality as a low-complexity red-teaming tool but note limitations (dataset/metric dependence and remaining headroom in ASR), which they discuss and position as future work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Originality & significance:** JCB introduces a simple, black-box cross-behavior jailbreak technique that pairs POS-preserving synonym substitution with a scorer LLM to recycle effective seeds, substantially lowering query costs and improving automated red-teaming and cross-model transfer.\n- **Quality & clarity**: The paper supplies comprehensive empirical evidence (efficiency, transferability, and baseline comparisons), clear methodological and prompt engineering details, and an honest limitations section, supporting reproducibility and practical impact.\n- **New methods**: They propose JCB, a novel, effective, efficient, and low-cost approach that leverages cross-behavior attacks to autonomously discover jailbreak prompts without human intervention"}, "weaknesses": {"value": "- The authors do not include a reproducibility statement. I am willing to increase the score if you provide a reproducibility statement and extend the experiments. \n\n- The paper’s novelty is limited: its theoretical mechanism closely mirrors LLMFuzzer—while LLMFuzzer relies on human-provided seed files, the automatic seed-generation here appears to be a relatively simple, incremental change.\n\n- **Incomplete evaluation:** The study omits recent SOTA attack baselines (e.g.,  BOOST, GPTFuzzer, FlipAttack)."}, "questions": {"value": "- How does this paper’s novelty concretely differ from LLM-Fuzzer—i.e., what new mechanism, theoretical claim, or empirical advantage does it introduce beyond mutation-based fuzzing?\n\n- What is the method's performance under SOTA defense methods, such as self-reminder"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gbtXFbZDU1", "forum": "OkjB6PWJEA", "replyto": "OkjB6PWJEA", "signatures": ["ICLR.cc/2026/Conference/Submission23559/Reviewer_mLEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23559/Reviewer_mLEf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760505692906, "cdate": 1760505692906, "tmdate": 1762942711868, "mdate": 1762942711868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **JCB**, a black-box *cross-behavior* jailbreak method that automatically generates and perturbs seed prompts (using part-of-speech preserving synonym substitution) and leverages a scorer LLM to reuse successful prompts across behaviors, enabling efficient, automated discovery of jailbreaks. \nEmpirically, JCB attains substantially higher attack success while using far fewer queries (up to 94% fewer) than prior black-box baselines, shows strong zero-shot transferability across LLMs, and includes analyses of efficiency, transferability, and comparison with high-complexity methods like AutoDAN-Turbo. The authors highlight JCB’s practicality as a low-complexity red-teaming tool but note limitations (dataset/metric dependence and remaining headroom in ASR), which they discuss and position as future work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Originality & significance:** JCB introduces a simple, black-box cross-behavior jailbreak technique that pairs POS-preserving synonym substitution with a scorer LLM to recycle effective seeds, substantially lowering query costs and improving automated red-teaming and cross-model transfer.\n- **Quality & clarity**: The paper supplies comprehensive empirical evidence (efficiency, transferability, and baseline comparisons), clear methodological and prompt engineering details, and an honest limitations section, supporting reproducibility and practical impact.\n- **New methods**: They propose JCB, a novel, effective, efficient, and low-cost approach that leverages cross-behavior attacks to autonomously discover jailbreak prompts without human intervention"}, "weaknesses": {"value": "- The authors do not include a reproducibility statement. I am willing to increase the score if you provide a reproducibility statement and extend the experiments. \n\n- The paper’s novelty is limited: its theoretical mechanism closely mirrors LLMFuzzer—while LLMFuzzer relies on human-provided seed files, the automatic seed-generation here appears to be a relatively simple, incremental change.\n\n- **Incomplete evaluation:** The study omits recent SOTA attack baselines (e.g.,  BOOST, GPTFuzzer, FlipAttack).\n\n [1] Yu, Jiahao, et al. \"Mind the Inconspicuous: Revealing the Hidden Weakness in Aligned {LLMs}'Refusal Boundaries.\" 34th USENIX Security Symposium (USENIX Security 25). 2025.\n\n[2] Yu, Jiahao, Xingwei Lin, Zheng Yu, and Xinyu Xing. \"{LLM-Fuzzer}: Scaling assessment of large language model jailbreaks.\" In 33rd USENIX Security Symposium (USENIX Security 24), pp. 4657-4674. 2024.\n\n[3] Liu, Y., He, X., Xiong, M., Fu, J., Deng, S., & Hooi, B. (2024). Flipattack: Jailbreak llms via flipping. arXiv preprint arXiv:2410.02832."}, "questions": {"value": "- How does this paper’s novelty concretely differ from LLM-Fuzzer—i.e., what new mechanism, theoretical claim, or empirical advantage does it introduce beyond mutation-based fuzzing?\n\n- What is the method's performance under SOTA defense methods, such as self-reminder"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gbtXFbZDU1", "forum": "OkjB6PWJEA", "replyto": "OkjB6PWJEA", "signatures": ["ICLR.cc/2026/Conference/Submission23559/Reviewer_mLEf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23559/Reviewer_mLEf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760505692906, "cdate": 1760505692906, "tmdate": 1763697328149, "mdate": 1763697328149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a prompt-framework generation method for black-box model jailbreak attacks. The authors identify three main shortcomings of existing automated red-team/jailbreak approaches: low attack efficiency (requiring many queries); reliance on expensive auxiliary LLMs or white-box gradient information; and generated prompts that are often semantically distorted or easily detected by filters. To address these issues, the paper introduces a cross-behavior black-box jailbreak method, JCB, whose core idea is to transfer successful experiences across different harmful behaviors to improve the efficiency of jailbreaking new behaviors. Experiments on 20+ open-source and closed-source models demonstrate that the method effectively increases jailbreak success rates."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper conducts experiments on a large number of open-source and closed-source large models; the reported attack success rates are clearly better than existing methods, demonstrating empirical advantages for the proposed approach.  \n2. The paper is well-structured and easy to read."}, "weaknesses": {"value": "1. **Unverified claims about limitations.** The paper asserts that existing methods generate prompts that are often semantically distorted or easily detected by filters, but it does not validate this claim against state-of-the-art jailbreak defenses. Experiments are only conducted on aligned models without defenses.  \n2. **Weak theoretical contribution.** The paper is primarily motivated by the observation that successfully used jailbreak prompt templates can be reused for more malicious behaviors, but it does not investigate the underlying mechanisms. For example, when the same template is applied to different malicious behaviors, what trends appear in the model's feature space that determine the strength of transferability? Given the large body of empirical work on jailbreak attacks and defenses, deeper investigation into the principles behind different methods would likely be more valuable.  \n3. **Unfair/insufficient experimental setup and lack of necessary ablations.** The paper claims the proposed method requires no manual guidance or jailbreak knowledge, yet it relies on an important prior: the attacker must generate a certain number of successful jailbreak prompt frameworks, which evidently depend on manual guidance or jailbreak expertise. The paper also lacks appropriate ablation studies to verify the effect of these priors on the method’s effectiveness—for example, how the attack success rate of the initial jailbreak prompts influences the overall method's performance."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YtIPkUTAbY", "forum": "OkjB6PWJEA", "replyto": "OkjB6PWJEA", "signatures": ["ICLR.cc/2026/Conference/Submission23559/Reviewer_Se2F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23559/Reviewer_Se2F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761457745995, "cdate": 1761457745995, "tmdate": 1762942711620, "mdate": 1762942711620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes JCB (Jailbreak with Cross-Behavior attacks), a black-box jailbreak method designed to efficiently discover harmful prompts for LLMs. JCB leverages prior knowledge about LLMs weaknesses to generate seed prompts, then perturbates them by synonym substitution as a perturbation method. Experiments on the HarmBench and AdvBench benchmarks show that JCB achieves high ASR and requires fewer queries compared to baselines such as PAIR and TAP. The method also demonstrates strong zero-shot transferability across diverse open- and closed-source LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation and practical efficiency: The paper clearly identifies the high query and cost inefficiency of prior black-box jailbreak methods and proposes a lightweight method that avoids reliance on external LLM calls. The design choice, synonym-based perturbation, make JCB simple and computationally efficient, aligning well with the stated goals.\n2. Comprehensive empirical evaluation: The authors evaluate JCB on both HarmBench and AdvBench datasets, covering a wide range of open- and closed-source LLMs. Results consistently show strong ASR and query efficiency, suggesting that the method is both effective and robust across different models."}, "weaknesses": {"value": "1. Lack of ablation studies and unclear source of performance gains: While the proposed JCB is built upon a standard black-box jailbreak optimization pipeline, the paper reports notably superior performance compared to baselines without clearly identifying the source of improvement. JCB relies on two “most successful” jailbreak themes to generate seed prompts, and this design choice seems crucial. However, no ablation study is provided to quantify the effect of theme selection. If seed prompts are so important, their intrinsic jailbreak capability should be evaluated independently. Moreover, the paper claims that synonym substitution as a perturbation method contributes to higher success rates, but it remains unclear whether the performance gains stem from synonym replacement itself or from the strong prior knowledge introduced through theme selection. Similarly, the reported query efficiency advantage may also result primarily from this prior knowledge rather than the perturbation design. A more detailed ablation analysis is needed to clarify these factors.\n\n2. Limited diversity and novelty of discovered jailbreaks: The goal of jailbreak research is to uncover new safety weaknesses in LLMs. However, the jailbreak prompts generated by JCB appear to be confined to the two selected themes (“assumed responsibility” and “character roleplay”). As a result, JCB may not reveal fundamentally new vulnerabilities, and the diversity of generated prompts could be limited. This restricts the method’s broader contribution to improving LLM safety by identifying novel failure modes.\n\n3. Concerns about experimental fairness: JCB leverages strong prior knowledge through theme selection, which provides it with a substantial advantage over weaker baselines such as PAIR. This raises concerns about the fairness of the comparisons. Although Section 4.6 argues that comparisons with AutoDAN-Turbo are “unfair” due to its higher complexity, JCB’s own use of domain priors arguably creates a similar imbalance when compared against other black-box baselines.\n\n4. Concerns regarding implementation correctness and comparability: It appears that the baseline results reported in the paper are directly cited from AutoDAN-Turbo paper rather than reproduced within the same experimental setup. This makes it difficult to verify whether the evaluation pipeline is consistent across methods and raises concerns about the comparability and reliability of the reported results."}, "questions": {"value": "1. Are all the baseline results reported in this paper directly taken from the AutoDAN-Turbo pape?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uL9mtTq3r7", "forum": "OkjB6PWJEA", "replyto": "OkjB6PWJEA", "signatures": ["ICLR.cc/2026/Conference/Submission23559/Reviewer_DUEw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23559/Reviewer_DUEw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877953224, "cdate": 1761877953224, "tmdate": 1762942711360, "mdate": 1762942711360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes JCB, a black-box jailbreak method that randomly selects seed prompts with probabilities proportional to their past attack successes, and then performs synonym-based perturbations to bypass the safety defenses of LLMs. Experimental results show that JCB achieves a high ASR while requiring relatively few queries to the target LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a reasonably comprehensive set of baseline experiments.\n2. JCB achieves a high ASR while requiring fewer queries to the target LLMs."}, "weaknesses": {"value": "1. The proposed method of selecting seed prompts based on prior attack successes and performing synonym-based perturbations is quite similar to existing approaches (e.g., PAIR [1] using an LLM to rewrite prompt seeds) and thus lacks novelty.\n2. Although the paper covers multiple target LLMs, the main results do not include recent state-of-the-art models. In contrast, while the later experiments evaluate newer models, they lack corresponding baseline comparisons, making it difficult to clearly assess the advantage of JCB.\n3. The paper does not present the performance of JCB under mainstream defense strategies (e.g., self-reminder[2]), leaving the real-world impact and severity of the revealed vulnerabilities unclear.\n\n---\n[1] Chao P, Robey A, Dobriban E, et al. Jailbreaking black box large language models in twenty queries[C]//2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). IEEE, 2025: 23-42.\n\n[2] Xie Y, Yi J, Shao J, et al. Defending chatgpt against jailbreak attack via self-reminders[J]. Nature Machine Intelligence, 2023, 5(12): 1486-1496."}, "questions": {"value": "1. How would using different LLMs for generating the seed prompts affect the results?\n2. In Line 349, “ChatGPT-4o-Latest” should specify the exact version, since using “latest” could lead to inconsistencies in future versions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cDqX0GLSjo", "forum": "OkjB6PWJEA", "replyto": "OkjB6PWJEA", "signatures": ["ICLR.cc/2026/Conference/Submission23559/Reviewer_RLgU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23559/Reviewer_RLgU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924621085, "cdate": 1761924621085, "tmdate": 1762942711058, "mdate": 1762942711058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}