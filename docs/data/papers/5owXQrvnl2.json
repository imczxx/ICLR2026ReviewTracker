{"id": "5owXQrvnl2", "number": 14527, "cdate": 1758238013280, "mdate": 1763661314645, "content": {"title": "Efficient MIP-LP Gap Mitigation for Predict+Optimize", "abstract": "The Predict+Optimize (P+O) paradigm seeks to train prediction models for unknown parameters in optimization problems, with the goal of yielding good optimization solutions downstream. Prior works have proposed strategies for gradient computation in neural network training, when the downstream optimization is a linear program (LP). Yet, in face of mixed-integer linear programs (MIP), much prior work simply relax the MIP into an LP, resulting in sub-optimally trained predictors. The issue is particularly stark in the recent Two-Stage Predict+Optimize framework, where even the MIP constraints can contain uncertainty.\n\nIn this work, we propose a (shockingly) simple and fast approach for addressing the MIP-LP gap, and show that it yields essentially the same or more accuracy gains over a much slower method adapted from prior work. Concretely, for the latter, we adapt the approach of MIPaaL (Ferber et al. (2020)) and introduce cutting planes into the LP relaxation, before using LP-based gradient computation methods. Such adaptation is slow and requires some work for the new Two-Stage P+O setting, given the constantly-changing constraint predictions during training. We instead propose and advocate for a far simpler method: replace the relaxed-LP optimum in the LP-based gradient computation with the actual true MIP optimum, avoiding the repeated use of (slow) cutting plane MIP solvers in the slow method.\n    \nExperimental results on 3 benchmarks show that this simple strategy yields the same or more accuracy gain over the much slower cutting plane approach, and the conjunctive use of the two methods yields only minor further gains at the expense of vastly increased training time, sometimes by a whole order of magnitude.", "tldr": "", "keywords": ["Discrete Optimization", "Predict+Optimize", "Decision-Focused Learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8272b078535e2dfed927dd9e6a902ec885b10ba2.pdf", "supplementary_material": "/attachment/e530b2c033877e7ba4fd45b600befdd2193d80e8.zip"}, "replies": [{"content": {"summary": {"value": "This paper examines how to address the MIP–LP gap in Predict+Optimize training, where integer constraints are typically relaxed. The authors propose a simple but effective fix: instead of using the LP relaxation solution when computing gradients, they directly use the true MIP optimum from neural solvers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The main idea is refreshingly simple. Instead of building a complex surrogate or adding a long chain of approximations, the authors just use the true MIP solution inside the training loop. The simplicity itself makes the message clear: sometimes the straightforward fix can go a long way.\n- The paper reads clearly. The authors explain the setup step by step, which is not common in papers that integrate optimization and learning.\n- The experiments are well organized and cover three different benchmarks. They show that this simple change can achieve accuracy on par with or slightly better than a more complex MIPaaL-style approach."}, "weaknesses": {"value": "- The main concern is the limited novelty. The method is very close to MIPaaL and differs mostly in implementation. The paper does not introduce a new theoretical idea or learning principle.\n- The use of the true MIP optimum is treated as a direct replacement, but the paper does not analyze why this replacement could contribute to more ideal gradients. The derivation in Section 3.2 depends on KKT conditions that are not valid for discrete problems, but this issue is not discussed.\n- The use of KKT-based differentiation for a discrete problem is mathematically questionable. The KKT system assumes smoothness and convexity, which do not hold when integrality constraints are present. The paper admits this implicitly but does not discuss what kind of approximation the resulting gradient represents. This omission weakens the credibility of the method’s foundation.\n- The method depends on solving a full MIP at every iteration. Although branch-and-bound solvers are faster than cutting-plane solvers, the runtime can still grow rapidly with problem size or solution variance. The paper does not analyze how training scales with instance complexity, nor does it discuss how solver randomness or multiple optimal solutions might affect gradient consistency."}, "questions": {"value": "- When the discrete optimum changes between iterations, how is gradient discontinuity handled? Are there cases where training fails to converge?\n- Since the gradient no longer comes from a differentiable mapping, what prevents the optimizer from following unstable directions?\n- How does the proposed method handle multiple optimal MIP solutions? Which one is used for gradient computation, and does this choice affect the results?\n- Can the authors clarify whether the gradients computed with integer solutions have any interpretation as subgradients of a convex envelope?\n- How sensitive is the training process to solver tolerances and time limits? Could these parameters change the gradient signal significantly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UKQFcmJsJY", "forum": "5owXQrvnl2", "replyto": "5owXQrvnl2", "signatures": ["ICLR.cc/2026/Conference/Submission14527/Reviewer_1ivb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14527/Reviewer_1ivb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761642921204, "cdate": 1761642921204, "tmdate": 1762924923344, "mdate": 1762924923344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers differentiable optimization for MIPs with both a first stage decision and then a recourse step (to repair feasibility violations at some cost), and where predictions enter into the constraints as well as the objective. They propose that instead of adding cutting planes to tighten the LP relaxation before differentiation, we can instead use a simpler straight-through style approach of substituting the exact MIP optimum into the LP before differentiating."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Predict + optimize with uncertainty is a challenging setting that most previous work avoids (preferring to handle only uncertainty in the objective). The proposed approach is very simple to implement and benefits from the (many) settings where MIPs are solvable efficiently via other strategies like branch and bound but not via cutting planes. It seems like a compelling drop-in replacement."}, "weaknesses": {"value": "The experimental validation is not particularly thorough, particularly since 2 of the 3 settings use the same prediction task (which is from a separate domain unrelated to the optimization problem). The paper would be more convincing with more substantively different data distributions. The experiments are also all done with respect to a single, somewhat strange, architecture (5 layer neural network with 16 neurons per layer). Since predict+optimize methods often struggle with difficult training dynamics, it would be worthwhile to see if the findings generalize to other architectures."}, "questions": {"value": "A broader set of experimental results are the biggest place for potential improvement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yymAfOgjkW", "forum": "5owXQrvnl2", "replyto": "5owXQrvnl2", "signatures": ["ICLR.cc/2026/Conference/Submission14527/Reviewer_aJEK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14527/Reviewer_aJEK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858300796, "cdate": 1761858300796, "tmdate": 1762924922919, "mdate": 1762924922919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies decision-focused learning for MILP and addresses the mismatch between training on continuous LP relaxations and testing on discrete MIP problems. Thus, The authors adopt a two-stage predict–then–optimize formulation, where an initial decision based on predicted parameters is later adjusted once the true parameters—appearing in both the objective and the constraints—are revealed, with a linear penalty capturing the cost of revision.\n\nTwo training strategies are proposed: adding cutting planes to tighten LP relaxations, and using the true MIP optimum instead of the LP solution during gradient computation.\n\nExperiments on Weighted Set Multi-Cover, 0–1 Knapsack, and Nurse Rostering demonstrate that the latter approach achieves comparable decision quality to more complex combinations while being significantly faster, providing a practical balance between accuracy and efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Originality:** The proposed strategy, which leverages non-stationary KKT information from integer optima as surrogate gradients, is simple yet novel in combining exact discrete solutions with differentiable LP-based learning.\n- **Quality:** The experimental evaluation covers three benchmark problems of different structure and scale. Results consistently show that using the true MIP optimum improves decision quality over standard LP-based training. The comparisons against cutting-plane variants are fair and well-motivated.\n- **Clarity:**  The paper is clearly written and easy to follow, with a logical presentation of the two-stage framework and experimental methodology.\n- **Significance:** The work addresses a practically important limitation in the gap between LP relaxation and MILP. The proposed approach offers a pragmatic balance between accuracy and efficiency and could influence future work on learning-integrated optimization for MIPs and related combinatorial problems."}, "weaknesses": {"value": "- **Missing related work:** The paper frames the problem as “predict + optimize” but overlooks a large body of existing research under the umbrella of decision-focused learning (DFL). Several prior works have already considered mixed-integer problems, such as SPO+ [1] and PFYL [2]. While many of these methods focus on predicting objective coefficients rather than constraint parameters, their formulations and theoretical insights remain directly relevant to the proposed approach. The absence of this discussion creates the impression that the contribution is more novel than it actually is, and the paper would benefit from explicitly positioning itself within the established DFL literature.\n- **Lack of theoretical justification:** The proposed “MIP Optimum Replacement” strategy injects integer solutions into the KKT system to compute surrogate gradients. However, this process is inherently non-stationary and discontinuous, since integer optima do not satisfy the first-order optimality conditions of the continuous relaxation. The paper provides no theoretical justification, convergence argument, or stability analysis to support the validity of such gradients. This concern becomes even more critical when the LP relaxation and MIP solution differ substantially, as the resulting gradients may no longer reflect meaningful descent directions and could introduce significant bias or instability during training.\n- **Strong Assumption Under Ad-hoc penalty:** The paper addresses infeasible predictions through an ad-hoc linear adjustment penalty proportional to the cost coefficients. This relies on a strong assumption that incorrect decisions can be revised and that the cost of revision scales directly with the original objective weights. While computationally convenient, such a design lacks theoretical or economic justification and is not a standard practice in optimization or decision analysis.\n- **Limited experimental scale and generality:** The evaluation is restricted to small, toy-sized benchmarks with only tens of variables and constraints. For instance, the Knapsack and Nurse Rostering problems involve fewer than 50 decision variables, which limits the relevance of the reported improvements to realistic settings. As a result, it remains unclear whether the proposed method would remain stable or computationally viable on larger or more structured mixed-integer problems.\n\n[1] Mandi, J., Stuckey, P. J., & Guns, T. (2020, April). Smart predict-and-optimize for hard combinatorial optimization problems. In Proceedings of the AAAI conference on artificial intelligence (Vol. 34, No. 02, pp. 1603-1610).\n\n[2] Berthet, Q., Blondel, M., Teboul, O., Cuturi, M., Vert, J. P., & Bach, F. (2020). Learning with differentiable pertubed optimizers. Advances in neural information processing systems, 33, 9508-9519."}, "questions": {"value": "1. The proposed method uses integer optima within the KKT system to compute gradients, which are inherently non-stationary. Have the authors analyzed whether these surrogate gradients provide unbiased or stable updates in expectation?\n2. Intuitively, the effectiveness of the MIP Optimum Replacement strategy should depend strongly on the tightness of the LP relaxation. Have the authors evaluated how the method behaves when the relaxation is loose and the integrality gap between the LP and MIP optima is large? In such cases, could the substituted KKT gradients become less informative or even detrimental to training performance?\n3. The experiments are conducted on small-scale instances with fewer than 50 variables. Could the authors comment on the computational scalability of their approach for larger problems?\n4. The paper employs CPLEX for cutting-plane experiments and Gurobi for MIP optimization. Have the authors verified that solver-specific behaviors do not bias the comparison in runtime or solution quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ChBriJpe1A", "forum": "5owXQrvnl2", "replyto": "5owXQrvnl2", "signatures": ["ICLR.cc/2026/Conference/Submission14527/Reviewer_9gep"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14527/Reviewer_9gep"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893566566, "cdate": 1761893566566, "tmdate": 1762924922322, "mdate": 1762924922322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes new methodologies for solving MIP in the predict+optimize paradigm beyond directly solving the relaxed continuous version of the optimization problem. \n\nSpecifically, the authors first proposed a cutting-plane-based method that aims to solve the MIP directly but has high time complexity. To address this complexity issues, they also proposed a faster method that aims to solve the MIP objective directly. Additionally, the authors use numerical results to illustrate the performance and interpret some intuitions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is very clear in its setup, contributions, and results.\n2. The paper proposed many new methods of solving MIP in the Predict+Optimize framework, instead of solving the relaxed problem directly."}, "weaknesses": {"value": "1. The exact methodology of choosing cutting planes corresponding to Section 3.1, as well as the branch-and-bound methodology, seems not to be included in the paper. Could the authors elaborate more on that?\n\n2. It seems the paper is purely experimental. Is there any theoretical guarantee on the performance? I am asking this question because both (1) the choice of cutting plane and (2) branch-and-bound choices may depend on the assumption of approximation in Stage 1, so any theoretical results will be greatly appreciated.\n\n3. If this paper wants to purely focus on experimental results, only two problem instances might not be strong enough to answer the listed questions in the paper."}, "questions": {"value": "1. Conceptually, what are the advantages of using the cutting-plane-based method?\n\n2. If the paper advocates for the faster method, I wonder why the authors still propose the slow one and use it as a benchmark, given it is not an SOTA algorithm.\n\n3. Please also see my questions in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZVrges5yqy", "forum": "5owXQrvnl2", "replyto": "5owXQrvnl2", "signatures": ["ICLR.cc/2026/Conference/Submission14527/Reviewer_eSbT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14527/Reviewer_eSbT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14527/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966254523, "cdate": 1761966254523, "tmdate": 1762924921351, "mdate": 1762924921351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}