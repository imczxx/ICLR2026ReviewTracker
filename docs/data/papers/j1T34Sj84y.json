{"id": "j1T34Sj84y", "number": 17717, "cdate": 1758279682048, "mdate": 1759897158602, "content": {"title": "Dual-Forecaster: A Multimodal Time Series Model Integrating Textual Cues via Dual-Scale Alignment", "abstract": "Time series forecasting plays a vital role for decision-making across a wide range of real-world domains, which has been extensively studied. Most existing single-modal time series models rely solely on numerical series, which suffer from the limitations imposed by insufficient information. Recent studies have revealed that multimodal models can address the core issue by integrating textual information. However, these models primarily employ coarse-grained meta information designed for the whole dataset (\\emph{e.g.}, task instruction, domain description, data statistics, etc.), while the use of sample-specific textual contexts remains underexplored. To this end, we propose Dual-Forecaster, a pioneering multimodal time series model that utilizes finer-grained textual information at the sample level through the well-designed dual-scale alignment technique. Specifically, we decouple the learning of semantic and patch-level features, enabling the direct extraction of both global semantic representations critical for cross-modal understanding and local patch features essential for time series forecasting. Our comprehensive evaluations demonstrate that Dual-Forecaster is a distinctly effective multimodal time series model that outperforms or is comparable to other state-of-the-art models, highlighting the superiority of integrating textual information for time series forecasting. This work opens new avenues in the integration of textual information with numerical time series data for multimodal time series analysis.", "tldr": "", "keywords": ["time series forecasting", "multimodal time series model", "cross-modality alignment"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/180ee9c95823376690ffcb7b5a24ecbdcaef0847.pdf", "supplementary_material": "/attachment/15180f5aba4fd0e7723ce6a6bf605d3377e6c801.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents Dual-Forecaster, a novel multimodal time series forecasting model that integrates sample-level textual information through a dual-scale alignment technique. Unlike traditional single-modal models that rely solely on numerical data or previous multimodal methods using only coarse dataset-level text (e.g., task instructions or domain descriptions), Dual-Forecaster incorporates fine-grained textual cues to enhance forecasting accuracy. The model decouples the learning of semantic and patch-level features, enabling extraction of both global semantic representations for cross-modal understanding and local patch features essential for forecasting. Extensive experiments on twelve multimodal time series datasets spanning synthetic, captioned, and real-world data demonstrate that Dual-Forecaster consistently outperforms or matches sota models. This work highlights the superiority of integrating textual semantics into time series forecasting and opens new directions for multimodal analysis that bridges structured numerical data with unstructured text for more robust and interpretable temporal predictions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces an innovative dual-scale alignment technique that effectively integrates semantic- and patch-level representations, allowing the model to jointly capture global textual semantics and local temporal dynamics. This hierarchical design enhances multimodal understanding and leads to more accurate time series forecasting compared to existing models that rely on coarse-grained or unimodal information.\n\n2. Dual-Forecaster demonstrates strong empirical performance through extensive experiments on twelve diverse datasets, including synthetic, captioned, and real-world multimodal benchmarks. The results consistently show that the proposed model outperforms or matches sota baselines, confirming its robustness, scalability, and generalization across multiple domains such as finance, health, and energy.\n\n3. The paper provides a comprehensive methodological framework and well-motivated design choices. By decoupling semantic and temporal feature learning, it offers clear interpretability of how textual cues contribute to forecasting accuracy. This design not only advances multimodal time series modeling but also provides valuable insights for future research on integrating unstructured text with structured temporal data."}, "weaknesses": {"value": "1. While the paper presents promising results, it lacks in-depth analysis of computational efficiency and scalability. The model involves multiple attention modules and dual-branch processing, which may increase training cost and memory usage. A more detailed discussion of runtime performance, hardware requirements, and potential optimization strategies would strengthen its practical applicability.\n\n2. The construction of multimodal datasets relies partly on synthetic or automatically generated textual annotations, which may not fully reflect real-world language complexity or domain variability. This could limit the model’s generalization to natural textual inputs in real applications, especially when dealing with noisy, domain-specific, or ambiguous descriptions.\n\n3. Although Dual-Forecaster integrates textual semantics effectively, the paper provides limited interpretability analysis of how specific textual features influence forecasting outcomes. Visualizations or ablation studies linking textual semantics to prediction changes would help clarify the model’s decision process and improve its transparency for applied domains like finance and healthcare."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VEOXTtEc76", "forum": "j1T34Sj84y", "replyto": "j1T34Sj84y", "signatures": ["ICLR.cc/2026/Conference/Submission17717/Reviewer_fLfx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17717/Reviewer_fLfx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760928855926, "cdate": 1760928855926, "tmdate": 1762927550739, "mdate": 1762927550739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dual-Forecaster, a multimodal time series forecasting model that integrates sample-level textual descriptions with numerical time series via a dual-scale alignment mechanism. The model consists of two branches—textual and temporal—and aligns features at both semantic and patch levels through Text-Time Series Contrastive Loss and Modality Interaction Module. Experiments across three types of dataset (synthetic, captioned-public, and Time-MMD) show consistent improvements over baselines such as UniTime, Chronos, and PatchTST."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tClear architecture and reproducible methodology.\n2.\tThe dual-scale alignment framework is conceptually reasonable."}, "weaknesses": {"value": "1.\tThe captions in public datasets are mainly generated through statistical analysis of the original time series, without introducing additional or external information.\n2.\tThe paper lacks experiments that clearly and intuitively demonstrate the contribution of textual information to forecasting performance.\n3.\tThe paper lacks theoretical or analytical depth—no discussion of why dual-scale alignment is beneficial for forecasting, or how contrastive alignment affects temporal representation learning."}, "questions": {"value": "1.\tHow can the authors demonstrate that adding text generated from the statistical characteristics of the samples helps the model learn non-trivial information that traditional time-series models cannot capture?\n2.\tHow are the evaluation metrics computed? They appear different from those in some related works such as PatchTST.\n3.\tHow does the model perform when the textual input is noisy or replaced with unrelated text?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8QcTv2tuph", "forum": "j1T34Sj84y", "replyto": "j1T34Sj84y", "signatures": ["ICLR.cc/2026/Conference/Submission17717/Reviewer_taE2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17717/Reviewer_taE2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547714812, "cdate": 1761547714812, "tmdate": 1762927550335, "mdate": 1762927550335, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Based on Time-MMD and MM-TSFlib, the author integrates a new text fusion strategy to align text and time series and conducts experiments to evaluate their method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "-  The framework is easy to follow."}, "weaknesses": {"value": "### **1. Dataset**\nThe authors evaluate methods on various datasets, such as Time-MMD; however, some datasets only provide related news about time series, rather than descriptions of the time series themselves. These are in conflict with the authors' motivation. I suggest the author could add more detailed experiment setups.\n\n### **2. Novelty**\nThe proposed framework based on MM-TSFlib does not exhibit significant novelty.\n\n### **3. Definition of Multimodal Time Series**\nFor multimodal time series analysis, I believe multimodal learning requires semantic alignment between modalities, such as speech and language. If there is no semantic alignment, for example, between news and stock, it is more akin to multi-source or multi-factor forecasting. \n\n### **4. Related work**\n\nFor multimodal learning for time series, I suggest that the authors discuss the following papers [1-4].\n\n[1] Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting\n\n[2] Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives\n\n[3] GEM: Empowering MLLM for Grounded ECG Understanding with Time Series and Images\n\n[4] Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting\n\n\n### **Writing**\n\nSome paragraphs and sections are heavily GPT-style."}, "questions": {"value": "Please refer to Weakness."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Some parts of the paper read in a GPT-generated style. It might be worth the ACs’ attention to verify the writing process of this work.\n\nFor example,\n> ## Appendix B. Broader impacts\n>This work introduces a groundbreaking exploration in time series forecasting—a multimodal time series forecasting model that leverages textual modality data to enhance predictive capabilities for time-series analysis. The broader impact of this research is multifaceted. By delivering high-fidelity and reliable forecasts, it empowers advanced decision-making in critical domains such as finance and healthcare, where precision is paramount. Moreover, its strong interpretability enables actionable insights for optimized resource allocation and enhanced patient care protocols. The societal implications are profound: this work establishes a novel framework for integrating complex time-series data with emerging AI technologies (e.g., LLMs), fundamentally transforming how time-series data is analyzed and utilized across diverse sectors. By bridging textual semantics and temporal dynamics, this approach paves the way for next-generation predictive models that address the growing demand for multimodal intelligence in real-world applications.\n\n> ## Appendix C.2 Multimodal Time Series Benchmark dataset Construction\n> In the realm of time series forecasting, there is a notable lack of high-quality multimodal time series benchmark datasets that combine time series data with corresponding textual series. While some studies have introduced multimodal benchmark datasets (Liu et al., 2024a; Xu et al., 2024), these datasets primarily rely on textual descriptions derived from external sources like news reports or background information. These types of textual data are often domain-specific and may not be consistently available across different time series domains, limiting their utility for building unified multimodal models. In contrast, shape-based textual descriptions of time series patterns are relatively easier to generate and can provide more structured insights. The TS-Insights dataset Zhang et al. (2023) pairs time series data with shape-based textual descriptions. However, these descriptions are based on detrended series (with seasonality removed), which may introduce bias and complicate the interpretation of the original time series data. To address these challenges, we propose six new multimodal time series benchmark datasets where textual descriptions are directly aligned with the observed patterns in the time series. The construction process for these datasets is outlined below."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YN1gHFdNuB", "forum": "j1T34Sj84y", "replyto": "j1T34Sj84y", "signatures": ["ICLR.cc/2026/Conference/Submission17717/Reviewer_LVrt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17717/Reviewer_LVrt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762019798600, "cdate": 1762019798600, "tmdate": 1762927549145, "mdate": 1762927549145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitations of existing time series forecasting models—single-modal information scarcity and multimodal models’ over-reliance on single-type textual data—by proposing Dual-Forecaster, a multimodal framework that integrates both descriptive historical texts and predictive future texts. The core innovations include three cross-modality alignment techniques: Historical Text-Time Series Contrastive Loss, History-oriented Modality Interaction Module, and Future-oriented Modality Interaction Module, which enhance the model’s ability to capture complex semantic-temporal relationships. Extensive experiments on 15 datasets (7 constructed, 8 existing) demonstrate that Dual-Forecaster outperforms SOTA single-modal and multimodal baselines (e.g., PatchTST, MM-TSFlib, Time-LLM) in standard and zero-shot settings. The work contributes a new paradigm for multimodal time series fusion, validates the value of dual-text integration, and provides a reproducible experimental benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Quality: The experimental design is exemplary—covering 15 datasets across synthetic, captioned-public, and real-world multimodal scenarios, with comprehensive baselines (7 models spanning single-modal and multimodal SOTAs). Ablation studies, zero-shot evaluations, and case visualizations (Figures 2, 16) provide multi-faceted validation of the model’s effectiveness and robustness.\n\nClarity: The methodology is explained with sufficient detail (e.g., Eqs. 2–9 formalize each component; Figure 1 illustrates the architecture flow), and the paper contextualizes prior work clearly to highlight its positioning. Technical terms are defined precisely, and the appendix supplements critical details (e.g., dataset captioning process, hyperparameter settings) without cluttering the main text."}, "weaknesses": {"value": "While the paper presents a compelling approach, several limitations warrant further consideration:\n\nReal-World Applicability of Textual Inputs: The method relies heavily on the availability of paired, high-quality textual descriptions. The \"future text,\" which descriptively outlines the pattern of the forecast horizon (e.g., \"rises from time point 74 to 95\"), represents a strong and often unrealistic assumption. In practice, obtaining such precise, oracle-like textual insights about the future is highly challenging. This limits the immediate practical deployment of the model in scenarios where only general news, event reports, or imperfect human feedback are available. The performance under noisy, ambiguous, or weakly correlated textual inputs is not thoroughly investigated.\n\nInsufficient Comparison with Contemporary Multimodal Baselines: The experimental section includes comparisons with general multimodal frameworks like MM-TSFlib. However, it omits direct comparisons with recent, highly relevant works that also focus on deep text-time series alignment, such as TimeCMA (Liu et al., 2024). Including these state-of-the-art competitors is crucial to precisely delineate the advancements offered by Dual-Forecaster.\n\nLimited Depth in Interpretability Analysis: The case study on cross-modality alignment (Figure 3) effectively shows that the model can compute similarity scores but falls short of explaining how these aligned representations concretely lead to better forecasts. A more granular analysis, for instance, visualizing the attention weights in the future-oriented interaction module to show which specific tokens in the predictive text (e.g., \"upward trend\") influenced the correction of the forecast trajectory, would significantly strengthen the claim of \"textual insights-following forecasting.\""}, "questions": {"value": "Given that obtaining accurate, shape-based descriptive text for the future is often impractical, how could the Dual-Forecaster framework be adapted to work with more readily available but noisier textual sources, such as news headlines or expert comments that are not precise descriptions of the time series shape? What is the model's robustness to such noisy and indirect textual guidance?\n\nThe performance gain is attributed to the advanced multimodal comprehension capability enabled by the three alignment techniques. Could you provide a more detailed analysis or visualization (e.g., using attention maps) to show how the \"future-oriented modality interaction module\" utilizes the predictive text to adjust the forecast? For example, which parts of the future text are most influential when the model correctly predicts a trend reversal?\n\nThe computational overhead of the cross-modality alignment modules is non-trivial. Have you explored any model compression or efficiency optimization techniques (e.g., using a smaller language model, reducing the number of layers in the interaction modules) to make the framework more suitable for deployment scenarios with latency or resource constraints?\n\nThe ablation study convincingly shows the contribution of both historical and future texts. However, have you observed any cases or datasets where the inclusion of textual information degraded performance, perhaps due to poor quality or irrelevance of the text? Understanding the failure modes would be valuable for assessing the method's robustness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eFVvVolfoa", "forum": "j1T34Sj84y", "replyto": "j1T34Sj84y", "signatures": ["ICLR.cc/2026/Conference/Submission17717/Reviewer_t8o8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17717/Reviewer_t8o8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762766905280, "cdate": 1762766905280, "tmdate": 1762927548686, "mdate": 1762927548686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}