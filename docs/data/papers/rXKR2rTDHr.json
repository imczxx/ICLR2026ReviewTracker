{"id": "rXKR2rTDHr", "number": 1197, "cdate": 1756863380545, "mdate": 1759898222164, "content": {"title": "Adversarial Attack and Defense for Denoising Diffusion Sampling", "abstract": "Denoising diffusion sampling (DDS) is an emerging approach for generating new samples that have the same distribution as some training samples. However, it is vulnerable to adversarial attacks by even a Gaussian perturbation. In this work, we propose a complete set of adversarial attack and defense methodology for DDS. In the attack side, we propose to inject a perturbation to the sampling stage, which significantly worsen the performance of sample generation. In the defense side, we propose a local variation based regularization model for the potential function minimization, which effectively tolerates the adversarial perturbations. Moreover, we develop a conjugate gradient algorithm to solve the defense model, which integrates with a recently-developed zeroth order rejection sampling method that saves computational and memory costs. Experimental results show that the proposed attack significantly worsen the existing state-of-the-art methods, but can be defended by the proposed local variation regularization.", "tldr": "We propose an adversarial attack and defense approach for denoising diffusion sampling.", "keywords": ["Adversarial attack", "adversarial defense", "local variation", "denoising diffusion sampling"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6824df77fef0d80aaedc76b6470a98ab40f970a5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "- The paper studies adversarial robustness of Denoising Diffusion Sampling (DDS), especially in decentralized settings where the state x_k is transmitted across nodes and can be perturbed. \n- Attack side: proposes injecting small perturbations δ_k at each sampling step (including Gaussian, FGSM, PGD), with a loss tailored to maximize score magnitude.\n- Defense side: proposes a local-variation-regularized objective W(x) = V(x) + λ|∇V(x)| for the potential V, solved via a nonlinear conjugate gradient method with Wolfe line search; the resulting V* is then used in a “zeroth-order” rejection sampling scheme (ZOD-MC) for score estimation.\n- Experiments (mostly in low-dimensional potentials) indicate that the proposed defense improves robustness against the attacks considered."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Problem motivation is timely and relevant: DDS can be sensitive to small perturbations during sampling; studying robustness in decentralized settings is meaningful.\n- Attack formulation is simple and practical, and the empirical finding that even small Gaussian noise can degrade DDS performance is useful.\n- The intent to reduce derivative calls via a “zeroth-order” sampling route is appealing from a computational perspective, given the large number of MC samples in DDS.\n- The paper attempts to tie classical variational ideas (total variation/LV) to modern diffusion sampling defense, which is an interesting direction."}, "weaknesses": {"value": "1) Correctness of the rejection sampling step is unclear/incorrect\n- The acceptance test u ≤ exp(V* − V(z)) is not a standard rejection sampler unless a proposal distribution q(z) and a valid global bound M satisfying p(z) ≤ M q(z) are explicitly provided. As written, acceptance can exceed 1 (if V(z) < V*), violating basic principles.\n- The paper claims V* = min_x V(x) (via Theorem 1 under differentiability). However, Table 1 shows the LV-regularized solver often returns points with V(x*) > min V and |∇V(x*)| ≠ 0, which contradicts the theorem’s implication and makes the acceptance test flawed. If V* is not a guaranteed global minimum, the acceptance probability is invalid and can exceed 1.\n- Overall, the rejection sampling mechanism needs a rigorous derivation and correction (e.g., specifying q(z|x,t), ensuring M is finite and valid, and/or reformulating as a Metropolis–Hastings step or importance weighting). As is, this undermines a central algorithmic claim.\n\n2) Theoretical support for robustness via LV regularization is thin\n- Theorem 1 states min V and min(V + λ|∇V|) share the same minimizers (under differentiability). If so, the regularization does not change global optima, and the claimed robustness derives only from “different optimization paths.” No formal robustness/stability bounds are provided (e.g., how δ_k affects the score error or acceptance rates under the LV term).\n- The use of |∇V| as “local variation” is intuitive, but the paper does not quantify how it attenuates adversarial effects in the DDS pipeline. There are no error propagation analyses, bounds on score mismatch, or guarantees under perturbations.\n\n3) Optimization methodology and efficiency claims are insufficiently substantiated.\n- Non-smooth optimization rigor: W(x) = V(x) + λ||∇V(x)|| is non-smooth. Applying nonlinear CG with Wolfe conditions on a subgradient surrogate needs justification. Please clarify assumptions and provide convergence/line-search guarantees or practical safeguards (e.g., smoothing, backtracking, restarts).\n- Setup cost and amortization: Computing ∂||∇V|| (effectively via Hessian–vector products/double backprop) can be expensive in high dimensions. The paper lacks a complexity/runtime/memory analysis for obtaining V*. Please provide a cost model, wall-clock comparisons under matched budgets, and acceptance-rate improvements to show when the one-off optimization is amortized by the zeroth-order sampler.\n\n4) Experimental scope and reporting are insufficient\n- Experiments appear limited to 2D potentials (e.g., GMM, Müller–Brown). This is too far from modern high-dimensional DDS tasks (e.g., images) to support broad claims. No FID/IS or realistic generative metrics are reported.\n- No ablations on λ, acceptance rates, computational overhead, or sensitivity to perturbation budgets ε and step counts are provided. The claimed efficiency gains from “zeroth-order” calls are not balanced by the added cost of the LV optimization and Hessian–vector products.\n\n5) Relation to TV and prior robust sampling literature is not well grounded\n- The link to total variation is mostly rhetorical; |∇V| is a pointwise gradient norm on parameters rather than a TV functional over a function space. The distinctions and implications are not clearly articulated.\n- Comparisons to alternative robust DDS defenses (e.g., robust integrators, gradient clipping in score estimation, denoising schedules, MH corrections in samplers) are missing."}, "questions": {"value": "- Rejection sampling: Please provide a rigorous derivation. What is the proposal q(z|x,t), what constant M ensures p(z) ≤ M q(z), and how do you guarantee acceptance ≤ 1? If you use min(1, exp(V* − V(z))), what is the implied proposal and bound? How do you handle the case when V* is not the global minimum?\n- Theorem 1 vs Table 1: Under what precise assumptions does the equivalence of minimizer sets hold? How do you reconcile this with Table 1, where your solver returns points with |∇V(x*)| ≠ 0 and higher V(x*)? Are these local stationary points? If so, can they be safely used in the acceptance test?\n- Non-smooth CG and line search: What assumptions ensure Wolfe line search success and convergence for W(x) with non-smooth terms? Can you provide complexity guarantees and practical settings (c1, c2, stopping criteria) that consistently work?\n- Computational cost: How expensive are the Hessian–vector products implicit in Eq. (17) in your implementation (e.g., PyTorch autograd)? Please report the total runtime and memory overhead of the defense per DDS run, and compare to first-order MC methods under matched budgets.\n- Robustness theory: Can you provide quantitative results (bounds or empirical curves) showing how LV regularization affects (i) acceptance rates, (ii) score error ε_score, and (iii) final sampling quality under increasing δ_k strength?\n- Experiments: Can you extend to higher-dimensional tasks (e.g., image diffusion) and report standard metrics (FID/IS)? Please include ablations for λ, ε, the number of time steps, and acceptance rates, and compare against more relevant defenses (e.g., MH corrections, robust integrators, score clipping/smoothing)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YIsyRqKbTO", "forum": "rXKR2rTDHr", "replyto": "rXKR2rTDHr", "signatures": ["ICLR.cc/2026/Conference/Submission1197/Reviewer_cwBv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1197/Reviewer_cwBv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761719017943, "cdate": 1761719017943, "tmdate": 1762915704996, "mdate": 1762915704996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an important investigation into the vulnerability of Denoising Diffusion Sampling (DDS) to adversarial attacks and proposes a novel defense mechanism. The work is systematic, with solid theoretical grounding and extensive experimental validation. \n\nThis paper identifies a critical vulnerability in Denoising Diffusion Sampling (DDS), where even small adversarial perturbations (e.g., Gaussian noise) can significantly degrade sample generation quality. The authors make a threefold contribution: 1) They propose a framework for injecting perturbations $\\delta_k$ into the DDS generation step, demonstrating that both simple Gaussian noise and advanced attacks (FGSM, PGD) can effectively sabotage the sampling process; 2) To counter the attack, they introduce a Local Variation (LV) regularized objective, $min\\ V(x) + \\lambda |\\nabla V(x)|$. They also provide a key theoretical guarantee (Theorem 1) that this problem shares the same solution set as the original min V(x), and develop a conjugate gradient algorithm to solve it efficiently. 3) Through comprehensive experiments on various distributions (GMM, discontinuous potential, Müller-Brown potential) under different attack settings, they show that their defense method, ADDDS, robustly outperforms several state-of-the-art baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The work proposes a systematic study of adversarial robustness in diffusion-based sampling, a crucial concern for their real-world deployment, especially in decentralized settings. \n\n2. The paper provides a full pipeline, not only demonstrating effective attacks but also proposing a principled defense strategy. This end-to-end analysis is more valuable than studying either in isolation.\n\n3. The proof of equivalence between the proposed defense objective and the original problem (Theorem 1) is sound and provides strong justification for the method. The convergence analysis of the proposed algorithm in the appendix further bolsters its theoretical rigor."}, "weaknesses": {"value": "1. While the paper emphasizes the effectiveness of ADDDS, it does not sufficiently quantify the computational cost introduced by the defense. Solving the LV-regularized model via conjugate gradient descent likely incurs higher per-iteration cost and potentially more iterations than minimizing $V(x)$ alone.\n\n2. The convergence analysis in Appendix A.2 relies on strong assumptions, such as $V(x)$ being an \"analytic potential function.\" It is unclear if these hold for the discontinuous potential used in the experiments. The practical convergence behavior of non-smooth functions warrants further discussion.\n\n3. The dimensionality experiments, while positive, are conducted in relatively low dimensions. The behavior of the LV regularizer and the conjugate gradient method in very high-dimensional spaces (e.g., hundreds or thousands of dimensions, as in modern generative models) remains an open question."}, "questions": {"value": "1. Can you provide a more detailed analysis of the computational trade-off? A runtime comparison table or plot, similar to Figure B1, but explicitly highlighting the cost of the defense module.\n\n2. Can the convergence guarantees for the proposed algorithm be extended to a broader class of non-smooth potential functions?\n\n3. How would the defense fare against a strong, adaptive white-box attacker who has full knowledge of the defense mechanism (the LV regularizer) and optimizes the attack accordingly?\n\n4. The experiments are on synthetic data. How can your method be adapted and tested in the sampling process of large-scale, pre-trained diffusion models (e.g., for image generation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lRFlCCPXuU", "forum": "rXKR2rTDHr", "replyto": "rXKR2rTDHr", "signatures": ["ICLR.cc/2026/Conference/Submission1197/Reviewer_MTWH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1197/Reviewer_MTWH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732246599, "cdate": 1761732246599, "tmdate": 1762915704736, "mdate": 1762915704736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the vulnerability of denoising diffusion sampling (DDS) to adversarial attacks and presents both attack and defense methodologies. On the attack side, it formulates Gaussian and gradient-based adversarial perturbations. In response, the authors propose a local variation (LV) regularized minimization method for the potential function and a conjugate gradient algorithm tailored to the non-differentiability of the LV term. Experiments demonstrate the efficacy of the attack and the improved robustness of the proposed ADDDS."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The LV-regularized minimization mechanism is an original formulation for defending against adversarial corruption in DDS, with theoretical guarantees and algorithmic innovation. \n\n2. The implementation of the Fréchet subdifferential, together with an efficient conjugate gradient procedure, is technically sound and aligns with non-smooth optimization theory.\n\n3. The experiments are extensive: results are shown for varying oracle complexity, dimensionality, potential functions, attack schemes and strengths with eight selected baselines."}, "weaknesses": {"value": "1. The paper does not summarize the role of the LLMs for this work in a separate section, which contradicts the ICLR 2026 submission policy.\n\n2. The gradient flow involving LV regularization is realized jointly, but there is no ablation on how sensitive the entire optimization is to the hyperparameter $\\lambda$. Only a trivial value $\\lambda= 1$ is used in the paper without a sanity check.\n\n3. Although the included baselines are broad, none are explicitly designed for improving adversarial robustness. The authors omit the discussion on adversarially trained diffusion model [1], which is a common defense strategy.\n\n4. It remains unclear whether the ADDDS will fail under white-box attack, where the adversary has the knowledge of ADDDS and can optimize the adversarial perturbations accordingly.\n\n[1] *What is Adversarial Training for Diffusion Models? arXiv preprint arXiv:2505.21742, 2025.*"}, "questions": {"value": "1. Are there limitations preventing extension of ADDDS to high-dimensional or real-world diffusion models (e.g., images or language)? Are there computational or convergence bottlenecks?\n\n2. Is it possible to derive a formal adversarial robustness guarantee for LV-regularization approach under $\\ell_p$-bounded perturbations or standard Gaussian noise?\n\nPlease respond to both the questions and weaknesses in the later discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NpNLPV7Tf3", "forum": "rXKR2rTDHr", "replyto": "rXKR2rTDHr", "signatures": ["ICLR.cc/2026/Conference/Submission1197/Reviewer_mQUP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1197/Reviewer_mQUP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863324335, "cdate": 1761863324335, "tmdate": 1762915703679, "mdate": 1762915703679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes adversarial attacks and also related defenses for DDS process, by introducing adversarial attacks into each denoising step, the DDS will effectively make the samples away from original distribution. The paper focus on theoratical sides and did experiments on toy examples to verify their insights."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper systematically study adversarial attacks that inject noise for each denoising step to attack DDS. \n- The proposed ADDDS can increate robustness of DDS process by adding regularization."}, "weaknesses": {"value": "- The settings of attacker sound impratical to me. Inject noise into each denoising step is too strong from my side. A more pratical and more interesting setting is to only inject noise at the start noise or one of the middle states.\n- For attack against DDS, the results under the settings in this paper is not suprising.\n- For defense algorithm ADDDS, V(x) could be unknow for high-dim settings.\n- The paper works on low dimensional settings on GMM, lacking high dimensional experiments e.g. CIFAR on image generation.\n- The paper does no take the vulnerability of s(t, x) into account."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ZKZKgR1BJL", "forum": "rXKR2rTDHr", "replyto": "rXKR2rTDHr", "signatures": ["ICLR.cc/2026/Conference/Submission1197/Reviewer_WpLD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1197/Reviewer_WpLD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1197/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894507053, "cdate": 1761894507053, "tmdate": 1762915703436, "mdate": 1762915703436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}