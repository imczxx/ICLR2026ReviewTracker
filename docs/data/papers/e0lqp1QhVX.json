{"id": "e0lqp1QhVX", "number": 21638, "cdate": 1758319958817, "mdate": 1759896911271, "content": {"title": "Schema for In-Context Learning", "abstract": "In-Context Learning (ICL) enables transformer-based language models to adapt to new tasks by conditioning on demonstration examples. However, traditional example-driven in-context learning lacks explicit modules for knowledge retrieval and transfer at the abstraction level. Inspired by cognitive science, specifically schema theory, which holds that humans interpret new information by activating pre-existing mental frameworks (schemas) to structure understanding, we introduce SCHEMA-ACTIVATED IN-CONTEXT LEARNING (SA-ICL). This proposed framework extracts the representation of the Building Blocks of Cognition for the reasoning process instilled from prior examples, creating an abstracted schema — a lightweight, structured template of key inferential steps and their relationships — which is then used to augment a model’s reasoning process when presented with a novel question. We demonstrate that a broad range of large language models (LLMs) lack the capacity to form and utilize internal schema-based learning representations implicitly, but instead benefit significantly from explicit schema-based scaffolding. Across chemistry and physics questions from GPQA dataset, our empirical experiment results show that SA-ICL consistently boosts performance (up to 36.19%) when the single demonstration example is of high quality, which simultaneously reduces reliance on the number of demonstrations and enhances interpretability. SCHEMA-ACTIVATED IN-CONTEXT LEARNING not only bridges disparate ICL strategies ranging from pattern priming to Chain-of-Thought (CoT) prompting, but also paves a new path for enhancing human-like reasoning in LLMs.", "tldr": "A schema-activated framework for in-context learning that enhances human-like reasoning.", "keywords": ["In-Context Learning", "Schema Theory", "Retrieval-Augmented Generation", "Human-like Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ea129c5f04ce95d3baa381c07e3bed37f67e30e2.pdf", "supplementary_material": "/attachment/6814f3b16277edd82448e7bffa5241345d513aff.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes SA-ICL, which enables large language models to form and activate abstract schemas from prior examples to guide reasoning on new tasks. This schema-based approach improves accuracy by up to 39.7% on scientific reasoning benchmarks while enhancing interpretability and efficiency compared to traditional example-driven ICL."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper introduces a cognitively grounded framework, SA-ICL that models human schema activation, bridging cognitive psychology and machine learning.\n- SA-ICL demonstrates substantial accuracy improvements (up to 39.67% in chemistry and 34.45% in physics) across multiple large language models."}, "weaknesses": {"value": "- The experiments are restricted to scientific reasoning (physics and chemistry) on the GPQA dataset, leaving unclear whether SA-ICL generalizes to other reasoning domains such as commonsense, mathematics, or open-ended tasks.\n- The model scope is also limited. It would be beneficial if the authors could expand to more models, such as the thinking models.\n- Typos: such as \"gicen\" in line 932.\n- The study primarily contrasts SA-ICL with One-Shot and CoT prompting, omitting comparisons to stronger retrieval-based or structured reasoning baselines."}, "questions": {"value": "- I'm still concerned about the intellectual contributions of this paper, and how the proposed methods could been adopted in real applications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WVsnuhSn6H", "forum": "e0lqp1QhVX", "replyto": "e0lqp1QhVX", "signatures": ["ICLR.cc/2026/Conference/Submission21638/Reviewer_1Fkr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21638/Reviewer_1Fkr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761694887386, "cdate": 1761694887386, "tmdate": 1762941866317, "mdate": 1762941866317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Schema Activated In Context Learning, a framework that augments example based prompting with explicit schemas that capture reusable reasoning patterns. Given a new problem, the method forms a representation, retrieves relevant schemas and past examples from memory, activates and adapts a chosen schema, and then uses it to guide inference. The memory links schemas with episodic traces so the system can associate a query with similar prior situations."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The human-inspired ICL is an interesting narrative. The discussion of schemas from cognitive science is inspiring."}, "weaknesses": {"value": "There is no comparison with many-shot ICL. It remains unclear the influence of exemplars. The paper only uses one-shot ICL which is very uncommon for ICL.\n\nI feel This work is not really about ICL. It is more like adding advanced system prompts to tell models what methods or tools to use to tackle the problem. The proposed framework is similar to an advanced prompting technique like self-refine [1] or reflexion [3]. The llms refine their strategy during prompting. There are also many advanced ICL frameworks (e.g., [2]). With that said, more baselines should be used as comparison. Currently the empirical support is weak.  And experiments are only evaluated on one scientific QA dataset. Evaluation on more general-domain data is needed.\n\nSchema is not very clearly explained. Appendix E and F are not explicitly mentioned in a clear way to let readers understand schema is actually a prompting template. The current writing is unnecessarily complex.\n\nThe citation format seems wrong through the whole paper. there is no bracket.\n\nThe exemplar selection part is weak. Ablation study or multiple experiments on what exemplars are chosen can be helpful to test the robustness of Schema-ICL framework.\n\n[1] Madaan, Aman, et al. \"Self-refine: Iterative refinement with self-feedback.\" Advances in Neural Information Processing Systems 36 (2023): 46534-46594.\n\n[2] Zhang, Yiming, Shi Feng, and Chenhao Tan. \"Active example selection for in-context learning.\" arXiv preprint arXiv:2211.04486 (2022).\n\n[3] Shinn, Noah, et al. \"Reflexion: Language agents with verbal reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2023): 8634-8652."}, "questions": {"value": "What is S=R(x)? Is S hidden states or verbal prompts? How do you get R(x)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NFQ8sdAUi5", "forum": "e0lqp1QhVX", "replyto": "e0lqp1QhVX", "signatures": ["ICLR.cc/2026/Conference/Submission21638/Reviewer_jLfB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21638/Reviewer_jLfB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830975897, "cdate": 1761830975897, "tmdate": 1762941865955, "mdate": 1762941865955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Schema-Activated In-Context Learning (SA-ICL), a framework inspired by cognitive science schema theory. The approach extracts abstract \"schemas\" (structured templates of reasoning steps) from demonstration examples, retrieves relevant schemas for new problems, and uses schema activation to guide LLM reasoning. The authors evaluate SA-ICL on graduate-level chemistry and physics questions from the GPQA dataset, reporting improvements up to 39.67% over one-shot prompting when high-quality similar examples are available. The method aims to bridge example-driven and abstraction-driven in-context learning approaches."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper contributes to the field of prompting engineering by proposing a new schema-based ICL framework with five stages: problem representation, schema retrieval, episodic retrieval, schema activation, and schema-guided solving. The method could be practically relevant to the recent emerging topic of vibe coding with LLMs.\n2. The paper makes a genuine attempt to bridge cognitive science (schema theory) and machine learning, which is relatively underexplored in the ICL literature. While structured prompting and retrieval-augmented methods exist, the explicit operationalization of schema activation that distinguishes between retrieval and activation, and incorporates assimilation/accommodation concepts, could offer an interesting perspective.\n3. While the scope is limited and the baselines are weak, the overall experimental design covers multiple levels of analysis and spans across multiple model families.\n4. The paper has a generally clear presentation with good organization."}, "weaknesses": {"value": "1. The evaluation is confined to only two domains (chemistry and physics) on a single benchmark (GPQA), testing exclusively on closed-ended multiple-choice questions. This limited scope is insufficient to support the broad claims made about advancing in-context learning. There are no evaluations on standard ICL benchmarks (e.g., tasks from BBH, BB-Extra-Hard, MMLU-pro subsets), and also hard reasoning benchmarks like AIME, MedXpertQA, and ZebraLogic.\n2. There are no open-ended generation tasks where schema benefits could be more meaningful. The single GPT-5 experiment on Humanity's Last Exam (Appendix G) is too limited and confounded by domain distribution (136/191 questions are biology/medicine, which were not tested in the main experiments)\n3. The paper compares primarily against vanilla One-Shot and basic CoT, missing important classic and recent structured reasoning methods. First of all, the most obvious baseline, Few-shot learning (3-shot, 5-shot), which is standard in ICL literature, is missing. Other missing baselines include: Self-Consistency with multiple reasoning paths, Unified Self-Consistency, Reflexion, ReAct, Tree-of-thoughts, Program-of-Thoughts, MetaICL, and Self-Refine.\n4. There is no analysis of computational costs despite the method requiring multiple LLM calls per query. The paper should report wall-clock time, token counts, and estimated API costs. Compare cost-performance trade-offs (e.g., SA-ICL with 1 example vs. vanilla 5-shot). Analyze whether performance improvements justify additional costs. The authors should also discuss practical deployment considerations.\n5. The schema template (Appendix E) appears hand-crafted with four specific components (Broad Category, Refinement, Specific Scope, Goal), but no ablation studies examine alternative designs. See more details in the Questions section."}, "questions": {"value": "1. How sensitive are results to the specific template structure?\n2. Would different schema components work better?\n3. Is this template optimal for chemistry/physics or domain-agnostic?\n4. Can the template be learned or adapted?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OnELFNOPW2", "forum": "e0lqp1QhVX", "replyto": "e0lqp1QhVX", "signatures": ["ICLR.cc/2026/Conference/Submission21638/Reviewer_JsMQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21638/Reviewer_JsMQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979326475, "cdate": 1761979326475, "tmdate": 1762941865550, "mdate": 1762941865550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Schema-Activated In-Context Learning (SA-ICL), a novel and framework inspired by the \"schema\" concept from cognitive science. The authors argue that in existing few-shot in-context learning, models only learn surface-level \"pattern matching\" and fail to acquire high-level, structured abstract features. To address this, the proposed SA-ICL method explicitly guides the model to extract the problem's structural information (the \"schema\"), indexes similar schemas and retrieves specific episodic examples from a memory bank, and finally uses these retrieved elements to refine (or \"activate\") the original schema. Experiments show that SA-ICL significantly outperforms the standard one-shot method on the GPQA dataset. Furthermore, extensive ablation studies reveal that LLMs lack the intrinsic ability to perform schema activation and require explicit guidance to do so."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Addressing the view that one-shot ICL is merely pattern matching, this paper innovatively introduces the \"schema\" concept from cognitive science. By explicitly guiding the LLM to extract high-level abstractions of a problem, it achieves significantly better results than standard one-shot.\n  2. The internal experiments (ablation studies) are thorough. What I find most impressive is the discovery of the \"schema dormancy\" phenomenon: LLMs do not spontaneously utilize schemas but require explicit activation. This is a point rarely discussed in prior work.\n  3. This paper seeks to explore an alternative path for ICL, introducing novel perspectives to this problem, which is commendable."}, "weaknesses": {"value": "1. The biggest issue is the use of overly weak baselines, an unconvincing experimental design, and a singular dataset. The main experiments are largely devoted to arguing the effectiveness of \"schema\" versus the \"one-shot\" method. However, a crucial end-to-end comparison against mainstream methods on the GPQA dataset, which is the most important part, appears to be absent,  This omission casts doubt on the practical utility of the SA-ICL framework.\n   2. Several aspects of the paper are not clearly articulated, mainly regarding the methodology and experiments. Please see my detailed questions below."}, "questions": {"value": "While this paper novelly introduces the \"schema\" concept in an attempt to align ICL with human cognitive processes and pave a new research path, I have some concerns regarding this type of pioneering work.\n\n1. Given the extensive literature reviewed in the Related Work, why are the primary baselines limited to only One-Shot and One-Shot+CoT? A paper that claims to \"pave a new path\" should provide a comprehensive experimental comparison against current mainstream methods, including an analysis of the performance-efficiency trade-off. Without such support, it is difficult to convince researchers to follow this work.\n\n2. Even in the comparison against One-Shot, the paper lacks an end-to-end (E2E) evaluation across multiple datasets. It appears that to isolate the effectiveness of the \"schema\" concept, the authors have fed a pre-selected, high (low)-quality example to the model for each query, rather than letting the system retrieve it. This functions more as a fine-grained ablation study, while the practical, E2E utility of the SA-ICL framework (Algorithm 1) is left unproven.\n\n3. The paper claims SA-ICL is an efficient and lightweight method. While Table 1 shows it is *effective* (high example-utilizing rate), no experiment supports its *efficiency* (lightweight). In practice, the full pipeline (e.g., Appendix F.1) seems to require at least three LLM calls (initial schema generation, schema activation, and final problem-solving). This suggests its computational cost is significantly higher than One-Shot. Could the authors clarify whether the token counts reported in Table 2 represent only the *final* reasoning step, or the *total* tokens consumed across the entire SA-ICL process?\n\n4. The paper criticizes CoT as being instance-specific. If the authors claim that schemas offer cross-domain generalizability, this claim should be substantiated by testing on multiple, diverse datasets beyond GPQA. At a minimum, a mathematics dataset should be added. Furthermore, should the method also be tested on non-logic-intensive datasets (e.g., commonsense reasoning) to demonstrate the breadth of its applicability?\n5. The formalization in Appendix A.1 introduces an association weight $w_{ij}(t)$ with a decay function. Does this imply that, in a practical deployment, the memory bank is dynamic and expected to be updated over time? This aspect of the framework is not explored in the experiments.\n\n6. In the main comparison (e.g., Figure 2, Table 1), SA-ICL is compared against a standard One-Shot baseline, which is given only an example (Q+A). However, the SA-ICL method is given both the example and its schema, which acts as an abstracted reasoning trace. For a fairer comparison, shouldn't the baseline also be provided with a reasoning trace? \n\n7. SA-ICL is presented as a retrieval mechanism operating at an abstract level, built on top of the RAG paradigm. Is there experimental support, in an E2E evaluation, showing that schema-based retrieval is better than query-based retrieval?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XUjLmmKXwl", "forum": "e0lqp1QhVX", "replyto": "e0lqp1QhVX", "signatures": ["ICLR.cc/2026/Conference/Submission21638/Reviewer_8TP9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21638/Reviewer_8TP9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21638/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185649083, "cdate": 1762185649083, "tmdate": 1762941865201, "mdate": 1762941865201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}