{"id": "kEx5VFVd9g", "number": 8469, "cdate": 1758085137507, "mdate": 1759897782269, "content": {"title": "Efficient Fine-tuning via Auxiliary Representation", "abstract": "The widespread adoption of large pretrained models has made fine-tuning an essential step for tailoring models to specific tasks. As these models continue to scale larger and as the demand for task-specific and personalized adaptation grows, parameter-efficient fine-tuning (PEFT) has emerged as a practical alternative to full fine-tuning. PEFT enables effective adaptation while updating only a small fraction of the total parameters. While various PEFT techniques have shown strong performance, many still suffer from increased inference latency and inefficiencies in multi-adapter scenarios. Motivated by these limitations, we propose a novel PEFT approach that leverages auxiliary representations to enable fast and flexible inference. In our method, Latent Task Embedding fine-tuning, a small task-specific latent embedding is concatenated to the original embedding. The corresponding weight matrices are extended, and only the additional parameters introduced by this expansion are trained. This design allows for efficient inference using a single matrix multiplication per weight, minimizing latency overhead, and supports task-specific masking to handle multiple adapters within a single model. We evaluate our method on large language models and latent diffusion models, demonstrating improved efficiency and flexibility over existing PEFT baselines.", "tldr": "We introduce a new efficient and flexible fine-tuning scheme by enlarging the latent embedding‚Äôs dimension", "keywords": ["Parameter-efficient-fine-tuning", "Representation tuning", "LLM efficiency"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/14e1eaa96cba05e60fe1e38c443a1a1fe97328bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Latent Task Embedding (LTE), a novel parameter-efficient fine-tuning framework. Specifically, LTE prepends a small task-specific latent vector to the original embeddings and correspondingly extends each weight matrix so that only the newly introduced parameters are trained, enabling single-matrix-multiplication inference and task-wise masking‚Äîdifferentiating it from prior PEFT methods that rely on fully shared or multi-stage computations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Extensive experiments on language and vision tasks show its effectiveness.\n\n2. the paper is well written and easy to follow"}, "weaknesses": {"value": "W1. Lack of novelty. Despite its motivation, the contribution of this work appears limited. Widening the original model‚Äôs width is not a new idea; on the contrary, it is quite straightforward and has been discussed in net2net (ICLR 2016) and LLaMA-Pro (2024).\n\nW2. The improvement is marginal. Under the same parameter budget, latTE shows less than a 0.5% improvement over lora across multiple datasets.\n\nW3. Although the authors claim their method has lower latency than unmerged lora, lora achieves the fastest speed after merging. Moreover, the authors do not discuss coupling with common inference acceleration techniques, such as weight quantization."}, "questions": {"value": "Please see weaknesses W1~3.\n\nMinor:\nTraining efficiency. Under the same parameter budget as LoRA, are the training-time memory usage and the time per iteration similar?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uv9sgacFS8", "forum": "kEx5VFVd9g", "replyto": "kEx5VFVd9g", "signatures": ["ICLR.cc/2026/Conference/Submission8469/Reviewer_NXHC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8469/Reviewer_NXHC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761288199297, "cdate": 1761288199297, "tmdate": 1762920350339, "mdate": 1762920350339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LatTE, a new PEFT method that introduces a small auxiliary latent embedding concatenated to the original input embeddings. During fine-tuning, only the parameters associated with this expanded embedding are updated, while the base model remains frozen.\nUnlike traditional methods such as LoRA or adapters, LatTE maintains single-matrix-multiplication inference, achieving comparable accuracy with minimal latency overhead. The paper demonstrates LatTE‚Äôs effectiveness across both LLMs and diffusion models, reporting competitive or superior performance to baselines like LoRA, BOFT, and OFT.\nThe approach also supports task-specific masking for multi-adapter and multi-task inference, making it scalable for personalized or edge-device deployment scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Instead of updating weights (LoRA) or inserting modules (adapters), LatTE introduces auxiliary representation-level adaptation, expanding the embedding space while keeping inference cost minimal.\n\n2. The design ensures single matrix multiplication per layer without additional latency, which is a well-motivated improvement for deployment and edge use cases.\n\n3. The extension of LatTE to both NLP and vision-generation tasks (text-to-image) demonstrates generality beyond transformer-based text models."}, "weaknesses": {"value": "1. While the paper reports that LatTE ‚Äúmatches or exceeds‚Äù existing PEFT baselines, most observed improvements are marginal (e.g., Qwen2.5-3B in Table 1: 86.63 vs. 86.60 for LoRA). Across many tasks, the best results are still obtained by other methods such as OFT or BOFT. Given the small numerical gaps and lack of confidence intervals or statistical tests, it is difficult to conclude that LatTE offers a consistently superior adaptation performance rather than random variation.\n\n2. All experiments are conducted on relatively small or medium-sized backbones (‚â§ 8 B parameters). Since PEFT is mainly motivated by the impracticality of full fine-tuning in very large models (tens or hundreds of billions of parameters), the absence of larger-scale experiments leaves open whether LatTE remains effective and efficient when model size grows substantially."}, "questions": {"value": "1. Why do the model sizes used in Table 1 (up to 8 B) and Table 2 (up to 3 B) differ? This inconsistency makes it difficult to compare results or assess how the method behaves across model scales and task types"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jKuY4Jz7DA", "forum": "kEx5VFVd9g", "replyto": "kEx5VFVd9g", "signatures": ["ICLR.cc/2026/Conference/Submission8469/Reviewer_hCrW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8469/Reviewer_hCrW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929056120, "cdate": 1761929056120, "tmdate": 1762920349710, "mdate": 1762920349710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Latent Task Embedding, a parameter-efficient fine-tuning (PEFT) approach that concatenates a small task-specific latent embedding to model inputs, expands the associated projection matrices, and trains only the introduced parameters. The claim is that LatTE achieves PEFT-level parameter efficiency while preserving single-matrix-multiplication inference (low latency) and enabling efficient multi-adapter composition via task-specific masking. Evaluation is reported on LLMs and latent diffusion models, with theoretical analysis supporting equivalence or bounded error relative to related PEFT methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear, simple construction: concatenating a learned low-dimensional task embedding and expanding projection weights is easy to implement and reason about. \n\nPractical motivation: latency in multi-adapter scenarios is an important deployment concern; addressing it directly is valuable. \n\nEmpirical breadth: experiments on both autoregressive LLM tasks and latent diffusion models (reported) show broad applicability."}, "weaknesses": {"value": "Theoretical arguments rely on informal equivalence / bounded-difference claims but do not fully characterize when LatTE can fail (e.g., when task embeddings need to encode highly non-linear, high-rank corrections). The bounds lack discussion of constants and dependence on embedding dimension. \n\nInference-latency claims are asserted as single-matrix-multiplication equivalence, but the paper under-specifies real-system measurements (how expanded matrices affect cache locality, memory bandwidth, precision trade-offs). No detailed ablation on embedding size vs. latency/accuracy trade-off.\n\nComparison to some very recent PEFT variants (that also target inference efficiency) is limited: authors should benchmark against the most recent low-latency schemes and provide wall-clock latency/memory breakdowns."}, "questions": {"value": "What are the formal assumptions behind the theoretical bounds? For example, do bounds assume small-norm embeddings or specific activation linearity approximations? Please state exact assumptions and constants. \n\nHow does your method perform when the required task adaptation is not well-approximated by augmenting input-space (i.e., when adaptation needs internal layer reparametrization)? Provide failure cases or diagnostics.\n\nPlease provide precise latency/memory benchmarking (hardware, batch sizes, eff. throughput) and show embedding-dimension vs. accuracy/latency curves."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u9kx9gTrQk", "forum": "kEx5VFVd9g", "replyto": "kEx5VFVd9g", "signatures": ["ICLR.cc/2026/Conference/Submission8469/Reviewer_xVq1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8469/Reviewer_xVq1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974145996, "cdate": 1761974145996, "tmdate": 1762920349013, "mdate": 1762920349013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To improve the inference latency and ef\u0002ficiencies in multi-adapter scenarios of Low rank adaptors methods, the paper propose method, Latent Task Embedding fine-tuning, a small task-specific latent embedding is concatenated to the original embedding. The corresponding weight matrices are extended, and only the additional parameters introduced by this expansion are trained. This design aims to achieve better efficient inference using a single matrix multiplication per weight, minimizing latency overhead, and supports task-specific masking to handle multiple adapters within a single model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly presented and easy to follow, with only a few minor typos.\n- The proposed LatTE method is conceptually straightforward and practically implementable.\n- The experiments are comprehensive, covering LLM fine-tuning across QA, reasoning, and diffusion tasks. LatTE achieves comparable performance to existing PEFT methods such as LoRA on models including LLaMA and Qwen."}, "weaknesses": {"value": "- The reported improvements in performance and inference latency over baseline LoRA are modest and may not convincingly demonstrate practical advantages.\n\n- The discussion of related work is limited in scope, particularly regarding sparsity-based methods in the parameter-efficient fine-tuning (PEFT) literature.\n\n- In PEFT, existing approaches typically fall into two categories:\n(1) Low-rank adaptation methods (e.g., LoRA) and\n(2) Subset or sparsity-based fine-tuning methods, which selectively update parts of the model parameters.\nThe related work section would benefit from a clearer categorization and inclusion of recent sparsity-based works such as:\nSeparating [4‚Äì8] from module- or weight-based methods and creating a distinct subsection on sparsity-based PEFT would strengthen the paper‚Äôs positioning and contextual completeness. Adding a discussion of the sparsity methods in the related work section with recent papers[1-3] would strengthen the contextual foundation of this paper since they are emerging trends in PEFT field.\n\n\n\n### Typos:\n\n‚Äúthe third category focus‚Äù ‚Üí ‚Äúthe third category focuses‚Äù\n‚Äúutilize auxiliary latent embedding‚Äù ‚Üí ‚Äúutilizes auxiliary latent embeddings‚Äù\n* ‚ÄúAnother group in this category modify or edit‚Äù ‚Üí ‚Äúmodifies or edits‚Äù\n* ‚Äúcombines LoRA with pruning or quantization‚Äù ‚Üí remove ‚Äús‚Äù ‚Üí ‚Äúcombine LoRA with pruning or quantization‚Äù\n* ‚ÄúWe now ready to implement it‚Äù ‚Üí ‚ÄúWe are now ready to implement it‚Äù\n\n* ‚Äúreasoning tasks additionally requires‚Äù ‚Üí ‚Äúreasoning tasks additionally require‚Äù\n* ‚Äúthe idential positions‚Äù ‚Üí ‚Äúthe identical positions‚Äù\n* ‚ÄúLoRA and LatTE both shows‚Äù ‚Üí ‚Äúboth show‚Äù\n* ‚Äútrainable parameters closely matches‚Äù ‚Üí ‚Äúclosely match‚Äù\n\n\n\n[1] Scaling Sparse Fine-Tuning to Large Language Models\n\n[2] Sparse Matrix in Large Language Model Fine-Tuning\n\n[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n\n[4] Parameter-Efficient Fine-Tuning without Introducing New Latency\n\n[5] Parameter-Efficient Transfer Learning with Diff Pruning\n\n[6] Diff Pruning: Parameter-Efficient Transfer Learning with Diff Pruning\n\n[7] Training Neural Networks with Fixed Sparse Masks\n\n[8] Composable Sparse Fine-Tuning for Cross-Lingual Transfer"}, "questions": {"value": "- Latency Motivation:\nThe paper argues that LoRA introduces latency due to additional adapters. Could the authors clarify why these adapters are a latency bottleneck? Each adapter involves small matrix multiplications per layer, which typically contribute marginally to total inference time. Moreover, LatTE also introduces extra embedding transformations (ùëì_in and ùëì_out), which are applied to every FFN layer. Why, then, is LatTE expected to reduce latency rather than increase it?\n\n- Latency Profiling:\nIn line 454, the paper reports latency measurements for generating 100 tokens with a 10k context length, averaged over 10 trials. However, the observed improvement seems trivial. Could the authors provide more detailed time profiling (e.g., breakdown by attention, FFN, embedding) to demonstrate the latency behavior of LatTE versus LoRA?\n\n- Architectural Overhead:\nBoth ‚Äúmore-heads‚Äù and ‚Äúwider-heads‚Äù LatTE variants introduce additional projections, which likely increase inference time. Is there quantitative evidence or profiling that measures the actual latency introduced by these modifications?\n\n- Initialization Strategy:\nIn line 194, the paper states that LatTE initializes the embedding with a constant value. What initialization strategy is used for the additional matrix dimensions (ùê¥ and ùêµ)? Initialization plays a crucial role in the performance of low-rank methods, and more discussion on initialization sensitivity would be valuable.\n\nI would like to discuss the questions I raised regarding the weaknesses and concerns with the authors. If my concerns are adequately addressed, I would be willing to reconsider my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SNrpNXSe5s", "forum": "kEx5VFVd9g", "replyto": "kEx5VFVd9g", "signatures": ["ICLR.cc/2026/Conference/Submission8469/Reviewer_2kbB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8469/Reviewer_2kbB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762109160949, "cdate": 1762109160949, "tmdate": 1762920348438, "mdate": 1762920348438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}