{"id": "4nyk3MBnpP", "number": 16750, "cdate": 1758268296740, "mdate": 1759897221451, "content": {"title": "Uni-directional Blending: Learning Robust Representations for Few-shot Action Recognition with Frame-level Ambiguities", "abstract": "Leveraging vision-language models (VLMs) for few-shot action recognition has shown promising results, yet direct image-text alignment methods, such as CLIP, encounter significant challenges in video domains due to frame-level ambiguities. Videos frequently include irrelevant and redundant frames, leading to intra-class ambiguity from non-essential content within the same action and inter-class ambiguity from visually overlapping elements across classes. These ambiguities hinder the learning of distinctive prototypes and robust semantic representations.\n\nTo overcome this, we introduce Uni-FSAR, a novel framework that employs uni-directional blending to selectively integrate relevant frames, preventing contamination of prototypes by irrelevant visual noise. Additionally, a learnable text query (LTQ) bridges the semantic gap between visual features and class labels, enhancing representation alignment. Furthermore, our LTQ-based Semantic Bridging Loss promotes focus on informative frames through similarity-based gradient propagation, mitigating inter-class overlap and fostering more generalizable representations.\n\nExtensive experiments, including cross-dataset evaluations, demonstrate that Uni-FSAR achieves superior robustness in handling frame-level ambiguities compared to prior works. Quantitatively and qualitatively, our method outperforms the state-of-the-art by an average of 2.34% across benchmarks, with a notable 6.5% top-1 accuracy gain on HMDB51, where ambiguities are most pronounced.", "tldr": "We propose uni-directional blending with learnable text queries and semantic bridging loss to enhance robust representations in few-shot action recognition, mitigating frame-level ambiguities and achieving 6.5% top-1 accuracy gain on HMDB51.", "keywords": ["Few-shot Learning", "Prototype Learning", "Vision–Language Alignment", "Frame-level Ambiguity", "Uni-directional Blending", "Learnable Text Query", "Semantic Alignment"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/21f470240a9409ef0875ef07ae64504cd9caedfe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Uni-FSAR, which introduces uni-directional blending with a learnable text query to mitigate frame-level noise in few-shot action recognition. A Semantic Bridging Loss selectively optimizes only the most relevant frames, and OTAM-based prototype alignment is used for temporal matching."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- S1: The motivation behind Uni-FSAR is well-grounded. Frame-level ambiguity represents a critical and realistic challenge in video-based few-shot action recognition.\n- S2: Consistent and significant performance gains are reported across multiple standard benchmarks, indicating strong robustness.\n- S3: The motivation is effectively conveyed through well-designed and intuitive visual illustrations."}, "weaknesses": {"value": "- W1: The LTQ and uni-directional blending appear to be minor variations of existing cross-attention or masked attention mechanisms in BLIP-2 and text-guided prototype learning.\n- W2: The proposed Top-K selection with a contrastive objective is quite similar to hard attention or selective loss used in prior video FSAR works, such as [1].\n- W3: SSv2 5way-1shot results in Tab. 2 are weaker or marginal vs SOTA → contradicts \"generalizable under ambiguity\" claim. \n- W4: While Uni-FSAR aims to mitigate frame-level ambiguity by selectively emphasizing the Top-K frames most aligned with text semantics, it remains unclear whether such single-frame–focused selection sufficiently preserves motion cues that are critical for temporal understanding, especially in datasets like SSv2 where the action is defined by subtle frame-to-frame changes rather than static appearances. By suppressing non-selected frames entirely during optimization, the method may risk discarding essential temporal evidence and oversimplifying the underlying action dynamics that require multi-frame context to recognize.\n- W5: Figures lack sufficient explanation (e.g., Figure 2 symbols).\n\n[1] Task-adaptive Spatial-Temporal Video Sampler for Few-shot Action Recognition"}, "questions": {"value": "- Q1: While an ablation is provided for K=3 in the Top-K strategy, a more detailed explanation or empirical reasoning could further support this selection.\n- Q2: It is unclear why LTQ is necessary when CLIP text encoder already embeds class semantics; what exact semantic gap is being \"bridged\"?\n- Q3:  Prototype formation relies on OTAM, but the paper does not clearly explain whether and how the temporal alignment interacts with the proposed uni-directional blending strategy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kPYQvAc6nR", "forum": "4nyk3MBnpP", "replyto": "4nyk3MBnpP", "signatures": ["ICLR.cc/2026/Conference/Submission16750/Reviewer_6P94"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16750/Reviewer_6P94"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761390858963, "cdate": 1761390858963, "tmdate": 1762926795225, "mdate": 1762926795225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Uni-FSAR framework. The authors identify two types of ambiguities: (1) intra-class ambiguity from irrelevant frames within the same action class, and (2) inter-class ambiguity from redundant frames shared across classes. To tackle these challenges, three core components are introduced: uni-directional blending strategy, Learnable Text Query, and LTQ-based Semantic Bridging Loss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) Problem formulation sharply defines intra- and inter-class frame-level ambiguities with quantitative evidence, providing a compelling, data-driven motivation.\n2) Extensive experiments demonstrate SOTA performance across multiple benchmarks, with particularly impressive gains on noisy datasets and strong cross-dataset generalization capabilities."}, "weaknesses": {"value": "1) The paper compares Uni-FSAR using BLIPv2 ViT-L/14 (508.21M total parameters) against CLIP-FSAR using ViT-B/16 (89.34M parameters). This 5.7× difference in total parameters and the substantially larger vision encoder make it impossible to isolate whether the reported improvements stem from the proposed methodological innovations or simply from using a more powerful backbone. (Tables 1-2)\n2) The proposed method directly applies the Q-Former with 32 learnable visual queries originally designed for static images to each video frame. While this reuse simplifies integration, it remains unclear whether these static-image queries are sufficient to capture temporal dynamics or motion-related cues that are crucial for video-based action understanding.\n3) The paper's core components—uni-directional attention masking, learnable text queries, and Top-K frame selection—are well-established techniques borrowed from existing work."}, "questions": {"value": "Please refer to weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l941e7tPe3", "forum": "4nyk3MBnpP", "replyto": "4nyk3MBnpP", "signatures": ["ICLR.cc/2026/Conference/Submission16750/Reviewer_1jCD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16750/Reviewer_1jCD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790852552, "cdate": 1761790852552, "tmdate": 1762926794747, "mdate": 1762926794747, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The Uni-FSAR framework introduces a Uni-Directional Blending (UDB) mechanism such that visual queries (VQs) can attend to a learnable text query (LTQ), but not vice versa. Here Top-K most relevant frames are aligned with the LTQ to suppress noisy or irrelevant frames and combined with a Learnable Text Query-based Semantic Bridging (LSB) loss,  \nThe model is further integrated into an OTAM-based pipeline for few-shot action recognition. While the proposed Uni-FSAR framework presents an interesting architecture for few-shot video understanding, there are some conceptual and methodological weaknesses which limit its claim to effectively handle frame-level ambiguity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The use of a learnable text query avoids reliance on handcrafted textual prompts.\n2. The LSB loss encourages semantic selectivity and yields better interpretability through attention visualization."}, "weaknesses": {"value": "1. Even after Top-K selection in the loss function, all frame embeddings are averaged equally to form the video prototype. This contradicts the goal of mitigating frame-level redundancy — the prototype is still influenced by uninformative frames.\n2. The choice of Top-K  and the loss weighting factor α  seems to be dataset-specific and tuned manually. This undermines claims of generalization.\n3. Despite claiming to address frame-level ambiguity, the paper introduces no mechanism  that measures inter-frame differences (e.g., variance). The Top-K selection in the LSB loss merely filters frames based on similarity to the LTQ, but it does not take into account the embedding variance between frames within a video."}, "questions": {"value": "I am wondering  how without ever measuring or minimizing frame-level differences in the learning objective it is possible to  “handle frame-level ambiguities”, maybe  I am missing something,  then kindly clarify!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YRngdHFBYn", "forum": "4nyk3MBnpP", "replyto": "4nyk3MBnpP", "signatures": ["ICLR.cc/2026/Conference/Submission16750/Reviewer_kEE2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16750/Reviewer_kEE2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850395359, "cdate": 1761850395359, "tmdate": 1762926794093, "mdate": 1762926794093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a Uni-FSAR framework that aims to improve prototype construction under frame-level ambiguity by selectively utilizing semantically relevant frame information.\nThe authors design a uni-directional blending strategy to prevent irrelevant frames from contaminating class prototypes and introduce a Learnable Text Query (LTQ) module to achieve semantic alignment between visual features and class labels."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The motivation is clear . \n2.The paper is very clearly written. The methodology is presented in a logical and accessible manner, with well-organized sections, clear mathematical formulations, and informative visualizations that make the technical design easy to follow."}, "weaknesses": {"value": "1.The main comparison is made against CLIP-FSAR, yet all experiments in this paper adopt a more powerful BLIP backbone, making it difficult to disentangle whether the performance gain stems from the stronger backbone or from the proposed method itself.\n\n2.The paper lacks a clear ablation study on the Uni-directional Blending mechanism.\nA comparison among uni-directional, bi-directional, and reverse-directional attention designs would be necessary to verify that the observed improvements truly correspond to the claimed motivation."}, "questions": {"value": "The motivation of this work is meaningful, and the results indeed demonstrate improved performance.\nHowever, it remains unclear why the uni-directional blending strategy can selectively integrate relevant frames while preventing prototype contamination by irrelevant visual noise.\nPlease clarify how the proposed mechanism theoretically or empirically achieves this selective filtering, and explicitly connect the motivation to the method’s operational design."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gSk0BaHJq3", "forum": "4nyk3MBnpP", "replyto": "4nyk3MBnpP", "signatures": ["ICLR.cc/2026/Conference/Submission16750/Reviewer_MdP9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16750/Reviewer_MdP9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16750/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999814152, "cdate": 1761999814152, "tmdate": 1762926793528, "mdate": 1762926793528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}