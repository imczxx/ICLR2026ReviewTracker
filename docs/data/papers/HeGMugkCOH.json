{"id": "HeGMugkCOH", "number": 5533, "cdate": 1757918440901, "mdate": 1763705104563, "content": {"title": "Continuous Test-Time Adaptation of Vision-Language Models", "abstract": "Test-time adaptation (TTA) has emerged as a promising paradigm for bridging the distribution gap between pretraining and test data in vision language models (VLMs). Unfortunately, existing methods either assume a static target-domain distribution or rely only on a small subset of samples, which fails to adapt the continuous real-world distributions. In this work, we propose Continuous Test-Time Adaptation (C-TTA), which adapts to the entire target-domain distribution via a continuously updated target prototype that adaptively incorporates visual features from incoming unlabeled test samples based on their class confidence. It is worth highlighting that C-TTA updates only a simple target prototype, which circumvent the heavy backpropagation and large cache access required by previous methods. This endows C-TTA with extremely high efficiency while achieving state-of-the-art performance on 15 image classification benchmarks. For example, C-TTA outperforms all existing training-required methods in cross-dataset generalization, while achieving 5.7\\(\\times\\) faster inference than cache-based TDA on ImageNet. Beyond image classification, C-TTA can be easily applied to 3D VLMs, achieving significant performance gains on 4 challenging point cloud analysis benchmarks.", "tldr": "", "keywords": ["Test Time Adaptation", "Vision Language Models"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/73e86ce0b742f02e843563253eab817dfc5a6247.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper argues that mainstream VLM TTA methods either assume a static target distribution (instance-wise prompt updates) or leverage only a small confident subset (episodic/cached), both failing to model continuous, evolving real-world distributions. It proposes C-TTA, a training-free, cache-free mechanism that continuously updates a class-confidence–weighted target prototype and combines this target distribution with the original CLIP distribution for prediction"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Simple, efficient mechanism: confidence-weighted EMA-style prototype update plus interpolation with text prototypes; no backprop, no cache."}, "weaknesses": {"value": "- Positioning / VLM specificity. The core problem—failure to capture continuous, evolving target distributions—is not specifically for VLM; it is a modality-agnostic TTA/CL issue. This problem is not new, already discussed in our CL/TTA papers. Thus, the method (confidence-weighted prototype accumulation) also appears not only for VLM. The contribution would benefit from (i) sharper VLM-specific phenomena/analyses (e.g., text–image alignment drift, prompt-space dynamics), or (ii) an explicit modality-agnostic framing with supporting evidence beyond VLMs."}, "questions": {"value": "- Which VLM-specific factors (if any) make C-TTA succeed where unimodal continuous TTA would fail? Any diagnostics on cross-modal alignment drift? (See weakness 1)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gFWAUfOZew", "forum": "HeGMugkCOH", "replyto": "HeGMugkCOH", "signatures": ["ICLR.cc/2026/Conference/Submission5533/Reviewer_UAvn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5533/Reviewer_UAvn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761028890302, "cdate": 1761028890302, "tmdate": 1762918117972, "mdate": 1762918117972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "ZPIHNZXNUh", "forum": "HeGMugkCOH", "replyto": "HeGMugkCOH", "signatures": ["ICLR.cc/2026/Conference/Submission5533/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5533/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763705103078, "cdate": 1763705103078, "tmdate": 1763705103078, "mdate": 1763705103078, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a Continuous TTA (C-TTA) method to address adaptation of VLMs to test distributions in an online manner. C-TTA accumulates visual information from test samples, by updating prototypes, in a soft weighted manner. The final predictions is given out as a combination of probabilities from CLIP and the visual prototypes. The method is simple, effective from the results shown."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed method is simple and intuitive to understand. The paper is well presented and is easy to understand.\n- The method is more efficient compared to prior methods like TPT, TDA.\n- The experimental results show significant improvements compared to previous methods which are even more computationally expensive.\n- The method is training-free and requires no cache memory to store features unlike prior methods.\n- Extending beyond Image classification, to 3D point cloud data, expands the scope of this method."}, "weaknesses": {"value": "- **Confusion in term C-TTA:** The term Continuous Test Time Adaptation is a very well established TTA setting where the test domains shift with time, for e.g., from rain->snow->fog, brightness->color-distortion->impulse-noise (CIFAR-10C/ImageNet-C corruptions) as studied in works [1,2]. However, here it not clear if this is indeed the setting followed. \"Continuous real-world distributions\" is mentioned several times, but the experimental details do not describe this original CTTA setting. The results are on individual datasets in Table 1. They are not indicative of domain-changing real world test environments.Continuous-TTA in this work only means that the continuous test data is leveraged to make the predictions (as done in prior works like TDA), but not where the test domains are continuously changing. I suggest the authors to modify the paper, to make this distinction clearly, not use the term \"Continuous real-world distributions\" as it is a misleading narrative.\n\n- **Episodic-TTA:** The works TDA,DPE,etc. are also single image TTA methods, which leverage information from continuous test stream, which is the classical definition of TTA. There is no concept of episodes in the problem-setting considered here. So, what does episodic-TTA mean here. A better convention would be to call all methods (TPT,DiffTPT,...,TCA, TDA, DPE,..,BCA) as Single-image TTA. Although not necessary, if further needed, differentiate instance-wise/no-cache(TPT,DiffTPT,...,TCA) and cache-based methods(TDA, DPE,..,BCA) rather.\n\n- **Sensistivity to hyperparameters:** As the authors also acknowledge in limitations, the method is fairly sensistive and need manual picking of hyperparameters, limiting its use in unknown test environments.\n\n- **Experiments in original CTTA setting:** While the motivation is to adapt to continuously changing real-world distributions in the original CTTA setting[1,2], there are no experiments presented in this CTTA setting. This is a very natural and realistic TTA setting and is important to show its effectiveness when the domains change. Datasets like CIFAR-10C/100C, ImageNet-C, DomainNet can be used to evaluate on single-image continuously domain changing test streams, as the classes remain the same across different corruptions, as done in [1,2]\n\n[1] \"Continual test-time domain adaptation.\", CVPR 2022.\n\n[2] \"Robust Mean Teacher for Continual and Gradual Test-Time Adaptation\", CVPR 2023"}, "questions": {"value": "- Please clarify the term C-TTA, Episodic-TTA or use a different name, as mention in the weaknesses above.\n- Have the authors tried other forms of weighting function $\\beta_j$ to update the prototypes? It would be good to explain the intuition behind choosing equation of this form.\n- The prototypes are updated in two steps: (1) Update class prototype using image feature with adaptive weight $\\beta_j$. (2) interpolate text and image prototypes. Even after doing this interpolation, the scores of CLIP text-classifiers and these new prototypes are combined. But, the text prototype is already incorporated in Step (2). Does this not make the step where probabilities combined redundant? An ablation study on these aspects would justify the need for these steps, adding clarity to the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LRMlOaYnjO", "forum": "HeGMugkCOH", "replyto": "HeGMugkCOH", "signatures": ["ICLR.cc/2026/Conference/Submission5533/Reviewer_qxs2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5533/Reviewer_qxs2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761419650120, "cdate": 1761419650120, "tmdate": 1762918116955, "mdate": 1762918116955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The article addresses the problem of continuous TTA, where the goal is to continuously adapt a vision-language model as test data are processed. Targeting classification tasks, the proposed approach (C-TTA) initialises a prototype per target class. Given a test sample, each prototype is updated using the confidence that the sample belongs to a specific class. The final classifier is then obtained by interpolating the cached prototypes updated at test-time and the CLIP original text prototypes. Performances show the advantage of this approach w.r.t. various TTA  and continuous TTA strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The approach is simple yet sound and effective. Computationally, it requires only to estimate the confidence of CLIP-based zero-shot classifier on the test sample and using these estimates to update the cache. This overhead is minimal w.r.t. standard CLIP inference, both in terms of memory (i.e., only the prototypes need to be stored) and speed (few additional operations). \n\n2. The model achieves good results, outperforming competitors on average on both standard benchmarks (e.g., Tab. 1 and Tab. 2) as well as a 3D application on point clouds (Tab. 3). These results shows the flexibility and effectiveness of the approach. Coupling the effectiveness with its simplicity, the approach might constitute a reference model for the community."}, "weaknesses": {"value": "1. The title is misleading. While the approach is indeed performing the task in a continuous manner, the title might hint that the setting of continuous TTA is proposed with this work. In the introduction, related work and experiments, the setting per-se is denoted as episodic TTA (e.g., 46-49) and the name of continuous TTA associated with the method (e.g., 51). However, the title/name of the approach does not convey any aspect of the technical component (e.g., use of prototypes): it would be better to revise it to avoid confusion.\n\n2. Being in continuous TTA implies that the order in which data arrives impacts the results. For instance, we may have a non i.i.d. distribution (e.g., data of a single class for the first N test samples): is the model robust to such a case? Moreover, how does the order of the data impact the results? These aspects are not thoroughly discussed/analyzed in the manuscript, while still being of importance for understanding the stability of the approach. \n\n3. Linked to the previous point: the gap between C-TTA and the second-best approach is rather small on average (e.g., <0.3 in Tab. 2, 0.06 in Tab. 1). Thus, the dependence on random aspects (e.g., order of data) should be accounted for, as well as reporting the number of runs used to compute the results. This is important for reproducibility as well as reporting a fair ranking.\n\n4.  The results are obtained using hand-crafted prompts (315-315), following TDA. Are all competitors using these prompts? And what is their impact? This again would help readers understand which aspects of the improvement are linked only to methodological design choices of C-TTA.\n\n5. The rationale behind the weighting of Eq. (8) is not clear. Is the function related to previous works, or were alternative mechanisms considered but led to worse results? This could clarify potential challenges of computing the importance of each sample within the weighting mechanism.\n\n6. A similar criticisms goes for how to compute the similarity score for updating the prototypes. According to Eq. (7) and lines 212-214, the scores are computed using standard zero-shot CLIP and the dynamic weights for the prototypes are derived accordingly. Why not using the \"old\" stored prototypes with the prediction described in Eq. (12)? This is counterintuitive as the latter leads to better downstream performance. \n\n6. A similar criticism goes for how to compute the similarity score for updating the prototypes. According to Eq. (7) and lines 212-214, the scores are computed using standard zero-shot CLIP, and the dynamic weights for the prototypes are derived accordingly. Why not use the \"old\" stored prototypes with the prediction described in Eq. (12)? This is counterintuitive, as the latter leads to better downstream performance. \n\n7. Related: Fig. 2.c report values of w in -log scale, showing that high w (e.g., =1) lead to very low results compared to much lower ones (e.g., ~0.1, or the used 0.01). These results are surprising as it seems that prototypes cannot be trusted when updating the cache (Eq. (10)). A similar behaviour is observed regarding the hyperparameter h: for the best value (h=20), $\\beta$ is in the range 0-0.05, thus preserving the original cache values. The latter is meaningful given that it is driven by the number of seen samples and their estimated probability. However, an interesting analysis would have been to compare how much the final prototypes differ from the original CLIP embeddings and from the ideal prototypes (i.e., prototypes obtained from averaging the representations of the ground-truth elements of a class). My intuition is that, given the modality gap, the scores and the resulting $\\beta$ should be quite low, thus vectors might be extremely close to the original CLIP embeddings. If we also account that prototype predictions are further averaged with CLIP ones (Eq. (12)), this may question how clean the prototypes are. As a consequence, if updates are noisy, why not update the prototypes after performing inference (e.g., Eq. (10) after Eq. (12))?\n\nMinor:\n\n9. In lines 470-471 it is stated that hyperparameters need to be manually tuned for each test sample. I guess this is not the case as for the statement and values in 315-316, but the sentence might be rephrased to avoid confusion and the impression that this is what has been conducted in the work.\n\nReferences:\n[a] Liang, Victor Weixin, et al. \"Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning.\" Advances in Neural Information Processing Systems 35 (2022): 17612-17625."}, "questions": {"value": "Following on the weaknesses above:\n\n1. Will the title be aligned to the contribution? if so, how?\n2. Is the model robust to data ordering and non i.i.d. cases?\n3. How many runs were performed for obtaining the results?\n4. How do hand-craft prompts impact the results?\n5. How has been the weighting function derived?\n6. How do the obtained embeddings deviate from CLIP ones?\n7. How would the order of update-prediction steps (point 7 above) and using prototypes for estimating scores (opint 6 above) affect the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rTDZ5zx5si", "forum": "HeGMugkCOH", "replyto": "HeGMugkCOH", "signatures": ["ICLR.cc/2026/Conference/Submission5533/Reviewer_ByLA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5533/Reviewer_ByLA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761679594612, "cdate": 1761679594612, "tmdate": 1762918116322, "mdate": 1762918116322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Continuous Test-time Adaptation (C-TTA) for Vision-Language Models (VLMs), aiming to handle continuous distribution shifts that occur during streaming inference. The method requires no gradient updates or memory cache, leading to significantly improved computational efficiency, reportedly 5.7 to 71.7 times faster inference than prior methods such as TPT and TDA, while maintaining competitive or superior accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. **High efficiency with a strong performance trade-off.**\nThe paper reports up to 5.7 times faster inference speed compared to existing SOTA methods such as TPT and TDA, which makes it highly practical for real-world applications (e.g., VLMs on edge devices or online streaming scenarios). In particular, the EMA-based update that leverages the entire test stream has an extremely small memory footprint and performs only a single-vector update per step, allowing real-time operation.\n\nS2. **Comprehensive and systematic evaluation.**\nThe authors conduct thorough experiments across a wide range of datasets and include comparisons with all major TTA baselines. Both the qualitative visualizations and ablation studies are clear and well-structured, demonstrating a strong level of reproducibility.\n\nS3. **Strong empirical stability and simplicity.**\nThe proposed method involves only a small number of hyperparameters, each with clear intuitive meaning. Its simple EMA-based update shows robust convergence and drift resistance, while removing backpropagation helps mitigate issues such as catastrophic forgetting and overfitting to noisy samples."}, "weaknesses": {"value": "W1. **Insufficient justification for hyperparameter stability.**\nThe paper fixes several hyperparameters throughout all experiments, but provides no sufficient sensitivity analysis to justify this choice.\nIn scenarios with large domain gaps or rapid distribution shifts, the optimal value of a fixed $\\alpha$ is unlikely to remain consistent.\nIt would strengthen the paper if the authors discussed potential alternatives such as adaptive $\\alpha$ scheduling or a decay mechanism to improve robustness across different test-time conditions.\n\nW2. **Limited analysis of robustness to sample order and outliers.**\nIn the TTA-VLM literature, it is standard practice to report the mean and standard deviation over multiple runs to assess performance consistency. Given that the proposed continuous TTA method updates prototypes sequentially, its performance may vary depending on the sample order and inherent randomness in the data stream. Therefore, it would be important to repeat experiments multiple times to quantify this variance and to demonstrate the method’s robustness to sample ordering and outlier contamination.\n\nW3. **Lack of specific results regarding efficiency.**\nIn Figure 1, the authors claim that their approach achieves approximately 5.7 times faster inference while maintaining higher accuracy compared to baselines. However, the main tables (Tables 1 and 2) only report accuracy results without providing per-dataset inference speed. For the sake of logical completeness and fair comparison, it would be helpful to include the inference time or throughput metrics directly in the main tables to substantiate the claimed efficiency advantage."}, "questions": {"value": "Q1. **Lack of evaluation under long-tailed or class-imbalanced scenarios.**\nSince the target prototype is updated only using a limited number of high-confidence samples, it may easily become biased toward head classes. The paper does not include comparisons with recent Long-tailed Test-time Adaptation (LTTA) methods, nor does it present per-class (head vs. tail) performance breakdowns. It would be valuable to demonstrate that the proposed continuous adaptation remains stable and effective under class imbalance or skewed label distributions.\n\nQ2. **Justification for the surprising effectiveness of EMA smoothing.**\nThe main contribution of this paper appears to be the empirical finding that a simple EMA smoothing alone can achieve stable performance under continuous distribution shifts. However, such a simple mechanism leading to significant performance gains calls for a stronger theoretical or empirical justification. Could the authors clarify why EMA smoothing is particularly effective in the TTA setting for VLMs? For instance, which properties of the test-time distribution or representation space make such averaging especially impactful?\nWhile the reviewer finds the proposed simple yet effective approach impressive, the fact that EMA smoothing alone yields substantial gains on continuous TTA benchmarks feels almost magical. A convincing justification for this phenomenon would significantly influence the reviewer’s final evaluation of this work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AbYj28dZru", "forum": "HeGMugkCOH", "replyto": "HeGMugkCOH", "signatures": ["ICLR.cc/2026/Conference/Submission5533/Reviewer_aV5Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5533/Reviewer_aV5Q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5533/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976639267, "cdate": 1761976639267, "tmdate": 1762918115859, "mdate": 1762918115859, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}