{"id": "wLMEtQ5wBa", "number": 3057, "cdate": 1757324195119, "mdate": 1762918848193, "content": {"title": "ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation", "abstract": "Video generative models pretrained on large-scale datasets can produce high-quality videos, but are often conditioned on text or a single image, limiting controllability and applicability. \nWe introduce ID-Composer, a novel framework that addresses this gap by tackling multi-subject video generation from a text prompt and reference images. This task is challenging as it requires preserving subject identities, integrating semantics across subjects and modalities, and maintaining temporal consistency. \nTo faithfully preserve the subject consistency and textual information in synthesized videos, ID-Composer~designs a **hierarchical identity-preserving attention mechanism**, which effectively aggregates features within and across subjects and modalities. \nTo effectively allow for the semantic following of user intention, we introduce \n**semantic understanding via pretrained vision-language model (VLM)**, leveraging VLM's superior semantic understanding to provide fine-grained guidance and capture complex interactions between multiple subjects. \nConsidering that standard diffusion loss often fails in aligning the critical concepts like subject ID, \nwe employ an **online reinforcement learning phase** to drive the overall training objective of ID-Composer into RLVR. \nExtensive experiments demonstrate that our model surpasses existing methods in identity preservation, temporal consistency, and video quality. \nCode and training data will be released.", "tldr": "", "keywords": ["Multi-Subject Video Generation", "Reinforcement Learning for Generation", "Semantic Understanding for Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7232b3dd329033336a60891e4022f2cbf07d976c.pdf", "supplementary_material": "/attachment/76ac5dae3dc1543a539398fdbd4bf2ce8887ec2a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes ID-Composer, a framework for multi-subject video generation. The approach consists of (1) a hierarchical identity-preserving attention mechanism, (2) semantic understanding via pretrained vision-language model (VLM), (3) a data curation pipeline, and (4) an online reinforcement learning (RL). Experiments follow the OpenS2V-Nexus evaluation protocol. The base video generation model is Wan Video 1.3B, and the VLM is Qwen2.5-VL-7B-Instruct."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The overall method is clearly described and easy to follow.\n2. The experiments include comparisons with both closed-source and open-source models.\n3. A project website is provided for direct visual comparison of generated videos."}, "weaknesses": {"value": "1. The authors have not reported the generation time complexity of the proposed method. Given that the hierarchical identity-preserving attention involves three operations, i.e., intra-subject attention, inter-subject attention, and multi-modal attention, it is likely to incur additional inference/generation time compared to the base model.\n2. The method uses RL for optimization, but there is neither an explanation nor a comparison with SFT. In addition, while the right panel of Figure 5 appears to show results with and without RL, this is not discussed in the main text. Quantitative ablation results without RL are also missing from Table 2."}, "questions": {"value": "1. The authors describe the training datasets in detail, but the evaluation dataset of 180 unique subject–text pairs is not well described. Could the training and evaluation datasets overlap? If not so, what have the authors done to avoid data leakage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gQ2uI75ecU", "forum": "wLMEtQ5wBa", "replyto": "wLMEtQ5wBa", "signatures": ["ICLR.cc/2026/Conference/Submission3057/Reviewer_T9qL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3057/Reviewer_T9qL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3057/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760946692882, "cdate": 1760946692882, "tmdate": 1762916532872, "mdate": 1762916532872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "HSt72mgKGD", "forum": "wLMEtQ5wBa", "replyto": "wLMEtQ5wBa", "signatures": ["ICLR.cc/2026/Conference/Submission3057/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3057/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762918846859, "cdate": 1762918846859, "tmdate": 1762918846859, "mdate": 1762918846859, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ID-Composer, a framework for multi-subject video generation based on text prompts and reference images. The method introduces a hierarchical identity-preserving attention mechanism to integrate subject features. Additionally, it leverages a pretrained vision-language model to enhance semantic understanding and capture complex multi-subject interactions. To better improve identity consistency, the authors further incorporate an online reinforcement learning stage. Experiments show that ID-Composer outperforms existing approaches in identity preservation, temporal consistency, and video quality."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. By reading the related work section, I found that the authors are very familiar with the field of subject-driven video generation, which is the focus of this work.\n\n2. The paper is clearly written and easy to follow. The authors provide sufficient background and motivation, and the structure of the paper is logical and coherent. The experimental design is sound, with comprehensive evaluations on relevant benchmarks, and the results convincingly demonstrate the effectiveness of the proposed approach. Overall, the experiments are thorough and well-presented."}, "weaknesses": {"value": "1. Flow-GPRO evaluates the advantage function using only noise-free samples. However, this important detail is not introduced or discussed in Section 3.3."}, "questions": {"value": "1. The paper claims to integrate the VLM’s advanced reasoning (e.g., line 242). However, to my understanding, ID-Composer only leverages the VLM to extract semantic features from text and images, without performing any explicit reasoning process (such as generating additional textual descriptions or conducting multi-step inference).\n\n2.  The computation of the advantage function is confusing: the right-hand side seems to be defined over entire trajectories, yet the left-hand side, $\\hat{A}^i_t$, is indexed by $t$. This discrepancy makes it unclear whether the advantage is intended to be computed per token or per trajectory. Although *DeepSeekMath*[1] adopts a similar formulation, they also define $\\hat{A}^i_t = \\tilde{r}_i$.\n\n3. Is the hierarchical identity-preserving attention mechanism realized through 3D self-attention?\n\n If the authors can address my concerns, I would consider raising my score.\n\n[1] DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "axIOBHbF2Q", "forum": "wLMEtQ5wBa", "replyto": "wLMEtQ5wBa", "signatures": ["ICLR.cc/2026/Conference/Submission3057/Reviewer_thth"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3057/Reviewer_thth"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3057/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761405954231, "cdate": 1761405954231, "tmdate": 1762916532122, "mdate": 1762916532122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of multi-subject video personalization. The proposed approach incorporates several components, including a hierarchical identity-preserving attention mechanism, VLM-based understanding, and an online reinforcement learning stage."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The method is technically sound and leverages strong foundation models, with modules specifically designed to enhance identity coherence.\n- The experimental results demonstrate satisfactory performance."}, "weaknesses": {"value": "- The paper offers limited novelty, as all three modules—the attention mechanism, VLM encoder, and GPRO post-training—are relatively standard in the field.\n- The use of VLM encoders for improved visual understanding is already common in current image and video foundation models.\n- The GPRO section mostly presents established results with minimal innovation, primarily adding identity loss to an existing framework.\n- Minor: lack related work reference [1].\n\n[1] Wang, Zhao, et al. \"Customvideo: Customizing text-to-video generation with multiple subjects.\" arXiv preprint arXiv:2401.09962 (2024)."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TiIZr21Jez", "forum": "wLMEtQ5wBa", "replyto": "wLMEtQ5wBa", "signatures": ["ICLR.cc/2026/Conference/Submission3057/Reviewer_UssJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3057/Reviewer_UssJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3057/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934330043, "cdate": 1761934330043, "tmdate": 1762916531721, "mdate": 1762916531721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ID-COMPOSER, a multi-subject video generation framework that combines hierarchical identity-preserving attention, a pretrained vision-language encoder, and an online reinforcement learning (Flow-GRPO) phase to enhance semantic alignment and video quality. While the design is conceptually sound and well-written, several technical concerns weaken the contribution’s clarity and empirical depth."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear architecture combining hierarchical attention and VLM-guided semantics.\n2. Comprehensive experiments with both open-source and proprietary baselines.\n3. Demonstrated potential for controllable multi-subject editing."}, "weaknesses": {"value": "1. The introduction of Flow-GRPO lacks a clear motivation beyond “improving video quality.” The paper does not specify the design of the reward components, their relative weights, or how they interact (e.g., identity vs. perceptual quality). Furthermore, there is no ablation study showing the effect of each reward or comparison with standard flow training.\n\n2. The generated videos often exhibit visible copy-paste artifacts, particularly when multiple subjects are composited into dynamic scenes. This likely stems from the lack of cross-pair or mixed-subject training data, which limits the model’s ability to synthesize coherent multi-entity interactions. The data curation pipeline should explicitly include cross-subject compositions or synthetic fusion examples.\n\n3. Several generated results (e.g., the “LeCun” case) show minimal motion and low temporal dynamics. The model tends to produce static or near-still outputs, suggesting that the training data or reward structure encourages conservative predictions."}, "questions": {"value": "As seen in weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yyxMKBmtud", "forum": "wLMEtQ5wBa", "replyto": "wLMEtQ5wBa", "signatures": ["ICLR.cc/2026/Conference/Submission3057/Reviewer_ZFyE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3057/Reviewer_ZFyE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3057/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976049820, "cdate": 1761976049820, "tmdate": 1762916531059, "mdate": 1762916531059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}