{"id": "H3hZPLrs4X", "number": 22103, "cdate": 1758326095710, "mdate": 1759896886303, "content": {"title": "Rethinking Texture Bias in Vision Transformer", "abstract": "Vision Transformer (ViT)-based foundation models have shown impressive performance on broad tasks but struggle in fine-grained applications that depend on local texture. This challenge stems from their lack of inductive biases toward localized visual features, a critical gap for tasks in graphics and vision. To investigate this, we introduce a base-to-novel generalization framework that isolates texture sensitivity while controlling for dataset scale and application-specific constraints. Our analysis reveals that ViTs exhibit a pronounced deficiency in recognizing local textures, while demonstrating a preference for global textures presented at large spatial scales. To understand the origin of this bias, we conduct a systematic study across training, data, and architectural factors, focusing on texture disentanglement, spatial scale sensitivity, and noise robustness. We further employ representational analysis to expose ViTs' limitations in modeling fine-grained texture patterns. Our work provides actionable insights for improving the inductive biases of ViT-based foundation models, informing robust texture representation in graphics applications.", "tldr": "", "keywords": ["Novel View Synthesis", "Dynamic Scene", "Gaussian Splatting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d21f2992373f217b552bf3958672e3f2d2c454c.pdf", "supplementary_material": "/attachment/d0133be6e1cd109d18a0fad15957f04e88ceae12.pdf"}, "replies": [{"content": {"summary": {"value": "This paper studies the texture information learned by the transformer-based vision model, ViT, revealing that ViTs cannot learn local texture patterns well. The texture analysis is based on the widely used local vs global information, the Navon dataset. This paper introduces (line015) a base-to-novel framework which is claimed to isolate texture sensitivity. This base-to-novel (line 185) seems creating a zero-shot setup, classes are divided to training and test sets without overlap. Also, different training objectives and strategies are applied to test how they affect the texture bias.\n\nA list of findings in this draft include: zero-shot setting for ViT-B16 and Resnet-50 to test their their texture biases, b) varying the proportion of the training data and training epochs. c) Tested different training objectives, data augmentation, and ViT or other transformer models. All of the experiments support the finding that ViT lacks of the inductive bias, and the effectiveness of the base-to-novel framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This study focuses on an important task that understanding the mechanism of the transformer model. XAI plays an essential role for any down-stream tasks. This study includes extensive experiments to support their findings."}, "weaknesses": {"value": "There exist several weaknesses in the draft, which can be roughly discussed in three aspects: a) motivation b) presentation and c) experiment design.\n\na) Motivation:\n1. This paper claims their finding throughout the paper, (line 017), \"Our analysis reveals that ViTs exhibit a pronounced deficiency in\nrecognizing local textures\". I wonder why this can be considered a new finding, the texture bias between CNN and transformer seems almost a common sense in the community. Especially, this study referenced a list of solutions to this \"lack of inductive bias problem\"(line 156), then why this problem can be defined as \"revealed\" in this paper?\n\n2.  (line15), \" we introduce a base-to-novel generalization framework\". Why is the zero-shot setup is important in this case? This seems another contribution in this draft, but the motivation behind is not discussed clearly. (line 075) \"To control for dataset scale...\"(line 082), \" This setup better simulates real-world zero-shot scenarios commonly encountered...\" All of these explanations are not clear, IN CONTRAST, I personally believe this is not a correct setting for this texture bias analysis. \n\nAssuming we have a well-trained model, could be any models(ViTs or CNNs), how can we claim the model has learnt more texture or shape biases, could the low performance on the unseen classes due to data shifts? For instance, if a CNN model works worse on a texture test dataset, is it due to the test set is very different from the training data? or we can conclude CNNs rely on texture less? On the opposite, the assumption behind those previous studies is the pre-trained model learns \"some information\", and then test the model on the same class because it is believe to contain the discriminant information for the class, the question is we don't know what the information is. Thus, mixing this zero-shot setting cannot better explain a model.\n\nb) experiment design:\n1.  Follow the base-to-novel setting, Table 1, this study shows ResNet-50 only achieves 7.83% on the base subset of Navon. Following line 224, it is unclear what the secret recipe is behind the ResNet-50 \"whereas ResNet-50 uses a much smaller dataset\", this comparison seems suspicious or unreliable. The low accuracy of ResNet-50 on both \"base\" and \"novel\" indicating the model did not learn at all during training, am I right? This could be the new finding in this draft, which could contradict with the finding in Fig 3 in [1](This reference is missing). I would keep my reasonable doubts on the result, I personally do not believe training this task is that difficult for the CNN model. Moving onto ViT, the base result is a lot higher than the novel, the explanation I can find is (line228), \"This performance collapse highlights ViT’s lack of inductive bias for local texture features\". I am not entirely sure if this setup can prove this claim, is it possible the model overfits on the base training data relying on whatever it uses(texture or global shape), how can rule out the possibility that the model indeed learns texture features but memorizing those training data? This echos the problem, I cannot see how this zero-shot learning can better analyze this task, texture vs. shape.\n\n2. Following the above design, assuming we can draw a conclusion based on the above setup in Section 4. Then the design of Section 5 is trying to disprove the claim in Section 4, line 245, \"Given ViTs’ inherent lack of inductive biases for local texture features, a key question arises: Can this limitation be mitigated through increased data or extended training?\". If this lack of inductive biases is due to not enough training? Then all of the claims previously made could be based on a false experiment setup. For instance, a less texture-biased CNN model can be created when the model is under-fitting, likewise for the transformer model. Again, the assumption behind those previous texture-shape XAI studies is the model is well-trained(e.g., ResNet-50 on ImageNet) and can achieve a decent number on the test set, but we do not know what the model actually learns. If the model behaviour can be changed or \"mitigated\" by training longer, then the intermediate state or the conclusion is not reliable. We can draw completely different conclusion when a model is under-fitting or just one iteration.\n\nc): Presentation\nThis is not a main issue in the draft now, and I might be conservative in naming new things. The name of zero-shot is clear enough and widely used in the community, the new name of \"base-to-novel\" could lower the readability. Further, I cannot see why the metaphor \" Leaning Tower of Pisa experiment\" and \"lead ball\", \"feather\" are helpful, all I can see is a transformer-based and a CNN model. Those models are already well defined and famous in this community, the metaphor may make this draft less clear.\n\nSummary: those key problems exist in this draft now. Further, this study is a replica of empirical experiments of previously used for CNNs. No solid findings can be concluded for now.\n\n\n\n\n\n\n\n\n\n\n\n[1] Hermann, Katherine, and Andrew Lampinen. \"What shapes feature representations? exploring datasets, architectures, and training.\" Advances in Neural Information Processing Systems 33 (2020): 9995-10006."}, "questions": {"value": "N/A, please see the above questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nzVpv3gADA", "forum": "H3hZPLrs4X", "replyto": "H3hZPLrs4X", "signatures": ["ICLR.cc/2026/Conference/Submission22103/Reviewer_sx3v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22103/Reviewer_sx3v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626526973, "cdate": 1761626526973, "tmdate": 1762942066413, "mdate": 1762942066413, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is about shape bias in ViT models as a deficiency in local texture sensitivity, and explores local and global texture/shape sensitivity. The authors employ 3 datasets that evaluate various axes of local-global and shape-texture. They explore the impact of training objective (supervised/self-supervised/generative), data (dataset size, augmentation), and architecture (CNN vs ViT)\n\nThe paper proposes a compelling theory that ViT's shape bias is rooted in a deficiency in local texture sensitivity, with an interesting and unique experimental setup and exciting result with a BigBiGAN training objective. However, the paper is fundamentally undermined by confusing and inconsistent experimental context, metrics, and conclusions in local vs global and shape vs texture. Furthermore, some results overly rely on an overly-simplistic dataset."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors present a thought-provoking approach to explore shape bias as local texture deficiency. Improvement in shape bias in ViT models is typically considered an improvement of model, but not typically thought of in the way posed by the authors. This moves the discussion in the field forward. \n\nThe evaluation of BigBiGAN to understand the effect of generative self-supervision and demonstrating that this dramatically improves shape bias is an exciting result, with implications for pre-training. \n\nThe experimental setup of testing models in base to novel is a compelling demonstration of zero-shot failure and transferability of local feature representation.\n\nThe cutout and cropping experiments are clever and controlled approaches of probing local vs global texture preference/bias."}, "weaknesses": {"value": "Despite multiple readings I am confused about claims regarding ‘shape’ vs ‘texture’, and in global vs local context. For example, the Navon dataset evaluates the ability to detect the identity of small letters that make up larger letters. As a classification problem I believe this tests local shape, not local texture (M and W are the same texture, O/Q and T/L nearly identical) The GST method on the other hand evaluates shape vs texture bias, but shape is global and texture both global and local, not just global. While the imageNet-C dataset is an interesting approach to explore ‘ texture corruptions’, I am unconvinced that this is only local and not global. \n\nThis confusion is made worse by shifting back and forth between Navon and GST datasets, which test different aspects of these biases. \n\nRelatedly, it’s unclear in some figures/tables what is better/worse performance. This confusion is worsened because the authors have a unique take on shape/texture bias, with shape bias usually a human-aligned goal, and here this is presented as a texture bias limitation that would give poor performance. \n\nThe Tower of Pisa experiment analogy is further confusing, especially with describing the CNN as the ‘feather’, which in the non-vacuum experiment falls slower because of air mass. The authors mention the feather, but do the authors mean to compare the two balls of differing mass? The experimental results reported seem to match the two ball analogy. \n\nSome claims surround the Navon dataset, an overly-simplistic, synthetic dataset of binary figures (letters made of smaller letters) that is very out-of-distribution compared to naturalistic images and is in many regards a toy dataset for such an exploration. With such a simplistic dataset, the strong claims could be over-generalized. Standard texture datasets like DTD or rendered graphics datasets would test global/local texture sensitivity of models and be more realistic in addition to the GST conflict dataset which is used.\n\nRelatedly, the authors state that stylized imagenet was not evaluated because ViTs capture global features well. Is this not the main claim that ViTs do capture global features well, but the downside here is lack of local texture sensitivity? Stylized imagenet alongside GST would in my mind be important experiments to include here in Table 1.The authors' stated reason for excluding it appears circular\n\nA claim by the authors is that architecture and patch size is a culprit behind these local texture sensitivity limitations, and explore this is table 6. However, they only demonstrate this for larger patch size, not the likely beneficial direction of 4 or 8. Especially in the context of the Navon dataset, what is the relationship of the token size to the pixel size of the smaller letters? In figure 1, I myself am unable to resolve the smaller ‘local texture’ letters."}, "questions": {"value": "Include a clear table explaining 1) Local vs. global features 2) Shape vs. texture, 3)How these interact (local texture, local shape, global texture, global shape) 4) Which of these is addressed by each dataset/architectural modification would strengthen the paper and also make it more accessible to those outside this subfield. \n\nThe logic of the Tower of Pisa experiment is confusing with the feather/ball/heavy-ball. Please explain this earlier when it’s introduced and make this analogy more clear. i think what you want here is not an analogy to feather/ball but the heavy/light ball. \n\nPlease include an analysis of stylized Imagenet alongside GST in Table 1, or improve the explanation as to why you do not. The logic is circular or at best unclear as to why only Navon is used here. \n\nI recommend including analysis of standard texture datasets like DTD or a rendered graphics dataset to have a more realistic dataset alongside Navon.\n\nWhat is the relationship of the token size to the pixel size of the smaller letters in Navon? I would recommend analyzing a ViT with token size of 4 or 8, to see if this improves performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jVrlTX17gZ", "forum": "H3hZPLrs4X", "replyto": "H3hZPLrs4X", "signatures": ["ICLR.cc/2026/Conference/Submission22103/Reviewer_kvdA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22103/Reviewer_kvdA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752848566, "cdate": 1761752848566, "tmdate": 1762942066119, "mdate": 1762942066119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework that isolates texture sensitivity while controlling for dataset scale and task constraints. Using Navon stimuli—where global shapes (large letters) and local textures (small-letter repetitions) are disentangled—the authors show that Vision Transformers (ViTs) consistently under-recognize local textures and favor global ones. A zero-shot comparison between ViT-B/16 and ResNet-50 further highlights ViTs’ weaker inductive bias toward fine-grained texture.\n\nThe study then examines how training objective, dataset size, training length, and data augmentation modulate this bias. Complementary representational analyses track how spatial locality and semantic abstraction evolve across ViT layers, confirming limited capacity for fine-scale texture modeling. Together, these results provide a benchmark and guidance for injecting stronger texture-aware inductive biases into future ViT-based foundation models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Proposes a evaluation protocol that disentangles texture bias from dataset scale and class overlap.\n\nIntroduces the Leaning-Tower-of-Pisa thought experiment for a controlled ViT-vs-CNN zero-shot comparison, making the inductive-bias gap immediately interpretable.\n\nExtends texture-bias studies to the Navon domain, enabling explicit separation of local-vs-global texture cues.\n\nCareful controls (same zero-shot setting, class partitioning, balanced datasets) minimize confounds and strengthen causal claims about texture bias.\n\nReveals a fundamental limitation of current ViTs—their weak inductive bias for fine-grained textures—which has implications for robustness, zero-shot transfer, and downstream tasks such as fine-grained recognition or graphics.\n\nProvides guidance: self-supervised generative pre-training and specific augmentation choices can partially mitigate the bias, informing practitioners designing next-generation foundation models."}, "weaknesses": {"value": "Limited engagement with recent work that revisits the texture-bias narrative. In particular, the paper does not discuss “ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression,” which exposes methodological limitations in the Geirhos et al. \"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\" cue-conflict paradigm and finds no intrinsic texture bias in CNNs, but rather a reliance on local shape cues. Situating the present results with respect to these findings would strengthen the contribution.\n\nThe distinction between “local texture” and “local shape” is not sufficiently clear. The Navon stimuli arguably contrast global shape with local shape, not texture per se, like in “ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression,”  raising questions about whether the observed effects truly reflect texture preference. A crisper definition and justification of the texture–shape axis in this context is needed for accurate interpretation."}, "questions": {"value": "Coverage of recent foundation models\n• Could you provide results—or at least a rationale for their omission—for widely-used pre-trained ViT variants such as DINOv2, SAM, CLIP, MAE, Stable Diffusion, RADIO, and FRANCA? Including even a subset of these models would greatly strengthen the empirical scope.\n\nArchitectural diversity\n• Have you considered evaluating diffusion-based transformer backbones such as DiT? Their training dynamics and inductive biases might differ from standard ViTs and could inform the generality of your conclusions.\n\nLocal texture vs. local shape\n• The Navon stimuli may be interpreted as contrasting global shape with local shape rather than texture. Can you clarify this distinction and explain how your metrics isolate “texture” per se?\n• Relatedly, how do your findings relate to the recent paper “ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression,” which argues for a local-shape reliance instead of texture bias? A direct discussion—or an experiment using their suppression framework—would help reconcile the two perspectives."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WBNzeL8xXi", "forum": "H3hZPLrs4X", "replyto": "H3hZPLrs4X", "signatures": ["ICLR.cc/2026/Conference/Submission22103/Reviewer_cJaT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22103/Reviewer_cJaT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892434734, "cdate": 1761892434734, "tmdate": 1762942065891, "mdate": 1762942065891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper systematically explored texture bias in Vision Transformers (ViTs), focusing on their local texture limitations and mitigation strategies. ViT-based models exceled at global semantic tasks but struggled in fine-grained applications (e.g., material classification) relying on local textures, as they lack CNNs’ translation equivariance and locality bias—key for capturing fine-grained texture features. To study this, the authors proposed a base-to-novel generalization framework and use three data sets. They also designed the \"Leaning Tower of Pisa experiment\" comparing ViT-B/16 and ResNet-50 in zero-shot settings, finding ViT-B/16 (71.20% base accuracy on Navon) collapses to ~8.3% novel accuracy—matching ResNet-50, confirming ViTs’ lack of local texture bias."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A key original contribution is the base-to-novel generalization framework, which isolates texture sensitivity by training on \"base\" classes and testing on unseen \"novel\" classes—effectively controlling for data set scale and class overlap that often confound comparisons of texture bias. Complementing this, the Leaning Tower of Pisa experiment offered a novel, interpretable analogy to contrast ViT-B/16 (a large pre-trained model) and ResNet-50 (a smaller CNN) in zero-shot settings, empirically validating ViTs’ lack of local texture inductive bias rather than relying on theoretical speculation. The paper also broke new ground by disentangling \"global vs. local texture bias\"—moving beyond binary \"texture vs. shape bias\" frameworks—and systematically linking this distinction to training objectives, data augmentation, and architectural scale, a nuance rarely explored in prior ViT or CNN bias research.\n2. Methodologically, it used three specialized, well-justified data sets (GST for global texture, Navon for local/global disentanglement, ImageNet-C for robustness testing) to isolate specific aspects of texture perception, avoiding the ambiguity of generic data sets like ImageNet. Experiments were rigorously controlled: for example, the base-to-novel split of the Navon data set ensured consistent evaluation of zero-shot generalization, while cross-validation (e.g., 5-fold for data efficiency tests) and statistical significance checked (noted for augmentation results) strengthen reliability. Analytical depth was evident in layer-wise representation studies—using linear classifiers to decode shape/texture from frozen ViT layers—and quantitative comparisons (e.g., Table 2 showing BigBiGAN’ s 85.4% shape bias vs. 48.2% for supervised training) that ground claimed in measurable results. The authors also acknowledged limitations (e.g., ViTs’ overfitting on ImageNet-C) and avoided overgeneralization, further enhancing the work’ s quality.\nIts organization followed a logical arc: problem statement (ViTs’ local texture limitations), methods (frameworks, datasets, experiments), results (key findings on bias), and implications (contributions to model design)—with clear section headings (e.g., \"4 LACK OF INDUCTIVE BIAS FOR LOCAL TEXTURE FEATURES IN VITS\") guiding readers. \n3. Theoretically, it advanced understanding of ViT inductive biases by identifying \"global texture preference\" as a defining trait, challenging the narrative that ViTs are simply \"shape-biased\" or \"texture-biased.\" It also bridged ViT and CNN research by highlighting how architectural differences (e.g., lack of translation equivariance in ViTs) mediated responses to training strategies (e.g., data augmentation), filling a gap in cross-architecture bias comparisons. Practically, the findings offered actionable guidance for improving ViTs in texture-reliant fields like computer graphics and fine-grained recognition: self-supervised training (e.g., BigBiGAN) to enhance local texture capture, random cropping to reduce global bias, and leveraging intermediate ViT layers for fine-grained features."}, "weaknesses": {"value": "1. Since the paper focused on controlled, synthetic data sets (GST, Navon, ImageNet-C), the translatability of its findings was limited to the real-world fine-grained texture tasks. While the Navon data set effectively disentangled local and global features through artificial letter hierarchies, it failed to capture the unstructured, natural local textures of real materials (e.g., fabric weaves, metal patinas) or fine-grained objects (e.g., petal veins in flowers, small parts in aircraft). ImageNet-C tested robustness to generic corruptions (e.g., blur, noise) but not the natural variability of local textures in real scenarios (e.g., lighting changes on wood, wear patterns on fabric), leaving uncertainty about whether the identified local texture deficit persists in practical use cases. This gap weakens the paper’s ability to guide improvements for the very applications it highlights as critical for ViT-based models.\n2. The paper’s analysis of architectural variations in ViTs was overly narrow, focusing primarily on standard ViT variants (ViT-B/16, ViT-B/32) and a small set of ViT derivatives (Swin-Tiny, PVT-Tiny) while neglecting architectures explicitly designed to enhance local feature capture. The authors did not evaluate how modifications like those in ConvNeXt (which merges CNN locality with ViT attention) or LocalViT (which adds local attention windows) affect texture bias, even though these models were engineered to address local feature limitations. Additionally, while the paper noted that larger ViTs favor global textures, it did not decompose the impact of specific architectural components (e.g., patch size, attention window size) to identify which factors drive local texture sensitivity, leaving researchers without clear direction for modifying ViT architectures to mitigate the local texture deficit.\n3. The paper failed to address the critical trade-off between improving local texture recognition and preserving ViTs’ strength in global semantic tasks, a key concern for foundation models that need to excel across multiple domains. For instance, it showed that random cropping shifts ViTs toward local features but did not measure whether this shift degrades performance on global tasks like ImageNet classification or scene segmentation. Similarly, while self-supervised training (e.g., BigBiGAN) was shown to enhance local texture recognition, there was no analysis of whether it harms performance on global tasks such as COCO object detection. This omission suggests that the paper cannot confirm if its proposed interventions for local texture improvement are viable for multi-task foundation models, as it did not validate that they do not undermine the global semantic capabilities ViTs are valued for."}, "questions": {"value": "1. ImageNet-C evaluated robustness to corruptions (e.g., blur), but real-world texture variability (e.g., lighting changes on wood, wear on fabric) differs from artificial corruption. Do the authors have preliminary data on how ViTs perform on data sets with natural texture variability, and does self-supervised training (e.g., BigBiGAN) mitigate performance drops in these scenarios?\n2. Have the authors tested their base-to-novel generalization framework on a real-world fine-grained texture data set (e.g., MINC-2500 for materials, Oxford Flowers 102 for fine-grained objects)? If not, do you hypothesize that ViTs’ local texture deficit (observed on Navon) would persist when local features are natural (e.g., fabric weaves, petal veins) rather than synthetic (Navon’s letter hierarchies)?\n3. Have the authors evaluated ViT variants engineered to enhance local features (e.g., ConvNeXt, LocalViT, or ViT-LocalAttention) using their base-to-novel framework? Specifically, do these architectures— which merge CNN locality with ViT attention—reduce the local texture deficit you identify in standard ViTs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1fWht7X9zt", "forum": "H3hZPLrs4X", "replyto": "H3hZPLrs4X", "signatures": ["ICLR.cc/2026/Conference/Submission22103/Reviewer_Fu1h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22103/Reviewer_Fu1h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22103/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929369324, "cdate": 1761929369324, "tmdate": 1762942065711, "mdate": 1762942065711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}