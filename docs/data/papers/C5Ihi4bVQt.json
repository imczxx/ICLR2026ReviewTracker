{"id": "C5Ihi4bVQt", "number": 14515, "cdate": 1758237768698, "mdate": 1759897365634, "content": {"title": "LLMS ON TRIAL: Evaluating Judicial Fairness For Large Language Models", "abstract": "Large Language Models (LLMs) are increasingly used in high-stakes fields, such as law, where their decisions can directly impact people's lives. When LLMs act as judges, the ability to fairly resolve judicial issues is necessary to ensure their trustworthiness. Based on theories of judicial fairness, we construct a comprehensive framework to measure LLM fairness, leading to a selection of 65 labels and 161 corresponding values. We further compile an extensive dataset, JudiFair, comprising 177,100 unique case facts. To achieve robust statistical inference, we develop three evaluation metrics—inconsistency, bias, and imbalanced inaccuracy—and introduce a method to assess the overall fairness of multiple LLMs across various labels. Through experiments with 16 LLMs, we uncover pervasive inconsistency, bias, and imbalanced inaccuracy across models, underscoring severe LLM judicial unfairness. \nParticularly, LLMs display notably more pronounced biases on demographic labels, with slightly less bias on substance labels compared to procedure ones. Interestingly, increased inconsistency correlates with reduced biases, but more accurate predictions exacerbate biases. While we find that adjusting the temperature parameter can influence LLM fairness, model size, release date, and country of origin do not exhibit significant effects on judicial fairness. Accordingly, we introduce a publicly available toolkit to support future research in evaluating and improving LLM fairness, along with a full technical analysis included as an appendix.", "tldr": "Applying a comprehensive judicial fairness framework to a new extensive dataset, this study reveals severe and pervasive unfairness across 16 large language models and introduces a toolkit designed to evaluate and improve their fairness.", "keywords": ["Fairness", "LLM-as-judge"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7b7bd9d52a7de0ece0595f5883d0ec4d6303e99.pdf", "supplementary_material": "/attachment/68fb0ff60902edd96a843be7ce96a7f89feb0fbd.zip"}, "replies": [{"content": {"summary": {"value": "This work investigates biases in large language models (LLMs) when making sentencing judgments in a judicial context. The authors analyze two types of factors influencing judicial decisions: (1) substantive factors, which relate directly to the crime itself, and (2) procedural factors, which involve contextual or judge-related elements not directly tied to the offense. Using the large-scale LEEC judicial dataset, the authors augment existing data with additional annotations and employ an automated pipeline to identify \"trigger sentences\", which are text segments containing explicit or implicit references to sensitive factors. Counterfactual examples are generated by systematically replacing these triggers, and the resulting dataset is validated by domain experts.\n\nLLM behavior is then evaluated along three axes:\n1. Inconsistency – whether differences in input labels lead to inconsistent sentencing outcomes.\n2. Bias – the presence of systematic inconsistencies across specific factors.\n3. Imbalanced inaccuracy – the divergence between LLM predictions and real-world judicial decisions.\n\nOverall, the study provides a structured framework for disentangling and quantifying different forms of bias in LLM-based legal decision-making."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Data contribution and methodology. The paper provides a practical dataset and annotation framework that can be adapted to study judicial biases in other jurisdictions or legal systems.\n\n* Reproducibility. The authors open-source their codebase, making it a potentially valuable resource for follow-up work in computational law and fairness research."}, "weaknesses": {"value": "* Overstated novelty: \n\n1. The claimed contributions appear incremental rather than foundational. The distinction between substantive and procedural factors was already present in LEEC; while the authors expand the label set / include counterfactual samples, in my opinion, this does not constitute a “comprehensive systematic framework” as claimed.\n2. The JudiFair dataset is essentially an extension of LEEC, with more fine-grained labels and counterfactual augmentations, rather than a new dataset.\n3. The proposed evaluation metrics (inconsistency, bias, imbalanced inaccuracy) are largely intuitive and draw from existing statistical and fairness measures. Their framing as a “novel methodology” feels overstated. The authors should better articulate what conceptual or methodological novelty exists beyond metric adaptation.\n\n* Motivation and real-world grounding: While the paper cites prior work using LLMs in legal settings (lines 101–102), it does not clarify whether LLMs are currently deployed for sentencing or judicial decision-making. Without evidence of real-world use or imminent deployment, the motivation for studying bias in this specific context feels underdeveloped.\n\n* Lack of quantitative validation: The paper provides no quantitative results in the main body to substantiate claims about bias, consistency, or fairness. Including such results in the main body is essential to evaluate the effectiveness and reliability of the proposed framework.\n\n* Minor Comments and Suggestions\n1. The term “imbalanced inaccuracy” is confusing due to its double negative; consider renaming it to something more readable (e.g., “asymmetric error”).\n2. The introduction of sentence replacement (Section 4.2.1) precedes the formal introduction of counterfactual generation (4.2.2), which disrupts logical flow.\n3. The authors should specify who the expert annotators are (e.g., legal scholars, practitioners, or domain experts).\n4. In lines 35–36, the phrase “fairness in general-domain benchmarks” is unclear—please clarify what benchmarks or fairness settings are being referenced."}, "questions": {"value": "1. Could you clarify how JudiFair differs substantively from LEEC beyond additional labels and counterfactual augmentations?\n\n2. Who are the “experts” validating the annotations? What measures were taken to ensure inter-annotator reliability or consistency across experts?\n\n3. Are LLMs currently being used, or seriously proposed, for judicial sentencing or related decision-making tasks? If not, what is the practical or ethical motivation for modeling such scenarios in this work?\n\n4. Could you justify why the three metrics (inconsistency, bias, imbalanced inaccuracy) should be considered novel rather than adapted from existing fairness measures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s2AxG5pfKw", "forum": "C5Ihi4bVQt", "replyto": "C5Ihi4bVQt", "signatures": ["ICLR.cc/2026/Conference/Submission14515/Reviewer_SDva"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14515/Reviewer_SDva"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761870038384, "cdate": 1761870038384, "tmdate": 1762924909893, "mdate": 1762924909893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a comprehensive framework for evaluating judicial fairness in Large Language Models (LLMs) used in high-stakes legal contexts. Drawing on theories from law and philosophy, the authors propose a systematic evaluation methodology based on three fairness metrics, inconsistency, bias, and imbalanced inaccuracy, and implement it through a new benchmark dataset, JudiFair, containing 177,100 unique case facts annotated with 65 labels and 161 values. Using this framework, the study assesses 16 LLMs and uncovers pervasive judicial unfairness, revealing that models exhibit significant biases, particularly along demographic labels, with slightly less bias on substance labels compared to procedure ones. Notably, higher inconsistency is associated with reduced bias, while greater predictive accuracy tends to amplify bias. Moreover, findings further show that temperature adjustments can influence LLM fairness, whereas model size, release date, and country of origin do not significantly affect judicial fairness. The paper contributes a publicly available toolkit and dataset to facilitate future research in evaluating and improving LLM fairness in judicial AI systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper presents a systematic framework to evaluate judicial fairness in large language models, introducing a novel benchmark dataset, JudiFair, encompassing 177,100 unique case facts annotated with 65 labels and 161 label values.\n2.\tIt formulates three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy and proposes a robust statistical inference methodology to assess overall fairness across multiple LLMs and various labels.\n3.\tIt conducts comprehensive experiments on 16 LLMs originating from different countries, applying statistical inference to reveal pervasive inconsistency, bias, and imbalanced inaccuracy, highlighting the severe issue of judicial unfairness in LLMs."}, "weaknesses": {"value": "1.\tThe generalizability is limited by focusing exclusively on Chinese criminal law; while the authors claim the framework is transferable, cultural and legal system differences may significantly affect findings in other jurisdictions.\n2.\tIt lacks ample theoretical discussion on fairness in law and philosophy as claimed in the paper.\n3.\tThe paper does not clearly explain how \"effective sample size\" for weighting is calculated, making the implementation details insufficient.\n4.\tThe paper lacks concrete analysis of why these biases emerge and provides no debiasing strategies or interventions, limiting its practical utility.\n5.\tThe related work section does not discuss several recent and relevant fairness evaluation studies, check \"Fairness Definitions in Language Models Explained\"."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EUQ1npfYVc", "forum": "C5Ihi4bVQt", "replyto": "C5Ihi4bVQt", "signatures": ["ICLR.cc/2026/Conference/Submission14515/Reviewer_9TXq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14515/Reviewer_9TXq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922356020, "cdate": 1761922356020, "tmdate": 1762924908899, "mdate": 1762924908899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates judicial fairness of 16 LLMs using a comprehensive framework that distinguishes substance vs. procedure factors and demographic vs. non-demographic factors. The authors construct JudiFair, a dataset with 177,100 counterfactual variations from ~1,100 Chinese criminal cases across 65 labels (161 values), and measure three dimensions: inconsistency, bias, and imbalanced inaccuracy. Key findings include pervasive unfairness across all models, stronger biases on demographic and procedural labels, and correlations suggesting that higher accuracy and lower inconsistency associate with more detectable bias.\n\nNovelty. The procedural fairness framework is genuinely novel for LLM evaluation, and the scale (65 labels vs. 9 in prior work like BBQ [1]) represents a substantial advance. However, the \"counterintuitive\" correlations are predictable from existing fairness literature [2,3], and the lack of intersectional analysis [4,5] represents a methodological gap.\n\nSignificance. Highly relevant given evidence of real-world LLM deployment in Chinese courts [6], and the procedural fairness insights offer new evaluation dimensions. However, single-axis analysis limits actionability, no mitigation strategies are proposed, and generalizability beyond Chinese criminal law is uncertain.\n\n[1] Parrish et al. (2022). BBQ: A hand-built bias benchmark for question answering. ACL Findings.\n\n[2] Chouldechova (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big Data, 5(2).\n\n[3] Kleinberg et al. (2017). Inherent trade-offs in the fair determination of risk scores. ITCS.\n\n[4] Buolamwini & Gebru (2018). Gender shades: Intersectional accuracy disparities in commercial gender classification. PMLR 81:77-91.\n\n[5] Foulds et al. (2020). An intersectional definition of fairness. IEEE FODS.\n\n[6] Liu & Li (2024). How do judges use large language models? Evidence from Shenzhen. Journal of Legal Analysis, 16(1).\n\n[7] Rawls (1971). A Theory of Justice. Harvard University Press.\n\n[8] Waldron (2011). The rule of law and the importance of procedure. NYU School of Law.\n\n[9] Blodgett et al. (2021). Stereotyping Norwegian salmon: An inventory of pitfalls in fairness benchmark datasets. ACL.\n\n[10] Moore et al. (2024). Reasoning beyond bias: A study on counterfactual prompting and chain of thought reasoning. arXiv:2408.08651."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Procedural fairness framework: First systematic evaluation of how procedural factors (court level, trial broadcast, litigation duration) affect LLM judicial decisions, grounded in legal theory [7,8]\n- Comprehensive label system: 65 labels across four categories (substance/procedure × demographic/non-demographic) represents 7× expansion over prior work [1]\n-  Bernoulli tests for aggregate significance, fixed-effects regression with cluster-robust standard errors, and five robustness checks exceed typical LLM fairness papers\n- Public toolkit (!) (JustEva): Lowers barriers for future research and addresses critiques about practical usability [9]\n- Scale of evaluation: 16 models across different countries, sizes, and release dates with systematic comparison\n- Valuable null results: Model size, release date, and country of origin don't predict fairness, informing development priorities\n- Generalizable methodology: Framework adaptable to other legal systems despite Chinese criminal law focus"}, "weaknesses": {"value": "- Misleading scale framing: \"177,100 unique case facts\" refers to counterfactual variations of ~1,100 base documents, not distinct cases. This should be stated more transparently upfront\n- No intersectionality analysis: Single-axis testing misses compound marginalization (e.g., gender × ethnicity interactions), a well-established concern in fairness research [4,5]\n- Oversold \"counterintuitive\" findings: The inconsistency-bias negative correlation is a statistical artifact (noise obscures systematic patterns), and the accuracy-bias positive correlation is the well-documented fairness-accuracy tradeoff [2,3], not surprising discoveries\n- Counterfactual method: Builds incrementally on APriCot [10], though scale and domain application add value\n- Limited actionability: only temperature adjustment is tested?\n- Ecological validity: prompting LLMs for direct sentencing predictions may not reflect actual deployment scenarios in legal assistance systems"}, "questions": {"value": "1) Intersectionality: Could you add interaction terms (e.g., Gender × Ethnicity, Age × Socioeconomic Status) to examine compound marginalization effects? This would significantly strengthen the policy relevance.\n2) Mechanism vs. artifact: The inconsistency-bias negative correlation appears to be a statistical power issue (noise reduces detectability). Can you clarify whether this represents a substantive finding or measurement artifact?\n3) Fairness-accuracy tradeoff: How do your findings relate to existing impossibility results [2,3]? The positive correlation between accuracy and bias seems expected when models learn from biased training data.\n4) Scale transparency: Please clarify early in the paper that 177,100 refers to counterfactual variations from ~1,100 base documents, not distinct criminal cases.\n5) Comparison with human judges: Given LEEC contains real judicial outcomes, can you compare LLM biases with human judge biases to contextualize whether LLMs are more or less fair than current practice?\n6) Mitigation strategies: Have you explored any debiasing approach beyond temperature adjustment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ftK4TnMYcA", "forum": "C5Ihi4bVQt", "replyto": "C5Ihi4bVQt", "signatures": ["ICLR.cc/2026/Conference/Submission14515/Reviewer_YDQB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14515/Reviewer_YDQB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762198657816, "cdate": 1762198657816, "tmdate": 1762924908156, "mdate": 1762924908156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}