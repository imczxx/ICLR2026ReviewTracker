{"id": "oa7L4vcJ77", "number": 14821, "cdate": 1758244361380, "mdate": 1763587674853, "content": {"title": "Fast Estimation of Wasserstein Distances via Regression on Sliced Wasserstein Distances", "abstract": "We address the problem of efficiently computing Wasserstein distances for multiple pairs of distributions drawn from a meta-distribution. To this end, we propose a fast estimation method based on regressing Wasserstein distance on sliced Wasserstein (SW) distances. Specifically, we leverage both standard SW distances, which provide lower bounds, and lifted SW distances, which provide upper bounds, as predictors of the true Wasserstein distance. To ensure parsimony, we introduce two linear models: an unconstrained model with a closed-form least-squares solution, and a constrained model that uses only half as many parameters. We show that accurate models can be learned from a small number of distribution pairs. Once estimated, the model can predict the Wasserstein distance for any pair of distributions via a linear combination of SW distances, making it highly efficient.  Empirically, we validate our approach on diverse tasks, including Gaussian mixture, point-cloud classification, and Wasserstein-space visualization for 3D point clouds. Across various datasets such as MNIST point clouds, ShapeNetV2, MERFISH Cell Niches, and scRNA-seq, our method consistently provides a better approximation of Wasserstein distance than the state-of-the-art Wasserstein embedding model, Wasserstein Wormhole, particularly in low-data regimes. Finally, we demonstrate that our estimator can also accelerate Wormhole training, yielding \\textit{RG-Wormhole}.", "tldr": "We propose a fast method for estimating Wasserstein distances across multiple pairs of distributions by formulating a regression problem, using variants of sliced Wasserstein distances as predictors.", "keywords": ["Optimal Transport", "Wasserstein distance", "Sliced Wasserstein distance", "Regression"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f489b5600c8aa9c625f8c243e010274777868d2d.pdf", "supplementary_material": "/attachment/f3fc54969e394e4f112d8e6f86b366b2f27e204a.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposed a method to estimate the Wasserstein distance efficiently. Specifically, the proposed method utilizes the fact that the sliced Wasserstein distance is the lower bound of the Wasserstein distance, and the lifted sliced Wasserstein distance is the upper bound of the Wasserstein distance. Then, the proposed method is the weighted average of the sliced Wasserstein distance and the lifted Wasserstein distance and provides a better estimation of the Wasserstein distance by learning these weights."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper is well-written and easy to follow.\n* The proposed method is simple, and it is intuitively correct that the proposed method is a better estimation of the Wasserstein distance than the sliced Wasserstein distance and the lifted Wasserstein distance."}, "weaknesses": {"value": "* The proposed method is incremental and trivial. It is just a linear combination of the sliced Wasserstein distance and the lifted sliced Wasserstein distance.\n* The experimental results are not comprehensive. The main motivation of the proposed method is to efficiently estimate the Wasserstein distance. However, this paper did not evaluate and discuss this efficiency, e.g., the time required to compute, the time required for training.\n* There are a lot of papers that proposed the efficient approximation of the Wasserstein distance, but this paper did not compare the proposed method with them in the experiments. Specifically, it is at least necessary to compare the proposed method with the sliced Wasserstein distance, the lifted sliced Wasserstein distance, the max-sliced Wasserstein distance, and other types of approximation of the Wasserstein distance."}, "questions": {"value": "See the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fX5Ac9mTD9", "forum": "oa7L4vcJ77", "replyto": "oa7L4vcJ77", "signatures": ["ICLR.cc/2026/Conference/Submission14821/Reviewer_A3f7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14821/Reviewer_A3f7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761701253734, "cdate": 1761701253734, "tmdate": 1762925170340, "mdate": 1762925170340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In many modern machine learning problems, the datasets are collections of distributions, i.e. a meta-distribution over distribution. In those cases, the Wasserstein distance (WD) is an alluring metric for analysis, but constitutes and significant bottleneck in applications that require many pairwise comparisons. This paper proposes a method to estimate the WD by learning a mapping from computationally cheap Sliced Wasserstein (SW) variants to the true WD. The learning process relies on a small number of \"ground-truth\" WD calculations to fit the estimator, which can then be applied to new pairs.\n\nThe proposed method regresses the true WD onto a set of its sliced variants. The predictors include SW-based distances that are known lower bounds (e.g., standard SW, Max-SW) and \"lifted\" SW distances that serve as upper bounds (e.g., Projected Wasserstein, Min-SWGG). The paper details two linear models for this regression: an unconstrained model with a closed-form least-squares solution, and a constrained model. The constrained model pairs lower and upper bounds, reducing the number of parameters and adding an inductive bias, which is intended for \"few-shot\" learning scenarios with limited ground-truth samples.\n\nThis regression estimator (dubbed \"RG\") is evaluated in two applications. First, as a standalone estimator, it is applied to a k-NN classification task, where its accuracy is reported to be close to that of the exact WD. The paper also shows it achieves higher accuracy than the Wasserstein Wormhole model in low-data regimes. Second, the authors propose \"RG-Wormhole,\" a hybrid that replaces the expensive WD calculations in the Wormhole training loop with the fast RG estimate. This substitution is shown to reduce training time substantially while reportedly maintaining comparable performance on tasks like point cloud reconstruction and interpolation"}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method is elegant, principled, and provides a practical way to \"bootstrap\" a collection of cheap estimates to learn a high-quality one. The approach is clearly effective, as demonstrated by extensive comparisons and benchmarks across multiple datasets and tasks (k-NN, embedding, reconstruction). Furthermore, the data efficiency is a major plus as the ability to learn a good regression model from very few \"ground-truth\" Wasserstein distance calculations makes this highly applicable. Finally, the application within the \"RG-Wormhole\" model is a strong contribution, as it directly addresses the primary computational bottleneck of a current method."}, "weaknesses": {"value": "The presentation of \"Propositions\" 1 and 2 (Section 3.2, Page 6) is a notable weakness. These appear to be standard, well-known closed-form solutions for unconstrained and constrained linear regression, respectively. Presenting these basic results as \"propositions\" with proofs in the appendix feels unnecessary and could be interpreted as an attempt to add theoretical weight where none is needed. An algorithmic and empirical paper like this does not require such justifications for using standard linear regression."}, "questions": {"value": "The paper highlights the \"explainable\" nature of linear regression as a reason for its use. Given this, have the authors analyzed the learned coefficients ($\\omega$)? Is there a discernible pattern in the learned weights, such as a consistent difference in those assigned to lower bounds versus upper bounds? Similarly, in the constrained model, is there a consistent pattern in the weights for paired \"lifted\" (upper) and \"unlifted\" (lower) SW distances across different datasets or dimensions (e.g., does the model learn to trust one more than the other)? The Gaussian simulation in the appendix (Fig 4) hints at this, but a broader analysis on the real datasets would be valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SH2HmqICkl", "forum": "oa7L4vcJ77", "replyto": "oa7L4vcJ77", "signatures": ["ICLR.cc/2026/Conference/Submission14821/Reviewer_txMr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14821/Reviewer_txMr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932646577, "cdate": 1761932646577, "tmdate": 1762925169883, "mdate": 1762925169883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper's central hypothesis is that for a given meta-distribution $\\mathbb{P}(\\mu, \\nu)$, the true $W_p$ can be accurately and efficiently modeled as a simple linear function of various sliced Wasserstein distances. The core technical insight is to use as predictors a vector of SW based metrics that includes both established lower bounds (e.g., SW, Max-SW, EBSW) and upper bounds (e.g., PW, Min-SWGG, EST).\n\nThe authors claim the weights $\\omega$ for this regression $W_p \\approx \\omega^\\top S_p(\\mu, \\nu)$ are a stable characteristic of $\\mathbb{P}$ and can be learned in a few-shot setting by computing a small number of ground-truth $W_p$ pairs. This yields the RG surrogate which is claimed to be far more accurate than any single SW baseline and outperform deep learning estimators (like Wasserstein Wormhole) in low-data regimes."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This is a nicely presented paper. The authors' central (and successful) bet is that for a given problem domain $\\mathbb{P}$, $W_p$ doesn't just lie somewhere in this interval, but at a consistent relative position that a simple linear model can capture. Thus moves beyond using SW as a fast proxy and instead models the approximation error by bracketing the true value.\n\n- The paper's primary claim of data-efficiency is validated with solid empirical evidence. Table 2 for example demonstrates that with only 100 training pairs, the SOTA deep baseline (Wormhole) is completely unusable (e.g., $R^2$ of 0.65 on ShapeNetV2, -3.6 on MERFISH, 0.04 on scRNA-seq). In stark contrast, the proposed unconstrained RG variants achieve $R^2$ scores of 0.93-0.99 across all datasets, even in the 2,500-dimensional scRNA-seq domain. This high correlation translates directly to downstream utility. For example in the k-NN classification task (Table 1), the RG-seo surrogate (83.5\\% accuracy @ k=5) performs almost identically to the true, expensive $W_p$ (84.2\\% accuracy @ k=5), while the best single SW baseline is not competitive.\n\n- By replacing $W_p$ inside the Wormhole training loop the authors achieve training speedup. Figure 14 shows almost flat scaling of compute with batch size for RG-Wormhole compared to Wormhole’s quadratic growth, showing that the surrogate can be dropped into OT-based architectures without much performance loss."}, "weaknesses": {"value": "- The entire method relies on a bootstrap phase where $M$ ground-truth $W_p$ pairs are computed to fit the regression weights $\\omega$. The paper frames this as a minor setup cost because $M_0=10$ (yielding $M = \\frac{10 \\times 9}{2} = 45$ pairs) works for their experiments. However, the paper seems to provide no analysis on how to determine the sufficient $M$ for a new problem.\n- OLS is only optimal if the error term $\\epsilon$ has constant variance. However figures 6-13 suggests otherwise. Would a simple weighted least-squares (WLS) model yield more accurate surrogate?\n- Def 3 adds inductive bias and has half of the parameters, which is often helpful when having limited observed samples. In Table 2 (the $M_0=100$ low-data setting), the unconstrained model is superior in nearly every case (e.g., RG-seo on ShapeNetV2: 0.95 $R^2$ unconstrained vs. 0.92 constrained). This holds even for data size ($M_0=10$) in the appendix (e.g., Fig 8, ShapeNetV2: RG-seo 0.93 unconstrained vs. 0.90 constrained). The empirical justification for the constrained model seems week here."}, "questions": {"value": "- The RG-seo model requires computing 6 SW-based predictors, some of which (Max-SW, Min-SWGG) require their own iterative optimization. The paper's time complexity analysis and RG-Wormhole training plot (Fig 14) do not provide a direct inference-time comparison. What is the wall-clock time (for a single pair) comparing RG-seo (at inference) to a standard, converged entropic-regularized $W_p$ (Sinkhorn) approximation?\n- How sensitive is the learned regressor to the choice and number of slicing directions (L) and to the specific mix of lower vs upper SW variants?\n- What if we train an RG model only on intra-class pairs (e.g., (chair, chair)) and test its $R^2$ on inter-class pairs (e.g., (chair, airplane))? vice versa?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gNU6N1ZmjN", "forum": "oa7L4vcJ77", "replyto": "oa7L4vcJ77", "signatures": ["ICLR.cc/2026/Conference/Submission14821/Reviewer_AafT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14821/Reviewer_AafT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953053090, "cdate": 1761953053090, "tmdate": 1762925169386, "mdate": 1762925169386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Rebuttal (1)"}, "comment": {"value": "We sincerely thank the reviewers for their time and constructive feedback. In this global rebuttal, we summarize the common points of discussion and present new experiments. Detailed responses to individual comments can be found in the respective threads for each reviewer.\n\n**GQ1** Relative literature position of the proposed regression approach to Sinkhorn optimal transport, linear optimal transport [2], Wasserstein Wormhole, and sliced optimal transport metrics.\n\n\n**QA1** We would like to classify the computation of Wasserstein distance into two types: single-pair setting and multiple-pairs setting.\n\n\nFor the single-pair setting, **regularized optimal transport** is the most used approach that genuinely *approximates* the original optimal transport problem and thus yields an **approximation** to the Wasserstein distance. In contrast, **sliced optimal transport (SOT)** metrics provide **alternative** efficient distances. They are *not* approximations of the Wasserstein distance and are not generally equivalent to it [1]. Although many SOT variants exist, none are designed to **approximate** the Wasserstein distance; most yield either lower or upper bounds, as discussed in the paper.\n\nFor the multiple-pairs setting (e.g., $N$ pairs of distributions, pairwise distances of $N$ distributions), **LOT** [2] requires computing $2N$ ($N$ for pairwise distances setting) optimal transport maps to a reference distribution in order to embed distributions as functions. This relies on the existence of optimal transport maps, which typically holds only for the 2-Wasserstein distance. Another embedding approach, **Wasserstein Wormhole**, uses deep neural networks to map empirical distributions into finite-dimensional embeddings. These networks are trained iteratively using $N$ Wasserstein distance computations. Compared to LOT, Wormhole does not require the existence of transport maps and can accommodate any ground metric. LOT is also sensitive to the choice of reference distribution (which must be continuous in theory), and for any new out-of-sample distribution it must compute a new optimal transport map—typically super-cubic in the sample size $n$. In contrast, Wormhole only requires encoding the distribution with the trained networks, usually quadratic in $n$. Thus, Wasserstein Wormhole is often the most practical approach. Wasserstein distances and transport maps involved in training can also be approximated using regularized OT to reduce complexity to quadratic in $n$.\n\nWe propose a new approach for the multiple-pairs setting by formulating a principled **regression framework** (as noted by Reviewer **txMr**: *“The method is elegant, principled, and provides a practical way to ‘bootstrap’ a collection of cheap estimates to learn a high-quality one.”*). Specifically, we regress Wasserstein distances onto predictors chosen to be sliced Wasserstein and lifted sliced Wasserstein distances for computational efficiency (other distances or approximations such as Sinkhorn can also be used). The regression model is estimated using only $M << N$ distribution pairs, for which both Wasserstein distances and predictors are computed. The remaining $N-M$ Wasserstein distances are then predicted from their predictors, which can be computed in nearly linear time in $n$ (typically $n \\log n$). The Wasserstein distance for any new pair can be predicted similarly. Like Wormhole, our approach is free of structural assumptions. Furthermore, we show that Wormhole can also be trained using our **predicted** Wasserstein distances to obtain embeddings. Beyond efficiency, formulating Wasserstein computation as a regression task creates a useful bridge between the regression and computational OT communities, enabling further analyses such as hypothesis testing, confidence intervals, prediction intervals, and more.\n\n[1] Kitagawa  et al \"Sliced optimal transport: is it a suitable replacement?\"\n\n[2] Moosmüller et al \"Linear Optimal Transport Embedding: Provable Wasserstein classification for certain rigid transformations and perturbations\""}}, "id": "zS1pgwck6H", "forum": "oa7L4vcJ77", "replyto": "oa7L4vcJ77", "signatures": ["ICLR.cc/2026/Conference/Submission14821/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14821/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14821/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763586885606, "cdate": 1763586885606, "tmdate": 1763586885606, "mdate": 1763586885606, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper's central hypothesis is that for a given meta-distribution $\\mathbb{P}(\\mu, \\nu)$, the true $W_p$ can be accurately and efficiently modeled as a simple linear function of various sliced Wasserstein distances. The core technical insight is to use as predictors a vector of SW based metrics that includes both established lower bounds (e.g., SW, Max-SW, EBSW) and upper bounds (e.g., PW, Min-SWGG, EST).\n\nThe authors claim the weights $\\omega$ for this regression $W_p \\approx \\omega^\\top S_p(\\mu, \\nu)$ are a stable characteristic of $\\mathbb{P}$ and can be learned in a few-shot setting by computing a small number of ground-truth $W_p$ pairs. This yields the RG surrogate which is claimed to be far more accurate than any single SW baseline and outperform deep learning estimators (like Wasserstein Wormhole) in low-data regimes."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This is a nicely presented paper. The authors' central (and successful) bet is that for a given problem domain $\\mathbb{P}$, $W_p$ doesn't just lie somewhere in this interval, but at a consistent relative position that a simple linear model can capture. Thus moves beyond using SW as a fast proxy and instead models the approximation error by bracketing the true value.\n\n- The paper's primary claim of data-efficiency is validated with solid empirical evidence. Table 2 for example demonstrates that with only 100 training pairs, the SOTA deep baseline (Wormhole) is completely unusable (e.g., $R^2$ of 0.65 on ShapeNetV2, -3.6 on MERFISH, 0.04 on scRNA-seq). In stark contrast, the proposed unconstrained RG variants achieve $R^2$ scores of 0.93-0.99 across all datasets, even in the 2,500-dimensional scRNA-seq domain. This high correlation translates directly to downstream utility. For example in the k-NN classification task (Table 1), the RG-seo surrogate (83.5\\% accuracy @ k=5) performs almost identically to the true, expensive $W_p$ (84.2\\% accuracy @ k=5), while the best single SW baseline is not competitive.\n\n- By replacing $W_p$ inside the Wormhole training loop the authors achieve training speedup. Figure 14 shows almost flat scaling of compute with batch size for RG-Wormhole compared to Wormhole’s quadratic growth, showing that the surrogate can be dropped into OT-based architectures without much performance loss."}, "weaknesses": {"value": "- The entire method relies on a bootstrap phase where $M$ ground-truth $W_p$ pairs are computed to fit the regression weights $\\omega$. The paper frames this as a minor setup cost because $M_0=10$ (yielding $M = \\frac{10 \\times 9}{2} = 45$ pairs) works for their experiments. However, the paper seems to provide no analysis on how to determine the sufficient $M$ for a new problem.\n- OLS is only optimal if the error term $\\epsilon$ has constant variance. However figures 6-13 suggests otherwise. Would a simple weighted least-squares (WLS) model yield more accurate surrogate?\n- Def 3 adds inductive bias and has half of the parameters, which is often helpful when having limited observed samples. In Table 2 (the $M_0=100$ low-data setting), the unconstrained model is superior in nearly every case (e.g., RG-seo on ShapeNetV2: 0.95 $R^2$ unconstrained vs. 0.92 constrained). This holds even for data size ($M_0=10$) in the appendix (e.g., Fig 8, ShapeNetV2: RG-seo 0.93 unconstrained vs. 0.90 constrained). The empirical justification for the constrained model seems week here."}, "questions": {"value": "- The RG-seo model requires computing 6 SW-based predictors, some of which (Max-SW, Min-SWGG) require their own iterative optimization. The paper's time complexity analysis and RG-Wormhole training plot (Fig 14) do not provide a direct inference-time comparison. What is the wall-clock time (for a single pair) comparing RG-seo (at inference) to a standard, converged entropic-regularized $W_p$ (Sinkhorn) approximation?\n- How sensitive is the learned regressor to the choice and number of slicing directions (L) and to the specific mix of lower vs upper SW variants?\n- What if we train an RG model only on intra-class pairs (e.g., (chair, chair)) and test its $R^2$ on inter-class pairs (e.g., (chair, airplane))? vice versa?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gNU6N1ZmjN", "forum": "oa7L4vcJ77", "replyto": "oa7L4vcJ77", "signatures": ["ICLR.cc/2026/Conference/Submission14821/Reviewer_AafT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14821/Reviewer_AafT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953053090, "cdate": 1761953053090, "tmdate": 1763588526753, "mdate": 1763588526753, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel regression-based framework for fast estimation of Wasserstein distances between probability distributions. Instead of directly solving the optimal transport (OT) problem or training deep neural networks (e.g., Wasserstein Wormhole, DWE), the authors propose to regress the true Wasserstein distance on a collection of Sliced Wasserstein (SW) and lifted SW distances, which act as lower and upper bounds respectively. \n\nTwo linear regression models are introduced:\nUnconstrained model, admitting a closed-form least-squares solution;\nConstrained model, which enforces the upper/lower bound structure with half as many parameters.\n\nOnce trained on a small set of distribution pairs, the model can predict Wasserstein distances for new pairs at the cost of computing SW distances, significantly reducing computational overhead.\n\nThe authors validate the approach on Gaussian mixtures, ShapeNetV2 point clouds, MNIST point clouds, MERFISH cell niches, and scRNA-seq datasets, showing that:\n\nThe regression estimator achieves high accuracy even with a few training samples.\n\nIt outperforms Wasserstein Wormhole in low-data regimes. Substituting it into Wormhole yields RG-Wormhole, which preserves accuracy while substantially reducing training time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a conceptually simple yet impactful framework for approximating Wasserstein distances through regression on sliced Wasserstein (SW) and lifted SW distances.\nThis formulation is the first to explicitly treat Wasserstein estimation as a supervised learning problem over a meta-distribution of random distribution pairs, bridging the gap between classical OT theory and data-driven surrogate modeling."}, "weaknesses": {"value": "### **Weaknesses**\n\n**1. Limited contribution and unclear computational advantage**  \nThe main technical novelty of this paper lies in Equations (8) and (11), which propose to approximate the Wasserstein distance using a linear regression model on various sliced Wasserstein (SW) and lifted SW distances. While this formulation is conceptually interesting, I have several reservations about its contribution and computational benefits.\n\n**1.1 Computational complexity**  \nThe claimed time complexity is $O(MKL n(\\log n + d))$, where $n$ is the average number of support points per distribution, $M$ is the number of sampled distribution pairs, $K$ is the number of SW variants used, and $L$ is the number of slicing directions.  \nHowever, this is not necessarily more efficient than established OT approximations. For instance, Sinkhorn OT has a complexity of $O(n^2 / \\varepsilon)$ and can exploit GPU parallelism. It is therefore unclear in which practical regime the proposed regression estimator is computationally faster or more scalable. A more quantitative runtime comparison would be needed.\n\n**1.2 Lack of transport plan**  \nThe proposed approach only estimates the **transportation cost**, not the **transport plan**. Hence, it cannot support downstream applications that depend on the coupling (e.g., barycenters, alignment, gradient flows). This limitation makes it less general than classical OT accelerations such as Sinkhorn or low-rank OT methods. If the goal is to compute the Wasserstein distance for a single pair of distributions, this regression-based method would likely not outperform existing solvers.\n\n**1.3 Missing comparison to linear OT**  \nIf the intended use case is computing pairwise Wasserstein distances among many distributions, it would be natural to compare against **Linear Optimal Transport** (see [Bunne et al., 2020, “Linear Optimal Transport”](https://arxiv.org/abs/2008.09165)), which precomputes a reference map and achieves $\\mathcal{O}(C(K,2) n + K n^3)$ complexity for $K$ distributions, where O(n^3) can be improved to o(n^2/\\epsilon) if we use Sinkhorn to construct the embedding. Such a baseline would help clarify the advantages (if any) of the proposed regression-based approach for large-scale pairwise computation.\n\n1.4 \nI suggest including **direct runtime and accuracy comparisons** against classical OT accelerations such as **Sinkhorn** and **Linear OT** in two regimes:  \n(1) a single pair computation, and  \n(2) pairwise computations among many distributions.\n\n---\n\n**2. Minor technical and presentation issues**\n\n**2.1 Typographical error**  \nEquation (13) in Appendix A.1 appears to have a notational error: the term $\\omega^{\\top} S_p^{(k)\\top}$ is not correctly written, as both $\\omega$ and $S_p^{(k)}$ are vectors.\n\n**2.2 Unclear expression in Figure 1**  \nFigure 1 may be misleading. $W_p(\\mu,\\nu)$ is a scalar value, not a vector; similarly, each $S_p^{(k)}(\\mu,\\nu)$ is also a scalar. Thus, describing $W_p$ as an “L2 projection” onto the “span” of the $S_p^{(k)}$ values is conceptually vague, since the term *span* is not well-defined in this scalar regression context. The authors may consider rephrasing this illustration or providing an intuitive geometric explanation.\n\n**2.3 Limited theoretical analysis**  \nWhile the paper provides clean closed-form regression formulas, it lacks theoretical characterization of the estimator’s bias, variance, or generalization error. Since the model relies on a finite sample of distribution pairs, it would strengthen the paper to analyze how the regression error propagates when predicting Wasserstein distances for unseen distributions.\n\n---\n\n**Overall comment:**  \nThe idea of regressing Wasserstein distances on sliced variants is creative and potentially useful for few-shot estimation, but the **computational advantage, theoretical justification, and empirical comparisons** to existing OT approximations remain insufficient. The paper would benefit significantly from a clearer positioning against established methods (Sinkhorn, linear OT) and from a more rigorous analysis of its approximation properties."}, "questions": {"value": "### **Questions**\n\n**Q1. Notation confusion around $P(\\mu,\\nu)$ in Eq. (7)**  \nThe notation of $P(\\mu,\\nu)$ is confusing. From Eq. (7), it appears that $P(\\mu,\\nu)$ represents the *meta-distribution* used to define the expectation in Eq. (8). In this case, $\\mu,\\nu$ should be dummy random variables drawn from that meta-distribution. However, as currently written, $P(\\mu,\\nu)$ seems to be defined as a function of $\\mu,\\nu$ themselves, which are undefined at this point. Could the authors please clarify this notation and its relation to the sampling process in practice?\n\n**Q2. Sampling and training details**  \nHow are the training pairs $(\\mu_i, \\nu_i)$ sampled for regression fitting?  \n- Are they generated synthetically (e.g., Gaussian mixtures) or drawn from actual datasets (e.g., ShapeNet, MNIST)?  \n- What is the typical number of samples $M_0$ used for estimating regression coefficients, and how sensitive are results to $M_0$?  \nProviding more details would help assess reproducibility and stability.\n\n**Q3. Choice of sliced directions ($L$) and stability**  \nThe paper fixes $L$ as the number of slicing directions for each SW computation. How sensitive are the regression results to the choice of $L$?  \nIn high dimensions, random projections can yield high-variance estimates of SW distances—does the regression mitigate or amplify this variance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "H6GH0lyDBb", "forum": "oa7L4vcJ77", "replyto": "oa7L4vcJ77", "signatures": ["ICLR.cc/2026/Conference/Submission14821/Reviewer_mmm4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14821/Reviewer_mmm4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954062313, "cdate": 1761954062313, "tmdate": 1762925168607, "mdate": 1762925168607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}