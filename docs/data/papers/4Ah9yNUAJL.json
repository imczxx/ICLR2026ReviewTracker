{"id": "4Ah9yNUAJL", "number": 9892, "cdate": 1758146837965, "mdate": 1759897688569, "content": {"title": "WidgetEval: Benchmarking Foundation Models on Dynamic Widget Generation for Apps", "abstract": "In this paper, we study the problem of generating application widgets on the fly based on context and user specifications. A widget in this setting encapsulates application APIs while providing a seamless user experience that integrates with the host application. We focus on domains such as Microsoft Excel, Microsoft Word, and synthetic environments such as calendar, file system, and messenger applications. Widget generation is a challenging problem that requires foundation models to understand APIs, write code, reason about application context, and design user interfaces. To understand foundation models’ ability to generate on-demand widgets, we first establish a dataset of widget tasks, test scenarios, and functionality verification mechanisms for various applications. We then devise an evaluation strategy which simulates user interaction with the generated widget and checks for runtime and functionality errors. We compare popular models with various prompting and generation strategies. Overall, we found closed source models such as GPT-4o, GPT-4.1 and o4-mini outperform models such as LLaMa-3.1-70B and Phi4, but even with the best configuration, their success rates on Excel and Word widget tasks remain around 50%.", "tldr": "A benchmark and evaluation framework to test LLM's widget generation capability.", "keywords": ["Code", "language model", "Javascript"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8754d5a174d163e3c9e86a6d8949090a6acad1e5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces WidgetEval, a benchmark for assessing large language models (LLMs) on dynamic widget generation inside existing applications. The goal is to evaluate whether models can understand a user’s natural-language intent, the host app context, and its APIs to generate runnable React components that encapsulate app functionality (e.g., Microsoft Excel/Word via OfficeJS). The work primarily aims to (i) draw attention to a new, practically relevant evaluation setting and (ii) contribute an evaluation pipeline and dataset (tasks, test scenarios, verification scripts) to study model capabilities and failure modes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The data construction process is well-designed and fully traceable, involving a rigorous multi-stage pipeline that spans from OfficeJS documentation crawling and LLM-based synthesis to execution feedback–driven self-repair and manual verification. This design, including a self-repair loop, reflects a strong commitment to data quality and reliability. The experimental analysis is also thorough and multifaceted, covering diverse dimensions such as model type, user instruction form, and UI library, while providing rich error categorization and visual examples—including runtime failures, logical inconsistencies, and UI load analyses—that give readers an intuitive understanding of model behavior."}, "weaknesses": {"value": "The overall task scale is relatively small—the real-world benchmark includes only 50 tasks (35 for Excel and 15 for Word). Although the authors emphasize their complexity, this limited number constrains the generalizability of conclusions about foundation models’ capabilities in complex API and UI generation.\n\nThe evaluation process also relies heavily on automated AI assessment with minimal human involvement. From functionality testing to UI scoring, most stages are conducted automatically. The functional tests use Playwright scripts generated by another LLM to simulate user interactions, which cannot fully capture usability issues that real users might face, such as unintuitive layouts or mismatched interaction logic. Similarly, the UI and UX assessments depend on a vision-based multimodal model to evaluate layout, readability, and cognitive load—criteria that may diverge significantly from those used by human UX experts. This raises concerns about the reliability and validity of the evaluation results.\n\nThe study focuses primarily on one-shot widget generation while overlooking interactive refinement. Given that even the strongest models achieve only around a 50% success rate, it seems unrealistic to expect perfect one-shot generation. A more practical evaluation would involve multi-turn collaboration between users and AI to iteratively debug and improve widgets. Although the paper briefly mentions “follow-up” settings, it does not fully explore this human–AI repair process, which arguably represents the most valuable and realistic aspect of dynamic widget generation."}, "questions": {"value": "1. The reported 50% success rate, even for the strongest models such as GPT-4o and GPT-4.1 on complex Office applications, seems low from a practical standpoint. In real-world settings, a system that fails in one out of two attempts would be difficult to deploy. It would be useful to understand how the authors interpret this one-shot generation paradigm in relation to the actual usability and robustness of LLM-based agents.\n\n2. The benchmark also omits the evaluation of interactive repair, despite acknowledging in its discussion that real users often engage in multi-turn dialogue with the model to iteratively refine or debug widgets. This lack of assessment for human–AI collaboration leaves a significant gap in understanding the models’ practical problem-solving abilities.\n\n3. The description of the “follow-up” scenario is somewhat ambiguous. As currently presented, it seems to test whether a successfully generated widget can generalize to a second task, rather than whether a model can recover from an initial failure using user feedback such as “the button click caused an error.” A clearer definition of this setting would strengthen the paper’s claims about generalization and adaptability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tSZjZkyu7b", "forum": "4Ah9yNUAJL", "replyto": "4Ah9yNUAJL", "signatures": ["ICLR.cc/2026/Conference/Submission9892/Reviewer_1cY8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9892/Reviewer_1cY8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461351487, "cdate": 1761461351487, "tmdate": 1762921354990, "mdate": 1762921354990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a benchmark named WidgetEval for code generation. Each task in WidgetEval is to generate a piece of code that can generate a widget in a specific application under a specific state to satisfy some natural language description. In particular, there are two sources for WidgetEval. The first source is tasks for Excel and Word, and the second source is tasks adapted from NLI (a semantic parsing dataset). This paper also presents empirical evidence for some LLMs to deal with WidgetEval."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. To my knowledge, this is the first benchmark for generating widgets.\n2. The empirical evidence demonstrates that the tasks for Excel and Word are challenging for existing LLMs."}, "weaknesses": {"value": "1. It is unclear how the proposed benchmark is connected to UI development in real world. The proposed benchmark seems to be launching some interfaces under specific program states. Therefore, it is unclear whether the proposed benchmark is helpful for improving LLM-based techniques for UI development.\n2. The tasks in WidgetEval are ultimately in the form of code generation. But it is unclear how WidgetEval differs from existing code generation benchmarks for evaluating LLMs. That is to say, it is unclear whether an LLM that is good at generating code would also be good at generating widgets for WidgetEval.\n3. The synthetic part may be less useful. Since the synthetic tasks are adapted from a semantic parsing dataset, it is unclear how these tasks can provide additional information besides the original semantic parsing tasks when used for evaluating LLMs."}, "questions": {"value": "1. How can the proposed benchmark benefit UI development that existing approaches to GenUI primarily target.\n2. Do you have evidence that WidgetEval can provide additional information that typical code generation benchmarks when evaluating LLM-based approaches to GenUI.\n3. Do you have evidence that LLMs would perform differently for the semantic parsing tasks and the widget generation tasks adapted from these semantic parsing tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TDP4VACrNl", "forum": "4Ah9yNUAJL", "replyto": "4Ah9yNUAJL", "signatures": ["ICLR.cc/2026/Conference/Submission9892/Reviewer_HSFF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9892/Reviewer_HSFF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823912818, "cdate": 1761823912818, "tmdate": 1762921354436, "mdate": 1762921354436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the generation of application widgets on-the-fly. Widgets are a particular type of code generation that usually involves interacting with a UI and wrapping APIs that are made available by the applications. This type of generation is challenging because it requires understanding the application context + APIs, and reasoning about the UI/UX. The paper puts together a benchmark of widget generation tasks that focus on applications such as Excel and Word, and a series of tasks that focus on synthetic applications. While the tasks are not that hard, the performance of the models, even the closed-source ones, are generally lower than 50%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Putting together a relevant dataset that could be used by the research community to work at the intersection of UI manipulation and code generation"}, "weaknesses": {"value": "The paper is in a niche domain and it is not clear how many would use/benefit from the benchmarks introduced in this work."}, "questions": {"value": "Can you elaborate on the utility of the widgets? How would they be used? How do they simplify user experience?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cji8hnl1Sk", "forum": "4Ah9yNUAJL", "replyto": "4Ah9yNUAJL", "signatures": ["ICLR.cc/2026/Conference/Submission9892/Reviewer_EHCv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9892/Reviewer_EHCv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924526431, "cdate": 1761924526431, "tmdate": 1762921354145, "mdate": 1762921354145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the task of automatic widget creation for simple, repetitive tasks in existing UI applications (e.g. word and excel). The task is well motivated. Building these widgets can be tedious for users but the widgets themselves can be quite useful, since they provide intuitive interfaces that allow users to do many repetitive tasks. \n\nThe paper introduces a benchmark created out of real and synthetic tasks and presents an evaluation some strong closed and open source models. The analysis sheds some light into the failure modes of these models on the widget generation task and highlight the difficulty of the problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strengths of the paper are the following:\n\n**Task Importance and Impact**  The task is well-motivated. \n\n- Widgets can greatly increase productivity and improve user experience. The choice of Word and Excel, which are some of the most widely used applications, mean that progress on this task can have broad impact. \n\n- The task is challenging along many dimensions. It not only requires code generation, and state tracking but also aspects of UI design, and ability to generalize to future needs. \n\n**Benchmark Construction** The major design elements in the construction of the benchmark are sound. \n- It covers the main types of use cases for benchmark creation (specific task, generalizing to all tasks of the same type, and more generic description of widget capabilities). \n- While the instances are sourced using LLMs, adequate controls (including manual inspection) are employed to ensure that the tasks are well-defined. \n- It also includes a synthetic dataset, which is automatically generated \n\n**Evaluation** The central pain point in tasks such as these is evaluation. We need to check if the widget is functionally correct and not just compare it to reference code. \n\n- The benchmark provides such a state-based evaluation to assess the effectiveness the automatically generated widgets. Playwright based evaluation allows for simulating user interaction with the widget. The generated widget is thus executed using the simulated user and the final state after widget execution is compared against an expected state.  \n\n- The evaluation is reproducible -- it provides resettable states via init scripts that ensure the state of the environment (i.e. document or spreadsheet) is the same for every run. \n\n- The evaluation also covers UI aspects in addition to functional correctness. This evaluation considers (VLM judge) scores on five different dimensions covering ease of use, and layout aspects."}, "weaknesses": {"value": "There are two key weaknesses in the paper.\n\n**Size of the dataset** There are only 50 main instances in total from the existing applications (e.g. Excel and Word applications). This is too small a dataset for a community wide benchmark. The synthetic dataset while useful as a supplement appears to be substantially easier (and different) from the main dataset. The details of this dataset construction are sparse. In particular, it is difficult to assess whether this is a high quality dataset since there isnt much description of what kinds of quality control was done on this dataset and no indication or discussion as to whether the tasks would benefit from widgets in the same way as the original tasks. \n\n**Analysis** \n- The results are presented mostly as just observations of what was found. \n- The error categorization is useful but is at a higher level. The analysis doesnt provide insights into why the models failed in these cases. \n-"}, "questions": {"value": "- Can you please provide some justification on why the synthetic dataset represents a meaningful collection of tasks that can benefit from widget generation? Also please provide some more details on what kind of quality control was done on these.\n- I am not quite sure about the number of instances in the training datasets. Are there only 50 instances in the main dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lJz44WQgWO", "forum": "4Ah9yNUAJL", "replyto": "4Ah9yNUAJL", "signatures": ["ICLR.cc/2026/Conference/Submission9892/Reviewer_ELhL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9892/Reviewer_ELhL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044597031, "cdate": 1762044597031, "tmdate": 1762921353763, "mdate": 1762921353763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}