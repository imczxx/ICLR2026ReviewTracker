{"id": "OsZr5T7Cd0", "number": 4642, "cdate": 1757732838461, "mdate": 1763637655108, "content": {"title": "ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs", "abstract": "While most autoregressive LLMs are constrained to one-by-one decoding, diffusion LLMs (dLLMs) have attracted growing interest for their potential to dramatically accelerate inference through parallel decoding. Despite this promise, the conditional independence assumption in dLLMs causes parallel decoding to ignore token dependencies, inevitably degrading generation quality when these dependencies are strong. However, existing works largely overlook these inherent challenges, and evaluations on standard benchmarks (e.g., math and coding) are not sufficient to capture the quality degradation caused by parallel decoding. To address this gap, we first provide an information-theoretic analysis of parallel decoding. We then conduct case studies on analytically tractable synthetic list operations from both data distribution and decoding strategy perspectives, offering quantitative insights that highlight the fundamental limitations of parallel decoding. Building on these insights, we propose **ParallelBench**, the first benchmark specifically designed for dLLMs, featuring realistic tasks that are trivial for humans and autoregressive LLMs yet exceptionally challenging for dLLMs under parallel decoding. Using ParallelBench, we systematically analyze both dLLMs and autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can suffer dramatic quality degradation in real-world scenarios, and (ii) current parallel decoding strategies struggle to adapt their degree of parallelism based on task difficulty, thus failing to achieve meaningful speedup without compromising quality. Our findings underscore the pressing need for innovative decoding methods that can overcome the current speed-quality trade-off. We are releasing our benchmark to help accelerate the development of truly efficient dLLMs.", "tldr": "", "keywords": ["diffusion LLMs", "parallel decoding", "benchmark"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d42d3ec17c31e0aec6c768665e3e884fa58d3a8a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presented an information-theoretic analysis on the capabilities of diffusion LLMs using toy tasks to compute their theoretic bounds. The paper performed a distribution based analysis and a decoding strategy based analysis. The paper also presented ParallelBench, composed of the toy tasks and realistic tasks, to evaluate the capabilities of dllms empirically."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a theoretical explanation for the capabilities and limitations of parallel decoding with dLLMs. This provides some intuition for whether to use dLLMs or whether to enable decoding multiple tokens per step depending on the task at hand. The theoretical result is corroborated with empirical results on the same toy tasks.\n- The empirical evaluation performed a wide variety of ablations that provide some insights on how different unmasking techniques and models compare.\n- The presented benchmark can serve as a baseline for evaluating whether a particular language model/decoding technique can adaptively exploit parallelism, beyond dLLMs."}, "weaknesses": {"value": "- The empirical evaluation on realistic tasks seem to deviate from the expectations set by the theoretical results. For example LLaDA 1.5 performs better on Latin Square than Sudoku, but Sudoku has C(Y|X) = 0 while Latin Square has C(Y|X) > 0. This weakens the significance of the theoretic analysis. The difference in complexity from the toy tasks to realistic tasks seem dominate the difference in task type.\n- The empirical results for 3f-j show accuracy going down as the # tokens per step increase, which is to be expected that the more naive parallelism (as in the technique does not explicitly target/decide when parallelism is appropriate) employed the worse the accuracy would be. Most realistic tasks should fall into these regimes, rather than the extremes of Copy or Replace Index. And it is unlikely to be possible to perform the same information-theoretic analysis on realistic tasks to quantify the theoretic accuracy. \n- The best the analysis provides is an intuition, which in general can be useful. However, the intuition that more dependencies between tokens in the expected response makes parallelism harder is hardly surprising. The difficult task is knowing when parallelism can be enabled, which the work does not provide directions towards any solutions, as the toy examples shown do not generalize/scale to real examples."}, "questions": {"value": "- How is it that Sudoku having supposedly C(Y|X) = 0 and Latin Square having C(Y|X) > 0, but the empirical results showing that LLaDA 1.5 performs better on Latin square than Sudoku? \n- On Figure 7, what is the arrow indicative of?\n- Is there a reason why ParallelBench is for evaluating dLLM specifically and not parallel decoding in different LLMS (e.g. parallel decoding in AR LLMs) in general?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vrqZiyjVQC", "forum": "OsZr5T7Cd0", "replyto": "OsZr5T7Cd0", "signatures": ["ICLR.cc/2026/Conference/Submission4642/Reviewer_vHaA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4642/Reviewer_vHaA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761853423403, "cdate": 1761853423403, "tmdate": 1762917486752, "mdate": 1762917486752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "To AC and All Reviewers (1/2)"}, "comment": {"value": "We want to thank all reviewers for their thoughtful comments and helpful feedback. We are particularly encouraged that the reviewers found that (i) our paper tackles a core challenge (`R-uEfF`), (ii) our paper is clearly written (`R-WAQL`, `R-o6Fd`), (iii) our paper provides solid theoretical analysis (`R-WAQL`, `R-vHaA`) with empirical validation (`R-vHaA`) and meaningful insights (`R-uEfF`, `R-vHaA`), (iv) our theory connects well to practice (`R-o6Fd`) with a well-rounded evaluation (`R-WAQL`), (v) our benchmark is comprehensive (`R-uEfF`, `R-vHaA`) and fills a gap (`R-o6Fd`), and (vi) our publicly released code (`R-WAQL`) and benchmark (`R-WAQL`) facilitate future research (`R-WAQL`, `R-vHaA`) by addressing limitations of existing benchmarks (`R-uEfF`).\n\nIn response to the feedback, we addressed each concern, added new results, and updated the paper accordingly, including newly added `Appendices J, K, L, M, N, and O`. \n\n---\n\n## **Major Update 1: Clarification on Real-World Coverage**\nWhile we agree that ParallelBench may not capture all aspects of real-world tasks, this design choice follows the same philosophy used by existing benchmarks like RULER [1] and IFEval [2]. Although these benchmarks are far from realistic scenarios, they deliberately adopt simplified environments to isolate particular capabilities, and they are widely recognized as valuable contributions.\n\n|Benchmark|Target Capability|Tasks|\n|-|-|-|\n|RULER |Long-context modeling|Synthetic long sequences; needle-in-a-haystack retrieval|\n|IFEval | Instruction-following|Unnatural, rule-heavy prompts (e.g., include specific letters)|\n|**ParallelBench** (ours)|Parallel decoding| *Waiting Line, Text Writing, Puzzles*|\n\nSimilarly, while we design our tasks to be as realistic as possible, our primary goal is to stress-test parallel decoding by exposing when the conditional independence assumption breaks down. Just as RULER and IFEval have advanced our understanding of long-context modeling and instruction-following, we believe that ParallelBench provides a similarly valuable foundation for developing intelligent parallel decoding methods in dLLMs.\n\n[1] RULER: What's the Real Context Size of Your Long-Context Language Models?  \n[2] Instruction-Following Evaluation for Large Language Models\n\n---\n\n## **Major Update 2: Comparison with Existing Benchmarks**\nParallelBench offers two major components that make it distinct from existing benchmarks.\n\n### **Simple but Hard-to-Parallelize Tasks**\nUnlike existing benchmarks such as GSM8K, we design simple tasks that achieve near-perfect one-by-one decoding accuracy but suffer severe degradation under parallel decoding due to the conditional independence assumption, even for ideally trained models. ParallelBench captures exactly these stress-test scenarios.\n\n### **Broad Coverage of Parallelization Difficulty**\nWe showed in [Figure 6](https://anonymous.4open.science/r/parben/f06.png) that existing benchmarks such as GSM8K and HumanEval cover only a narrow region of the easy-to-parallelize spectrum. To further validate this observation, **we additionally analyzed MATH [1] and IFEval [2]**, which likewise span only a narrow portion of the difficulty spectrum. In contrast, ParallelBench spans a much wider range, from very easy-to-parallelize tasks, such as *Copy*, to very hard-to-parallelize tasks, such as *Shuffle*.\n\nDeveloping new parallel decoding methods only on a narrow range of benchmarks risks overfitting to a small range of parallelization difficulty. In contrast, ParallelBench enables rigorous evaluation of whether a method can intelligently adjust its degree of parallelism from very low to very high depending on task difficulty, thereby supporting the development of truly efficient dLLMs.\n\n[1] Measuring Mathematical Problem Solving With the MATH Dataset  \n[2] Instruction-Following Evaluation for Large Language Models"}}, "id": "BmsIybmaMu", "forum": "OsZr5T7Cd0", "replyto": "OsZr5T7Cd0", "signatures": ["ICLR.cc/2026/Conference/Submission4642/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4642/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4642/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763638794719, "cdate": 1763638794719, "tmdate": 1763638794719, "mdate": 1763638794719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ParallelBench, a benchmark and analysis suite for diffusion LLMs (dLLMs) that isolates the speed--quality trade-offs of parallel decoding. It provides an information-theoretic lower bound (via conditional total correlation) showing why parallel decoding degrades when token dependencies are strong, confirms the theory on analytically tractable list operations, and then demonstrates the same failure modes on realistic tasks across three categories (Waiting Line, Text Writing, Puzzles). The study further contrasts static (Top-k) and adaptive (Threshold) unmasking, semi-AR strategies, and \"oracle\" per-sample thresholds, revealing substantial headroom for adaptive methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The theory connects nicely to practice: the conditional-correlation argument predicts the “New City” style errors you later see in real tasks. It’s satisfying when the math lines up with the plots.\n\n2. The benchmark fills a gap. We've all seen parallel decoding look great in one demo and fall apart in another; this suite finally gives a way to measure where it cracks and by how much.\n\n3. The oracle threshold curves are especially helpful -- they show there’s real headroom for adaptive, per-sample control rather than one-size-fits-all knobs.\n\n4. The write-up is clear and the figures are readable; I didn’t have to reverse-engineer the setup to follow the argument."}, "weaknesses": {"value": "1. Coverage feels a bit narrow in the main text. A compact table that pulls model/method results out of the appendix would make the big picture easier to scan.\n\n2. The quality metrics are fine, but a couple more \"at a glance\" indicators (e.g., BERTScore or constraint-violation counts) would help interpret how quality degrades as parallelism goes up.\n\n3. Most examples are short sequences. One longer-form scenario in the main paper would help readers judge whether the conclusions hold in more realistic lengths."}, "questions": {"value": "1. You show big gaps between fixed thresholds and the oracle per-sample threshold. How close can a simple learned heuristic (say, a tiny classifier on token stats) get to the oracle without heavy training?\n\n2. The task taxonomy is useful. Could you add a small table of \"meta-features\" (rough dependency strength, locality vs. global constraints, etc.) so practitioners can guess which decoding policy to use before running heavy tests?\n\n3. Semi-AR helps sometimes and hurts other times. Do you envision a lightweight runtime policy that flips block sizes per sample using quick signals (entropy, margin, \"AR-ness\")? Any early results there?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UH9IhDLGJ0", "forum": "OsZr5T7Cd0", "replyto": "OsZr5T7Cd0", "signatures": ["ICLR.cc/2026/Conference/Submission4642/Reviewer_o6Fd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4642/Reviewer_o6Fd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888681769, "cdate": 1761888681769, "tmdate": 1762917485084, "mdate": 1762917485084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "his paper investigates the speed–quality trade-off in parallel decoding for Diffusion Language Models (dLLMs). Although dLLMs promise faster inference through parallel decoding, the underlying conditional independence assumption often leads to severe quality degradation in tasks with strong token dependencies. To expose this issue, the authors provide an information-theoretic analysis and introduce ParallelBench, specifically designed for evaluating dLLMs under parallel decoding. It comprises 17 tasks across three categories (Waiting Line, Text Writing, Puzzles) that are easy for humans and autoregressive (AR) models but challenging for dLLMs. Experiments reveal that (1) dLLMs experience substantial quality loss during parallel decoding, and (2) existing decoding strategies (both static and adaptive) fail to balance quality and speed effectively."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper tackles a core dLLM challenge: the quality impact of parallel decoding, a key advantage over AR models, and highlights the limitations of existing benchmarks in assessing it.\n2. Provides theoretical insights.\n3. Clearly illustrates token dependency variations across tasks through analysis of synthetic “list operations” such as Copy, Replace Index, Replace Random, and Shuffle.\n4. Systematically evaluates multiple dLLMs—including LLaDA, Dream, and the closed-source Mercury—across various decoding strategies such as Top-k, Threshold, and Semi-AR on PARALLELBENCH."}, "weaknesses": {"value": "1. Need for benchmark comparison: More experiments are needed to show PARALLELBENCH’s added value over existing benchmarks.\n2. Missing actual speed measurements: Experiments focus on parallelism vs. quality, but no wall-clock latency or time–quality curves are provided.\n3. Limited real-world coverage: The benchmark may not capture all challenges dLLMs face in complex, open-ended tasks."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8IHwVfLBvZ", "forum": "OsZr5T7Cd0", "replyto": "OsZr5T7Cd0", "signatures": ["ICLR.cc/2026/Conference/Submission4642/Reviewer_uEfF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4642/Reviewer_uEfF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902659770, "cdate": 1761902659770, "tmdate": 1762917484805, "mdate": 1762917484805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies parallel decoding in diffusion-based large language models (dLLMs) in order to address the challenge that parallel decoding often fails to capture inter-token dependencies leading to quality degradation, due to the conditional independence assumption among tokens.\n\nThe authors make three main contributions:\n\t\n1. Information-theoretic analysis — They formalize the lower bound of parallel decoding quality loss using conditional total correlation C(Y|X), proving that even ideal models face an inherent speed–quality trade-off.\n\t\n2.\tSynthetic case studies — They analyze list operations like Copy, Replace Random, and Shuffle to quantify how dependency strength affects parallel decoding accuracy.\n\t\n3.\tPARALLELBENCH — A new benchmark with 17 tasks across 3 categories (Waiting Line, Text Writing, Puzzles) to empirically evaluate dLLMs and AR LLMs. It exposes how parallel decoding degrades quality in realistic settings and shows that current adaptive unmasking methods (e.g., Top-k, Threshold) cannot fully balance speed and accuracy.\n\nThe paper concludes that dLLMs suffer from severe quality degradation under parallel decoding, especially in dependency-heavy tasks, and that current decoding strategies fail to adapt parallelism dynamically to task difficulty."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The use of conditional total correlation to quantify unavoidable quality loss provides a solid mathematical basis for analyzing the parallel decoding trade-off, making the paper has clear theoretical grounding.\n\n2. The combination of theoretical proofs, synthetic tasks, and realistic benchmarks (including grammar-sensitive and reasoning tasks) provides a well-rounded evaluation.\n\n3. The combination of theoretical proofs, synthetic tasks, and realistic benchmarks (including grammar-sensitive and reasoning tasks) provides a well-rounded evaluation.\n\n4. The writing, presentations accompanied with interpretations are overall clear. Code is publicly released, and the benchmark tasks are well-documented, facilitating future research."}, "weaknesses": {"value": "1. Although 17 tasks span diverse categories, most involve short outputs or synthetic patterns; results may not generalize to long-context reasoning or dialogue generation.\n\n2. Comparisons largely focus on fixed unmasking or basic threshold schemes, missing newer adaptive scheduling approaches (e.g., dilated scheduling, SlowFast decoding, or hybrid AR-diffusion). \n\n3. The paper focuses on decoding strategies, not how model pre-training or architecture might affect the issue. \n\n4. Other decoding/scheduling work that should be covered (and possibly compared): the paper could be strenghthen to include decoding strategies that cover dynamic stage-based scheduling [1], dilated unmasking[2], block decoding [3,5] and revocation/remasking [4]. \n\n\n\n-----\nReferences\n1. Accelerating Diffusion Large Language Models with SlowFast Sampling: The Three Golden Principles (Wei et al., 2025) \n2. Plan for Speed: Dilated Scheduling for Masked Diffusion Language Models (Luxembourg, Permuter, Nachmani, 2025) \n3. Fast‑dLLM: Training‑free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding \n4. Wide‑In, Narrow‑Out: Revokable Decoding for Efficient and Effective DLLMs (Hong et al., 2025)\n5., Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models (Arriola et al., 2025)"}, "questions": {"value": "1. The oracle analysis suggests per-sample thresholds could yield large gains — how feasible is this in practical inference (e.g., latency, calibration)?\n\n2. How do results change with larger blocks or variable block scheduling strategies beyond fixed lengths (with semi-AR)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CiDFtfl4we", "forum": "OsZr5T7Cd0", "replyto": "OsZr5T7Cd0", "signatures": ["ICLR.cc/2026/Conference/Submission4642/Reviewer_WAQL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4642/Reviewer_WAQL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4642/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956671886, "cdate": 1761956671886, "tmdate": 1762917484573, "mdate": 1762917484573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}